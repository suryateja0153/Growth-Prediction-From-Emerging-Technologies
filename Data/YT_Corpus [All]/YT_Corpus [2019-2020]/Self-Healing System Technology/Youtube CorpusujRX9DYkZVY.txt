 My name is Xinlong Li, I'm the principal engineer from Tencent Cloud. Today I will share some attempts about software and hardware integrated designs from our team. So I'll start sharing from the following three perspectives. First, the past and the present of NVM. In this section, we'll talk about the trouble of storage system, some attempts to solve this problem by using new storage and our research and practices on using new storage. Second, I will show how we use persistent memory to improve the database performance and availability. Third, I will talk about academic research on persistent memory in databases and our vision for persistent memory in databases. First, the past and the present of the NVM. What's the trouble with database storage? I think, from two perspectives. One is delay, the other is capacity, and the gap between the CPU cache to DRAM is one order of magnitude, from 10 nanoseconds to 100 nanoseconds. But the gap between DRAM and SSD is huge, it's from a hundred nanoseconds, to a hundred micro seconds. It is about three orders of magnitude. So hot data must be stored in DRAM,  but that data is volatile. We need non-volatile storage to store the hot data. This will improve the performance. On the other hand, the CPU performance grows faster and the data demands grow faster. But the capacity of DRAM grows slowly, from 4x increase in three years to 2x increase in four years. So, I think persistent memory is perfect for the storage system. The access delay of the persistent memory is from 100 nanoseconds to 1000 nanoseconds, that is 1 microsecond, according to different product forms. The concept of persistent  memory was proposed very early, but the commercial product was only released in recent years due to development of new material. Here we list some materials  to build persistent memory. First MRAM, a type of nvm which stores data in magnetic domains, and FRAM, which uses ferroelectric layer to achieve non-volatility. and it is worth mentioning that PCM uses the unique behavior of chalcogenide glass, is announced as 3D xpoint memory cell, which is developed by Intel and Micron. And the delay of the PCM... This table gives the comparison of PCM DRAM and NAND flash. The access delay of PCM is between DRAM and NAND flash. and they can be used much longer because its erase time is much more than that of NAND flash, so it can be used much longer with lower static power. PCM can be accessed in bytes but NAND flash can only access in a page and of course PCM is nonvolatile. So intel uses 3D Xpoint to build 2 kinds of products. One is SSD, Optane SSD and the other is a memory device, AEP. We hope to pursue the ultimate database improvements through the new storage. So we chose AEP for further research. AEP has a complete system on chip, the Optane media, which is made by 3D Xpoint, is on both sides, primary side and secondary side. And energy store caps ensure flash on all the models when the power fails. AIT DRAM holds the address indirection table, so it is a complete system on chip. And now I will talk about how the CPU uses AEP in computer systems. As we know, the current CPU architecture is NUMA,  Non-Unique Memory Access architecture. So the CPU cores in the same socket can access the AEP much faster than that from remote sockets. Because remote socket access this memory will go through UPI bus, it costs a lot when the load is heavy. And the AEP can be configured as memory and persistent memory, as memory AEP will shadow the DRAM. The memory capacity is the AEP's capacity. And as persistent memory, AEP is displayed as a device on the operating system separated from the memory. The memory size is the DRAM's size, and AEP's size. The AEP can be accessed as a raw device, or block device, or as persistent memory. And now we'll see how we use AEP on the operating system. As a raw device it is hard to use, so AEP is normally accessed as a block device or as persistent memory. In old operating system versions, AEP can only be accessed as a block device like SSD, and it spends much more time on the operating system and filesystem than the device itself. In a persistent-memory-aware filesystem, AEP can be accessed as a block device using a POSIX interface as well as persistent memory using a PMDK interface. AEP is mapped directly from device to userspace by using the PMDK interface. PMDK is an mmap-like interface. We compared the write performance of the PMDK interface and the POSIX interface by using different block sizes, from sixty-four bytes to thirty-two kilobytes and we found two things. The write latency using PMDK is much lower than that of the POSIX interface. The yellow in the graph is PMDK and blue is POSIX. The higher, the more data was accessed in the same time... in the same access time. And the best PMDK performance block size I think is a quarter kilobytes to two kilobytes. In this section I will talk about AEP and CDB & CynosDB Integration Optimization. As we know, CDB is the most popular product from Tencent DB and it is fully compatible with the MariaDB ecosystem. The main node and its replica share different storage. It's share-nothing architecture. The typical writing process is as follows: the data first writes the buffer pool, then writes the log buffer and the log writer asynchronously flushes the data to the redo log. When a commit request comes, we first flush the binlog then check and wait if all the redo log has been flushed or is permanent. If, with semi-sync configuration, the master nodes need to wait for the ack from the replica before the transaction is committed. So, we found the redo log writing process, the binlog writing process and the relay log writing process may be the key processes which affect the performance. The key point to optimize those writing processes is to use AEP as persistent memory. Now, here we show how we optimize the redo log writing process. We do three things; one, we write to disk once to complete the persistency. We disable the log flusher thread and log writer thread flushes data directly and then notifies the user that the data has been made permanent. And second, we use the PMDK interface, of course, to write the redo log file. And then we bind the log write thread to the cores which has the memory bus to access AEP, which means we bind the log writer to the local CPU cores or the nearby CPU cores to the AEP. Second, this slide shows how we optimize the binlog and relay log. As we know, the binlog commit is a group commit. It has three stages, the flush stage, the sync stage and the commit stage We remove the sync stage and flush data directly, then go to commit stage. This stage checks and wait if the redo log is flushed. And then we use the PMDK (interface) to flush data to binlog and relay log, like the redo log. And then we bind the three threads to the right proper CPU cores. The first is the binlog dump thread, second is relaylog IO threads and third is relaylog SQL thread. Because the binlog dump thread writes the data and then sends the data to its replicas, and each replica has its IO thread which flushes the data to the relay log and the SQL thread read the relay logs to apply them. So we bind these threads to improve the performance. A lot of work has been done to bring this to production. As we know PMDK, is a map-like interface, and its space is a fixed size after the file is open. So we do a lot of work to simulate the appending writing like POSIX (interface). This will make PMDK interface and POSIX interface compatible . As a result, the writing performance has been significantly improved; standalone, with the redo log optimization, a 15% increase in writing performance, and asynchronous, with the redo log and binlog optimization, a 50% increase in writing performance And synchronous, with redo log /binlog /relaylog optimization. 60% increase in performance. With sync_relay_log set to 1, strong synchronous, 2200% increase in writing performance. As we know, here, every single event has to be flushed and synced by the relay log and then send ack to the master. So if we skip the relay log sync stage, this will give a lot of help. Finally, I will share the work of AEP on CynosDB. CynosDB is a novel architecture to the relational database which offloads the data to a fault tolerant and self-healing cloud storage CynosDB just sends the redo log to storage, and the storage applies the redo log to the page. CynosDB reads pages from cloud storage through the net. This design improves database availability and reliability, but performance is the issue when the buffer pool size is limited, but the data grows fast. So we designed two layers of the cache system. The first is the primary cache in the buffer pool as usual. The second is the secondary cache on AEP, which buffers many more pages. The secondary cache improved performance by changing remote IO to local IO, and CynosDB can warm up quickly from a crash with many pages in AEP still available. Finally, we'll talk about the way ahead. First, we talk about an academic paper on database indexes designed with persistent memory. As we know, the B and B+ trees are based on page-by-page reading of traditional disks; the read delay is high and the less access, the better the performance. So the height of index must not be too high. Because if we want to access the leaf node and the height of the trees is too high, more pages will be read in this path. But persistent memory breaks this limit, which makes it possible to design better performing indexes with persistent memory. This table lists a summary of persistent index technology from 2011 to 2019 in various top academic conferences. You can see the discussion in this area is maturing and we will publish our own paper in the very near future. And in our vision for the future, all pages will be stored in persistent memory, not only the binlog and the redo log, which means the buffer pool is in persistent memory and will hold the all data. This design will lead to very fast checkpoints or no checkpoints at all. So the recovery will be very fast. When the data in the buffer pool in persistent memory is permanent, the log will reduce or disappear, which improves writing performance significantly. So this is all I have to share, thanks! 