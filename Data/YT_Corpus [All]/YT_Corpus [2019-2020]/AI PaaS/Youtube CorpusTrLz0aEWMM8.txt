 We will get out of the way so that Gautam and can show off some magic with CDS and .NET. Over to you, Gautam.  Thank you folks. Thank you so much, Donna. Thank you, Seth, for that warm introduction. Now, let me go ahead and share my screen. Hopefully, you guys can start seeing it and maybe we can get the session kicked off. All right, cool. Good morning, good afternoon, evening to all of you. My name is Gautam Thapar. I'm a Product Manager on the Common Data Service Team. Thanks to all of you for joining us today at our very first Powerful Devs conference. I want to caution you, it is my eighth day on the job. This is the new job that I've taken very recently, so if I don't have answers to any of the things that you ask today, I promise you that I will get them to you as quickly as possible. Today, the top track is going to be about the Common Data Service; what is this about; how is this going to be interesting to you. We'll cover different interfaces or interactions that you will have with the Common Data Service, so how do you import data into the Common Data Service; how do you get it out of the Common Data Service. Once you have the data inside the Common Data Service, what can you really do with it? Can you build Azure Apps using it? For the known things that you already are using today, things like service bus, Events, APIs, and Functions, how can we magically integrate with some of those capabilities? We'll have some demo sessions for those capabilities as well. Then we'll finally wrap up by talking about once you have this data in CDS, how can you actually go create some cool reports and visualizations and stuff using that. Now, I want to put a word out there that this action is going to be heavily biased towards demos and that it was primarily driven based on feedback that we got from the Build Conference as well because this is somewhat of a redo of that session based on the enormous and the positive feedback that we received. I want to give a big shout-out also to one of my colleagues, Marc Mercuri, who actually ended up doing the recording for this session because this is my eighth day on the job, as I told you about. Towards the end, like Seth already mentioned, we will have a Q&A session on the blog site for the session, please feel free to post things over there. I'll be joined by Jim Daly, who's a pretty senior and tenured member on the CDS team to help with some of the answers over there. Let's get going. At the heart of any development project that you take on is really digital transformation. The term is overloaded because it means different things to different people and different organizations, but when you look at it, at the core of it, it's really the need to drive efficiency. That's what we really look forward to when we build products like the Common Data Service. To explain that a bit further, let's actually really start with an example that'll home the point to you about what Common Data Service is all about. Let's take an example of this organization that is going through their digital transformation journey, and what they're doing is transforming and automating their supply chain. As part of their digital transformation, they've gone ahead and built a system that digitally tracks their supply chain, but they want to actually go add more. So they want to add new scenarios where a customer could come and search the product and the products' journey through the system and basically get a little bit more meaningful data out of it. Easy, right? Let's see what it takes to actually go build these two requirements into the product. You have an existing app. You have a source database. You're actually going ahead and taking the source database, and pulling all the relational database out of it. You have some unstructured data that you're pulling into files, and images and stuff into blogs, and now you're adding search and find capabilities to the platform. Think about it. These are three disparate systems. We need to coalesce the data coming out of three systems and then meaningfully provide it back to the existing app that exist there, so you probably need a web tier that sits in front of it. This web tier is going to go ahead and go across these disparate API surface areas, capture all the information, and then provide it to the existing app. Sounds good, but as we start building this, there's a couple of more requirements that come into with the execs pretty typical of any application that you end up building. Let's try to absorb those requirements. Well, you want to go ahead and add some real-time semantics to the application, so which means if something's happening, let's take some action. Let's do some eventing into the system so we can respond to things. While we're at it, for Product Managers like me, how about we add some reporting capabilities so we can send these awesome reports about how the system is doing, how the application is doing, and so on. To gain that competitive advantage, how about we take all the data that we have now coalesced into a data lake and in addition to it, add some AI on top of it so we can do some predictive analysis. Not a problem. It sounds pretty cool. At Microsoft, being the organization that we are, we provide businesses everything, right from the infrastructure that you could use to actually host these services, to platforms, to finish SaaS services to take care of all the scenario that we just talked about in the previous slide. You have SQL DB that replaces your relational databases. You have Azure Storage to take care of all your unstructured data needs. You have Azure Search to talk about your relevant search. You have API App that is acting as a middle tier. You have Logic Apps and Service Bus taken care of all your eventing needs. You have Power BI for all your reporting and stuff that you want to do and then for anything that you want to do with data coalescing and then eventually doing any machine learning on top of it, you can actually go ahead and put Azure Data Factory and then get the data into Azure Data Lake and use Azure Remote to build your custom models and build AI models on top of it. That's awesome, but the immediate recognition that you would have is that 10 different services are at play over here. That means you need to understand 10 different API services, their pricing models, how they scale, how they handle security collectively together, which really means that they have to work in unison, and, frankly, how you operate these 10 different services at scale. Not to forget, when all these things come together in the application, you have to think about threading the needle and to enter 10 different services for your telemetry and error handling. That's completely fine if you want to have that fine-grained control, all power to you and we provide that capability. But there's always this thing about, what if I just focused on my core IP for my product and have everything else sort of managed and handled for me? Could there be a managed data platform that does that for me? That's where CDS comes in. With the Common Data Service, we give you the managed data platform that you desire that takes the need of it for all the management overhead that we talked about from all the services that you just saw in this scenario you need to enable. We have data flows that help you bring the data into CDS. We have SQL Database, Azure Storage, Azure Search, Azure Data Lake, Cosmos DB, all bundled together in a heterogeneous managed data service. That is what CDS is all about. So really the point to take home is that you're not making the choice between Azure Services and CDS. Azure Services are what make up CDS because that's what we're built on top of. The secret sauce that we are providing is in making available to you a whole that is more powerful than the sum of its individual parts. You get the same security, privacy, and guarantees that you come to expect from Azure. You literally can use the Service Bus against the API surface for all your eventing needs, Power BI. Out of the box is enabled with a recent announcements with DirectQuery endpoint that we enabled so you can build awesome visualizations and such. For your AI needs, Power Platform itself as a no-code, low-code offering for your AI predictive analysis that you want to do so you'd really don't need to be a data scientist and you can actually go to AI. But if you didn't want to do that, we already, as I mentioned, are heterogeneous store. We have data pumping into an Azure Data Lake which is managed for you. You could actually go run an Azure ML custom model on top of it. In a nutshell, the Common Data Service is really there for any type of your data needs. Heterogeneous data needs from relational data stores to semi-structured data like files and images. You can search things on top of them. You can log your data any way you want to add. Like I said, there's data lake capabilities managed available to you there as well. That, too, you can now do for any type of application. Naturally, Common Data Service is the basis, as Donna mentioned, for our Power Platform and Dynamics 365 Application. So you get a first-class, rich experience for those scenarios. But that doesn't stop you from using the CDS service for your web apps, for your mobile apps, for IoT scenarios, for services, backends, anything that you wanted to, and that's some of the thing that we're going to focus on in the session today and give you demos about. The Common Data Service is really easy to use, because at the root of it, it was built with the premise of the low-code, no-code approach for the citizen persona so that you can model your data. You can model your business tools to your workloads via awesome visual designers that are extremely easy to use. Naturally, it being a managed service, it's extremely easy to operate. You don't worry about your backups; you don't worry about your restores; you don't have to think about disaster recovery. Everything from query optimization and how to scale your service with your data growing is taken care for you, and that too extremely secure because we're built on top of Azure, encryption address and a transit, auditing capabilities and activities getting logged natively into the platform, row-level security, everything built-in into the platform. So you don't have to worry about it. You just focus on doing your core IP work, which is building your app. Sounds awesome, right? So let's actually see what are the things that you can do with the Common Data Service. How can we get data in and out of Common Data Service? There's many ways. One of them is to use the Power Platform data flows and Power Query to move data into CDS. You have capabilities to schedule your data when it comes in, when it goes out. You can do that across a bunch of services. You can do data transformations. You can do map, match, and merge on your data coming from third party services like Salesforce, or first-party applications within Azure or for that matter, your own data-centers as well. That's just one way to do it. Another way is to actually go use Azure Data Factory, and we'll actually go to a demo for that. The idea really here is you can define your datasets, your input datasets, and your output datasets from where the data is coming in and where it is going to. You can define your pipelines, and you can move your data as you wanted to. I made the point before as well, because CDS is the heterogeneous store, you can actually have your data being pumped automatically into a managed data lake at any point in time. If that wasn't enough for you, we have Logic Apps and Power Automate for you to be able to get your data from upwards from 250 connector services ranging anywhere from Adobe to ZenDesk. These connectors are shared between Logic Apps and Power Automate, so you don't have to worry about anything. In fact, 35 of these connectors are for the most common Azure Services, so you are really well covered. For that matter, if you didn't have something that we made available to you, I think Seth mentioned this in his example of what somebody who has tweeted, you go ahead and create a custom connector. Really, all you need is a REST tab sitting on top of your service, your function or your code, whether it is running in a VM, in IaaS, or in a container at Kubernetes services for that matter, not even in Azure. You can actually go and register it behind a custom connector, and now, that is available for you to move data in and out. Now, let's talk about exporting the data. All you did with importing, all those mechanisms available for exporting as well naturally. When you talk about now data within the Azure CDS and you want to use it in applications, it's easily exposable to our OData API. So everything that is exposed in CDS is an object. It's your data, your metadata, your schema, business rules, constraints, everything is an object available, and it's actually AAD backed, so you're completely secured. You don't have to code about anything when it comes to your access mechanisms. If that was awesome for you because you are a managed service, we actually provide you plug-ins where you can register a custom .NET code to be actually invoked at any point in your event pipeline, right from your API request coming into your database operations to your API response. You don't have to worry about at all where is it that you want to run your code. In fact, we make it available to you, to actually go run it automatically, so you can manipulate things. You can do error handling. You can do transformations, modifications, anything that you wanted. If that didn't do for you, you can always go ahead and fire some events through Service Bus, Event Hub, Logic Apps and stuff, and have any of your applications, go subscribe to them and then react to them. We'll actually give you a demo of some of that Azure Functions today on how you can actually have coordinate in Azure Functions, and be able to react with some of those events and then consume it. Let's dive into the demo section of the session today. There's going to be about three demos. Again, shout out to my colleague, Mark, who has actually helped me with this. The first one is going to be about importing data into CDS, and we're going to use Azure Data Factory to do that.  What you can see here is the Azure Data Factory. I've authored a pipeline that's going to copy data from Azure SQL DB into the Common Data Service. I'll do the same and set the two datasets first, so let's take a look at these. So here, I've created a dataset for CDS Azure integration; that's what I call that, and you can see here that I've got a connection that set up, so let's go ahead and open that connection, and you can see here that I've identified the service URI. This is available to you if you look in the settings inside of your CDS environment. Here, I use Office 365 authentication. Typically, what you're going to want to do is use an AAD service principle. I've entered in my details here. This is just for testing, and we're good to go. We click "Test connection" and see that we're good. Go ahead and click "Cancel." Now, once I've set that connection up, I can specify for my dataset which entity I want to choose. Here, I've chosen a shipping company, but I've got multiple that are available to me within there. Then on Azure SQL DB, I'm going to go ahead and create a data source as well. You can see here that there's the connection, and I've used SQL authentication. I've used the username and password I've created. One quick thing to remember here is on SQL DB, just make sure you've set up your firewall settings on your server so you can enable access for Microsoft Services. So now, I've gone ahead and specified the shippers table here, Shippers2, and then going ahead and getting that data over here is actually pretty simple. I'm going to come over here. I'm going to add a new Copy Data item here from "Move and transform." You're going to be able to see here that the source is going to be my Azure SQL DB source. I'm specifying a table as an input here. I also could have used a query or stored proc. I can preview my data here and see what's going to come across. You can see here I've got a Shipper ID, a RowGuid, ShipperName and PhoneNumber, and then on the sync, we're going to go ahead and use our CDS instance here. We're going to do an upsert, which will update or insert the data here, and I'm going to go ahead and do a mapping. Now, in this mapping, I've already got it set up, and so I've got the RowGuid. It is going to the shipping company ID, which is a Guid that's created for me automatically inside of CDS, and then the shipper name is going to map to the name that I've specified over here. So I've got a mapping of the two key fields that I care about over here, and then if I want to, I can go ahead and click "Debug." It's deploying them. It's now going to go ahead and run them for me. You can see now that it's queued. It's in progress, and it succeeded. So you can see here, it's very easy to go ahead and bring data from SQL into CDS. Of course, you can use all the other goodness that comes with Azure Data Factory. But if you just want to get data from SQL into CDS, super-simple.  Excellent. So you saw how easy that was, so let's move on to the next demo that we wanted to do. This is about how you actually use CDS and build a web app on top of it, so we'll use the Blazor web server template for this. Let's dive in.  One of the scenarios we talked about is the ability to bring CDS data into a new or an existing application. So bringing into an existing application is relatively straightforward using the OData API from CDS. Now, prior to coming into Visual Studio Code, we've set up an Azure AD app, and we're going to reference that inside of this application as we go. But let's talk about what we have modified in this app. So this app is the Blazor server templates. We've modified this. If this format looks familiar, that would be why, and a lots of great sessions in Blazor here this week. Now, what I do want to show you here is of some changes we've made. Because we're using Azure AD, we're going to go ahead and add authentication here. We're going to read in our config info, so the CDS Web API, and we're also going to add the HttpClient CDS here under "Services" and have that all teed up for us for OData. Great. So now, we've got everything we need in startup setup. Now, let's move over to data. So data, we left the initial functions in here which were tied for weather forecasting and we added one more, which is for a Web API service. So we're extending an application like we talked about at the beginning of the session. Now, when I come in here, you're going to see a couple of functions. One of these is going to be Get, which is going to execute a query and return back details I will be showing in our RazorPage. Before that, we're going to pass in our Azure AD object, and we're also going to pass in an OData query. We'll see more on that in just a minute. You're also going to find it here, a method called GetAccessToken. Here, we're going to go out and retrieve the token that we're going to use in our calls. Great. Now, we've got these supporting functions that are there. Let's look how we use it in this page. If you open up Shipments, you may recall, there used to be a Fetch Data page, so what we've done is we've added a Shipments page that is based on that. Relatively simple and straightforward in terms of what we're going to see, and again, we'll run this in just a moment. You can see here that we added a reference to our data class that we just viewed. We've got a reference to Newtonsoft.Json.Linq. We've also added in our web API service as a service. We're also adding Microsoft.Asp.NetCore.Components.Authorization, and we're also adding in the AuthenticationStateProvider. Now, we'll come back here in a moment, but first, let's look at the code that's executing when we kick off here. You can see here on initialized async what's happening. We're starting up this page. It's initializing. I'm going to go out and call this function getUserAObject. It's going to be at my Azure Active Directory object. Then I'm going to call that service that we saw a moment ago, user a object ID is being passed in. Then you see this set of strings here. These are what we call logical names for our fields and then for entities inside of CDS. I'm pulling back the shipments entity, and I'm selecting these values and bring back the shipper, the recipient, the initial shipping company, the minimum temperature, the maximum temperature, and the shipping status. We see here, it says, underscore value. You might say, "Hey, Mark, why is it only underscore value there?" Because in CDS, these are fields that are called lookups. Lookups are effectively what you might have in a relational database like SQL as another table where you've got a foreign key that's attached to that. Then the scoring value, you're going to get the actual value as opposed to the key for that information. Great. Here's our call. It's going to go ahead and pull back these Azure update object. Now, I go ahead and get this information back. When the page is rendered, what we're going to do is take those values, and we're building a table on the fly here, and we're going to go ahead and populate those values into the table. You can see here, I'm being asked to log in, I'm going to log with my ID for that environment. Signed in, great. Now, what you can see is this is the Blazor app, the default server app. You can see that I'm logged in here. Whether you'll see is where it normally says Fetch Data, it now says Shipments. Let's go ahead and click on that. Now, you can see it's pulled back all the information from my shipments' entity.  That was awesome. You were able to actually go ahead and use CDS data, and build Blazor application on top of it and use it seamlessly that I could have done with any other SQL database or relation database. For our third demo, I'm going to talk about integrating with Service Bus and CDS API through Azure Functions. Let's dive in.  As we just mentioned, CDS is the ability to publish to Service Bus, and that can enable integration with Azure Functions. Here, we're going to show you an interesting sample that uses that capability. First, we're just going to show you that there is a service endpoint registration where we're registering the Service Bus namespace address. The next thing you're going to do is configure the system so that when there is a create message called against the shipmentjourneysegment entity, something about like a table, when that gets called, it's going to go ahead and post a message out onto the Service Bus that we've configured. Now, what we have on that Service Bus is a function monitoring that. That function is called ServiceBusQueueTrigger1. When that message comes in, what will happen is we've got code here that will parse it. We're going to identify the last recorded temperature, the shipment journey segment ID. Now, shipment journey segment, if you think about anytime you've ordered a package and you've tracked to see where it was, it doesn't usually go from point A to point B. There's four or five steps that are in there. We're calling those journey segments. It's getting the one that's just been added, and it's finding out some details about it. It's then looking at it for the shipment it's attached to, what are the maximum, minimum temperatures? These are frozen goods. In this case, it's ice cream that we're shipping so we want to understand what's the maximum and minimum temperatures. Then we're going to compare the temperature that we just received as part of this most recent segment because we've got an IoT device monitoring that, and see if it's greater than the max temperature. If it is, then we've got a problem; we need to give somebody a call. When we see we need to give someone a call, we're going to use a service called Twilio. Twilio will allow us to go ahead and send out a voice response message to someone. In this case, you can see that we're going to say that your shipment of ice cream by shipper name, this is all dynamic, is likely melting and no longer in sellable condition. We'll hear more when we get the alert; it's there. You can see here, we've got some functions that are going to go ahead and get our token as well as go ahead and send out that request. Now, what's going to happen is that'll make a phone call; we'll see that in just a moment, and then we'll get a response from that phone call, and that response can come back to this other function which is basically a webhook for Twilio. It's going to go ahead and parse what came back in, did the user pressed 1, 2, or 3? If the goods are bad, now ice-cream can melt and refreeze and make you really sick. So you might say, "Hey, just return that product. Or you know what? Go ahead and destroy the product. It may cost too much money to send it back. Or you know what? Go ahead and deliver the product. It turns out it was a faulty sensor, and we're all good." Once we've done that, we're going to go ahead and update the shipment and we're going to make sure that we update it with the right response from that user, and then we'll see it reflected back here in CDS. Here, we're going to update this record here which is nme569, and you'll see right now, it says In Transit. In just a moment, we'll go ahead and update this and it will change to Destroy Product. Here I am. I've got my phone on screen. We expect a call. There it is, just like clockwork. [MUSIC]  The ice cream handled by Tailspin Nerd Transport is likely melting and no longer in a sellable condition. To have the shipper return the product, please press "1", to have the shipper destroy the product, please press "2".  Okay, so we've just hit "2". In just a moment, this is going to go ahead and say, "Destroy Product." What did you just see? You saw CDS integration where it published an event to a Service Bus. You had a function that was monitoring that event, processing it, evaluating it, and interacting with CDS to get some additional data, and realizing, "Hey, the temperature on that was outside of what was agreed to. I need to make a phone call." Use Twilio to give me a call. Ask me what I wanted to do. I pressed "2," so destroy the product. It sent the message back, got picked up, sent into CDS, and we had round-trip integration there. Also, integrating voice is a fun bonus.  There you go. That was an awesome demo right there. I just want to finish up this discussion by talking about how you can actually use CDS data and create some really nice visualization reports, and continuing with the tradition of natively providing all these capabilities as part of the managed CDS service, we provide charts and paginated reports out of the box. You can access them using SQL Server Reporting Services so you don't have to do anything else, but if that doesn't work for you, I did point out before that we just enabled a DirectQuery endpoint which is in a preview right now so you can use Power BI again, so you can create awesome visualization and charts if you wanted to. If that doesn't work for you, you always have the ability to use the things that I talked about in getting the data and exporting it out of CDS. You could do all sorts of preparation, transforming the data, doing ETLs and stuff on it, and then publishing it to a SQL Data Warehouse, and then have Power BI consume it as well. All of that is fair game. All of that is available. There's no cliffs at any point in time. Just to recap, CDS is your managed service that we talked about that brings you the goodness of the heterogeneous data store. Any data operations that you do with it, we just make it available to you out of the box. It's easy to use; it's easy to support. It's extremely secure because it's all built on top of the Azure infrastructure that you've come to expect good things out of. There's multiple options to get the data in and out from. It easily integrates with APIs, Events, and plugins, and you saw some demos of that. We have native capabilities of charting and paginated reports built-in, but If that doesn't work for you, go ahead and use our Power BI endpoint that we just opened up. That concludes my session. We have our Q&A on the blog site, so I've posted the link to that right here. For the next half an hour or so, I'll be there as well as I'll be joined with Jim Daly, who's one of my senior tenured colleagues on the CDS team to help with some of the questioning. If you are seeing this session after the fact, no worries, just drop me a tweet on my handle right there on the screen. I hope you found this interesting today. Thank you for taking the time. Again, I hope you've had a really powerful experience today. Goodbye. 