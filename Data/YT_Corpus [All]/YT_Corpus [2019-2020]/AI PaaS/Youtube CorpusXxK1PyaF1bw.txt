 all right good afternoon everyone we're gonna go ahead and get started thanks for taking some time to come in and and learn a little bit more about deep brew I'm John Francis SVP of analytics and insights at Starbucks and I promise I'll try to make this as painless as possible I know we stand between you and the happy hour session after this so we'll make sure we get through this and you know we also have a booth if you haven't been to it yet so if you miss anything here and you want to go even deeper with the team feel free to spring swing by the booth and check it out so I'm going to go a little giving a little bit of context for what we're doing with personalization and why it's important to Starbucks and and you know I'll quote the obligatory McKinsey slide here just to speak to why personalization is important and you know as you can see here from a couple stats you know 50 54 percent of businesses saying that they wanted to play some kind of machine learning capabilities over the next three years significant investment through a lot of enterprises you know across fortune 100 fortune 500 and in the space but you know the reality is you know I think there's there's some table stakes here just in terms of you know personalization anyone who's used an e-commerce platform like in Amazon this is not net new technology this these are not net new capabilities so you're probably thinking to yourself well it feels like Starbucks is really kind of late to the game here and and why are we getting into this now you know it's it's it's been a long time like I said in the space well the real is personalization actually before we ever loaded a piece of data has really been in our DNA and as a company and if you think about your own experience with with Starbucks and I'm assuming a lot of you have had experiences of Starbucks I didn't see some cups in here today it's it's really about the the one-to-one interaction between the customer and and the partner or the baristas in the store and you know I even think about my home so are up in Queen Anne and when I walk in the the barista knows my name knows my kids names what they like to order what I like to order so you you think about it and you know and again it's sort of this offline experience with with personalization and in how that really has helped us build an amazing brand and and so you think about the applications digitally and and it really what we're trying to do and and what you know what makes Starbucks special is the it's as much about the coffee as it is that elevated customer experience between like a so the the the employees at the store in our customers and and really you know we certainly are very passionate about the coffee but we we care just as much about that experience and we also know that our customers through a lot of research we've done internally it's just as much about the convenience and consistency and so what what we've observed over the last 50 years almost is that it's actually becoming even more so about the convenience and more so about the consistency that that you might that you'd expect and and how that's manifest itself is really through the growth and adoption of our mobile application of which we do a ton of transactions through every day and it sort of if you think about where I started and I talked about how we want to elevate the experience between the baristas in the store and our customers this feels a little foreign to that right when you start talking about digital applications well we actually think about this a little bit different in that it's really the the app itself and frankly any digital properties we have has actually provide some really good surface area for how we can take what we've done and what's made Starbucks special in terms of that relationship between our customers and partners and how do we bring that to life through the digital application and and in a lot of ways you know like I said you know recommendation systems they've been really commoditized or years and so what we're trying to do is something a little bit elevated which is we we really think long and hard about that experience that you have in the store and how do you apply it digitally so you feel just as good about the experience digitally as you do in an offline setting with with a with a brief step so how do we how do we try to make this special and personal for you as a customer well if you think about some of the data that we have access to and in how we can build the math around the recommendations you know it's everything from your your transactions and your purchases to the ingredients within the products that you're buying as well as the context in terms of what time of day and what story you're in geographically and and also as you think about whether and you can you can take all these things and and and help really inform like I said an elevated experience that in terms of the recommendations that we're making so you know how we bring it to life and you know I spend most of my time in email and powerpoints so I'm not going to go deep into this pseudo architecture diagram but but really deep brew at its core is a machine learning capability that we built internally at Starbucks that allows us it sits on top of a juror it allows us to quickly deploy machine learning algorithms across a number of different endpoints and whether it's our mobile order and pay experience within the app or it's a drive-through menu board or I could go on into some other areas that were we're exploring it's really thinking through you know what what deep brew really allows us to do is give us effectively a playground that allows us to quickly iterate and test a lot of different algorithms very quickly in a beat us them through those customer experiences so that we can understand really kind of in a champion Challenger way what's working best and allow us to deploy quickly with with not a lot of impediments so the team is a little bit more a deeper into the architecture but I just kind of wanted to set the stage here and and really as we think about you know again kind of doubling down on you know why this is important to Starbucks you know I think we we've had some incumbent runs at recommendations and just to give a specific example where you know we had an early version that we would end up making recommendations based on things like the wisdom of the crowd for instance and you might as a vegetarian you might actually see repeatedly a bacon Gouda breakfast sandwich and we the algorithm wasn't smart enough to really understand that over time that you actually if you're not engaging with those recommendations it's probably something that we need to be evolving and thinking about a little bit differently so fast forward to deep Roo we've now built the infrastructure that allows us to you know take that that spirit of comfort and connection between again our how we think about the relationship between our customers and umbria says and really take what they intuitively know when they interact with you and how do we think about that in a digital world so for instance in this example here you know we have a customer who is vegetarian they like lattes you can see the chocolate chip cookie dough cake pop and so what what we do is over time and we may not intuitively know this the same way a barista would know but we learned from a customer's behavior and in terms of the products that they're buying and how they respond to the offers that we're extending the recommendations and so you can see three examples here where we have an ingredient level observer that looks to see and it's constantly monitoring to say what what types of ingredients and preferences as a cap customer have and we can evolve our algorithm over time to account for that or if it's a category level observation and and how we can observe that a customer likes baked goods and they may not necessarily want to be egg bites for instance or frankly even price awareness so these are just three examples we might see a customer who really just enjoys deep brew excuse me brewed coffee and may not necessarily want an expensive Frappuccino or higher ticket item so really these are just three illustrative examples of what's gone into the the recommendation the the algorithms itself that allow us to make sure that we're constantly tuning and honing in on what's going to be most relevant for our customers so just just to quickly touch on this and then I'll hand off to the to the team here but so how we're thinking about the the platform that we built I mean the good news is we it took a lot of blood sweat and tears to get to where we are in terms of the capability that we built in close partnership with our technology friends organizationally and now that we have this infrastructure and this capability it really frankly allows us to open up a lot of avenues where we can deploy pretty seamlessly so you and again if you saw the keynote or you've come by the booth you'll see that deep prereq amend ations are currently running within our mobile order and pay experience and you know you feel free to download the app and try it out if you don't already have it but excuse me but I think that's where you'll see the most personalized recommendations because we we know a lot about you as a customer just in terms of your behavior and your transactions so we really allows us to optimize the most and then going to the right we're currently piloting some capabilities within the drive-thru experience so it would be less personalized we're not doing facial recognition or any license-plate scanning anything of that nature but really there are attributes that we can assign at a store level around you know what's relevant for the demography of that particular store and by date part and by leveraging weather as well and then of course voice ordering is another application where we have some capability that's currently live that allows you to do transcription and then leverage the recommendation platform so we're really just kind of scratching the surface I didn't mention digital menu board yet but that's that's an area that as we start to scale and deploy any boards into two more stores across our 13,000 plus stores that that will be certainly be a canvas for us to think about how we can elevate recommendations with that within that environment so that that was just a lot of context I'd like to hand it over to vish who is the director of engineering and his partner closely with us on the infrastructure in the backend thank you John technically I know we said between you guys and beers but we're still brewing something here so I think we're good so brew kit right so thanks to you know the concept that John outlined in order to support world-class applications we need world-class infrastructure you know as we looked at our analytics platform there was a need to essentially have that kind of reliability and scalability to support consumer facing global scale applications like application like D brew so we need world-class infrastructure and essentially the goals were to make modern analytics capabilities across Starbucks to enable the largest and smaller data science teams to do analytics at scale and with our restriction Interbrew kit so before I get into what brew kit is I want to get into some of the challenges we saw on the data engineering side before you know applications like D prove that you know that led to frameworks they brew a brew kit being deployed you know from traditional tech stacks all sudden we're dealing now with petabyte scale data you know as well as real time data products batch products plus a lot of unstructured data it was a non-optimal engineering experience as well because from a modernization standpoint scaling a lot of our training let's say they run on sparkle esters took a lot of time our older infrastructure you know you should take more than 30 to 40 minutes to run our spark environment our distributed programming environment and we also had to build our own frameworks to write merge update our own frameworks to on the data not to mention small files problems on distributed file systems as well as you know these compounded into SLA issues in production because their pipelines were fragmented and there was no adherence to any sls also now these problems now started after manifesting themselves up through a data science perspective there was a need for co-located their data sets the ease of access the Gulf of execution between actually executing your workloads and your jobs was massive between the infrastructure that could actually do it longley times and data fragmentation as expected as well as platform friction increasing complexity now as we're looking at scaling out applications you know the deep-blue comes along we now need to start looking at you know distributed computing distributed GPU processing etc also this meant that you know are the older infrastructure had users running models under their desk in a box now when it came to engineering we had to rewrite that to add to make it ensure that it's running at scale also more importantly it's cloud and security compliant and the infrastructure lead time in this case rewriting all these applications was months so essentially the goal we set out to do was how do we deploy these environments in a matter of not months not days but in a matter of minutes how do we enable analytics teams across the board essentially deploy these environments and make sure they're dealing with their applications and building world-class stuff like deep blue and not just battling infrastructure broke it so broke it essentially was a framework that we built as part of the offerings around Ebru to offer zero friction analytics essentially we built a unified analytics platform to say to handle batch real-time and all all of the analytics load workloads on a single technology stack we want to reduce any impedance between model deployment and operator operationalization and infrastructure and essentially deal with all the problems that I outlined before from a platform value proposition it meant that our business users could now focus on the applications and outcomes not you know figuring out how to land provision certain services in Azure and then essentially ending up with a bunch of non-compliant infrastructure so I'm going to take you real quick into how we are actually looking at blue ket I knew you know I would invoke Murphy's life I try to deploy the environment within the ten minutes that I have here right now but so I just deployed it right before this talk so essentially I'm going to use one Azure service in this case we pick data breaks as your data breaks to essentially show the way we are deploying and some of the features that we've deployed on this environment user access to workspaces are delegated so teams like John steam which actually used Ebru essentially use a bring your own Active Directory group model so essentially what they do is they do not have to deal with long lead times of infrastructure teams deploying you know adding users to groups etc they own their own user groups and essentially we build processes which we call as AC sync in-house which essentially syncs user groups to our Active Directory environments so that our user the users that need to have access to the environments are specifically the users that the business wants have access to the environments all users are set up right with the right permissions and right after the environment is provisioned we also use secure dbfs mounts on to access enterprise data what that means is we take away the complexity of users no annoying you know keys and passwords etc now - you know configure each their own environment think of a large enterprise like Starbucks if you have every business group and hundreds and hundreds of users trying to configure their own access patterns it becomes a mess pretty quickly enterprise modes are read-only what we needed to do was essentially I'm going to just try to write this to our enterprise mount here essentially what we needed to do was we need to make sure that enterprise data which is industrialized data sets that we are provisioning are not writable we did not want you know users essentially be able to write it should be read-only access to enterprise data products in this case for example I could right to it because essentially I had an ACL error because all our enterprise published mounts are essentially locked down however we also wanted to make sure that users have the right level of access across different tools we had terror scientists deploying models while Scala we are data scientist writing Python we had a huge swath of users who essentially they were only using SQL to read their data essentially we built our own medicine which is backed on a hive meta store to now start syncing all these published data across different user environments this also gave us the flexibility to essentially isolate environments based on usage as well as business use case personal storage now that told you that you couldn't write to publish layers and enterprise data assets users and applications like deep blue still needed the flexibility to write to push a personal storage as well as write to specific you know our team workspaces which is where we you know we provided application storages and then we also now we deployed auto scaling best in you know the most hiked well configured and optimize clusters in order to support different workloads for example if deep Roo needed specific GPU configurations in order to run training on a daily basis we kind of provided that functionality out of the box another one of my favorites started notebooks what essentially in order to democratize analytics across the organization with applications like deep blue we also started deploying you know starter notebooks so that we could truly in our democratize this across data scientists and analytics users regardless of skill level we also using and you know more about it at the booth but we are also now violating a lot of our machine learning work works use cases using Azure ml and Azure ml is helping us deploy a fully functional model lifecycle n2n and we also use DevOps temp list deploy and monitor AI pipelines and models views Azure data factory for orchestration and last but not the least this kind of the decentralized model helps us now start tracking the cost of deployments in a very fine-grained manner essentially applications like deep blue now have extreme high insights on what they're spending on the services that are costing be blue the amount of money that it's costing them as well as what they can do to optimize their application switching back over so essentially like I said the data science template that brew kit deploys for an application like deep blue uses a plethora of azure path services for our rest endpoints we use app services views a combination of cosmos DB as well as a relational database layer we also use schema keyboard for secured key management and you can read the rest of the different path services or highlight also that you know this is powered by excellence in engineering from a Starbucks automation perspective where we do essentially one-click deployment of our template across other services the deployment time of this template essentially is less than 15 minutes which means that we can deploy this to any analytics team or a business group that needs to have a fully fully functioning analytics environment to create world-class applications like deep blue in a matter of minutes we also provide audit frameworks as well as you know getting started as well as the medicine and AC sync which are tooling that we built on top of the stack so essentially now we have hundreds of users on-boarded on the stack and I'm now as once we provision deep brew kit we essentially were able to now deploy applications like D brew and I'm now going to have ed the Rose our data scientists come up on stage to talk about the nuts and bolts of D bro it's fish everybody Rosa I'm a data scientist at Starbucks and just by show of hands vixie any other data scientists or people practicing machine learning out there if you could just raise your hand okay maybe 10% that's great so if you are data scientist or working machine learning you know it can be a challenge to deploy your models to production to real-world customer-facing applications and besides just making sure your models right you need to make sure you have all the systems in place for security reliability scalability and things like privacy so all those have to be addressed so it's great having a partner in Starbucks technology and Microsoft to help us do that in Azure and create a platform where we can deploy our models so our first use case was recommender as John mentioned so we focusing on a couple of business objectives we wanted to provide that recommender in Starbucks through Starbucks rewards mobile order and pay application and so the two main objectives here were to maximize the probability of a customer purchasing an additional item two items already purchased in their cart and to or and/or increase or maximize the revenue or a total ticket for that sale so the idea from marketing purlins is a upsell or cross-sell sub products so we built a recommender in our Deepu platform we named a purple fish and so the main thing was a couple of main key points is we wanted to learn about customer product preferences fast and continuously so to do that we needed an algorithms ticketed app like a reinforcement learning for example and observe the individual customer purchases through their history as well as interaction with our recommender we needed to be proactive on a couple of edge cases like if the user hasn't yet added a product to their cart or if the user has no history that's issues known as the cold start problem so you don't have any history about the user or possibly you don't have a history about a new product and as you know Starbucks has seasonal products and they have a limited time offer products so we needed to address that as well and a key feature is being extensible so with recommendation systems the a lot of different choices right you have collaborative filtering filtering you have Bayesian bandits things like FB growth so we wanted to be able to have a flexible system where we could deploy many different types of algorithms around recommendations and test against them test them against each other and so I'll talk about a little bit about some of the intuition and science behind recommended so you know how do we learn about the world so this there's this idea that for people that we try to observe the world we have certain beliefs and those beliefs are altered based on our observation or evidence about the world and so on the left you have base form of Bayes theorem or Bayes law so this this was a key element to first recommender and you have an equation here which basically allows you to update probability distribution and that's your posterior so you take a prior about your belief which may be an inner form prior so you don't really know much and as you have multiply that by the likelihood which is our data generation process is a binomial so it's either a person might be vegetarian or they might not kind of thing heads or tails and the coins so you might remember that from statistics or a probability class if you took it and we divided it over a normalization value and we get a posterior distribution and in the bottom right you see the beta distribution and initially there's a very uninformed prior and as we get more information from the evidence about a customer purchase behavior we can increase our confidence which is shown by the narrowing of that distribution and at that point at the peak we have a belief that we have high confidence in it okay so a little bit more about how it works they break up the problem into two phases first it's a pretty big state space all right so you have a lot of different products combinations and pairings so we really wanted to narrow down the options and so we use the context to do that context is very important extremely important to personalized recommendations so we people will make purchases based on different context right so you might have context of where you live what day part it is what what you've already put in your basket the weather those things will influence your choices and decisions about what products you're going to purchase as well of course is your user history so we narrow down the options first by understanding the state we got into in a minute with a little bit more depth about the context observers but these are used to determine some hidden or implicit preferences so as John mentioned can we determine if you're vegetarian or can we determine if you have a preference for a particular type of product a category like blended which is are like a lot of frappuccinos so and then we can also optimize based on prices sensitivity so we do that and then there's this necessity to use reinforcement learning right so we need to constantly adapt in canoe to learn and we determine what what we have shown a user and how they have responded so this is kind of a conversion in a web page right so you need to get that feedback into the system and then you need to generate a list of product recommendations based on a lot of those things I mentioned like the state the product pairing for both users or if the user doesn't have any history the wisdom of the crowd if you don't have any history on the user and then on top of that you can take that recommendation so in a minute and then you can add in the context observers and look at how that works in a second lastly we have if you have multiple algorithms that are competing against each other we can use Thompson sampling it's a a Bayesian solution to the more bandit problem' which if you aren't familiar with it's more of a issue where you have multiple choices to make and you're trying to optimize your Maximizer award over time so and the the the diagram there kind of shows three different three different arms and it after given enough information it optimally finds the best arm to pull which is I think the one on the right and blue so here's an example of a context our context observers which really is what makes our algorithm special so we have over time we're gaining more evidence so the transactions are kind of shown in the upper left as we have more transactions we can learn about category preferences so like there's a blending category which consists of all our brainer products blending products like frappuccinos we have at the bottom you have a vegetarian context observer so as we as we learn more about a customer from their interaction with the system we're updating our belief about you know those particular contexts and so what you see here is you see somebody who's leaning toward vegetarian it's pretty high confidence and it's in the simulation well they don't really like the blended products or at least they're not showing a lot of activity in that area so so our potential Rex for the products are then upgrade a prank tore down ranked based on our categorical context observers so with that I will turn it over to Brian to talk about the journey a little bit more as well as the future deeper thanks ed hey everybody first I'd like to just start out and say thanks it's a real pleasure working with this crew so one of the things that's great about deeper is the people that you get to meet along the way and the things you get to do when you reach this stage so I'm gonna give you a little bit of a tour of the journey but it's gonna be really quick three phases phase one is building credibility with Azure and with your platform so in order to do anything with machine learning you have to unlearn how to talk about how great this thing is going to be and to focus strictly on security so we had many many meetings where we had to talk to people and say basically your middle name is security from here on out so thanks to everybody who was involved with the security and making sure that when you're in the cloud your data is secure and you're actually doing everything you can to make sure that you're being a responsible custodian of people's data in Phase two you have to make sure that your process is great so data scientists can just do the work that they do and one of the great pleasures of my job is getting to see how the culture has changed at Starbucks and getting to see the data scientists empower to handle real world problems but here we are in phase 3 where it built and we're talking about digital menu boards and drive-throughs and mobile apps and this allows us to have some of the best conversations and that are the what-if conversations and why couldn't we so whenever I think about machine learning I think about the great brand that Starbucks has the great data that we have and the fact that we've got these great partners that are giving unbelievable service and to a certain extent I wonder why we can't replicate that in the mobile app so every one of us here in this room has a little inner dialogue about coffee in their brains that's one of the things that we have in common and you know Starbucks is very proud that that inner dialogue is often where can I get my Starbucks or how quickly can I get my favorite drink and so machine learning allows us to start to think about how we reduce friction and do a better job of as soon as that inner dialogue ticks off we can have you think boy I'm gonna get this thing just the way that I want it and I'm gonna get it where I want it so shameless plug here is that in Seattle uber and Starbucks have linked up and you guys are able to have Starbucks delivered to wherever you are and it's a great step but we want to go further and so deep brew allows us to start to think about some some cool things like quick order so the mobile app is fantastic it's serving millions and millions of people every day and the model that we have serves up recommendations multiple millions of recommendation each day and you know a fraction of a second and everybody's really happy about it but there's a question like why wouldn't we make it super easy why wouldn't we know exactly what you want and when you want it couldn't we actually predict when you're gonna want your next latte or whatever your favorite drink is so purple fish is a variant of that is available for us to start to experiment with this so the mobile app and everything the machine learning does can be tested with data thoroughly and and quickly like we're into a rapid testing cycle where amazing things can happen and we can let the customers vote on what they want so it's a it's an exciting time for that customer experience in the mobile app but we can do other things too like I don't know how many of you have driven past the Starbucks and you see that the drive-thru line is filled and you just pass it off you know let's let's go to the next place or I'll wait well when you have a machine learning model when you have the infrastructure in place and you can talk to any touch point we can start to take a more aggressive stance on how to market during peak hours so when we have a lot of traffic and the store is clogged up we can now have all of the marketing from the mobile app to the menu board so the drive-through recommend products that are quick to prepare we can help you get your coffee a little bit faster that throughput makes it's a win for everybody everybody should have what they want of course but we can sure pitch things that are most convenient to get that that line moving quickly and you know another thing is that Starbucks has some great values and we want to make sure that we're being good custodians at the of the planet here and so there is a an aspiration to have zero waste in the food that we buy and what's left over at the end of the day so everything that vish and his team are working on is allowing us to have visibility into exact inventory position for all of our stores and if you know your inventory positions and you know what the sales are in that moment you can create a prediction of what's going to be left over at the end of the day and potentially waste so combine that with the ability to talk to any touch point and you can start to promote products to lead an inventory zero position we want people to be happy they should get whatever they want but again the machine can be like the best manager that's out there that says hey we've got extra of X product let's all push this a little bit so in our heads we see a very bright future for tackling important customer facing and business facing problems through machine learning and you know vish talks about being able to deploy the right infrastructure as you're certainly done a good job for us in that front I won't sugarcoat it but there have been many late nights and there have been problems that we needed to fight through but this is the build conference you know that can be fun stuff like there's not a moment of that that I would want to turn back so as your it also has a beautiful sort of angle to it that it's it's pretty deterministic once you head down a path and once you learn all the capabilities and when you see all the things that are being unlocked at a conference like this you know that you're gonna be able to get your job done it's just a matter of time and sometimes you need to work through it but that's fun but the most I think the most important thing is we are in it like this new world where it's a battle for trust Starbucks is a trusted brand and it's a battle for convenience and companies that are able to deploy machine learning models can scale reach customers and solve problems that allow us to compete in in this world of convenience and trust and with that I will turn it over to some questions for anybody who hasn't [Applause] questions so then the native app was the first place that we started and we were actually just served in the midst of the the mobile the web app that that's gonna be the next application for us to think through Abed we started the journey that wasn't actually frankly even a thing at that point so yes yeah so from an ROI perspective and why maybe if I could extend your question a little bit so instead of just maybe using a vendor or something off the shelf versus why we build the capability ourselves so I think it consciously and you know you heard Brian talk about trust I think one thing for us well there were a couple things so one we we've really invested a lot of energy over the last two years or so to actually build up the data science practice internally at Starbucks and and over over the last few years we've taken different four raisin to you partnering with different vendors but I think we all recognize that the world is changing just in terms of data privacy and security and you know and again I'll kind of reflect back and what I said about the most important thing to us is that relationship with a customer and whether that's offline or digital so for us as we think about how do we ensure our customer our customers that they can entrust us with their data and how we treat that data and as you think about it so there's there's two ways then to think about one would be you know the best way to ensure that trust is if you keep it yourself and you build those capabilities internally I think the the second thing I would say on that is you know the worst thing that we could do probably one of the most significant assets that we have as an organization is our customer information and the knowledge we have about it and it's not just what these guys have learned through the models that we built but it's also the power powering that with just what the marketing team and other teams organizationally intuitively know about our customer behavior so as soon as you hand off in the you know I talked a little bit about sort of the come out is commoditization of racks like the reality is you could go give you your data to a third party but the reality is it will never be as good as what you could build internally so we view this as a competitive advantage for us and it's also again really just to kind of double down on how seriously we take our customers privacy and insecurity around their data so you get a question yeah it's it's a good question I think and you guys feel free to jump in I think we've done a good job of building the infrastructure I think the way I would view that then as you know the reality is the the platform is there the algorithms are there you can run a different country's data thread now I think there probably some nuances and as I just discussed around what you understand about from a marketing perspective and what you intuitively know and customers in Japan are different from customers in China are different from customers in the US so that's where the earth science comes in a little bit but I think the the good news is that we built the platform and you know I think as Ryan had talked about the sort of the blood sweat and tears of that actually was a lot of heavy lifting I was just getting the infrastructure in the right place now we could think about how you might easily extend or expand to other markets of course there are some conditions to that there certain markets of course where the the data needs to stay in a particular geo like like in China but I think we're pretty well positioned right now I think it's really just about building those partnerships with some of the other countries to how you might take the platform and build recommendations that are relevant for their customers in those markets I think I'll also add that you know that the brew kit functionality and now we can like I said we can deploy in minutes right essentially we can now isolate services to regions and locations and deploy based on those locations where we have the flexibility of either shared-nothing or share everything so I think that helps us you know that's been a huge step for us to be like you know unfazed by deploying globally anywhere because as long as the service is available we can deploy ya know I'll take a pass only to shut up if you want or say something else but I think the way I think about it so you know I think the the other thing so the thing we didn't talk about right was how we're really starting to think through building a customer 100 360 degree view and how we look at multiple data sources so work that vicious team on the technology side is doing is not just enabling the infrastructure but also frankly helping us to seamlessly build data integrations and and whether it's you know the a lot of the data we talked about here or as you start to look at other data signal so how do we start to look at unstructured data through Yelp or Google or Glassdoor where or you're parsing social data and how you can start to I think to your point where you can start to cooperate across different data sets to see where there might be a pain point or a point of customer struggle or app reviews so you can really start to think through like any and just think about your own journey as a customer with Starbucks or another brand all the exhausts that you create from a data perspective I think what we're trying to do is really collect all that and harvest it into Azure within the data leg so then that we can start to look across a lot of those data sources and really kind of create more that holistic view and start to expose some of those problems but it's a good point and I think you know that's definitely a journey we're on and I'd say we're probably about 50% of the way there in terms of the data that we want to integrate and you know we're we're gonna keep going down that path to start to paint paint that picture into a customer behavior yeah and so for a quote start if you don't have any user history personally straight then we can use the context around your location on day part to provide wisdom of the crowd type recommendations on the other hand if you don't have any product new product that comes out like LTO or a seasonal product the algorithm is actually doing sampling across multiple choice context observers so you might actually get a random you know pick something that's not optimal for you so it does have some amount of exploration it's happening based on the higher the doubt the more that it's going to try to explore so that's that's the Bayesian type of explore it explores white for multi embedded the algorithm do it right so there's a probability distributions there's multiple private distributions say when context observer is on front blended right and so and and so if if that if we sample that we can potentially get something that you haven't tried before so you want to we would definitely want to have the sense of novelty because that's important to a lot of our customers to try to try new things and so that that's one way the other way we have marketing right so our marketing folks can also work with our within our system we can up a prank brand new products for example cloud macchiato if we needed to yep yeah I mean our sorry I didn't know who asked the question okay yeah so I think you know from a model perspective yes we needed to in this case as we were training obviously we needed to have a considerable amount of data so we had to go back pull in a lot of our history data but you know we also are you know setting retention policies on that data so that we're not essentially you know it's not a full-blown look through for all you know personally I believe we should start doing more and more with less data essentially make the models and algorithms you know given them as much as possible and I know ed and team are doing a great job of you know in that direction but to answer your question yes we did look at all history data to essentially establish the baseline context and then derive the algorithm from there thank you howdy unalloyed right yes yeah so we employees exponential smoothing that allows us to capture the previous year and have those items kind of seasonality also there's exponential decay and it's happening so there's a recent sense of recency in the algorithm that's right we have options to do both discounted and not discounted so we can let the algorithm kind of decide based on the users you know history so not currently we had an existing vendor who gave us rec so we first wanted to make sure we were being in the vendor and so once we establish that baseline and we're running running a hundred percent of traffic now so that's our opportunity now to improve algorithms test new things and have it learned you know through Thompson sampling a good question I it wasn't me it was it was the results of late night coding and maybe some beer was involved yeah we're not marketers so we'd I'm not good at naming things it was the same person who named deep brew so he got one for two its 50% not currently we have to use the Starbucks rewards data so number number data but we'd like to move figure out ways to move in the direction source of their data yes yes so essentially our data has the transaction data identified based on whatever you know the channel was you know historically yes if you you know multiple card and that's why you know I mentioned the merge inserts etc so in a lot of cases as you know users are juggling through multiple channels we need to make sure that you know there is a single point where we can you know pinpoint that user activity so historical and new data across different channels so we monitor a lot of everything's happening so we have KPIs business KPI as we look at our dashboards and make sure that our conversion rates are good and our average ticket or our good and that's also how we were able to view our ad testing so so that's basically a lot of health checks as well make sure the system's up and giving Rex do we ever ask I think what we'd like to do is take give the customers the opportunity to put their preferences in there and some of that is available but we need to lean into that so we can actually determine it whether you're really over tutoring or not now then we just you override our algorithm right because it we only can we only can base our beliefs based on the data and if there's insufficient data we don't know and so there's there's some ideation around and you know there's plenty of examples here whether it's stitch fix or Netflix and how you might game fi a digital experience to start to think through around preferences and for you to play frankly more of an active role in the products and recommendations you're getting so that that's something we don't currently have but it's something we're discussing internally and building our capabilities around that okay dad I mentioned we're hiring and they're just just to highlight a couple of things so youyou can get a free bag of coffee beans every week I've tried hold your applause summer fridays so half days on fridays all through the summer and frankly we are literally just scratching the surface I know I said it once already but we are well-positioned right now to do some incredible innovation all the way up frankly to Kevin Johnson who is a huge advocate for what our team is doing and if frankly you want to be on that journey come and talk to me find me on LinkedIn I'm not gonna give you my cell phone number here but you can you can figure out how to get ahold of me and then you can also go to our careers website we have a few rolls poster right now but we're excited to be building the team we're excited to share with you guys today and like I said earlier feel free to swing by the booth and you can go even deeper and experience the drive-thru as well as the mo P experience for yourself and thank you for your time [Applause] 