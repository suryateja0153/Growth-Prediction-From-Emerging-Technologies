 [Music] alright hello everyone and welcome to your afternoon session of day 2 here at Google cloud next my name is drew Houghton I'm a customer engineer on the machine learning team here at Google and I work with customers that are using our cloud AI platform this is tackle challenging data science and machine learning problems in this session I'm allowed to say that we have three customers we're gonna share their stories using cloud a platform to solve some of these tough challenges so we have a series of three lightning talks from cruise automation from the American Cancer Society and Spotify we're going to talk about the different tools are using on Google cloud the different philosophies that they're implementing in a machine learning use case standpoint and from a platform standpoint so we do have a QA in the app we call this a dory we in an interest of time we aren't gonna be able to do a live Q&A but if you want to submit questions through the the dory will respond online and then as always the teams will be here afterwards to answer some of your questions if you want to stick around after the session so with that I want to let's give a warm welcome to Brian Calvert from cruse automation [Applause] hey everybody I'm Brian Calvert I'm a data scientist at cruise automation and I'm the tech lead for the machine learning platform team today I'm gonna talk to you about how crews is building our ml platform on top of GCP tools services and software and the kind of theme of this all is don't reinvent the wheel we basically we want our engineers to be focusing on crews specific problems so that they can deliver the most value speaking of our engineers if you haven't heard of crews where a self-driving vehicle startup here in San Francisco primarily backed by GM but we also have about five billion dollars of funding from Honda and Softbank we basically were very focused on our mission which is to build the world's most advanced self-driving vehicles to safely connect people with the places things and experiences they care about and through this transform the future of transportation six core behaviors that we live and breathe as we execute on our mission are stay safe stay focused work together own it seek truth and be humble being in the autonomous vehicle space as you might imagine ml is very integrated into our development and tech stack we have an extremely diverse set of ml applications that crews that run the gamut from simple to complex research to production these include things like object detection trajectory prediction money on our cars fleet management in the cloud safety modeling active learning and that spectrum is a challenge in its own right to support but there's also two primary unique challenges to crews when it comes to ml development one of those is the scale and complexity of data we have a fleet of cars driving around in San Francisco it's a crazy complex driving environment these cars are equipped with arrays of high resolution sensors collecting lots like multi petabyte scale of semantically rich high dimensional data it's basically like a you know data scientist dream from a sandbox perspective to play around with and speaking of driving in San Francisco this leads us so to our second challenge we have real-world cars driving in the world interacting with real people safety as I said stay safe safety is a core principle of the company so we have things like safety drivers extensive integration testing before we launch code to the car but this also plays out in our ML workflows traceability reproducibly these are core requirements we we do not sacrifice on them but we have these complex workflows so we have to have very solid tooling to basically facilitate that that traceability so this is where the ml platform comes into play how crews basically delivers value to our ml engineers through our ml platform we have three core product areas data preparation training experimentation and model deployment they basically match one-for-one with core parts of an ml workflow it's a kind of combination of interfaces so likes a style along with frameworks for defining abstract computations and then of course the execution layers to execute said computations so a pass and there's these three principles that are part of it a unified streamlined experience for our end users and and tracking of workflows as I mentioned before and then this this other part which is an emphasis on discoverability and ml democratization we basically believe at our core that a diversity of opinions technical backgrounds those are gonna be the key to success for us as a company but as I mentioned earlier there's this spectrum of ml workflows we have to support so this is kind of where MLP challenge comes into play Cruise's MLP is how can we make these principles on the Left work across this full spectrum on the right while of course still being robust scalable so on and so forth and this is what the rest of my talks can and focus on as you'll see the crux of our solution is basically to build unified tooling and abstractions in both a horizontal sense and a vertical sense this is maximizing the breadth and depth of what our platform can serve and a critical component of this the kind of what I got to at the start of the talk is leveraging existing solutions whenever possible don't reinvent the wheel we want our MLP engineers to focus on crews specific problems that they're delivering the most value the first of these problems is data base any mo workflow is gonna involve lots of data flowing through the system and this data is fundamentally heterogeneous in nature you have things like model binaries which are just binary artifacts you have structured feature data you have the containerized ml applications themselves each of them with their own different storage back-end are often with their own different storage back-end the MLP we need to maintain touch points on all this data flowing through the system as well part of it is that traceability and reproducible research reproducibility pirates I and then the other part is even end-user features like orchestration we have to know where all the data is because one workflow might deposit a payload of data into the workflow tracking and another one needs at a later stage so how we actually solve this as the diagram implies in text is federated storage we basically have a unified DSL so users don't have to worry about this they can hit this DSL register artifacts that are important the dispatcher will manage the backend specific api's this naturally provides a single point of truth for tracking workflows and end so we've got our data under wraps you've got it under control and this brings us to then actually streamlining the different parts of the ML workflow itself the first is this being data processing preparing the data for you know ingestion by models downstream or eventual of course deployment of the models themselves similar to the scale and complexity and spectrum of workflows of ML workflows period the models our data processing itself has this whole spectrum it covers you have simple things sampling selecting repackaging data maybe to say TF records for ingestion by tensorflow but we also have very complex data processing let me re simulate the entire AV software stack AV autonomous vehicle like let me change my localization algorithm and see how that affects my downstream tracking use that as a feature extractor each of these jobs they have their own unique dependencies or many of them do and so you kind of immediately have this challenge of like how do I solve this and one idea is okay let's target the upper bound let's build a single super complex back-end that's going to manage all this if we can hit the upper bed can hit the lower bound it's not a pragmatic solution one your sre is there they're not gonna enjoy dealing with that too it's of course as part of that gonna challenge your SLA is but also from this kind of applicability standpoint it's basically taking a sledgehammer to a finishing nail for many of your workflows of course at the other end of the spectrum where you just you know let bedlam chaos scattered backends whatever that's also not gonna work that's explicit fragmentation it adds cognitive load to your users so we need some form of unified solution that kind of toes that line and balances it and this is where Tara comes into play instead of unifying everything we unify the pipeline definition part so Tara's a unified DSL or SDK for data processing jobs we use Apache beam for the pipeline definition this is a very like native like MapReduce native style DSL this gives us native integration with workflow tracking by basically building on top of this it allows us to support multiple different compute backends through basically a run or dispatcher we can have a data scientist and m/l engineer define a pipeline the dispatcher can go oh this is a simple workflow I can deploy it to data flow or this is a very complex that simulation workflow and deploy it to our custom simulation backend of called crews Hydra and so what this lets us do is is leverage scalability out of the box for example data flow for these simple jobs it's it's super super easy to use as an example and even for our custom stuff we build it on top of GCP to manage the compute part of it one other component I want to hit on is this democratization bit by unifying on a on a DSL here we've basically gotten ourselves kind of free sharing of operators feature extractions etc so we've got our data prepared and now we want to train models and we kind of hit the same same problems we have a myriad of different frameworks complexity of their requirements how can we unify all this especially given that mo models are often that most blackbox part of the workflow where there's this ubiquitous need for clear prescriptive traceability the diagram looks very similar to the last one because it's a very similar way you can solve this is unify your definition unify your definition of pre-processing transform your primitives of what is a model and then you can dispatch to the appropriate backin if it's a scikit-learn we can train on a single machine if it's some crazy multi-layer deep neural net doing temporal convolutions use hoorah deployed on GK custom MPI cluster so we've prepared our data we've trained our models and now we want to deploy our models similar story as the other problem areas varied requirements complex dependencies so of course given the similar problem you have similar solutions which is namely leveraging the kind of abstractions or unifications at the abstraction level the DSL level work with dag based frameworks this is giving you strong semantic structures so each node telling you what it needs giving you have all that the dag itself can tell you what it needs so you can encapsulate it as this kind of black box operator of course once you have a computation with explicit semantics expectations that is a container risible computation so we can put this black box transformer into a container G RPC endpoints throw protobufs in and out once I have a container with G RPC I have a deployable microservice or a deployable application into any application layer so we basically promoted ml to now transform it into a micro service that lets us do it for whatever purposes whatever business logic we need and of course leveraging GCP for example for gke to manage these kubernetes pods so to kind of summarize it all back there's this this huge spectrum of ml at crews researcher production simple to complex and for us to serve that from the MLP side we basically focus on core ml primitives for building these workflows things like unified frameworks and principles dag based low-level primitives like what is a model we also provide relevant abstractions for multiple levels of complexity we want to abstract away compute storage layers task-specific dsls this is of course facilitating than building more complex things these are your Lego blocks as part of that we don't want to necessarily focus on how are we dealing with storage or compute or the deployment of applications and so that's where we leverage GCPs tool suite to provide a stable scalable solutions and this this kind of brings me to my closing points the same one I opened up with is as you're working on these applied AI in your own companies don't reinvent the wheel understand where existing tools and services and open source software slot in and have your engineers proactively build on top of that this is exactly where they can deliver the most value it's going to lower your technical risk it's gonna accelerate time to market kind of one one closing point about that is even with all this storage compute all that you should obviously leverage that but of course if you don't need super complex DSL so you could use things like tf-x tends to flow cue flow pipelines so the whole point here is that you want your company to have your AI in production as quickly as possible traceability reproducibly all these principles not being sacrificed with that thank you [Applause] awesome thank you Brian that's a great example of giving data scientists and MO researchers from the platform side the best of scalability but ease of use so with that our next speakers are Mia Gaudet from the American Cancer Cancer Society and Jake Evans from slalom if we give them a warm welcome [Applause] hello so Jake and I want to talk about a project that initiated because they at slalom they had lost a colleague to cancer and wanted to do something to pay back the community and in his honor and so they had approached the American Cancer Society about doing some work and through some conversations had pieced together a project that would really utilize the long term investment that an American Cancer Society has made in gathering data resources for cancer research and utilizing the machine learning expertise at slalom so we wanted to focus on breast cancer as an important public health problem breast cancer is the leading cause of cancer and the second leading cause of cancer death and women in the u.s. so there within breast cancer even though it's all in one side breast cancer is actually several different diseases and we can tackle this understanding of the different ways to define these different subtypes of breast cancer through multiple different ways and so in this project we're focusing on the actual tissue specimens and what the tissue specimen looks like under a microscope so traditionally and in the clinic a pathologist would make the final diagnosis of a breast cancer and they do this by looking at the different cells what they look like under the microscope and being able to distinguish between cancer cells and non cancer cells and they also provide pathologists regularly provide in the clinic some other characteristics about the cells but to go beyond that and to understand avila Eric turistic there's a lot of difficulties that a pathologists face so some of that is the subjectivity of describing these cells what they look like in relationship to each other as well as it can be very time-consuming to look at different patterns to score these different patterns across many different types of many different patients as well as there are apologists are very much needed in the clinic and there are not enough pathologist for the research so this is really a problem that can be solved through machine learning because of the consistency that we get through algorithms as well as we get more precision as well as quantification compared to a pathologists making a descriptive qualitative score as well as then we can score this to many many different people across many patients and across many people in a large research study so in one of these research studies is the American Cancer Society's cancer prevention study - and this is a cohort study which means we collect information about people we recruit them collect information about them when they are cancer-free and then follow them over time and during that time we collect blood specimens we collect questionnaire information that includes questions about lifestyle and diet and medical factors and so you can see that we're quickly accumulating a lot of data on these individuals and then link them to national registries about who gets cancer and who doesn't as well as who dies of that cancer and who doesn't and so in this project we used about 1,700 breast tissue specimens from women who had been diagnosed with breast cancer in the cancer prevention study - so what do you think tactically about this problem how do we take what are originally just small glass slides with these specimens on them and extract novel patterns that exist within them and then organize that information in a way that can actually be folded back into more traditional studies linked to clinical outcomes that journey is a pretty long and complicated one and we're going to be talking about those tactical steps here for the remainder of the presentation originally these slides are just that glass slides with what looked like very small pink blobs on them but when you put them under a microscope you see a wealth of information down to even the cellular level you can see things like red blood cells and individual cell nuclei that's a ton of information but unfortunately we can't use our preferred toolkit to actually do any analysis on it right what we want to do is we want to move that information to the cloud so the first step to do that is actually to digitize these these slides a lab at UNC did that for us using modern microscopes and they digitized them in a way that is a proprietary format but allows us to actually zoom in and out a different objective magnification levels so that we can capture both macro and micro level patterns from there we actually move those proprietary image files to the cloud and our journey really begins using GCP we do the standard pre-processing that many analytical projects take we clean the data up where it's dirty we normalize things where necessary and then we actually use some really cool deep learning techniques to identify and extract the patterns that exist within these images the advantage of doing this is that it's a completely unsupervised technique so we take the human outside of the equation which removes the inherent biases that always come when a human is is trying to do really any sort of problem from there we encode that information and organize it in a way that can actually be linked back to some of the more traditional studies as I mentioned that can be linked to actual patient outcomes in terms of the tool set that we use just a quick overview we leverage Google Cloud Storage a lot in this project not only to hold those raw files but also to hold all the interim information as we are processing those raw files and then the actual output from the machine learning steps the processing piece is actually done completely by scratch and compute engine it's completely scalable which allows us to convert these proprietary image files into normalized ready to be analyzed files on the machine learning step which of course we use ml engine for in order to train the model at scale the model vetting process was done in data lab using primarily Carris and tensor flow so when these slides are made they are the tumors removed from a woman when she's diagnosed with breast cancer the tumors removed it is put into formalin which stops any cellular activity then put into wax to maintain the 3d structure of the tumor and then slices are taken of the tumor and very thin and then put on the glass slide and then the pathologist stains them with a stain that allows the the different components of the tissue the different cells that that Jake described to be visible under the microscope and but this staining process has changed over the last couple of decades and these tissue specimens have been collected since 1993 through 2015 as well as across hundreds of hospitals in the u.s. so you can imagine that there's a lot of variability they are introduced from that as well as some of those slides in the stained sitting around also fade over time as well as the pathologists as you can see from these images here the pathologist circle areas that might be of interest and make other notations at their own will not knowing of course that they will be valuable to us at this point now as the digitized image so that was really a getting rid of these artifacts was an important first step to overcome definitely and again from a tactical point of view the way that we can overcome each of these hurdles that Mia mentioned is to use our preferred toolkit for doing some of these pre-processing steps for us it was using Python and all of those rich libraries that we can do image processing with but the first step is to convert these images from that proprietary format which is called leica Sen into something more accessible that we can actually read into Python that's a tiff format because it's lossless so in order to do that we investigated some open source libraries and there are something that exists out there but unfortunately because the file type that was specific to our problem was so new there was nothing out there that existed that successfully convert this file type from like a Sen again into a tiff format so one of the software engineers on our team dug into the actual source code of one of those open source libraries altered it a bit and successfully converted each one of these 1700 images from Leica Sen to a tiff from there we actually go about removing all of this image noise them you just described the first step being to normalize the colors that exist within the images from slide to slide a purple nuclei should be a purple nuclei should be a purple nuclei but unfortunately due to the reason to me and mentioned that's not always the case so we did some pixel level clustering in order to coerce the range of colors that exist within and across each of these slides to the same range for that color consistency and in a way that loses no information from there we go on to computationally identify each of these artifacts them you described use these pin marks that you're seeing here are Sharpie marks there's also some ink that sometimes left over after making the incision for the tissue sample so we wanted to identify which regions actually had too much noise that we wouldn't be able to do any machine learning with from there we've performed what's called image tiling this is breaking this very large image which I fail to mention these images are absolutely large absolutely massive rather around 10 gigabytes on average in their raw state what you can imagine is you know thousands by thousands of pixels so from a machine learning standpoint we want to explode the number of samples that we have while also reducing the noise which is what image tiling does for us it reduces each of these large images into smaller sub images and then from there we can actually filter out those tiles with little to no useful information so we filter out tiles with primarily white space and also those which are primarily composed of artifacts what we're left with is this collection of really information rich tiles that we can feed into a really complicated but very intelligent deep learning model to identify patterns and encode them in a intelligent way that model specifically is a convolutional auto-encoder so the way that that works is we feed it each image tile and it distills it down into a simpler form called a feature vector that feature vector is just a sequence of numbers around 500 or so floating point numbers and then using that feature vector the auto encoder tries to recreate the original image this process of distilling down in recreating across the entire collection of tiles forces the auto encoder to learn the patterns that exist within each of those images in a way that is you know really robust and really salient these feature vectors are then used in the next step which is clustering as a human if you're just gonna look at 500 floating-point numbers it's probably a little bit more difficult than actually looking at the raw image itself to identify patterns but to a clustering algorithm you know this is actually its preferred format so we cluster each of these feature vectors and we actually perform this clustering process two times the first time we find ten clusters and we call these prime sorry we call these primary clusters this is dominated by white space in terms of the differentiating factor between each cluster and then within each one of those clusters we perform a second round of clustering in order to capture some of that variance that was lost from that initial round and that also allows us to look at some of the nuances in the feature vectors which again is analogous to the actual patterns that exist within each one of these images now this machine learning portion of the process especially the auto encoder portion is really compute intensive the auto encoder has around 50 million free parameters that it learns over each training process which is you know it's that's a ton right through some hacky ways we got this to run on a local computer but what took you know multiple days on a local computer took around a few hours using ml engine so Google Cloud really helped us out on this front from here we have these these secondary clusters and that's really what you're what you're looking at here so each row is going to be one unique secondary cluster and the actual images are the most representative images for that cluster now I'm a layman on no pathologist or at the geologists but I can see you know some very interesting differences between each one of these but I think the more interesting question to ask is how does this relate back to things that are actually important like patient outcomes yes so the great thing about these different clusters is that we were both able to identify clusters that seem to be picking up on things that we knew that it might pick up on so one of those things is referred to as tumor grade and we would expect it to be picked up on an image based analysis because as the tumor becomes more and more aggressive the DNA and they in the cells center gets more and more disordered and clumped up and then becomes darker and darker so of course we were we were reassured that we would expect to see that as a cluster this something that is related to tumor grade because of this visualization aspect of it but as well as we saw clusters that were we have no words to describe because it was something that the machine could pick up that a pathologist couldn't so it we got both clusters that had a known entity as well as novel features so we're utilizing these clusters now in combination with this long term data that we've been collecting to both look at for these women we're looking at are their risk factors that are related to having a predominance of one cluster over another to live it provide us some insight into the etiology or what caused the breast cancers as well as then are these clusters related to long term survival and we are working on that on those analyses now as well as now we also want to see this was done in the cancer prevention study too but we want to see can we replicate it and our more recent cohort study referred to as cancer prevention study 3 and also we've been talking about breast cancer but this algorithm could also be very relevant to other cancer sites like colon cancer or ovarian cancer and as well as we want to share this information with other other groups too because certainly there's many applications to machine learning and AI in the medical space you really particularly in imaging to better to go beyond what the clinicians can do so I just wanted to take this opportunity to thank slalom and Google for their generosity and making this project possible for us thank you thanks thank you so much that's a fantastic example of how applied machine learning is becoming more and more a transformative part of healthcare and potentially saving many many lives so for our next our last session I'd like to welcome to the stage mark Romain and Joe cutter ooh CEO from Spotify okay hi everyone my name is Joe I'm here with my colleague mark and we are both machine learning engineers at Spotify and today the plan is for us to talk to you a little bit about recommender systems but before I go a little deeper into some of the machine learning that underlies these recommender systems I want to talk to you a little bit about music music albums tracks artists playlists these things four are the entities that form the item space for our recommender systems our fundamental task is to give you music you love to give you things that will resonate music joke before I can do that I need to ask a much more simple question as a user what music is pretty good for you I need to turn this entire space of items into a smaller but still sizeable set of relevant items we call this process candidate generation once I have an idea of what the relevant items are I need to fine-tune things a little bit more the product proposition dictates how I'll turn these candidates into awesome recommendations for example maybe I want to turn them into a playlist with a very specific objective like discover new music you love or maybe I'm interested in clustering these items together into mixes maybe I want to pick out items that match something you're already listening to there are infinite possibilities with one common thread being able to turn preference for music into math how do I do that if I was to ask you if you loved a certain artist you could probably tell me yes no maybe not sure I need to somehow predict that response so I need some mathematical operation between users and items to equate the preference or predicted preference okay so one straightforward way is to cast the preference problem as a matrix reconstruction problem mm-hmm okay there we go specifically if I have users and items and using historical data I extract some interaction measure for example did the user play the track this represents an observed proxy for preference and this is what I'm going to try and predict I then attempt to find some representation of users and items here denoted you I and I J such that if the user interacted with the item an operation between these two vectors will lead to a predicted value of that interaction measure hopefully it's close to the observed value in the picture a bug for above for example this operation would be a dot product between the two vectors in a diagram the vectors are two-dimensional this two-dimensional vector space is therefore my preference space more generally if I have a user and some features I can use to describe them and an item also with some features I can try to use these features to learn these latent factors such that an operation between the latent factors approximate the measure of interest and so which you end up with at the end of the learning procedure is a vector space that users with the items that they'll love sounds great but there are many many methods you can use to accomplish this task each with their own advantages and disadvantages for example maybe I have a model that does exceptionally well but it takes a long time to train and requires a lot of training data regardless I for sure have multiple models that I need to analyze and compare and this is the problem I've many experiments I need to run and keep track of ok moreover with a large item spaced and relatively low interaction density I'm gonna need a good bit of training data to actually produce a meaningful model and I'm gonna need to track experiments on a large amount of data now offline experiments are nice but online experiments are a lot better right you never know how the recommender system will perform until you try to test it live but if I'm constantly iterating on these models and the only really way to know what's successful is to launch it live I'm gonna need to tightly couple my experimentation environment with my production environment it all sounds a little bit daunting before you even mention machine learning it's a pretty interesting infrastructure problem yeah so joe said we want to do it given the constraints of our production stack so we wanted to try coupe low pipelines and T of X that said that it would solve this for us so basically the conversation you see here on the slide was very close to what we had me and Joe in the beginning of this project and to unpack a bit what I've tried to say here let's take one step back and try to figure out what we're actually trying to do here so we're trying to push the boundaries of recommender system in order to improve the experience of our users and what we're actually trying to do is very close to this quote that I love from Joe Joe Ben Joe which is research is like a random exploration guided by intuition it's ok to fail but it's more important to try and this is one of like the core abilities we have when building out this infrastructure you want to have infrastructure that let us move faster and try more things and by trying more things you would build it more in like a better intuition and give us a better idea what to try next so what we're trying to do is like faster our iteration cycle and actually lower the time for an idea to a production because as Clemens Clemens did p.m. M p.m. from tf-x mentioned research often leads to production so we want to be able to do to make it very easy to go from any to production and this is very important as you already mentioned because it's quite hard in an offline session as fashion for a recommender system to know if you're actually doing the right thing and actually improve the experiments for a user so the way we do that is we run about a lot of a/b tests but in order to get to a B test we need to put our models in production and it is quite hard especially since the big scale Spotify and our latency requirements so when we started thinking about what kind of infrastructure we needed for this we started by thinking what do we actually need so Joe started by basically zooming in on this orange box but if you zoom out a bit there are many different areas we have to think about we have to think about data collection feature engineering serving infrastructure in order to successfully go from an idea to production so in order to like accumulate this we needed some common infrastructure but we weren't the first one to actually get this problem so we started by having some inspiration from other companies so we started reaching out to other big tech companies we started reading a bunch of research papers and tried to figure out what is actually important here and come up with a set of requirements so the set of requirements were essentially coming up with tooling for experiment tracking mostly like preferably UI make it easy to run big distributed workloads so especially with our scale we have big models that train on multiple GPUs or multiple CPU nodes we want it to be very flexible since ml is changing so rapidly we wanted to don't have to reinvent the wheel all the time but just built one infrastructure that is very flexible we want to allow for easier hyper parameter tuning because you can get a lot of gains by doing proper high parameter tuning and we wanted to do all of these in Python because this is the language of choice for most of our data scientists and machinery researchers so we want them to build those pipelines within a language they know so we decided to go for T of X and cube flow pipe lines because those were both open sores and it seemed to be well supported in Google Cloud the issue is that are quite early stage project so we knew we were taking a risk but we saw great potential so we went for it anyways so what is T of X T of X is a platform for end-to-end machine learning pipelines and it basically consists of several components that actually are extremely useful when to go from an idea to production so to start with there is the example gen so it takes in either CSV or bigquery and outputs T of Records and T of Records is the data format of choice for tensorflow it's basically a certain format and protobuf and then the first component it has is the data validation so basically you have a bunch of data and you want to know statistics for you status so you want to know how these features are behaving so you want to know histograms main max number of missing values and these things are useful for a few different ways one is for a feature engineering so this is the next step so actually taking this raw data and outputting in a vector form and basically start doing the math that Jose described earlier and the other thing you want to do is you want to be able to see and detect drifting features and anomalies in the data so we have if we have a model and we train it on a bunch of data and then we collect more data from it we want to see if that data that is coming in is is fundamentally different to the one before because episode and the model performance is going down and we want to retrain and then it provides a trainer component that basically takes in an estimator the estimator FBI in intensive flow and is able to in a distributed way execute is on air kubernetes or on ml engine and then the last part is basically a model analysis so if we train our model on a bunch of data and we have a bunch of data that we didn't use for training we want to see how our model performs on that data that we didn't use for training and this data valid this model analysis component basically comes with the UI and it allows us to break predictions down in different dimensions I will come later why that's actually useful and then the last part is basically seeing if the accuracy of your model is good enough and if so you want to push it to production and a nice thing of T of X it as it comes with this metadata layer so every component outputs all the artifacts into the metadata layer and therefore we can do things like lineage tracking so if we have a model we actually could tell could ask the metadata layer okay what was the data that was just rained on and is actually able to tell us and we could do things like okay give us the distributions of my data I given this model and in the second part of our stack is cube flow pipelines which is very complementary to T effects it's able to do things like experiment tracking we can see the output of our runs it comes with a Python DSL so every data scientist or machinery engineer could actually construct these pipelines and then execute them on kubernetes because it's Q prop I plans is both the orchestration layer as well so every step or component in the pipeline is actually a darker operator that runs in your kubernetes cluster and that is very good because it allows a certain amount of isolation so we can do things like sharing components throughout the company so part of our work is actually coming up with like specialized recommendation components that then the rest of Spotify could use in a natural way all within Python so taking it all this in together after a couple of Springs of work we got to all the initial rough edges because as I said before it's still a bit early stage and it took us around like a month to port at our all infrastructure to this new stack but we saw great benefits from it we went from a month or two months from one model iteration to the next got it down to a few weeks an hour a matter of days so basically we're using cube for pipelines surround end-to-end machine running pipelines using the T of X components and a bunch of our own custom components and next to the increase in aeration speed we also got more control and we got more understanding what our model is actually doing so here you see a visualization of this data validation the model analysis component that I mentioned earlier and what it basically is it executed a pipeline in a distributed fashion and is able to break down your data in certain dimensions so an example for us why this is useful we can take a model and then break it down for a market so we could actually see how well our model is doing per country and this is very important for us because music culture varies a lot from one country to the other and here you see the plot of our improvements in model accuracy so here like the blue line is randomness and you see if we go further up in the left upper corner the better it is so we see with each iteration we actually improve or offline metrics and we are very confident giving us new stack and giving the increase in aeration speed that we can keep improving this so to wrap it all up we encourage everyone to try T of X and cube flow pipelines because it provides a lot of value for us we think that it's very good that there are two tools available that are developed in the open source so we can encourage everyone to start contributing back and making em all running ml pipelines less painful and making it better for everyone thank you [Applause] [Music] 