 so this is my invitation to everybody wear this suit it's covered in about so 24 to 36 depending on if I've lost any little little spheres that are covered with retroreflectors and in a moment you'll see how we're using this so the immersion lab and MIT nano so tomorrow is our sneak peek but let me tell you a little bit about that you can go to the web page if you go to emergency mit.edu we know how those AR URL link you over to MIT nanos web page that talks a little bit about all the things that we're providing inside of the immersion lab at MIT now it's a very flat display you can go read that or if the clicker works there we go so right now we're standing outside of nano tell us something a little unusual going on with the little blue dots looking around okay I'm gonna go walk inside of nano here for a second I'm not sure where I am okay let's see where am i with respect to nano so this is far more sort of realistic if you were wearing a heads-up display but we were able to capture 3d scanned imagery of all of MIT Nano both inside and outside we're now walking down the halls in the entranceway to MIT Nano you'll see on the the directory there for a second third floor the cave before we opened the immersion lab originally was called the cave but it's it's now called the immersion lab so if you're looking for the immersion lab you follow the directions to the cave so let's go to the third floor okay we just teleported up there we're sitting in the little lounge area between building 13 and and 12 and took a zoom back cleaning room over there orient where I was sitting go down around the hallway and I want to work my way around and my team you know so first okay I'm on the third floor now we'll just double check make sure that the immersion align moved okay it's still there the cave third floor so let's keep going we'll pause in the moment as we like to call this space and we'll take a look up at the dome nice bird's-eye view there let's take a zoom out zoom around real quick let's accelerate our path down there orient ourselves down the hallway head go here we go walking down the hall parallel to the infinite corridor look carefully you can see the cameraman that would help take all the photography there so the conference room right there on the right turn the corner and we're just about the entrance just about to the entrance to the to the immersion lab okay so now let's go into the immersion lab or let's talk about a little bit how we came to decide what we should put into the immersion lab for the first set of capabilities that were making available to the community they might be community collaborators as a sort of the model for the rest of MIT Nano it's a it's a phenomenal resource let's talk about the resource that is the immersion lab so Megan Roberts I'm not sure if she's here at the moment I see your hand in the background so she's waving back there it's been leading a outreach effort across campus to understand what should be in the immersion lab what are the set of capabilities that we should have there to interact with MIT Nano in terms of being able to visualize the data that's coming out of MIT Nano to be able to visualize the production data if you will or how because when we fabricate or to visualize the metrology data on the cryo electron microscope or broadly what does the MIT community need what are the resources that we should put inside of MIT Nano that are prohibitively expensive for one other single investigator to to purchase and to use so we've had and we've continued to have a series of outreach to across all the schools we've identified Megan's identified a number of very interesting interests and I would say this is not the final list but this is the place that we're able to get started people are very interested using VR virtual reality tools that can experiments developing novel technologies for AR and VR applications and in medicine in manufacturing and design and blending the physical and the digital world so what that means is sort of having technology that allows us to capture motion which is why I'm wearing this thing and to be able to visualize datum a number of interesting questions how do we use AR and VR for education should we have an open courseware a textile of pedagogical content that's aligned with using AR and VR technologies there's a lot of undergraduate and graduate student interests that would just like to play and we're constantly seeking advice on what we need to incorporate so we'll see in a second what we do have and and the sneak peek starting tomorrow and and Wednesday you can see more still have a lot of displays this is one of several types of head mounted displays fully immersive experience virtual reality will seek sort of if the displays are in front of your face the ability to track where these things are in space and you'll see Wladimir actually up in the harvard facility interacting with with I think was nano me to be able to visualize molecules and things at the nanoscale so we have technology that you can immerse yourself into virtual reality environments um this is not me at the moment but why is that interesting so Eleazar Edelman for example is teaches a class here in physiology and they'll want to be able to demonstrate for example what the normal heart looks like and pathological cases of the human heart when doing a biopsy or doing a cadaver study it's typically easy to find good healthy hearts it's not always easy to find the sort of the non healthy heart and be able to convey what it looks like when a normal heart is beating or if I want to zoom into the human heart and understand the the vasculature understand the valves understand how things work you can get one level of familiarity when you're doing a dissection but you can get a whole new level of familiarity if you can take data in this case the human heart or data that's coming out of them trolley tools and bring it up to the human scale to see the textures that here are micron and tens of microns of size its features but to put those at human scale it appeals more to some of the intuition that we may better gather when when looking at data here we go so we have two some of the first tools that we enabled inside of MIT Nano two new cryo-electron microscopy we have as I like to say the biggest of big data challenges this tool will generate over four terabytes of data in 15 minutes and it should be booked at nonstops will be generating petabytes of data and in a non-stop clip with one tool now when we turn on the 12 imaging days with not every tool will generate so much data but we have a big data problem and how do you manage how do you visualize how to manipulate MIT Nano the immersion lab certainly provides an entree point to the computer scientist or the graphic artist so people who want to understand how to manipulate and visualize and interact with data and as well we have the the machine learning aspect of law how do we robustly compare data that's coming from a gold nanorods a or it's aged in looking at it tomorrow or six months to be able to robustly compare multi-dimensional multi spectral features robustly easily it's not like when a new graduate student comes on board and you're sharing a Word document you just copy and paste it you don't copy and paste four terabytes of data so some of the just the data plumbing or some of the really interesting research questions that are generated because of MIT Nano this is just a graphic that maybe gives a little bit of explanation for why the the cry electron micrograph II looking at a sample under different orientations and looking at imagery through creating an image projection through a thin slice from many different angles and bringing that all together so you have now a volumetric replays there we go a volumetric representation of biological structure or a material structure that now you're not just constrained with looking at it on a flat display but you can bring it into either your heads-up display or something that's out at human scale okay so it can't just be all heads-up displays this was some a we borrowed some hardware from from Sony and we had a very small TVs grow when you get them home well this one didn't grow quite enough we're in the process now of designing and going through the process of figuring out what is that full immersive display that goes on the entire outer wall so this is in design and that will not be in the sneak peek but this is something that will be in the immersion lab to come okay so it's on a visualization side we envision the visualization capabilities being a combination of both a very large wall mounted displays singular big screens projection or or monitor and then also a number of different technologies for head mounted displays now this is why I'm wearing this suit this is a rail system that as cameras around the outside and I'm wearing a suit that just makes it really easy for those cameras to detect the features that are on me and you know we can do either biomechanics studies as elucidated in the bottom or have multi actor multi people working around in one immersed physical environment one of the important things and it came up earlier today when you're interacting in a virtual space the sensors that you need you need to know where you are how you're moving to be able to keep track of the physical orientation between your data representation and you I want to physically interact with data I need to know how I'm moving to interact with that virtual thing otherwise if there's a disconnect if there's lag that leads to the motion sickness and things that sometimes virtual reality has been known for so we need very robust abilities to track motion this is the immersion lab before when it was almost empty we have some some floor tiling on there going through the process of installing a rail with bunches of different cameras each one of these cameras has its own illumination source so that now you know you can wear this suit you can come into the lab this is just tracking it was it was a good stress relieving day for me just spinning on a chair the 3d representation so I have these markers on my on my elbows on my head and then so on the joints and then you need them at least one place between the joints you can get the rigid body translation and rotation of your various limbs okay so if you ask nicely will let you juggle these $500 balls that really aren't intended to be juggled but they are instead of wearing the suit again they are these little little objects that can be tracked through space understand the dynamics here of how those things are moving around while I'm juggling so we could quantify my ability to to juggle and fortunately at least in this video I didn't drop anything and so Tom offered he said for five bucks if I did this on serve I did it on stage now he'd give me five bucks I'm not good I'm gonna do it but so you know you can get all the types of motion that you would want in terms of doing biomechanics studies but now why is it important for visualization if I'm wearing a heads mount a head-up display and I want to reach out to a piece of data and it's physically located at this place I have other people in the room I need to know how things are moving around right so it's a combination of visualization and motion so as we said we're continuing the outreach across campus we've certainly identified a core group of potential users both there's a major need just with inside of MIT Nano to support it and the things we want to do for training and visualization of data and then reaching out broadly across campus to identify those other areas of interest it's a big box it's 28 foot cube the equipment that's there now this full motion capture system anti latency floor for doing even faster tracking of the head-mounted displays lens cloud which I'll have up in a moment different types of head mounted displays and the things that are paled out in the bottom are things that are in process that the data infrastructure of the compute infrastructure to be able to process data and the the wall the full wall display okay so the sneak peak tomorrow please come by we won't be able to have 200 people in the room at once I think fire code tells us we can have 49 but we'll have a series of demos throughout throughout the day and we're really looking to identify the community of users that can further inform what we should be doing the hardware we should be buying and the infrastructure that the MIT community and our collaborators will be interested in having okay just want to highlight already there's an ongoing sort of emergent collaboration between iams Iams Institute for medical engineering and science very interested in the healthcare and the biomedical applications of being able to do biomechanics analysis tracking people in the home as Deena was talking about earlier so there's a nice intersection between what I'm just trying to do over and B building a 25 and what we're doing in MIT Nana okay we heard from the this morning from ncsoft NCsoft has graciously provided not only some seed funds to to launch various projects on campus but also provide the the capability to provide or to purchase equipment to help us to originally outfit this space okay the last thing so this is the delenn's cloud so I'll just go right to the the actual image of it the lens cloud is a very rapid way to do 3d scanning it's I forget how many cameras and how many illumination source but you walk into the cylinder and instantaneously in effect it gets the data to be able to create the avatar of me or if I need to scan a couch or if I need to scan a small animal if I want to scan anything it can instantaneously get that data it takes some time to process it but we'll end up with a 3d and virtual model and so if you if you're if you're willing you come step into the lens cloud during the sneak peek tomorrow and then on Wednesday so we're open so come on by it'll continue to evolve and this is our at the bottom the MIT was a very poor attempt at trying to spell out MIT with our bodies but with that I'm at zero so thank you 