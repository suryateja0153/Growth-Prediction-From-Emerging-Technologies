 Thank you Philip and William I learnt so much from these lectures  I think we all have a very stronger common ground  In terms of trying to define the relationship  between computation and urban design  And asking what are the socio economic  values generated in the process  but we all taking slightly different perspectives So that is perfect to trigger   some cross discussion It’s already 10pm on my side  So pardon me if I sound a little crazy My name is provides  I’m tutoring RC10 and RC19 To which the theme is ‘compression’  I know its been a long day for some of you so I’ll try to make this entertaining!  My two lectures are a series  of discussion on compression  During Last lecture I introduced how compression   relates to information and entropy It was a purely historical introduction   to high level concepts in compression  with examples from computer sciences  So today I’ll show a few projects to  ground the problem in urban design  Aiming at clearing some confusion we will be discussing specifically  on the relationship between  preemptive designs and compression Today, we will have three focus that  illustrates some utility of that  What is preemption What is the value in preemption  How is it related to  compression, thus, information  Before I get deep into it I thought I would first  recap on the main points from last seminar To keep our friends from RC16 on track In our Information Age we often take it for granted   that information is just what it is But really information is not   a simple straightforward idea But rather it can be used in   many different ways and many different context Where information can be understood in terms of  entropy or negentropy Entropy is the measure of surprises and disorder This measure  is the foundation to every digital signal   processing technologies on earth Including digital images,   video gaming, live streams Like the one you are looking at right now  All the way to big data  analytics and machine learning  Last time, we looked back in history at  two of the greatest science  figures in the 20th century  Who related entropy to  information in the first place Claude shannon, the father of information theory And Norbert wiener, the father of cybernetics,  Where does the story begin Well, both scientists were the brightest   minds of their time, So they were both involved in Anti-aircraft fire control   (AFC) research during WWII, basically AFC concerns  how do you shoot your opponent’s aircrafts down Shannon was obviously more dedicated to communication of information,  where the less ‘bits’ you need to use to retrieve a piece of information,  the better it is for the war That's shannon’s entropy,  Which is the limit to a lossless compression Whereas Wiener was working on Signal processing in the time domain  how to compress time operations, So the more information you derive from a system,  The less surprises there are, the less entropy The more likely you can hit your target in the war  That is negentropic  Which is the minimisation of  entropy in a complex system This is the beginning of compression Which exploit the redundancy  Or the lack of surprises in data, to reduce the size in data representation  This caused significant savings in the length of transmission  Meaning the better you can compress  The faster you are about to  send information back and forth  And communicate But what does better in compression mean? Our biggest challenge is not  solely on the act of compressing  Because anyone can compress We can take any image file and  press it down to just one pixel regardless But that wouldn’t be of much use to us right?  So the key question is actually how do you, the sender,   compress in a way that the receiver  Can reconstruct the information to precision That’s what lossless compression means  For instance if I have a video file that’s  too big, how do I compress it in a way  That when you receive the file The video will be reconstructed in your device  And displayed to you looking almost no  difference as before the compression  Shannon was basically saying in this diagram That if we can figure out   the redundancy in the data For instance the repetitive units in data  We can just eliminate it to achieve down sizing The limit to which we can compress losslessly is  the information entropy Which is the measure of the non-redundancy  The measure of surprises The measure of disorder in data Quick summary,  On the relations between  information, entropy, and compression  According to Shannon An increase in   entropy is the increase in information and our abilities to compress losslessly  So, to shannon, compression + bandwidth is value  Whereas wiener quantified  information from another perspective  From the time domain Which is   relates a system to its larger environment So, wiener held the belief that  within a world which always decay into chaos Living beings, like you and me,  always strive to minimise entropy through the creation of information  thus, Wiener held that information is Negentropic Where our environment contains no information  But information is always the  construct of living beings  Through prediction, this clearly  has design implications where  prediction on less probable  events yields more information  Thus, information = - entropy What's the value of this in our society?  How does compression create  socio-economic value for us? Both Wiener and Shannon  set their research question  On the problem of prediction  and probability distribution  Shannon was actually a former student of Wiener So, its natural that they based their hypothesis   on similar grounds Which is the Markov process, it is a random process in which  the future is independent of  the past, given the present.  Where The time relationships are summarized by the notion of a state,  Which evolves over time according  to some probability distribution thus, knowing   the state of the system at any point in time  not only provides a good description  of the system at that time,  but it does seem to capture the  critical information we need  in order to answer questions about the future evolution of the system. Btw  do not reference Wikipedia in your bibliography UCL has a website dedicated to referencing where  reliable sources are explained to you So please visit that site Shannon used an interesting word in his writing,  ‘Choice’, he wrote ‘Can we find a measure of  how much “choice” is involved in the selection of the event or  of how uncertain we are of the outcome?’ Information is measured relative   to the amount of choice within a stochastic processes, Put simply, Compression   in this sense doesn't mean less choice, but through interactions at the horizon   of system boundaries Which mediate between   an individual and its environment the amount of choices become more apparent  So the ability to predict your environment Which are complex systems,  becomes your freedom That's the socio-economic value  In other words, because you can preempt your opponent’s choice  Better than your opponent, this gives you freedom This increase our capacities in preemption Which brings us to the theme of this lecture  What are preemptive designs? Preemption is the capacity   to act before a risk is realised it is an anticipatory action facilitated   by speculations on proximate futures It involves prediction,   decision, and resultant behaviors Together, this is a form of negentropic action While architects have always used simulations to predict the expected impact of a design, which  in turn provisions their decisions, preemption is different in that it is   non-deterministic and goal-oriented.  With the advancements in big  data and machine learning,  preemptive analytics are often used to achieve  specific outcomes by diminishing an entity's   range of future options, where prediction is part of the tools that  precedes the preemptive  action (Kerr & Earle, 2013). In an information economy, We are constantly engaging   in preemption at different scales. For instance,  health insurance and futures markets  are forms of micro-scale preemption;  military strikes and geoengineering  are forms of macro-scale premptions.  Why are they preemptive For instance, geoengineering,  is climate intervention  using large-scale engineering This differentiation of  preemption at different scales  makes apparent two   critical variables to be accounted for - scale of time and scale of information.  The former concerns the rate of sampling and feedback loops, which denotes  iterative processes with a potential  rise in the degree of uncertainty  as the scale increases. The latter concerns the   amount of existing or required data, which is relative to the complexity of the problem.  Thus, a preemptive study of architecture is a study of complex systems, where  its component parts would interact to give rise to properties that the parts themselves  do not possess, thus, are intrinsically difficult to model. Along these lines, my research question is,  how can preemption be applied to an  information economy of architecture? A notable example would be  carbon capture and storage (CSS)  Which is the process of capturing CO2 in the air, And put it into solid form, like a rock  They do so via chemical combustion Say we have a project like this,   a CSS supply chain but then we would need information on For instance How many facilities   we will need before the next century So as to follow up on our 2C carbon budget  What are the architectural, infrastructural,   human and economic capital do we need Meaning, how much do we have to build  What kind of buildings do we need in  order to support this carbon supply chain How is it going to generate profit to  overcome our existing fossil fuel industries  How many architects do we need to train  Who are equipped with such forms  of interdisciplinary knowledge  Also, We not only need to know how  much CO2 is already in the air,  We also need to know how  much we will be producing,  So, we need projected data on the future, As a matter of fact, research  has shown that in China alone  We have a carbon storage capacity of  65 - 1500 Gigatons storage capacity  As you can see, the gap for  approximation is relatively high  The reason behind is that there are  always unpredictable fluctuations  in geological and demographic factors And you can imagine, the larger the country,  the larger the uncertainty Also, All these numbers that I   pulled from different research They are all projected based on   different estimated scenarios So all these will have to be   calculated and indexed according To variables,   such as the mass of population, and how much energy a country is exporting  In the support for global consumption, etc. And such stochasticity, is  exactly Markov processes  Where the past and the future have  asymmetrical influences on our present actions  As such, it is a form of preemption Such preemptive strategies are enabled   by the amount and quality of information  that an individual or a network possesses.  And because we speak of qualities of information, Thus, compression is crucial.  As a matter of fact, We would not be able to run   any digital signal processing nor any simulation  or algorithmic prediction without compression  Unless we are willing to wait years  just to generate one set of output. Every time you look at some digital video, listening to some digital music  Some compression based on Fast  Fourier transform (FFT) algorithms  Is running in the background, Fourier transform is exactly what Wiener   studied for compressing time operations Last seminar I explained a   little bit on how FFT works, If you are interested you can go back   to it either on my instagram Or youtube Herein, I am going to further illustrate  what preemptive designs may be  With two projects that I’ve been working on Of course, we were not necessarily thinking   specifically about compression  and preemption at the time  But these theories help us, as designers to better spot   my own weaknesses and potentials As I always tell   RC10 and RC19 By getting a good coupling   between design and theory I don't mean explaining   your design with some books Because that only translates philosophies   into architecture on a superficial level But I mean, using the theoretical   and reasoning skills you learnt from literature To better understand your position in history  Know the people who had come before you  to learn from their mistakes and success  Thus, you may better understand the  socio-economic demands of our time,  And begin to search for values in  your own work relative to such demands Let's begin with a project I did on the  information economy of architecture,  Which is a form of autonomous emergent system Because every individual   act towards their self interests But together, they give rise to an economy,  Which has properties that its component  parts do not have on their own And let's say we want to digitise  the entire transaction system  And decentralise it with blockchain technologies,  In architecture, this can  help us to better integrate  Smart tools, from smart contracts,  smart workflows, to smart cities  Where all transaction data is simultaneously  anchored and broadcasted in an immutable   manner at the point of creation In this case, you protect transaction   data via peer-to-peer review mechanism    What is this going to do for architects?  we need systems that enable  real-time communication  Between a scattered chain of people to enable participatory design also, facilitating a communal  information environment  to build up an expert system where designers can contribute micro-IPs via BIM portals While providing instant reward functions to sustain cash flow  Especially in economic  shocks, such as this pandemic How do we do it? Buildings are physical properties,  they are expensive stuff But architecture is less rigid than that,  it is intellectual property Which can be transacted at any scale  From 1 byte to 1 gigabyte  What the research proposes is to look at Architectural information systems  The same way we look at other  commercial information systems That was the reason why I started thinking about  how to couple information technology - blockchain  With industrial specific applications - BIM What is blockchain At its most basic level, blockchain  is literally just a chain of blocks  Where digital information (the “block”)  stored in a public database (the “chain”)  What kind of information?  Blocks store information about transactions like the date, time, and dollar amount  who is participating in transactions each block stores a unique code   called a “hash” that allows us to  tell it apart from every other block  What it does is basically allows a secure  yet transparent way to publish data That is super good for BIM, Because it enables a common data environment (CDE)  Where all architects can  collaborate on various projects  And multiple users can contribute  to participatory designs Blockchain’s Shared Data Layer  within its application stack  can help to anchor digital assets to facilitate  an immutable Common Data Environment for BIM also, blockchain’s universal communication and  control protocols can help the stacking of multiple applications  and API (application programming  interface) units into  software packages on demand. This facilitates a means for BIM systems by  freely integrating with crowdsourced efforts  to foster a diverse integratable app stack Especially with open source AI algorithms GANs’ ability in synthesising large sets of data  is ideal for coupling with BIM to generate floor plans  and optimise building layouts specific to local building regulations,  this enable users to search and browse through a large body of qualified articulated options Also, AI can help to automate  repetitive visualisation and  alteration works Using pix-2-pix To provide designers with the autonomy of curation  And collapse the linearity  between 2D to 3D pipelines  Basically, together, the system and the users They continuously creates data that  feeds into itself via diverse value routes And streaming data channels  Which can, in turn, be used for the training of AI But when you distribute data across the network, This concerns problems of compression,  You will need compression not only  in the processing of digital signals  But also in encoding large sets of data  into representations in a lossless manner  So when you try to retrieve a piece  of information at a later stage  You can reconstruct these distributed signals Meaning, compression not only concerns   getting rid of redundancy in data But also how do you recover information Let me quote my friend Mark here Who has a blockchain startup 21e8  blockchain nodes today copy data but once they get bigger   they will compress everything a lot more and distribute the data collectively  filling in the missing bits with error correction you can see blockchain networking from p2p   as a type of error correction so when there are   more fuzzy signals to be reconstructed You will need compression algorithms like FFTS  This is the connection between  compression and distributed information Also, this is where preemptive design lies in Because you are essentially creating   a large pool of data Through a p2p consensus,   you have a natural selection on information Where you may be able to correct errors   through probability distribution With machine learning that   feeds on this pool of information We may be able to act before a risk is realise For instance, automated process discovery analyses  unstructured data in event commits and application   logs to predict business outcomes and find  solutions for preemptive transformation (Reeves,   et al., 2020) (Maggi, et al., 2018). So if you have a business, you may be   able to identity problems in your business model Before a risk becomes apparent and problematic  The same logic applies to  architecture and urban design,  Via p2p validation and ranking on information  We may be able to develop not only  common data environments in BIM  But models of the socio-economic structure  specific to the building industry  And be able to describe, analyse,  predict, and preempt the outcome For instance, the building industry increased  to be accounted for 13% of the world’s total GDP  But why our productivity has only  increased 1% over the past 20 years,  Even though we have been putting  in more technologies and more work  Preemptive systems may be able to help us  in searching for answers to these questions  Where compression in essential from encoding,  distributing, and reconstructing data  Mention the stage of compression we are at now We are good at specific purpose compression  For instance if it’s an image We can have image compression  Video data We can do H.264 compression  Which is MP4, works pretty well But we are very bad at general purpose compression  That’s the same reason why we are bad  at general artificial intelligence  How will be able to do general compression  without knowing the structure to data  We will need machines that are  able to learn from problem solving  That is the ‘why’ and ‘what’ of compression But we need to advance on ‘how’ These are various papers I  published based on this project,  Which I am not going to bore you with the details The second project is ‘current’  Current is a speculative urbanism project that thinks about  the future of broadcasting cinema, and its impacts on our cities It has 4 core ideas:   livestream data, volumetric reconstruction, AI  image processing, and algorithmic personalisation  Which I am going to show you  a 10 min demo of this project  before I start illustrating  how it is a preemptive system (current) Me and my collaborators experimented first hand  with a range of emerging technologies That are readily available to any individuals,  and proposed a production pipeline That facilitate the collective production,   processing, broadcasting, and archiving of urban data in real-time Livestream includes image and metadata which can be extracted for environment   and events reconstruction With machine learning,  you can have estimation on what is behind a foregrounded object  So that is perfect to be coupled  with photogrammetry frameworks  Which calculates based on vantage points On the other hand,  we experimented with AI image  processing like Autoencoder,  Which can help to fill in missing information  on texture maps based on archived data,  and object detection can help  to estimate scene descriptions.  The output volumetric data will then be  plugged into personalisation algorithms,  which will label, rank, and deliver  recommended content by collaborative filtering Finally, the output will be  pulled into displays on demand  Which are volumetric navigation  engines, like VR devices,  and accessed by a network of users. This basically helps to reconstruct   3D environments based on multiple   vantage points from sequences of 2D images Especially with the ever mounting terabytes   of image data we are producing via social media This is a demo we produced in the   most economic way possible for participatory purposes  Meaning because we lowered the cost of production, More people can participate That facilitates an attention economy,  where the reconstruction of  certain events and environments  May direct value back to the entity For instance,  In this world we re-constructed polar bear tracks  A bear who walked though  mountains of trash because  he couldn’t find enough food in the poles using livestream data from bear cams  He is actually looking at a livestream of himself Through visual aesthetics and world building  We tried to harness network effect This can potentially generate   financial and social credits for the protection of the species  Of course this stays as an  artistic expression for now  But it tries to illustrate how  such system may look and feel like Techniques of Compression is also used  the visual aesthetics in ‘current’  Okay so i'm about to share with you  part of the confidentials to current  As you probably noticed, Current is produced entirely of Single shots  With a style of image morphing We you as the viewer is actually   navigating from world-to-world Instead if frame-to-frame,  So this techniques is used in gaming a lot Current achieve this not only through   volumetric reconstruction But also with AI image processing   and quasi AI image processing What do I mean,  While we used autoencoder and GANs AI Which inferences estimation between images  Both AI uses compression, especially the Fast fourier transform (FFT) to  Compress and encode data While outputting the reconstruction  The whole process is iterative, meaning Output is always feedbacked into input   until an optimal is achieved So our team tried to   imitate the same mechanism manually So we have better curatorial control on visuals  Meaning we learnt from AI, To rapidly compress and reconstruct the image data  And that gives the visual aesthetics you see here With a smooth transition between worlds Current can function as an urban archive  Which act as a preemptive  system via hyper-personalisation  Hyper-personalization leverages artificial  intelligence (AI) and real-time data to   deliver more relevant content, product,  and service information to each user.  But instead of just doing  personalisation for preemptive marketing  It can take this one step further,  and deliver hyper-personalised systems  For industrial or disciplinary data, If we go back to the carbon capture and storage   example Current   can be a form of microservices architecture That is personalised to our carbon supply chain Which can deliver real-time broadcasting  and reconstruction on carbon data  Thus, deliver a pool of information, which  enables us to work on preemptive designs  Such as geoengineering, So we switch from personal data-collection,   to industrial or disciplinary data collection But where is the market demand in this? Personal data collection helps digital  platform on preemptive marketing  So they can advertise better  and push product consumption  How can industrial data achieve  the same sort of incremental profit  Imagine, if carbon data can be made entertaining, As entertaining as ‘current’ is  It can function to generate  profit for our carbon supply chain  so it catches not just large  capitals, large investments but aggregating the efforts of even you and me For instance, as an average citizen,  why would you pay for access to some tree data Well,   how can looking at some trees be entertaining, that's within the expertise of the media industry Based on this We are currently working with 21e8,  the magic number company to realise a blockchain camera system,  where chains of cameras can authenticate  urban events amongst themselves  To increase our capacities in retaining evidence  for the much contested event  landscapes in our anthropocene  Potentially, we can have micro-licensing  anchored on parts of the entity  Be it human or post-human entities, like trees or the north pole  For instance, we can have  the 3D scan of a polar bear  Let’s call him ‘Sam’ We can have Sam micro-licensed  And whenever Sam Or part of Sam generates a profit,  either received a red pocket from livestream, or someone purchased the license for use,  Micro values, will be directed to NGOs for the protection of the species  All transaction data is transparent  and immutable on the blockchain  So that we would be able to know  whether the money is indeed used for the bear this is some architecture stacks we are   proposing based on John Nash’s  theory paper in Agencies models And these are my collaborators  We come from very different  disciplines and countries  Where livestream and algorithmic  personalisation actually got us together Before I let you go, Lets just go through a few points of takeaways  To recap the relationship between information,  entropy, compression, and preemption, information = + entropy Compression + bandwidth  information = - entropy Prediction / preemption   (operations on a time series) Negentropic designs  Complex systems (e.g. society,  economy, urbanism, etc.)  Not the grid, but system designs  that define the boundary of a system  Through probability distribution We may be able to compress and preempt  Where compression is a  quantitative measure of information  Is the data representation that  get rid of the redundancy data  But also concerns the reconstruction  of data to retrieve information  These functions are essential to preemption Which is the first right that allows an   individual to act before a risk is realised;  it is an anticipatory action facilitated by   speculations on proximate futures Through information exchange While architecture has always been an  economy that relies on information exchange,  A preemptive design is different in that it  looks at the information system as a whole,   and emphasis on the collection and  distribution of relevant information   at the appropriate time (Castells, 1999). Where productivity of actors in a network   is dependent on their capacity to  efficiently create, realise, accumulate,   and circulate knowledge-based information, and preemption helps to create value for users   by ensuring their rights to act  through applying such information. 