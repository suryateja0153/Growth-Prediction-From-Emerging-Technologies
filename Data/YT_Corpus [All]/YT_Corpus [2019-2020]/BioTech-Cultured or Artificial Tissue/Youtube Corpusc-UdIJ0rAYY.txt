 Good morning, everyone. Thank you for joining us today for the NCI Data Science Seminar Series. I am Daoud Meerzaman, Chief of CBIIT Computation Genomics & Bioinformatic Branch. I know that we have been away for a while, since actually March of this year due to COVID-19, but we're back, and we will be holding this seminar series, the Data Science Seminar Series from here on. So as a reminder, today's presentation is being recorded and will be available on the datascience.cancer.gov website. You can find information about future speakers on that site as well. And by following us on Twitter, @NCIDataScience. Today, I am delighted to welcome Dr. James Zou, who is an Assistant Professor of Biomedical Data Science at Stanford University, as well as Chan-Zuckerberg Investigator, and the Faculty Director of Stanford Artificial Intelligence in Health Programming. The title of his presentation is: "Computer vision to deeply phenotype human disease across physiological, tissue, and molecular scales." I guess, without any further ado, James, please go right ahead. You've got the floor. Thank you so much again for joining us.  Okay. Thank you so much for organizing this, and for the kind introduction. Yeah, so it's very nice to meet everyone. Good morning. So I'm happy to tell you a bit about some recent advance that we've been making in developing new computer vision algorithms to study human disease across these very different physical scales. Okay, so to set the stage for the presentation, so I would like all of us to do a thought experiment, right? So I've written on the screen a few standard descriptions of a particular individual, right? And I would like you as a part of this thought experiment to sort of visualize in your heads, right, what you think the face of this person should look like. And just take a few seconds to do that. Okay. So now I'm going to show you the actual face so you can compare that against what you visualized, right? And you can see that they -- I think this is actually quite a challenging exercise, right, because when I did this, or when my students did this, right, the face that we visualized is actually very different and it's much less vivid compared to the face that we actually see here on the screen. And this particular photo I think is quite interesting for a few reasons. So this is a photo from a New England journal article from a few years ago, right? So it's actually a photo of a truck driver, right? So you can see that half of the face -- it's sort of asymmetric, right? So this half here, it's sort of more wrinkly, sort of sagging, right? And as you can imagine this half of the face actually corresponds to the part that's exposed to the sun, right, exposed to the window of where the person's driving, right? So that sort of gives a vivid illustration of how the environments can interact with the biology and the genetics of this individual to inform his health status. The second reason why I thought this photo was quite interesting is that it really illustrates how much more information is actually contained when we can look at something, right? So there's a lot more information in the image itself compared to if we're looking at just a standard natural language or standard categorical descriptions of this individual. If we look at standard categorical descriptions, there could be things we can extract from EHR. In this case, maybe they will have kilobytes or bytes of information. But the image itself would have megabytes and gigabytes of information. And so this illustrates that, you know, there's a lot of rich information that's captured by the images that's actually hard to capture in other types of features. And this is the case for something that's quite familiar to us, like these human faces, right? And I think it's even more so for objects that might be less familiar to us, right? So a lot of what my group's studying -- applying computer vision to study different aspects of human disease. So here I'm showing you three different, you know, objects, three videos, right? So you have the ultrasound of the heart, right? So you have something that is smaller, which is the tissue biopsy from a breast cancer tumor. And then you have something even smaller still, which is looking at the individual cells, right? So these objects are much less familiar to us, and it's actually really not clear how could we really try to describe the morphology, the dynamics, and the geometry of these different objects, right? And this is where really the goal of our research comes in, right? So we would like to use computer vision essentially to learn a new language of morphology. And then from that new language of morphology, which hopefully is more precise, right, we would like then to study human disease and human biology, okay? So the part of the challenge that we hope to capture here is that we really lack a standard language to precisely describe these objects, and this is where we'd like the computer vision algorithms to help us to fill in that gap. Okay. So as many of you know, right, so there's been a lot of recent advances in deep learning algorithms, and a lot of those advances are in particular driven by computer vision progress, right? So everything from looking at, you know, image recognition to self-driving cars, I think there's a lot of computer vision that's really behind of it. And a lot of the work in my group in particular has been in developing computer vision algorithms, and in particular, making these algorithms more reliable and more robust, right? So some of the technologies we've been developing, those have been used by companies like Google and Facebook, but in particular we're very interested in [inaudible] health applications. So for this presentation in particular, I would like to give some ideas of what are the most exciting recent advances in computer vision, and how we can apply those to study, to extract phenotypes and morphologies that are relevant for human disease, right? And I would like to do this sort of by going through three examples, and in order of larger-scale objects to increasingly smaller-scale objects, right? So the first example, sort of like a warm-up, we're going to look at something that's maybe more familiar to us: basically, the motion and phenotypes of the human heart, right? And then we'll zoom in to look at biopsy tissues. These are actually specifically from breast cancer tumors. And there's some really interesting cancer biology we can hope to learn by looking at this. And finally, we'll look at individual cells and even subcellular information, right, to kind of biology can we extract just by using computer vision? And the goal of this presentation is to hopefully, by going through these examples, I want to illustrate some of the general technologies, right, and some of the general design principles of developing and deploying these kinds of algorithms. And hopefully, this is also useful for your own research. And I also want to say that I'm happy to take questions throughout the presentation, right? So please feel free to, you know, to either raise your hand or to type it into the chat box at any time. I also pause at a few places every 10, 15 minutes to take some additional questions. Okay. So we'll start with this example of using computer vision to phenotype the human heart, right? So this is a work that's led by two terrific people in my group. So David, he's actually a cardiologist, and Bryan is a terrific computer science PhD student in my group. And the paper was recently published a couple of months ago that describes this algorithm. So the problem they wanted to tackle here is the problem of looking at heart disease, right? Now, heart disease, as you know well, is actually the leading cause of death in the U.S. It's actually responsible for 1 in 4 mortalities in this country, right? And one of the most commonly and routine ways to assess how well the heart is functioning is actually by these ultrasound videos of the heart. These cardiac ultrasound is also called echocardiograms, right? So these ultrasound people take these ultrasounds and compute various scores, such as ejection fraction, that assesses how well the heart is functioning. So these kinds of ultrasounds is actually very routinely done, you know, in this country and around the world. So in the U.S. there's over 10 million of these ultrasound done every year, and they're not cheap because each one will cost potentially over $1,000. So one of the reasons why these ultrasounds are actually quite expensive is that there's a lot of human vision that's used to manually assess how well the heart is functioning based on these ultrasounds. So here I'm showing you four examples taken from four different patients at Stanford, right? And you can actually see there's quite a lot of subtle differences across these four different patients, across these different ultrasounds. As a quick primer, right, so what a cardiologist is looking at is this particular chamber of the heart, right? So you can think of the heart as basically this pump that's generating power. And if you want to assess how well this pump is doing and how much power it's generating, what you do is that you go through this video, right, and the cardiologist will actually manually select the frame of the video where the heart is the largest, when it's most expanded, and also the frame where the heart is most contracted, when it's the smallest, right? And they're looking at -- and then they would trace out this volume of the heart by looking at how much the heart expands and contracts. There's a mathematical formula that derives sort of this ejection fraction which measures how well the heart is functioning. So as you can imagine, this is actually quite a manual and labor-intensive process because you have to find those frames and you have to trace it out manually, and then finally you do this computation. So this is actually takes quite a bit of time, making this process more expensive. It's also quite variable. So you have two different experts looking at the same picture and they might actually often arrive at different assessments. So with that -- okay, so this is actually a great problem to trial and to deploy and to test those computer vision algorithms, right? Can we actually use computer vision to do this process more reliably and faster compared to currently what's done with the human vision? So this is basically an example of how we actually apply our algorithm. I'll describe the algorithm in the next slide, right? So the input of the algorithm would be exactly these kind of ultrasound videos of the heart, right, that people routinely collect. And this is the output of the algorithm, right? So it would actually -- segments in real time, right? So this left ventricle chamber, the relevant chamber of the heart. It also in real time assesses, right, for every beat of the heart, how much power is generating? So how much ejection fraction corresponds to every beat of the heart? Okay. So here's a quick overview of how the algorithm works. And again, I want just to focus on what I think are some general recommendations that might be useful for other projects and maybe for your own research. So behind the scene of this computer vision algorithm is a particular type of neural network architecture, right? So this kind of neural network, this kind of deep learning model actually has -- in particular here, we designed it to have two branches, two arms, right? So the top arm is a particular kinds of spatial-temporal convolution, right? Because here the input is this video, so we will have to have the neural network scan across both space, it also has to scan across the third dimension, corresponds to time. The bottom of the algorithm is quite interesting. This is the arm where the neural network is actually trying to automatically identify the chamber of the heart as the relevant and to segment it for every, you know, every instance, every frame, right? So it's doing this automatic segmentation. And the reason why these two arms are both important is that, you know, you can imagine, in this video, there's a lot of things going on, right? So in order to teach the computer vision system, it's actually useful to have a [inaudible] based on the segmentations to sort of focus its attention on the more relevant parts of the heart, more relevant parts of the chamber, right? So basically, what these two arms do is that they come together, right? The bottom arm, the segmentations identifies the individual beats. And then the top arm, right, it's actually based on this algorithm, produces the estimate of the ejection fraction for every individual beat. And finally, these things are aggregated across the beats to come up with a patient level assessment for ejection fraction, for heart failure. It can also produce and predict other functions related to liver or to kidney. Okay. So the algorithm actually works quite well, right? So one thing that's important is that when you develop and then evaluate these algorithms, it's very important to evaluate them on different hospitals. So what we did here is that we trained everything based on data, based on patients' data from Stanford. And then we froze the algorithm, right? So we fixed all the parameters, all the hyper parameters. And then, with some modification, we just shipped the algorithm to a different hospital, to Cedars-Sinai. It's a hospital in Los Angeles. And then they tested it on patients' videos from Cedars. And we're actually very happy to see that here are the two AUC curves. That's the performance of the algorithm at Cedars-Sinai, this external hospital, has very high accuracy, right? It's about 0.96 AUC, which is very similar to the performance at Stanford, 0.97. And here are some more examples of the applications of the algorithm. So in addition to working well across different hospitals, we also really wanted to make sure that the algorithm works very well across different quality of data, right? So here what we do is to test how an algorithm works when the input data is increasingly noisy, right? So the x-axis actually corresponds to different levels of noise in the input videos, right? So all the way to the right is you have very noisy input videos where about half of the pixels are corrupted. They're missing, right? And the y-axis corresponds to how well the algorithm works, right, how accurate it is when it's applied to different types of data, when it's becoming increasingly noisy. And we're also very happy, and you are sure to see that even when the data is very noisy, when half the pixels in the video are corrupted, right, the algorithm here is still highly accurate, right? So the accuracy drops off a little bit, but it's very marginal and still certainly within the range of acceptable clinical applications. So if you're interested to learn more about this and interested in the data, we have actually released all of the code, open-source code, and all of the data that's used to train and to evaluate the models. And I think that's actually quite useful because there are actually very few publicly available medical video data sets that's currently out there. So I think this is currently one of the largest publicly available video data sets, so over10,000 patient videos that we have anonymized for these corresponding to these corresponding cardio ultrasound, along with patients' annotations and diagnoses. So if you're interested in this, please check out our video and data sets and also the code. Okay, so I'm happy to pause here for a bit to see if people have any questions before I move on to the next example. Great. Okay, cool. So I'll go on a little bit, and then we can pause more to see if people have any questions. Okay, so now that we've seen an example application of how we can use computer vision to phenotype or to study human physiology -- in this case, the heart, right? So now we move on to [inaudible] and see, how can we use similar types of technology to study histopathology? And this is maybe a little more relevant and more familiar to people in the audience. So as you know, that these kind of histology images, right, are very routinely collected to assess patients, especially different types of tumors, right? So these are very widely collected across these different pathology clinics. So here we in particular have a histology image, right, that's taken from a breast cancer biopsy. And typically, what people would look at is there's a lot of rich morphological information that's contained in these sort of H&E histology images, right? And what people would do is that they would actually look at these -- the image, right, and based on their experience, then we can actually segment out sort of at a coarse level -- we know which part of the image corresponds to tumor regions and which parts corresponds to normal cells. So here's an example clinical annotation that corresponds to this particular image. In parallel, right, there's also another workflow. It's a more recent workflow based on genomics, right, where people would actually take these biopsy samples. They actually dissolve and extract individual cells from it and then they can then perform all sorts of genomic analysis, right? For example, you can do RNA sequencing. You can do DNA sequencing, right, to extract information about this sample. So both of these type of workflows are very powerful, and they're actually quite complementary because they have complementary pros and cons, right? So the pro of these kinds of imaging, the histology imaging workflow, is that it's very visual. You can capture a lot of information in the spatial information, right, just by looking at these biopsy samples. So that's great. The downside is that the type of -- or the amount of information you could extract from the image with the pathologist is actually very limited. Okay, maybe you have some binary readouts of the tumor [inaudible], but you can't really learn too much about just by looking at it with humans. You can't learn too much about it from about different types of mutations or different levels of the gene expression. In parallel, right, so this kind of genomic single cell analysis is very powerful in that you could actually extract very rich information, right? So that's the pro. We can get gene expression values for thousands of genes from each individual cell. The downside is that you actually lose all of the spatial geometry, right? So you don't know where each cell come from. You don't know how they're interacting with their neighbors. So that information is lost. So the goal here of our project is that we wanted to see, can we actually use computer vision to get to the best of both worlds, right? Can we actually still retain the geometry and the spatial information from the original images, at the same time learn about all this very rich genomic information, right? Can we combine these two kinds of outcomes? And that's what we developed, was this algorithm we would call ST-Net. It's another kind of computer vision algorithm, right? So the architecture is actually quite similar to the first example I mentioned, and I'll go over that in a couple of slides. But here's basically the example application of ST-Net, right? So it would take this input, these histopathology images, right, which are widely collected, and then based on just the image itself, right? There's no external annotation, additional annotation needed. It's just based on just the image, right? The algorithm actually translates this image into hundreds of other images, right? So each of the new image it generates corresponds to the gene expression profile of -- the spatial gene expression profile of a different gene. Let's say, for example, if you're interested in this gene FASN which is one of the breast cancer markers, right? So the algorithm ST-Net will generate a new image that contains the gene expression profile of FASN across different regions, right? So the yellow regions corresponds to where FASN's highly expresses, and blue is where it's lowly expressed. If you're interested in a different gene, right, maybe a collagen marker, the algorithm will generate a different image, right, that corresponds to spatial expression patterns of this collagen marker. And it's able to do this accurately for about 100 genes, over 100 genes, right? And we have experimental validations based on actual spatial transcriptome measurements, which I'll describe in the next slide, that validates that the algorithm's generated spatial profile is actually quite accurate for over 100 genes, right? So in a nutshell, you can think of this algorithm as basically doing image-to-image translation. It's translating the histopathology image into hundreds of new images correspond to spatial gene expression profiles. Okay. And if you have a different patient, right? So we validated this on some external patients as we did before. But if you have different patients, right, then it would generate -- we can very easily apply the algorithm by just giving it a new histology image, and it will actually generate a new profile. And this is quite interesting, right, because when we gave this image to the pathologist's -- right, the pathologist would actually annotate -- basically, this image is almost uniformly corresponds to tumor regions, right? However, actually when we look at the spatial gene expression profile, of course, of this biopsy, there's still a lot of variation within these tumor regions for these cancer markers, right? So this demonstrates that the algorithm is able to actually learn more fine-grained spatial heterogeneity, right, of gene expression even compared to the clinical annotations. So this work is actually recently also described in a recent paper, and it's led by my student Bryan, who is also one of the authors on the previous cardiology work. Okay. So I'll only explain a little bit -- oh, I have a question from Sameer [phonetic]. Let me see. Okay. So the question is that when we develop these AI algorithms with different modalities, like histology and gene expression, do we use a unified model or do we actually co-train separate models? Yeah, so that's a good question. So in this -- here we actually have slightly different models, right, correspond to different applications. And I'll actually show you the model in the next slide. So in particular, for this spatial transcriptomic cancer project, we have a base model, which and then has slightly different variations that are co-trained for different genes. Good question.  So we have another question for you from Habali [phonetic]. I'll unmute your line.  Yes, please.  Hi. So I have two questions. First is [inaudible] for the first -- the cardiology model that you showed. My question is, did you look into understanding what it is that the model is learning and perhaps connected the biology? And the second question is for the data that you're showing here. It's very intriguing, very interesting. My question is, what's the ground truth? How do you validate that what you're finding is truly what you have? Because you're not going to have biopsy samples for every data point.  Okay, great. Thank you for both questions. They're very good questions. The second question is actually what I'll describe in this slide of where exactly the ground truth experiment come from. So I'll do that next. The first question corresponds to the cardiology example. So there we actually did do various interpretations to see exactly which part of the heart the algorithm's looking at to make its assessments of the ejection fraction, right? And there's some actually interesting biologies there. I'll actually come back at the end of the talk to talk more about how do we interpret these kind of deep learning and provide some general technologies to extract more insights from these deep learning models. Okay. But for your second question about where the ground truth for these spatial transcriptomes come from, it's actually come from this particular technology that's developed by my collaborator, Joakinn Lundeberg, based on Sweden. So Joakinn's pioneered this spatial transcriptome technology, which I think is actually really powerful. It's a brilliant idea. So he developed this chip, right? And then what you do is that when you have a biopsy sample, you can just overlay the tissue directly on top of the chip. And what's nice is that this -- his chip actually you can see these little dots, right? So they're dotted across the chip. So basically, each dot corresponds to one column of probes. And each set of these probes have its own bar code that locates, right, with the x-y location of that column. Okay, so it's like a zip code for each of these probes. And what happens when you overlay the tissue on top of this chip, right, is that these probes actually penetrate into the cells. And when the probe actually encounters, you know, one of the transcripts, they actually attach its zip code, or the barcode, together with the transcript. So that means that when we sequence the RNA of this tissue afterward, we actually know the x-y location of each of the transcripts, right? So here's basically the experimental outcome, right, when we actually do this sequencing. Then we have these barcode that tells us the location. And for each gene, like for FASN, we actually know where it's highly expressed and where it's lowly expressed across all of these different probes. So that gives us the spatial ground truth. And for different genes, we have different images, right? So we can get this in parallel for thousands of genes experimentally. So that's the data that we generated both to train and to validate how well the algorithm works, right? So an algorithm itself is another kind of convolutional neural network, right? So it's starting from these very large histology images, because they're quite large, 10,000 pixels by 10,000 pixels, and it's scanning across the image, and it's actually looking at it at a high resolution, right? So each region it's looking at is around 100 to 150 microns, so it's quite high resolution. And it actually learns to extract features, right? Based on the neural network, it extracts features from each of these small regions. And based on that feature representation, then maps that onto a [inaudible] of gene expression profiles, right? So this is how it's able to do this image-to-image translation translating from the original histopathology image into the gene expression profile. Okay, so that gives you a sense of how the algorithm works. All right, so, as before, here we also wanted to evaluate the algorithm on very different external samples, right? So then we trained and developed the algorithm on our own data that we collected from, I don't know, these breast cancer biopsies. So we were actually quite fortunate. There's a company called 10x Genomics who had generated -- in parallel had generated -- their own samples, right? So these are very different patients, right? They're breast cancer patients, but these are different patients. And the imaging and the sequencing was actually done also separately, independently, by 10x Genomics actually on slightly different platforms. So this actually constitutes quite a challenging external validation data set. So we're actually very happy to see the algorithm well even on this external validation, right? So each dot here corresponds to one gene, right? So the x-y axis, this is how well is the AUC on two different test data sets, right? So that indicates how well would our algorithm, ST-Net, work for each of these genes on external data? So and we're particularly interested in the top right corner, right? This corresponds to about over 100 genes where we believe the ST-Net algorithm can actually make accurate inferences. It can generate accurate spatial profiles. So these are genes that are validated both on our internal samples and also on these external test sets. And this actually includes many interesting genes, including many of the key tumor markers, immune markers, as well as genes that are related to sort of mobility and architecture. So here's the example, right? And this is, I think, really illustrates the power of these kinds of algorithm, is that they could actually perform computational sort of super-resolution, in some sense, right? So here is basically -- on the top here, I'm showing you, like, a zoomed-in version of this histopathology image, right? So each box here corresponds to like 150 microns. So the algorithm actually learns, right, even at a higher resolution of where inside each patch, right, where the gene is highly expressed, right? So for example, it actually learned that for this gene FASN, which is a tumor worker, right, it's actually the highest expression of FASN are localized at these large, you know, these cells that have the enlarged nuclei, right? So that's not the kind of information that we could actually experimentally obtain because that actually involves going down into the subcellular resolution, which we can't currently do yet. But the algorithm is able to learn that based on this training process, right? And I think this is actually quite powerful, right, because this enables us to systematically associate, right, how gene expression changes depends on the different morphologies of individual cells. It's sort of like association study. And we can do this across all of the genes, right? For each gene, we can see, okay, how does the variation in that gene expression correlate with changes in the different geometries of the nuclei, like the size, the elongation ratio, the density of the nuclei, as well as other geometry related to the tissue and cellular architecture, right? And I think that this could be -- it leads to a lot of really new, interesting biological discoveries. Yes, questions? There's a question about, how -- does the size of each probe equal to the size of each pixel in an image? Yes. So the size of each probe is actually one of these squares here, right? So it's one of these black box. So it's actually than the size of the pixels. This is basically what I also mean by computational super-resolution, is that even though the experimental measurement, right, is the size of the probe, the algorithm itself is actually able to learn sort of sub-probe resolution to see where the individual genes are expressed, right? And it actually learns that these corresponds to some of these enlarged nuclei. And we have more recently done some additional experiments at higher resolution to validate these predictions of the algorithms.  James, this is Daoud Meerzaman. Since I guess we are all asking question, I have a burning question that I wanted to ask you. [laughs] So for those genes that we may not have a morphology associated, those -- I mean, are there any work going on to be able to identify expression of those? Or is it just mostly for the genes that are actually associated or correlated to the morphology?  Yeah, that's a great question. And it's also related to the question in the chat about, you know, how do we select these 100 genes? So I would say that the algorithm, because it's looking at the morphology directly, right, so it's most powered, right, it has the highest power to capture the gene expression, the expression of genes that are more directly related to morphology. And those includes, for example, many of the tumor growth factors, the immune factors, as well as genes that are -- that we know are related to sort of cellular architecture, cell skeleton, mobility.  Yeah.  So those genes. There are many other genes that are less directly associated with morphology. So it is much harder for the algorithm, for ST-Net, to actually capture those other genes, right? So that's why the 100 genes that we were able to capture here are mostly corresponds to the genes that we know have some sort of morphological associations. So in order to capture these other genes, I think we have to do much deeper sequencing. I think that's actually one area where if we have deeper sequencing data as far as larger number of samples, we could actually push the ST-Net algorithm to capture several hundred genes or even thousands of genes.  Great, thanks. Okay. So I think one of the really -- the power of this is that it works well on lots of data, right, lots of images that people have already collected, right? Many of the pathology labs, right, the cancer labs have a lot of these histopathology images, right, and we can already apply the ST-Net algorithm to those images to infer, to computationally generate the spatial result of gene expression patterns, right? And we have successfully applied those to data from TCJA and from other clinical labs. And it's enabled us to quantify things like tumor heterogeneity, cell-cell interactions, as well as the level of immune infiltration. And we're currently working on extending this to other cancer types, right, beyond breast cancer. And we also are extending this to impute, computationally impute protein localization, right? So here's just an example of what we can do now, which is, again, taking from the same kinds of H&E input image, we can actually impute immune-histo chemical stains or IHC stains directly from the H&E image. So in this case, we applied it to impute sort of neuro -- so tangles, right, which, of course, which are sort of one of the markers of neurodegenerative diseases. These are phospho-tau tangles. So that one of the ultimate applications of this that I'm very excited about is sort of this vision of using this for real-time sort of instant pathology, right? So, you know, as soon as we have collected these H&E histology images, right? So sort of like a, you know, like a Instagram filter. But instead of applying, you know, cats and dog features on top of the image, we can directly apply the ST-Net algorithm, and it would in real time impute a spatial result gene expression transcriptome profiles directly on top of the histology image. And this can be done sort of in real time to enable sort of fast prognosis, treatment recommendations that are based on these tumor expression heterogeneity information. And so we saw that this also enables us to link -- study fundamentally biology by linking genes to cellular morphology, right? So that can also discover some new functions of the genes. Okay. So I'll also pause here for a bit to see if there are any additional questions. Cool. So the third application I wanted to tell you about is sort of really taking this to the limit, right? So we've seen how we can apply this to study organs, to study tissues. Now, can we actually take this to study even cells and things even within individual cells? So this is a collaboration that's led by my student Michael, a really terrific, talented PhD student, in collaboration with Shalin and Tom at UCSF and at Chan Zuckerberg Biohub. So here's what we want to study, right? So previously, we had videos of the heart, but now we have videos of much smaller objects. So these are microglia, sort of the ending cells in the human brain. So here we have primary cultured human microglia, right, that's taken over 24 hours. And you can see these actually -- this is taken with label-free microscopy, so these are not really [inaudible] cells. And you can see they have all sorts of interesting interactions and interesting behaviors, and that's what we want to study. So Michael has developed a really nice computational workflow, right? So where he first actually identifies the individual microglia cells to segment them, and then he would actually basically follow the individual cells around. And you can think of this as being sort of like a private eye for each of the thousands of microglia, right? The private eye's following each of them around to see, how do they interact, right? So for example, if I'm interested in this particular cell here in the red box, so this is the zoomed-in version of it, right? And then you can see this microglia has sort of quite interesting interactions. It changes over time. And perhaps, you know, even interacting with some neurons. And I can also look at its neighbor, and its neighbor has very different behavior. It seems to be -- seems like spinning around, and you can even make out some of the subcellular organelles. So these are really interesting behaviors of, you know, of microglia cells. And, you know, one of the challenges that -- as scientists, we don't really have a precise language to describe these kinds of behaviors, these kinds of dynamics, right? So our, you know, natural language -- like, English is just not precise enough or rich enough to describe these kind of behaviors. And our goal here is to see, can we actually use computer vision, right, to actually learn a new language, a new language of morphology, to capture the dynamics of these individuals cells? And Michael has extended a particular type of neural network architecture called VQ-VAE as a way to learn this language of the individual cells. So here's basically the outcome of the analysis, right? So this is -- in the middle here, I'm showing you sort of the learned representation. Just think of that as sort of a new morphology space that we learned that the algorithm has learned, right? And here we have four different microglia cells, right, so each cell would trace out a particular trajectory in this morphology space, right? So the cell's changing and moving around over time. That actually corresponds to a different trajectory. And you can see that these trajectories actually gives you a lot of quantitative information of how the cell is actually changing and behaving over time, right? So if I look at this brown cell here, you can see that it's actually changes -- because of interactions it actually changes quite a bit over time, right? And these large changes actually corresponds to these large jumps, these large steps in the morphology space. In contrast, the green cell here, right, is actually mostly stationary. It doesn't interact very much and doesn't change very much, right? So that corresponds to that cell being -- trajectory being very much localized in the morphology space, right? So this morphology space that we learned from the data is actually a really convenient way to quantify these different morphodynamic changes. So here's where it gets really interesting, right? So when we look at this morphology space across all these microglia cells, thousands of cells, we see that they're actually, broadly speaking, two large clusters in the morphology space, right? So two different distinct type of morphodynamic states. And I'm coloring them to be blue state or red state. And here I'm showing you two examples, one from each of the states, two example cells. So in the blue state, right, basically you have cells that are sort of like, you know, couch potatoes. They're a bit large. They don't move very much. And when they move, they're quite slow, right? In the red state, you have things that are much more active, much more active microglia, right? They're smaller, denser, and they also have more interactions. And because of our algorithm, it's actually very easy to quantify these different morphodynamic features, right? So we do see that, you know, as cells in the blue state, right, they are much slower overall. And when they move, they seem to be moved mostly aligned in parallel with the long axes of a cell, right? Whereas, in contrast, cells in the red state, right, they are moving much more quickly. And when they move, they're moving more sort of in random directions, sort of like a Brownian motion. So these are I think two quite distinct morphodynamic states. And so it turned out that when we actually do single-cell RNA sequencing of these microglia, in the gene expression space, in the transcriptome space, there are also two large transcriptome clusters, right? So I color-coded them to be blue and red to sort of suggest this correspondence between the transcriptome and the morphodynamic states, right? And we can actually experimentally validate that correspondence by making systematic perturbations. So what that means is that we can, you know, take these cells so we can actually hit them with various stimulation, for example, in this case Interferon-beta, right, and we see that when you have this stimuli -- when you have this perturbation in the environment, right, so the cells mostly collapse, migrate, and change their behavior into this blue state. That's in the morphology space. And in the gene expression space, right, so most of the cells also their gene expression has changed and transitioned into this particular subcluster, right? So by making these sort of orthogonal perturbations, we're able to link the specific clusters in the morphology space, specific clusters in the transcriptome space. And I think this is actually really a promising interesting way to study how to -- really to link and study how expression changes corresponds to changes in the dynamics of these cells. Okay, cool. So just taking a step back now, right? So I think, you know, the takeaway message, in some sense, from the presentation is that it's really the following, right? So, oftentimes we have a lot of different diverse problems, right, in biomedical applications. And oftentimes, if I can somehow take my challenging problem and frame it or reframe it as a computer vision problem, then that can oftentimes be a good approach to try to solve those problems, right? And the reason is that, you know, if I can reframe it as a computer vision problem, then we have a lot of fairly advanced and mature technologies in computer vision that we can immediately apply and to adapt to solve these problems. So I think it's generally a good sort of meta-strategy for problem solving. And I think this also works well, in my experience, even when we are faced with problems that, initially, it looks like it has nothing to do with computer vision, right? So here's a project we've worked on for the last couple of years where we're trying to develop a [inaudible] algorithm to predict CRISPR repair outcomes, right? So it's a very important problem. You want to identify, where are the safe targets in the human genome to make CRISPR genome edits? And then to do that, you have to identify, "If I make a cut here, right, what are the likely insertions and deletions that are likely to be introduced due to CRISPR genome editing?" So this seems like a fairly standard, you know, just a genomics problem. So there's nothing about vision here. But what we realized is that there's actually a lot of structure in the DNA sequence, right, this DNA sequence near the target cut site that's quite useful and quite relevant for predicting different types of insertions and deletions at the cut site. And actually, a very nice way to capture those structures, by basically looking at these sort of self-alignment maps. Here is basically just the sequence, the DNA sequence near the target cut site. I can simply alight it against itself, right? So a dark square corresponds to where there's a match of the nucleotide, a white square if there's a mismatch, right? So you can see there these diagonal elements, stripes, and those stripes actually corresponds to microhomologies, which are really important structural features in the genome. And if I zoom out a bit, right, across, you know, 50 to 100 base pairs, then I get this self-alignment map which corresponds to like a really nice, almost like a Prussian carpet, right? So you see all these interesting patterns that corresponds to these intricate microhomologies across the genome. And this is actually where it becomes very much like a vision problem, right? Because then I can have a computational neural network -- basically, it just scans across this carpet-like patterns. And then to predict which of these microhomology patterns are likely to generate different types of insertions and deletions, right? So now we're in sort of a good -- now we can plot out the computer vision algorithms, right? So we have a particular tool here called SPROUT, right? So if you're interested, -- it is an online tool for predicting and for optimizing CRISPR designs. Okay, so hopefully, through these different examples, that gives you a flavor of how we can use computer vision to, you know, to do deep phenotyping and also to integrate with spatial information also temporal information and integrate those morphology changes with genomics. Now, one of the questions that came up before and a question that people often are interested in is, how can we -- you know, here we have these pretty vision algorithms, which are really quite big neural networks. How can we really understand what these neural networks are doing, right? Can we actually use that to do interpretations or to learn interesting science? So a lot of the work in my group has also been developing new technologies to really understand, what are these computer vision neural networks are doing? So I want to give you an example of this, right? So in this example, let's say you're trying to build a neural network, a computer vision algorithm, to predict active volcanoes, right? So it takes this input image, passes through this neural network, and predicts a volcano. So here we have one of the most commonly used type of neural networks. It's a particular architecture called Inception-V3, but we can also apply this to any other architectures. So for this particular architecture, right, it will have over 20,000 artificial neurons within this network. So it's quite a big model. But we have developed the algorithm that we call Neuron Shapley value that tells us that even though there are 20,000 neurons in this network, only a small number of them, about 15 of these neurons, are actually important, are critically important for the -- [ Video Freezes ] -- neural network algorithm. We apply the Shapley value algorithm, the Neuron Shapley algorithm, and we can identify quickly these 15 most valuable, most important neurons. And the reason why we know they're important is that if I remove the neurons that have the highest Shapley values, right, if I remove 15 of these, I see that the performance, the recall of my algorithm has completely collapsed, right? Where, in contrast, if I just randomly remove neurons -- that's the red curve here. If I randomly remove neurons, then nothing really happens to the algorithm because these are just, you know, 13, you know, 15, 30 random neurons out of 20,000, right? So it doesn't really change the algorithm very much. So I think this is actually really interesting, and I think quite beneficial, right? Because now we know that there are just a small number, about 15 really important neurons in this trained network. And to understand what this network is doing, I don't have to understand this whole beast of 20,000 neurons. I just have to understand what these 15 are doing. And that actually turned out to be quite straightforward. So we're going to do this exercise together, right? So here I'm showing you the single most important neuron based on this actually real experiment. The single most important neuron, the one that has the highest Neuron-Shapley value that we estimated, right, for this experiment is in the sixth layer, right, is this particular neuron here. And one thing we can visualize is to see, what are the natural images that leads to the highest activation of this important neuron, right? So I'm here showing you three images. I gave you the highest activation. Would anyone like to guess or have any guesses at what this neuron's looking at? Okay, good. So white cloudy stuff, right, exactly. So it seems like this neuron's basically just looking for white puffs, right? The white puff could be mashed potato. It could be clouds, water fountains. But it's looking for white puffs. And similarly, the 2nd most important neuron is on the next layer, on the seventh layer, right? And we can also show, what are the images that seems to be mostly activating that neuron, right? And here, if we do some interpretation, we can also see that "Oh, this neuron seems to be mostly looking for sort of like green mountains, right, or green triangles." And that's effectively what this whole network's doing, right? After training, the network basically has the one neuron that's looking for white puffs, and one neuron that's looking for these green mountains. If it finds both of these objects, then it will have high confidence that this is sort of, like a active volcano. And we know that these two neurons -- the puff detector and the green mountain detector -- are unique because if we delete either of them, right, so the whole network will stop working. Now, we can do this, you know, because there's just a handful of these that we can just systematically interpret and to evaluate, and then we'll have a good way to see what is the -- what are these networks doing? And I think that's really the key idea here, is that it seems like after you train these deep networks, right? So after training, they often have only a few, a small number of critical neurons. And we have an algorithm now based on this Neuron Shapley value to quickly identify these critical neurons. And we can then interpret these critical neurons. And I think that's actually very good news for people who are interested in a good understanding of what these networks are doing, right? Because then we can just systematically look at these critical neurons -- and there are only 10, 15 of these, right? I can just do that in half an hour, and now I have a pretty good understanding of what this entire network's doing. Okay, so I think I will wrap up here, and then we can have more time for discussions and questions. So here -- so all of the works that we've listed throughout the presentation are available on our website, and I've shared the references for each of these works. We also have the code and data, so please feel free to, you know, to play around with that. And I also just want to acknowledge again the terrific students that led each of these projects. So I'm happy to stop here and then we can take some more additional questions.  Thank you so much, Dr. Zou. Very, very fascinating and outstanding presentation. I'm sure there will be questions following up in a minute. At this time, I think what we'll do is to ask those of you who are on the WebEx, please indicate with a raised hand on the WebEx on the dashboard so we can unmute you so you can ask the questions.  Okay, and I will start with some of the questions that are in chat first.  Okay, great, since I can't see it, that would be great. [laughs] Please go ahead.  Okay, cool. So, yeah, so [inaudible] asks the question of: "How are the most important neurons selected?" Yes, so I didn't go into detail with that. So that's based on a new algorithm that we've developed, and it's called Neuronal Shapley algorithm, described in this upcoming "NeurIPS" paper. Basically, what the algorithm is doing is it computes this score called the Shapley value for each of the individual neurons, right? And the Shapley value intuitively corresponds to, how much does that individual neuron contribute to the network if I add it or if I delete it from different subsets of the network? All right, so it's kind of a very nice important score, and we have a very efficient way now to actually estimate this Neuron Shapley value that can be scaled to these very large neural networks. So that's how we identify the most important neuron.  So, James, a quick question. I don't know if it is actually related to what you just answered. But there's been a lot of, you know, issues in terms of making sure to have enough data to do your training model. Do you think with this Shapley that you were just discussing, you know, describing, we can sort of eliminate that limitation that everyone is being, you know, sort of complaining or actually worrying about the fact that, you know, you have to have so much data to be able to train your model to be able to, you know, have a decent or an accurate estimation or prediction?  Yeah, so that's a great question. One thing this Neuron Shapley approach tells us is that after you train the model, right, after you have trained the algorithm on your data, there's oftentimes only a small number of neurons that are actually important. So many of the other neurons are actually less important so you can actually prune them away.  Right.  So this actually gives us a way to, you know, quickly reduce the size of the training network --  Right.  -- to make it smaller. It doesn't directly address the ultimate challenge that you're mentioning of, how do we train this network if [inaudible] in the first place? But it does show us that, you know, after we train it, we can make that work more efficient and smaller, which also makes it more robust and generalized better to other domains.  Correct, thanks. All right, are there any other questions?  Right, so Sameer has a good question in chat about: "What is the dependance of these key neurons on other neurons? So in other words, is there a hierarchy of significant neurons? So that's actually a good question. So they do have a dependence, right? So for example, we do find that actually it seems like a lot of the most important neurons are not in the beginning of the network and they're not in the end of the network, but they're actually in these intermediate layers of the network, right? So in this example, the top two most important neurons are in the middle sixth and seventh layer. And actually, most of the other important neurons are also in these intermediate layers. And they seem to have a very sparse dependency on each other, right? So they don't depend too much on the other non-important neurons, but they do depend on each other. Spencer asks that the code, do we have the code as well as the data sets? So, yes. Yeah, so for each of these projects, right, so we have released both the code as well as the data sets.  And I wanted to actually ask you sort of a philosophical question. You mentioned about the -- when you showed the echocardio video and you were looking at some incredibly important problem that we're facing is reproducibility. And if you can imagine that, you know, whether you're using a different instrument, whether it's a CT scan or echocardiogram, in your case, it was 96 to 97% reproducibility, which is outstanding. Are there any suggestions for us? Because in the past we have seen somewhere in the range of 50 to 60%, if we're lucky enough. But to be able to get to that very high level of reproducibility, what are your suggestions? What do you think we should be -- or what did you guys do to make it that incredibly precise?  Yeah, that's a great question on it. I agree that it's a very important question, especially when we want to deploy these AI algorithms actually in medical practice. So there's a couple things that we did to make sure the algorithms are really robust and reliable, right? So first, I think, just on a evaluation perspective, it is really important to evaluate the algorithms on external data, right, so not just the held-out data from the same hospital where I trained the algorithm, but on ideally data from a different hospital introducing an entirely different workflow, right? And I think that's really how we want to rigorously evaluate the algorithm. And the second part is that we have done actually a lot of data augmentation, which we found to be quite useful in proving the generalization and reproducibility of these algorithms, right? These data augmentation techniques would generate, as we saw in some examples, generate sort of noisier versions of an image or of a video. So we can actually take different chunks of the videos, right? I think that actually is quite effective both as a way to, you know, to enlarge my training data set, but also as a way to make the algorithms more robust.  Excellent. All right, folks, are there any other questions?  There is actually one. It says: "Can you comment on the throughput of the process of imaging and analysis?"  Yeah. Yeah, that's a good question. So one of the -- so overall, these algorithms are very efficient, right? For example, for the evaluation of the echocardiogram, you can evaluate this whole video in less than a second, right? Which is definitely, you know, much faster than the sort of, like, the 30 minutes or so it takes generally to assess that ejection fraction from all these videos. I think that the main -- maybe the main bottleneck going -- and similarly was the analysis of the histopathology I think is also very efficient. I think the main bottleneck there is just the size of these images, right? So, oftentimes the images are quite large. If they're like 10,000 pixels by 10,000, then I think maybe there's just a little bit of work in even just storing and uploading these kind of images. But the uploads in themselves I think are overall are getting to be quite efficient and quite fast.  All right, it is 12 o'clock. I think if there are no other questions or comments, we can go ahead and -- we hope that you can join us for our next presentation on Wednesday, December 2 when Dr. Nicolas Fernandez from Vizgen will present "Exploring High-Dimensional Biological Data with Clustergrammer2." Once again, I'd like to thank you all for joining, and especially I want to give a special thanks to Dr. Zou. James, thanks you so much for your inspiring and wonderful presentation.  Thank you. And it's very nice to meet everyone. Thanks for the great questions and comments.  Take care. 