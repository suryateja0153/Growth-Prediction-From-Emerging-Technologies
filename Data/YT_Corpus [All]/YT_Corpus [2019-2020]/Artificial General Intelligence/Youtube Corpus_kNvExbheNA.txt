 Hello and welcome to session 196 in the AISafety.com Reading Group. Tonight we’ll be discussing the first half of the podcast “Ben Garfinkel on Scrutinizing Classic AI Risk Arguments” by Ben Garfinkel, of course, together with Howie Lempel, Robert Wiblin, and Keiran Harris. Ben Garfinkel is a research fellow at the Future of Humanity Institute and at 80,000 hours, we have Howie Lempel asking the actual questions, as well as Robert Wiblin and Keiran Harris supporting. This is a podcast, and this link here also includes the transcription, transcript, and published on the 9th of July, recorded about a year ago. So, some new developments like GPT-3 are obviously not covered. We’ll be discussing the first half, up to the 1:43:11 mark, mostly starting at the 50 minute mark. One thing I should point out is that it is not precisely defined what constitutes the classic AI risk arguments. So I’ve chosen to mostly define that as the “AI foom debate”, as well as the book Superintelligence. But whether that’s entirely correct is a bit unclear. You could also argue that the classical period was before 2015. I’d really like to say as the first thing: A big thank you to Ben Garfinkel for making this podcast and doing the work he’s doing - trying to really look into if the arguments that we have for AI Safety are sound. I really strongly appreciate that. Also it is 1 hour and 43 minutes of podcast, so when I summarize this I have to do it by removing qualifiers, so if Ben Garfinkel says something like, “Arguably, A - B”, then I’ll summarize it as just “Ben Garfinkel claims A - B”, and I’ll give a counter-example, and I won’t even include my qualifiers. So I’ll do that throughout, and that means this is kind of a straw-mannish presentation: Not of Garfinkel is actually saying, but what someone could say if they were making stronger statements. Another thing is that Robert Wiblin claims that Ben has probably scrutinized classic AI risk arguments as carefully as almost everyone else in the world. One problem that I had when I made this presentation is that podcasts aren’t really the best medium for having a text which you need to engage very, very deeply with. A formal article can often be much better. And there were sometimes some sentences that I wasn’t able to parse, and that is of course also problematic. There is some substantial chance that some of my objections here are based on misunderstandings. I think Ben will agree that it’s very very important that people write this thing down, and if Rob says that Ben is the one person in the world who is best positioned to do so, then I would really, really appreciate it if this could be written in a more formal way, in a precise way. Right, let’s get on with it. There is a general thought I have with this and it’s related to counter-factual value of the classic AI risk arguments. So we ended the previous talk, Andrew Critch’s article, with this quote: “The most useful form of pessimism is one that renders its own predictions invalid by preventing them”. And here, of course, we have Nick Bostrom and Eliezer Yudkowsky making these kinds of arguments. And this has, indeed, had an effect. And I believe you can trace a direct line from Bostrom and Yudkowsky’s argument, to certainly OpenAI and quite a bit of DeepMind’s work. And I think a lot of the insights we have into where is the state-of-the-art of the AI right now depends on these two organizations. And, in particular, our epistemic state is also strongly influenced by organizations like LessWrong, Future of Humanity Institute, MIRI, and their work. So counterfactually if we didn’t have that we would be in a situation where we had much much less information publicly available. And that would of course make things seem much less smooth. So in that way, there are two questions being mixed up somewhat here: Were the arguments correct when they were first written down, first made? And are the arguments still relevant? Because now we are actually doing something about AI safety, and do the original arguments still hold? And of course that is what I’m mostly gonna focus mostly on here. The podcast starts with a description of AI and AI risk, and why effective altruists should want to focus on this. And it’s quite good, I really like it actually, and I think it makes a great case that AI is important, neglected, and tractable. And he also seems very positive on AI safety more broadly. He points out that the case for AI safety has been broadened recently by some new arguments about political instability and lock-in scenarios. I don’t think we… I think we can find them in the book Superintelligence but they’re being given low priority there. And it certainly seems much lower than what the effective altruism gives them right now. Ben Garfinkel however returns to the classic arguments and the importance of understanding those, and really thinking hard against the weak points in those. In particular among the classic arguments, it’s the Bostrom/Yudkowsky arguments that’s been written down in a very very formal way, and those should probably be prioritized in a way. And that’s something I really, really very strongly agree with. I believe that figuring out… this kind of skepticism is something that I’ve engaged very very much with and very very deep with. So one of the objections that people come up with is one called “painting a concrete risk scenario”. And this is something that’s not being done very much. And one thing that I noted here was when I linked to the transcript on Facebook, and then Facebook added a picture to that, that wasn’t actually in the transcript. So I looked where that came from. It’s a meta tag, and unfortunately that indeed includes a very concrete risk scenario - A dystopian vision of how we would end up if we actually lose. To me that was NSFL - not safe for life. But I might be more emotional than most other people with that regard. So the argument here is that the descriptions of AI risk don’t really seem rooted in reality. And I think most of them are not. I think Life 3.0 actually contains something that is quite detailed, and, you know, seems quite grounded. And this, the argument goes, is not really true of pandemics or climate change. And that, I’m not really sure. The descriptions of existential risk from global warming in particular, the ones that I’ve seen, don’t really seem to be very grounded in reality, and not very fleshed out at all, compared to the picture of, like, a modest increase in global warming. So this seems like an odd thing, right? We have an existential risk which you can’t really describe in concrete terms. And, that seems odd. But we do actually have theoretical reasons why we should be unable to predict how this would go. And this is related to The Singularity and the problems of figuring out what someone who is smarter than yourself would do. Some of the existing skepticism have a different focus, that the original arguments don’t engage with how AI research is today. Because we’ve had the deep learning revolution from 2012 onwards, and AI was very different in 2008 during the AI foom debate. And so, I think the book Superintelligence suffers a lot less from this than the AI Foom Debate, partly because it’s newer. It seems moderately agnostic towards what methods could be used to achieve AGI, and he does... Bostrom is quite clear that Machine Learning is the most likely path to AGI. And so the thing we have mostly seen is that Machine Learning has improved far more than people expected. And whether the other approaches toward AI are stagnating or if they are also improving towards AGI, I think that’s a good question and really hard to say, because all the focus is on Machine Learning, because things are moving really fast there. Ben Garfinkel makes the following claim: “I believe reinforcement learning techniques only get about two paragraphs in the book Superintelligence”, and so I looked that up. And I think this is kind of an example of the problem that I have with the fact that this is a podcast. Because if this was not a podcast where Ben has to make things up as he go but, you know, if this was an article, then of course Ben would have looked this up and, you know, CTRL+F’d the document, and found over 26 places where it says the word reinforcement, and he would see that there is a subsections called reinforcement learning on the book, one page with two paragraphs, but after that there is a lot more about how to use reinforcement learning techniques for value learning. And it goes into quite a bit of details actually. And I think this here is the formula that Bostrom ends up with in the book Superintelligence. And I think it’s fair to say that if Bostrom had gone substantially deeper into reinforcement learning theory than this, he would have lost, well, more than 99 percent of all readers, really. I don’t think it’s reasonable to expect Bostrom to go deeper into reinforcement learning than this level, basically. So apart from that, there is more to machine learning than reinforcement learning, so his history of AI emphasizes neural networks, and if you think, neural networks and reinforcement learning are, you know, recently related techniques, then I think it’s not really far away from how you would write it in 2020. But of course, yeah, machine learning really took off in 2012, and I think the book was finished in fall 2013, so he just got the very start of a revolution there. And this might not really matter a lot because the problem is that, sure, it doesn’t engage that much with how much is actually machine learning. But this is an argument that never actually cashes out into anything. So you can’t say or use this as a reason for why other things in the book Superintelligence are not true. And Ben Garfinkel is, of course, realizes that this is not a very good argument. But he’s still sympathetic to people who react dismissively to AI Safety arguments, in spite of the fact that the arguments don’t really cash out into anything. I’m substantially less sympathetic, right? If you have an argument that doesn’t cash out into anything then, I mean, then it’s just a poor argument, and if the people who are working with AI are aware of this criticism, and don’t really engage with it, I think it’s something you could criticise quite strongly. Another potential problem is, one of the analogies for, certainly, the intelligence explosion is the evolution process, in particular the evolution of hominids. This is something that hasn’t been written very thoroughly yet. Ben says he’s not aware of any more than a page long piece of writing that uses this analogy to try argue for this discontinuity. So the most detailed piece about this is written in AI foom debate. I tried to count the number of times wherever they said “chimp” and “hominid” and “evolution”, and this is written in like, many, many places, but in a somewhat verbose and informal sense, trying to use this as an analogy, and “thorough” is not the… the AI foom debate is quite meandering. It’s also something that’s discussed over 4 pages in the book Superintelligence, and this book also contains references to other people who have been working with this. I think more importantly, the book Superintelligence argues that this is a rather weak analogy, and probably you can’t use that for very much. And this is the reason why I and many other people… We are busy people, right, we don’t really have the time to put a lot of work into something that we don’t expect will lead to anything. And that’s something that I think should... the podcast is… would be nice to have more clarity here, that the evolution argument is actually not really in any way central to why we could fear an intelligence explosion. Another analogy that hasn’t really been explored according to Ben Garfinkel, is how much compute does the human brain use. Like, could we get timelines by looking how much would it take to emulate a human or something like that. And compare that with how much compute do we have right now. And this is something that, I would say, actually a lot of people have explored this really well, notably Robin Hanson in the book The Age of Em has written a lot about this. And, unfortunately from this, I’m not really very happy because sure, if we can get some lower bounds, lower bounds would be worthwhile, but they seem really, really weak. And I don’t think this analogy is actually going to be very useful. Ben Garfinkel says that maybe the fact that the arguments haven’t been written down is something that caused him to disregard them too much. And actually, no, I don’t think that. I think it’s quite fair to not value arguments that haven’t been written down as carefully as, for instance, the book Superintelligence. But the book Superintelligence in fact has been written down in a way that is certainly sufficient. And this is why I believe that this is the key piece of writing we should focus on, maybe focus even less on the AI Foom Debate for instance. To get into one of the main points that Ben Garfinkel makes against the classic AI safety arguments, and that is what’s called Brain in a Box scenario, which is a specific AI development scenario that is purported to be implicitly implied in the classic AI risk arguments. And as you might be able to see from the screen, I think we’re at a straw man fallacy here, in that this argument is actually not one that is central at all, almost not mentioned. And I had to dig deep into the old sources of the AI foom debate to try to figure out why this would be relevant, and if anyone is actually using this. Brain in the box scenario is that there is a time where we have some narrow AI progress, roughly like what we see now but nothing that really transforms the world, and then relatively abruptly we have one AI system that is very cognitively similar to a human, to a brain. And from that, we get an intelligence explosion. That is my understanding of the Brain in the box scenario. But I might be wrong here, and Ben Garfinkel describes it with roughly with these words, but he doesn’t make a reference to anything, he can’t do that since it’s a podcast and not an article. So I tried to see where I could find that, and I tried to google for it, and the best thing I could find is the Foom debate. Where after the actual blog post, there was an in-person debate where Eliezer Yudkowsky described this, and he uses quite different words, for the brain in a box scenario. There’s nothing about it that would take a day or a month, it just takes a while, and the thing this Brain in a box can do is reprogram itself, and whether it’s cognitively similar to a human is not mentioned at all. Similarly, Nick Bostorm has this vision of an Intelligence Explosion and talks a lot about continuous improvement. It is not abrupt… this intelligence explosion. And the thing the AI system is doing is improving itself. It’s not discussed at all whether it does anything else. One thing however that will be discussed later is the Concept of Deception, that one thing the AI system might do is to start to conceal its abilities. But apart from this, “very cognitively similar to a human” is not something that is described at all. So if I look, Eliezer Yudkowsky is using this brain in a box but Nick Bostrom is not. So you could argue this if you put the emphasis on BRAIN in a box, then it sounds like the focus is on that it is like a human brain, and I think another way to state this is to focus that it’s IN A BOX, so if you put the emphasis there, then it’s not just the mathematical object, it’s scalable and you have two boxes, things like that. And I think the second interpretation of the words brain in a box as is the one that are used by Eliezer Yudkowsky. Is this discussed in Superintelligence? Well, not really. That’s of course problematic if you are scrutinizing the classic AI risk arguments, that it’s not included in the classic AI risk arguments. So there is nothing here that address the relative plausibility of something like the brain in a box scenario, compared to something that is more smooth, or present an argument like why you would expect something like a brain in a box scenario. So part of it is clearly wrong because chapter 4 of the book Superintelligence is called the Kinetics of an Intelligence Explosion, and that is indeed precisely this. So the way that a single AI system undergoes an intelligence explosion is indeed described in a very, very, very great detail here. So I think that there is some kind of misunderstanding here, and Ben Garfinkel actually means something slightly different. So if I should take a guess, one of the things that Ben Garfinkel might be putting a lot of weight here is whether what is sometimes called a “seed AI”, that is able to improve itself but not able to do very much else is really able to do other things than that. Bostrom in the book Superintelligence, he doesn’t describe this, he doesn’t really care about this. But whether that might be…, I think right now, with GPT-2 for instance, it seems like these abilities actually are correlated to such an extent that it’s quite reasonable to expect that it might be able to do poetry, if it’s capable of writing computer code. It’s also possible that Ben Garfinke doesn’t mean this, but is talking about an AI system even earlier than this. So before it can self-improve. In this case, he’s talking about this very early stage, that’s something that’s described in the earlier chapters, You can find some of this in chapter 2.1 and parts of chapter 3, but I’m really guessing here so I’m quite unsure precisely what Ben Garfinkel means. Howie Lempel actually tries to put things back on track, asking the question: Assume that among the things that these narrow AI’s are good at doing, one of them is programming AI, and so you end up with that leaping to human level AGI and then take of from here. So trying not to focus on the very broad, cognitive things that a human can do, into just the task of programming AI. And unfortunately, Ben Garfinkel dodges the question and instead talks about that if you’re trying to do science, then there are actually a lot of different tasks in this instead of a single task. For instance you need to create new hardware, well, if you need to do that physically then you need a very long list of skills. And that is undoubtedly true. But the thing Bostrom is worried about, and Howie Lempel is asking about, is the “simple” task of actually programming the AI, in particular, improving the AI itself. There is some talk about feedback around the human levels, whether the AI can outweigh the contributions to AI progress for all the other AI systems. Again this is a very very broad frame, like AI progress is much broader than just improving a single program. Ben Garfinkel believes that if it’s able to do that, something interesting must have happened before. But if that does happen, then the risk is indeed substantial. I guess I could make the intuitive argument here that I can’t prove, I think, but: Just about every program in the world can be improved with moderate effort. And from that reason, I believe that with moderate effort compared to the amount of work that went to actually creating the program. From this claim it seems quite clear that it is something we should expect that the AI itself will be a program that can be improved. Another vision of the future is the Comprehensive AI Services model by Eric Drexler, where we see capability increase without increase in generality. And there may be really strong arguments that specialization may be favored over generality. And we might be able to see that in AI. In a different world this might be something that we see where something like GPT-3 doesn’t happen, but apparently when you make something that can predict text like GPT-3, then apparently it can do both poetry and SQL statements. And whether we’re talking about something that is really general or specialized, in this case what we really care about is the ability to improve one particular software program, and that is something that doesn’t really require a lot of generality. Another thing that would be different in the comprehensive AI services scenario is that we will build narrower systems because of safety. The book Superintelligence doesn’t really assume that the person who is building the seed AI that is undergoing the intelligence explosion really cares a lot about safety. So is it something that is likely to happen? It remains to be seen. Ben Garfinkel probably believes that, seems to believe that it is more likely we’ll have something very weird, a mix of things. And I think that trying to predict the future is very hard, and the future is going to be weird in general, but we don’t really care about the future in general. We care about whether this particular AI, the first one which is capable of making an intelligence explosion will actually do that. Another scenario is called the smooth expansion scenario, which is not… it’s hard to figure out precisely how much weight Ben Garfinkel places in this. But that’s where we slowly see an increase in how many relevant tasks the AI can do and how general the AIs are, what time-horizons they are working at, how independent are they. Once you see the first system that can do everything a human can do, which is basically the brain in a box scenario, then maybe they are already better at most things. That might be true, but in particular we care about the 6 cognitive superpowers in the book Superintelligence. Those are the ones that are strategically relevant. And the others are mostly not relevant. Of course in particular, very very concretely we care about the AI improving itself. Ben Garfinkel here has a statement that says: the first system that can do everything a human can do might be preceded by superintelligent systems, and that’s kind of, just wrong by definition of superintelligence. If we are in a world where AI development is more smooth, then we might have a lot of other… yeah, this is a direct quote from transcript and I’m not 100% that I understand correctly, but if we are in this smooth world, where AI is improving gradually, than people are not so likely to be caught off guard because we can do work ahead of time, we can build institutions, we will know about specific safety issues, in particular because we might have seen some of them before, things like specification gaming. We’re seeing that already and we might see more of those. I think specification gaming is probably quite likely something we’re gonna see more of, but in particular we’re caring about the problem that’s called the treacherous turn. I think Ben Garfinkel would return to this in the second half. But I’ll just quickly say here that finding low level versions of the treacherous turn... that seems really difficult to have that happen before we get software that is capable of, you know, improving a particular software program. And so, there is some more discussion about whether the capability improvement will be smooth in this way, and I believe that it could be smooth. But this conception of that the AI should try to hide its own intentions, that might be a candidate for a strong discontinuity in the safety landscape. Another model that is implied in the classic AI risk arguments is the race between capability and alignment. The argument goes something like this according to Ben: And again, there are some of the… some of the sentences from the transcript that just doesn’t make sense, so again, it’s possible that I’m misunderstanding this. But this model have a steady creep of AI capability increase year by year, and I think this is strongly not what the classic AI risk arguments say because in those, AI capability doesn’t increase quite gradually, it is by that we have an intelligence explosion. At the same time, the capability/alignment race model has the AI goal progress, the alignment basically, happening in a much more uncertain fashion. And this creates some kind of deadline in that we get capability before we get alignment, then bad things happen. And we need to figure out what goals should the AI have, before we have, as Ben calls it, “extremely intelligent AI”. And actually, as I read the classical AI risk arguments, we don’t really care about the point, extremely intelligent AI. We care about the point where the AGI is able to self-improve. Still however, this deadline metaphor is one that is commonly used, it’s just we have a lot more uncertainty about how fast the AI capabilities will increase. The deadline metaphor has a lot more uncertainty in the classic AI risk arguments. Now for one of the key points: The entanglement between the capabilities and goals. Ben says: It’d be hard to argue against the idea that there’s a deep entanglement between advancing of goals and making AI act in a way we’d intuitively regard as intelligent. And, no, that would actually be really trivial to argue against, because if we think about places where we see capability improve, we are thinking about things where we have a benchmark, like chess, for instance. The goal in chess is to win, and in 1950 the goal in chess was to win, and in 2020 the goal of chess AIs is to win this game of chess, right? And if we have benchmarks, something like ELO ratings, then these benchmarks often imply that the goal is fixed and if we have something... we also talk about capability improvements like, say, image recognition or, you know, all these kind of games that are commonly talked about when we talk about that AI improves, then in all these examples there is not any entanglement at all between the goals being advanced and the capability of the AI. They are completely disjoint. And even if we take some more general things like self driving cars for instance, right, Tesla probably when they are programming their self driving cars, then they probably do something, likely do something like, let’s say, cars should do like the average human would do - except outliers, and in particular except the outliers where the car crash into things. And they probably do something like, you know, a bit more complex than that. But the real challenge of self driving cars is in the capabilities. The real challenge is to have a model about what’s going on around the car, to have an actual robust model of that. Ben claims that making something more capable and the after project of making it have the right goals often seem really deeply intertwined. And it’s not, like, it’s two separate goals. So selecting the right goals is an alignment problem, essentially. To me, these are quite different. If you try to create an AI, work on the goals of an AI, then you try to say “work on this, and this, and avoid this” etc. where the alignment problem works in a much more indirect way where we want the AI to, you know, work where you have a pointer to the humans, the masters of the AI, and say it it should do what we want, and try to specify that in a robust way. And this is quite different. And I’ll argue for the disentanglement in two parts: the first is that even if we have a lot of progress in alignment, then that won’t help in capabilities. We won’t get AGI just because someone comes up with the perfect solution for the AI alignment problem. Because if you take an AI right now, something that’s implemented in Python or whatever, and you say, “Oh! It should do what I want!” Then the AI might try doing that but it will obviously fail because we don’t have the capability to make an AI that can look at a human and figure out what it is that we want. And a lot of the techniques that are used in AGI, like “AI safety by debate”, if you try to implement them with current AI capability methods then that’s obviously going to fail, because we don’t have AGI yet. The other example that Ben uses is with a house cleaning robot. Programming a house cleaning robot is pretty hard right now, because it’s hard to specify a goal. And I will argue that, no, that’s actually not the real reason why it is hard. But first let’s talk about narrow and general AI, because this is something that the classic AI Risk arguments stress. And cleaning a house: is that something that can be done by a narrow AI or general AI? One classic test of whether something is an AGI is the coffee test by Steve Wozniacki, whether you can go to a random house and make a cup of coffee, and it seems strictly easier than actually going into a house and cleaning it. So this house cleaning robot is probably something we’ll only have once we have AGI. And that’s of course the reason why we don’t have cleaning robots right now. Not in particular because we can’t specify the goals, but because we don’t have the, we can’t make it do what we want. Ben Garfinkel’s vision of this house cleaning robot problem is we could do it with some kind of reinforcement learning where we hand-code the reward function, but if we make the reward function too simple we get some side effects, it will ignore things that are not there. And it’s really hard to capture everything we want in a reward function. I actually believe it is going to be rather easy to do this. Not really easy, but compared to all the challenges of a housekeeping robot, I expect this will be a really minor one. You know, if you just say “dust minimization”, sure, that won’t work, so you can’t literally solve it in five seconds. But if you, like, spend the day on it, you can get much much farther. In particular, as long as the AI is narrow then this kind of hand-specification is probably going to work really well. So, the major difference that I see here is we need to just specify some approximate instrumental goals like “don’t knock down the vase”, and in the alignment problem we’re trying to precisely specify the final goals of the AI. And those things are really, really different. And, yeah. And then Ben Garfinkel makes an argument that, sure, this is a problem for AI safety but AI safety is more. Real housekeepers sometimes set the house on fire, and avoiding this kind of problem is something that is relevant for AI safety. And I strongly disagree here. I believe that this is something where, which the classical arguments for AI safety make a big point about saying that this is not what they’re about. This is not about ensuring that a self-driving car doesn’t hit a pedestrian or something like that. This is, this is not related to this at all. In Ben Garfinkel’s model there is an unhappy valley that is required in order to have a disaster. And the first is that if there is no progress on alignment, well, then we won’t use an AGI. If there is a lot of progress on alignment then we will use that AGI but it will also be great because there it will be aligned. And the unhappy valley is where we have enough progress on alignment, and can get capability, but not enough that it is actually safe. And my objection to this, I believe I’ve stated a number of times, is that the framing is much too general: improving just one specific piece of software, and we might even have a fixed benchmark. Ben Garfinkel makes the following statement here, I think it’s like, quite likely to be false, that is that: only malicious or insane actors would make an AGI pursue a narrow objective. And I think a narrow objective that can be specified really precisely is to make a profit-maximizing. You have a software program trying to… you know, make sure that you get as much money on this particular bank account while not breaking any applicable law. Or something like that. I think that is eminently plausible and we will get AGIs pursuing very very narrow objectives. And also the fact that, you say, only insane actors would do this seems unsafe. Well, I think the vast majority of AI researchers and AI users, they are not convinced about the merits of AI safety at all. And I think it’s not just insane people who don’t care about safety. A lot of really smart people are just basically… plainly not convinced of the arguments. Finally we get to the analogy of strawberries on plates. According to Ben Garfinkel, Eliezer Yudkowsky posed the following challenge: how do we get an extremely superintelligent system to move a strawberry to a plate without blowing up the world? And this kind of framing doesn’t really conduct the way that machine learning research works. I am not really sure what the word “conduct” means in this particular sentence. I tried to look up whether that’s actually something that was said. It’s not in any of these classical things. The best place I could find was Eliezer Yudkowsky’s Twitter where he has pinned something that was very similar: “Place onto this particular plate here, two strawberries identical down to the cellular but not molecular level.” Which is, I think, I think that’s quite different but other people have used this analogy, and I found someone claim that Eliezer Yudkowsky had said this, but had a dead link for the quote. So it’s quite possible that he at some point said this. But I think the key thing here is the framing doesn’t relate very much to the way standard machine learning research works. And it’s indeed on purpose because the purpose of this strawberry on plates problem is to show that instructing a superintelligence is very, very different from what we’re doing with our current machine learning. And this state of machine learning techniques cannot be assumed to help with this particular problem. That is all for the first part of Ben Garfinkel’s podcast. Thank you for watching and see you in two weeks. 