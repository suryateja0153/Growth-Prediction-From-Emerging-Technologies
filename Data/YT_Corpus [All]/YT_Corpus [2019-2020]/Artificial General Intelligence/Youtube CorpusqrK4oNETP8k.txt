 Are machines as smart or smarter than humans? Do  they have extensive world knowledge, common sense,   or the ability to reason and solve problems  like human experts? How would we even know the   answer to these questions? Stick around  to the end of this video to find out. All right, before we dive in, be sure to subscribe  to this channel if you haven't already. That'll   help you learn about and stay current on all  things artificial intelligence. So are machines as   smart as humans, and how do we evaluate that? Also,  why does it matter? Starting with why it matters,   machines that can understand language in the same  way that humans can would help make much better   virtual assistants and chatbots for example. As  a result, they'd be able to answer any of our   questions, translate dialogue between languages  on the fly, and maybe even have meaningful   conversations. Natural language techniques  such as natural language processing, or NLP,   and natural language understanding, or NLU, are  hot areas of AI right now. In fact, we interact   with these forms of AI almost daily when speaking  with our virtual assistants or allowing our email   clients like Gmail to complete our sentences.  Many state-of-the-art natural language text   models such as GPT-3 and UnifiedQA are pre-trained  on massive amounts of website and book-based text   data that covers a wide range of topics. All of  Wikipedia is included in this data for example. These models have gotten pretty good at tasks  in areas they were never trained using something   called fine-tuning and other techniques such as  zero-shot and few-shot learning - don't worry if   you don't know what that means. Anyway,  to figure out how smart these models are,   and how well they understand language, a bunch of  tests, also called benchmarks, have been developed,   each consisting of tasks like answering questions.  GLUE and SuperGLUE are two of the most common   benchmarks, and some models like GPT-3 have  achieved superhuman performance on them.   That's great, but we know from our  experience interacting with language-based   AI systems that they're nowhere near as smart in  the sense of overall language understanding as   we are. This means that benchmarks like GLUE only  measure narrow linguistic skills and understanding   as compared to overall language understanding  like humans develop over time, and apply every day.   To try to better determine how well machines  truly understand language, a paper was recently   published by a team of researchers that  proposes a new multitask benchmark, or test,   called Measuring Massive Multitask Language  Understanding. Their benchmarks tasks are designed   to be much more challenging as compared to  existing benchmarks, and to more closely resemble   how human understanding is evaluated. They're  also designed to test how well today's leading   text models can apply any knowledge gained  from pre-training. According to the paper,   the benchmark uses multiple-choice questions of  varying difficulty across almost 60 different   subject areas. Questions ranging from simple  to expert level, and that focus primarily on   humanities, social sciences, and STEM fields. To  put all of this in context, humans typically learn   through the traditional process of elementary,  secondary, and higher education. Followed by any   continuing education and professional development.  Most of this learning comes from reading,   listening, practicing, and studying. Computers  can learn about the same information, if not more   information across a broader range of subjects,  but unlike how humans learn in study, machines   are trained using machine learning techniques.  Now imagine a computer and a human taking the   same test to determine how much they learned and  understand across many different subject areas, as   well as how well they can demonstrate expert-level  reasoning and problem-solving ability by applying   knowledge gain from previous learning. In this  scenario, we'd expect a machine to do at least   as well as the human on this test in order to say  that the machine is as smart as humans. At least in   this context anyway. In the case of the massive  multitask benchmark that we're talking about,   ultimately the purpose of this test isn't so  much to compare a human to a machine directly,   but rather how different types of leading  AI text models perform on the test relative   to each other and relative to other benchmarks. This benchmark helps with exactly that. In fact,   it has exposed some significant limitations of  leading state-of-the-art models that perform   at superhuman levels on less sophisticated  benchmarks like Glue and SuperGLUE. Some of   the key takeaways are that these best-in-class  models perform very well in certain subjects,   while much less well in others, and don't seem to  learn in the same way or order that humans do.   It also appears that some of these models do  well at advanced tasks while doing poorly at   much simpler tasks such as elementary mathematics.  There are many other interesting results so you'll   want to check out the paper via the link in the  description for more details. To wrap things up,  let's revisit the questions from the beginning  of this video. We asked, are machines as smart or   smarter than humans? Do they have extensive world  knowledge, common sense, or the ability to reason   and solve problems like human experts? Despite  superhuman performance on previous benchmarks,   the Measuring Massive Multitask Language  Understanding benchmark tells us that even today's   best AI text models aren't much better than  randomly guessing answers to the questions in the   test. This means that there's still quite a ways  to go for AI to be considered as smart as humans,   especially when evaluated in ways that are similar  to humans. One last thing worth mentioning.  The current lack of human-level performance by  top AI models on this benchmark should make us   wonder whether we can ever get to human-level  performance on the benchmark, and if we can, how? Perhaps we need improved and more complex  versions of the current top models,   or perhaps huge amounts more specialized data  will do the trick, or maybe we need new and totally   different models altogether. Only time will tell.  All right, if you like this video be sure to click   the like button and share with people you know.  Thanks again, and I'll see you in the next video. 