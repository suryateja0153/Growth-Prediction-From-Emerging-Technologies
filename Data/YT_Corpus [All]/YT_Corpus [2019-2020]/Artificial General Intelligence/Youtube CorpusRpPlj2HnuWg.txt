 hope you all are enjoying the class so far it's it's always interesting we have people with a variety of backgrounds here so it's just great to kind of hear the questions as we're going along so I'm going to give a quick example to begin the class right now they kind of drive in some of the concepts that we talked about on the first class but this is kind of real rather than sort of cartoon neural networks but it's sort of a real research result that that we have before I begin I'd like to thank Emily Joe who was one of my graduate students who actually did all the work I just put some of the slides together for not even all of them just a few so really the credit for this work goes to Emily anything I misrepresent is my fault if not hers she graduated so I'll take the paper that's not interesting so with that we'll begin so all right so the overall goal of this project was really to detect and classified Network attacks from real internet traffic and in order to do this as many of you can imagine we had to find a dataset that was of interest so this is probably a problem any of you are currently thinking about which is what data set should I get my hands on right so we have a variety of data sets some are sensitive in nature right so think of internal network traffic that we're trying to collect no one's going to let us end this you know and silver to graduate students to kind of work on and then more publish so the first thing that we wanted to do was look for a dataset that was kind of open and out there and we were fortunate that there is a group in Japan called the Maui working group which stands for the measurement and use my measurement and analysis of wide area Internet traffic working group that actually has a tiny link that they've tapped using a network app and they actually make this data available it's actually continuously updated even to today so that's really cool and this is within that its thing called the day in the life of the Internet and so this has been going on for multiple years the data sets reasonably large when you convert it into what we call an analyst friendly form which is something that you are I could read and make some sense out of it's about 20 terabytes in size so reasonably large data set not going to fit on a single computer or a single node so certainly a good use case for using high performance computing their supercomputing and one additional piece that we should know this is something that as you're starting off your projects you may also think about IP addresses are often seen as reasonably sensitive right so you can see where traffic is coming and going from so in this particular data set they've actually deterministically anonymized each of the IP addresses and as you're downloading the data you have to essentially sign the user agreement saying that you will not attempt to kind of go back and figure out what the original IP addresses were so as you're coming up with the data sets and I think Jeremy's going to talk a lot more about this in the in the next part you might think about you know are there certain fields in the data that I'm collecting that might be deemed sensitive either today or in the future and if so are there just simple techniques that I could use to anonymize the data and then with a with a decent and user agreement you might be able to enforce some level of you know people not trying to break it and trying to go back it's not impossible to do it but any legitimate researcher who's using data really shouldn't care about what the original IP addresses in this particular case are or maybe other such data within whatever you're collecting all right so just a great definition of anomaly detection I really love this definition of what an outlier is from paper by Hawkins so an outlier is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism so when we're looking at network traffic that's what we're looking for right we're looking for these mechanisms that are present in network traffic that deviates so much from normal behavior that it arouses suspicion and it could be from over up for a variety of reasons it could be somebody uploaded this cool video of you know somebody falling flat on their face or it could be a botnet or a DDoS attack or something like that so outlier could be sometimes referred to as an anomaly surprise or exception and within the context of cyber networks these mechanisms can be botnets C and C or command & Conquer servers insider threats or any other network attacks such as distributed denial of service or port scan attacks there are a number of general techniques to deal with outlier detection so the first one is to look for changes is to essentially use statistics you take you look at the you know what's going on in your mechanism and you look for statistical anomalies from that and that those of you you'll highlight those anomalies and then you'll kind of drive deep into that you could do clustering so if you're looking at unsupervised learning you could cluster your data and things that are really far away from existing clusters or known clusters something that you want to take a look at you could move similar to that is also distance based technique so these are kind of closely related that you look for observations that vary significantly from other observations in some sort of a feature space and finally you can use a model-based technique where you attempt to model the behavior of normal right now you use air quotes for the word normal and then you come and you look for things that deviate from this background model so all these four techniques are approaches to a nod leaders are kind of related with each other it's not a hard and fast rule about how they deviate from each other but for the popularity and the complexity of network traffic we decided to go with a model-based technique because we found that any other means of trying to represent the data would be difficult so this is sort of a top-down approach of how we kind of thought about well let's look at network traffic let's think about anomalies in network traffic and let's come up with what's a good approach to to address that so when we talk about network attacks not to go into detail there is a wide variety of network attacks out there you know every day we see news articles about new ones the focus of this work to look at just a couple of these one was in this section that we call probing and scanning and another was in this resource usage or resource utilization area and specifically we looked at port scanning and distributed denial of service attacks as a quick example of what happens in one of these network attacks so this is a attack often in within the bucket of probing and scanning so essentially what happens is you have an attacker that attempts to find out what ports are open on a victim or a target system it does this by sending requests similar to pings to to a victim or a target system if the target system acknowledges one of these things you can say oh this particular port is open and then one may go and look at what software typically runs on those ports and attempt to use some of the known vulnerabilities of that software right so a lot of software packages have specific ports that they tend to use so you can say oh you know port the guy making up a number here for 10,000 is open and I know that's used by by Microsoft SQL Server or any other piece of software and then you can say well here's a known vulnerability that I could use and then I'll try attacking that so this is just a simple technique that a lot of attackers use very low bar very easy to do you could write one by mistake but just to find out what for so and then trying to use you know in vulnerabilities or every works now the least common denominator the the easiest piece of data to collect is what's called a network packet I'm sure many of you are familiar with the concept of a network packet it's for those that are not think of if you're sending a letter it's all the material on the envelope right so it's the address where it's coming from who it's going to plus a little bit of information of sort of what's in the packet as well the reason that we often use network packets and I'll kind of open this up to the class a lot of the cyber research actually focuses on network packets but those often form a very small percentage of the actual data the payloads we're actually a lot of the data actually is can anyone guess why we tend to use a network packets rather than payloads not Alber head or oh sorry in the network header yep sorry the network header not the packet yeah so yes anyone guess why we tend to use the header rather than the PAP then the payload of the packet encrypted yeah encryption exactly so very often especially these days the payload of the packet is typically encrypted so there's not too much that you can do with it so using the header information the header is not right it's the outside of the envelope the inside of the envelope is something that you cannot see by putting it up to the light so it actually they tend to use me the header of the packet so the headers so this is what a packet looks like on the on the left side that's what a packet looks like and if you kind of convert that into a human readable form just a header information this is the type of data that you get out of it so it gives you things that you would expect what was the IP address that I started from what was a port that I started from what was the destination IP address what was the port you know the destination port plus a bunch of other information about certain flags that were said whether it's what direction the flow in what direction the traffic is moving in and stuff like that and then any flags associated with it what type of packet it is of what type of protocol is a packet using etc all right so just as a reminder the data we're using is from the Maori working group but there's a lot of other data out there that you might be able to to get your hands on so one of the challenges how many people here have worked in cyber security or done some research and cyber security and full of people so one of the huge challenges this is something that you may run into in your in your data sets is the is Brown truth is understanding when so if you're trying to find out when there was an attack in your data someone needs to have you know there needs to be some ground truth associated with that as much as we'd all love to sit and write DDoS attacks and port scan attacks on ourselves sometimes you know getting that at scale can be very difficult so for the purpose of this work we used a synthetic data generator in which we took real Internet traffic data but then injected synthetic impacts into it that would and that was sort of our way of looking for establishing ground truth so the Maori working group actually does published ground truth based on a number of that they have a lot of these are heuristics based detectors in our in our you say now looking at that ground truth it was really difficult to actually understand when an attack was occurring you know sort of like somewhere in this hour an attack occurred which if you're trying to chain the machine learning classifier on that or an anomaly detector that's kind of vague right because there's a lot happening in an hour of Internet traffic and so if it's somewhere in that hour an attack occurred that can be very difficult to find so that's sort of a reason that we focused on using a synthetic attack generator yep pieces a little bit more about what kind of guys contracted so you know is just evident where you're joining some colorized post that it's injecting data to some destination is there also like return clothes that are in there yeah yeah so the question that was asked is you know I guess how sophisticated is this tool and how detailed can you do it so the exact details of that I would you know probably forward you to Emily but from looking at what what was being done these are reasonably sophisticated so it's not as simple as you know you have to give it a text file of all the IP addresses sourcing so but you do have to set some parameters so what you can do for example if it's a DDoS attack right that's maybe the easiest one to describe is you would give it a maybe a range of IP addresses and ports that you'd like to see injected you'd give it a flow rate and then you could tell it what type of package you'd like introduced and has it sort of allows you to do a number of these different this particular tool at least allows you to pick a number of these different features that it then injects into the original pcap file if you're looking for some of the other attacks so this is just a list of the attacks that that they currently support as of a few months ago and they might be adding to this you know we focus on a few of these that that were also easy for us to reason and kind of go back into the actual data that we collected to see if the actual injection made sense to us or not but they do have a number of different parameters that you could that you could when you're doing that so that's just a little bit you know as we talked about in the data conditioning phase of labeling data is often a pretty difficult process in the world of cyber anomaly in the world of network network packets and network security going back and trying to label is an extremely time-consuming task and very difficult and up to a lot of interpretation as to what one calls an attack so what we did manually inspect a few of these that's certainly not scalable so we decided to go with a synthetic attack generator and I'm sure some of your projects you may also kind of think about you know is that it is that a route that you need to take at least to start off with ok so this is the overall pipeline of the system so as I mentioned the first time we chatted we spent a majority of our time in sort of the first step which is the first step here which is which is data conditioning which is data conditioning so within data conditioning we did you know taking the raw data which is downloading this from the internet this comes to you in a binary format which is a dot pcap for those who are familiar with a packet capture outputs that's that binary format that comes from like TCP dump for example we then convert that pcap file into flows parse that into human readable form feature engineer training machine learning classifier do the same thing with the synthetic data and then get good results we hope so step one a beta condition was to download the actual raw data the data was downloaded in a compressed binary format which is probably what a lot of you will get your hands on typical size of a single zipped file or typical compressed files about 2.2 and 1/2 K when you decompress that it goes to about 10 gigs about 4x and a single pcap file corresponds to such about 10 gig for about 15 minutes worth of traffic so if you're trying to do multiple days you can imagine how this starts to explode in terms of volume and about 15 minutes you know on average is about 150,000 pack but it corresponds to about 150,000 packets per second so this is a reasonable amount of network traffic that we're getting our hands on but even close to the types of volume that large-scale providers have to deal with on a regular basis once we have the actual what once we have the the pcap file downloaded just the network packets don't really tell us much it's tough to do things with so what we want to do is convert this into a representation that's useful to do analysis and so a lot of work has been done using network flows and a network flow is essentially a sequence of packets from a single source to destination so as you can imagine when you're streaming a YouTube video for example it's not one big packet that comes to you but it's a series of packets and so flow essentially tries to detect that and say ok all of these things was a person watching this video rights from this source to this destination within some time window is defined as a network flow and so we convert each of these 15-minute pcap files into network flow representation using a tool called GAF which stands for yet another flow meter and so that that helps us do some of this you know flow establishing these network flows and so the size of about 15 minutes worth of flows goes to about two gigabytes and we set a timeout of flow to be about five minutes okay once we have the flows they're still in this binary format we need to now convert them into a representation that we can look at so flow comes with an ASCII converter called ASCII sorry yeah comes with a ASCII converter called yaks key and that essentially converts the data from this floor representation into a tabular form and if there's one lesson that I think Jeremy will talk to you a lot about next is tables tables tables people love looking at tabular data it's very easy to understand I wouldn't recommend it but you could open this up in Excel a huge but it would open up but it's very easy to kind of look at especially when we're trying to establish truth in the data to go back and take a look at it so far for our pipeline we take the java files converted into text files each of these ASCII table so again about 15 minutes worth of data is about 8 gigabytes in size and we record a number of different container pieces for flow we're going to the details but if you're interested you can kind of look at what constant what are the various entities or features that come up in a network flow alright so now that we have network flows that's that sort of like sort of part one of data conditioning right we've converted into some representation that makes sense we have in a human readable form sort of step one data conditioning done now the next thing is to convert it into something that actually makes sense for our machine learning algorithm and so this is sort of bucketed into the area of feature engineering when we kind of think back at it and so feature engineering is really really important you'll spend a lot of time doing this on new datasets so each of these flows contains about 21 different features and one of the end we need to look for essentially which of these features make sense for us to pass into a machine learning model others we're just going to end up training an unknown Z so we have to use some domain knowledge right where we talk to experts in cybersecurity and say well you know IP address this kind of an important thing but maybe this other flag not as important and then we use a lot of trial and error and luck as well to help make some of these features once we have these once we kind of pick a set of features that we're interested in when we look back at the anomaly detection literature a lot of work has been done with this concept of you know you have these flows but you need to convert them into some some other format that makes sense to look for a long lizard so just converting into a feature space in which Menominee's makes sense and so we looked at using entropy and the basic intuition behind this is that if we if the we're looking at various flows between different fields and that the entropy of these flows should be roughly equal should be roughly similar should be about the same not equal that should be unchanging when there is no big mechanism defined by an outlier detection mechanism that's involved with changing the entropy so for example right just to make this more clear if you're having a DDoS attack this is typically a number of different source IPM attempting to talk to a single destination IP address so you would expect to see an increase in the entropy of the source IP field in that particular example and enforced and attacks you would probably see something on the destination port entropy would go up right that's a little bit of intuition so a little bit more complicated than that but that's a very high-level view of what's going on and for entropy we just use the standard you know sure many of you are familiar with information theory are very familiar with Shannon entropy and we compute the flow associated with each of the features that we paid from the network flow before so from the feature engineering we pick to some set of features associated to that we all remember what neural networks are this is just there for completeness so we take these now network flows these these entropy associated with the network flow we use those as input features for a neural network model now you'll recall recall from the neural network talk that we had last week we have inputs and outputs we have weights associated with them and then we have this nonlinear equation that sort of represents inputs and outputs and inputs and output that each of the layers there's the equation you can see very giving me the Q that I'm walking towards the slides so the features that loops the features that we have over here correspond to the various entropies that we've computed in the previous step and the outputs of this network correspond to a class of attack so in particular we focused on you know obviously no attack DDoS attacks port scans network scans and p2p network scans the model itself had three hidden layers with 130 and 100 mils respectively and an output layer and we used for illu activation I'm having to talk in more depth about why we selected these but I'll save that for another time and kind of going through this process we actually had very good results so that's a sort of the short form of the research work but what I wanted to really emphasize is the amount of effort that went into the data conditioning the importance of you know collecting data anonymizing data making it you know human readable converting into a format that people understand and then kind of walking through this sort of it was certainly an iterative process as much as this is a very short form of what we did this this took a long time to get to these results and what we're presenting over here is the attack type and intensity so this is the ground truth from the synthetic data generator and then this is sort of the prediction that our model has across the site and this is a confusion matrix so a higher number or a darker blue indicates that we did a really good job and we've also used varying ten cities of these an attack so certainly if we have strong which means we injected about twenty thousand packets in the fifteen minutes we can we can detect those are really really well as you have weaker weaker attacks of our system doesn't do as well as probably reasonable but certainly an area of one important research so with that I just wanted to hand it over to Jeremy but these are some initial work and results of detecting across line Network attacks the keynotes was you know data conditioning was where we ended up spending a majority of the time cleaning up the collected data coming up with we struck we spent a lot of time trying to figure out how to label this data because we were just unable to really do it and finally we ended up going the synthetic generator path but you know we actually did spend a significant amount of time try to figure that out and then we spend a lot of time on feature in engineering which did consist of a lot of trial and error okay thank you okay thank you very much I'm Jeremy Konner from Lincoln Laboratory supercomputing Center most of you know me already just really introducing myself for video and this has been this topic has been teed up really well by Sid and Vijay I think they Illustrated sort of going from if we think about our pipeline going from left to right so they've talked about kind of the end state that what you're trying to do building these models Vijay talked about the data that feeds into that I'm going to talk about the data architecture that you go through to kind of build up to that and the reason we've done it in this order which is the reverse order that you do it in real life is because it's difficult to appreciate the data architecture piece without an understanding of where you're going and so you see from SIDS talk I think it became very clear that having lots of data is really important for creating these models without the data you can't you can't do anything and likewise you didn't see it but all that data was created in a very precise form that made it easy to do that kind of work and then vijay talked a little about some of things given a data set what did he have to do to get in a form such that it would work for his AI training application and what vijay is doing and did in that application is very much a sort of microcosm of which many of you will be doing in your own projects you will try to identify data sets you will try and do feature in engineering and then go through this sort of trial and error process of coming up with AI models that suits on purpose and a key part of that is what we call AI data architecture so i'm going to talk about some of the things that we've learned over the last ten years in data architecture as a head of a supercomputing Center I have the privilege of working with really everyone at MIT on all their data analysis problems and over the years we've learned some techniques some really simple techniques that can dramatically make data architecture for your applications a lot there's a lot of people out there selling very fancy software to solve data architecture problems we have found that we can use very simple techniques to solve most of those problems using software that's built into every single computer that that is made today that's don't need to go and download some new tool to do any of the things I'm going to talk about right now so with that introduction I'm going to just go over there's the outline of my talk I'm going to add some introduction and motivation which I'll go through quickly because I think Sid and Vijay did a fantastic job of motivating what we're doing and then I'm going to talk about our serve three big ideas and how to make AI data architecture simple and effective across a wide range of applications one is and as Vijay talked about we're going to really hit this concept of tabular data there's a reason why tabular data is so good and why it is such a preferred format and so you want to try and get your data into these tabular formats and the nice thing is that should that should that if that sounds really simple it's because it is really simple lots of people that want you to buy tools that make this process complicated because that's how they inject themselves in the process and we've discovered actually you can do world-class MIT quality AI org with really really simple tools getting things in a tabular starting and then how to organize the data how to organize the data in the pipeline that Vijay showed which is pretty standard across a lot of applications again there's lots of applications and tools that will try to do that for you we've just found that simple folder naming and file naming schemes get you a long way they require no tools and they make it really easy for other people to join your project because they can just look at the data and it's pretty clear what's going on and it also makes the data easy to share so that's another simple thing and then finally I want to talk about every single application we've demoed here involve datasets that were designed for sharing designing your data is such that you can give it to others actually addresses a lot of the data architecture problems that you will encounter yourself by thinking about okay how do I make this data such that I could give it to someone else you're really making it much better product for yourself if you're making it to the other people can join your team much more quickly and be much more effective and so thinking about sharing of data is really an important thing and what you'll discover is that generalized sharing of data is very hard just pointing to a data set and saying I want to share it is very challenging there's a lot of barriers to do that but if you have some idea with the end purposes with the particular application that you want to share this data for then it becomes a lot easier both technically and administrator and we will talk a little bit about some of the administrative burdens that you need to go through to share data and this is where knowing the purpose in this case we want to do particular AI application dramatically simplifies the the process of doing data shown so just some motivation this is a slide that Vijay presented earlier and the big thing to emphasize here is we have a bunch of breakthroughs and the key is the bottom row which says that the time from algorithm to breakthrough is like two decades but the time from getting data set that is a shareable well architected data set to breakthrough was much much shorter was only three years and so that's kind of motivation for the fact that why data is so important to making progress in AI the data is actually writing the program as vijay mentioned in the first lecture and so having data is really important and it has been often observed that eighty percent of the effort associated with building an AI system is data wrangling or data architecture as we call it and one of the benefits of these challenge problems that are these open and shareable datasets is they have done a lot of that data architecture and data wrangling for the community which is why they're able to have a lot of people work on it and make progress on it if you're doing a new application you have to do this data architecture data wrangling yourself and and we're going to share with you the techniques for doing that effectively and I would say in most situations this 80% of the effort actually is going to discovering the methods that we are going to cover in a few minutes that is it takes people a long time to figure out how simple tables are good we can organize our data that way and just having a good set of folder names and file names really solves most of the problems right that's the beauty of the solution is sound so phenomenally simple how could it possibly work that's actually you know if there's one thing we figured out here at MIT is that simple can be very good and very powerful so as was mentioned before sort of these neural networks as depicted here dry this space of applications most of the innovations we've seen in the past few years have been using these neural networks this one slide sort of covers all of neural networks and one slide you have the the picture you have the input features which are shown in grey dots you have the output classifications that are shown in blue dots and then you have their neural network and all their various weight matrices or weight tables in the if you had one goal in terms of understanding AI making a commitment to understanding this one slide and you pretty much understand a good fraction of the entire field and while you are here at MIT please feel free to talk to any of us about this slide or these concepts and we will work with you to help you understand it because anyone who's ever coming to you and offering an AI solution that uses neural networks which is a significant fraction them they're just doing this this is all they're doing it's all on this one slide many of them by the way do not know the math that the one equation here that sin does they have software they play with knobs but they don't even understand the underlying mathematics and just understanding this math taking the time to do it you will know more than anyone who is actually trying to sell you AI solutions now as a Sid said you talk about back sighs I just added batches to this figure to emphasize the fact that all of these letters are mostly uppercase and if you want to know one thing about matrix math is the easiest way to go from a vector equation to a matrix equation is take all the lowercase letters and make them uppercase and most of the time that actually just works in fact typically what we do if you're a linear algebraic person is we do that and see if it breaks anything and if it doesn't break anything while we assume it's right and all these matrices right are tabular a matrix it's just a big table it's a pretty big spreadsheet so basically all of neural networks and AI is just about taking spreadsheets and transforming them and so this is just motivation for why these tables are so important here is this standard pipeline Vijay talked about it briefly and again what we've discovered is that in most data analysis which includes AI this pipeline represents pretty much what most applications are doing it's not that any application does all of that but most of the steps in an application fall into this pipeline basically you have raw data you parse it usually into some tabular format you may ingest it into a database you then query that database for a subset to do analysis or you might scan the whole database if you're doing all the files if you want to do some application and involve all the data and then you analyze it and then you visualize it and so this is sort of the big steps in a pipeline that we see and again most applications we'll see will use three of these steps they're not using all of them but it's a great framework for looking at any data analysis pipeline it's easy to use it's easy to understand and it's easy to maintain and those are we you know we like all of those features and I'll get into those more data and more more detail and then finally I've already talked with significantly about data sharing those are some examples of the data we put up for sharing the moments and time challenge the graph challenge and again creating sharing quality data really it's not just a service to the community it's principally a service to yourself and in fact we've worked with a number of applications and sponsors where we help them do this and they find the data than they made shareable is instantly the most valuable data in their organization because it's shareable within their organization to the same things that make it shareable to the broader community make it shareable within your organization to other partners especially important and again as I said before Co designing the sharing and the purpose of the data is quite if you need to do that in order to do a fective sharing you can't just sort of share data arbitrarily it's not to be very difficult to do but if you have a particular application in mind sharing the data is quite simple and I'll get into that later so I'm just gonna then sort of hammer home some of these points here so nothing really new here just through repeating what I've already said about these things but with having a little bit more attachment to them so why do we like tables well we've been using tables to analyze data for thousands of years so on the right is a 12th century manuscript it's actually a table of tides and if you could read Latin and Roman numerals you already can tell that it's probably oh there's some column there's some roads and those Roman numerals or numbers even 800 years later we can still see that this is a data table and it goes back further you can find Kamiya form tablets that are thousands of years old that are clearly written in a tabular form so humans have been looking at data in a tabular form for for thousands of years part of that is to how our brain is actually wired one the 2d projections since we live in 3d space is the only projection that will show you all the data without occlusion so if you want to look at all the day that kind of has to be presented in 2d form we have a 2d optical system in addition we have special hardwired parts of our eyes the detective vertical and horizontal lines for presumably detecting the horizon or trees or who knows what and that makes it very easy for our eyes to look at tabular data and because of this tag their data is compatible with almost every data analysis software on earth if I make data analysis software they can't read tables or write out tables it's going to have a very small market share and this is just an example of some of the software out there that is designed to use tables so whether it be spreadsheets or databases for the neural network packages that we've already looked at various languages and libraries and even things that you wouldn't think are tabular like hierarchical file formats like JSON and XML can actually be made compatible with a tabular format so I'm just going to walk through these a little more detail so of course our favorite is spreadsheets here's an example of a spreadsheet on the left and what's great about it is that I have four different types of data that are very very different and you can all look at them and sort of sense of what they long and that just shows how flexible this tabular spreadsheet form is is that we can really it can handle really really diverse types of data in a way that's intuitive to us some of the biggest pieces of software that are you that the implements are Microsoft Excel Google sheets Apple numbers it's used by about a hundred million people every day and so this is the most popular tool for using data analysis and there's a good reason for it and thinking of things in a tabular format so they can be pulled into this really makes communication a lot easier and so again just from hammering home this element of tables within that there's specific formats that we found to be very popular so CSV and tsp are just not proprietary plain text ways of organizing data we highly encourage people to use them because then it can be used by any tool CSV just stands for comma separated values it's usually file name dot CSV in lower case or file name ducks use to be in uppercase and on all it means is that the columns so each each row is terminated by a newline and within each row the columns are separated by a comma that's all that means we tend to encourage people to use TSV which stands for tab-separated values because we can replace the comma with a tab and that allows us to write higher-performance readers and writers because it is not uncommon to have within a value and so it makes it very difficult to sort of write parsers that can do that fast but it's okay to say to a person generating data if you put a tab inside a value that's a file a lot you you should do that and so that means I can read some I can write a program that can read this data really fast and it can write this data really fast and that's why in sort of the big data AI space people often tend to use a TSP and again it's easily readable and writable by all those spreadsheet programs that I have said before databases play a very important role as well there's several different kinds of types of databases that are called SQL no sequel a new sequel they're good for different things I won't dwell on that here but there are different types of databases all of them operate on tables in fact the central object in a Navis is literally called a database table and so native aliases play an important role I won't go into detail here but I just want to mention this likewise you've already had some demonstrations of different languages and libraries think everything you saw was in Python there's other languages Giulia MATLAB octave are many other languages that play important roles and data analysis we've talked about Jupiter notebooks which is a universe so interface to these different languages and libraries to allow you to do data analysis in addition to the various machine learning tools we've talked a lot about tensorflow but there's others they on Oh cat cafe and video digits all are examples of different tools for doing AI and again they love to ingest tabular data and they're very good at writing out tabular data completely consistent with with here and then finally I want to mention XML and JSON which are very popular in object oriented languages like C++ Java and Python those languages rely on data structures which are basically just a set of fields and then those fields can also be data structures and this can continue on these are called hierarchical data structures the way these languages are organized and so it's very natural for them to have formats that they can just say write out this whole blob of data and it'll write it out into a hierarchical form called XML or JSON however it's not very human readable and I just told you that all the data analysis tools want tables which means that these formants aren't necessarily very good for doing data analysis on fortunately we've discovered some ways that we can basically convert these hierarchical formats into a giant what we call sparse table and sparse just means a table where lots of the value entry or I'm sorry are empty thought of the entries are empty and so we can just use that space to represent this hierarchy and all the tools we've just talked about can adjust this data pretty easily because tables are so popular every single different approach or software package and they have a bunch of them list listed in the in the first column here assign different names to the standard ways we refer to the elements of a table and there aren't that many so when you think about a table there's certain standard terms for dealing with what do we call the table in Excel it's called a sheet what we what do we how do we refer to a particular row what do we call a whole row how do we refer to a particular column what do we call the whole column what do we call an entry and what kind of math can we do on it and so whenever you're encountering a new piece of software and we deal with it all the time a lot of times it gets offered and maybe it's related to data analysis it seems really really confusing it's probably working on tables and so all you have to need to learn is what are they calling these things and different software gives different names just for arbitrary historical reasons and so if you can figure out what what it's calling these things then you'll discover that the software is a lot easier to understand so we definitely encourage you to to just map whatever software into these concept and all the sudden life becomes a lot easier and because tabular data is used so commonly whether it be in spreadsheets or analyzing graphs or databases or matrices it's a natural interchange format so by reading in tabular data and writing out tabular data you're preserving your ability to have that soft that data go into any application again you have to be careful about people who want to write proprietary formats because they're basically sort of preventing you from basically working with other tools and other communities and this is another reason why tabular data is so great all right so moving on here I want to just talk about basic files and folders so hopefully you two becoming tables are good that's sort of lesson one and we're going to talk about how files and folder names well-chosen is also good right again this sounds so obvious but it is really the thing that we've discovered is working with thousands of MIT Searchers over the past decade that these are the sort of fundamentals that told you out I've already mentioned this pipeline that you talked about it's a way we organize the the data it's easy to build it has built-in tools in every single computer you have to support this to just create folders and files that means it has no technical back that is you never have to worry about if you have folders and files that in ten years oh my god folders and files are gonna go away they've been around for 50 years in pretty much their present form we don't anticipate them changing they're easier to understand I was talking about tablets already talking about tabular file formats and how useful they are and we will talk about a simple naming scheme for your data files ai requires a lot of data which means often thousands or millions of files having good schemes for naming them really helps you and helps share the data amongst people in your team they can just look at the folders and files and I sort of understand what's going on and again this folder structure is very easy to maintain it doesn't require any special technology and anyone can look at it getting into some more detail about those table formats some sort of real truth in the weeds types of things that are really important if I just talked about you wanna avoid proprietary formats if you can using CSV and see are great they might be a little bulkier you can compress them no problem with doing that then you get pretty much all the benefits of a proprietary format but in a way that's generally usable within a within a TSP or CSV file it's good practice to make sure that each column label is unique so that that's actually a big help you don't want to have essentially the same column label or column name occur over and over again and likewise is helpful if if the first column are the row labels and those are each unique as well and just those two practices make it a lot easier to to read and organize the data otherwise you end up having to do various transformations if a lot of the entries are empty ie the table is very sparse there's another format we use which is called sometimes called triples format basically every single entry in the table can be referred to by its row label its column label and its value that's forms a triple so you can just write another file can be a TSP file that has three columns the first column is the row label the first column is the next the next column is a column label and the next column is the value label and most software can read this in pretty nicely it's a very convenient way dealing with very very sparse data without losing any benefits so that's a very good thing in terms of actual file naming as I said before you want to avoid having lots of tiny files most file systems most data systems do not work with small files really well you do want to compress that decompression doesn't take much time it's usually quite beneficial and so it's better to have fewer larger files a sweet spot for a long time has been filed sort of in the 1 Vega byte 200 megabyte range this 1 megabyte is big enough so that you can read it in and give high bandwidth get the data into your program quickly and up to a hundred megabyte once you start going beyond that you maybe can start exhausting the memory of your computer or your processor and so this is 1 megabytes a 100 megabyte range has been a sweet spot for quite a while you want to keep directories 2 less than a thousand files that's pretty common nowadays creating directories with a really large number of files files so most systems don't really like that you'll find it's also hard to work with and so that's something you want to keep your directories less than a thousand and so you do that by using hierarchical directories and so when it comes to naming files to things that tend to be pretty Universal is source and time of data right there's when you when you call it where you collected it when you collected it are pretty fundamental aspects that you can rely on being a part of almost all of datasets and so this is a simple naming scheme that we use that we find is very accessible we just create a hierarchical directory here begins with source and then we have the years the months the days the hours and this however far you need to go to kind of hit your sort of thousand files per folder window and then we repeat all that name but from the file name itself so there's the file ever gets separated from its directory structure you still have that information and that's really important because it's easy for files to get separated and then you know it's like a kid wandering around without his parents so but if it's got a little label on it you know it says this is my address then you can get that get the kid back home so and likewise sometimes you'll do the time first and then source it really depends on the application they're both both fun but this again it sounds really simple but it it solves a lot of problems if you give data to people in this form they're going to know what it means right it doesn't require a lot of explanations like dates are pretty obvious source names are pretty obvious and then they can basically work through it pretty nicely my databases and files can work together you'll have some people like we do everything with files or we do everything with databases they do different things they serve different purposes so databases are good for quickly finding a small amount of data being a big data set so if I'm going to look up a little bit of data and a big data set that's what databases do well likewise if I want to read a ton of data like all of the data and I want to analyze that's what file systems do well so you a lot of times keep them both around you have databases for looking up little elements quickly and you have files in data and file system if I want to read a lot of data so they work together very harmoniously they serve different purposes and again different databases are good for different things I won't be later these you can read them yourself but there are a variety of database technologies out there they serve different purposes so let me talk about the folder structure a little bit more detail this is getting really like it is really just this simple when we talk about our standard pipeline it's just a bunch of folders that we made like we show on the right so every unit we have an overall folder that we usually call pipeline and then we basically labeled each step with step zero step one so people know which is the beginning and which is the end and what's happening in there so we have steps you're a rock step one parts step to ingest step three query step for analysis step five five is it is if you're doing that and then within each one of those within each one of these will have a folder will have a readme which basically tells people something about okay what's going on there those reading these are really good even if you never share this data with anybody else as you get on in life you will often be asked to revisit things and just having this information so that two years later you can go back and look at it is really really helpful and because otherwise you'll discover that you don't remember what it is you were doing so you write a little read easement to give you some information about what's going on and then within each staff there's usually read me that talk to you about what the instructions in here and then we have a folder called code and folder called data and basically the code describes how you put data into this folder so in the step 0 raw this is often the code maybe for downloading the data from the internet or down or how you collected the data that's the code that basically puts the data into that folder and then with in the next hole to hear parse this is the code that takes this raw data and parses it and puts it into this folder and then for ingest you know this will be the code that takes the data from the parsed form sticks to the database and these might be the log files of the database entry so a lot of times when you ingest data into a database you'll have log files in the keep track of what that happened and likewise this continues on down so very simple and you and I would offer to you if you someone gave you this and you looked at it without any documentation you probably have a pretty good idea of what's going on oh this is some kind of pipeline I wonder where it begins probably step 0 I wonder what that is it's probably raw data right it's that simple and this just makes it so easy the more you can give stuff to people and they could just understand it without any additional explanation that's incredibly valuable and again allows your teams to contribute quickly to projects and then finally we'll add a little thing here which is a data file list this is a list of the full names of all the data files because especially on our systems people are often processing lots of data which contain millions of files if you're going to be using many processors to do that it's it really punishes the file system if you ask a thousand processors to all Traverse a data directory and build up the file name so they can figure out which files they should each process that really that's actually a great way to get kicked off a computer is to have a thousand processes all doing LS dash L on all the folders in a directory with millions of files right people bring the file system to its knees so if you just have this one file that you generate once and lifts all the names of the files then every single process can just read in that file pick up which files they want to read and then do their work and then even know what the names to write the files are and that's really a very efficient way dramatically speeds up your processing and keeps the pressure on the file system at a minimum in addition if you just want to look at what you have you don't have to type LS to list the directory which might take a while you could just look in this file and see what what do I have right it's very convenient to be able to know what what the datasets alright you could go in there you see oh I see all these you know timestamps and sources I know what I have right it makes it really easy for people to get through this so finally I want to talk about using a charity and so again co.design using and sharing together most data that she will deal with starts out unusable and um shareable as the data is not suited for the perf you wanted to use it for and do not have permission to share it with anybody and so if you understand the data and its purpose you can get through both of those problems so really practical aspects here of data sharing we're gonna talk about a few different roles that are very common in data sharing the first is the keeper of the data this is the actual person who has the ability to make the copy of the data you can talk to lots of people but until you find the keeper of the data you haven't really gotten anywhere right because logs as oh yes we have data and we can share it or we can give it to its center and there's someone who can eventually type a copy command to the data you want to find that person that's the person you really need to work with they're the one person who can really really help you because they actually have hands-on access to the data so you want to find that then you want to get a sample or copy the data to your team to assess it is always good to do good OPSEC which is good operational security which means fewer accounts fewer people having access is better a lot of times people say oh here's the data give me a list of names that you want to have access to it if you provide them one name that's often pretty easy to get approved if you provide ten names now questions will happen why are we giving ten people access to this data you don't really need ten people to have access to the data you can just have one person get the data on after the team that's relatively easy to prove you start giving a list of twenty names have accidents and that might just end the exercise right there and once the question starts happening you'll discover well maybe we've lost that money and we need to move on to someone else so and guess what that keeper do they like you now that they've raised all kinds of questions because something they thought was okay well now we're people no you may no longer have a friend right so you've lost a friend you've lost the sorts of data so minimizing the access to the data getting just the people access to who need you to make it to make your next step really good practice I have been that keeper I had been the person who has to approve and nothing makes me sort of shut something down quicker than someone says well I want 20 people to have a census I'm like well no zero works for me that's a that's a number in 20 right that's one of the numbers there how about zero that works for me so then once you get a copy of the data you then convert a sample of it to a tabular form you can identify the useful columns or features in that and then you can get rid of the rest that's a huge service to your community is if you have a volume of data you know like all of this stuff is not going to be useful now you're just giving people useful data that's really really valuable that's sort of written and since you're talking to keeper of the data they may have the most information about what these different columns or features mean that's not a conversation that a lot of people are going to get to have and so you get to help your whole team by saying oh we know these columns mean this they're not relevant let's get rid of them that's a huge service to your community it's less data it makes it easier to release you then make it well if there are is data columns or features that are sensitive in some way there's a variety of techniques you can use to minimize anonymize possibly simulate or use surrogate information to make it so that you can share that data and there's lots of techniques but we need those are only applicable when you know your application so if I look at this data and I see this one column that might be sensitive but valuable for my application I can often come up with a minimization scheme that will work for that because all data analysis based on tables which is mathematically just matrices one of the fundamental properties of matrices that we really really like is that reordering the rows of the columns doesn't change any of the math that's kind of like the big deal of matrices and tables you can move the Rope reorder the roads really columns and everything should still just work which means if you re label the columns or leave re label the rows things should still just work and you'll get the be able to do the science you need so a lot of times when you share data with researchers information that normally you would delete that you would think would be valuable someone else the reasons like I don't care right it doesn't change as long as it doesn't change fun something fundamental about the structure of the data I can work with that and again that's something you would never know unless you have an in purpose in mind so then you want to obtain a preliminary approval to take and I have the data set I've worked with the keeper is OK for me to share this the broader set of people to make sure that they they like it too that's usually easy it's just a small sample and then you can test with your AI users you repeat this process until you basically have Dan's like if this is what we want this is we want to share and then you can sort of go through the process of how do we really get an agreement in place to do that you also then create the folder naming structure for scaling out to more data files and then finally what's great is if you can then automate this process process at the Dana Boehner's site it's tremendously valuable then you have just gifted them a data architecture that makes their data AI ready and then moving forward they can now share with you within their own organization or with others right so this is incredibly valuable right we said data architecture and wrangling is often 80% of the effort you do this you've now eliminated potentially 80% of the efforts of anyone else who ever wants to do data analysis of AI on this data right this is a phenomenally huge win for anyone that you partner with is to give them a very simple data architecture they can maintain going forward it's a huge savings to them alright so another roll we want to talk about is the data on our subject matter expert or Smith so we've talked about the keeper of the data there's often a subject matter expert but associated with the data that is an advocate right the keepers often like to pay I maintain the data I do with these subject matter experts tell me to do but there's someone out there who's wants to help you get the data because they see it's going to be valuable to you and all these AI data products are generally best made by people with am knowledge so it's we're highly motivated to engage with these subject matter experts pull them into the AI community so they can see the best practices that will allow them to understand which data they have how it might be useful to AI and how it how to prepare it so that's better so whenever we get engaged with subject matter experts and other communities you know at data owners we really want to pull them into the community have them come to our environment so they can see what's going on and help us identify new data sets and help make those data sets appropriate for our consumption so one of the big things you'll run into is that there are concerns with sharing data that is there's a lot of confusion about the liability of sharing data with researchers and data owners often have to think about the lowest common denominator if it's a company they have to think about us and he and other requirements all kinds of different frameworks for thinking about that the good news is there are standard practices that meet these requirements and I'm going to list what these standard practices are we have worked with many folks who've tried to share data successfully and these are the properties of successful data sharing activities and the activities that don't do these things often are unsuccessful we didn't invent any of this this is just our observations from working with many people who tried to share data one you want data available the curated repositories just taking data and posting it somewhere without proper curation really does no good for anyone you want to use standard anonymization methods where needed Aisling sampling simulation I've already talked about those but those do work particularly when they are coupled with access requires registration with a repository and legitimate need so there's a data agreement associated with getting the you know getting the data in some way shape or form and those two together really really work well and you also want to focus on getting the data to the people who are actually going to add value to it so one of the misnomers take data and we'll make it available to all 10 billion souls on the planet er when really less than a hundred could ever actually add value to us so why are we going through all the trouble and the issues associated with making data available to anyone when really we only want to get that data to people with the skills and legitimate research needs to make a use of it and so by setting up curated repositories people then apply I want to get access to the data they talk about what their need is that sets sort of a swimlane you can't just do anything you want with the data when you say I'm going to use it for this purpose you have to stick to those guidelines if you want to change them you have to go back and say look I want to do something different if you're a legitimate researcher with legitimate research needs that's usually pretty easy to approve and then recipients agree not to repost the whole data set or to D anonymize it this is what it makes anonymization work not that the anonymization or the encryption is so strong it's unbreakable is that people agree not to reverse it and the good thing about working with legitimate researchers is they're highly motivated not to break their agreements if you're at MIT or any of these universities and you break your data sharing agreements that is professionally very detrimental to you because it is very detrimental to the organization no research organization ever wants to have a reputation that it does not respect its data usage agreements that will put that research organization out of business so there's a lot of incentive for legitimate researchers to honor these agreements which may not be the same just general people who are in working in other types of environments so they can publish their analysis and data examples as necessary they agree to cite the repository program provide the publication's back and their project Ori can carry enriched products so if a researcher discovers some derivative product loci I can apply this to all the data you know I'd like to have that be posted somewhere they can often use the original repository to do that and again we encourage everyone to encourage people to follow these guidelines they're very beneficial so I've talked about the data keeper I've talked about the subject matter expert now I'm going to talk about a final critical role which is the ISO which is an information security officer in almost every single organization that has data there is an information security officer that needs to sign off on making the data available to anyone else they're the person whose neck is on the line and saying that it's ok to share this data the benefits outweigh the potential risks and subject matter experts need to communicate with the information security officer they're also the ones advocating for this data to be released but they don't know the language of information security officers and this causes huge problems and I would say 90% of sharing efforts died because of miscommunication between subject matter experts and information security officers because the information security officer fundamentally needs to believe that the subject matter expert understands the risk understands basic security concepts in order for them to trust them with their signature to get it released and so typically what hand happens is that an ISO needs very basic information from the subject matter expert in order to approve the release of information what's the project what's the need where is it the data going to be who's going to be using it for how long they are expecting one sentence answers to each of these a subject matter expert typically replies with an entire research proposal and so I'm an ISO and I'm trying to assess whether subject matter expert knows anything about security and I gave them really easy softball questions and they come back with something that clearly took them way more time and effort that answers none of my questions what's my opinion going to be of that subject matter expert and how much I should trust them it's going to be zero and from that moment onward I'll be polite but you know how about zero data that works for me right that's a good number right so so this is really important and so if there's one thing we can do because very few of your ISOs and something that my team gets involved on a lot is to intercept this conversation before it goes bad is we can help you work work with with ISOs work with people who have to release with data and help answer these questions in a proper way and some go through a few examples you so a really common question is was the data you're seeking to share so this is a very common question to get back from a nice sub describe the data to be shared focusing on its risk to the organization if it were to be accidentally released with the public or otherwise misuse very common question and so here's an example answer very short answer the data was collected on these dates at this location in accordance with our mission the risk has been assessed and addressed by an appropriate combination of excision anonymization and/or agreements they're released to appropriate legitimate researchers will further our mission and is endorsed by leadership very very very short answer but we've covered a lot of ground in that short answer now explain sentence 1 establishes the identity finite scope and proper collection of the data in one sense sends 2 establishes that risk was assessed in the mitigations were taken sentence three establishes the finance coppa the recipients and appropriate reasons for release and mission approval we've covered we've answered nine questions in three sentences at the level of detail and ISO ones I can always ask you more questions for more detail and they're usually looking for another one sentence answer and you build these up to be three four five sentence things that cover broadly what you're wanting to do and you see here you don't want to do anything that requires the I so that have detailed technical knowledge of what you're doing they're looking for overall scope overall limiters on what you're doing so that is somehow contained to a reasonable risk another question here that's very common is where to whom is the data going so please describe the intended recipients so an example answers the data will be shared with researchers at a specific set of institutions that'll be processed on those institutions own systems meeting their institution security policies which include password controlled access regular application of system updates encryption of mobile devices such as laptops authoritah it accesses the data will be limited to personnel working as part of this effort right again you've covered a lot of ground very simply we didn't really go into too many details that are specific to this to this particular case and then the final one is what controls are there on further release either policy or legal so an example is an acceptable use guidelines that were - dia na mais the deal will be provided to all personnel working on the data publication guidelines have been agreed to the Lao for high level stats apply needs to be published but prohibit including any individual data records a set of notional records has been provided that can be published an example of the data format but is not part of the actual data set the research agreement requires of all that it be deleted in the end of the agent except those items retained for publication I'm not saying you have to use but again this is another example of what Isis are expecting you give them language like this their confidence swell to the guy I'm dealing with someone that understands the risks I'm taking on their behalf and we can move forward in a very productive way and if you ever have questions about how the news is feel free to talk to my team or other people or other ISOs who've had to deal with this we actually are working with a lot of people on this language a lot of people in the government to sort of get these best practices out there we have socialized this with a fair number of experts in this field and there's general consensus is a very good methods for helping you get data out so that brings me then the talk so as I said before data wrangling very important significant part of the effort analysis really relies and tap your data so that's a really good format you can do it in very simple non proprietary formats taking you're going to have to name your folders and file something you might as well name something that's really generally usable and then finally co designing the sharing and what the intent is at the same time that's doable generalized release of data very very hard tailored release for specific applications very very doable if you have good communication from all the stakeholders as well so these are just really practical things isn't sort of fancy AI this is sort of bread and butter how you actually get data analysis done and with that I'm happy to take any questions you 