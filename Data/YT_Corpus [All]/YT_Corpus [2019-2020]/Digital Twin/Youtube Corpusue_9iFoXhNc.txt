 Hey, everybody this is Kai Waehner from Confluent. Today I want to talk about IoT architectures for digital twin for Apache Kafka and MongoDB. Let's get started. Here's the agenda for the next 40, 45 minutes. I want to explain what a digital twin is and discuss some use cases about that and then introduce Apache Kafka and event streaming and why that helps so much with many IoT projects. It's also important to understand how this relates to other IoT platforms and how to use them together and then to build a digital twin with these combinations and of course, together with MongoDB for having a good storage for doing analysis and reporting and so on with this great combination of technologies and an open, scalable ecosystem. Finally, we'll also show a live demo about a digital twin we've built for connecting 100,000 cars for real-time processing and analytics with Apache Kafka and MongoDB. Let's get started with the introduction about the digital twin and the motivation. The motivation and the end is pretty simple because in the IoT space, typically it's really a big shift. Software and digital services become the key differentiator. This is an analysis by McKinsey & Company, but this is a common scenario and every vendor has to change its business model. In the end, it's not just about producing and selling hardware and doing physical services to repair hardware, but it's more of like providing additional software to make additional revenue to make the customer more happy and to compete on the market. This is really the main motivation for building a digital twin to combine the hardware with the software. Here's how it looks like and it really doesn't matter which industry you're in. Many people think about a digital twin as something for especially manufacturing and production lines or maybe robots. Of course, that's true, but a digital twin is also used in healthcare and not just for machines, but also for the human body. It's also sometimes used just to improve the processes, not even a physical device inside that. In the end, in short, a digital twin is a living model that drives the business. That's a really I think a great definition of a general term digital twin. Here's just a few examples and I don't go much in a deep type but trusted you see, you can really use it, that [?]. This is a smart infrastructure from Siemens where they built digital solutions for the entire building life cycle. They, in a dark term, in marketing, they call it a building twin, for example. I think you'll get the idea, you want to really track that we're all building all the time from the design of a construction and operations, but also up to demolition at the end and you have all this data to monitor it, to plan it, to simulate and to make the best decisions for the real physical building. A connected car infrastructure is also common scenario for digital twin where the car has a digital twin. Each of the hundreds of thousands of cars has a digital twin so that you know the exact status. That's one example where Audi has worked together with Confluent a lot to build this digital infrastructure with streaming data from all the cars in real-time to the Cloud, for the processing. Then there's different use cases here obviously things like predictive maintenance where we can send an alert back to the driver or the car if the engine is breaking or something like after-sales. Everyone to have a better customer experience and of course, also increased revenue with that [?]. This is a great example and this is actually also the demo which I will show in the life team at the end of the talk. The last example is about twin human body to enhance medical care or this is an example from the Philips HealthSuite. This is really to just get the idea that a digital twin is not just about physical devices, it is also about of course, machines here in the hospital, but it's also about a digital twin of the human body. No matter if you're in the hospital or at home, depending on your situation and the health you have maybe devices working with you, but you really track yourself all the time so that if there is an anomaly with your heart then you can get an alert or send it to the hospital or something like this. That for our digital twin can be really used in any kind of industry for many different use cases. What's also important to understand when we talk about a digital twin, of course, it's about a lot of data and typically real-time processing, but machine learning is really complimentary and in most cases, you combine it with the infrastructure of a digital twin. It's a complimentary concept because with the flowing data all of the time, no matter if it sends or if it's human data, you always want to do continuous learning, monitoring, and acting. This is typically done not just by simple business rules which you can define, but by statistic models or by machine learning algorithms of any kind like decision trees or neural networks or an ultra encoder for anomaly detection or whatever makes sense. This is just important to keep in mind and I also will keep that be part of my life demo at the end because it's so critical part. If there's an understanding about what a digital twin is, let's think about the challenges because that sounds great but there's a lot of challenges. Here's a picture from the automation industry. This is always the best example, I guess because if you're in manufacturing and production lines, you'll see a huge difference on the left side between the machines which are running for 20, 30, 40 years in a factory. On the other side of things, we are talking here today about MongoDB and Kafka and Cloud services which are here for maybe 10 years now, but they are also changing all the time. This is something very different. You have to think about how do we deploy this open and flexible architecture, but let it fact [?] with these machines. It doesn't matter if you're in a factory or in a hospital or in a building. These guys live for 10, 20, 30, 40, 50 years. This is a huge challenge in how to integrate this kind of different technologies and the different way of how you have to think about building these products. With this key trend in IoT is the evolution of IT and OT. This is really converging. As most people from a conference like MongoDB come from the IT world, this is about software development where you typically change things every year and you add new features or maybe now with HR development and AP testing in the Cloud, you can roll out new features every day and it's just explaining the OT world this is very different. This is not just different for the hardware like the production lines or the robots, but this is even true for the software side like an ERP system or an MES system. Yes, they are also a little bit flexible and you have to change configuration, but the ERP system itself is also running at least for 10 years. I'm based in Charming and if I take a look about the SAP systems deployed all over the world, and they're very old versions, where most people don't use SAP HANA on S40 or some immigration process which takes 10 years or longer off. This is a huge chance to bring in modern software also, especially then with real-time processing and scalability into this OT world about a digital twin. Obviously, with that, flexibility and scalability are important because you get more and more data and want to use it but on the other side, that also needs to be somehow combined with a good kind of cost and complexity perspective because when you directly go to the vendor of the machines and buy some proprietary software, it can be very expensive. Also, it's not just expensive, but it's also typically a monolith which is built just for this thing. It's not really extendable and not really scalable. Of course, if you have a second machine, you can buy a second software installation, but that's not the kind of how we're thinking in the software right today how to scale things up and how to decouple applications from each other. That's why, especially in IoT, often see that the mount for an open, flexible, and scalable platform. You might guess it. This is where things like Kafka and MongoDB perfectly fit in. That's what they were paid for. They are open, so you can build any ecosystem around that and change things. They're also scalable and they are mission predict. This means like the things you know from the OT world for 24/7 operations. This is also what you can do with things like Kafka and MongoDB and no matter if you're running at the edge or on-premise or if you're running in the cloud or even in hybrid scenarios. This is the demand. We see a lot in many IoT projects. That's in the end also, the intention for this talk here today. With this motivation, let's now think about what Kafka is and why it works so well in an IoT project. Kafka is an even streaming platform. I see many people thinking about Kafka as a messaging layer. For example, you use it to ingest log data or sensor data into a data lake like Hadoop or AWS S3 for a batch analytics. This is one use case, of course, but it's just really one of a small fraction of use cases. You have many, many other use cases instead of just data extraction. The reason for that is because Kafka is not just a messaging system, but it's also much, much more. I believe that the core of Kafka is also storage layer and a huge thing here is that you decouple producers and consumers from a charter of that. In an IoT scenario, sensors produce data all the time. If you add more sensors, maybe like humans or machines or buildings, then they produce more data. It doesn't matter to them if the consumers of the data are falling behind or if they cannot scale enough or it takes a week until they can scale. Therefore, with the storage system, you can decouple the producers and consumers, and also you can easily reproduce data. That's also important for many different actual twin use cases, for doing analytics on old data, for comparing data of different machines or systems and all these kind of things. Also in addition to the messaging and storage, which is the core of Kafka, Kafka there was [?] includes Kafka Connect for integration. This means that you can integrate with any kind of source and sync. We will use this in the demo, for example, to integrate MongoDB. Also, Kafka provides Kafka Streams and other native open source tools like KCQL for doing continuous processing of the date. It's not always a good idea to first store in a database and then wait until someone else takes it and processes it. This is a data at rest, and this is sometimes okay for reporting and analytics. This is also a good use case where you put data in a MongoDB and then do something with that. For many other use cases, especially in IoT world, we want to continuously crosses the streams of the data for some of the use cases. This is why this foundation of Kafka, if it's event log in the middle, works so well. Completely decoupled from the producer, each customer can do with the data what it wants and what can be a real-time consumer. All right. For example, for predictive maintenance and real-time alerting. Another one could be an ingestion layer into a data store like MongoDB where you keep the data forever and do analytics on top of it. Kafka was built for scale in the beginning, 10 years ago. Companies like LinkedIn use it for really huge data sets. Also, Confluent Cloud where we run Kafka and it's ecosystem as a fully managed service, with servers customers would run over one gigabyte per second with one single Kafka cluster. It's really working at scale, but keep in mind, so Kafka is not just used by the tech giants. I think you know it already, but today almost every big company has several different Kafka projects for many different use cases. What's even more important, what I wanted to point out is that Kafka is not just used for big data. That's what it was built for, but today I see around 70% or 80% of the projects I do with customers, and I meet around 100 customers a year, it's about not so big data. That's really important to understand. Kafka is used for many mission-critical deployments but it's not okay if you lose some data because it's not just log analytics you have to do. It's more about instant payment platforms or fraud detection or in the case of a digital twin, about use cases where it cannot be down like, think about literally a friend of a human. If you're in the hospital it has run 24/7 for data works. Therefore, here's just a few different examples so that you get a feeling about what Kafka is used for and what customers we see at Confluent. The key message here is really not to think about all these use cases now but it is used for many different kind of platforms and scenarios in IoT, but also in many other projects or really in any kind of industry for the typical values looks like increasing revenue and for customer experience or for reducing costs like migrating data away from a mainframe or like middleware system or for mitigating risk for fraud detection or regulatory things, or in the case of a digital twin may be for reducing the downtime of the production line or for increasing the health and reducing the risk of the person in the hospital. There are so many use cases where event streaming in real-time at scale is important. That's actually also the reason why so many people use Kafka in IoT projects. This is really a summary of the main arguments I hear all the time. Of course, it's a scalable real-time system but in addition to that, it's really critical that it's up 24/7. Depending on how you deploy it, you can run Kafka with zero downtime and zero data loss, even if a complete data center installed. This is in combination with these characteristics I explained before like decoupling the producers and consumers, being able to reprocess the data and integration to a legacy system and sensors and big data stores like MongoDB. This is really a huge system and you can also deploy hybrid combination of on-premise and add some cloud and so on. Of course, it's open and you can deploy it without any vendor lock-in. That's really also a huge characteristic if you want to go global in many different countries with different infrastructures. Here again, what's also important, I mentioned this in the beginning, it's important to think about machine learning and how to combine it because for a digital twin, machine learning is important to really use the data and get value out of that. Therefore, machine learning is complementary. This is where Kafka fits in a [?]. The thing about machine learning, on one side you want to ingest data into a data store, to train analytic models, to find insights with algorithms. This can be in different areas. This can be consuming the data directly from Kafka or just can be consuming the data from something like MongoDB, with a Spark connector. It doesn't matter where your training model but the point is you want to find insights in historical data. Then the second part, you want to deploy this model to a production system and the best case, in digital twins in real-time at scale. No matter if it's predictive maintenance in the factory or if it's the human body where you may want to act in real-time if there is an anomaly. This is where Kafka fits in perfectly with these real-time events streaming at scale, mission-critical. No matter if it's for industrial IoT, or for health care, or other use cases. What's also important, we need to clarify really that Kafka is not an IoT platform. That's what I really want to point out. There is things which Kafka doesn't do, like device management or working in unreliable networks. Directly, I'm connecting to hundreds of thousands of users if you are in customer IoT and build something like Lyft, which is a big Kafka customer, which connects to millions of users. There are of course some kind of proxy or IoT platform, so it's combined with Kafka. Therefore, it's important to understand that Kafka is used a lot in IoT projects but actually it's complimentary to other IoT platforms. There is many IoT platforms in the market. This is just an example. From 2019, probably in the meantime, you have a 1,000 platforms or even more for specific niches or far more general things. Let's take a look at a few of them, without going into too much detail but there is things like proprietary IoT platforms. That's often provided by vendors of the machines and hardware, like, as I said, I am based in Germany, so I know Siemens very well, where you have, for example, S7 SIMATIC PLCs. This is the controllers running in the production lines. Then you have to connect to them from IoT platform. As I said so, Siemens MindSphere, the IoT platform from Siemens is great for that. The same is true then if you go to GE with Predix, and to Cisco with Kinetic and so on. All of these have the end results of some kind IoT platform front. The big limitation here is that they're often a monolith and they're often very complex and very expensive. They're not really, what I just discussed with Kafka about building an open scalable [?]. On the other side of IoT offerings from the cloud providers, all of them have IoT services in the meantime. This is not even just cloud services. That's what they focus on, of course, because cloud providers want to get all the data in the cloud to make more money and security data there. That's why getting data out of the cloud is typically 10 times more expensive than getting data into the cloud, and that's true for all these providers. They also have some kind of extra abilities like, in this example here, you see Azure Sphere, but you can't directly connect to the Azure Cloud from something like [?] control. The big imitation here still is that, first of all, you are really locked into a vendor, which is of not really good if you want to be flexible or if you want to roll up globally and big companies don't use just one cloud. Also even at the edge, this is not really built for it yet. If you don't just want to ingest the data into the cloud but really do processing at the edge, then this is the beginning of the right solution alone. Then, of course, there is other standards-based and open source IoT platforms. There's implementations around standards like MQTT, which I think it was a good scalable solution. They typically focus on a specific topic like [?] so one solution and that's what they do very well. MQTT works very well in unreliable networks and it works very well with hundreds of thousands of connections. That's the demo I will show later about connecting to cars because via [?] on MQTT because it's a great combination. Therefore, again, I want to point out with Kafka and an IoT platform, no matter if it's proprietary or open-source or standard-based, this is a great combination. Definitely now we have to spoilt for choice for digital twin, how do we build it? Honestly, there is no best option. It really depends on the use case and on what you want to do. Then you can evaluate what makes the most sense here. Here are a few characteristics of a digital twin technology so you can think how to build that. This is actually what I found on Wikipedia from some references so many people agree on that, while they have different definitions under the hood. The point is, you need to meet cognitivity, on the one side to the physical assets like the machines and PLCs, but also to enterprise software, MES, and UOP systems and CRMs, and also, of course, auto data storages like MongoDB. [?] direction and communication is very important because you don't just want to do analytics, but also to command and control to send data back to the digital twin physical devices. Then other characteristics are also important like decoupling and standardizing the interface, like sharing data with multiple agents over different infrastructures and even over different regions. It's important to be able to flexible to adjust the programs and the deployments no matter if it's just a production line or the overall architecture. With that, of course, you also want to have a digital traces where you can analyze also historical data, either with batch analytics or with re-processing [?]. The best case there's this modular so that you can add new components and remove old ones. This is very important as you have seen in the beginning, where I talked about industrial IoT, where machines run for 20, 30, 40 years. It's very important to be open and flexible. Of these characteristics, let's now take a look at a few options for an IoT architecture for digital twin. One option is of course, you just go to the IoT provider of your machines like example, if you have Siemens 7 and PLCs, and you take Siemens MindSphere for that. That might work if you just want to do IoT integration and create some dashboards or integrate with a SCADA system or something like that. This is one option but if you take a look at the characteristics on the top right, like being flexible and changing things, and also being modular and integrative with other systems outside this plant or factory, like with the global ERP system or something like the CRM or other data lakes. Well, this is a little bit limited and a monolith. It's really often also complex and expensive to do it like this. Therefore of course, you could just outsource that database for the digital twin, something like MongoDB. In this case, the IoT platform does all its integration with the edge and then also sends all the data to the MongoDB database for analytics of machines there. Still, this is working, in some cases, but you don't have the connectivity and the flexibility to connect to the rest of the enterprise and to really decouple the different systems from each other in a modular way. Therefore, a very common solution we see a lot is that you combine Apache Kafka with IoT platforms for integration with the edge devices. Also, you use the IoT platform for doing something like Device Management, but then you keep Kafka in the middle for decoupling all the other services from the IoT platform, like the digital twin in this case with Mongo, but also from other consumers like real-time apps, or a mobile app, or, for example, our CRM system. In this way, you have a really modular system, but you still can keep the data where you need it like maybe an Apache Kafka just for a day and in MongoDB forever. That depends on what you exactly want to do. With this architecture, you're very flexible and scalable and open. Then in some other use cases, we actually even see this kind of architecture where you don't even use an IoT platform. This is an example where we directly connect two technologies like MQTT or OPC UA, or even directly to PLCs, like Siemens 7 or [?] or anything like that with a Kafka connector, and then process data directly in Kafka, and of course, also interested into something like a digital twin technology. This was many different options, you always have to spoilt for choice, and there is no one single option. It depends on your use case. With this understanding now, it's also important to discuss the IoT architectures for Kafka and Mongo. What you see here is a reference architecture, which we've built at Confluent with MongoDB together, but you can see how the combination of these two technologies add a lot of value. First of all, it's important to understand this is now open and scalable, but also mission-critical for 24/7 deployments. With this, you can combine the best of both worlds, using Kafka for real-time streaming at scale and including doing integration to all the sources and the things and decoupling the systems from each other. Then also integrating with MongoDB as database for long-term data storage for doing things like the batch and real-time analytics there, but also integrating with dashboards or maybe even with other BI tools like Tableau or Qlik when you use the SQL interface of MongoDB. This is really a powerful and great combination. Then you can deploy this in different options. Here's an example where deploy an edge digital twin, this means you're focusing, for example, on a factory, or on a hospital, or maybe just on a few machines running in a hospital or factory. Here you deploy as a single cluster or even a single broker, if you don't need high availability in some scenarios, like in a retail store, and build a digital twin chest in the factory of the machine. This is one example, which is pretty lightweight, and you deployed in the factory for doing real-time processing and storage in the factory. Of course, you can then replicate that and also deploy it in another factory or hospital. You can also replicate data to another more central solution like the cloud or a big data center. Therefore, this is another option where you have local processing and storage at the edge for local problems. You want to keep data locally for things like edge processing and local analytics for doing things like real-time predictions. From a latency and cost perspective, it doesn't make sense to ingest all data into the cloud to do all the processing them. That's, of course, what the cloud providers want you to do but that's not the real world in IoT. In IoT, you keep the things to the edge where it makes sense, or where maybe you even have to do it from a regulatory or privacy regulation. Also, you replicate data to the central cluster, either in the cloud or maybe in a big data center in a region. There then it's the same story. We integrate from different edge locations, and maybe they even have different deployments there. On the top left, this is what I just discussed before, you see one deployment where we have an open architecture at the edge with Kafka and MongoDB. On the bottom left, you see something where maybe in a production line, it's just Siemens in Germany, you just use Siemens MindSphere at the edge and integrate that with your Kafka connector to the central Kafka cluster. Then you get all this data from the different factories with different technologies into one single MongoDB Atlas in the cloud, to do the comparison of the digital twins from the different factories to find out why is this one factory in the US working better than the one in Germany while the hardware is exactly the same, but under the hood, maybe the temperature in the factory is different, or some other characteristics. This is the main idea behind this digital twin architecture. Then, of course, you can also go more global and that's what our biggest customers do. Sure, it's also so important that you are open and scalable because in different regions, things are different. First of all, a good example from the beginning was Audi, where they started rolling out a connected car infrastructure in Germany. They started ingesting the data via proxies into a Kafka cluster in AWS, but they knew from the beginning that they will also roll this out to China. In China, there is no AWS so you have to go to something like Alibaba Cloud or your own data center. With this open architecture, you can build one template of a digital twin architecture and then deploy it in different regions, either in different clouds or maybe in one region just in a data center. This is pretty powerful and therefore, you can either just roll it out in different regions, or then of course also replicate the data between different regions for aggregation scenarios and analytics of all the data. With this overview about different IoT architectures, I want to conclude this session with a demo about a digital twin which we have built for 100,000 connected cars. Intentionally, we wanted to do a real scalable demo, not just hello world. Here it looks like a lot of components. Let's quickly walk you through this. On the left side, we have the cars, the simulated cars obviously, as we have 100,000, which produce data continuously and ingest it into the Kafka cluster. From here, then we do first real-time pre-processing at scale with, in this case, we're using KSQL. That's Kafka native technologies so you don't need another processing unit like Spark or Flink, you can run a chat with Kafka if you want. Then, we also do the model training just from Kafka data with TensorFlow and TensorFlow I/O Kafka plugin. Of course, you could also ingest the data into a data lake like Hadoop or maybe Mongo to put some MapReduce or Spark ML on that. In this case, we simply train the model directly from the Kafka data. Then we deployed it to a Kafka application for real-time scoring. This is really important, no matter where you train your models, which has to be more like a batch mode, the model deployment, and scoring, and monitoring that has to run in real-time. That has to run at scale for a connected car infrastructure for, in this game, hundreds of thousands of cars, but in the real world, even millions of cars. Then parallel to that, however, we also ingest the data into MongoDB. In this case, we want to use it there for analytics and dashboarding so that we can analyze the different digital twins from all the different systems. This is really a huge scenario, which adds so much value. In the same way, of course, you could combine this on the left side of some proprietary IoT solution at the edge like Siemens MindSphere in examples I've discussed. This is the example and now, let's walk through this quickly. I will only do a five-minute demo. If you want to see a longer demo of that, you can also check the YouTube channel or my GitHub where I have a 25-minute recording. The first thing we are doing is I'm starting some test generators. This is a tool from [?] which simulates 100,000 connected cars. This is by far not the limit, this is just that we have a good scalable demo, but we could also start millions of cars and also process that data in real-time with Kafka. We have running this on Google Cloud in our example, and as you can see here, we are running on Kubernetes. This is intentionally so that we show that this can be a template framework, like an example of Audi, which we can roll out everywhere, and be very flexible and adjusted like we need. In this example, I have a distributed Kafka cluster running with three brokers and three [?], and some add ons like a schema registry and KSQL, and also important I have two Kafka Connect instances because I have deployed the MongoDB connect too there to do a scalable and robust integration in real-time into MongoDB, wherein this case, we built this digital twin. With this, getting started now, let's take a look at the UI. Now, here you see a dashboard, where it's important to understand this is where we ingest the data. The top right you see we have now 100,000 connections. These are producing 10,000 messages per second. This is not much, but this is the real world and in connected cars, and it's expensive to send data over 4G or something like that. Each car sends data every 10 seconds. You can see this is really still a big scale because it's coming from a lot of devices. Now, we want to process the data with Kafka. What we are doing here, first of all, we have to roll JSON sensor data in here. This is coming from the cars. As you can see here, and this is the data we use later, like the engine vibration, amplitude. This is the values that we use to detect the anomalies, in this case, model which is an autoencoder. We ingest all this raw data in JSON, but for the processing and for other reasons, we want to get the data into Avro format. This is better compressed, and this allows schemas and versioning and so on. Therefore, we have built a KSQL query and deployed that, which in the end is doing one single query, where we do processing of the data in real-time at scale. While this is just a SQL query, you can deploy this to production and run it at scale for millions of events per second. With that, we transform the data into Avro format. This is what you can see here, so the data is more or less the same. We didn't do any ETL here. The big point is now we also have a schema for that. With that, now, we can ensure that the rest of the platform integrates well with this. This can be the MES system, this can be a CRM system, or this can be our digital twin, which is then also MongoDB, where we ingest the data. Before that, we also want to do some real-time scoring, that's in this case, part of the use case predictive maintenance. In this case, you'll see a digital twin can be many things. It can even be spread out over different technologies like we do the historical analyzes and long term storage in Mongo, but the real-time scoring with Kafka native technologies. Therefore, what I'm now doing is starting a model prediction. What I'm doing here is I deploy a new application, in this case, again, on Kubernetes. Don't be worried if you don't know Kubernetes well. What I'm doing under the hood is I'm deploying a Python application. I can show you if you're from space, but this is in the end, one deployment, which is a TensorFlow application with deploying an autoencoder. The big point here is really that this is, in this example, just a very lightweight Python app. This means that in this case, I'm not even using something like Java, but a data scientist can just use the TensorFlow, Python API, and the Confluent piped and Kafka API to produce and consume data in real-time, and build a pipeline around that and even deploy these models because as it's using Kafka under the hood, it's highly available scalable, even if it's just Python containers. You can still scale them up without downtime and data loss and all these things. Now, here you see this is model predictions and these are also coming in real-time on the pipeline we have built. In this case, what we have done is we have built one single pipeline for model training, which is fetch, for model predictions, which is real-time, both Kafka native, and we are now also interesting the same data into MongoDB for long term storage and further analytics. Therefore, if we now take a look at MongoDB and I have indeed created one collection here as you can see, which I call it digital twin. This is known as sensor data. In addition to doing the real-time processing and storing in Kafka, we are storing all these events also in MongoDB. With this, in MongoDB, of course, you can use all the features, which, for US guys from the MongoDB conference, you know, MongoDB well, so I won't focus on that too much. The point is, now you can do anything you want to MongoDB like creating dashboards, integrating with BI tools, doing photo processing, and ETL here with the data, and simply keep it long term for storage for doing further analytics with that. That's really huge. Then of course, you can also send it back to Kafka is like the Change Data Capture, which is available as a Kafka connector. With this combination and end to end deployment, I think it's really huge. Let's recap here. We have really built a highly available and highly scalable digital twin for real-time processing of millions of events with Kafka native technologies and with long term storage and analytics capabilities with MongoDB. I think this is really a great example. You can also check this out on my GitHub page. To recap and think about the key takeaways. With Kafka and MongoDB, the three are a great combination. We can easily build digital twins to match the physical and digital world. Also, as you have seen, this is really open, scalable, and flexible. This is really important for IoT, where you only one side have machines or devices, which are sometimes 20, 30, 40 years old, and often use proprietary technologies. Or maybe you have to put on a monolithic IoT platform in the middle. Then to integrate with the rest of the world into the data analytics, you use these open platforms. This is both for event streaming in real-time and for long-term storage analytics. That's why Kafka and MongoDB is such a great combination. Then especially because with this, you can deploy this at the edge or in the Big Data Center or cloud, or even hybrid. Actually, to be honest, in most IoT scenarios this is the case where you deploy things at the edge, and in the cloud, and then often even multi-cloud or over different regions or even over different continents. This is what our biggest customers do. With this, I hope I gave you a good overview about IoT architectures and how to build a digital twin with Kafka and MongoDB. If you have any more questions, please connect to me on LinkedIn. That's also best to keep updated in the future or feel free to connect me to me via Twitter or via email. Thanks a lot for watching, and have fun with Kafka and MongoDB. 