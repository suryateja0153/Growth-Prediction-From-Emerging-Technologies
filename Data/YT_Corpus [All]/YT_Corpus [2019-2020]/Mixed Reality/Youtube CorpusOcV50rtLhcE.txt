 Jimmy hey everybody how you doing how's build good right on all right sue myself from these fine gentlemen here are going to be giving you a little bit of an overview of how to get started using Unreal Engine 4 for a mixed reality development so yeah how many people in here have used Unreal Engine before before maybe a handful okay great so some concepts in here that we're gonna not go super deep on that are how to use the tool itself but a lot of that stuff is available in our documentation and some of the most rudimentary things we've got plenty of tutorials on we'll be focused more on the API itself and how to leverage it to build your experiences so welcome first and foremost who are we Ryan who are you I'm Ryan Vance oh yeah so kind of handle the team that deals with Mr ARB are all the ARS all the artists the ARS cool Jackson I'm Jackson and I'm a software engineer at Microsoft and I'm responsible for getting Microsoft bits over into you and real yeah and I'm chance I work on number different things that epic including a lot of our demo projects what was helping us get support for different new platforms like whole ones to you off the ground and moving with content it's a little bit about what we're gonna cover today is a little we're going to talk about what we support for the hololens one the hololens to you and kind of what that looks like some of the prerequisites you might need to get started today or actually what we get started with today that you'll you'll need to get started once full support shows up we cover how to handle touch interactions using the hand tracking system and we're gonna talk a little bit about anchors and pans and how you can create anchors and create content that is pinned to those anchors that stays where it is and then we're going to cover a little bit about building content that reacts to the user since we have information about where the user or the viewer of the head that's wearing the headset in the space is all using a project that we put together over the last few weeks just to kind of demonstrate some of these things after that we'll talk a little bit about deploying the device some of the things you probably already know if you have worked in the Nuba for just slight caveat with some of the features that are currently in progress and then the schedule in which you might see them and then the last is a slide for developer resources that you can take a snapshot up today so you can look ahead add some things if you have a use Unreal Engine some places to get started and some links that will be hosting some of this content whenever it is come online so let's talk a little bit about what we support right now what we will support in the future Ryan can't give us an overview here yeah so let's start with streaming support that actually exists now for hololens one and will be coming out shortly for hall ones - and when I say streaming support that's the ability to run on a local desktop and then connect to the device wirelessly and then you're basically doing all of your rendering and heavy lifting on the desktop you see all like it's shipped over to the device to be displayed and then device sends back to lemon treating odometry to the PC so he knows kind of the state of the user and all the input and everything that's going on the device is detecting it's great before taking advantage of a giant machine I mean pushing really cool visuals and I can gameplay over the device it's also a great free iteration because it's literally just a button click it connects the device and you're immediately using it you don't have to go through the whole compiled and deploy stuff so you're kind of bringing your project up it's a great choice next up is our native platform support so this is new this is gonna be for Hollins - we're actively working on it now so she requires no PC but you do need PC people to use to build and then you ship it over and it runs natively on the device and all this is being built on top of rx our framework so it's the same framework we use for all of our various VR and AR devices so if you're building content on top of that platform as long as device you or targeting supports that capability your contacting you build on device a will also deploy a device view in vice versa and then for each individual device if they have capabilities that are specific or unique person will have a way of getting at those as well which is also the case with all one's - yeah I was really cool about that too is you might not be able to get your hands and some of the hardware that's been talked about quite a bit at this show but that doesn't mean you can't start building content that will work on it by the time can so a lot of the concepts we'll be covering today work across others other hardware as well so if you're used to working another you know extended reality hardware platforms a lot of what you can do there can be directly applied over here which is really great for not having to build content more than once some of the prerequisites and this is coming from specifically where we are today and a lot of the experience we've gone through over the last few weeks the project that I built for this talk is all a standalone application on device for the hololens - and so these are things that i've had to set up here and they may change other can maybe speak to a little bit about that once before a little bit but these are things that we had to do before we could actually get moving so one we had become windows insiders and get the latest version of Windows 10 sign up online and insert it as a calm pretty straightforward process I get access to some of the latest and greatest it's there and then come on side that we'll need to grab the the preview SDK so we could actually compile the code that is required for the devices on the Unreal Engine 4 side we will be releasing later this month the full source for all this work on our github and there's a URL here that's a short URL for a reason once because that's member of one too because it's not really yet the actual sources will won't be released quite yet but when it is you'll be able to get out of here the reason we want to have added this and same for the project so you have a picture of it today as I see it taken which is great I won't be able to talk to you in person whenever it does come out no want you to have the opportunity at least know what that is but we will be hosting this soon yeah so the current kind of pre-release will be coming out on github at the end of the month and then we're working towards shipping the full released version and 423 which would be later this summer all right so let's talk a little bit about using hand tracking to do touch interactions I assume the best majority of you have worked with the heldens one a used one Holland's two as well any kind of mixed reality headset a lot of what you do with your hands is you reach out and you touch things inside your world that don't really exist in the real world the way that we've approached that with both the the NASA demo and with this project today is using hand tracking and specific key points on hands do you actually know that you're overlapping with something virtual in the space so how many how many touch points we track on on each in 26 26 which is knuckles and palm and wrist are all available here and there's a really just simple a call that will show here in a second that you get it's called a get hand tracking getting hand transform for key point you pass the one you want in so purely about the index finger here and that's what we use and this little guy right here does animate yeah my gifts work you'll see me reaching and I'm spawning a new object here it's a robot I'm able to spawn in and I'm on the desk there this is just using that simple it's a play PA : so let's take a look at what that looks like inside the editor so this is the blueprint graph for my my pawn now upon his petition is traditionally what you use to represent your reviewer or your player user and the scene it's it's a part of the gameplay framework that represents the person that's actually driving the experience for us to get started with with with hand tracking what we need to do first is we spawn a couple of motion controller components and these are also part of our extra frame works the same way we've done other hand tracking or other device tracking you know logic built on the platform so we spawn a couple of those and we add them to our pond we make them children you can see here of the route of the camera component and once that's done you set a hand for each of those and then your hand tracking is ready to go now right now we're calling get touch or check touch for two different hands missed wink at the ink they grab back up here every frame we're going through really quickly and doing a quick check touch on left hand and on right hand we're passing the key point of index tip in this is an e new you can kind of see all the other individual points we'll be tracking here and if you ever wanted to draw you bugged kind of figure out what's going on with your hands you can actually just draw a little dots on these all these individual key points or draw lines between them and actually draw the physical hands you can kind of see what's going on inside your your application funny little yeah it's great right a little skeleton hands moving around your space super cool and so really this is the this is the the winner here the get hand joint transform call what this does is it passes the hand and it passes a key point and I never turns a transform value now transform value is gonna give you world space information about where that specific joint is and what I'm doing here it might be a little bit of inefficient if you're you know really putting this to an actual production application but it's just showing showing how this would work here I'm doing this on every single frame and I'm doing a quick sphere overlap for specific actors in the scene with a really small radius so it's like a little circle that spawns on my hand and checks every frame to see if I'm touching anything in particular now see that I have this actor class filter for interaction object I don't know that I have that open yet so I'm gonna go ahead and pop it open and you'll see that this is just that's the wrong one there we go this is just a little object in my scene that represents my little menu buttons it's got a box Collider on it and that's what receives those oh I've been hit so I can tell that check touch that yeah so we did to find something so what I do for this is for all the actors that come across cast them quickly to that interaction object and I tell them that it is touched this is a local member variable there that helps drive some of the touch logic because of that I'm able to do this right here on each of these buttons here so that's the logic that's driving that and that's how I ever handle it but let's talk a little bit about anchors and pens anchors are really cool because they represent like a consistent 3d space or a 3d location in your world space but it's kind of the same every time you try to get at it so you create anchors inside your world and then you can access them by name later to get information about what they are so it's really powerful if you want to put things pin things to your desk or to your wall around you say you're you're wearing your hololens at your desk and you want to have your calendar on one side and your little debug rubber ducky on the desk in front of you you might build a pen called calendar on the wall and a pen called rubber ducky there and what's really neat about that is you can just use that information to say hey every time I come back here those things need to be there and you can write that into your application based on those names I need to add on anchors yeah another cool thing with Acres is because there are a point in world space any content that you're pinning to it can be in local space so it can be relative to that anchor rather than relative to some arbitrary world origin yep yeah so let's take a look at what that looks like it's actually the same and if you saw before I'm actually spawning these right now a drone and a robot might recognize these assets from a robo recall kit we use them because they looked good in EM our I'm spawning him in these specific locations and I'm creating a pen for them which creates an anchor that lets me associate those two things together so let's take a look at what that looks like so in my wmr spawner menu I'm able to spawn an actor the same way that I normally would in Unreal Engine I'm I've stored this variable locally here from another class so I know which one to do it's part of the bigger logic here but it's fun in a location that is specifically where I told it to spawn I'm basing on a little preview and then after the fact I'm calling this function call called create named AR pin now what this does is it creates an AR pin in the space and it creates a WR anchor at the same time based on a name since I'm dynamically building these things I want to give it a special name that I can use later and so I've got display name and I'm adding a timestamp to it I'm saving that down into this era pen and that air anchor and I'm appending that to the world transform of the same exact spot that I spawned this character so they're right there and exact same point and then I'm pinning the component together you'll see I'm able to grab the actor that I spawned grab its root component and then pin it to that spot and that's what keeps it in space in that runtime now you see here I'm also adding it to a local array of active pens and the reason I'm doing that is because I just looked at that monitor those time there sorry the reason I'm doing that is because you can build anchors and pins and pin things to them inside a specific run of your application and then if you reset it you can have them wipe away you can have them persist only for that for that session or you can save them down so later when you come back those things are all ready for you to go again so let's take a look what that looks like the most thrilling gif here save pins button magic so my saved pins button what that does it's accessing the the active pins array that I have currently stored I'm able to call just safe Arup into wmr anchor store and what this does is it takes the information about that transform that we've created the anchor saves it down into the device this could be queried later and get it whenever the device turns back on when the application is run again and do something fun with it so what does that look like what is something fun to do with that see so now I can come back through on a subsequent run through here and I can reload from pens or responds all the items I had here before so before I had spawned a drone a drone and a robot and place them in the space here and I'm able to then now go through load pens and bring those back up but we're talking about the doing this in the cloud in the future yeah yeah so we have a facial anchors which post post-launch will be exploring adding all of the benefits to the mixed reality toolkit into unreal and with that a lot of the the Microsoft services that you might know and love from from existing air stuff that you've done and so spatial anchors in Azure I can be uploaded to the cloud and then viewed with a whole bunch of different devices as well cool all right so let's take a look at how I'm responding all the dynamic information based on that pin information so I'd like to my spawn I mean you see go straight load guns so when my load pins button is selected there I'm calling this load WM our anchor store pins and I get a response of w WM our air pin objects I'm important to call out here that this isn't always readily available right off the bat there's a quick call that says anymore anchor store ready so we can call is WM our anchor store already and if this returns true then it's good to go right now you would set this up on a timer and just check every once in a while to see if it's right it usually takes about half a second after the first time the application loads eventually we be moving this into an event that you can bind to and just say hey once they're ready then we can go ahead and spawn these things and get them ready but just want to call that out I know I'm not doing that in this project right now so as I get all these back through here I'm able to take the array element each pin that comes through and since I put the name of my class when I spawned it into that anchor ID I can do some quick checking here to see if they contain pacific strings so so I'm only responding I'm only able to spawn two specific classes a biped in a drone I just want to make sure that string contains by a pet or drone and then I'll do the appropriate the appropriate logic after the fact so you'll see biped path sends me up here and I will spawn after biped I'm actually spawning it at a transform that is in the middle of my world doesn't really matter where it is and I'll show you why in just a second it's right after that we saw the pointer to that pen it's coming across from that loop and I just like before I'm pinning our route component to that pen basically what that does that moves the component from zero space to or that pen has and rotates it appropriately never get a drone it would do the same thing here now this is a pretty rudimentary example on how you would build dynamic content that you would save back down I save down and then reload you probably do is in like a real production environment is have some sort of dictionary that uses a unique name for each individual thing you would save off into the Unreal Engine safe system you could then pull up specific information based on that key like what class am I supposed to spawn are there there you know initialization settings that need to happen or other delegates that need to be fired other places you could do that and that's how we get the that's how we get the result you see here where everybody spawns right back where they should yay all right so so we know a lot about the space the headset knows a lot about the space you're working in how do we build content that knows about you as the user a little bit more information than it currently has one thing that we found that as a lot of people like to like to do is care about where the user is located and where they're kind of looking in the general area because if they're if I'm here and you're behind me there's no really no real reason for me to interact with your content or even waste cycles on running any procedure that is on you so one thing that comes up quite a bit in conversation is working with things like track space and world space and it could be quite tricky to understand the differences there or get you can get caught up in the weeds of where we're actually working in what we try to do is make sure that we provide a consistent place for you to always get at user and the content relation to the user so the important piece here is that the parent of our camera component is the mapping between world space and track space and oftentimes that is inside your pond in this case it's mr pond and you see default route scene so do you fault route scene is your pond it's like the the default piece of your pond that you never want to use it mentioned earlier but in general in games you want to sometimes move your pond the controller actually moves the pond through your space if you ever worked in Unreal Engine before for a game it's always that you drive your player character forward it's a pond that moves in the scene in this case we actually have the pond alone moving the pond actually changes the relationship between track space and world space and the camera itself as a as a as a sub object of that pond it kind of moves as the user moves so you'll see here in a minute I'll play this in editor and we'll drag the camera around you can see how the concert reacts to it so you may have seen in the demo that the robot was doing something a little bit special with us he's able to follow us around point the gun at us and based on where we're currently standing and looking at him and you'll see on the one here on the right once I move it behind it it loses where I am until I come back around so it's using the location of the camera to know where I am doing some quick vector math and then trying to figure out how to rotate its animation to point out or to not point at me let's take a look what that looks like so instead of iPod biped bot here we're taking some world locations of two and important pieces here one we grab our player upon which is our default routine and then we're getting the camera component from underneath that and we're able to get world location of where that is that represents me as a user in this case and we're also taking the revolver that he's holding in his hand the revolver mesh we're getting the location of that we're able to do some calculation here to get the actual look at rotation and then we're going to pass pitch and yaw into a couple of local variables that we'll use to drive an animation blueprint now I'm gonna get into animation blueprints today but effectively there are ways that we can use data on a specific class to change things dynamically inside of an animation in this case we don't animate the entire robot we're just telling the robot to blend between spaces based on these two variables doing a little bit of you know fudge math here to make sure that we can get it the robot we're pointing exactly where it is because it's gonna change just a little bit based on where it is in relation to mesh that's less than important right here and then the second image here you'll see I'm able to take that Y value that I have a storing locally and make sure it's in range between 4 degrees and 180 degrees I use that as well to set this should fire this is also driving animation blueprint to drive in from the vs. I'm pointing at you it's kind of neat let's so again that's other thing so about where the user is looking itself the drone does the same follow logic that the robot does except we're applying it to the entire object as opposed to driving an animation and you'll see here right when he enters the very center of the viewport he does this little aggressive a Moute because he knows that I'm looking at him and it doesn't like that my dog you don't look it right nice so I don't get that information right back over to the project instead of looking at the actual drone object we're gonna start from the actual pond and the camera I'm gonna function calling here called check hmd look this is also called every frame than this again might be a little overkill for an actual production project but just to prove it out just wanted to show it here so this one we take the camera component as before we get the world location gives us right where the user is and then we also look at the forward vector which is specifically right where that camera is pointed and we do some quick vector math so you build a line trace it's 300 units out from the users face line trace and Unreal Engine is basically saying hey I'm gonna shoot this out and if I find the do you think I'm gonna tell you what I found in this case we're looking for a specific drone so hey we got a hit let's go ahead and see what the actor wasn't that and then if you find a drone let's go ahead and tell the drone that the player is looking so it can do the appropriate thing so forward vector gives you where the user is looking forward vector plus well location gives you the exact location where they're looking from and we're able to just add some ad units to that forward vector to actually get a linked out to actually to Atlantis so let's actually go ahead and see what this looks like all in the editor another thing that's kind of cool is if you're working in a lot of this content in Unreal Engine you may not even have hardware that you can test this separately on I'd say that probably 70% of the work I did here was not on device and then throwing it on device to make sure that things reacted the way that I wanted them to so I can play an editor here and you'll see my menu pop up like before and I can go ahead and just throw some debug commands are here so I'm gonna go ahead and spawn up a robot drop him into space then I'm going to eject from my pond here it's clanking back it's just a little bit like this he's currently pointing at I feel like for my pawn over here grab the camera component it's pointing right at that camera component transform within tolerance right my fudged math is just a little off that's okay robots aren't that great a shout-out anyway and so you can see as they move this it follows me all right I'm able to make sure that using just those components without without a headset without anything here I can drive that animation and make sure it's doing exactly what I want and get behind it and losses come back you're on disease it's like you know come over here gonna repossess now that I'm next to it I don't see any more I'm going to one-up drunk so put a little drone robot here too and pop back on you saw him do the aboat in front of me able to again come in here and grab the actual camera component snapped up there it's underneath the pond I can rotate it rotate rotate that around move that around to actually emulate what a user would be doing if they're in here and you can see every time it points directly at the drone robot here he will do the emot-- if you set up the local remoting like we were talking about before the next step would just hit play an editor and it would connect to the device immediately and then you could test it right there or that have any do need to play step arena this is what is dreaming yeah yeah super awesome so you might also see that I've got this menu floating around following me too and that's using a lot of the same techniques that we use to check if the robot is being looked at except instead of telling the row about to do something and the hmd telling me about to do something the menu itself is looking for the camera and gonna say hey I need to follow her that cameras going so let's take a look what that looks like kill a session where are you follow player great so it's like before we have our rotation calculation that we've used to drive the robot's rotation or the robots animation and we use that to drive the drones rotation and that comes from grabbing the scene component which is just our menu getting the world location of that and then getting it a world location here of our camera and doing a final look at relocation and just driving the yaw we don't really care about pitch roll in this situation and that's gonna pipe into the new rotation setting for set world location and rotation of ourself but what about making it follow the player itself in actual locations as well not just looking at that we're able to do the same thing where we can grab the pawns location and forward vector and then do and then add or multiply by a desired distance from view or I think I have two sets of 60 so what that does is it makes a new place right outside the user I'm adding that back to the location of itself so that's basically sixty units in front of the user and then I'm using a vector interpolator to move the actual menu from where it is to that new target location over time so who's who's familiar with looping the interrupts it's great so if you're not basically what it does is it move something over time as opposed to immediately from place a to place B or I guess variables it doesn't matter if it's moving or whatever but in this case it's moving this menu from place a to place B over time or smoothly that way you don't have any noise or jaggies from the the micro differences and vectors over time if you didn't do this you're just trying to set it directly you might see a little bit of movement because as the player moves in very a little bit at a time the menu itself will also move and try to adjust based on that new location B interpret Jeep not super expensive but they're not cheap so you're probably when you use it for everything we don't use it to drive animations or drive the rotation for anything else mostly because everything's so far away from the player even if there was some noise in it you wouldn't notice but since menus are kind of right in your put right in your face sometimes it's easier to do this it also has a little bit of a floaty effect too which is kinda nice it's also a feature called stereo layers that we're going now which allows you to kind of achieve the same thing but leverage just got the platform to put the quad in the right space and so they'll be another way of doing the same right yeah and you'll see right here here's our menu right a little Collider boxes and all the things that you can touch out there already neat right or the bun yeah there we go great alright so back to deploying to device say you've built this project and you want to put some robots in your desk and save them it sounds like a lot of fun to me I've been doing it for last couple weeks what are the ways that you can test this first and then build a package so you can share with friends or other ways you can do your iterations or just build a project let's mention before we have streaming and then we have just the native support you want to talk us through a little bit what that looks like sure so you've got your string up and like I mentioned before just put in the IP of the device and get go that's essentially it obviously that sticks to your device and your network connection so if you actually want to deploy natively on device you have a couple different packaging options there's package for basically sharing a distribution that's gonna basically produce a binary that you can then install on the device through the same tools you would use just on the other package on the device and then there's also launch on which is essentially the same kind of idea is the the plain-error streaming or yudice press a button and it goes but in this case instead you'll actually compile everything install it and hook up all the debug channels so once it launches unreal is connected to it and you can get four information until i'ma trade back from device as it runs so I'd say most people would start with streaming just because it's really fast iteration and then from there probably moved to launch on so you're actually getting debug information as you work on your project and then eventually package for distribution yep yeah the streaming is great because you get the quick iterations to say my logic works everything I expect to do is doing what it's doing and then launch on lets you optimize for the actual platform that you're gonna be working on you can look at it and real time right there with all your outputs and everything there - and then packages for when you're sharing it to other people so they can break it into new crash dumps and think of adding this week it's it fun so let's talk a little bit about other features that are currently in the works there were cranking on right now have been for a while that are on the way and some of the timelines and those things and this ins Ryan's that's in say I'll just leave you with the laundry lists because that's essentially what we are going to deliver initially in the preview which I said earlier at the end of the month and then we'll continue to hammer on these and update them up until they release at 4:23 and our generally these process is there'll be a bunch of preview releases coming kind of midsummer as we kind of iterate and get everything in the right place and eventually 4:23 to be the main release and that we support going forward which I think both those dates are on there then that's promised some resources for you to go take a look at if you do ue4 definitely check out our documentation needs some of our getting started resources but then add a you should be able to be building some cool things in blueprints based on some of our tutorials a really powerful tool for designers and you know keep blip programmers to use and if you're wondering to you all the blocks we showed today for wmr where a blueprint blocks all of the api's are also available in C++ we call those blueprint calls in C++ is quite a bit anyway so we'll have this source at época GM /wr youíve or source whatever it's live again that'll be later this month so if you go there today you'll be disappointed and I will be disappointed do the same thing with this example project well we'll have a copy that's up there to you so you kind of take a look at it anything you missed today I want to try it out and just see what else you can do with hand tracking and hopefully we can add some more features to it too over time if you want to get started you could if you have a device has similar capabilities you know W market for VR or another device you could start on that now kind of build out the baseline feature set and then when this comes online you get access to an actual holland's - you can kind of add that extra layer to connect to the unique features on that device definitely and of course you join the forums chat with other developers about all kinds of different things as a extended reality specific and sub form there or there's people that have been doing similar for quite some time you can help others and get help over at the answer hub oh wait a timer you got a little over ten minutes if anybody has any questions [Applause] 