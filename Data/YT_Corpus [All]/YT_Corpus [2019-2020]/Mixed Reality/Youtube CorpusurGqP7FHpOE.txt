 take it away hi everyone my name is Bala and today I would be presenting a research or research work done at Autodesk research loci which is a system for facilitating remote instruction of physical tasks using bi-directional mixed reality telepresence so for a long time physical tasks have been taught and guided primarily through in-person interactions it allows boat users to take different perspectives to learn and teach using the different tools and objects present in their shared environment nowadays learning and guidance using video calls are more common but teaching through this is hard they may not give the users the freedom of different perspectives and the flexibility to adopt different styles of instruction mixed reality style guidance like this would be useful in live instruction say a learner's shown here wants to learn a very specific task from a very specific instructor who is far away how would they do it the spatial and immersive nature of mixed reality offers some promise so in this work we explore on how we could better design and use mixed reality systems to support this we specifically introduce low key which is a two-way symmetrical EMR system to simplify the convention here anything belonging to the learner such as Jeff space avatar and annotations are colored in green and that of the instructor in orange and they're situated in two different locations in loci both users who are a mixed reality headset the headset allows users to either be entirely in virtual reality or be in a pass through augmented reality next it provides users with 2d videos that are the current standard norm for teaching physical tasks along with the control UI that each of them can interact with besides the standard tools loke provides each user with depth and spatial data using point loads this helps them choose their own viewpoint and enhances the 3d perception of the tasks these point loads are captured by depth cameras like Kinect that are placed in corresponding spaces so whenever a remote user views your space they are represented as virtual avatars in your space so for instance if the instructor is viewing the learners point load here they represent they're represented in the learners air environment using an orange avatar this gives an idea about scale position as well as macro gestures of the instructor in the contact in the context of dad part of the learners environment that the instructor observes this can happen either in augmented reality or in virtual reality where the instructor only focuses on the learners environment and interacts with it in an immersive manner similar interface exists for the instructor to look he also provides annotation tools like for example one can annotate a live point cloud shown in the left and these annotations appear in the space with that associated with that point load and finally recording playback tools allow the instructor and learner to rewind in time and review a performance together in a shared virtual reality space it is important to note that both the instructor and learner have access to their own such interface so loki is symmetrical and bi-directional so from viewpoint of users a learner this is what the loki interface would be like with a local view of your own space in green a remote view of the instructors face in orange and an indication of where the instructor is looking at your space with an avatar here's the screen capture of the same from the actual interface this picture also shows two different kinds of annotations that one can make in loci one on their own physical environment that appears a solid lines and the other in the hollow glyph that appears as outline lines here is a quick snippet of loci in action in which an instructor is teaching sculpting to a remote learner who first observes it in virtual reality you can see that the remote learner can move around an instructor space and choose different viewpoints the instructor sees them as a green virtual avatar in dead space the learner first carefully observes the instructors actions and once they have gotten an idea of it they then switched to an augmented reality view and begin to work along with the instructor while using their point load as a reference on the side let's now look into some prior related research work still okie early researchers have addressed this bigger domain of guiding and teaching physical tasks using novel interfaces such as augmented reality virtual reality and other model it but these rely on a synchronous learning and tutorial generation and many of these focus only on the psychomotor phase of learning remote live guidance of physical tasks is not not a new domain either and prior works has shown the need and value of extensions such as tracked objects spatial annotations and access to multiple viewpoints but these approaches often rely on just a single modality of data such as 2d video and annotations and the spatial nature of this task is often lost or reduced recently improved software and hardware has made it possible to map manipulate and transmit in real-time the 3d spatial data of a scene this provides an additional channel of information and recent works have used this information for the purposes of general telepresence this works show value and promise in using mixed reality for purpose of live instruction though these works do not specifically focus on it and also do not offer the flexibility to easily support the different stages of learning now what does it mean by the flexibility to support different learning stages and why is it important : settles model of cognitive apprenticeship talks about the changing role of an instructor in transforming a learner from being a novice to an expert it starts with the instructor modeling a desired action while the learner observes then getting the learner to perform coaching them and offering feedback gradually staff holding the tasks to build and remove the support as learner performs and learns the tasks then the learner articulates their gained knowledge finally they jointly review their performance and allow them to reflect upon it after that the learning still continues for the learner as to independently explore their newly learned skill sets while this seems unidirectional it's important to note that in practice one may switch from any stage of learning to any other stage and a live instruction system for teaching physical tasks should be able to support that but current systems only support at most few of these stages and also do not allow for easy transitions between them in low-key we allow users to switch across these for instance consider here a learner who is interested in learning guitar they view the instructors live point load and annotated to ask queries instructor sees their learners avatars and and discusses the relevant guitar instructions with them they then switch modes in loci so that the jam and practice together live while having their partners point load in full scale placed in front of them the instructor here's an incorrect note so they stop the learner and together they switch modes to playback the learners recorded point loads and discuss it in a shared virtual reality space but how do we generally design such a merge systems that allow users to flexibly adopt different styles of instruction to answer this question in a structured manner we build on prior works and formulate a design space that would enable different styles of remote instruction our design space has four dimensions first is the perceived space in the process of instruction a learner or the instructor might be interested in seeing either their own local space or the remote partners space next is time the time dimension refers to when the data was captured the data could be live data in which case the users see a real-time view of their own local or remote environment it could also be a recorded one that allows for collaborative review and reflection of past actions the display configuration refers to how the users can see and interact with dead space in this work we explore two such means one is augmented reality while which allows presenting virtual elements and data in the user's physical environment and the second is virtual reality that eliminates distractions from the real environment other display configurations are also possible but outside the scope of our current work finally the data modality dimension refers to the type of data collected and used to convey the information one a spatial data captured using which the users can explore the 3d nature of the instruction and other as high-resolution 2d feeds again there are many other data streams that can be used but in this work we limited these two it's important to note that both the instructor and learner have their own search grid from learning theory we infer that interfaces for both the instructor and learner need to be flexible enough to support these multiple different modes of interaction let's now look into some interaction experience of Loki Loki can be spatial information using hollow glyphs hollow glyphs contain point loads and corresponding 2d video of a space they also contain any annotations that are made in them the point loads can be placed in the real environment implementing an augmented reality style usage these hollow glyphs can also be positioned scaled and navigated using VR hand controllers different video feeds can be accessed by clicking on the video and when a remote partner views your live space they are shown as an abstracted virtual avatar locus UA also allows toggling between an augmented reality view and a VR you using the control UI in low key annotations made in point load appear in real environment and vice-versa now let's see a sample usage of loci for the remote peer learning of a specific workshop tasks this would be an example of coaching a novice user here is trying to make right angle joints but they are unsure of which tool to use so using loci they call the remote pair for help the remote pair uses their control UI to switch modes to virtual reality and then jump in jumps into the learner's environment and VR to examine all the tools and point the learner towards the right one so in our design space the instructor here first needs to understand the learners issue and their environment so they need to perceive the learners environment which is remote to them while the learner needs to see their own local environment this is a live data dat bawt user c so both of them operate in a live mode of the time dimension the instructor operates in VR to focus only on the relevant spatial data and in a 2d video of the remote environment this prevents distractions and occlusions by the elements of their AR environment and the learner operates in air viewing the entire spatial rate of the local environment directly with their own eyes depending on the nature of coaching this configuration could vary now the instructor understands their learners issue and identified to them the right tool to use but the learner does not know how to use the tool so they invite the learner to their environment and show a variety of right angle joints that they had previously made in the own workshop space now they need to teach the learner on how to use the tool that they had pointed the learner towards to do that they use a tool in their own environment that works in a similar fashion they first explain how it works and modeled actions that the learner needs to do in order to create the joints if the learner has any doubts the learner makes any required annotations and ask them verbally these annotations appear in instructor sierra environment along with a learner's avatar that represents the learners point of view now this is an example of a modeling phase here board users still deal with live data but now the instructor perceives the local space and this an augmented reality whereas the learner perceives the remote environment of the instructor and gives them in virtual reality now consider different different tasks of teaching to scuds remotely in the modeling phase the instructor sculpts and the learner gets to observe now the learner has some doubts to ask both supporters and switch to a collaborative VR space which has a recorded Hollow glyph the learner first navigates to that part of the instructors recording that they are interested in then both of them go to the 3d sub space from their viewpoint of interest they both then review and discuss the sculpting process the instructor used the annotations here act as a tool to facilitate this discussion this is similar to the reflection phase of learning where we review and reflect review and reflect on prior performance the data being dealt with is recorded and a scene in VR using point loads and 2d video it can be seen that even within a single training session the ability to switch fluidly between these design space dimensions is very beneficial and Loki allows that besides these two scenarios we used located with two other scenarios one being teaching of Gita that we mentioned earlier where we carried out stages of modeling articulation and reflection and the other is to coach a baseball bat swing where we carried out the learning stages of active coaching reflection and scaffolding now let's quickly go to how Loki's implement as we now know there are two spaces each space as a yvr system am enabled with an pass through augmented reality using jet mini stereo camera pcs of the two spaces handle all the computation and our land connected and a pair of connects depth map depth connect depth maps 3d maps each space and transmits it with the help of Microsoft's aroma life toolkit and finally Wireless wife 3d position trackers are attached to connects and allow dynamic real-time setup of connects to produce a coherent point load this is the entire set up of loci we are elevated loci and utility of these mode transitions with an informal qualitative evaluation with eight participants who used loci for 30 minutes each and learnt a form carving tasks to create the 3d pyramid from the user study we found that participants appreciated locus ability to combine benefits of different modes and features such as annotations point loads live and collaborative review and like the ease at which they could transition across them more details of the study has been discussed in the paper in summary in this work we have presented a design space for exploring the design of systems and interactions for mixed reality based live instruction we then contribute a system that allows users to operate and transition to different configurations of the design space and thus better support learning at different stages in future there are three main verticals that we think could be interesting to explore currently annotations are for are useful for static objects but with moving objects they become misaligned exploring spatially and temporally consistent and consistent annotations would be interesting scaling use low key to multiple user interactions as well as exploring other display configurations and data modalities would be interesting vertical shakes load with this we tank royale for being a super user of low key and helping us with testing and carrying out different low key scenarios used in the paper we thank Justin for assistance with figures and for feedback at different stages I would not be happy to take any questions okay that's a really cool project thank you so please ask questions we have a little bit of time hi very cool talk I was wondering can you comment on the importance of visual quality and visual coverage so you've done all these other studies and I think there's a lot of cool stuff there and I'm wondering does it matter that we still have a long way to go before it actually looks good I totally agree right so currently the resolution of data offered by depth cameras is not very precise enough to understand fine grained physical activities that are like that involves taking a few millimeters of resolution but it's useful for activities which involves larger objects and objects that are not reflective because reflective objects are currently not very well mapped by depth cameras that's definitely a long way to go but I believe that this set of interactions would still hold with the depth cameras of the future thank you so much for a nice presentation so my question is the have you measured the users satisfactions like when they are going to use the rookie so how do you feel or how I haven't done any anything about that so we primarily it was a qualitative study so we I think the the goal of the study was primarily to measure if Loki if users were able to use a Loki system to successfully carry out a task and that they were able to do they were able to do that and they were able to successfully use a Loki a particular feature that or I mean in general the feature that users like the cloak is the ability to transition across these different modes right it's not just one user is continuously teaching and the other users confessing listening but the the interactive experience offered by Loki is something that the users liked okay thank you so much okay [Applause] sorry huh yeah um okay that was very very cool thank you yeah 