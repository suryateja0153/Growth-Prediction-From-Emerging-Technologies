 thank you very much and so welcome to this session I hope you had coffee so you don't fall asleep after lunch and this is a short paper so I will take you very quickly to what we have done in this short study the project was run by me supervising Elena on her undergraduate studies she's graduated last year with a bachelor's degree in computer science at the University of Cyprus and myself at the moment I am at the Open University of Cyprus the Cyprus Centre for algorithmic transparency and I'm also collaborating with a new Research Center in Cyprus rise which is dealing with emerging technologies hence the topic of this interaction of this presentation on analyzing user starts driven interaction in mixed reality so most probably you're already familiar with the literature on this topic on mixed reality however just to give you an idea if you're not mixed reality it deals with combining the real with the virtual content so we see both at the same time with we can interact in real time with 3d objects that are interacting themselves with the environment the physical environment and the motivation behind this study is what we were discussing actually today during the panel before lunch and what some presenters yesterday and the day before mentioned that we really need to take into consideration the individual differences of each user and in these demanding visually demanding environments like mixed reality environments this is very important so we wanted to see whether some differences cognitive and visual differences that exist between people whether they affect their interaction in a mixed reality environment so since this was an undergraduate project and we had limited time we focused on the construct of feel dependent Panda and for those of you who are not familiar with this construct a person who is feel dependent demonstrate a holistic approach when he or she is processing information especially visual information and it finds he or she finds it difficult to extract simple objects from a complex surrounding a field independent person now it's the completely opposite so it's following this person follows a analytical way in processing information on especially visual information as I said before and finds it very easy to to extract concepts that are of interest within a complex surrounding so this becomes very important for mixed reality interaction design and this is why we wanted to to explore this construct so what we did we used microsoft hololens to as the the device and we employed the galaxy explorer application for two reasons the galaxy explorer application is a very popular one with hololens users and actually it was voted the most worn an application to be developed by equivalence developers for the public so we thought that this would be interesting to the participants and the second reason is that this is an open source application so we could build a logger within this application to extract interaction data in real time so that's what we did we could extract gaze so where the user looked at which objects the user looked at in real time gestures speech whatever the user was performing during the interaction the study we had 31 subjects 17 male and 14 female aged between 2029 and we wanted the participants to have no previous knowledge of this device or the interaction in hololens which was not difficult since this was very expensive and the device is very expensive and the average user doesn't have this at home so we provided training to the basic operations the gestures that the user needed to perform you know that in order to interact with the environment and from feedback we do code from the users the training provided was enough for them to perform the task then we needed to see whether the person's the participants were classified as feel dependent or feel independent so this is what we were we wanted to control actually in order to see how their interaction was changing and in order to do that we provided the gift test which is validated test from psychology it's been around for years it's a paper and pencil test so it's not available online and the scores are from 1 to 18 marks so what do they have to do in this test they have to pick a simple object within a very complex a schema so we had used participants classified from 1 to 17 and the cutoff score was 11 and this is what is used in psychology so whatever is below whoever is below 11 is classified as field dependent and whoever goes 12 to 18 classified as field independent so overall we had 20 participants s field independent and 11 participants s really dependent the task now the task we piloted in order to be able to identify is D moderate and difficulty levels in this task so the users had to go in this application and I identify objects so one was such Tara's a star earth and Uranus and the difficulty is according to the search behavior the visual search behavior that they uses user has to perform in order to identify these objects okay what was our hypothesis the first one was that there is no interaction effect between the field dependent independent and the difficulty level towards the site time spent for each user to identify an object so what we got out as a result is that at the easy and moderate levels we didn't have any differences between the two however when the difficult our level came up the field dependent will were much slower in achieving each target object compared to their field independence and the second hypothesis we had is regarding the objects they viewed before they achieve the target object so how many objects so this was basically giving out their strategy so again at lazy model moderate levels we didn't have much difference but the difficult level was obviously significant differences the listicle significant differences so ok statistics confirm what we basically thought that was the case however we had also in up video recording so we were able to record the interaction of the users so we we did some observations by analyzing the videos and also observing the users performing the tasks in real setting both Helen and myself were present during the the experiment so the feel dependence they were very frustrated when they had difficulties they didn't have a good experience so their psychological aspect of the user when was interacting was obviously and not not a good one however the field independence were very happy with it the feedback we got was that I yeah that was very easy for me and they took much less time to complete the task and to locate the objects that we requested them so the results that we got out are aligned with the literature in other environments so reports that we got from TV for example viewing or 3d environments on desktops and so on so the the results are aligned with the literature however we want to run further further tests and we have already run three more studies to examine other aspects of the cognitive abilities of the users and the dividual differences when interacting in mixed reality environments so before I close this presentation I want to bring this up professor Samaras used to be a keynote speaker in 2017 IUI and he was the person who provided us the hololens we're under study under his lab he passed away last time july he was supposed to host you map this year in cyprus co-chair with george papadapolis however he's no longer around thank you very much and if you have any questions [Applause] okay yeah the difficulty level was actually how that the effort that the user needed to put in identifying because the I don't know if you've used hololens and the application so the planetary system comes in front of you and you have to actually perform some visual search to locate the object so Uranus for example the difficult object was located somewhere here so the user needed actually to navigate in the physical environment in order to be able to identify that object and before we actually set the difficulty levels we run pilot studies so we had data that confirmed that these were easier difficult and moderate levels in me yep actually yeah in the paper I didn't have time to discuss it now but in the paper were provided providing some implications in the design actually so this is the motivation of the research anyway so maybe fee dependence mostly they could benefit from some timely adaptive and notifications for example because we we saw the people searching around and they were making these movements and it was difficult for them to locate the target object so maybe there was something like an arrow or a message or even a vocal thing a speech or something that would guide them that would be much better for them the field independence now they are usually more fluent in these environments due to their ability however they seemed satisfied but uninterested so maybe challenge them a little bit but yeah we need further studies in order to be able to yep I was just wondering how exactly you were able to track a user's case because from my understanding um the hololens doesn't have the capability capability to like track actual gaze it just more or less tracks where you're moving your head yeah have your head stable and you could actually be looking at some other objects that you would miss out if you weren't doing like eye tracking the way you select objects in hololens is through your eye so you actually when you look at something this is what hue is like the mouse like you you use the mouse to select something this is how you select objects in hololens through your your gaze so you gaze up to them and you select them it's not like I tracking gauge so it doesn't give you more semantic information but it gives you the target object so the user targets these objects so you can get that out of thanks for your intention so I was wondering since your tasks need users to search and locate some targets so to be major their specialty you know how they rotate you know the image and how they used the users rotate or the user's mental ability to rotate in objects since you had yeah we actually one of the tasks for them was to you mean to rotate objects through hollow lands or to rotate themselves around we call special ability the way oh yeah we didn't the their spatial ability I mean the themselves to move around no we didn't track that thang 