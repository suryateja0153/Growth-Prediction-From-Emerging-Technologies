 good morning everybody good morning twitchy's let's hope we have good enough wi-fi for streaming wi-fi for streaming wi-fi for streaming wi-fi for streaming you guys see that with cartman glitches out on the zoom call uh all right so i was gonna do something today it was nice and peaceful and i thought that i would share it with you streamers um i got tea prepared for me by the by the lovely alex made me tea i'm very happy welcome my streamies welcome my streamies as long as i'm not you know blocked like the new york post story you guys can all come in here and watch me stream yo what up all right so you know we're getting back into how we're gonna do some coding uh and i guess love when i do coding we're going to start we're going to try to write mnist we're going to try to write deep learning from scratch um and we're going to see and you can all watch me embarrass myself with how little calculus i know uh so i've already done a little bit of it this was done off stream and i'm like you know what this will be a nice stream for everybody here so you know i just went to the mnist dataset page um and here are the files that are available i show this little fetch function uh which here we can restart this kernel here we're going to do it without pi torch and without tensorflow we're going to do with nothing or this little fetch function here which uh you know will fetch them and then we can like uh you know like an m show one x train oh look it's a five blow yo let's see if it's a five yo look at that it's a five all right all right we're doing well we're doing well all right cool um we're gonna try to really we're gonna try to write this entirely from scratch um and we're gonna see what error rates we can get uh so who knows how to write back propagation without using a library maybe if we should first write it in the library hype train incoming wow that was fast all right you know what let's first guess just start and do a little pie torch just because we can uh so what am i going to call uh my net we're going to call it bobnet uh let's say what is it torch all right so let's just let's just first let's just first write it in torch you know uh we'll import torch nn i'm gonna write bob net as a torch uh let me just make sure that i'm not blocking that part of this screen i'm actually going to move me um what do you guys think we're going to move this over here and we're going to move whoops i'm going to move me up here yeah there we go i think that's nicer cool all right i moved i know i moved um so yeah we're going to start with a little torch knit you know it's just it's just relaxing on a beautiful saturday i went to blue bottle this morning got some nice coffee super bobnet self and like you know what doing these doing these old doing these like simple exercises it never gets uh old so we'll just say you know we'll call it l1 and then.linear we're going to go from 784 down to let's say 128 and then we're going to have an activation function we'll use a good old value and we'll have another linear layer which takes us from down to three and we'll use a logarithmic softmax so that shouldn't be a problem we gotta write the forward pass you take in an x you apply layer one to it what why is that this used to be better i feel like i used to hit that and it would not do that it would just go back to the beginning of the line but whatever um all right so we can instantiate a bob now we'll say model equals bobnet all right now i can run my model on say now we gotta do a lowercase model one say x chain one through ten okay forward takes one positional argument but two are given i forgot the self you always need the self don't forget about the self people uh empire ndra has no oh we gotta turn into a tensor all right good given numpy array is not writable and pytorch does not support non-writable tensors oh that's a tragedy let's just do a dot copy in there because i did it from buffer it's actually from a python buffer so it's not writable in numpy expected object or scalar but got type float oh we're gonna have to turn this into floats okay so let's do a float wow this is harder than i thought well i'm being a noob here all right don't be a noob all right let's go let's go let's go i'm gonna reshape this into that times that all right good so we got some stuff out um let's write a quick training loop for this uh how many epochs do we want for i in range 100 um let's set our batch size equals 32 do something like np random brandon's yeah i can say something like size equals perfect shape size equals 23 size equals bs bs is not defined oh no um we'll say samp equals that let's just do two for now print samp okay good so those are our samples so we'll just sample from the array um we'll call this just x uh we'll run the model we can't just write it on x we gotta run it on that hmm that pndra is not oh okay we'll say y equals y train substance all right so we have our out equals this um and then cross entropy loss and pi torque yeah this i think i just want to say loss out why is that exactly what i want um we'll call this loss function first we're going to write it in pi torch and then we're going to try to replicate the same thing without the use of pi torch and that's what's going to end up taking too long and be too hard for us to do on the stream into object is not callable who's trying to call an int who's calling an inch don't call ins oh because i gotta make that a tenser yeah you know what i'm actually going to move this here and then here we're also going to say torch tensor y chain same i didn't get that could you try again no siri bad siri sorry no siri bad siri okay uh what do i want this to be a long can make it long yeah it's long okay cool let's check our loss all right perfect so now we just got to do backwards so we'll say lost dot backwards we're going to need to write a uh we need an optimizer and then pi torch atom yeah torch optum is something like optum equals torch dot optimum dot atom sub model dot parameters that should work uh and then oh we're gonna have to zero the gradient if we didn't zero out the gradient did i forget to do zero grad in one of my examples i think i did that might have been why i had such a problem learning models that output two but that was different okay um import from tqdm import train let's make it a train uh and then actually oh who's ready for some walrus oh no don't tell me i only have python 2.0 no no no no what python version do i have right now get python version oh you know you guys know i love the walrus operator um you know let's just try a walrus and see if it's going to work come on walrus i believe in you uh the walrus is my favorite operator and anyone who doesn't believe in is a wire yeah i got a walrus all right so look now i see you know i can just use beautiful things like t dot set description let's say like loss 0.2 f like that yeah i know there's like a better way to write that in python now uh dot item oh yeah wow that was so nice oh i love walrus operator uh who loves walrus operator this guy all right um let's also check the accuracy so let's say uh torch dot arg max out uh dim equals one say cat equals that let's not do that let's see if i wrote something right okay um and then what we want to know with accuracy is cat equals equals y and dot sum accuracy is that uh now we want to actually need on that oh can only calculate means on floating point oh i gotta make them floating points okay good they're floating points all right good accuracy and we'll say here accuracy all right i can do that right perfect we want to make some quick graphs up i know you guys like graphs who likes graphs you like graphs graphs look good for the youtube right let's go let's go let's go call it losses accuracies equals that comma that say losses dot depend loss accuracies dot append accuracy and then let's just throw a plot down here plot loss plot accuracy oh no we got plot losses and accuracies all right yeah buddy let's get it all right so let's go for a thousand steps 32 is a good batch size run train train train train train train train train train loss goes down accuracy goes up oh let's not let's clip that too can i say plty limb well accuracy doesn't go higher than that so let's just say like minus one one it's all good yeah cool but one's a little low let's go to two all right cool beautiful loss goes down accuracy goes up everybody's happy right is everybody happy good i'm glad if everyone's happy you know you know i'm happy and we're all happy all right let's lower that only to 100 boom boom let's go okay good wow accuracy goes up so fast oh it's unbelievably fast 300 400 let's go um okay cool so we wrote a little trainer in pie torch and then let's write the final evaluation who can write this quickly model um i don't want to copy that code we're going to copy it let's do it model torch tensor x test reshape float and then we want to take out and we just want to do an arc max on that m1 let me say y test threads uh let's just throw a dot num pi on there so we're not testing it in torch y test equals equals y test threads dot mean i think that works in numpy cool oh we got ninety percent good that's an a all right who's glad we got an a i'm pretty glad about my a all right evaluation um training model all right model training evaluation no we only got an 88 that is more mediocre i'm not going to write adam let's change it over to sgd oh we got to give it a learning rate for sgd i can't use the default one is that a good learning rate that was not a good learning rate that did not learn um did not learn okay let's try a tiny learning rate let's actually initialize bob net up here so we don't does sgd even work we got to give it some momentum too you know what we're going to do without momentum because we're going to implement this from scratch in mnist from scratch um you know what let's just be explicit about our lack of momentum no momentum that's right okay how well do we do well we got an oh that's a b it's a b it's a b plus i've got b plus i don't know it's been a while since i've been in school set up b plus oh 91 oh that's pretty good let's up our bs we need more bs just like you know all right let's go let's go let's go oh 93 oh that's a pretty good number who's pretty happy with that all right that's too many let's do less steps 91 i do a thousand steps i get it more all right how long does that take only a few seconds right we can be patient 92 all right who's excited we got eminence can i do losses common accuracies no this is going to complain to me isn't it no i should have tested that without having to do that yeah yeah great high quality oh wow it's like losses versus accuracies can i do that is that good i want to save that line you know i don't like writing crappy code oh that's terrible that's not what i want to do you know yeah you guys probably in the chat right now that i'm not reading are probably like oh you just got to put the squiggly you put a semicolon and then it doesn't but we don't have any semicolons my keyboard doesn't have that my keyboard is actually out of semicolons oh that that 2 is aggressive let's go 1.1 and minus 0.1 let's really get a get a nice graph oh that's that's a beautiful graph what do we all think oh that's so beautiful all right let's evaluate it we'll get 93s yo that's an a that's not even an a minus i'm just out of that a minus range i mean 95 would be nicer you know but i'm pretty happy with that all right so that was it in pi torch we're never going to do it in tensorflow because tensorflow is a piece of garbage and we don't use tensorflow around here we use pytorch all right let's just check in on chat before we get started on the heavy math part of this where you guys can watch how bad i am at calculus we're doing mnist from scratch all right you know what let's just try let's try 256 for the bs just quickly is it does it do a lot better no it doesn't do a lot better it does marginally better bob nat bobnet is probably the crappiest uh amnest uh yeah no that's hard we're gonna do this all from scratch oh we got frame drops oh there's no i don't got friends it's just looking pretty good hell yeah math let's go boys let's go do you think we can reimplement this thing without the use of pie torch um the mug is sick right alex they liked your mug deep learning is absolutely not a scam we're gonna win self-driving cars uh i probably totally missed a hype train i'm sorry for missing you guys but without pie torching without numpy whoa whoa well that's like like like giga hard mode yo we're not going that far we're definitely not doing it in c we're doing it in python we're going to do numpy and we're going to show that we know math and we're going to see if we can replicate these amazing mnist results truly amazing mnist results what do i have can i not use requests what do you want me to do and see you want me to use socket want to use socket receive and send do you want me to write an http oh you guys would love that oh you guys would love that oh i bet you would i bet you'd love it if i didn't use requests and i like opened port 80 oh you want me to write some tls negotiation oh yo by the way can we talk about how the walrus is the sickest operator in python and anybody who isn't pro walrus is a loser that's how i feel i'm the biggest python three shell you know you can you can put some clips of me next to me from the past talking about how python 2 was great and now shilling for the walrus and you can be like he's a flip-flopper like john kerry and you guys could do that and wow that's some sick branding if i'm still calling john kerry a flip-flopper 16 years later damn all right the rg flip-flop i carry chat you guys distract me yo yo yo the walrus is bad you're lucky you're a subscriber all right please subscribe to my channel it's only for my love of the subscribers that no that's not i'm not even i'm not even playing this is the new twitch channel i'm just doing this because like i was gonna write this today and it's more fun to talk to myself and you guys give me an excuse to talk to myself lex friedman part 2 coming soon boys i think i did well i don't really remember what i said lex friedman is a great guy people like him are going to save the world hard target for thank you for subscribing um no i don't know when it'll become available lex has a lot to do i hope his social media app can work what i told them about that was i think it's a harder problem than self-driving cars i know how to solve self-driving cars i don't know how to solve human nature um okay i'm never doing jerry all right we got we got to hide the chat you guys distract me sorry sorry chat you guys are a distraction um wow you know you're just going chat you're getting a whole chat mindset you get out of a coding mindset you see that was fun you know you guys could like clip those clip those like 30 minutes and it's like you know what guess what i don't have bobnet written in another window unlike certain scam streamers that i know who write all the code beforehand you know scam streamers don't be a scam streamer you just gotta disclose man like everybody you know it's like freestyling you know this is real off the dome you know and like battle rap is great you know like like the pre-written stuff you know i look like the type to win a pulitzer prize until i pull it surprise and bullet your eyes right like you never came up with that you know in the moment so that's why it's nice that it's pre-written but you know you just got to disclose everyone's got to be on the same page about that kind of right and then we're all good you know no sense as long as their permission preach kanye west all right let's go okay so what is a linear layer does everyone know what a linear layer is oh well actually hmm bias equals false i don't really want to write that bias what does this need about i don't even think this one needs a bias because i'm doing yeah that's fine actually that's an interesting point i don't think i need biases if i'm using well no actually that's not exactly true let's see how the accuracy does pretty much the same all right we're not going to write biases but we should write biases at some point wow it doesn't matter at all i'm trying to think what happens if i do that without a bias if it matters probably does but okay so what is a linear layer without a bias it's just a matrix l1 equals np.array let's first see if we can copy let's first see if we can copy the um the weights from torch and get the same result um you know is known by array normally what type is this in 64. oh well that's not i meant to say empty zeros float 64. that's disgusting numpy float64s are disgusting and you should be ashamed for making that the default type scientific computing is everyone knows float32 is all the floaty wait a second i'm only saying that because of nvidia only supporting float64 and the expensive gpus i've never actually tried training a float64 neural network i wonder if they work great yo you think if we up the accuracy to float 64 everything's just going to become super precise now it probably means you were lazy with your gradients yeah yeah yeah in date works you just got to add noise uh oh yo all right guess what i bought for tomorrow and this is a little i bought oculus quest 2 and i don't have a facebook account so you all know what that means uh but i bought a quest to i don't have a facebook account i don't know am i not going to be able to use the hardware i paid for pogba lucky i would love your five thousand dollars i would appreciate it i would frame that check check from the oculus founder for jailbreaking the oculus yeah that'd be cool all right but enough daydreaming let's focus on writing this okay so we have model i love pytorch by the way if i haven't expressed my love for pie torch you know i can just say stuff like that oh that's so beautiful that's that's so beautiful am i gonna have to detach oh you have to detach numpy you can't trust numpy you have to detach it from the graph first i don't really understand what graphs are but i do know oh wow they got it twisted all right let's get it twisted okay we'll call this the init the network no more by torch here now that's a lie we're not going to lie copy weights from pie torch yeah that's right don't lie um that's not actually what i want to do like how do i assign the does that do what i want is that still the original array uh what there's a python way to do this how do i get the memory address of something in python get object memory address python it's not wrapper id i'll check your id all right okay that is the same id so but now if i do this it's going to have a different head yeah okay so i want to do that i want to actually copy in the weights i don't want to uh not copy on the white side let's go okay um so now if i want to actually evaluate this let's write a numpy forward pass um we'll call it forward x we want to say what is it x dot l one all right it's gonna look very similar to the pi torch forward pass um we've got to write a value um what is it max 0x our np dot yeah that's right isn't it oh is it max yeah it's max numpy's not picky about its types uh what see that's what i mean i'm not sure shapes are not aligned yeah see i don't like that i'm i'm confused by why pi torch has their matrices like that way isn't it really like that uh what do i want to say transpose only integer scalar arrays can be converted to a scalar index um okay good we got some it's not the right shape i didn't even get the right shape out [Music] well let's not make the batch size 10. let's make the batch size 32. okay we got 32 times 128 now the batch should still be the same here what oh that's not what max is oh my god my relu was broken we got to do maximum let's just make sure that actually does what i want let's do a batch size of one for now does my relu work did i write a crappy value well that's a lot of zeros you know what they call that relu death because you know what the gradient is with zero it's zero okay so that now what do i want to do we can just say um y test preds equals uh np dot argmax dem equals one you know see in pi torch it's dim and a numpy it's axis why do they call them different things that's mean of them all right let's go speaking of mean uh that one all right good well look we get the same same results in numpy as we do in pi torch how unbelievable is that boys that's unbelievable all right uh we're not really going to copy the weights from pi torch now we're ready for the real challenge training in numpy because we wrote tested numpy we still got to write training in numpy because we didn't do that yet let's check in on chat and see how you guys are doing also we're not plugged into the wall here uh so my laptop's gonna die and then there's gonna be no more stream and you're all gonna be sad [Music] it's european powers loves european power yeah it looks european no my laptop loves european stuff don't worry build autograph who build autograph now that's that's that's real like like giga chad mode there i don't know um no i know vim today uh is this my mom's basement is that a piggy bank that's the most realistic looking pig i've ever seen is there pagans out of that we're programming the matrix we did do some matrices that's you got to know how to multiply matrices yo should we do it without numpy should we should we make this work without num pi should we program matrix multiplication are we allowed to just use np dot dot like savages or should we program our matrix multiplication so we get the axes correct do you guys know how to multiply a matrix numpy is cheating bro uh but not with default python lists i mean np dot dot is a little bit cheating do it with cython oh now it's going to be slow why don't we just code it and see while we're at it please don't use python what oh yeah check out my american flag spatula what alex everything i have is here i brought you clothes today what am i gonna do go cook at my am i am i crack done uh what's kona do it with eigen oh yeah that's c plus plus you know what's k kona now no we're going to write we're going to write k kona means american i don't know if i believe that are you guys giving me some memes that'll get me banned from twitch all right it's an emote for america alright we do love america on this channel we do love america um all right well you know i'm real scared of getting banned you know they're coming for you know first they came for the the the the the white supremacists and i didn't speak out because i wasn't a white supremacist and then they came for hunter biden and i didn't speak out because i wasn't hunter biden and then they came from me but there was no one left to speak out for me guys you know when the censorship comes listen when the censorship comes and nobody can speak freely fairbanks alaska let's just go let's just go let's everybody go to fairbanks alaska and i want everybody to remember that you know when when when everything is dead and there's no hope left in the world the shelling point is fairbanks alaska and we'll all just descend on that place and uh yeah all right uh when they come for us um mics sound weird yeah they're trying to censor fairbanks alaska but don't worry the message is getting through nothing left on twitch when white supremacist and hunter biden are gone um ooh ooh f dot value oh that looks so much nicer than having to like put a rel there oh let's upgrade bob net with an f dot value oh f dot rel u oh that's so much nicer than n n dot value it seems fast don't you think look it's lower case that means fast what does the f stand for fast oh look at that we even got pretty much the same accuracy with f okay uh no i'm procrastinating now because to be honest i don't really know how to do this all right we got to compute derivatives there's probably tutorials but we're going to try to figure this out without a tutorial oh god this is this is hard mode this is hard mode no we're going to need a tutorial i don't know how to do this stuff what's the chain rule again i don't know i don't remember we gotta like get derivatives of logs and this is too hard the loss can be described as minus x sub class in the case of the weight argument being specified super hard [Music] okay oh i know this is called a log x sum let's just break this up a little bit so i can do like np dot x dot sum i'll take the log of that x sub class whoa no way it's like the same it like knows it's seven that's pretty good oh we can yeah look we can compute uncertainty let's see how uncertain is that uncertainty um let's see if i can write this for so it works for multiple i'm just gonna have to change sum to the axis equals one oh the european power is shocking my hands you know the converter doesn't do anything that was just a wire yeah it's just a wire it doesn't do anything it is shocking my hand though can i write that is that right it's not what i want that is definitely not what i want how do i write this no that's how you write it in torch no that is not what i want i want to like select it for each do you see what i'm saying that's that and then for each one i want to select is this not obvious how to do like so we have y test and y test has shape you know that we want to select for each one of those that's not right how do i do that you see what i'm trying to do let's see if chat can be useful bro you need to get your cost function which is a funk of output which is the funk of your net params and then differentiate bro you got two eyes and differentiate i'm in the cloud you all know that you guys paying attention how many people are watching right now i don't have that up like how come that's not an easy operation in numpy i really don't understand how to do that um it's called like numpy gather take oh wow how have i never used take before that's not what i want now np take that's not what i want still what i really want is like this for i in range y test spreads out dot shape sub zero and i want to say this well i'll just write it in this notation uh y test spreads out sub i uh y tests of i i want to say that what this does not already bode well if i can't do that oh i guess i could like encode it to one hot and do it like that who invented logs and 714 700 oh that's how many people we got watching why test preds out oh you're right it is that thank you author zero you're a bro um i have seen this before yeah i've seen this before it's that and that's so stupid just you know i don't want a range there i want to write that but why is that a different thing all right cool that's a nice measure of uncertainty you want to figure out what the highest uncertainty is let's take a look uh let's say mp.max rat no and argmax rat yeah so let's take a look at this image who's going to be confused by it x test rat raise uses indices oh god yeah see what number is that nobody knows nobody has any idea i mean it's probably a six but really nobody knows see how confusing it is i'm confused too i am just like network network and me are the same because if you don't understand this we're not even up to the hard stuff yet we've been doing easy stuff we've been playing on easy mode we've literally been playing endless classifier 101 looks like a crippled six yeah that's right let's see what else is confusing um what do i want to do let's zip ret range red dot shape zero uh let's do this sorted list i know this is the content you like i know this is the content you like that you're here for we're working hard to make content that people like oh let's look at more messed up images oh yeah i don't even know what is that what is that oh that's that image is so messed up but if we set reverse to false let's see let's see what the least messed up images are 13 let's see oh yeah everyone knows what number that is wow that number is so obvious oh let's look at 69. oh yeah wow wow it's really good at zeros this one a zero two oh that's an obvious three that's a beautiful that is the that is a canonical example of a three if michelangelo sculpted a three instead of david he would have sculpted that three uh this is a beautiful three right let's look at more messed up numbers is this the content you want yeah yeah we're looking at meth oh yeah oh oh six is a really ugly sixes are like yeah they're spermic you know it's like phallic but oh um two that's not two that's a you think that's a two maybe it's a week nine so week nine seven is artistic seven i hate when people try to be artistic when they're drawing sound oh look at that artistic too got a little swoop in there plot them all in a grid oh you guys want me to write a grid all right let's go let's go what do you want a 4x4 grid grid oh you know i'm really am i really writing a grid for you guys oh what's the easiest way to write a grid i don't know a good way to do this who wants to who wants to write a grid on easy mode um x bad dot reshape 4 28 times 4 28 and now we just say np dot concatenate i concatenate on axis two but is that going to be the shape i want what a bounds of array of dimension two oh that yeah yeah all right let's see let's see let's see did i make my did my magical grid code work oh that was the most beautiful grid code i've ever written oh all right let's make a parameter called g for my grid g equals four call this g times g put some g's in there got a g up in there too oh that's the most concise beautiful grid code i've ever written oh that's beautiful no but actually let's look at beautiful numbers let's really look at like the pinnacle of number oh these are the pinnacle that 4 is kind of ugly you know i think that was supposed to be the pinnacle of number let's make g big yo let's make g8 that's a big number oh that's a big g what how does it know what number that is that's the most confusing ass number i've ever seen it's like really confusing numbers though oh these numbers are these numbers are gross oh by the way is that not the most beautiful grid code you've ever seen i mean i've written that coded comma as like a big function and then i'm just like np dot concat reshape that's right that's how you write a grid all right so who knows how to differentiate this how am i going to differentiate log x i don't even know how to differentiate a matrix oh we only look at pretty numbers here let's go 16. oh it's crazy uh like the computation power that lets me yo let's go 32. wow the computation power that just lets me do that i love the future i love the future i just want you like eniac you know like eniac who'd like to like oh well i have to make sure there's no bugs in the vacuum tubes and i just did that i just i just made a grid 32 by 32. 32 times 32 is such a big number that i need a calculator to do it is it 496 it's 102 4. wow see i don't even know yeah well it's two to the fifth and five plus five is ten ten to the two tenths is one or two four i should have known that i should have thought before i spoke oh that's that's beautiful let's just okay all right all right we're getting distracted see i have chat open and i'm distracted okay we are going to make it train in numpy i believe in us we're going to finally figure out how to differentiate a log x i don't even know i mean i can have pi torch print the gradients and we can check if they're right no i'm closing chat peek strip one maybe if you're a subscriber i'd care about your opinion all right whoa np dot gradient you're a vip so i believe in you wait wait can i actually do this yo let's go let's go np dot gradient uh well actually i do want to add this to the forward pass well we'll add the log x to the forward pass that's not even i can't even do that all i can add to the forecast really is the x differentiate by hand and write the function bro you know not all of us went to school okay oh this is np.gradiency that's that's my school let's go does this work oh the gradient of an n-dimensional array i can't just do that wait that just takes the average or some but you're a vip so i believe in you the gradient of an n-dimensional array oh it's change in y divided by change in x so it's divided by so this one they divided by one and this one they divide up i don't get it i don't think that's right i don't know okay we're gonna we're gonna we're gonna use we're gonna use google in this section we will develop expertise with an intuitive understanding of back propagation which is a way of computing gradients of expression through recursive application of the chain rule understanding this process and its subtleties is critical for you to understand and you think i should understand it but i don't i'm a neural network script kitty we're gonna read slow mode read all right let's go um recall the primary reason is this written by this is this written by mr carpathi himself um recall that the primary reason we're interested in this problem is look at neural networks f will correspond to the loss function let's start simple so we can yeah so i believe we're trying to compute the gradient with respect to the input that looks really hard yo you know what why am i using dot transpose when i can use dot t oh that's so much nicer so all right let's first look at the gradients in pi torch let's use pi torch as a teacher i mean i have done this stuff once in my life but like i don't know it off the top of my head and i should if i want to really be good at neural networks um yo are there there's people like who know off the top of their head what the first test image in mness what number it is is it a seven it's a seven oh that's a beautiful seven you know what i made the fig size too big so that my seven was too big oh yeah tiny fake 1d target tensor expected multi-target not supported up because it's y test okay all right so can i say like lost.grad it's not a leaf tensor use retain grad so i don't understand why it's not a leaf tension there's probably ways to like view the graph oh and then before we do backwards i forgot to do optum zero grad and that's in the optimizer i can say model.0 grad as well i think that makes more sense i really forgot to do that and one of my things and that's why the learning rate was so high and i couldn't get my examples to work well i mean i had to set the learning rate so low okay okay so we zero the grad we compute the model we compute the loss function and now we check that so now can i say like model l1 grad linear object has no grad oh wait dot god it can't be zero it can't be that perfect can it zero up there what's not zero everywhere okay that looks quality oh can i do out.grad grad attribute won't be retained during autograd backwards if i want it it's not a leaf tensor because it's only used in other computations can i do that there we go okay so this is the gradient with respect to lost that grad is one why is it one there's also a mean there's like a there's like an implicit mean in there uh let's get rid of that implicit mean you can do it with a reduction what if i say reduction equals none um and then shape is this 128 okay so that's for the batch size that makes sense oh yes it's slightly better okay uh i don't get what oh that's one okay that's fine because there's only one example there if i did two examples can implicitly only be created for scale i want okay let's see if we can compute this which is the gradient through the loss function which i think is what we have to get right first it's just ugly i hate when it gets hours like that okay um backward pass cross entropy loss well we can break this up a little bit more we can write log softmax right and our loss is a different thing all right let's add us if we use normal softmax then what's the loss um let function just solve these logs of x and then instead of doing cross entropy loss we'll use nll loss implicit dimension choice for logs off max has been deprecated oh m equals one wow maybe we should pick a more controversial sample is that really right that it's just minus one there can be described as i guess that's kind of right yeah it's still minus one even when i reinitialized bobnat well that's a simple enough loss function i do know how to take that gradient um what if i say model.sm.grad log softmax object has no attribute grade okay so yeah because the weight would have to be the gradient so it has a gradient it's just that tensor is created implicitly there's ways to like debug autograd in uh this stuff isn't there okay well either way that's easy enough to compute which i like so it's just seven right zero one two three four five six seven so minus one and then we have to put it through logs off max what's the derivative of that you know we don't have to use a model here either this is using a lot of fancy torch stuff we don't have to use kind of meet in the middle um like if i just made those normal matrices and for that i can just run that let's check in on chat bro just use wolfram to get the derivative bro you know what by chad69 that is not a bad idea let's just get the derivative of this thing wolfram alpha so i want the derivative of log soft max okay so log x x divided by oh sigma let's just say x x plus x y it's not a vector assuming log is the natural logarithm that is exactly what we want derivative see i'm not even really sure what that means because i can get the partial derivative with respect to x or with respect to y but i really want that okay let's first figure out what i'm trying to do i'm trying to get the derivative of the output with the input with respect to the output all right so first let's start with np.010 uh then we say out sub 7 equals minus 1. okay look that one looks the same as that one oh i did it so well all right now derivative of log softmax oh yeah what happened to ikea i have an idea why don't we read the pie torch source code you know if there's enough food for me ah this function doesn't work directly with nlos which expects the log to be computed between the softmax and itself i mean so i should be able to i should let's use a little pie torch for a second so if i do where is that defined uh torch nn dot let's say gn wait no that's the gradient in that's not the in fact i can just say like l2 sub zero so it's that that i want to take the soft max of well no not l2 just x action no next test spreads out yeah that let's retrain the model quickly all right i think what we're going to do is we're going to get lunch and then we're going to come back and then we're going to solve this hopefully it's all set up got it beautifully set up let me just i can push this to github if you guys are interested in playing along at home what let's take a break for a minute um half done eminence from scratch promise i will finish soon all right you guys can play along at home just so you know what we are trying to do is it clear to everybody what we're trying to do we're trying to re-implement that backward pass i mean the forward pass was so easy we already got the forward pass done we're trying to re-implement the backward pass in numpy and then we're going to train this exact same network in numpy but we have to figure out how to get the derivative of log softmax you know that's this is this is this is complex math but we're going to be able to do it when we come back after lunch i will see you all soon thank you for subscribing symbol lab oh log of soft max derivative on math stack exchange i was reading about the stack exchange controversy last night or some monika girl we are all monica um no they they monica man everyone's protesting i hear static exchange is horrible to their users because they didn't reinstate monica um i stand with monika i think i'm not really sure from the from the two minutes i spent reading about this last night wait what's the lavender letter wow no no no no no no we're not we're not we're not going here um log of softmax function derivative using the log identity c see this stuff wow that's why you never go on math stack exchange compute stack exchange would give you codes uh cool yeah yeah well we'll be back from lunch in like uh i don't know like an hour or something maybe maybe eighty percent chance all right thank you for watching part one we'll be back for part two soon oh we never back down when there's a challenge never give up never surrender we got some caffeine food um let's go bring my mug back okay we're going to cook and by we i mean well not we and we're going to have tortellini and we're going to have salsa grip chips yeah oh we got chips too and no sour cream disgusting food yeah oh this sounds so lovely my even my days are so lovely welcome to sunday saturday of that all right we're going to conquer these derivatives right now we gave up when it gets hard and if we give up get we get when things get hard what example would we be setting for the children a bad one that's right set a good example for the children let's go okay um let's first do this in pi torch torch dot tensor that torch and then log softmax of chin jout uh functional i'm gonna put did dim equals zero let's print gout okay those are big numbers i hate one torch torch no scientific notation oh no they love sound who likes scientific notation who made that like the default oh my grid is ruined i can't call things grid oh those are much more reasonable looking numbers what do you all think um we have out i feel like i can't do the backwards pass yet okay so that's what we got from log soft max oh that's good that like makes sense right because it's so sure that it's a seven wasn't so sure about on this one this one it's almost so short it's a three we should probably do this too just for politeness okay no no we don't need politeness let's go um so now we have like this and like that's the derivative i guess that's the derivative that's not the target what is nll loss let's just read what happened to our losses and loss uh with reduction sentinel it's this so wait oh i guess the loss is just like selecting it so if we just actually it's this vector so we can say something like loss equals gout oh yeah i love spicy wow torch hole 64. um let's just actually put this up here all right and then here we can take the mean so we have no loss there doesn't seem right [Music] that one has lost no like no no no i'm forgetting a negative sign somewhere oh there's my negative sign that i'm forgetting ah so the negative sign goes in front of the weight which is that thank you um does that like match what we're seeing for the loss yes it does so if we just do this one yeah this one has no loss and that one has big loss that one has less loss okay um now if i do lost dot backward does that work element zero of tensor does not have require grad sour cream is disgusting it's not this isn't a debate that i'd like to have on stream but i believe no never i would happily tell them that that i think sour cream is is absolutely disgusting but i haven't really rehearsed my debate yet and you know everything i do on this stream i practice it before i do it on stream wait tenser got an unexpected keyword argument retain grad why can't i retain grad oh maybe i just do gin dot retain grad oh dot retain grad not retain grad equals true yeah because we want to retain grads that's right cannot retain grad on a tensor that has oh requires grad okay we gotta require the grad all right we did lost dot backwards and now we can do g and dot grab oh well okay that one doesn't really have any loss so that one's kind of unfair but if we do this one we'll have some loss okay good okay so yeah i think that's our output um dsm equals gen dot crack yeah yeah look we took the derivative yeah who likes our answer okay that's pretty good um let's just do the same one here yeah now let's see if we can get the same answer okay the other derivative should be easier i think that was the hardest one and we did it by cheating oh oh i know why the loss is so high because the answer is not actually seven yeah now the loss ain't so high anymore eh that one still has loss let's just check here if this has loss now that one doesn't have loss but this one does that's a good amount of loss you guys want to take a look at it just so we see like what we're looking at here it's a 2. let's confirm that it's a two no we can't m show it it's a two oh okay so the derivative says make it more two and less three i believe or more three and less two i don't know this doesn't make any sense less three more two okay lesson learned and then we want things that look like this we actually can't get that derivative there oh but actually i think i can say not requires grad but i this one's going to complain it's going to say that you didn't retain the grad and you need to retain your grads but just to confirm that this is going to be that but minus 1. what do you mean what am i doing um oh they love when i explain what i'm doing uh what am i doing right now i'm yeah well do you know what mnist is what do you know what mnist is um mnist is numbers and you can get the computer to tell you which number a number is oh numbers yeah yeah it's pretty good at it too like you want to see some numbers um i'll show you some numbers and then the computer can tell you which number it is using matrices you know because we love matrices that's correct um yeah but getting the matrices to be good at telling you which number numbers are is that what we're trying to do and it involves calculus and i'm not very good at calculus i'm good at programming but i'm not good at calculus so we're cheating and we're using programming instead of calculus and it's sad uh we shouldn't be doing that we should figure out how to compute that without it but we'll get back to that one i promise we'll go back to that one let's move on now to l2 we got to get the derivative of l2 l2 uh yeah so this one's just like we're gonna like transpose the matrix or something number rectangles go burr um see and now it's yeah uh okay so we should be able to produce the same image using numpy is it really just you know let's just try some like dimensional analysis crap what if i just do l2.t is that the same image invalid size oh it's only in a valid size because we should really do yeah let's make everything batch because we don't make it batch we're going to regret it later how did that break oh because dim equals 1. and then when i wanted to make dim equal one and you guys would like don't make dim equals one that's a lie you guys are just controlled opposition uh dsm dot shave l2 dot shape i don't even think i have the shapes right if you oh because i used 10 there no i didn't use ten just is ten yeah okay so we just don't understand what we're doing let's read back prop it tells you how to do it with a matrix um weight dot t dot transpose dot d d tips use dimensional analysis oh good that's what i'm good at sweet okay um what is x oh okay so we're doing the forward pass where we're doing oh w dot x should i not have transposed those w dot x x dot that seems right probably the most tricky operation is the matrix matrix multiplication if we want if we want dw suppose we have the gradient on d from above in the circuit okay so we do have the gradient on d which is dsm so we want to say dsm dot i think it's l2 dot t and i think that is the derivative dw is that it doesn't have the right shape well so if that's d w like somehow that became okay so i guess first we compute well so this isn't dw this is d oh yeah no we have to retain the things from the forward pass yo if we didn't do that of course it doesn't work yeah what am i trying to do we didn't retain the thing from the forward pass i'm trying to compute this without actually using the image okay we gotta name things more carefully um so we're gonna say uh x underscore l1 what do they name things you know what we'll just say yeah x underscore l1 x equals x test two and this is going to be x dot l1 which is x dot l1 x value equals np maximum xl1 0 xl2 equals x rail u dot l1 yeah so it's really this that we have to compute ah this isn't l1 this is that wait all right let's just print the shapes here thank you oh this looks so good alex there's cheese yeah that's delicious thank you oh and then actually we can confirm that these are the same things like it has to be right yeah they're the same well but they're not actually perfectly identical they're very close but they're not perfectly identical all right so then here we can just make this xl2 um all right so what was the input we want x value and then actually i think we want let's just try that no oh in this example our w is x value and our l2 is x we want to compute dl2 and dl2 equals l2 dot transpose dot dsm that's not right same shape as d it was the same shape as d oh did you make your own what did you make your own yeah with sour cream i don't know how you eat that what i don't know how you eat that what am i doing wrong you guys like screaming it in the chat chat is quiet today too spicy okay so in our forward pass let's just be explicit w is x value and x is l 2. our x-ray u has shape 1 comma 128 and l2 has shape i'm going to get the derivative of x so we're trying to get dx so we can do x relu oh is that just right yeah okay that's right perfect is that the same image i mean we can transpose it but okay good look that image looks the same as that image computed by pi torch oh yeah we're making progress we made our first derivative we also need dx value should go on to the next because we're going to need that for oh now what's the derivative of a rally you know what this one i think i can figure out no actually i can't what's the derivative of x it's just one and then the derivative of z of zero wait is the derivative of a value really just that that and that i think that's right so so it's just one wow is that really the derivative no so huh i mean that seems right right what no nothing can be removed everything on the internet is forever ask hunter biden he knows we're trying to figure out what news is fake and what news is real um it really does seem like the derivative of value is just zero if x is negative and and one of x is positive but that just seems too simple i mean let's try it derivative of value so we're trying to figure out what dx l1 is and then according to that it would just basically be this maybe let's name this better dxsm name this d alto the derivative of l1 if that's x-ray u then i think that's just x dot dx i want does that image match almost i think my relu derivative is probably aggressive it's probably not right because that doesn't look like that other image thank you we got tortellinis here's good to me yeah that doesn't look right okay the one derivative that i calculated myself is wrong no okay that says right that wow is that really the derivative of that's unbelievable if that show okay um but no that doesn't explain why my image doesn't look like the other image so x value is what's going into l2 and x is what's going into 11 l1 so that dot that oops that's an l not a one what'd i do wrong probably no i probably got one tiny thing wrong i'm just a little i'm a little shocked that the derivative of value is that i probably have to like multiply it by something for the chain rule just remove debris bro no differentiation relu is not a continuous function it can't be differentiated yet nobody cares about that i probably have to multiply it by something for the chain rule after all the good comments who reads my crap back prop for relu is you take the dx coming backward and zero out the entries that had negative pre-activation is that really right you're telling me it's this i mean this seems more right to me i'm not exactly sure why it's true that still doesn't look right i guess i mean we have to use a chain rule [Music] i'm pretty sure relu evolves into lucario um [Music] i know torch does the chain rule for me one if x greater than zero else zero but that's exactly what i wrote here like i just can't believe it's so harsh okay so i guess i do have to i probably have to multiply it by the input then all right because that's the chain rule that still doesn't look like that all right let's look up the chain um did you want to look at the numbers and the computer will tell you what those numbers are oh that's cool yeah like reads numbers it's like this is like the classic machine learning yeah it's famous what did you just ask the why question oh um his feud with schmidt uber is lame no young lacoon i don't know i mean i guess that was a big schmidt hoover fan but then like what does schmidt hoover do today he still complains about the feud although it was like a feud and there was like leaked videos on youtube of like you know him like like looking at y'all it was weird um no the feud had to do with like who invented deep learning it was stupid um who cares i think andre karpathy did actually that's what i'm gonna go with okay then the derivative of if we define yeah so this kind of seems like y we do have to multiply it by dx value so that's probably right why doesn't my picture look like that picture too much wood oh wait no i think one of these shouldn't be dx value one of them should be x-ray yeah there we go boom we use the chain rule and we figured it out is that right yeah so we just basically we just zero out the derivative everywhere to zero out the derivative everywhere that uh the original function was negative okay and now we're done now we implement the learning rule oh now we have to do the derivative of log soft max that isn't done like that the sauce is spicy you told me 10 minutes ago yeah yeah yeah i'm sure you told me 10 minutes ago okay [Music] can't use mse for classification bro that's exactly bitchin 69 knows what's up so therefore we have to figure out how to get the derivative of log softmax okay well so we know it's going to involve will involve xl2 out i'm going to move this down back here we'll involve xl2 out and produce dxsm um the derivative of log soft max equals 1 minus soft max minus softmax if it's not equal first note that that's what soft max is so if we want the derivative oh my god see look at all these upside down why do they use symbols in math why can't they just use normal people things like letters all right that's the softmax function cross entropy with softmax oh this looks nice oh now that i turned off science what wait oh i didn't turn off scientific notation for numpy that one actually know how to do so foreign all right i don't know where they're getting these z's and y's and k's from it's just equal oh the time of the forward pass you have that okay grad import one so i don't really know what the grad output i is i think it might just be that but negative well let's just first try it looks like we're doing an x on the output so if those are the three things what's the input those are really large numbers um it's not really right nan's worse we hate nands output sum minus x times sum grad output data okay so soft max is that so so this code doesn't look very competent because this is gonna it's gonna set this over and over again okay let's go back to first just computing the log soft softmax which we were doing whatever that range thing was here so that's wait if i do if i set softmax let's just use softmax forget log softmax uh just want to know how to turn that and that into that uh long softmax update output th tensor new contiguous grad output grad input equals grad output minus x times sum so first we sum up the grad output data which i think is just minus one those numbers are just crazy small oh probably it's related to yeah oh okay i think i get this uh all right so we're going to need the sum what i just don't understand is how it's not the sum of the x guess that is output data yeah okay so first well first we have to get the output data we haven't actually computed the forward pass we haven't computed x underscore sm yet so how can i expect to take the gradient yeah so we can do the forward pass on soft max um i made that same mistake twice i forgot my forward pass so the forward where do i compute that again it's that range crap that i did here we go this and i called it big stupid things i feel like i just did that already okay xsm equals minus xl2 uh we need out here um times out uh plus np dot log np dot x xl2 dot sum axis equals one oh that looks similar to that um i think you're supposed to do a stability hack for soft max but i don't really care no it's like not really times out maybe it is okay so we have that so those numbers look suspiciously close so that's output take the x for the output data really big numbers oh actually let's rename this to lsm because it's log softmax so to be right if i go like that does that give me like reasonable probabilities no well actually this should be the same thing as gl right am i not even getting the forward pass right yeah i'm not getting the forward pass right okay because there's a negative that i'm forgetting somewhere the log softmac function this like simplifies to something though it's like this in theory but like you can simplify this right so in log space that's just the same as dividing is the same as subtracting so we can just do some 2 there if i want i can just take that outside and i can do np.log that minus log that um why is that all right oh well that didn't compute it for each one and that computes it for each one so times out negative infinities oh i didn't do an x oh but okay log of x is just the same thing so yeah it's that right i mean they're similar i feel like that's just due to numerical instabilities in here log x sum is that a numpy function yeah that's probably just numerical instabilities okay so that's the forward pass of the soft max i mean okay that's no that's actually a lot to attribute to numerical instabilities i don't think it's numerical instabilities because out's not right oh i have to it for like each one what if i do that oh there we go okay don't attribute to numerical instability until you've tried harder good okay so this is a log soft max forward pass and it matches all right let's clean up some of this crap we don't need that anymore all right now we're actually finally doing the forward pass those two things match now we have to figure out how to get the derivative okay that shouldn't have anything to do with out yeah so the log is just this so we have to take the derivative of that oh wow okay i was being the news i was trying to combine the things together don't do that okay all right so that's the derivative of out i'm not exactly sure how it got that oh because the mean is the derivative i get it d out equals negative out divided by 10. so those should be the same okay those two are the same all right now let's see if we can produce that and now we can go back to that code and we can look at what it's doing okay so we want the sum of the grad output data so the grad output data is d out so we can say d out dot sum this sum and then the output data itself i don't think that's right it's not out is it it's the grad output data i guess that output data is the output data from log softmax which is x let's x that and see if it looks reasonable okay okay so what if we just is it this d out equals npx lsm times d out dot sum just call it alt for now d out minus oh yeah buddy dx lsm equals d out minus npx lsm gotta cite my all right now we have dx lsm computed from that and the pictures look the same yeah buddy all right um you know i kind of want to keep that pie torch coat around it's not deleting now we don't need this pie torch code we just want people to think we're pros and we didn't need it so actually to be complete we can say x loss equals um x lsm negative out times x lsm dot mean uh dim one axis equals one so that's the loss now if we want dx loss which is that do we actually have to use x loss anywhere i feel like we should but yet i don't um what do i call it up there samps samp uh it should work for two samples too right um i don't know why it doesn't work for two samples but we'll do with that later okay forward backward pass okay we have the gradients now the weight matrices written entirely in what is x loss oh i'm not even setting d out anywhere we have to set d out that's probably why having two samples didn't work no that's not why all right let's go through it's test subsamp x dot l1 and p maximum delete that delete that see if it works for a different one uh actually let's use the same code just say sam equals one samp there so what if i do one comma two there it does not work square can only be implicitly created for scalar outputs uh oh because we have to take the mean i don't really understand how to do that that's going to be a nightmare because there's only one weight gradient and it like computes it over the mean okay well you know we can just compute the gradients one at a time it shall be slow but you know what they say about slow it's okay great it's just okay okay so those two pictures look the same they don't huh what did i not do right oh i should have left that pie torch coat around we could have compared them yeah my my x1 gradients totally don't look the same let's just hope i made a typo dx lsm d2 x value you know what i know how to check for a typo restart and clear output training the pie torch that's the gradients at zero copy the network numpy forward pass confirm they're equal ah dxsm aha dxsm because that should be lsm my dx rally was wrong and that's why my pictures didn't match beautiful junk junk junk junk junk junk let's commit this we have numpy backwards pass no what what's wrong you can tell me in your favor ice cubers um oh appreciate that switches twitches i know you're still there i see you can you see me you just see a lot of things how many pixels yeah outlet was pretty good you want me should we stop should we stop with the net i promised alex today we were going to smoke weed and watch the sopranos and instead you guys all got a shitty calculus lesson how many people we got watching this you can always see me oh they can't see yeah they can't see over there better cal class than i had in college yeah we struggle until we get the derivative to match torch yo imagine the first guy who had to figure this out and couldn't just compare it to torch man that's the lewis and clark of deep learning oh carpathi wrote an autodesk oh we could do that nah newton just wrote it on paper newton couldn't import torch man imagine you let newton import torch for a week i'm lost without test vectors too what oh you want me to use the big handle wow i feel like newton i bet newton had some that looked like this all right let's go to the bathroom and let's get back to work we've still got to implement the fun stuff like gradient descent and xavier initialization that stuff's fun you know it wasn't fun the derivative of log soft max and the derivative well the derivative of revenue was shameful but i didn't know that with the derivative of log softmax that one was so complex so uh okay um first i think we're gonna have to no we're not playing chess the legacy of com tom cruise is being taught in college security courses you too can have this kind of skill all right let's first figure out let's get it to work for batch gradients first all right so like let's do 0 1 2 3. that doesn't work grad can only be implicitly created for scalar outputs and that's because we got to stick a mean on there that's a lot of feet wait what is i commenting out that print how something's still getting printed oh lost.grad and outdot grad oh look at that wait is that really true oh that's minus one why is it minus one if i do mean and not minus one if i don't do mean still minus one never mind what loss function i'm using a lot loss i mean that's just going to change the magnitude of the gradients we can just do that okay so those that's a picture let's confirm that the two pictures look the same i'm using samp two okay those pictures look the same um okay now if i do samp01 oh it does that all this we just have to make length of samp okay operands could not be broadcast together uh oh that's what hmm got reshape oh whoa look at that that's trippy how'd that even happen okay so that's what zero comma one looks like let's see if zero com1 looks like that up there no they look totally different i mean that the out is different now those are the same i don't know i kind of like mine better should i kind of just like stack them i like i like my gradient better can someone tell me why it's not better they're stacked more fairly it seems you know we love fairness on this channel oh okay so this is one of the problems actually my dx lsm is just wrong it's not even the right shape is it 2 comma 10 is right but yeah we wanna do some axis equals one i believe yeah and then we sum axis equals one and then we put a reshape there okay good now those look the sack and now it looks the same oh that was really easy to make work overflow encountered in log um we can actually fix this too there's a there's a stability hack for this where you subtract why is it printing stupid scientific notation it's really insisting on scientific notation um yeah you want to do like log sum x oh log add x oh it only handles two inputs well that's stupid um what's the numerical stability trick for this overflow encountered in log see any overflow do you see an overflow those look like very reasonable things to take the log of to me i think i can do something like i want to subtract the max because x gets big if the numbers are big so if i subtract this works out to like the same thing mathematically that doesn't look like the same thing subtract them in add them in something like that log sub x trick uh oh we subtract c okay if we set c to the max so the way to do this if we want the same value and so my logs don't complain about overflowing anymore because we say xl2 dot max axis one um dot reshape oh wow yeah it's really just the max okay that looks the same but now if we want that fast one we can say i'll call it say where does that not match that matches oh because we don't want to reshape it there i only want to reshape it there perfect beautiful stability hack and yeah the derivative doesn't do any logs it just takes an x so there's nothing i can do about that probably there probably is but you know that would involve knowing calculus well you know all right so let's check a bunch of gradients now yeah that looks like wow look it's like learning look at how much it's learning look at all that learning it's doing well that's beautiful those gradients are beautiful all right let's check it down here looks the same oh man imagine newton had pytorch pie torch newton lost that crowd is just one i don't really understand why it's one but we'll understand someday you don't need those anymore that's all right okay def forwards backwards xy you know what my posture yeah i know it's not good i'm hunching over the computer this chair doesn't have a headrest like my gaming chair i don't know about these just you go boss chair herman miller i get those guys confused okay so these are the only learnable parameters in the model so we can just return them beautiful all right let's get my xavier knits to match you know it's really important how you initialize your your networks if you don't initialize them right um torch linear annette linear one is here and it caming uniform wow i don't even know what's a gaming that's some crazy kind of noise whoa is kaming the new xavier h is just left oh it's okay i think it's called random numpy actually too okay random okay times math wow gaming crazy not even going to start on how that works we will test it a little bit yeah those numbers look pretty similar to those uh okay so what can i copy we're going to want that same code that can come with me l1 equals gaming l1 dot shape zero i can just say star l1 shape renet okay now we want to do forwards backwards but not on the test and we'll just copy that too that's fine y train l1 l2 no sorry dl dl2 one of these days right atom runs fast uh now we want to compute the loss and accuracy oh well actually i can just compute the loss kind of crazy high losses is that right is my log sum x oh it's because it's not initialized well those are actually reasonable losses the only question is why is there no loss for the first one weird okay so those are actually reasonable losses um okay so we're getting an accuracy now of nine percent that all makes sense now we want to write sgd uh so we have the derivative so sgd just just as mostly this one i actually think i can implement from memory um oh we have to put our learning right we're not gonna use any momentum uh okay that's x loss i don't know about xl2 as well which is the out uh np.org max cat oh we do need okay now we need to copy the appends and we can copy that line when we update the description um posture posture posture update description and then we'll copy the code we plot them too okay so of course nothing gets better because we didn't actually implement the update rule yet and the update rule is l1 equals l1 minus lr times the derivative oh that looks like decent accuracy um let's evaluate it it's called numpy eval 85 that's pretty good i mean it's not as good as torch but i don't know it's not supposed to be plus is it definitely not supposed to be plus wow that's big negative numbers yo last time i saw numbers that big okay so the only question is why is it shittier than torch should be pretty much the same let's just rerun the whole kernel i do my caming initialization wrong did numpy forget what the square root of 2 was it's a pretty common thing to forget so close like it's almost the same and it's still learning maybe we'll just let it learn maybe it learns slow you know maybe yeah pi torch has 160 iq numpy only has this is probably due to some numerical stability stuff they're always talking about these kind of things and no one ever takes them seriously i wrote sgd oh yeah look 93 it just is slower that's so weird my batch size is 128 that's the same right it learns it just takes longer picking random stuff oh landsamp oh look what i did yeah oh so that was oh i had a learning rate that was effectively [Music] 32x too high let's try it now it's learning faster i think it seems like it's running slower never mind it doesn't learn at all like that okay well that was a bug yeah the laptop fans going you know takes a lot of math to learn deep what other bugs did i write you know that's the problem with optimizers you write bugs and they're impossible to find because the optimizer just optimizes over them you know i guess i didn't check the magnitude of these gradients i mean m show is going to hide all that from you let's check them hmm they're very different orders of magnitude actually they're very different i don't know never mind no they're similar they're slightly different it doesn't just look like a scaling thing they're different they're actually different they just look similar the means the same though we have these here transpose obviously i think this is just numerical instability pie torch works really hard on this stuff the l1 weights have similar but the l2 weights are different this is useless what is numerical instability it has to do with if you do like uh yo you ready for some craziness let's try 64. does the same hmm all right should we cheat and add momentum a little cheating let's just see if it fixes up one of these days we'll write no i think you know what i think i think uh 86 percent is just what we deserve um but actually np set seed is it re is it red reproducible it is let's remove the log x sum hack it does the same i didn't do anything like leave samp anywhere in here right this is one of the downsides of uh notebooks you know what let's restart and clear output and let's just try the numpy ones let's not import torch up here that kind of ruins it you don't even have torch imported okay oh that wasn't numpy well actually this might already be uh oh well okay never mind what already is float 30 float64s maybe float64 has made it worse and then got an ex unexpected really i have a fig size somewhere in here i'm sure i do on that stupid crap no other fake sizes all right chat i'm pretty happy with this i don't know why it's not as accurate there may be bugs but and just the fact that the two gradients aren't exactly the same i'm pretty happy with this even though we only got an 85 oh torch uses momentum by the formula i turned it off you know it gets there it just takes a little longer i don't know let's give it four thousand let's make the batch so smaller it gets there maybe 2000 is fine let seed hack let's mess with the seed until we get a good one yeah just like real deep learning papers yeah that's right i have so much compute that i'm willing to burn to get a better accuracy oh look this one's better over 90 percent with hand coded stuff here with numpy wow numpy perfect um is there any other is there any other like hacking that sgd is using no is my initialization bad well there's something we can do what if i don't edit the things i just use the ones from pie torch oh look it improves them it's possible that their initializer is just better than ours initialization's super important uh let's just run that oh no i don't want to run that i should just want to run model equals bobnet and then yeah okay so that's junk but now we don't do the inet huh it's good now it's really just all the initialization my initialization isn't right that's wild stupid gaming initialization it's not right look now it learns how could it be wrong look at that if i steal the weights from bob nat it learns oh wow wow that really that really makes you think how important initialization is wow that's it that's using uniform noise that's this if this is what changes it i mean i already know it's that [Music] yo that's even better i think i've managed to beat pie torch with my sick uniform initialization look at that we're getting 96 percent now that's insane that's just insane that that does it and none of these match pi torch which does something totally different use gaming initialization get crappy evaluation use uniform initialization get strong but none of those match pi torch which is something entirely different it's entirely determined by your layer initialization oh i can't get over this enough pie torch linear layer and knit only calms down was just supposed to be m times h i feel like it's supposed to be m times h that was just bugged six divided by i don't even know i don't even know this doesn't make any sense but that one's pretty good as well okay they're all good they just need to be scaled right yeah somebody forgot a times h i think in that in that crappy example that i copied wow numpy is better than pie torch on both of them but what is pi torch using i don't know this is a great mystery of the future but for now let's run our notebook sit back and enjoy our 96 percent accuracy using two linear layers we've actually beaten pi torch inaccuracy um didn't even seed hack changed it back to one three three seven everyone knows that's a no number up my sleeve seed beautiful pie torch 93 percent numpy 96 yo and what if i make it float 64. i bet it's going to be even more accurate same accurate did that look less noisy to anybody else what if i make it float 16 more accurate wow what if i make it you win date you win date is weak float 16 look float 16 such a winner float 16 just wins note add more accuracy with load 16. all right colonel restart and rerun all let's commit this i'm very happy we got over 96 that's accuracy that exceeds pi torches accuracy because we're using we're using uh good initialization functions initialization is super important when you do your deep learning always remember that that's the lesson we learned today and we're getting 96.3 of accuracy we'll do very quick questions no we can't be overfitting this is the test set we train on the train set and we test on the test set don't pi eval test on the test set oh yeah look pytorch got a crappy patchwork sucks compared to numpy i'm going to write deep learning but it's going to be accurate no what is this both a thousand i did a thousand for both pi torches is noisy crap which actually makes me wonder if i wrote bugs but actually you know what we can do we can use layer in it on pie torch just for fun initialize nd as parameter weight dot copy torch assign weight i don't want to do that i want to copy just this can i assign a numpy ndi i can make a tensors oh we gotta transpose them don't we cannot optimize a non-leaf tensor what um leave with torch torch no grad oh now it matches yo whatever torch's default initializer for layer is pro tip if you like accuracy next next time through the notebook consider pro mode weight init with numpy instead of beta torch and knit weak crap now we don't want to say we want to say instead of beta torch init weak yes that's what we want to say oh yes virgin torch versus chad numpy oh that's yeah yeah consider chad mode wait instead of virgin torch init mode perfect all right i think we can commit this i'm very happy with the work we did today boys very strong accuracy to do why is torch linear init bad is a hundred percent accuracy not possible oh well i mean that's a philosophical debate on the test set of course it's possible in life and in general 100 accuracy probably means you have an evaluation problem um numpy accuracy greater than torch accuracy all done ah can you feed your own drawing to test okay okay how do i make a drawing okay fine guys what number should we use what number should we use for our own custom drawer no we're not using paint we're using okay okay can we get can we get a vote going here let's draw a nice three all right four all right we'll draw a four right i'm sold on a four okay so it's 28 so that means we gotta go seven by seven four fun can it recognize four okay so we're gonna draw a nice four and we're going to go through all by hand and we're going to compute this and then we're going to see if we've written high quality code or not or if we've really just over fit to junk we can improve our 4. this four looks upsettingly like a nine all right should we go should we go to 14 by 14. no 14 by 14 is too much that's a good four if it doesn't recognize that as a four nothing's a four do people believe in my four all right that's a good four okay so let's um resize it how do we resize it quickly we gotta resize it um how do i resize resize resize or size resize uh in a one-liner well it's not so num it only takes in things that are 28 by 28 so we got to resize it that's not going to work i want to stack those all i concatenate them upscale to 28 by 28. okay you ready we're going to write out we're going to write out the whole thing by hand we have an m m dot reshape all right let's take a look at the forward pass dot l1 uh we gotta do a relu next uh np.org max x oh it's a nine it's a nine oh no it's a nine that's really a nine let's change it until it's a nine i mean to be fair it kind of is a nine nine it's a nine i mean like you understand where it's coming from right is that more fory oh now it's a four yeah now it's a four that's right what is it um wait how do i get large text ipython notebook large text never mind i don't know how to get large text but let's run it again oh just put a four in the output and use four backwards to get at the canonical four pick yeah that's not as easy as you think it is all right let's draw two you know i don't like fours they look too much like nines who who let me convince that it was four okay fine at three threes are good that's a nice three that's a five damn it deep learning doesn't work still a five we're putting it back to four and we're just calling it deep learning doesn't work no we like to end things on a positive note let's draw a better three that's a four these are all fours well really that that cross over there really makes it a four a nice four oh someone wrote a nice four oh user four no your four is too large all right we will try your four just briefly just to see if it works oh that look hell kind of four is that list has no attribute reshape that's what kind of four that is your four's not even the right size it's 812. you have one too many zeros over here oh yeah why was i trying to draw my 4 the stupid way what if i just draw it like that i can't believe i'm deleting the zeros in your example and you know what the worst part is i'm not even 100 sure that i'm deleting the right zeros oh but now i accidentally deleted one of those no no no i deleted the wrong zeros it's a seven okay that's not a four but you're right what if i draw the four like this that is undisputably a four beautiful that one's a four i'm very satisfied with that we're gonna leave it at that i bet even if i delete this it'll still be a four oh yeah that's a nice four oh oh how does this four look oh that's an even more beautiful four to even bring it closer together let's bring it closer together oh that's the four why don't you let me draw my four with the slant that is a solid four it prints four let's see how convinced it is that that's a four oh super convent wait what isn't that number bigger than that number zero one two three oh four's over here i didn't count right oh look it's so convinced it's a four it might be a nine it also might be a three and let's see what number it's definitely not it's definitely not a two it knows for sure about that okay let's rerun this let's commit it and then that was today's stream that is a beautiful four except we actually have to change this to four train evaluate four four love it look it knows what a forest very impressive do you wanna draw a number are we done we're done it knows four thank you all for watching everybody should we do questions um hmm it works to be fair the other four did not look that much like a four i only had seven by seven to work with oh the quest jailbreaking stream want to know about that um you gotta go who even gets out of bed for 5k i will i will give you an exclusive license to this four recognizer for only 500. it can be yours i'll take it off the internet just like that wu-tang album it can be yours nobody draws four with a slant exactly i don't know what i was thinking of course i thought it was a nine you know will i raid someone after finishing stream to be honest i would but i don't know how i'm not a very good twitch how much for a five recognizer i don't know about that what are you gonna shower all right and then and then i'm done with streaming i'm done with the computer for all that is there water involved raid awkward chats all right you're not chilling for yourself are you this is an ad i'm watching an ad right now um investigating info this sounds too much like doxing we gotta i will have i will have people to raid one of these days you got oh you just got an ad for this stream i'm sorry you get yeah yeah no we got um i know the oculus is challenging me yeah yeah we got we got things to do oh yeah lana locks yeah well that sounds good lenolux has raided me several times she's streaming right now his channel is intended for mature audiences start watching what i got an ad again wait can i not i already have a raid in progress all right excellent passing on the viewers enjoy everybody thank you all for watching i hope to see you here again next time we stream and we'll do something else all right bye-bye i got things through today 