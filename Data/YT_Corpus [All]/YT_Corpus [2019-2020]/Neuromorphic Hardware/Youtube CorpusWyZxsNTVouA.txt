 Hi everybody, my name is Cedric Scheerlinck, and welcome to my talk. So, here are our final results: videos reconstructed using an event camera. Cool, so how did we get here? First, I'll explain: what is an event camera? On the left you see a normal frame-based camera, on the right, the output of an event camera. The event camera only outputs brightness changes, so when the brightness increases it will output a positive event and if the brightness decreases it will get a negative event and at the pixels where the brightness is constant there are no events. The aim of our work was to take the sparse event data and reconstruct a full brightness image. On the left we see that we discretize the events into a space-time voxel grid and then we use a very lightweight recurrent, fully convolutional neural network. You can see it's a very small neural network, there are only a few layers. There are convolutional gated recurrent units and residual blocks. The output is an image. With a sequence of inputs we can get a sequence of images as a video output. Compared to the previous state-of-the-art, our method is much more lightweight. It runs three times faster on a modern GPU and four times faster on CPU. It uses ten times less floating point operations, that means we are much, much more computationally efficient. Interestingly, we actually acheive this with a very small neural network, 280 fewer parameters, that means our network has only 0.4 percent the number of parameters as the previous method. All of this while acheiving quite a similar accuracy. So it gives a hint that the required capacity of the network is much smaller than initially thought. To demonstrate our results, I'll show you some example sequences. On the left will be previous state-of-the-art E2VID and on the right is our method. These are some high speed sequences captured with the event camera and you can see that quality of the images is very similar. That's me popping a water balloon. I'll show some more results on a different dataset. Just again, to show that we get quite a similar image quality, but our method is much faster and much more computationally efficient. Thanks for listening. 