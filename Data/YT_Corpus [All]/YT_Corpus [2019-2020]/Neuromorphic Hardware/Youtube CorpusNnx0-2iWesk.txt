 you [Music] the next talk is going to be given by Professor Timothy Roscoe he's commonly and fondly referred to as mati I think he prefers that so I'm just gonna call him mati mati graduated from the University of Cambridge he worked on an operating system called nemesis I encourage you all to go and read read about it I think I have known mati since he was a graduate student ah after Cambridge he worked in startups he worked at Sprint he worked at the Intel research lab I lost track of that after Intel but he now teaches at the ETH in in Zurich Barry does his group does operating systems research hardware research and so on so forth I think I was a grad student so thanks very much for inviting me this talk is gonna be very different from what Jung's talk so it's gonna be I need to preface it with a bunch of disclaimers first thing is that unlike his talk I'm not going to show you much in the way of nice numeric results or indeed results this is very much work in progress and at the moment we're fairly confident that it's going to work but he doesn't necessarily work yet okay and you'll see you'll see that to go through second thing is that I am NOT a machine learning or an AI person I pointed this out to John do and he you know did not rescind my invitation as a result of this but I'm very much sort of systems systems software person as Charlie says operating systems and networking so this is very much from that sort of perspective someone who doesn't necessarily work much in machine learning has some friends who do but at the same time machine learning is one of the key sort of drivers behind some motivates kind of work I'm going to talk about here the third thing I'm going to do is is say that you know it's trendy so they'll spend about almost exactly half of my career post-graduation in industry but I'm going to talk about this work very much from the perspective of an academic okay which is well I an hour but this is a talk specifically about what it means to be an academic doing systems research right now right this is a very interesting time to be doing systems research because a whole load of things are changing a particular there's some very interesting changes with workloads machine learning partly enabled as people have said by these you know the advent of very high performance computing has itself then factored back and changed how people are thinking about the computing platforms that they build all right and that you know there are other drivers for that as well you know mobile phones and things have really internally now look very very different to the way that a PC used to look for instance in terms of the number of processors the kinds of processors how they're connected up inside the phone or in some basis inside the SOC inside the phone so it's very interesting time to be doing systems of operating systems research the workloads are changing the hardware is changing the question is what do you do if you like me or an academic doing research into system software okay and a particular the advent of a lot of custom software very diverse software and also reconfigurable logic okay at one level workloads like large-scale machine learning big data workloads things like that it's changing and the question is what do we do right and I'm going to talk about what it means for us to do that I'm going to talk about a computer that we're building in order to do research okay and this seems like a strange sort of thing today I mean it's a great excuse to build a computer but I'm going to talk about why we're actually deciding to build a computer for ourselves what that means given all this fancy Hardware out there why are we then building another piece of hardware okay that's really what this talk is about so I'm gonna start with an observation about what is happening in the hardware landscape okay almost all the interesting systems out there increase out at scale are increasingly using custom hardware they using chips that have been designed specifically for particular workloads or they using reconfigurable logic in the form of FPGAs or both okay there was a time there was a long period of time which I can remember the start of and I couldn't just about remember the end of all right when Chandu and I were young and enthusiastic about things computers were hugely diverse all right there are many many different kinds of computers and operating systems out there many people built their own computers but the hardware architecture was quite diverse and so the field of system software research was sort of navigating this quite diverse Hardware space okay then a few things happened the PC came along PC typically run you know a fairly standard operating system if you're out in the commercial world this was Windows if you're in the academic space or the hobbyist basis was Linux and suddenly everything became a remarkably standardized okay even even inside phones things looked a little bit like pcs but with ARM processors almost all the commodity desktops laptops sort of appliances were pcs also for IOT early IOT devices were little pcs basically and in the data center there were just clusters of PCs there's extraordinary standardization horizontal ization of the market okay and just recently that has all changed again it's changed in the phone space which I'm not going to talk about all the edge space because custom hardware is needed to keep the power down or lower ending all the rest of that kind of stuff and in the data center space which is probably where I'll be talking about most it's changed as well we have a lot of custom hardware a 6f PJs on chip accelerators beyond GPUs GPUs I think maybe one of the first symptoms of this but now you've got things like you know Google's cloud TPU widespread deployment of FPGAs in the cloud data centers places like Baidu Alibaba Amazon f1 you can go on to Amazon Web Services and you could rent a machine with some big big FPGAs in it download your bit stream onto those FPGAs and you know do your blockchain hashing or whatever it is you want to do on amazon's cloud and pay them in accordance for it a lot of people building custom hardware some of which makes it to market some of which is canceled some of which pivots into something else but projects like the machine from HP Oracle's rapid system the spark m7 has been very interesting database acceleration calls for big data Microsoft of course one of the pioneers in this area of deploying FPGAs and data centers in the form of the project that was originally called catapult now as your smart NIC is one of the applications for that particular piece of hardware and so on and so on and so on okay so this is interesting there's a lot of stuff going on in the way of you know custom hardware right that doesn't look like a PC however that means if you're a university you're doing system software research whether it's databases or operating systems or frameworks for machine learning or something else you have a problem which is what you have access to is essentially what you can buy which is sort of in this box here the available feasible hardware design space for systems for artificial intelligence or machine learning or almost any other application is vast ok and what you're getting is people a building you know cloud TP use or something else these little point solutions which are optimized for particular kinds of workloads where some deep learning or something like that around here they do this for a very good reason these things really work well ok but if you're a researcher and you want to explore this space you're stuck in this box this box is clearly not where the interesting action is so at a university what can you do about this problem what can you do to sort of explore more of this space that perhaps companies are not exploring because they have particular commercial imperatives there are particular time frames and horizons beyond which it is not easy to justify business wise to look if your university perhaps we should be taking a broader view longer term view that's why they that's why the the companies tend to fund us but we don't have any access to the hardware ok so what do we do ok and and I'm very lucky to be at ETH in Zurich it's a great place one of the great things about it is that companies like to talk to us a lot and we don't like dealing with companies and so the big challenge the first big challenge that you have as a systems researcher trying to do some work in this area is for all you need to get hold of some hardware ok get try and understand what some of this hardware actually looks like so this is where you hit the first problem simply getting hold of some of the hardware this is the version 1 of the cloud TPU just purely did inference inside Google it's very unlikely that any of us will ever get our hands on one of these unless we happen to work in Google ok alright and there's a good reason for that right this is not Google being mean they're doing this for very very sensible reasons right typically these pieces of hardware are developed Tiffany data center pieces of hardware developed by something as a service companies for their own internal use right there is very little value to them in enlarging the ecosystem of users of this hard work directly particularly when that hardware is accessible as a cloud service anyway okay so the very little point in google saying you know oh here's here have some cloud GPUs do interesting things with them because anyone wants to actually run inference on version one of the cloud GP you can do it by renting the virtual machines from Google and using it right there's very little value in doing that and there's also a subsea value in keeping some of the hardware smarts in this proprietary right you couldn't program this with tensorflow but you don't need to know anything more about that because that's valuable intellectual property for the company or at least the legal department probably think so but also there's a wall concerning reasons why Google would not want to do this right they don't want to have to support an ASIC that Asics probably built on a fairly aggressive process or at least it's pushing the envelope it may not be thoroughly tested he works well enough for Google and that's what they want and that's good enough they don't want to actually have to pay for support they don't want to have to actually deal with other people getting strange results out of these pieces of hardware that Google themselves can prevent happening because they actually control the entire execution platform so it's really it's a huge extra effort for Google to let anybody else use a cloud GPU it really isn't economic so it's unlikely even at universities you're gonna be able to get one of these things and it's not I'm not complaining right I can see why this happens ok somehow though occasionally universities some universities we're fortunate enough to be one of them can get hold of some pieces of hardware so for example this is the version one of the Microsoft catapult board okay which is very interesting idea of being able to put an FPGA on the data path between server and the network and use it universally and since on a PCI bus and whatever it's a it's a very interesting thing this has evolved gone through several more you know revisions it's deployed widely inside being an azure the challenge of getting one of these things is figuring out how it works once it's arrived so actually most very very kindly donated a bunch of version one of this because we're doing a lot of work with FPGAs accelerating big data workloads and machine learning on a few days so a box of these arrived at my colleagues office one day with a you know very nice note from from Microsoft saying you know here they are and no documentation we eventually did get some documentation in the case of the of these boards but other piece of hardware do arrive with very little documentation and that's probably because the companies often don't have this documentation they built this the hardware designers are sitting next to the programmers why would you need documentation it just takes time and time to market is critical the documentation falls by the wayside okay and in some cases the lawyers really don't want you to release the documentation you can lend people the hardware but don't give them the documentation because the documentation says things and that has written snapmare have statements written down that could get the company into trouble okay so once you've got the hardware if you're a research early university doing something with the hardware is stymied by need the documentation but the big big big big problem right like for example we've got a bunch of catapults we eventually did get the documentation they're very cool we did some work with them but the big big big problem with almost all of this hardware from an academic perspective is the big decisions have been made it's been designed for a particular kind of workload particularly use in mind okay it is cost optimized right it is designed to be deployed commercially it's designed to provide the best return on investment possible at in volume and so there's a lot of cost-saving specializations both in the hardware and in the drivers it's range of you know testing but also design is quite narrow okay and so you're gonna spend a lot of time finding bugs in the hardware that are not bugs from the company's perspective because you're trying to do something with the hardware that was never intended by the company in the first place or you're doing research on Rails you're doing research into what the hardware designer thought you should be doing with the hardware anyway in that case the people who are using this hardware commercially are way ahead of you and so your research value may be problematic even fairly general pieces of purpose pieces of hardware this is an intel part you can buy this this is a single package with a skylake modified skylake processor on it and an FPGA very closely coupled to it and you'll see this is quite close to what it is that we're interested in building this is Intel's view of what you should be using to experiment with FP J's and this is great it's very cool thing except for us this FPGA here is a long way from the rest of the system Intel's model is the FPGA is something that helps the processor do what the processor was doing anyway and so you're sort of you're restricting the model again quite a lot by what Intel's view is of what you can do this is a very cool part you can buy this it's a product we had some until very kinda gave us some early access to this hardware but it remains you know limited right there's certain things that we might want to do that are very hard to do on this kind of system so we've talked about this a lot because we've had access to a lot of custom hardware at ETH and we've we've dealt with we've worked through a lot of these problems and so we we found ourselves thinking this way what if we actually got hold of a hardware platform that was not optimized for running a commercial workload it was actually optimized for exploration it was optimized for research what would it mean to build a computer for research people he used to do this back in the 70s they have not done it recently okay it's optimized for exploring the design space of hardware and the implications of that design space to software okay so it's not designed to maximize performance per watt or unit cost it can be more expensive right but it should be over right it should be as flexible as possible much more than you could ever commercially justify - you're selling this to people you know in business maybe these days you know you want you don't just want just one of these as well you wanted as a building block for building bigger things so what if we had a research platform that actually allowed us to massively explore the the hardware design space and build our own very complex machines that were really not constrained by the custom hardware people are already building for their own particular commercial needs could we do this right and of course you know we said you know our university we can't build computers you know even ETH which has a long tradition of building computers at Niklaus Viet built a whole series of computers in eth with interesting kind of features but its universities have typically not done this at scale right they've done it in the electrical engineering department but then build the kind of computers you'd actually want to use to do real computer science whenever you think about doing it you know you think well you know we're limited to some extent by our imagination here is what our imagination came up with when we were discussing this mythical computer that would actually help us with our research it's not doesn't do everything it has its limitations has its compromises but it was based on the kind of research we were doing and the obstacles we were hitting right where we got into trouble all we'd like is a server a proper realistic server decent memory decent network bandwidth decent peripherals you know proper cause right processor kind of processor that you might actually have in a datacenter and what we want is very close to that ideally connected by some very low-level protocol right the biggest FPGA we can find something is def PJ we can find but I'm the biggest in the fastest FPGA we can find it okay and this FPGA should have a lot of memory itself and a lot of network bandwidth in fact all the network bandwidth you can stick on this Fe J plus you know some peripherals and things if we want it okay this is a realistic today's computing platform this allows us to explore the hardware design space having the two together gives us a very powerful platform for exploring things this is what we thought we wanted based on our research you know I colleague cast our vote on Lonzo and I have been doing a lot of working FPGA is operating systems databases these kinds of things wouldn't it be great if we had one of these things we asked first people if they would build it for us various companies if they build it for us and people said well you know this is really expensive and there's really no market for this so we thought well maybe we could actually build it ourselves and one of the things that has driven this diversity of hardware is also but you made it quite easy to build hardware it's not easy but it's easier than it was in this sort of post PC era we're actually entering a phase where it's becoming reasonable to think about building quite complicated pieces of hardware even on a universities kind of budget rigidly we have a quite a nice budget but it's still an academic budget we did not want to rely on standards okay so other things you'll notice in this this diagram here is that I talked about this sort of cache coherent interface here there are some standards out for this in our opinion they're all problematic we don't like any of them from a research perspective there might make sense commercially in the short term but in particular we did not want PCI Express if you want if you're prepared to accept PCI Express as the way of connecting these two parts of the system you're in good shape you can go out and buy cards from Xilinx or from our terror or so that'll do this stuff we did not want that and there's plenty of reasons why we had you know P so Express is good for what it did it gets in the way for a lot of the work that we're doing right so what can we build one of these could we actually actually build a computer that did this job surprisingly we actually thought we could we can use it for two things right the design I've shown you is good for emulating possible new platform designs right we can use that FPGA to make it look like all kinds of things particular we can use it to make it look like a catapult or look like an Amazon f1 instance or look like you know other things that are done with FPGA is and we can even do little toy versions of TP use and things on there not as fast as an ASIC but nevertheless we can try things out but actually perhaps this is actually quite a good platform anyways purpose hey that fpj gives you huge flexibility so it's an interesting question ok and we managed to get hold of some hardware we cobbled together some hardware that particular company called caveum gave us this is an evaluation board for their version one of their 48 core 64-bit ARM processor ok what's cool about this board is that this processor is a dual socket part that's it has external cache coherence but this board has only one socket and it has three connectors and that actually carry the cache coherency traffic onto another board so you basically they gave this out to OEMs and if you want to prototype a dual socket system you just plug two of these boards together turn it on it boots up in a two socket system we didn't do that what we did was to plug these connectors here into an FPGA board which we also got from Xilinx with the biggest FPGA we could find which is the exceed 9p at the time this is the one that's used in Hamden f1 and by plugging these things together we could actually try and implement something that caused us to talk to the caveum cache coherency protocol and caveum were very very helpful at telling us how this stuff worked and giving us access to a lot of information okay and so we actually built it it looks she looks like this this is my postdocs office here's the caveum board here are these three eye watering lis expensive cables that carry cache coherence onto this board which we built ourselves which is really just an adapter board that plugs into zai Lynx's evaluation board over here okay and this machine is the initial version of something we call ends in your boots it runs Linux but also runs barrel fish the last operating system we built in the group and they're actually two of these you can see there the one in the background there but this next picture for this to work on okay this next picture shows you actually have two of these systems together and this is what we've been basically working on for the last year so it's building these things together now this is a big clunky machine I said we didn't need to be cost optimized or power or anything but I want you know if I'm gonna have a research platform like this you know III 2 of these is fine but I want a hundred of these in Iraq and we're not going to be able to deal with a hundred of the we actually want is to shrink all of this stuff down to a single board and actually put a lot more stuff on it okay that's actually what we're doing right now actually in the process of doing that right now and this is the design of the board that we actually want and we should get prototypes of this back in about five weeks from manufacture whereupon we have to get all the software working we have a very short amount of space at the time to get the software working but this is this is the board that we think represents the right compromise between what we can build what we really want to explore that huge hardware design space we've got four hundred gigabits of network bandwidth over here 16 25 gig sewer lines you can configure these as Ethernet and whatever we've got half a terabyte of memory here it's not gonna be enough but it's good right half a terabyte of DRAM on the FPGA is fine 128 gigabytes is pretty much all the the memory this Thunder axe will take so fully fully stuffing this 40 to 40 gig Ethernet interfaces is fully utilizing all the ethernet hardware on here basically everything is maxed out this is the biggest thing we can fit on one board deliberately so maximum reconfigurability maximum flexibility we don't care about unit cost turns out unit cost isn't so bad it's totally dominated by derail so everything else is kind of lost in the noise apart from this and Xilinx have been very very helpful and generous with us with these chips but then you've got you know PCI Express you can plug your GPU in here if you really want to nvme nvme piso express all the usual stuff but the really key thing is this very close coupling here right we can send data onto custom hardware and here back to the processor extremely fast an extremely low level without having to go through any of these sort of buses or anything like that as I said this surprised to my surprise because I'm not a hardware person first time I've done this we're actually very close to having one of these things actually built we're having 15 of these things actually built which is the first production run this is actually what its gonna look like this is well sent but sent back by the design house who we contracted to build this is this pretty cool this is a render what the board's going to look like for me is the sort of you know software person it's really cool to be able to play with these things this this thing here is the the FPGA this is the caveum processor got all the usual connectors here USB SATA and stuff ah said designed for flexibility and also designed for reliability rather than price so we cut corners so for example these connectors here actually fly over connectors for very high speed signals it makes the board easier design makes it easier more likely to work the next time less worried about signal integrity all this kind of stuff here's all the all the network interfaces are on the back here two different PCI slots one for each side most of all I showed on the previous diagram you can sort of recognize on this board I thought this is really cool I've never ever built Hardware before and to my surprise at a university I could actually get this stuff done hopefully it'll work it arrives pretty soon now we obviously don't want to hang on to this just for ourselves we'll keep the first 15 assuming they actually work but the goal here really is that this is this is is really how we change how the system's community the operating systems community the database community and their systems for machine learning community can actually work we've talked to lots of people who think that this would be very useful to them may be useful to you our goal is to build a lot of this do these things and seed them out to where the universities this is a an attempt to shift the research agenda into something that is more exciting but also more relevant in the longer term and to get operating systems research and other kinds of systems research out of this sort of box that it's got a little bit tied into so there's precedents for this you know net fpga you may be familiar with planetlab system i built a while ago at Intel DP DK even BSD our ideas of the research community building artifacts that helped them then do their research this is kind of one of those things and so that's really what I wanted to sort of talk about hopefully in the next few months we'll have the first ones back and then you know we'll start talking to other people who have already expressed an interest in having them and start building a community around this kind of platform rather than commodity pcs as a way of doing research lots of people have been involved in this it's quite a big team and we tons of help from caveum and Xilinx they've been really really nice to us these are the current people on the team it's probably going to change next week as well we'll get some more people on board and with that I'm done now so the summary here is that common off-the-shelf hardware even GPUs to some extent these days they're not necessarily realistic for this kind of work in the future if you really want to be out there in the future you need to get ahead of people building custom hardware and this I think is our best bet for getting ahead of the custom hardware people because the custom hardware people will already have made a load of decisions that will restrict the kind of exploration that we can do and so hopefully in a few years time a significant chunk of the people will actually have access to platforms like this or something like it and we can actually explore this design space it's pretty cool it's a pretty exciting way of the rethinking system software and the light of this kind of reconfigurable Hardware so that I am done thank you for listening I'm happy to take questions [Applause] your team that this board should come see the light of the day very soon just one question do you want to do you have any donating neither we can't donate it we're not a charity we're not interested necessarily in making a huge amount of money out of it that may just be me I have a slide you said I've died I've worked for startups and I I survived are our goal at the moment is to make this available to the research community essentially at cost okay so our sort of template is net FPGA right which was done about five six years ago it's a PCI NIC essentially without fpga on it what they did was they set up a non-profit company is essentially got the range for the manufacturer distribution of these things at cost we've been talking as eiling's the big unknown about the cost of it is the cost of that huge FPGA and Xilinx are very interested in providing that to whoever we get to build these in volume academic pricing or say to two kinds of pricing one for academics one for companies that might want to use it we have had interest from companies in actually acquiring a few of these things at even Microsoft there's a co-ed Microsoft Research she wants to yes first challenges the boards are not available I'm glad that your team is taking care of their children second challenge told that the documentation for the board is also not here I hope somebody in your team is taking care of that hazard documentation for this for our our hope is to actually well first of all provide a lot of documentation a lot of useful example software and everything but also to make as much of the design open as possible there are bits of that design that we're in negotiation with companies to actually talk about even if we even if we can't talk about some bits of it we think we can wrap those up into the FPGA shell and then still without impeding the flexibility of the rest of platform so the goal is to make this absolutely as open as possible yes this is an academic thing open-source is good more information allows people to do more stuff so absolutely outside that's our goal okay I'm sorry very good belong to me Viktor - yeah so I think this is great I actually do I think gone so many of these sort of things in the past but I have a bunch of questions I'll reserve them for later with you but I want to ask you here in public what's what are you burning to do with this board right now what are we burning yes so what I mean by that so many of these so you know I also have some experience doing things like this in the past and and so there's a sort of difference between what gets adopted and what the street runs with versus what papers get written by in terms of the research so is there something that is well other things that we can't do right so I so I have about you know in back up here I have about fifteen different slides each of which has a different research project we want to try out on this I didn't run didn't run through the product it's a time but because many of them are not to do with machine learning but we have a whole list of different applications we'd like to use this for and use it in various different ways you can view the FPGA as a gatekeeper you can view it as a smart NIC as a monitoring system as an accelerator as a memory controller there's all kinds of different things we have lots of ideas there it is a compromise you know but it but it's given us it gives us way more flexibility that we can get with any other platform we could buy from that perspective and so before we started we had a whole load of ideas for doing this but there's another benefit to actually going through this process which is one of the reasons that I'm still smiling after all this stuff which is that we never built a computer before we've never known you know this is a you know it's an eighteen layer AIT export this is a big deal it's been really an interesting experience learning about this I've had a superb postdoc who's really into formal methods os design and hardware and he's been doing a lot of the heavy lifting on the hardware side of this stuff so we've learned a huge amount about how modern computers work which you don't tend to learn we don't teach and it could be in a university curriculum and we could not not good about teaching curriculum that itself has generated a whole lot of other ideas so it's been a very generative project even if it never works we you know we have come up with so many interesting questions because of the different perspective we have gained on computer hardware as a result of doing it so that's an additional benefit I mean these are hard to quantify but fortunately at a university there's less pressure on quantify these things so it's yeah III would say it's been it's been worth all the effort so far um will it have been all the way they have but so far if we never get it working maybe not but yeah it's close it's been a journey with the software into pieces for such a many many thoughts we don't think one size fits all because there are so many different researchers cases I mean this question of whether you you know in the system that you're evaluating whether you make the FPGA visible or you try to forget about the FPGA X it's emulating a different piece of hardware and stuff so all of those will really change how the software works into in the specific case where the FPGA is part of the application right it's actually doing functioning like an accelerator or a smart Nick the right way to do that software interface is either one of the several of the research projects we want to do at different levels right so there's how do I interact with an accelerator what is the kind of accelerator design we want if you talk to operating systems people about the in the software interface to hardware you know they can happily ramp for ages about how broken this is and how hardware designers don't understand what it's thinking it means to program these things but if you ask us what it should look like we go kind of quiet because we tend to lack the language to talk about the rights or the hardware software interface ourselves one of my goals in doing this is to get a better understanding of that so I can actually say what the right hardware software interface is from an operating systems perspective and try to encode some of those rules ultimately because I've also found myself drifting into formal methods recently what I'd like is to build a writer specification of the software and an interface to a piece of hardware that is emulated on FPGA in its in in some formal language and have it generate both the driver and our log for the hardware at the same time and then I can build into that compiler a lot of the tacit knowledge that operating systems people have about the right way to talk to hardware you know enough asynchrony self virtualization protection you know access control that sort of stuff we can actually put into the compiler as a way of encoding what we what we've learned to talk about in the process of doing this but don't know these are these are really open questions and and I think they're important ones okay code that the current state of the art is very I don't and I think we can do a lot better as scientist [Applause] 