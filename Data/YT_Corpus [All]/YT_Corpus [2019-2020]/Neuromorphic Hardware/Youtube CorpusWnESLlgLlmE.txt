 (soothing music) Hello, I'm introducing the perception and robotics group. That's Yiannis Aloimonos and myself, Cornelia Fermuller, and we are working on vision, but for, vision for systems. So, we consider vision and motion together as perception doesn't exist in isolation, but systems of vision in order to do things, in order to move, so, that's the common philosophy. And, the activities in our lab right now, the ARC Lab, center around two things. One, that's manipulation actions. So, we are interested in people doing things with the hand, cooking, moving things around. And, the work has to do with robots, perception for robots, but we also process videos. The other branch that we're currently working on is autonomous navigation, lower level tasks, that have to do with drones, cars moving around, and again, they see in order to move. So, for example, here, some work the drones need to solve a problem, a rescue operation, and we work also here with multiple sensors center integration I'm going to tell you about. So, manipulation actions. Somebody's cooking. It's a very difficult activity. It involves the perception, we need to understand what's going on, what are the objects, what are the people doing, where are their locations that we're interested in. So, that's computer vision parts. But, you can't just understand a complex activity by processing edges and building it up, you need knowledge too. So, here we are working with language processes and also in collaboration with linguists, to get this high level understanding. And, then you understand the things because you're doing them. And, here, we're working, both theoretical, but with robots also, the application being that the robots can learn how to do things. So, some example, for example, for a video projects, purely video, is the task of, now we're interested in introducing common sense knowledge to help AI. Things like for example, you have an idea about a high level activity, you know how to make a breakfast. You grab the milk, you pour milk, you stir, kind of general idea. How can you use this knowledge? Now you're giving videos, you don't want to use supervision but just at high level knowledge in order to learn how to assign labels to the different parts of video and segment. Or, a lower level task is we want to look at videos and understand actions we haven't exactly trained on. So, we want to generalize on verbs, that's an ongoing projects. So, we can describe these verbs in terms of what is the effect on the objects, the topology? What's the general movement and then use this together with the objects and linguistics to learn on, so, that we're given a new video, we can understand it. For example, we may have seen cutting and chopping, but we have never seen slicing. And, that should help us generalize. So, why do we do manipulation actions? We are interested also in robotics and we have worked with humanoid robots in our lab here Baxter. In this case, for example there was a project where we processed videos from the internet, cooking videos. We built schematical representations, hear the language, like understanding an action is, it has a structure like language has. Then we built this schematical structures here in terms of the tree, and then the robot is executing this structure. So, let me see here some video that was recorded we then invite it to the top of what we're and put out this presentation. It consists of two tables, under one table, a person preparing a drink. On the other table opposite is our robot doing mixing the same drink. So, the person has your juices in front of him, orange, green. In the case that he puts, for example, the green juice and the orange juice at the same time, Baxter can only use one at a time. She has to representation and know through planning process, executes it, runs down all the processes too, how to do the actual manipulation and carries out the task in her way. Under the hood, you have a lot of things going on. You build models of the objects, you monitor the liquid, the robot is executing planning, you have on the hand cameras to do the manipulation. And then at the end, a lemon was added, and Julia would fix you the same drink, Julia, because she's a cooking robot. So, what else? What else is going on? We're excited to have in the lab, a new arm with a shadow hand. It's a human hand like structure, it has tactile sensors on the fingers and we are trying to combine and study to do tactile processes, together with vision, how do you execute it? How do you do slippage? For example, why would you want to learn tactile processes? Right now in robotics, any tactile processes thought in what we call kinesthetically, you'd grab the robot, you touch it, it remembers exact the execution. Eventually, you would like to do the thing on the right. You wanna, you have trained your robot. It has a general understanding. Now you show it and decomposes it. That's the global idea for robotics. But if we envision, I talked about embodied perception, we do understand multimodal, so, there is, from neuroscience, we know we have even cells there that we see false body and vision measurements, the so called mirror neurons, neuron system. So, one for example, project was we collected data bimodally. We strapped people with four sensors and asked them to do things simultaneously and recorded and we trained a neural network for motion to them for ascending, that's what you're seeing. So, you request these two forces, now, if you use during training those acquired for sensing, during testing, you don't have to force this anymore. Then if you combine it you can show that actually because of the low level representation, you get better perception. You have learned this representation. So, projects that, will, we are interested in, introduce machine common sense concepts. We are interested in combining data, forces, eye movements, body movements to do this embodied perception and under the hood, there's a lot of problems to be solved the low level. Point cloud processing, all these bottles where we reconstructed, we're working on new processes. We're working on basic concepts like for example, symmetry for segmentation. And then the coupling happens due to reinforcement learning that's now big interest in the lab. The other activities, visual navigation, with applications in drones, and also for autonomous driving. And the idea being, but currently, the main approach is so called SLAM. You make a map of the world, a 3D map, then when you have this 3D map, which is of course computationally expensive, then you can do other things, you can plan and move around. But such a computational heavy process is difficult to transport to a small processor like a drone. Instead, inspired by biology, simple insects, birds, they solve this problem directly in the coupling. If I wanna do obstacle avoidance, I really don't have to have a map of the whole world around me or whatever, the visual scene around me. I just need to know where's that object, in which direction, since, the essential information. So, along this line we are solving ego-motion, independent motion detection, obstacle avoidance, target pursuit, then making a home to go to it, very interesting problem, what do you actually store? How do you land? So, here's for example, a recent paper called EVDodge. Here, the drone needs to dodge. Things are thrown at it. It has to recognize these other things, or other drones and avoid them. It is stuffed with two new sensors, one looking down and one looking frontal, that only see motion. It detects this motion and then avoids them. And that all runs in real time. And if you can do that process, you can also invert them and do pursuit. So, here, okay, that drone wasn't happy. So, one of the things that's essential and interesting to study is here we're doing neuromorphic sensing, that's an approach to build hardware that's bio-inspired which I work with this community. And one of the sensor that comes out is a motion sensor, it only sees motion. It's built in analog. It's not just an academic activity at this point anymore, but also, Intel gets into it, they built processors, and the camera will appear at the end of this year by Samsung, in big quantities, at which point it will cost $10. So, this camera only sees motion, this is a visualization, it only sees where things change, if the intensity changes, which you see here visualized by getting brighter or getting darker as black and white. And the advantage is it has very high temporal resolution, it can see from very bright to very dark. Is has since it only records the changes, very low bandwidth, low latency. And now we have this new signal and we want to study it. It's like point clouds. So one approach is solving the problem of independent motion detection here visualized. If we find out kind of what is the motion as the sensor is moving over on the right, then the things that are not conforming to that model are standing out. And the basic idea is we have to find algorithms that align and what isn't aligned is moving independently, getting a little bit more complicated, but that's the idea. And we also do now neural networks, so, we have done unsupervised neural networks. The idea is basically since the flow is coupled to the scene and the motion, you can learn in an unsupervised way. If you estimate correctly the image motion, you can train the depths and the 3D. So, these are some scenes from the DVS. And we created recently a dataset, that's work going on. And if you will be working on this research, there is problems on studying, this events-base. There's lots of applications now to solve the corner cases of autonomous motion, to develop learning algorithms that work with events, because it has to be real-time, one thing at the time coming on and applications in drone research and also collaborating with hardware people. Thank you. (audience applauding) (soothing music) 