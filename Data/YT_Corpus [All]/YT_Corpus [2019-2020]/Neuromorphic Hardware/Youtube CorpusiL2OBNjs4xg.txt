 good morning everyone we are about to start our next transition of nano explorations welcome it's my absolute pleasure to introduce Emily to me from Karl Bergin's group she is dr. Emily to me as of a week and a half ago when she defended her thesis and we will be treated to the highlights of that defense at today's nano explorations Emily please take over yes oh I'm sorry Allah just make one more very quick announcement which is that please turn off your videos it does speed up everyone else's web connection you can just say stop thinking on your end if you have questions please hold them towards the very end and you're gonna be able to provide them to me either by sending a chat or indeed by raising a hand and I'll call upon you but with that being said Emily please do take over all right thank you so I am part of Carl Perkins group the quantum man instructure and then a fabrication group and today I'm going to be talking about how were using super Anna wires to develop an artificial neuron intrapsychic neural network so this project really grew from the fact that today we're facing a bit of a computing process due to the rapid expansion of technology that really had started with the development of silicon based CMOS in 1963 so if we look at how technology has advanced since then you have simple devices like the handheld calculator but then microprocessor use with space exploration and then finally something as small and complex as the Apple watch but looking ahead to the type of technology that are currently being developed we have things like autonomous vehicles social media the Internet of Things all of which are requiring massive amounts of data at unprecedented rates and the hard truth about that that the hard would leave the light on to get us to this point is now struggling to keep up so for instance if we look at trends and microprocessor performance over the past 50 years or so and we look at critical performance metrics such as operation frequency here in green what single thread performance here in blue you can see that they once increased linearly with time up until about 2005 at which point they started flatlining each issues with heating and scaling on vacation you can also look at this problem from an energy perspective so here are projections for today's system level energy cost of computing in comparison to the world total energy production and you can see that based on projections were estimated to surpass the world's total energy production as early as 2035 so unless you're a huge optimist about the world energy production suddenly increasing exponentially we really need to rethink the way that we do think it in in order to have more powerful overall more energy efficient systems and I think the way to do that is really a 2-pound approach so the first is to investigate alternative computing schemes that are inherently more energy efficient too powerful and in parallel with that is to reexamine our hardware to look beyond so again for low dissipation materials that can then support these alternative communities so recently people have been inspired by the human brain as a source of inspiration for building an alternative architecture but just for comparison I've shown with our traditional architecture here on the left-hand side I'll define oh I'm in the architecture and what I want to draw your attention to is really you have this input and the output but more importantly you have a separation between processing where things like logic control happen and memory or information restored now in order to do any operation which means that information has to be transferred between your processing and memory elements which takes a certain amount of time now the problem with that is that our processing is continuing to speed up but memory has come as lag behind and so that time ends up becoming more and more of a problems with three or four tiers the memory bottleneck problem now in comparison with our traditional von Neumann architecture the human brain by contrast is highly power it's very parallel meaning that one braid of cell or neuron next to hundreds of thousands of other neurons and additionally it communicates using spikes in comparison to the overall high low signal city and then furthermore the memory and processing elements are actually united together within the single brain cell which again is a huge punch to the separation we have no additional architectures so basin's appealing characteristics people have been building what's called neuromorphic can get in architecture it's inspired by the human brain and perhaps the most bio realistic of these types of architecture is called spiking neural networks where you're trying to mimic the actual spiking dynamics of the human bringing in order to compute so things like low-power computing and machine learning so you can do this in software where you actually encode all of these dynamics and this gives you very precise control but once you start expanding to a very dense network it becomes quite computationally expensive so instead what research researchers have been doing is looking at hardware approaches so looking at devices and simple circuits that naturally generate the type of spiking behavior on their own and so this has been explored and a wide variety of platforms such as magnetic materials CMOS and then history and all of these have their own advantages but all of you still struggle with power consumption in comparison to the performance that we see in human brain and so as a result there remains a massive need for a low-power hardware that can naturally generate spiking on the phone in order to really see this alternative architecture through and what I'm gonna be arguing forth today is you think superconducting nano wires as a low-power platform for these types of alternative architectures so just to give a basic overview of how stupid muttering nano wires operate I've shown their current voltage spot here on the bottom left so essentially when nano wires are in their superconducting State it can pass current through them here on the x-axis without developing any voltage or resistance because they were superconducting but eventually you reach a point from the critical current or switching current at which point super conductivity breaks down and then an awareness this is a thermal process so the undergo Joule heat in and then form what's called a resistive hotspot here now in this portion of the curve where the resistance is actually quite high on the order of one to ten kilo ohms now the nanowire stays in that resistive state until the hotspot cool sound and you move the bias current to a point that's the cutlery chopping current and then you regain superconductivity so because you have this transition between the zero voltage superconducting state and this high impedance for this would see nano wires are really great for driving large voltages while maintaining low power additionally they have known static power dissipation and their interconnects so it has been a huge problem for some month architectures so it based on these features they've been use for a variety of applications traditionally they're used for single photon to deflect them and I've shown a scanning left a micrograph of one such device here on the on the bottom right and you can see if they're quite small which means if they're great for being densely packed for scaling but more recently in the past five to seven years or so we've been taking advantage of the different characteristics to develop new circuit elements to think of amplification and memory as well so how do we understand these devices from an electrical engineering perspective and what kind of simply put we have this transition between the superconducting and resistive States modeled as a resistor that's in parallel with a switch that relates the ground but what I want to draw your attention to is the series inductance here now superconducting nano wires have this really unique feature for their intrinsic conductance called kinetic inductance and the important thing about that is that it scales with the length of the structure for a fixed width so essentially the longer your nano wire is the higher your inductance is so you have this really great tunable circuit element that can be changed simply by changing the geometry at the structure so how does that matter well I mentioned not only need you need low-power hardware but you also need hardware that naturally generate spiking and we can take advantage of this intrinsic inductance coupled with this transition between the super connecting a resistive States in order to generate spike in our system why we do those is that we put the nanowire in parallel with a shunt resistor here labeled RS and the bias current oscillates between the nanowire and the Sun as the Nana wander memoir undergoes transitions in order to generate spiking so up at the top here I've shown kind of a simplified schematic of one such all defaults you can see on the rising edge it's quite steep and that's what happens when an ax wire has transitioned and that resisted regime the bias current is now being redirected from the nanowire into the shun with the very fast elevator our time constant dominated by that Killa on the impedance of our resistant hotspot now conversely on this much slower falling edge Bethel - from the nanowire has regained the superconducting state the bias current is now being redirected from the shan into the nan wire and that elevate our time constant is dictated solely by the kinetic inductance of our nano wire and they are a touch on position so in my previous work I am studying from of how these oscillations are changed by L over our time constants and what I found is that when you have an elevator time constant on the order of one to ten nanoseconds so you get sustained pulses in what we call the relaxation oscillations in our system now relaxation oscillations certainly are not unique to the super flexy nanowires we're in a lot of physical systems and perhaps the most dominant one is along with in our own heads so I mentioned that biological neurons have these electrical spikes with action potentials and I've shown a simplified circuit schematic of that here and I want to draw your attention to really this rising edge and this falling edge of the action potential now this rising edge is what happens when it's called the sodium ion channel opens up and sodium floods into the cell which raises its membrane potential converged on this falling edge that's what happens from the potassium ion channel opens up and potassium floods out of the cell which essentially resets it and allows it to fire again so overall you have the spiking that's dominated by the two voltage-gated ion channels so we were inspired by this and decided to design our own nano wire based spiking element or soma using two relaxation oscillators and now obviously it's the two ion channels and biological mad but I've shown the overall circuit schematic here we have this main oscillator which is analogous the sodium ion channel the potassium I later here and they're linked together within superconducting loop so to show you how this works in simulation initially we had a bias current coming in from the top where we buy us both oscillators right below the critical currents so they don't switch however if we then send in an input current from the left hand side the control oscillator done with fire because the input current and the bias current are in opposite directions so the main off does fire because if some along the same direction so once it fires it adds current into the superconducting loop similar to the sodium influx that then causes the potassium oscillator to fire in the opposite direction removing current from you take the potassium outflow so if we look at what happens with the overall loop current you can see that it increases with the firing of the main oscillator and decreases with the firing of the control oscillator then the overall voltage signal that we're sending out it's really the superposition of these two relaxation oscillators so once we have a basic circuit schematic we can start simulating what types of fire realistic characteristics were able to achieve and we explored a variety of them but here I'm just highlighting the two that are pretty much universal all biological neurons so on the left hand side I've shown what's called the threshold response meaning for a fixed bias for a new membrane potential what's the minimum amount of input current required to get our neuron firing and if you look here at the x-axis you can see that we needed about more than for my grams of current to get our neuron firing on the right hand side of chrome what's called the refractory period and we take that to mean the minimum amount of time between two input pauses such that both elicitor on outfit spikes so on this top panel you can see the impulses marching red you can see that this sufficient amount of time between them for nanoseconds so that both have their own outfit spikes however if we reduce that time by half now in the bottom panel you can see that only that first input policies you look at the puppet spike that's because you've run into that refractory limit of our system meaning that our main oscillator isn't fully biased by the time that second impulse arrives so once we have spikes being generated how do we use that to control the behavior of the downstream neuron so typically in a biological system what happens is you have this slow chemical release of neurotransmitters in response to these rapid electrical pulses so to mimic the same types of slow dynamics in our system what we can do is use the charging and discharging of a large inductor to essentially accumulate current and then believe that into the target neuron downstream so I've shown two examples of that in action on the left hand side we have the simulation of explanatory control meaning that we're trying to get our target neuron firing so that the top panel shows we bias the main neuron in the positive direction if you look now at the middle panel you can see that we've accumulated about one microgram of current on that synapse inductor which then causes that target neuron to fire conversely for inhibitory control on the right-hand side we can by us our main there on in the negative direction that accumulates a negative friend on the synapse conductor which then stops are not inspiring but having control isn't enough a really important characteristic of the human brain is that we have changing connections between neurons all the time whenever we learn something I forget something and similarly from machine learning applications we'd like to be able to have synapses that have different weights of strength we need some type of tunability and one way of doing that in our system is to take advantage of another form of non-linearity and then wires and that's the relationship between kinetic inductance and bias currents I've shown one the original reference this year on the left but essentially what they found is that if you run a modulating current through a nanowire close to its critical current you can get an enhancement in conductance by about twenty percent so it becomes a tunable inductor so we decided to incorporate an ANA wire as a tunable inductor in parallel with their overall synapse and run a modulating current through it that way if we increase our nanowire inductance we increase the overall parallel inductance of our synapse which decreases the amount of current being sent to our target so essentially we're reducing that's not constrained so to show an example of that in action and our simulations we have one main melon connected to four different target neurons each with their own individual synapses now initially let's say we're trying to inhibit all four target neurons so we can have all the modulating currents turned off and here now if you look at the response in the right hand side who have our main neuron biased in the negative direction so that now in the central panel you can see that all the snap synaptic currents are about minus one messing around so consequently if you look at the output voltages in the bottom panel which have been shifted on the y-axis for clarity you can see that all four target neurons are inhibited for a short period of time now let's say we don't care about target neuron number four anymore we no longer want to inhibit it instead what we can do is turn on its modulating frame and now if we look at its snap to current which was here in this pink trace you can see that it's receiving less current than the other three targets so as a result if you look at the voltage traits now you can see that targeting our number four is no longer inhibited but the other three targets remain inhibited so not only are we able to get some level of parallelism in our circuit but we're also able to get adjustable connectivity without disrupting the rest of our circuit so once we know the synapse design we can start making some basic estimates about its energy performance in comparison to existing technologies and the typical figure of Merit used in these systems is synaptic operations per second per watt which is shown here on the right hand side and we compared the nanowire neuron and simulation to the human brain and then also to to existing CMOS technologies to north and mirror grid out of IBM and stanford respectively and you can see that based on our estimates even when we include cooling costs we have a figure of Merit about four orders of magnitude better now I want to be kind of cautious about these exact figure of Merit comparisons because our Institute in simulation where I was pure north and NeuroGrid are actually measured on chip experimental II so I'm not taking these as one of the final say in figure of Merit but really what we were trying to show here is an argument for further development of a nanowire nod at the technology that could potentially be very competitive from an energy standpoint so once we have our design for the soma we can actually start building it and this is the exciting part so I won't go into too many details about the fabrication process but I could certainly answer on afterwards essentially we had two electron beam lithography stuff the first was to pattern the titanium gold resistor so circuits of shunt resistors and then the second was after we deposited our super connecting film which was maybe a nitride was patterning those animal IRA structures so here's our scanning electron micrograph of an example the fabricated Summa you can see that we have this main oscillator and the control oscillator which are now linked together within a superconducting loop with our input signal coming in from the left the bias means puff and the output signal going right I should mention this is done in positive tone resist everything in darker areas no in my chart and then these light gray outlines are all the underlying thermal oxide so if we zoom in on one of these oscillators just to see what it looks like you can see we have our very narrow 60 nanometer searching element in series with these long nanowire inductors in order to get these longer L over our time constants the relaxation oscillations and all that's in parallel with our contain gold chunk adjuster so the first thing we can do experimental II is look at the response to the soma T input pulse and compare to what we got in simulation so I've shown the simulation on the top it makes your mentleman bottom then you can see that we have pretty good agreement both in terms of amplitude and frequency of our signals and we can also look at reproducibility of these pulses so the top trace shows an example of when there's so much like three times over the duration of an input pulse and I've marked the two interspike intervals as delta T 1 and Delta 2 2 now if we take a hundred sequential captured waveforms and kind of time tag when all of these welted fall for her we actually get a histogram for each of the individual feet so there's some finding together and on the right hand side if we take an overall histogram with delta T 1 and L 52 we can see that we've had a mean interspike interval of about fifty point four nanoseconds so if the standard deviation of about 6.5 nanoseconds and this is actually comparable to what you get in human motor neurons where the standard deviation is about five to ten percent of the mean and defendable we were able to get some by realistic timeline characteristics out of our system as well additionally we can look at firing probability and we measured this on the y axis as a mean voltage output at five hundreds of Punky measurements meaning that if the Simla fire is more often its mean both the job that is higher we measured this as a function of input current here on the x axis and each of these individual Teresa's corresponds to a different bias current arresting control and the important takeaway from this is that in comparison to that simulated threshold measurement or we got that really clean stuff like behavior here we're getting something that's much more F shaped or sigmoidal and that's actually due to the underlying stochastic nature of minute why our session the kind of Y Tunes beginning that nanowires have this critical current that the switch on what happens really experimentally is that because nano wires are susceptible to thermal and quantum fluctuations when you measure their transition point over and over again you actually get a histogram or a distribution of the switching points because they have this firing capability and so when you map that onto the side of measurement you end up getting a more s-shaped type of behavior now this might seem like a bad thing because typically in circuits we don't want anything stochastic but also show you a little bit later biological neurons actually have higher abilities as well and so certain types of algorithms and applications take advantage of stochasticity in their system for their own type of operation and so nano wires are actually a great platform for these types of applications because they apparently have some type of big mortal function or firing capability to them so the last thing we measure it experimentally was through a factory period which again is a minimum amount of time between two input pulses such that both have their own output sites and I've shown the simulation on the left hand side and experiment on the right and you can see initially the two pulses and read are well separated in time to get to outfit spikes and blue to bring them closer together with still true output spikes so let's work together so we still chew up with bikes but eventually we reach a point at which we're only getting one output spike in both the simulation and the experiment and when you look at what's happening in the simulation you can see that by the time is that first action potential has ended we've already reached the end of our second input pulse so we've actually run into that refractory limit of our system and of course if you increase that overlap even further you're still only getting one outfits like and we repeat this measurement 200 times and record the time at which each of these spikes occur you can see that we initially have these two well-defined histograms based on the two individual outfits legs forgetting but eventually that collapses into one as you run into that exactly with out of our system so now that we have an experimental measured stoma and we've adjusted our circuit model and simulation to match our experimental results so you can start simulating what types of applications are sewn up to be useful and so I want to end my talk by focusing on two applications that we simulated where the nanowire neuron might be useful and the first is probably what's most conventional II done with spec neural networks which is pattern recognition and here we've took in a data set that's been used to test their mysteries and Josephson junctions it's essentially a set of 30 images representing the letters Z V and n in a 3x3 pixel array and each of these images has its own ideal representation and also nine single pixel error in the movies when our ball is to build a network of 9 pixel neurons and three output letter neurons so that if we present any of the images to the 9 pixel neurons the correct output letter neuron to fire meaning if we present the ideal z image to each of the 9 pixel neurons only that is the alpha letter neuron to fire and furthermore if we present any of the error images of Z still only that V output letter neuron to fire now how do we kind of understand these pixels well shown on the right hand side here we're actually mapping the color of the pixel to the input current of that neuron and so we chose to have grey represent a nonzero input current and white represent the 0 in behind so here's our overall system we have again these nine pixel neurons where the input currents are determined by the color of that of that pixel that image and we have our three output letter neurons here on the right here we have our inductive synapses I mentioned again that in our system the strength of our synapses relates to the magnitude of the inductance and so we've solved for the synaptic weights externally in a Python code and then use that to map onto the inductances of our circuit simulations and then the currents are inductively coupled between the pixels and our out the letter neurons and so here is the overall results of our system and so the first nine rows here representing the input current to each of the nine pixels and the outputs of the three letters are shown in the bottom here this to kind of walk you through a little bit that first column represents the ideal z image and you can see the code of that ideal z in the cheer corresponds to each of the input currents being received by the pixels and if you look at the output you can see that only that Z neuron fires meaning that it correctly identifies its ideal image and then the nine subsequent columns are for each of those single pixel error images and you can see that the Zener on fires afterwards we represent or we send in all of the the images and you can see the first to be ideal image novena on fires and then finally all BNN the Joads and so we correctly identified all 30 of our images and I said now I want to be clear this isn't an example of learning this is more of an example of inference where we pre solve for all the weights ahead of time well we're kind of showing here is that the nanowire neuron platform couldn't be used as a low-power and French platform for image recognition purposes so finally the last application I'll talk about is to have someone buy a realistic application and it's called the winner-takes-all simulation and the idea behind it is that in our heads we have hundreds of thousands of neurons it's not the same time we're receiving thousands of input signals from everything we taste touch see smell etc and with all of your neurons fire in response to all of these input signals the overall results your message is kind of meaningless it's hard to make any distinction ideally what you'd like to happen is to have only a select number of neurons fire and in that way you're able to make some type of discernment or decision a simpler way to look at it is that you have a set of inputs that map to their own set of outputs if all of the output signals fire and go into the overall message again it's kind of meaningless and so eventually what you'd like to have happen is to have only let's say one of the output signals be firing and then that way you can kind of make a decision or some type of discernment there's different ways of how the brain does this so one possible model is called the two inhibitor winner-takes-all Network and the idea is again you have these sets of inputs it's mapping to these sets about books and here the blue arrows represent excited for a connections and the red arrows represent inhibitory connections and all of the outputs share a set of two inhibitors the idea is that if all the inputs start firing all the outputs then start firing but they then turn on these two inhibitors which starts trying to suppress all the outputs until just one of them remains now the two inhibitors have different roles the convergence inhibitor here fires and at least two outputs are active and it really fosters that competition and the stability inhibitor fire once there's a winner and it continues to fire as long as just one outfit is active and it basically keeps the other ones best now a really important characteristic of this is that the output neurons are stochastic meaning that it's a probabilistic scenario of which one wins so to incorporate that into our model we actually matched our experimental firing probabilities to our simulated firing probability piece by incorporating noise of our system so I'll show you an example of a winner-takes-all simulation with our three and put neurons connecting to three output neurons and our two inhibitors so initially what happens if you have all three inputs start firing if you look at the bottom all three outputs are firing as well and eventually y1 is the only output neuron firing meaning that it's a winner of that competition if you look at what's happening with the stability inhibitor so you can see that the stability inhibitor is es continues to fire basically keeping the other two suppressed but the convergence inhibitor has only fired as long as two F that neurons are firing and again this is a public ballistic scenario so any of the output neurons have a chance of winning so if you repeat this simulation you have examples like on the left hand side where y2 ends for example tick on the right hand side for y3 lens and if your result many many times and record which neuron wins which competition you can see that y2 y1 and that y3 all win roughly the same amount of time because this is the probabilistic scenario and so these types of simulations or these types of networks can be used for applications like learning and decision-making and when we're trying to highlight here is how it's a great example of how you can take advantage of impairing so fast a severe probability in neurons to have a unique purpose so finally where do we go from here well in this talk I really focused on what's the processing element of our eternity of architecture which is this low power naturally spiking hardware and I showed that it has possible Network applications and things like image recognition and the modeling of fire realistic dynamics and the other part of our work which I didn't really get to talk about today was the memory element that we're also forming nanowires and our overall vision is to really bring memory and processing element closer together in the same platform and even within the same fabrication step in order to have faster more efficient computation that no longer has this memory bottleneck issue and kind of in parallel with that is to continue to be inspired by these natural dynamics to super collecting nano wires and really investigate when we can harness them to develop new computing architectures and devices so with that I'd like to thank you for your time and your attention and I'm open to any questions so thank you Emily I enjoyed the presentation this is Craig Keith actually have a couple questions for you one is we were doing sort of the energy comparisons to CMOS spiking I guess neuron circuits what was that sort of the cooling factor that you were using yes so in our case and in our case we use a factor of 400 watts per watt and we test our devices at four point to tell them I guess a related question from an area density standpoint how does the area of your superconducting nanowire spiking neurons compared to sort of a that's a good question so in this case we're not optimizing for for area yet so if you saw with our show my cell design it's actually quite large in this case it was about 20 microns crossed and that was mostly limited by the size of our inductors so we could kind of continue optimizing for area by first of all increasing our oscillation frequency by reducing our apartment's and then also by using a thinner film which has a higher sheet inductance which kind of will allow us to shrink our overall cell size and so in this particular example for our proof-of-concept device we're not optimizing for area yeah I think eventually you know with our synapses on here that also takes up a good amount of area and so it's going to come really between decision between energy efficiency and overall density as I'm making a question to that the the one of the energy loss is right in integrated systems would be the capacitive energy loss between in the wires that connect different elements of your system are your wires that connect your different neurons also superconducting and that's not an issue right that would be our goals to have superconducting interconnects that we're not running into those types of losses so in that case would the area density really be an issue given the fact that distance is not being reflected in the neck strain energy loss essentially I mean I was thinking more just the overall like cell size itself I mean it is larger than what we'd like it to be present on the order of 20 microns or so and that is still taking up more areas I think they would like a final design I'm not sure if I understood I'm answering your question yeah yeah well I mean if your R is very small your RC time constant with me or you know also very very small hence I would expect that you wouldn't suffer it's quite as much capacity loss in the process but that might be a wrong assumption overall would have lower is there also a sense on how does this scale compared to I mean you're started with a plot emphasizing by 2030 we'll run out of energy to actually provide for computational power the planet would you be able to address it using these systems do you actually like if you look at power usage to do a computation would this be the way to go about it that's a great question so in this case we've really only looked at the overall energy costs of like a simple operation really connecting to four neurons and so we haven't looked at it in terms of actual computation cost one of our concerns that we have to look into is is really in linkable network because we're easing bias current devices is the costs of distributing that bias current and I think that will be our biggest concern moving forward performing like a full extended network and if you look at other superconducting circuits as well like those using Josephson junctions that ends up being kind of a considerable cost so we haven't factored that in to the art system yet we don't have the done but that's what we're kind of those where you there's a chat question by Stephen Philippe own semen I could read your question or would you like to unmute yourself and ask it sorry it's a little loud here so maybe I'll be quick but basically thank you for that talk how does this compare to our RAM based cross our arrays I mean you already sort of commented on the device size but maybe you like the same two approaches yes thank you thank you very much for that um yeah that's a great question so I guess our biggest thing with this is that like in a lot of our my trades in order to do spiking you need additional circuitry and in this case are so kind of all be an inherent part of the device but something that you should we should point out or maybe I should emphasize a little bit more is that we don't yet have a scheme or fur for learning in these types of systems and so that is our kind of an area that we're currently working on some students have been brought on board is you know we have some chain ability that we don't actually have a learning mechanism yet and so other approaches have started to incorporate those and showing those and their work experimental II into work we've actively develop things not versus them if you have additional questions please raise your hand or send me a chat the present way of making the neurons is by using superconducting thin films on silicon which would imply that this might be compatible with a present lithography processes or CMOS processes is that the case and how integratable is this with a digital logic yeah I know we think it would definitely be able to be Instagram integrated as long as you can you know get the the feature size I mean previously in our other work we done the same superconducting processes on tops of other chips that have you know like magnetic memory elements so we've shown them that these types of processes are quite compatible so that's kind of one of the advantages of superconductors and super connecting unaware specifically actually because they have these high impedances they're able to be interfaced with CMOS devices which other if you're not comprises of doses and junctions have a hard time doing that type of integration because their penises are much smaller and so one of the things that glue is up saying about nano wires is that there are really great interfacing technology as well because it kind of moved between the superconducting inside make nothing roads so there are two more questions are lost one of them and then I'll open up the floor for the other which is a comparing your technology to memories their technology will you be able to do that I guess in terms of what what respects for I guess the the memory store technology itself has a capability of remembering or cool it in a way of making yeah how would you say you know side-by-side comparison between the two which one should I choose I don't know if I have enough prepared to be like a full side-by-side comparison I mean the way that we incorporate memory systems a little bit I didn't get to talk about it today but that's the other part of my thesis work and that's using basically super from Echenique where we chop logistic curve and that becomes the state of our memory and it can stay there indefinitely as long as you don't break that way and so in our types of systems you know you can potentially hopefully thought as the modulated current and our synapse inductor it's basically stored just or not there and so that would be move our idea of memory in these sexy systems and and I guess just to follow on to that the packing density of that is very much dependent on how much the heat spreads from your heating current right I mean you apply the current to break the superconductivity in your nano wires so I guess the spatial extent of that King is microns or all right yeah it'll be smaller than that it's an order of hundreds of nanometers okay so that would be one consideration and the other thing is really like the inductance of our system so all of our oscillations are based on having long enough L of our our time constants that you're having these sustained relaxation oscillations and so it's really just making sure that the inductance is large enough to support that and so we again we haven't optimized that in this particular design so it's still quite a large cell size but we could definitely make it fun there's a question from Omid Sami if you like to UM you tonasket oh yeah my question is really about your last line you said that you want to bring it together you know the computing part with the memory and which is which is a holy grail of normal computing I would say many aspects one of the challenges there is how do we do a non-binary computer right so if you have a memory cell how they actually store values beyond zero and one today it's in furnaces from your slides I I noticed that it's spiking and so you have a high resistivity or a low resistivity which corresponds to ones and zeros but what if you could do once you run something in between right the die-hard an for for AI based hardware based architectures and what would be your approach is my question to achieve that kind of a non binary multi-value normal competing thanks it's a great question and so I guess we've been most thinking about non-binary from the standpoint of memory kind of if you're talking about and actually the part of my work that I didn't get to talk about today was about turning our binary nanowire memory into a multi-level memory cell so we've been able to show that we can have multi levels traps and a super connecting loop and do that controllably and change the number of states simply by changing some of our circuit parameters so we're actually able to do non-binary memory and if I can maybe conclude with the last question that this is from a cache to become an acrostic did would you like to unmute and ask questions thanks for the talk so your desired IV characteristics and similar the actual device are we characteristics of this view are somewhat different them do the novice I was wondering in your simulations is it possible to add the simulation and have you done that um you get there's a great questions so we added noise actually to our simulation in order to deal with the winner-takes-all example and basically in order to incorporate a firing probability into into our model so that it doesn't always fire the exact critical current and that's going to be on the way you can get that when they're kids all sit like the situation to work out because all three of your output neurons are identical and so that's one way we incorporate noise into our simulations and we essentially had two Gaussian white noise sources and we for each soma and we swept the magnitude of that until the Ferren causal abuse match that we got experimenting and so that's one way that we incorporated noise in our system and we kind of made sure that the history of programs that we get from measuring that switching point many times correspondents what we get experimentally as well alright Emily thank you very very much for a fantastic talk and great set of answers to multi user questions so I very much appreciate you participating in this version of nano explorations again you can voice your reaction by indeed clicking on it if you wish and again thank you so much for participating in today's nano explorations as an audience listening to great or finally zoom it to me I'll just alert you that the upcoming nano explorations is this coming Thursday at eleven o'clock and the topic is 2d material enabled multifunctional mid IR optoelectronics join us for that we'll hopefully see you then thank you all very much well done 