 ANDREW MOORE: What I want to remind folks of is the distinction between artificial intelligence and machine learning. And right now I and, I would say, many of the faculty and students at Carnegie Mellon are passionate that it's an important distinction. It's actually one that Manuela brought to Carnegie Mellon's department of machine learning. And it is very much about the future. So to tell you about the distinction between machine learning and artificial intelligence, I'm going to give you a quick recap of what happened at the start. So in the 1960s when AI was getting started, the heroes of early AI such as Allen Newell, Herb Simon, John McCarthy we're really looking at the question, of what is human-level intelligence? What is it comprised of? And they broke it down into three pieces. Perception, deciding what you want to do, and then acting. And there was this big discussion about the loop that an autonomous or intelligent system goes through, we go through in our lives when we're constantly seeing stuff, making a decision what we want to do, acting to change the world, observe the results of that. And we keep going around this control loop. And this was the version of artificial intelligence that I, and I think Manuela were both sort of born into in the late '80s, early '90s when we were so excited about a future where we can have autonomous decision makers. Probably the peak of these early days of AI was in '97 when computers had superhuman performance of the game of chess. What was interesting about that is it was kind of the peak. And after that there was a little bit of sort of plateauing or decline in the use of artificial intelligence. And the reason for that decline is this thing about decide is really difficult except in a few special cases, such as the game of chess. Reason is, as you all know, and I know many of you here are very advanced technologists so this is not news to you. All that happens in an implementation of decide is you iterate among possible actions, or if you're slightly more advanced than you're planning, possible sequences of actions. And for every single one of them, you look at the outcome and then you do an arg max, and whichever is the best one, you choose. And you keep going around like that. But in order to figure out what the best one is, for each of your possible actions you've got to know what you expect or what the probability distribution is over what will happen next. It's easy in chess because you can write down the rules of chess in a very short program. In general though, that was the really difficult part for artificial intelligence. You heard people talk about rule-based systems where folks were coming up with sets of methodologies for allowing robots, and computers, and other scenarios other than games to make decisions. And you could do it for circuit board layout. To some extent you could do it for scheduling. Not in a very meaningful way were you able to do it for trading. And in general, the problem was we couldn't find a way to have the software engineers write down the predictive rules. And there's no point in arg maxing over a thing which is not going to be accurate. So that led us to the sudden growth of an area of artificial intelligence which had been a little bit quiet in the 1990s. Which is, can we get rid of the idea of humans writing the predictions of what happens next and instead have computers observing historical data predict what happens next? And so that's what led now to-- there were a few huge successes there. One of them was at Google where it turned out that all the companies which had tried to do search engine results by coming up with a set of rules for how to turn a query from a user into a useful thing to show were being unsuccessful because it was so hard. But if you've got enough clicks and enough data you could build a predictive model of what the likelihood is that someone will find something useful in answer to a question. And that's what led to the massive growth of Google. Subsequently similar things were used for all kinds of recommendation systems. And then start to jump into other verticals such as weather prediction, some aspects of physics, many medical decision systems. And so during the mid-2000s to about now there's been an explosion of people successfully using machine learning to predict what happens next. And we're all excited about that. That means that this big bug which stopped us in 1997 seems to have been overcome to some extent. What we at Carnegie Mellon-- and in fact, it's through a lineage of geniuses of AI, starting with Herb Simon and Allen Newell, followed up by subsequent folks such as Jaime Carbonell, who was one of the forefathers of natural language processing, and Tom Mitchell, who was one of the people to introduce the idea of computational machine learning, and then frankly, going up to folks like Manuela Veloso. Carnegie Mellon has always held onto the idea that we have got to be good at perception, decision, and action to build autonomous systems. And now that machine learning is able to help us with decision, we really want to go and work once again on the full loop. So this picture, which you're going to see a lot of in this talk, is a way to think about the field of artificial intelligence. And it also helps me personally structure my thinking when it comes to, what is the right curriculum for creating a person who is an expert in AI? Not just a person who can use AI, but people who can invent new parts of AI. It's also, over and over again in my career, it's been the diagram that helps me set up a large AI project because this is a decomposition of the engineering that you need to do for an AI system, which kind of helps you actually see, I'm going to need a lead for machine learning. I'll need a lead for perception. Probably, I'll need a lead for decision making. And then, at the very top, I'm going to need people who can work out exactly what the consequences are of acting-- either discussing stuff with humans or acting autonomously. So I want to now walk over this diagram, giving a bit of flavor of the key technologies involved. And we'll begin with sensors. The interesting thing, those of you who own a Tesla-- actually, everyone here that lives in New York, you don't own cars, but for those remote folks, especially our Bournemalvians, if there's any Bournemouth people in the audience who are actually able to drive, the interesting thing right now is that, because computer vision, which has itself really grown through the use of machine learning, is now a commodity, we're doing things like detecting and counting vehicles. Is basically something that you can download open source or buy from a software vendor. We can do things like that. This is a very nice example of a system in about a mile radius around Carnegie Mellon where all the traffic lights can see how many cars are waiting at which intersections in which directions, and the traffic lights can be a little smarter about how they time themselves on the basis of this. And they can talk to their neighbors so that between them, they can actually start to strike deals. Like, OK, I'm going to give you these six cars, as long as you can promise that by the time they get to you, you'll be ready for them to take them further. And so that's the nice sort of thing we can do now that perception is a commodity. Another example of where perception is really changing things is it's getting so much better every year at the moment, in almost a frightening way. So Marios Savvides in the ECE department at Carnegie Mellon is an example of someone who has solved a problem-- it's actually an interesting problem of the control of the pan and tilt of a camera so that if you walk into a room-- if you walked into the back of this room and Marios' camera was here, the camera, in a fifth of a second, would pan and be able to zoom in on your eye to get this quality of image. And then do identification. And in fact, actually some other diagnostics based on eyes to do with emotional state and other information, within, in total, a quarter of a second. So that's this year. Next year, we may perhaps, with some of the additional technology I'm discussing later on, be able to look at all the eyes of someone in an audience or track all the faces of someone in an audience. And so don't for a moment think that we're kind of plateaued in perception. We can go a long way forward, and as many of you are probably realizing, this gives us the opportunity to create an awful Orwellian world. It can be really useful. Certainly, the idea of supermarkets where you need to walk into the supermarket and take out a piece of plastic to show your credentials for purchasing something is immediately old-fashioned. And as we've seen with Amazon Go stores, it is now perfectly reasonable to create a retail experience where you just walk in, pick stuff up, walk out, and the payment, without you having to produce anything, can happen behind the scenes. Lots of conveniences. Lots of consequences and potential dangerous things. I want to mention another very important thing that's going on. And I think this is extremely relevant for folks in your organization who are looking at edge computing of various kinds or computing that's forced to be extremely close to the sources of data. And that is, how do we get to do extremely low latency massive processing of information? One of my dreams, which we may have time to talk about later, is a world where, after a disaster, such as a flood or a terrorist event or a fire, you have autonomous fixed-wing aircraft in the air at about 3,000 feet, able to watch an entire city to immediately see who's in danger, who's doing what, where people might be trapped, where's a dangerous location. If you want to do that, you would need to put tens of thousands of cameras on a single plane. At the moment, you can't do that. First, even if the cameras are just an ounce each, the plane will plummet from the weight. That's solvable. Second, you could not afford to have a network link getting real-time data from 10,000 cameras to ground anyway. So you would end up wanting to put the compute on an aircraft, and it needs to be very, very low power. So a big push in the world of artificial intelligence right now is something that I don't think would ever have normally been discussed in an AI lecture or an AI textbook. It is how we can make these neural networks and arg maxes-- the big search algorithms-- run at about 1,000th of the power that they run at right now on a very tiny piece of equipment. What's interesting to me and what might be a topic of follow-up discussion is those of us who are working with aerospace massive scale surveillance and satellites, we're doing this work for that use case. What's interesting to me, and I'm sure is interesting here, is, what would we do in the financial industry if we've got down to 1,000th of the power requirements for doing things like neural network inference? And would that actually have a big impact on the business? Especially is, that technology, we're going to develop anyway because we need it so much in aerospace. I'm just going to-- [MUSIC PLAYING] Sorry. I apologize for the music, but I wanted to give an example of a use case. This is a couple of grad students in the Robotics Institute who were able to come up with a deep learning system for detecting all the elbows in an image, all the knees in an image, all the pelvises in an image, and be able to run that in real time on GPUs inside regular cameras on regular cell phones. And what they're able to do now is, those of you who have used Microsoft Connect which can track one human body, they can now track whole crowds of people. It's an example of the kind of direction that we're thinking of moving. Unfortunately, at the moment, this technology is not quite real-time for crowds of thousands of people. And so that's the next thing. Those students and their advisor, they're now looking primarily at these hardware questions so that they can run this on massive groups of people, again, to help out with situations of public emergency or to do various studies. What we know at the moment about whether, as an infectious disease is spreading in a city, we can see about the actual gaits and walking movements of the population coming into work on any given day to sort of notice small changes in behavior. The other place that power is really important-- this one was power and algorithms-- is in the three-dimensional understanding of the world. When you are building rescue robots or things like this, you need to be able to go into environments which have never been mapped before and have the robot figure out how to map them. These days, it seems like such a 2015 thing to imagine taking the data off your robot, going to a data center to compute that stuff, and then send it back to the robot. Now, with both hardware technology and algorithms, we can do this sort of thing in real time. I'm going to keep moving now as we cover other areas of machine learning, because I think I've given a sense of this explosion in uses of machine learning. I want to mention one thing which I need everyone to be aware of because it is going to be a big change in the early 2020s for all of us, is this is the first piece of work I know of for doing neural network inference at about a 10,000th of the power that it uses now. This will be critical for self-driving cars. Many of you probably know this, but at the moment, the computer that you need for a self-driving car raises the temperature of the car by about 10 degrees because it's so compute-intensive. So you actually have to put in extra air conditioning in the vehicle to make it reasonable for humans. That's one of the many reasons why we want this. These guys have now built something which is a camera you can wear on your clothes. It is powered entirely by your body movements, and it can do face recognition. Currently, very slowly. It takes about 10 minutes to do recognition from a single frame, but that power requirement is amazing. As that starts to get commoditized, we can have cameras doing compute, which you just sell in Walmart on sticky label strips to place them all around your house, if you want to for some reason. This again might be relevant to you guys. For the purposes of preventing anyone from disrupting the entire global communication system, it is now possible to imagine putting up billions of very small computers into orbit, instead of having all your computes sitting in a few cans floating around the world-- is extremely vulnerable. And here is another thing which has been really interesting for us, is voice. SPEAKER 1: Mayday. ANDREW MOORE: From that piece of sound, one of the faculty at Carnegie Mellon, Rita Singh, was able to deduce this much about the person who was uttering the sound. And why was she able to do this? It turns out that a lot of the frequencies in your voice are very much characterized by the size and shape and flexibility of your trachea. From your trachea, you can actually deduce a whole bunch of things about you, including height, age, and weight, as well as ethnicity. And one of the things I really liked was she was able to figure out that he's sitting on a metal chair on a concrete floor. Again, it's not actually science fiction. It turns out that mayday is about 100 megabytes of data, and the frequency analysis and the careful looking at various forms of echoes means that this is quite solvable. In this particular case, it was used to successfully apprehend a serial false alarm caller who had been sending out lifeboats in serious storms for unnecessary reasons. Now, here's where I want to be really clear about something. We at CMU are very excited about machine learning. We formed the world's first machine learning department. But-- I don't know if I'm speaking for Manuela as well as myself-- it is not enough to just do machine learning. The real art comes in the decision making systems you put on top of machine learning. The things-- exactly how you do this arg max over possible actions. And it's a message that I cannot be too clear about. Do not get obsessed with neural networks or deep networks as the sole component of an AI solution. The number of things where you actually just need to do a one-pass prediction of something, a minor, compared with the number of use cases where this is wrapped up in a bigger control system, where a decision you make on this time step is going to have repercussions for the next few seconds. In some cases, the next few minutes, and in some cases, the next few years. So you still have to be doing a pretty sensitive optimization over what you're predicting. Here's one part of this, is safety of machine learning based systems is still a black art and it has serious consequences. This is an image of a system developed by the army called TARDEC. In 2010, it first came online. It's something where a series of trucks going in a dangerous area, Afghanistan and Iraq and so forth, can have one human driver in the front and then the rest of the convoy just follows each other. It has still not been deployed. Why has it not been deployed? Because absolutely correctly, this is a safety-critical system. You cannot risk it causing an inadvertent death of a civilian or a member of the armed forces. So you have to be able to prove its safety. Eight years later, we still haven't managed to prove its safety because it's using cameras and LIDAR and other sensors. That's going into machine learning systems which create these models of what the world is doing, which goes into control systems. And unlike advertising, which I was doing at Google, or many other aspects of the economy, you cannot make mistakes. So a big growth area for us are systems which can make proofs about the behavior of an overall machine learning system. There are two warring factions in the technology world as to how to do this. And, like a good dean, I am encouraging both sides of the war. These folks, Mike Wagner and Philip Koopman, are using statistical machine learning AI methods. They're actually adversarial AIs who watch an autonomous system practicing, and the AI figures out from all the data what is the most difficult thing they could ask the system to do. So it's constantly searching for ways to break the system. This is a technology used in security at the moment but is now far more automated, and with their help, we've now moved TARDEC several years forward in being able to rapidly discover weaknesses in the overall machine learning system. The other side of this battle-- I'm going to stand on this side-- are the theorem proofers. And this, I love because I always used to love mathematical logic. These are the folks who say, forget the statistical testing or having an automated agent trying to break it. You need a mathematical proof of correctness of an automated system. The surprising thing is that, in some very real situations, we can now get proofs of correctness. A good example has been the new AFA anti-collision system for aircraft over the United States. That is a new autonomous system powered by-- it doesn't learn online, but it has a model which it learned from data. And then, actually, very intelligent aerospace control surface stuff to take over if two aircrafts are actually about to collide in some way. When Andre Platzer applied his theorem proofing to prove the safety of this system, he discovered hundreds of thousands of edge cases where the system would actually not follow its specifications and would unnecessarily fail to manage a potential collision. And so, in his case, the use of theorem proofing has been able to let us iterate to the point where we're not seeing edge cases where there's a disaster in the system. And so this was a perfect example of theorem proofing. Problem with theorem proofing, you end up being a little conservative. You have to, for instance, if you are putting bounds on the shape of a jet engine, you cannot computationally afford to do anything other than call it a big sphere. And so you're conservative, but obviously, in the safety engineering, that's the way you want to go. Another important area which we're betting on is game theoretic reasoning. Here, over and over again-- and I personally had a big disaster in one of my previous jobs based on not accounting for game theory. When you are using a machine learning system to predict what's going to happen next as the effect of an action, and then use that to make your choices subsequently, if someone else knows you're using that methodology, there are a whole bunch of vulnerabilities for how they can exploit this. I had an anti-spam system. This was something for detecting if an ad being served on Google's search engine was likely to be a scam of the form of what we called an unbelievable claim. So unbelievable claim ads, which you've probably all met, usually have a caption of something like, what the government doesn't want you to know, or doctors hate this trick. And not surprisingly, we built good detectors which tried to figure out if either the landing page or the copy on the ad were in this form. And it worked really well. We had a several-factors reduction in the amount of spam ads that got through as a result. And it got better and better, and then it sort of remained at a current sort of stable state and so we went onto other things to improve. Then, about nine months later, it suddenly started to fail miserably and a whole slew of different spam ads-- not just one, but many, many different spam ads were getting through the system. And as we did the post-mortem, we figured out that someone on the other side had figured out the form of statistical model we were using for doing the spam prediction, and they had successfully sent us a whole bunch of training examples, which meant that our model became uninterested in a certain critical piece of information. And so they were adversarially gaming the fact that we were using machine learning to make our model continue to perform really well but actually become unaware that it was creating a vulnerability. And so they were able to attack. It was annoyingly on Thanksgiving a few years ago. So lots of us had to scramble on our Thanksgiving because we had not put a game theoretic assumption into the machine learning system. So this is another area where you will see places like CMU aggressively hiring at the moment because so many parts of the economy cannot get away with this thing called a certainty equivalence model, where you continually learn from data, assuming that the data is independently distributed. It's not just that we're looking for outliers. We're looking for adversaries. Another really nice example of this kind of AI is one of our faculty members who, in the spring, has come up with a system for an AI to do redistricting. So the kind of problems where we're all worried about gerrymandering. And the interesting thing about this AI is, the problem that this faculty member gave himself was, how can we have a third party, if you like, do the redistricting which both the Democrats and Republicans will trust? And thinking about it, he came up with a game where both players can play the game, actually, as selfishly as they like to come up with the set of districts being drawn in a region. And you have a guarantee that the results of the game will be something which an optimal algorithm would have also decided to use, based on no concerns about fairness or bias. It's an amazing discovery. It follows a protocol which a few of you may have heard of called the pie-cutting problem-- the question of how a group of seven people can cut up a pie into seven slices in a way that none of them feels that they've been hard done by. If you want to look at something even more interesting than the reinforcement learning survey that Manuela mentioned, look up a pie-cutting problem on Wikipedia. It's really, really cute, the algorithm that it comes up with. The fun thing is, now, it's being used really usefully in important areas. Oh yes, and another really good example of this. I mentioned earlier patrolling jungles. One of our faculty members is very interested in reducing poaching in game parks, primarily African game parks. She used machine learning-- a very similar story to the one I told you about my own use. She used machine learning to predict which areas of the jungle were most likely to experience poaching on any given day, according to the migration patterns, the weather, and other information. So that, rather than deploy the understaffed forest rangers group over to, like, the whole jungle, they could really be focused on the places with high likelihood of poaching. This was successful for a short while. For a while, they were able to really cause a dip in the amount of poaching that went on. But before long, things went back up. And you won't be surprised to know what was happening was that the poachers were basically choosing the second most likely spot for poaching instead of the first most likely spot. And in general, we're just trying to think one step ahead of the algorithm. With game theoretic reasoning, Fei Fang instead solved this problem using the same kind of technologies that had been used in game theory throughout the 20th century, including the kind of game theory used in building up Cold War protocols, to form a randomized strategy where here patrols now, actually, are non-deterministic. But they're much more efficient than if they randomly patrolled the whole forest, which almost never hits anyway. They can be maximally inconvenient to the poachers, and they have found what is essentially a Nash equilibrium for it to do. Again, emphasizing the big theme of, you've got to use classic or maybe to-be-discovered decision methods on top of your machine learning model. Don't just use the machine learning model naively. That will work fine for little games that us academics play within our labs. It will not work fine with the real world. Now, I'm going to move to the top of the stack. Another area where we're hiring and another area where-- again, I don't want to put words in Manuela's mouth, but we feel as a CMU faculty, it would be irresponsible not to train AI practitioners. When you're about to take one of these systems and put it into the world, you have to care about what it actually means to the people who are operating with it. What are the not just immediate effects, but what are the second-order effects of having AI assistance or AI advice in your life? The place that we all think of initially are the big AI chat bots. Whoever created these slides, why the heck didn't they put Google Home on this slide? It was actually me, but at the time I created the slides, these were the top three chat bots. So this is an area where, when you ask a question, like, where can I get a good cappuccino near here, underneath, that machine learning system is all in place, both for your speech and for predicting how satisfied you will be with particular actions. There are search algorithms. Suddenly, in New York, if you ask that question, you're going to be looking at 10,000 possible answers, evaluating each of them before you give your response to the user. But there's more to it than that. There is also the question of, how am I going to make it so that the user trusts me in the future? That I don't risk giving 1 in 20 of my answers being crazy wrong answers, because if I do that, then the human is not going to be able to take advantage of me in the future because I will lose their trust. So you end up making decisions based on what is known as risk-averse reasoning when you're giving advice from a chat bot. Again, in the interest of time, I will move forward on the discussion of a concept called the open knowledge network. I just mentioned-- SPEAKER 2: Hey, I'm Josh-- ANDREW MOORE: --some other areas-- SPEAKER 2: --and this is my 112 term project. ANDREW MOORE: Sorry about the noise. I don't know if we can mute the noise. Here are some other really nice areas of human AI in action. There are really interesting things we can do now, with figuring out how people with what some might call impairments, such as missing a limb, can operate as effectively or more effectively than other folks. This is an example of a game which trains you to do rock climbing by projecting test paths for you to climb with, and it cleverly adapts its level of difficulty so that it can train you from being a beginner up to being a high-quality player. This is a system which allows you to talk to a physician in a way where your emotional state is being recorded during the discussion. And what this has been able to do for this researcher, LP Morency, is he has been able to effectively help physicians ask questions which gets to the root of a problem when humans are nervous about talking about a particular subject or going into detail. This has been effective. It is now shown in a bunch of blind tests to come up with better diagnostic prediction. This question overall, of assessing human emotional state, is another area which I don't quite know where it's going. But it is a huge growth area, in the same way that computer vision was a huge growth area five years ago, or natural language processing was a huge growth area 10 years ago. I don't know how it applies in the world of fintech, but the question of being able to get some notion of stress and these things called involuntary microexpressions from users is both Orwellian and really exciting. Another piece of LP's work has actually been in predicting whether a patient who is under treatment for depression, predicting whether that specific treatment is going to be successful or not. And he has been able to show that he can predict this before the physician who's treating the patient or the patient themselves, based on analysis of eye movements and microexpressions. So very interesting new world that we're entering. Many other examples. I want to keep moving, though. Autonomy. So this is an example of an autonomous system produced by some faculty somewhere-- I'm just trying to remember-- I don't know who they were. Actually, it was Manuela Veloso, for those of you who don't know. To me, it's the perfect example of putting the whole of the perceive-decide act into place. In fact, I'm going to play it again. You're seeing here a group of robots which are cooperatively and in real time planning how to get that ball into the goal. And you see here how the plan is actually executing, where these things successfully, even given uncertainty-- and it was hard to see but that actually went into the goal. That is an autonomous system. It is the least understood of all the parts of the AI stack. In fact, despite the pioneering work here, I am unsatisfied with the work of the academic community or of the commercial community here that, at the moment, if you want to build an autonomous system, you're going to do it artisanally. There is no strong theory to support it. Let me show you some other examples of autonomous systems. Built with Leidos, a big defense contractor, we have a semi-autonomous battleship. It usually has 300 crew. This one has 12 crew. It is currently patrolling the Pacific Ocean. It's a submarine hunter. It does not have weapons on board. That's an important dimension. But it does autonomously obey the rules of navigation when other ships or coastal activities are in sight. This has not yet been validated. You can't validate it with a single experiment like this. But there is a real concern that navigation errors are actually kind of serious for the Navy right now. There's so much information, that human decision makers are not able to process it all, and so you are seeing ridiculous things like ship collisions. So that's another highly important autonomous system. Again, it would be against the rules of warfare and most of us in the world of artificial intelligence would not agree to do it if we were applying this to armed platforms. But for unarmed platforms, this is another place where you want to build an autonomy. Another really good example is in cyber defense. And here is an example of a rare use of cyber technology. This was a challenge created by DARPA where you're actually using both offensive as well as defensive network security. And this was a challenge with about 16 racks of computers. This was one physical rack, and there were 16 of them arranged in a circle, networked together. Where, after the Start button was pressed, the systems had to try to break into each other and take over each other. And it really was the last machine standing. The system from Carnegie Mellon won. It was created by Professor David Brumley. It was the only one which was actually using game theory, as well as machine learning and search. And what it was doing at a very high level was it had a known set of exploits that it could use, but it was also able to watch for incoming exploits used against it which it could then redeploy against third parties within the game. And it's a very interesting hidden state game where you hoard the exploits to use at the right time, trying not to get them out into the wild until you know you really need to use them. And this won. The very interesting thing about it is that, in DEF CON 2017, which is a human-controlled cyber warfare simulation, this machine placed 17th. So there were still 16 human teams which could beat it but there were over 200 teams entering. And it was really interesting that we have an autonomous system placing in the top 20. So another area where autonomy is important. And the thing which I'm not satisfied about is, if I want to build an autonomous system like one of these, I can't really train up new engineers with the skills to do it. I have to search around for one of the 100 or maybe 200 people in the world I know who have experience building autonomous systems to put this sort of thing together. So academia or commercial world needs to come up with an engineering discipline around autonomy instead of it being an artisanal thing. With autonomy, we're back where we were with data management before the relational database was invented. And something similar to that, I believe, needs to happen. Here's a faculty member who really inspires me and really scares me. Her name is Claire Le Goues. She has had an important insight. She's not the only one with this insight, and now, there's a rapidly growing academic community around it. Those of you who are in software development or manage software development systems, you know you use big change management systems. Probably not Git in the commercial world. Maybe you're using Perforce or something like that. As your organization grows, you get lots and lots of data about all the changes to the software which have happened. And it's captured by your source control system. And you get patches where someone has changed the line of code, preventing a fence post era or patching security problem 37. What Claire and a few others are doing is using machine learning over these big repositories to predict in advance which pieces of code will eventually get patched and for what reason. The thing that Claire is also doing which makes her really stand out is where it gets a little thrilling, shall we say. She's predicting the fix. So she's trying to predict how a really experienced programmer would actually improve that system. And I want to be really clear. What I'm about to say is science fiction. We have no idea if this is actually going to happen. But if things go well, my hope is that by the middle of the 2020s, we just have autonomous daemons running over code, constantly improving it autonomously, instead of relying on messy wetware of human brains to improve the software. So that is a big, grand challenge. I think it can make the world a much better place. Certainly deal with security patches and efficiency mistakes much more effectively. All right. I want to mention one more point before I wrap up. I'm going to go back to this example. And I've already shown it to you, but here, you can see it in full scale. This is an AI system which watches you. It gives you tasks of where your hands are meant to get placed, and then it watches you and it gives you increasingly difficult or less difficult problems. A perfectly fine AI instantiation, not much different from some of the others you've seen here. The reason I think this is so important is, in 2013, this would have been a PhD thesis. 2017, this was built by people who, most of them-- there was a team of six students who built this. Only one of them could program before the class, where this was their end project. And by using open source libraries and Cloud for the big data management and so forth, they were able to create this prototype system within the time of their course. That's really exciting to me because before I went to Carnegie Mellon, I thought that a big problem for the world is that we have one or two orders of magnitude too few people who are qualified to build AI systems. Now, I believe, although it has yet to be shown, that we can educate them relatively rapidly. And the reason we're able to move much faster than I had expected is the components you have seen on the stack, with the exception of autonomy, we can encapsulate the performance and behavior of those components in a way where you do not have to be an expert in how every single one of those components is implemented. You can glue them together if you know what they're for and how the glue works. So that is my overall summary of where we see AI at the moment, what we're scared about, what we're excited about. And again, the big call to action here is, I've been in the situation where I began to think that machine learning was the only important thing in AI. And over and over again, I've learned that that is wrong. And so places like Carnegie Mellon, although we continue to invest in machine learning, to be really relevant in the world of artificial intelligence, we're also investing in the parts below machine learning and above machine learning on the stack. Thank you. 