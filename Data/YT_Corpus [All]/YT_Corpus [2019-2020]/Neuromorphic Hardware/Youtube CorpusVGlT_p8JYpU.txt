 PAUL JOSEPH CLIFFORD: Good afternoon, Penn Staters, and thank you for joining us. I'm Paul Clifford CEO of the Penn State Alumni Association, and I'd like to welcome everybody to today's virtual speaker session. PAUL JOSEPH CLIFFORD: As many of us have transitioned to living, working and learning remotely the Alumni Association is ensuring you stay connected to one another in the university and also Staying informed and as a reminder, today's discussion will be recorded. PAUL JOSEPH CLIFFORD: Today we welcome Dr. V J Mariana today's discussion is the latest installment of a recurring series of virtual programming that the Alumni Association is bringing to the Penn State community. PAUL JOSEPH CLIFFORD: Will be speaking with experts university leaders and other Penn Staters who will share insight and perspective with you in the coming weeks and months. PAUL JOSEPH CLIFFORD: Additionally, the Alumni Association offers a host of online networking and career programs and webinars and you can view a full list of all of these events at alumni psu.edu slash events. PAUL JOSEPH CLIFFORD: Now, I'm happy to welcome Dr VJ Mariana and who is the Robert Knoll chair, professor of Computer Science and Engineering and electrical engineering here at Penn State. PAUL JOSEPH CLIFFORD: He has published more than 500 papers in the fields of power, where computing embedded systems and computer architecture with more than 22,000 citations. PAUL JOSEPH CLIFFORD: Well, our day to day lives have changed significantly due to the pandemic. There are some for whom social distancing practices are even more challenging. PAUL JOSEPH CLIFFORD: Doctrine Mariana will discuss the technology that his research team is developing in this area. PAUL JOSEPH CLIFFORD: As well as how student researchers continue to push ahead and their effort to assist those in need, in spite of all the social distancing challenges. Thank you for joining us this morning, Dr. MARIANA and I'm going to turn it over to you. Vijaykrishnan Narayanan: Thank you very much, and good afternoon to all of you joining in the presentation that I'm going to share with you is a work that we have been doing for Vijaykrishnan Narayanan: Around eight years and has needed some additional changes over the past two months, given the pandemic situation and the student researchers and my colleagues have been Vijaykrishnan Narayanan: Wonderful and responding to coming up with some new innovations to help those that are visually impaired in our base. As you will see, and I would also like to share some of the great things. The students continue to do innovate in their basements and in their bedrooms, even as the universities. Vijaykrishnan Narayanan: Stairs physically closed for most of us. Today's talk is going to be about a system system that we have been developing for the visually impaired. Vijaykrishnan Narayanan: And if you were wondering the title today maybe some of the pictures here give you a clue. But if you look across cultures. Vijaykrishnan Narayanan: You basically have the notion of third eye as the eyes of compassion or the eye of you if you do something wrong and it basically has things from Asian cultures to Vijaykrishnan Narayanan: Also pre Inca cultures where the third eye is actually ubiquitous and using this cultural inspiration to be benevolent and helping people could not be more appropriate at this time when we all need to put all our week together. Vijaykrishnan Narayanan: The third eye incarnation that I'm going to be talking about is more computer incarnation and we have been working on this for more than 15 years on various applications, including Vijaykrishnan Narayanan: drones that can identify objects identifying tutors and facial analytics and the key to all this is the ability of the computer to autonomously understand and reason about the visual images that it actually lens obscene. Vijaykrishnan Narayanan: students in my lab who now runs a company based on this video analytics area. So he likes burgers more than salad. So these are some of the type of things that you can end up doing, feel free to have a bite when listening to this virtual talk Vijaykrishnan Narayanan: So, Vijaykrishnan Narayanan: Application that I'm going to be discussing about of this visual analytics is on developing an assistant for the visual effects. Vijaykrishnan Narayanan: To be Vijaykrishnan Narayanan: Shopping on their own and this sort of independence is particularly important, as you will see is some of the social helpers who can hold hands with some of our visually impaired. Vijaykrishnan Narayanan: Community neighbors is not really an option in this social distancing either and some of the technologies that I'm going to be talking would be even more helpful in this scenario. Vijaykrishnan Narayanan: Technologies going from vision algorithms to hardware architecture is two technologies that Vijaykrishnan Narayanan: Design how you interface with the visually impaired are all challenges that we have been addressing towards developing this application. Vijaykrishnan Narayanan: But the first thing to do in all these things. It's not to come back and say, Hey, I have a technology, where is the US but rather to go and try and figure out what is the need, how can technology coming address thingy and overcome obstacles and challenges that are There. Vijaykrishnan Narayanan: Ever because their search big spaces is figuring out the layout of the location for like more like For malls and for maybe office buildings and places like that that are much larger than you know what a power line environment would be to have assistance. Instead of Taking you know 510 minutes to try and figure it out, all on your own. Sometimes There are a couple other workarounds using apps with face to face people maybe sometimes you don't really want to talk to somebody. So just having that Is Really currently not a Vijaykrishnan Narayanan: Choice. You can see from this video really is not a great solution to help individuals be independent and while this we do and feelings were recorded before Vijaykrishnan Narayanan: The pandemic, it's even become excruciating Lee difficult with the non availability of people to help you in these type of situations. And this is really where technology can actually play Vijaykrishnan Narayanan: The need as you can be summarized by what independent shopping would mean for visually impaired. We need to be able to help them navigate indoor spaces, while all of you are used to. Vijaykrishnan Narayanan: GPS controlled signals when driving the car are walking in outdoor spaces navigation indoor spaces is a lot more challenging. We look at that. Vijaykrishnan Narayanan: How do you help them find the product in a store among thousands of different objects that look very similar. Vijaykrishnan Narayanan: And of course, one of the things that I show you that we have developed as a priority right now is also to try and help find objects around the hoses. All of us are staying home more due to the safety reasons. Vijaykrishnan Narayanan: So the key challenges that we try to address. Vijaykrishnan Narayanan: To try and figure out how to make computers understand complex visual information, you may be reading lot of articles which basically says artificial intelligence and machine learning has made computers really Vijaykrishnan Narayanan: Powerful in detecting objects face tracking other technologies how understanding more complex visual information remains an open challenge things that we have developed include Vijaykrishnan Narayanan: How to help navigate a store, how to identify products in the store and something unique that we have developed and tested over the last two months is how to actually help pick the objects using smartphones that most of us do have access to Vijaykrishnan Narayanan: Even if we can make computers understand complex visual information. One big challenge remains in the viability of power cast and the speed at which things would work. And I'm hoping Vijaykrishnan Narayanan: My videos are streaming properly to you. They may be bandwidth restrictions, things like that connectivity issues, those are Vijaykrishnan Narayanan: Very real. If you are really have to rely on existing systems, which basically let the visually impaired show their phone transmitted image remotely and get a Vijaykrishnan Narayanan: Response back to Skype or Facebook from a distant related to a friend. Those things become challenging and from a board perspective, if you use a phone just to record video, not even to do all the computational Vijaykrishnan Narayanan: Complexities associated with understanding the with which will information you drain your phone in forever. So if you start streaming that we do are doing Vijaykrishnan Narayanan: Additional performance, it will drain even further and in fact we know from some smart classes that they actually run out of power in about 20 to 30 minutes Vijaykrishnan Narayanan: And even if we saw these type of problems to try and translate the rich visual information that we are used. Vijaykrishnan Narayanan: To people who are visually impaired remains a challenge. And I'm really thankful to many of the visually impaired volunteers that we have here who have helped us refine the technology to make it accessible to more at this time. Vijaykrishnan Narayanan: One major inspiration for a processor that can do video analytics effectively is our bring Vijaykrishnan Narayanan: And independent of what you ate for your breakfast. Your brain is operating at 20 watts of power and typically the wishful portion of it. Vijaykrishnan Narayanan: Takes about 50% of this power or it is pretty versatile and it's pretty robust, even if you show included face, you can recognize Vijaykrishnan Narayanan: If you are seeing someone after a long time, you probably recollect the context. So the power of doing such complex which will understanding in 10 Watts is unparalleled in the world of computing yet. Vijaykrishnan Narayanan: And consequently, we have actually tried to develop brain inspired software algorithms and we have also started building brains like hardware structures. Vijaykrishnan Narayanan: Not to mimic the brain, but to try and implement functions of video analytics, a lot more efficient and these have resulted in promising advances as be a bird. Many of these are system systems. Vijaykrishnan Narayanan: The team that's work as actually looked at many different aspects and just to try and tie up pieces of different aspects of this research. Vijaykrishnan Narayanan: We have had neuroscientist trying to understand how the visual cortex works in a human brain. And based on some of these studies we have actually build Vijaykrishnan Narayanan: New computational models for example the video that you're seeing on your left corner out here kind of shows what are you likely to see next. Vijaykrishnan Narayanan: Given that you saw something before when probably trying to, let's say you're ordering a Subway sandwich for lunch. So you probably look at the one first, then the sausage in between and others. So the likelihood of seen or object increases based on what you saw earlier. And as far as Vijaykrishnan Narayanan: Computer Systems architectures are concerned, we wanted Vijaykrishnan Narayanan: A system system to be immediately deployed on some of the processors and technology that is available right now, or we are also building technology that has actually seen itself start appearing commercial products. Vijaykrishnan Narayanan: As we have developed things over the last seven, eight years. And then there are still some fundamental science advances that we continue to make that may end up coming into a product near you in probably the next Vijaykrishnan Narayanan: Five to 10 years and the final aspect of it is the user interface haptic sound. Vijaykrishnan Narayanan: interfaces with which we can interact with the visually impaired people like study getting a glimpse of Vijaykrishnan Narayanan: The insights into how each one's Vijaykrishnan Narayanan: Each one of these elements contribute to the design of the system. Let's start with some of the brain inspired algorithms that we have designed Vijaykrishnan Narayanan: So if you look at the video on the left corner. This makes use of a very simple technique that our brain uses, which is the attention mechanism. Vijaykrishnan Narayanan: And this one on the left side is what's called as a bottom up attention. If you are looking for moving cars. The first thing that you look for is just things that move in an image. Vijaykrishnan Narayanan: And instead of actually having to process the entire image you can see the attention map seen in this insert helps you to locate one only places in this large image. Vijaykrishnan Narayanan: That have cars. Consequently, you can throw a lot of redundant information in high resolution images and still be analyzing smaller portions of the image efficiently. Vijaykrishnan Narayanan: Now let's imagine a trip to a grocery store and of course you want to be really quick in the grocery store these days. So one of the things is if you know that you're going to pick Vijaykrishnan Narayanan: Tomatoes. Then we have what is called as bottom of attention. You already know tomatoes are likely to be Vijaykrishnan Narayanan: red in color. So your brain kind of gravitates to just this portion of the image really quick and filters out everything else. Vijaykrishnan Narayanan: Trying to apply some of these brain inspired information into our systems helps us to process things much more quickly and with very little a fraction of the power that's required for processing things by brute force. And here is another technique that I Vijaykrishnan Narayanan: Showed you. This is about trying to look at prayers by trying to bias. What you would see based on what you already saw previously. Vijaykrishnan Narayanan: And this reduces the cognitive load from trying to recognize every object in the world to one Lee most likely objects that you're about to see, after you've seen something before. So there is another technique that we learned up implying in our algorithms. Vijaykrishnan Narayanan: And Vijaykrishnan Narayanan: Let's consider another technique that we deploy in our systems and give you a few seconds to kind of guess what this is, of course, you saw a sock and of course the paper goes along with it. So even if you didn't see the paper and are visually impaired persons camera. Vijaykrishnan Narayanan: Has an occlusion, which is preventing us from seeing the real object. Vijaykrishnan Narayanan: It may basically get some hymns, based on what it sees next to it and enhance the accuracy of our record recognition systems. And this is something that we do to our in our grocery stores, for example. So when we Vijaykrishnan Narayanan: What we have done is we have taken these pictures and looked at colocation of these objects. Oh, I see. So now do I see some Vijaykrishnan Narayanan: Washing machine. Vijaykrishnan Narayanan: Parts next to it. I see a type A of soda and I see a type to have a soda. They're both typically next to each other in the store that I'll end up going. So if I see this. I'm more likely to see something else next to it. Vijaykrishnan Narayanan: And here is our system at work when you kind of have a shopping cart moving around an island. Vijaykrishnan Narayanan: It's basically trying to learn these type of relationships particular to your own grocery store that you are going. So if you're gone. A couple of times with someone else. Vijaykrishnan Narayanan: This database is actually developing as you go and see the and most of you probably what this way, intuitively, right. Vijaykrishnan Narayanan: You go, you know, where each product is once you see another object you're able to pick things much more quickly. And what this lens of doing for our visual system is if you're trying to identify what is behind this rent box that you see. Vijaykrishnan Narayanan: You could probably guess this by looking at everything that is spatially next to it. And this is what we call as a visual co occurrence network or visual co occurrence inflammation. Vijaykrishnan Narayanan: And if you were to guess you're probably getting hungry for chips right now. And if you guess Doritos. More specifically, yes you have right in this particular scenario. And now you can see how it got easy Vijaykrishnan Narayanan: That the brain does seamlessly and these are type of things that we have included in our system implementation to make things faster and more efficient. Vijaykrishnan Narayanan: And yours. Another trick that we actually used knowing that we now are assisting a visually impaired person who's wearing this assistive technology. Vijaykrishnan Narayanan: And we really did throw everything at the algorithm to take care of all the scenarios but really leverage the abilities that this person head in trying to refine our system years. One such example. Vijaykrishnan Narayanan: Self is a feature extractor that basically looks at an image and tries to look out for ages shapes and other aspects. Vijaykrishnan Narayanan: And one interesting property of this feature detector is that it is rotation in variance. So if you rotate a particular object. Vijaykrishnan Narayanan: It still will be able to match the feature in this case from honey but bunch of words to the reference feature offered independent of the direction Vijaykrishnan Narayanan: In cases and this may be something that you are also used to when you're looking at a note or when you're looking at an object. Vijaykrishnan Narayanan: When you look at it from one direction, you probably don't identify them. So maybe you're looking at some person in their side view and you guess here. This appears to be BJ. Vijaykrishnan Narayanan: But then when you look at me at my face and the frontal view. You're sure who this is definitely VJ and what we'll end up using is using this service rotational invariance property to now come back and not just make a detection. When you see me. Vijaykrishnan Narayanan: At the site view but asked the person to move around and look at the object in the front view which increases the confidence of our matches and increases the accuracy with which our cameras are identifying objects in the eye. Vijaykrishnan Narayanan: And consequently, we don't really need five cameras, looking at five different directions, which would have made our system lot more complex. Vijaykrishnan Narayanan: Leveraging the simple notion that we have the human in the loop. Ask them to move around until our confidence in the system improves was a real useful function and this small subtle change made us get close to hundred percent accuracy in many of our trials. Vijaykrishnan Narayanan: And of course, one other aspect that is going to be the undercurrent in many of these things that I don't expand too much until the very end is we have also been building Vijaykrishnan Narayanan: specialized hardware that lens of processing these type of image processing algorithms much faster than they exist in traditional processes that you may end up getting in your cell phone on your laptops or desktops. Vijaykrishnan Narayanan: So you see our custom accelerator is about Vijaykrishnan Narayanan: Either one for our half the Vijaykrishnan Narayanan: Time is required in trying to do this computational aspects and the final aspect of course is, how would be interact with the visually impaired person, what would they, where would the camera be and one Vijaykrishnan Narayanan: Thought that we had a sense in a grocery store. The primary goal is to actually pick the object we lined up having the camera as close to the pump as possible. Vijaykrishnan Narayanan: And we attach this to a glow, which also had tactile feedback. And you can see the items label 1451 is actually a communication tablet that was going to the computer to Vijaykrishnan Narayanan: Get commands from the computer based on what the camera was see and there were motors mini motors placed at the fingers. So, for example, Vijaykrishnan Narayanan: stretch out your hand and if you imagine your pinky finger is moving. You would move this way. If you're done with moving you would move left Vijaykrishnan Narayanan: If there was something vibrating on the top of your palm, you would move up. Vijaykrishnan Narayanan: If something was vibrating below your palm, you will move down. So this was kind of a very simple tactile feedback that we incorporated to glow to do this. Vijaykrishnan Narayanan: And of course, we also tried to overlay speech like up Dawn forward type commands in trying to help pick the object and what resulted from this was our first Vijaykrishnan Narayanan: Prototype Vijaykrishnan Narayanan: Is making machines that can go see and perceive the world. Vijaykrishnan Narayanan: The system is auditory and haptic feedback on the glove to guide them to their intended product. Project gives me confidence to do a task that people aren't Vijaykrishnan Narayanan: Once we started experimenting with the system we started Vijaykrishnan Narayanan: Looking finding out some limitations. So when you basically have a hand the camera right on top of it. And when you're trying to pick something like I'm trying to come and grab my screen. Vijaykrishnan Narayanan: The hand basically prevents the camera from looking at the object. And so what really happened in this particular system is we Vijaykrishnan Narayanan: In some of the cases. What kind of a heart wrenching as you looked at some of these excellence. They had almost got to the object. Vijaykrishnan Narayanan: But then at the last second. The palm basically occluded the camera and the camera lost sight of the object. And basically, Vijaykrishnan Narayanan: Made the system stop giving commands left, right or stop vibrating and the same thing happened is Vijaykrishnan Narayanan: When you try to come to object, but Mr object to the side or the top then again the camera loses sight of the object and then you lose. Vijaykrishnan Narayanan: Track of the object. So what was required was some sort of a memory to indicate when I was in position x the object was Vijaykrishnan Narayanan: slightly in front of me and I moved my hand a little further than would object be even if the camera loses sight. Vijaykrishnan Narayanan: And that was the insight that we got that we needed to incorporate in our next version of the system. So the occlusion did not prevent. But of course, another thought was the trains figure Vijaykrishnan Narayanan: Was our idea to place the camera and the palm, the best option. Right. So that was another aspect and another challenge that we observed Vijaykrishnan Narayanan: Is when we were trying to give comments on left and right. All of them were with respect to the position of the hand. Vijaykrishnan Narayanan: And in case the portion of the hand was a little different from the position of the body, then the directions of left and right were very different in this particular scenario. Vijaykrishnan Narayanan: So we came up with a next enhanced prototype to try and address some of these issues. And again, the great thing about this whole project. It was a really team effort. We have students from Vijaykrishnan Narayanan: Middle School, all the way to veteran scientists participate in some of these ideas. And here you see a bunch of undergraduate students who really landed up doing the Vijaykrishnan Narayanan: Most of the work in building this prototype that you're seeing they included head view camera to prevent and occlusion. They started tracking the hand related Vijaykrishnan Narayanan: To inspiration to the object that they needed to bring and they use the combination of both speech and haptic feedback to the use Vijaykrishnan Narayanan: Of groceries assistant system is the Microsoft HoloLens and augmented reality headset, which has many of the sensors. We need for computer vision tasks. Vijaykrishnan Narayanan: At the center of the system system is a wireless glove, which provides haptic feedback in the form of vibration felt on the palm and fingertips. Vijaykrishnan Narayanan: These vibrations provide an intuitive form of instruction to the user, indicating in which direction they should move their hand, we have attached blue lights to each motor giving you a visual indication of which motor is vibrating. As the user faces the store shelves images are streamed to a server in the cloud. Vijaykrishnan Narayanan: In the cloud server processes. The images from the assistant system utilizing a mix of CPU, GPU and custom hardware accelerators on FPGA. Vijaykrishnan Narayanan: Here frames from the assistant system are being processed in real time on an FPGA accelerator running surf interest point detection. Vijaykrishnan Narayanan: The results of the processing are then sent back to the system system where they are used in conjunction with the HoloLens his hand tracking system to calculate the hands position relative to the item on the shelf. Finally position information is sent over Bluetooth to the glove which issues feedback in the form of vibrations at different points on the glove. Here you can see the results of the hand tracking from the point of view of user side by side with the beautiful which motors are vibrating on the Vijaykrishnan Narayanan: Grocery system systems able to quickly and accurately navigated user on the shelf. Vijaykrishnan Narayanan: So developing all these technologies. Vijaykrishnan Narayanan: Around 220 16 or so, there was a ground shift in some of the technologies and also in some of the user patterns that we started seeing Vijaykrishnan Narayanan: There was a lot more smartphones coming into the market space. And there are a lot of assistive technologies that some of these smartphones head. Vijaykrishnan Narayanan: Beyond technologies that we were developing, for example, that are accessibility features like Weiss over when you use your touch form. Vijaykrishnan Narayanan: Touchscreen forms initially as a person not aware of the day to day challenges of people with visual impairment. I was not very clear that a touchscreen would be user friendly to people with visual impairment, but there are a lot of people with visual impairment, who use voiceover Vijaykrishnan Narayanan: With these phones pretty well. And there's something that I have learned and another aspect is when we worked on all these feature extractors like surfing. Vijaykrishnan Narayanan: Other brain in spite algorithms that I saw there was another aspect called deep neural networks that was starting to get better and better. Vijaykrishnan Narayanan: If you've been able to feed it a lot of training data lot of training data. And we'll see how much. So, for example, to train a very small network you need at 1.2 million label training data. Vijaykrishnan Narayanan: So that was too much for us because in our particular application when we started our project. We never had any standard grocery store items database, let alone. Vijaykrishnan Narayanan: Things label and we tried labeling using some graduate students and undergrad with students who volunteer time Vijaykrishnan Narayanan: Well, it's a time consuming process and it's error prone, someone could basically label the entire shelf and say it was honey. Vijaykrishnan Narayanan: Cheerios in that particular image or someone would basically circle just the text on each of us in a cereal box and say that is Vijaykrishnan Narayanan: The label for the particular object. So this really did not work as well for us to generate you utilize the power of these deep networks and what was very fortunate is when you have a lot of Vijaykrishnan Narayanan: Undergraduate students and people just Vijaykrishnan Narayanan: Coming out of the teenage years, you basically see a lot of people with experience in gaming environments and that came in very handy. Vijaykrishnan Narayanan: For us, because they were all familiar with what will environments in gaming environments and they could create virtual supermarkets for me. Vijaykrishnan Narayanan: And this was really good, because now you're actually placing objects in different places you can protect them. You can create data sense, really, really quick using these virtual environments and this played a big role in enhancing some of the accuracy of our downstream. Vijaykrishnan Narayanan: Systems that we actually built Vijaykrishnan Narayanan: And we also ended up adapting our third eye prototypes to smartphones and one thing that I would like to note out here, this this particular system was something that was in progress and some of the testing. We were supposed to visit some of the groups across Pennsylvania to do some Vijaykrishnan Narayanan: Face to face testing. Vijaykrishnan Narayanan: But due to the resilience of our visually impaired volunteer community as well as graduate students Nelson so young and changan and my colleagues. Vijaykrishnan Narayanan: Jack Carolyn Mary Beth Rosen, we were able to continue doing these tests even remotely and years. Now we're going to present a demo over application. Vijaykrishnan Narayanan: Scanning. Please, please, slowly move the camera in front of you. I'll tell you when I find the item. Vijaykrishnan Narayanan: Guide button. When you are ready. For Vijaykrishnan Narayanan: Guidance market charms is 35.1 inches away 34 degrees to the right and 11.7 and just below from the camera view. Right, right, right. Left, left, left, left, Vijaykrishnan Narayanan: In front of the camera click the Confirm button or shake the phone. When you are ready. Guide Vijaykrishnan Narayanan: Confirm button. Vijaykrishnan Narayanan: Please move the item in front of the camera. Vijaykrishnan Narayanan: You got it. You have Lucky Charms. Vijaykrishnan Narayanan: So if you notice that particular video, we were able to put these detection systems into a form and hell and try to try to test this inside homes of the graduate students and then they were able to ship. Some of these smartphones to some of our volunteers. Vijaykrishnan Narayanan: Who have Vijaykrishnan Narayanan: Triangle this technology and updates to the software that were used to help pick objects in their homes and this is a technology that could Vijaykrishnan Narayanan: Continue to help people were alone in their homes without any assistance and this extranet was carried again by zoom where the graduate students interacted with the visually. Vijaykrishnan Narayanan: Impaired volunteers to try and evaluate the technology and help them adapt to new technologies, really quick. And again, I'm thankful for the opportunity to work with such volunteers, as well as graduate students who were Vijaykrishnan Narayanan: Really trying to push the envelope on this of course one challenges to try and pick an object in front of you, but how do I navigate a complex environments such as Vijaykrishnan Narayanan: Indoor grocery shop and one technique that we are applying right now is to actually have Vijaykrishnan Narayanan: A path tracing when Vijaykrishnan Narayanan: A participant who's who can see records a path loads that for the person with a visual impairment, who can retrace the pet. So imagine going into Vijaykrishnan Narayanan: A new building, and you're walking them to the restroom. But then if they need to come back. You don't need to wait for them there. They could basically retrace the path. So here is one such Vijaykrishnan Narayanan: functionality that highlights the staple for indoor navigation assistant Cafeteria Vice versa. Stopped racing navigate Navigate Vijaykrishnan Narayanan: So it's basically again looking at the visual teachers, leaving notes of what visual features and what sequence and helping you get there on your own. Vijaykrishnan Narayanan: So let me talk a little bit about the brain inspired architectures and hardware that we are doing. And one of the challenges is, if we start doing lot of these complex video analytics with Vijaykrishnan Narayanan: Just the machines that we have and looking at the task complexity of Vijaykrishnan Narayanan: visual analytics go beyond just detecting individual objects. If you want to understand more complex scenes. If you want to understand interactions between people. If you want to identify Vijaykrishnan Narayanan: actions that are necessary, then the complexity in traditional machines really grows really quick. Consequently, we have been Vijaykrishnan Narayanan: Trying to build architectures that you're seeing on the left corner, which are very similar to the neurons and synapses and what has really made this possible over the last Vijaykrishnan Narayanan: Five years is the emergence of technologies where you put these very small vertical Vijaykrishnan Narayanan: Cylinders. These are non volatile memory non volatile memory essentially means storage devices that do not lose your state when you remove power. Vijaykrishnan Narayanan: And you can place them in a very small footprint across to contact wires and you can place them very densely and the presence of this technology is held to Vijaykrishnan Narayanan: Enable highly dense memories and they also have capability of incorporating some amount of computation while doing the memory or it has transformed the traditional notion of computing and memory being different in current day processes. Vijaykrishnan Narayanan: And consequently, let me just illustrate three examples of things that we have done inspired by the brain, we have been able to mimic the brains working memory model. Vijaykrishnan Narayanan: By selectively storing certain things and erasing certain other things. When storing in memory. Vijaykrishnan Narayanan: And of course one thing that I would like you to do is selectively stole the highlights of my talk, and forget about any mistakes that I had in the top right. Vijaykrishnan Narayanan: And doing such a thing in an architecture like this gives you 150 X performance. So it's 150 times faster than CPU and GPU. What does that mean you could basically analyze more complex things in a given time. And we also sorry about Vijaykrishnan Narayanan: I don't know what happened with the Vijaykrishnan Narayanan: Sorry about the glitch. The next part is we use an encoding similar to the brain. Vijaykrishnan Narayanan: Rather than actually work with traditional type of numbers like integers are fractional numbers we think about all of pulses encoding. Vijaykrishnan Narayanan: And when we are able to use these type of pulse type of systems, we find that we're finding times more energy efficient than state of the art accelerators. That means if you had a battery. Vijaykrishnan Narayanan: That battery would last find a time smoke. Another thing, as I mentioned, you Vijaykrishnan Narayanan: Are architectures have transformed the notion that memory and compute are different in traditional computers now they are one and the same fuse together. Vijaykrishnan Narayanan: And this has both energy and performance benefits. These are some of the benefits that we have derived from brain inspired Vijaykrishnan Narayanan: Computations. Yeah. Now if you think about the Brain, Brain is a 3D structure but typical chips have transistors and active devices only on one layer. Vijaykrishnan Narayanan: We are transforming this and we have ongoing work, which is basically building 3D structures of chips that now are able to reduce the amount of time that it takes two covers Vijaykrishnan Narayanan: By clustering things together. And this has resulted in technologies where we are seeing about 50 times faster than what we can do with state of the art today. Vijaykrishnan Narayanan: What landed up sinking together, we have not been able to replace these metronome with tiny nano devices nano devices are thinner than your hair. Vijaykrishnan Narayanan: Right. And when you have to have these nano devices that are oscillating. So the going up and down oscillation is similar to your metronome oscillating. And when you put Vijaykrishnan Narayanan: coupling capacitors, like the substrate out here, they start doing functions that are very different. Traditionally, if cameras needed to detect color. Vijaykrishnan Narayanan: You needed to do complex math that you don't want to imagine right now. You want to do square root. You want to do square. You want to do division. Vijaykrishnan Narayanan: But now in our new system, all it does. It takes the pixel values into bunch of these oscillators, and based on what state the US later lands of being in and didn't you detected. You are able to go back and say this is brown Vijaykrishnan Narayanan: Color in the horse and the beauty of it is, since you simplified how the color is detected it takes far less power for the same time, then your traditional computational methods. Vijaykrishnan Narayanan: So we still have lots of ongoing effort efforts, you're trying to support more advanced analytics, maybe even query prior data store and these are things that could be useful for Vijaykrishnan Narayanan: Also cited individuals, especially for people with memory loss. Vijaykrishnan Narayanan: We are looking at other user interfaces, which is not just telling you hey, these are the objects that I want to buy from a grocery store. Vijaykrishnan Narayanan: Can it actually helped me scan the visual world and alert me to the presence of some objects. Vijaykrishnan Narayanan: We are working on hardware technologies that will now bring these things to a mobile platform your handheld devices and can last without battery charges for a long time. Vijaykrishnan Narayanan: And one thing that we are really, really interested in is to make sure this really reaches wider community for usage scenarios I leave you with some quick thoughts on Vijaykrishnan Narayanan: What the students have been working on and how they've you could repurpose some of the technologies that we have been doing for other things. Vijaykrishnan Narayanan: That are essential in the state home days and show you some examples of those, of course, one of these things are remote interaction. Vijaykrishnan Narayanan: My cameras in front of you, if I had your feet. I could see whether you're happy you're surprised. Vijaykrishnan Narayanan: Hopefully you're all very happy, looking at this video right so this can also be used for remote education to make sure the students are still Vijaykrishnan Narayanan: Engaged when we do these type of remote lectures, of course, one of the things that at least stopped in Centre County was we stopped recycling for a few weeks. So this inspired another student to do this. Vijaykrishnan Narayanan: Particular has become so ubiquitous in the US that over $9 million each year to develop trust, it's harmful to our environment or structure and our health. Vijaykrishnan Narayanan: So how can we combat this by using the abs and machine learning trash disposal. First we collected and labeled over 2200 images of trash teacher did medications lightning occlusion, etc. Vijaykrishnan Narayanan: From here you created a quantum is Tensorflow model using transfer learning with the model our model is designed specifically around on the call HTTP accelerator that attaches to the Intel ready to fly drones on onboard object detection to identify fresh Vijaykrishnan Narayanan: Eyes model or Vijaykrishnan Narayanan: Overall, Custom 3D printed Vijaykrishnan Narayanan: Future Plans about creating an in house system. Vijaykrishnan Narayanan: None of the project that you're probably going to hear a lot more about is a collaboration with props with Christina Ross singer, the students have been bringing up these cameras to try and detect pollinators. So what you seen this district has a camera, looking up at Vijaykrishnan Narayanan: One of the insects that Cody had in at her home and these are all things that were ordered and assemble during this particular Vijaykrishnan Narayanan: Stay At Home period this really shows how resilient our student community is in continuing with these innovations. Vijaykrishnan Narayanan: This project has provided me the fortune to work with a lot of people, a lot of different outreach activities to be able to in WA, and we are looking forward to more of this. I leave you with. Vijaykrishnan Narayanan: Final video Vijaykrishnan Narayanan: To showcase Vijaykrishnan Narayanan: And leave you to ask any questions. Vijaykrishnan Narayanan: Thank you very much. Vijaykrishnan Narayanan: Any questions. PAUL JOSEPH CLIFFORD: Presentation Mariana, and we do have a couple questions coming in. First question is in what ways are grocery stores adapting to your technology and how is that partnership developing Vijaykrishnan Narayanan: So right now, one of the things when I talked about the complexity reduction of our system rather than throw everything at the computing system to try and decipher what's there. Vijaykrishnan Narayanan: Some of our initial collaborators are willing to share the plan Akram of the grocery store. So it basically Prime's me to what is there. So instead of looking at a typical Vijaykrishnan Narayanan: Grocery store with about 45,000 objects when they give me the plane or claim. I know where I am. It really reduces the task of a computer from trying to find the object among 45,000 to probably just Vijaykrishnan Narayanan: Another hundred within that particular shelf and that substantially reduces the complexity and the accuracy of our vision and goal rhythms and right now our partnerships are more one to one in local state college in Vijaykrishnan Narayanan: Places around universe deeper if there are other alumni who are willing to participate in this effort and help us make the connections, I'd be very thankful PAUL JOSEPH CLIFFORD: Right, so, um, so we know by going to the grocery store. There's such a diversity in not only products, but in sizes that those actual products come in. PAUL JOSEPH CLIFFORD: Has your technology developed to the point where you could tell the difference between eight and 16 ounces size and how do they know that they have exactly what they're looking for. Vijaykrishnan Narayanan: So at this point I will not pretend that my prototype actually does the eight and 16 ounces. But one of the things from our user studies is Vijaykrishnan Narayanan: Typically the eight pounds and 16 pounds is going to way differently and the shape is different. That is not a part that I have Vijaykrishnan Narayanan: Seen are visually impaired volunteers asked for help. In fact, one of the trial test that we did just over the last Vijaykrishnan Narayanan: Two months we actually had a very similar scenario where boxes were of two different sizes and they were easily able to say which object was what we thought my system actually helping them. Vijaykrishnan Narayanan: Having said that, we are also in the confirmation process that you saw briefly in one of the videos we also have Vijaykrishnan Narayanan: Text recognition capability that we are trying to embrace that's, again, developed by my colleagues lead Giles and then Kieffer Vijaykrishnan Narayanan: And we have one of the state of the art text recognition systems. So we could easily apply that right now. But our prototype doesn't support it as this. Currently, but we are means to support it. If you find any PAUL JOSEPH CLIFFORD: Correct. So we have time for one more question. The question is, PAUL JOSEPH CLIFFORD: Talk a little bit about PAUL JOSEPH CLIFFORD: The, the level of accuracy that you will need to PAUL JOSEPH CLIFFORD: That your prototype will have to perform at to take it to the next level where we become readily available to the visually impaired, I would imagine that there's some PAUL JOSEPH CLIFFORD: That there are some legal concerns that you're thinking about, like, what's the liability of this product is used, and it's likes the wrong product and it's dangerous to the end user. So talk about kind of what what accuracy milestones, you need to hit to take this to market. Vijaykrishnan Narayanan: So in fact, I believe, as far as taking into market. We have actually as I showed the prototypes that we have developed and tested. It's actually over the span of about seven years. Vijaykrishnan Narayanan: We know many of the ins and outs and the most recent user study that we have done promises that it's probably going to be soon where the app is going to be available in an Android or an Apple phone next to you. And as far as the legal Vijaykrishnan Narayanan: ramifications for it. I would probably like my liar speak for me. Vijaykrishnan Narayanan: So, but, but having said that, Vijaykrishnan Narayanan: One of the key aspects in moving the technology. Again, I've been very fortunate to work with students who are very interested in moving this Vijaykrishnan Narayanan: Technology to the commercial space. Some of these elements have been transition to real products, either by some of my former students to their own companies and there are a bunch of students currently Vijaykrishnan Narayanan: Participating in one of the commercial challenges in getting these phones out to the people and I'm hoping and I know some of my students are listening to that this technology will actually get to their hands. Vijaykrishnan Narayanan: By the end of summer. So we are really hopeful about that part. And again, if there are people in the audience with better knowledge about Vijaykrishnan Narayanan: The legal ramifications and other aspects of it we would definitely appreciate any insight that you have. But I think the accuracy and Vijaykrishnan Narayanan: Other technological aspects are probably pretty good. None of these videos were just mark videos that I actually show they're not concept videos they really work. Vijaykrishnan Narayanan: Currently, they've been tried and tested in half so I'm not worried about accuracy. But now you got me talk think the legal consequences. But that's one step at a time. PAUL JOSEPH CLIFFORD: Absolutely. Well, fascinating research that you're doing. It's really going to have an impact and help people that PAUL JOSEPH CLIFFORD: That need it the most. Thank you for joining us today, that's all the time that we have. I'd like to thank you Dr. Marianne, and for joining us. And I'd also like to thank PAUL JOSEPH CLIFFORD: All of you who have tuned in to the virtual speaker session, whether it's through zoom or on facebook live PAUL JOSEPH CLIFFORD: As a reminder, will be having additional speaker sessions in the coming weeks. And this programming is in addition to the wide array. PAUL JOSEPH CLIFFORD: Of online networking events and career programs that are available throughout the year. You can find the full listing of all the events are the Alumni Association at alumni psu.edu slash events. Thanks again. And we are Penn State. Vijaykrishnan Narayanan: Thank you very much again. PAUL JOSEPH CLIFFORD: Thank you. PAUL JOSEPH CLIFFORD: That was great. 