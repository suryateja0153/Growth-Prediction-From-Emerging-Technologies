 Hi, good morning everybody. Welcome to the ECE Colloquium. My name is Maryam Fazel, I'm a professor in ECE and I oversee the Colloquium series this quarter. We have an exciting lineup of speakers this quarter. Please check the Colloquium website for the full schedule. You can ask questions at the end of the talk. Please use the "Raise Hand" feature on Zoom and for those of you who are taking the Colloquium for credit, I just wanted to mention some statements on your attendance of the seminars are on Zoom. For details, please see the course Canvas page and I'd now like to ask Arka to introduce our speaker. Thanks Maryam, I am Arka Majumdar. I'm an Assistant Professor here at the University of Washington in Electrical and Computer Engineering and Physics and today it's my great pleasure to introduce Dr. Sonia Buckley. Sonia Buckley is a physicist at the National Institute of Standards and Technology known as NIST in Boulder, Colorado. Sonia received a Ph. D in Applied Physics and a Masters in Electrical Engineering from Stanford University in 2014 and also her undergraduate degree in Physics from Trinity College Dublin in 2009. Her doctoral work was done under the supervision of Professor Jelena Vuckovic at Stanford University on nonlinear frequency conversion in three five photonic crystal cavities. Her current research interests are in the integration of optic electronic devices with superconducting electronics for application in integrated quantum optics and artificial intelligence. I'd also like to mention that this talk is supported, co-supported*, by Quantum X Initiative at University of Washington campus so without further ado, Sonia-- Thanks Arka and Maryam for the introduction and for the invitation. Just so you know, I actually went to graduate school with Arka who we were already calling Professor Majumdar at the time so for any embarrassing stories or if you have any or you know for any questions you have about my talk, you can email me at sonia.buckley@nist.gov.  So I'm, like Arka mentioned, I'm a physicist at NIST. I work in the group of Rich Mirin and Sae Woo Nam and Arka asked me to give this talk on quantum optics or engineering and so what I've mostly worked on has been the technologies for quantum optics more than doing quantum optics experiments myself so that's what I decided to talk about in this talk. Hang on, my slide isn't... there it goes. So that's the motivation is that we actually need to develop key components if we want to bring quantum optics to an integrated platform and what I mean by that is if we want to bring quantum optics on chip. And then if we develop these technologies, I'm not going to talk as much about that in this talk, but if we develop these technologies for integrated quantum optics, they will have applications beyond just quantum technologies. You never know what technologies will be useful for in the future. So to start with a little bit more motivation, I'm just going to give an example of a large quantum optics experiment that was done in 2015 in our group at NIST. And that was one of three Loophole-free Bell tests that was done this year. So you know for those of you that don't already know about what a Bell test is, you know you don't really have to worry about it for the for the rest of the talk but just a reminder for those of you that that kind of have an idea, basically you have a photon entangled photon source at the source and you send a photon to a distant station at Alice and when it's off and then you make you randomly choose a polarization basis to make a measurement of those photons at both Alice and Bob and then basically based on the correlations between those photons, you can rule out a classical explanation essentially. So that the Bell inequality essentially can't... you basically can't explain the result of the Bell test classically. Loophole-free just means that there you know you have to make certain assumptions about your experiments and they call those you know assumption loopholes. So you can see on the right, there's a schematic of the building at NIST where the experiment was done and you can see that the distances are you know over a hundred meters so the distance between the source analysis and the source involved so this was a large experiment and the reason for that is you need to you know you need to be able to make your measurement in order to... you need to make the measurements faster than light can travel from Alice to Bob. And so I'm going to zoom in on the source part of this experiment here so you can get a better idea of what that looks like in this experiment. So you know this is kind of a complicated schematic of the source I'm just going to point out this nonlinear crystal and then this is what the optical table actually looks like. So this was in 2015 what the source in that Bell experiment looked like and I point out the nonlinear crystal here and so you can see it's a pretty big optical table so the motivation behind what I'm talking about is to take an optical table that looks like this for a quantum optics experiment and shrink it down to the size of it of a chip and you know there are regions beyond just making it smaller. So another reason you might want to do this is this experiment took a huge amount of work to align and constant work while it was running to make sure that its state aligned because the light is steered by mirrors and other you know and other bulk optics components and these all have to be aligned. A second is if you want to scale up this experiment-- suppose you think of a quantum optics experiment that requires three Bell tests, well now you essentially need three of these tables whereas if you have it already on chip you just need three chips. That's a you know you just refabricate the same chip three times. So you can also scale up to much larger experiments-- much more complex experiments-- if you can bring everything down to the integrated scale. So then here are just some other examples of large quantum optics experiments that could benefit from being brought down to the to the chip scale. A random number beacon this is something that the group that did the Bell test at NIST has been extending it to become a source of certified random numbers. Boson sampling is another thing that's already more or less mostly being done on chip and then you can do things like multiphoton quantum state generation and linear optical computing; all of these are large experiments that really if you want to... that will really benefit from being brought down to the chips scale. Okay so then what do we need to do to bring a quantum optics experiment onto a chip? Well I'm going to talk about that in sort of three broad categories that most experiments can be broken down into and these are say detectors, the integrated optics, and the sources. And I'm going to explain each of these one at a time and sort of give an overview of our work at NIST on on these or at least the work I've been involved in at NIST in these three broad categories. So the first is the integrated optics. So what I mean by integrated optics are waveguide devices for guiding and manipulating light on ships and these can be things like beam splitters, it can be filters, optical filters, and things like polarization optics: polarization rotators and filters and and a whole host of other things as well. So the waveguide is a primary component of on-chip and integrated optics and then just a reminder for those of you who forgot or never knew, you essentially have a core with a higher refractive index surrounded by a cladding of a lower refractive index and if you do this, you can guide light in a mode and the scale of this thing can be anywhere from hundreds of nanometers to hundreds of microns. So the first thing you need to do for your quantum optics experiment is select a material platform which you're going to be using. And so there's a lot of things that go into that decision things like what if your operating wavelength? Your material obviously needs to be transparent in your operating wavelength. Are you going to be integrating it with electronics for example or active materials? Do you need an active material? For example, do you need modulators on chip? In nonlinear optics, a lot of quantum optics experiments use  parametric down-conversion as a source, which requires a nonlinear optical material. How big can it be? What's your loss tolerance? And what's your loss tolerance for coupling on and off chip? And so unfortunately a lot of the times these things come in trade-offs. For example, the lowest loss wave guides are also usually the biggest wave guides and a lot of the time if you want to integrate with an active material or integrate you're using a higher loss material as well, you may be using a smaller waveguide size and now, on and off chip coupling is harder and has higher loss. So all of these things go into your decision here deciding on a material platform and then what are the kinds of materials that we're talking about? And what we're talking about are things like crystalline silicon and then there's also deposited material, things like amorphous silicon, silicon nitride, and other deposited dielectrics. They can be things like three five so gallium arsenide, gallium phosphide, indium phosphide, other things laser written glass waveguide and then a host of other material, things like diamond, lithium niobate, aluminum nitride, and a whole bunch of other materials are also possible. But I'm just going to choose a two material platform that I've been involved with to talk about today and you can use that in mind but there's a whole lot of other materials of interest. So first is silicon-on-insulator, so silicon-on-insulator is essentially you take a bulk silicon wafer, you grow thermal oxide around three microns on it, and then the wafer bond thin film of silicon usually for telecom wavelength use around 220 nanometers tall. And then you can make wave guides in it. So your high index material is silicon. Circuit is transparent from around 1.1 to 8 microns. Typical losses in the telecom are around a 3 DB per centimeter and the typical size of a waveguide for 1.5 microns should be about 400 nanometers wide and you can do Ben radiuses of around 3 microns. This is an important one-- there are specialized boundaries available, so you can send your design to a foundry and they'll make it for you and process design kits and so on are available too. You can dope the silicon and make active devices, things like modulators. Light sources are pretty hard in silicon but I'm going to talk about a silicon light source that we've been working on a little bit later in the talk. It has a high non-linearity so you can do some kind of nonlinear optics and silicon and like I said has compact size. So this is just an image of AIM from AIM Photonics. AIM Photonics is a foundry specifically for photonics in silicon-on-insulator. You can pay for wafer and then you can have the device design fabricated there. So that's a big advantage of silicon-on-insulator. If you work on it, you don't necessarily have to fabricate it yourself in the cleanroom. What are the kinds of devices people make in silicon-on-insulator? And examples are ring resonators, things like beam splitters where light comes in here and it's split in two, gratings for coupling light into and out of the waveguide. And then those are passive devices and active devices are things like modulators-- here's a ring resonator where you can inject carriers and shift the residence of the ring and that can access [inaudible] This is an example of one of the light sources that we built that I'm going to talk about later. So this is an example of a device that we made in silicon-on-insulator. Essentially, light comes in here through this input grating. Plastic screws waveguide is split in two. This is a beam splitter, a 50-50 beam splitter, a second 50-50 beam splitter, a third 50-50 beam splitter and light is essentially divided equally in eight parts. You can couple it out through this output grating or can be measured on one of these seven detectors. And so the detectors which which we used here are actually super-conducting detectors and these operate cryogenically so we need to actually be able to couple light into these waveguide in the cryostat. And that's actually a pretty challenging problem-- it winds up being really hard to couple light into a waveguide in a cryostat. And we are interested in in doing cryogenic fiber packaging. This is you know a scalable and cheap easy way to do it and this is the first way that we've developed and so here we have a grating and the silicon waveguide. The light comes in here and couples into this waveguide. This is an SU-8 collar that we fabricate. This SU-8 is a polymer that's around 50 microns tall, so you align the fiber into this collar to the grate to the grating. You line your two fibers and glue them down and you glue them to this aluminum package that we also made. And then you can just put it in the cryostat and start your measurements so we've cooled these below 1 Kelvin with only around one and a half GB of additional law. But we're working on a second generation... unfortunately the grating couplers we've made so far haven't been really efficient so we're getting you know like at best 6 DB of coupling loss which you know as you'll see in a minute isn't really good enough for a lot of quantum optics experiments. So we're working on this next generation of fiber packaging. So here we may get tapered optical fiber-- in this case the paper is made by dipping the fiber in HF and letting it evaporate and then we coat that tip of the fiber with a higher index material. In this case, SU-8. The reason being that if you want to couple to a waveguide that has oxide on top of it, you can't use an oxide fiber you need something with a higher index. And so this couples to a tapered waveguide on chip and we get around between 1 and 1.5 dB transmission loss. And this is also Epoxy down and packaged. This paper is all at room temperature but we've made a really good progress cryogenically recently as well. And this is critical if you want to use these superconducting detectors, which is what I'm about to talk about in a minute. But first I want to mention a second material platform and that's the deposited dielectric platform. And the reason you might want to use this... one reason you might want to use this is you can actually deposit these waveguides on top of another chip. That's what what I mean by back end of line compatible you can put it on top of the fabricated chip and one of the main reasons that it's back end of line compatible is you do everything with room-temperature deposition. So you take your wafer and you deposit silicon oxide on top of it and then you deposit your wave guiding layer on top of that, which can be amorphous silicon or silicon nitride and then we can deposit our superconductors on top of that. And then this is our collar layer, which we use for the original packaging scheme.  So using these deposited dielectrics, we were seeing-- this is a ring resonator we fabricated in silicon nitride. I have a Q amount a little over a hundred thousand which corresponded to losses of around 2.5 dB per centimeter. And the reason for even this high of a loss was basically that the precursor that you use to deposit this film has hydrogen in it and this hydrogen gets incorporated in the film and there is an absorption peak around Telekom wavelength from this hydrogen and I caused the problem. So one of the scientists in our group actually came up with the idea of using an isotope of hydrogen deuterium. So hydrogen has a nucleus that's a single proton, deuterium has a proton and a neutron in it. So if you use silane with deuterium in it as your precursor, then you ship the absorption peak in the deposited film to much longer wavelength and you can wind up with propagation losses as low as 0.2 dB per centimeter so you really reduce the losses that way. We've also fabricated detectors on top of the silicon nitride film so here you're seeing a device where there's again, input grading. Light comes in here-- it can couple through one of these rings and is dropped at one of you know at a detector and this happens at four different detectors. And you can see if you scan the wavelength of the laser that you put in you can very clearly see the ring resonator response in the spectrum. So what's another good thing about these deposited dielectrics is that you can actually stack them. So you take your first deposit dielectric layer and then you can etch your wave guide, deposit silicon dioxide again on top of that, polish it down, and then you can deposit more layers of waveguides on top of that. And you can get transition losses as low as I think we got point 0.5 dB per transition between layers so you can couple light from one layer to the next. And you can also get waveguide crossings in that I think with losses in the tens of milli dB. So we can make these in a morphous silicon or silicon nitride. Again, and what you might want to do with this in bringing it back to our quantum optics experiment is you might want a low loss dielectric layer for low transmission loss and you might want to transition to a silicon or 3/5 waveguide to do active tectonics if we're doing nonlinear photonics here. Okay so now there's a whole bunch of other materials that I'm not I'm not going to talk about it but I'm just going to mention so that you know that's not the be-all and end-all. III-Vs are excellent light sources and a lot of progress has been made in wafer bonding and III-Vs on insulator. In our group you can check out this paper, but there's been other papers from other groups as well. Then there's a laser written glass waveguide. These are actually the lowest loss platform and they're very low on chip coupling loss. It's hard to make active components in them and  they're are a lot bigger so the silicon waveguides are about hundreds of nanometers wide. These are hundreds of microns wide. I mean all kinds of other materials and hybrid platforms are also possible. I know a lot of people at UW who also work on them. So then let's just bring it back to a Bell experiment example. In the Bell experiment, currently routing is done with hundreds of meters of optical fiber and the total transmission loss of that fiber is around 0.5dB. And in the Bell experiments, in order to close the detector loophoole, the total losses can't exceed 1.8 dB. So bringing that back to silicon waveguide has 3 dB per centimeter loss so you better not have more than a centimeter of silicon. You better not have a centimeter of silicon waveguide in there and if you remember our grating coupling losses with many of you. So you really need to bring a losses down after this experiment. And also it's going to require to integrate with nonlinear materials. They use a spontaneous parametric down conversion source, for example the current cell experiment. Ok so I'm going to switch now to talking about the detectors which I've already previously mentioned and showed you some examples of but let's dig a little deeper into that. So what do you need in a detector? So you usually for almost every quantum optics experiment, need single photon detection with high efficiency. If you want to use certain waveguides, you use the wavelength of the light. You need the wavelength of the light you're using to be higher than 1.1 micron... So that means your detection have to be sensitive out there. And if you want to use low loss optical fiber transmission you want them even longer at 1.55 micron. The typical detector people use are silicon APDs. These only actually work above the silicon bandgap for the same reason that you know you want so wavelength can be longer than 1.1 so that you can transmit on silicon waveguides if the [inaudible] in the silicon APDs don't work. Then you need to absorb the light in order to detect it. So we're going to talk about superconducting detectors because these work in wavelength ranges as well as being fast and efficient. Specifically, we're going to talk about superconducting nano wire single photon detectors. So how these work is you deposit a thin film of a superconducting material and you pattern it into a narrow wire. You buy an [inaudible] with a bias current. When a photon hits, it breaks Cooper pairs in the superconducting film and generates the hotspot that's not superconducting. And then the current diverting around the hot spot increases the current density everywhere else, keeps it up, and you basically get this normal region in your wire where the whole width of the wire turns normal. And now all of a sudden you would go from zero resistance to some resistance, you can develop a voltage across it and you can detect that voltage. And then the hotspot closes off and you just set it to the original state. So what are the specs of these? Well, with these detectors using [inaudible] have gotten total system efficiency of up to 98% at 1.55 microns. You can get temporal resolution or jitter as low as 3 pico seconds so those tens of pico seconds is more common. You can see count rates of up to ten MHz. You can get wavelength sensitivities from 359 nanometers all the way to 10 microns and then kilopixel array with over 99% yields have been demonstrated. The one thing is you do need a cryostat, so they operate depending on the material, you can either operate them at 4 Kelvin or for the ones that we need to operate around 1 Kelvin. So I mentioned that you can use these detectors from 350 nanometers to 10 microns so if you have but you can't necessarily use the same nanowires for all of that range. So for the narrowest nanowires, if you have a low-energy photon hitting it the first time, the hotspot can go to the width of the nanowire. If you have a wide wire, kind of learn that the hotspot might not grow to the full length of the nanowire and so you don't measure the resistance. Higher energy photon, on the other hand, is able to generate it-- deposits more energy and is able to make a hotspot to grow to the width of the nanowire. So essentially, the shorter... the longer the wavelength you're working with, the narrower the wire you need.  This is what a typical nanowire detector looks like, so the white region is the actual detector so you can see that you pull it up on itself in order to collect more light from an optical fiber mode. And then you can see that the wire width is around 100 nanometers. So when you get one of these detectors, this is what account versus bias curve on one of these detectors should look like. So as you increase the bias curve across the detector, you suddenly start detecting photons. Maybe you won't detect every photon but as you increase the bias current, you detect more and more in the foci. And eventually, it starts plateauing because you're detecting every single photon that's getting absorbed by the detector. And this is where you want to operate because this indicates you're in a region of maximum quantum efficiency and also you can bias it. Let's say here at 7 microgram and if current source is at 7.2 or 6.8 your efficiency won't change. So you can bias, it is very stable. So in the Bell experiment example, superconducting nanowire single photon detectors are already being used at the end station. So here at Alice and Bob , they've cooled to 1 Kelvin below 1 Kelvin and the original detector efficiencies were around 90 and 92 percent-- I think the setup has been upgraded now they're closer to 98 percent. And everything in the [inaudible] has been fabricated in the NIST cleanroom, here's a picture. But we do ultimately want to move a lot of our fabrication to a commercial foundry, where other people can take advantage of design and the process design tips and so on. So one thing is we do have both [inaudible] and photo lithography tool. If you use the photo lithography tools, in practice we can make the 100 nanometer wires... very easily. But everything is even easier if you can use photo lithography to make small [inaudible] detectors-- integration is faster and easier. And so one thing that  we've done in our group recently is by varying the materialistic connectors, we found that we can made wider wires that detects photons at 1550 nanometers. And this is safe because it really helps you see that you can do everything with photo lithography. As you can see, wires that are micron wide can actually detect these protons. Here you can see counts of this bias current for a wire that is 2 microns wide. So let's bring it back to the typical nanowire detector, like I said that was 109 nanometers and we're able to make them wider if we want to for integration and that's what we did with the waveguide integrated detector. So if you want to detect light that's coming in on a waveguide you basically put the detector on top of the waveguide and now light is traveling down from this waveguide with the evanescently absorbent including this detector, which is evanescently absorbed by the detector. And here in this cross section, they didn't see the optical mode. I mean drawing in the detector is in the other method [inaudible] the light propagating this makes it the absorb. And so I'm bringing you back to this work where we have the device that looks like coming into this grating evenly to seven different detectors. And so you can imagine you could use these single photonic detectors for some kind of multi photon state characterization experiments. One thing you can use to characterize the detectors is that you can use detectors that use the same single amount of light so you could [inaudible] in the detectors. We've also made other devices, this is just another example of another device we made. In this one, you put light in through this input grating. According to here, put into again to three different devices here and each of these devices, we call a high dynamic range array so basically light coming in here. 90 percent of it goes to the first defector, 10 percent goes straight on, 90 percent to the second detector, 10 percent straight on and the idea is that you can get an additional 60 dB of dynamic range using this. In practice, what we can what we've actually used it to characterize the scattered light coming from here, we can measure how much scattered light was sitting on these on these detectors. So the real question is, which quantum optics extenders are devices likely useful for or do they work for? And it really depends on if we can get an insertion loss low enough you know-- that's the packaging problem. It doesn't seem that interesting but it turns out a lot changes on it because things like higher photon number states will actually look like thermal states in the presence of too much loss. You know things like the Bell experiment requires the loss to be low enough and quantum is hard and you know, there may be other classical applications that are more tolerant and I'm going to put one or two slides on that at the end. But really it all hinges on getting a loss that's low enough. Can you get close to the phone again? I think you are kind of dropping off a little bit, thank you. Yeah so if we bring it back to the Bell experiment example, we're already using fiber coupled superconducting detectors. To switch the waveguide detectors, we're really dominated by the insertion loss and it's not really good enough yet. If we could use both detectors onto one chip, we would significantly reduce the equipment overhead. Like I mentioned, there's a separate cryostat and associated equipment at each of the den stations. Okay now the final component I'm going to talk about are the sources. So at least in my opinion, the sources are the most challenging element. And so what do we need from a quantum source? So on the right I've shown here coherent state with you know a certain average photon number. A single photon state would look more like this where you have, you know, probability of one of getting one proton rather than you know probability of getting different photon numbers. But you might also want other fun states like entangled photon and so on. You might have some kind of speed requirement if you're doing something like quantum key distribution or random number generation-- you might need a certain number per second. Size, you might have some kind of size requirement. Efficiency, if you want your sources to do some kind of interference, then they need to be in the same mode, which we call  indistinguishibility. If they're not indistinguishible, the two photons can't interfere. We need, or you might need, them to-- you know-- you might need to get the quantum state every time you push a trigger on demand and then I'm going to talk about a problem with stray light. So here again is the schematic of the source that was used in the Bell experiment. So there was a Ti-Sapphire Laser pump and that pump [inaudible] for the spontaneous down conversion. So in down conversion, you pump at with light of one energy and then you generate for every photon at that energy or for some of the photons of energy, you generate two different energies of light that add up to that original-- to that original energy. So how do you do that on chip? Well, you can't actually do that process in silicon, you can do it on chip with other materials. In silicon, what you can do is something similar where two pump photons generate a signal in an idler photon spontaneous four wave mixing. So to do that, you can pump with an off chip laser and then you can use a high Q microring resonator. You essentially set the laser to be on resonance with one of these resonances and then you can generate correlated photons in the adjacent resonances, the idler and signal beam adjacent resonances. And then ideally, on chip you would have these filters you need to get rid of this pump which is going to be you know over 100 dB brighter than idler and signal. And so you filter the pump out, you can use on chip filters and then you can send it through a beam splitter into detectors to do your quantum state characterization. And so this experiment was done in a CMOS process, it was done at NIST. It was fabricated in a CMOS process and you can see here that, well, the pair source and the filters were all on chip. The detectors were actually off chip and one of the main reasons for that was even though there were filters on the waveguide to filter out the pump light, there was so much pump light scattering around the chip that it dominated the response-- would dominate the response--on the detectors and so the detectors have to be brought. The light has to be brought off chip to really get rid of the pump light so that you can detect the correlated photon pairs. And so that's actually kind of a big problem. I mean it's only a technological problem, there's no reason it can't be overcome and I'm sure you're good [inaudible] too but that is that is a problem. Another problem is that if you put these filters on chip, they all have to line up. You need multiple filters and they need to line up with the pair source and it turns out that you know the fabrication tolerances just aren't good enough that you can guarantee that. So what you wind up doing is fabricating you know a bunch of filters and then just finding the one that wound up working. Or you can put heaters on chip to tune assessors, so you wind up you know if you want to scale this experiment up to have multiple sources, now you need you know tunable filters at every source which is again possible but it's another you know another layer of complexity and another challenge. Okay so another option is to try to build an electrically injected source because this actually eliminates the problem of the scattered pump light, if you can electrically inject your source. And so examples of electrically injected single photon sources are things like quantum dots and emissive centers so you essentially have a two-level system where when an electron decays from an excited level to a ground state, you emit a single photon. So I've been working on emissive centers in silicon and so the way that these silicon emissive centers work is they're very similar... in an emissive center, there's a defective silicon and you make them by implanting silicon ions in a silicon wafer. And so they're similar to let's say a nitrogen vacancy center or something like that and [inaudible] . There are similar type of center. There are trigonal symmetry center and you can lithographically pattern where you want to put them. So you might have heard that you know silicon is a really poor light source, which it is but it does turn out that at cryogenic temperatures there are the emissive centers that will emit light. And so then here's a spectrum from an [inaudible] emissive centers in silicon so you can see that the emission wavelength is at 1218 nanometers and the lifetime is around 30 nanoseconds. And these emissive centers operate below 40 Kelvin. And then more recently we've actually coupled some of these emissive centers to ring resonators and then to waveguides. So you can see this is a, you know, this is an in-gas camera where you can see light-- it's a light, some units emitting above the ring and also above the  collection grating. But well what I'm interested in talking about more here is electrical injection. So you can actually electrically inject and get light- we've got a couple of light out of these things.  If you implant the W centers in the "i" region of a "p. i. n." diode. So you put the W centers in the middle of a ridge waveguide and then you follow it by a diode and you get light emission coming out along the waveguide. And then here is the cross-section showing the ground if you didn't see where the W centers are. And so then here's an optical microscope image of LEDs coupled to waveguides coupled to these nine wire detectors, which we fabricated at NIST. And so you can see, this is if you take LED one and turn it on, you get you detect about 40 dB more light on that on the detector nanowire one then you do on the detector nanowire two. And that's shown here in the optical link versus cross talk spot. And we measured a total system efficiency of around 5 by 10 to minus 7 so that's not you know that's not a great efficiency, but we've done some work on folding in the message and optimization of the design system that suggests that we can do better in the efficiency although our second generation so far has only been you know a little bit better-- a little under an order of magnitude better. But we have some ideas for how to improve that further. We've also... we want to bring this ultimately to a publicly available process. And so to start with that, we have a collaborator at [inaudible] Poly who has been helping us implant W centers and 300 millimeter wafers in their clean room. And so there's me standing with a 300 millimeter wafer and then we characterize the cross wafer uniformity of the photoluminescence. And the next step now is they're actually working with us so that we can fabricate LEDS in their process so they're going to do the electrical implants and the etch as well. So we're really trying to move towards towards doing that process in a real foundry. You know as you can see here is a sort of a comparison between our cleanroom where you know someone like me standing there and then commercial foundry. Okay so I didn't talk about anything quantum about that light source and the question is will it ever be quantum? And the answer is you know, we don't know yet if the silicon and emission centers can emit single photons. There's more emission centers than just the W centers that can be investigated and you know part of the reason they haven't been investigated more is because they emit at long wavelengths where silicon APD's the traditional detectors people use, don't work. But when you have these super conducting detectors, now you know it opens you up to investigating these longer wavelength emitted centers. And if that really doesn't work, you know, we know that III-V electrically injected single photon sources are possible.  But even with those, there's more work with quality issues Basically the photons that they emit are not always indistinguishable and especially when you electrically inject them, they're not indistinguishable. But that's still an area of active research that people are really working on improving new science sources as well. So then you know let's bring it back one more time for the Bell experiment example. You know for that experiment you need an entangled photon source on chip, not just a single photon source. And in so really from the current state of the art, these nonlinear you know pump sources are the best bet but scattered light from the optical pump is still is still an issue with those-- it is by no means solved technologically. And like I said the electrically injected sources usually have worse quantum properties. So that brings me to the conclusions of this, I'll have a few more comments after this. But um you know essentially on the detector side superconducting nano-air detectors-- they can do very efficient single photon detection. They're already being used in a lot of quantum optics experiments and they work well on [inaudible] as well.  For the integrated optics, one of the main messages you know coming out of this talk is that really that no one material platform satisfies all the needs and so integrating different materials together is necessary. And that you know integrating quantum sources and detectors together really remains an open research question and remains challenging. The last thing I want to say is that if you have all this technology, you can do more things than just quantum optics with it. And it makes me do a whole bunch of other things-- you know here are a couple examples: mid-IR spectroscopy, on chip nonlinear optics, dark matter detection. And then the one that I'm going to highlight for the next five minutes which is neuromorphic computing platform. So the big advantage of the neuromorphic computing software from the technology point of view, is that there are no quantum requirements. It uses all the same elements so we don't need quantum sources and the detectors are also a lot. We don't use single photon detectors, just a few photon detectors. And so what is neuromorphic computing? It basically means trying to build hardware with the capabilities of the frame. Now if you want more on this, you'll have to invite me back for a second Colloquium some other time but I'm just going to give the one minute overview here. So the idea is basically that you know part of the reason that the brain  such unique cognitive and processing capabilities is actually down to the hardware in the brain. And it's really hard to emulate on current hardware system... current electronic systems. And so we've been designing a hardware platform that uses a lot of the elements that I've talked about today, things like multiplanar waveguides (routing) so the idea is that you know in the brain, one neuron can be connected to over 10,000 other neurons. And so optics is a really good way to do that. It uses superconducting detectors for the opposite electrical conversion. If you're going to signal this light, light is great for communication but we want to use electronics for computation and so you need to convert between the two. And then there's a couple other superconducting elements that I haven't talked about. And then the light in LEDs, the silicon LEDs that are not quantum light sources that are great for this.  They're just LEDs for producing optical pulses. And so the basic idea is that you have if you have an integrated fired neuron, you basically exceed the threshold in the neuron and it fires an optical pulse of thousands of other neurons. And so the thresholding element we're using is the superconducting to normal extreme non-linearity and so when the superconductor... you know you have a superconductor in parallel with an LED when the superconductor goes normal it diverts current to an LED in your mid and opposite pulse. And this can go down a series of branching waves like here. We shall attend to attend to a 10 to 10 neuron connection in the multiplayer waveguides material platform. So when the vision is really you know, first and foremost to try to understand the hardware in the brain but ultimately you could imagine using something like this as an accelerator in a computer similar to you know you have your quantum accelerator and your neuromorphic accelerator, you know some kind of super computer. So with that, I just want to finish up and thank the team at NIST. Obviously this isn't all my work and I would be happy to take any questions. Thank you, thank you so much for a very interesting talk. So we will be in the question session and if you have any questions, please raise your hand and I will unmute you and we can ask your question. [inaudible] We can't hear you well. Okay now, it's better. [inaudible] ...if everything on the chip unionization for optimum in quantum computing a vacation right because otherwise unwind for optimal communication that if you don't hang your own stigma has to go off the trip and you know over a long distance right because it makes sense but putting you know how the wave guy on the chair it could you see something about you know people computing what so what's the perspective there So I'm sorry, I had kind of a hard time hearing but I think what you were asking is you know pointing out that there are some quantum optics experiments things like probably quantum key distribution where you don't want it all on a chip and one of the main ones where you want it all on a chip is quantum optical computing. So yeah, linear optical quantum computing and yeah this kind of technology is definitely applicable and interesting for that so for that, you need to generate yeah you need to generate a lot of entangled photon states. You basically need to do all the same things I was talking about but many many more of that. You need many entangled photon states to you know those are your Q bits essentially. At NIST, we've been more focused on quantum optics experiments things like, I said the on chip Bell test for certified random number generation but there are you know companies and and so on like I think you know working on linear optical quantum computing trying to implement some of these technologies for that. I don't know if that was an answer to your question or not, I'm sorry. Yeah that's fine and the psych quantum recently raised 265 million dollars, I just saw in the news. Eric? Uh yeah, can you hear me? Yes, I can hear you. I was just wondering to what extent the devices that you're making and characterizing have been simulated and modeled and is there a role for that? I understand of course that the modeling would be incredibly complicated but building a simulation or a model of your device before you built it, of course would be good. Yeah so we I mean simulated a model of all of our devices before we built them. That's you know the typical procedure, is you you simulate it, you fabricate it, you test it, you compare it to your simulation, you figure out why it's different. So you know that's not you know the thing is it's hard to to fabricate exactly what you simulate I guess. Yeah so I guess is there like a feedback loop where you know you build something, you refine the models and then if so, you know what are the kinds of things that that help with that process in terms of modeling or is it always going to be sort of empirical? No it I mean what helps is you know so if you had a really standard process you design it you fabricate it you test it maybe you add some test structures that give you more information about things like loss and so on. And you could you know if your process was exactly the same every time, then you could be very very good at modeling it and and I think that's one of the main drivers for trying to move to an integrated photonics foundry is  that their processes are very reproducible. You know the the real issue occurs when I make a wafer and then I wait six months before I go back to the cleanroom and fabricate another wafer and now something's changed and it's again different. So I think it's kind of a problem of small clean rooms almost. I mean there are other problems but I see that as one of the biggest problems is you know things are constantly changing in this kind academic cleanroom or you know government lab cleanroom. Great, thanks. Is there any other question? So when people think... let me just ask one simple one this neuromorphic computing that you guys are doing is it, DNA or spiking in  this neural network? What's the neural network model or neuromorphic computing model you guys are trying to introduce? Yeah, it's spiking, it's spiking neural network model. It's kind of like what Paul [inaudible] from Princeton is doing, similar to that. I mean I think Paul at Princeton is doing several kinds of of neuromorphic computing. I think one of their main ones is not spiking. Actually I think we might even have Alex paid around to to answer your questions more about that but I think where the main silicon photonics neuromorphic computing platforms that have been proposed are not spiking and are  deep... But you guys been looking into spiking? Yes, we're looking into spiking that's right and if you're looking at spiking. Yeah well I'll just jump in I don't know if you can hear me... so it's a split I think there's kind of a division of some architectures looking at spiking in silicon photonics and some that are not and there's spiking is kind of it's the potential for better processing potentials but it's harder to integrate because it involves lasers and III-Vs, unless you're cryogenic, then you can have silicon light sources. it's like when you talk about yeah. Yeah I think then we can... if there's no other questions, we can hang the speaker by clapping in Zoom. So should we Maryam, how do you close it just in the meeting? You're muted. Yes, I also wanted to thank Sonia for the great talk and yeah see you all next week for the next Colloquium talk on Tuesday. Check the Colloquium website for the schedule and check the course Canvas page for attendance details. Thank you very much. 