 Today's mobile voice assistants are powerful but like fine-grained contextual awareness. We present WorldGaze which enhances mobile voice interaction with real-world gaze location. We achieve this by simultaneously opening the front and the rear cameras of a smartphone. We use the iPhones front-facing camera to track the head in 3D, including its direction vector. Because the geometry of the front and back cameras are known, we can raycast the head vector into the world as seen by the rear-facing camera. This allows the user to intuitively define an object or region of interest using the head gaze. Voice assistants can then use this contextual information to make inquiries that are more precise and natural. WorldGaze can also facilitate rapid interactions and densely instrumented smart environments. User: "on" Assistants: "consider it done" User: "on" Assistants: "okay" User: "what's the temperature?" Assistants: "it's 23 degrees" User: "down" Assistants: "setting to 21 degrees" User: "down" Assistants: "volume to 70%" Finally, retail settings are also ripe for augmentation. User: "I really like this one, add that to my wish list" Assistants: "you've got it" User: "does this come in a different color?" Assistants: "no, only in black" User: "how about the price difference between this ... User: " ... and this" Assistants: "left armchair is $50 less" Please see our paper for full details 