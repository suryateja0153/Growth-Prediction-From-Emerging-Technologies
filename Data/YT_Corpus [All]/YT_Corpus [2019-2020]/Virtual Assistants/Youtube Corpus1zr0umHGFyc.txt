 In this episode of the AI Show, join us as we hear from Vishesh Oberoi, Senior Program Manager on the AI platform team. Vishesh will show us how we can build a voice assistant using speech in Azure Cognitive Services with a deep dive into custom commands. Make sure you tune in. [MUSIC]  Hi everyone. I'm Vishesh Oberoi, Product Manager on the AI platform team at Microsoft focused on speech and language products and services. In this section, I'll talk to you about building voice assistance using custom commands. In this section, we will talk about typical customer scenarios we've heard around voice interaction experiences. We will dive into building a quick demo using our new custom commands product, and lastly, we will talk about extensibility, as well as how we're investing in tools and services to ensure you can build the most unique experience for your brand. So voice user interface is an emerging pattern that is gaining a lot of traction to a broad range of scenarios from media and entertainment all the way through to hospitality, automotive, telecommunications, and call centers where customers see voice-based interactions ranging from a whole lot of devices, from phones to speakers to cable boxes, TVs, cars, and so on. In a lot of these cases, it often ends up being that developers end up having to learn a lot of different technologies to build the solutions for their end users. Here at Azure Speech at Microsoft, we want to ensure that you have all the right tools to build the most unique solution for your end users. Whether it is about building a custom keyword all the way through to being able to recognize all the domain specific words that are unique to your business. As well as being able to build a custom voice that is designed and representative of your brand for your end users. Today, I'll spend most of the time on custom commands. Now custom commands makes it easy for you to build your own voice commanding solutions optimized for voice first interaction experiences using speech and language understanding technologies. With custom commands you enjoy a single unified authoring experiences that provides you with a low-latency runtimes. So you can deliver the best experience to your end users without having to worry about learning these new different technologies to build a simple voice commanding solution. Custom commands is ideal for companies in the hospitality, retail, and automotive industries, where you can easily build and create capability through voice for your end users. Be it building the best in room voice control experience for your hotel guests without having them fumble with switches and knobs and remotes, or controlling the inventory in your store through voice by collecting it, tracking it, and adjusting it, or being able to switch on the seat warmers in your car. It's really about helping you build the best solution for your needs in the simplified single authoring experience. Now without much ado, let's create off jump to a demo, and I'll take you through a simple custom commands hospitality app. So over here, on the right-hand side, I've got a simulated smart room that allows me to control my room using my voice that I'll be testing within the speech portal using the test your application feature using my voice. So let's get started. Turn on the TV.  Okay. Turning the TV on.  So that was easy. Just said a voice command that led to my TV being switched on. Okay. Turn on the AC.  Okay. Turning the AC on.  In this case, I was able to turn on the AC. Make it a bit warmer, please.  What temperature do you want to set it to?  Seventy five degrees.  Okay. Setting it to 75 degrees.  Now let me walk you through how this demo was built. So on this Azure Speech portal, I will jump into custom commands, and within custom commands, as you can see, I've already got my hospitality demo setup. Now within this portal, we've actually made it super easy for anyone to author commands that enable you to complete an action using your voice. So to begin with, I've got this section over here that has different commands. I've got a fallback command, which actually helps you define some example sentences and a default response for things that you haven't configured. I'm also enabled a turn on and off command, which essentially allows you to turn on and off different devices. The way I've configured them is by providing some example sentences using the LU format, or the language understanding schema. I've been defined specific parameters over here for different devices. So in my case, I am controlling a television and air conditioning unit and some lights. I have defined a function for switching devices on and off. Lastly, I've got different subject contexts configured as well. In some instances, you might need to have some completion rules, and to fulfill those completion rules, you can define what are the conditions. Specifically, what are the parameters you need to complete a condition, as well as what are those actions that you need to define once you've got those conditions made. So in my case, I've got a simple template that defines what those conditions are, and what are the different parameters I need filled prior to me calling an endpoint. Now through our Web Endpoints feature, I can actually enable a call to an existing API, as you can see over here, and then simply configure it in my completion rules over here. So in here, I'm calling in the control backend endpoint, and here are my parameters that I've set up for that query string. On success, I simply respond with this speech output as well. So there might be instances when these conditions are not met. In those cases, you can actually define what are the minimum required parameters for you to define an interaction rule, which allows you to complete your business logics for your command so you can complete an action on that. Where in this case, if, for example, a subject device is not mentioned, I prompt back the user by asking what device they'd like to turn on and off. Then that allows me to complete my action based on that input from the user if it's met. So once you've built your app, all you have to do is train it, publish it, and/or test it. Can you please switch it off?  Which device would you like to turn off?  Turn off the TV.  Okay. Turning the TV off.  It's super easy to set this demo for yourself. All you have to do is jump over to the Cognitive Services Voice Assistant repo on GitHub, and while we've got a whole bunch of different clients that we've actually open sourced for you to use and tinker with. At the same time for this specific demo, all you have to do is go into ''Custom Commands,'' and under the Hospitality folder, we've got a deployment scripts that you can easily run and deploy your solution in a matter of minutes. All you need is an Azure subscription and these three tools, PowerShell, Azure CLI, and.Net Core SDK. Since all of them across platform, as you can see, I was able to run them on my Mac as well. First of all, I'll login with az login, and then I'll simply run the deploy all script in PowerShell on my machine to deploy these resources in Azure to get to setup. Switching back to the slides now. As he saw in the demo, custom commands is optimized for voice to action scenarios. I was able to use my voice to control the devices that were in my room, and through language understanding, I was able to specifically define what those actions were as well. It provide multiple points of extensibility where you can actually use the Speech SDK. If you go visit our GitHub repo where I first started off, there are a few different client apps that you can use to prototype, as well as being able to integrate with existing API end points so that you can make some stateful change in a backend system. There often might be cases where you might want to grow more broader than the current custom commands capability, and that's where you can use remote skills, which is part of the settings tab within the speech portal that allows you to expand and extend your custom commands app to a much more complex multi-turn dialog system using the Bot Framework. Our goal is to meet you where you are to our platform, and being able to build your own Lego spaceship brick by brick. For that, when you're thinking about building a voice interaction solution, it's important to first start off with a great audio stack that allows for high-quality audio input optimized for voice list and speeds scenarios. Microsoft has an audio processing stack that is available and extendable to different platforms, as well as custom keyword, where you can actually define a custom keyword that will enable voice activation for your users when interacting with new products completely hands free by speaking to the keyword. Next, we got the Speech SDK, which is available in different languages with different platforms that allows you to stream your audio as well as different app state elements and events scraped to Azure. Now what we've done is we've optimized our backend to ensure that we have all our speech services co-located so that you can not just verify the keyword that you've created for your device, but also have a direct correlation from speech recognition all the way through to your dialogue engine and the backend. Now whether that is custom commands or Bot Framework, will depend on different scenarios. Similarly, you have a robust set of choices when you're trying to build language capability within your solution as well using LUIS, QnA Maker or others. On the way out, you are able to use the speech synthesis service to go from text to speech, and through our neural extra speech services, you're able to build a human-like voice that really represents your brand in a unique way for your customers. Finally, our goal is to make it easy for you to get started. We want you to be able to build a unique conversation experience for your customers that really reflects your brand, and being able to customize phase at the heart of it. Lastly, one of our core principles of developing any solution in cognitive services to ensure that your data remains our data and your insights remain our insights. I'll encourage you to go on and look at our docs at aka.ms/speech/va-no-code, as well as the code samples that we've got available on the GitHub repo at aka.ms/speech/va-samples. Finally, I can't wait to see what you build with that platform. Thank you. [MUSIC] 