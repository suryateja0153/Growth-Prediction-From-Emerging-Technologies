 so i'm very happy to introduce nick roy who is a new member of cbmm and nick is also professor in the department of aeronautic astronautics and is a member of seasale and a director of the bridge part of the quest for intelligence so the nick is one of the main representative of robotics research at mit has done great and amazing things and you will describe some of them to us today and at cbrm we of course have the goal to have research in engineering like in robotics interact and be inspired and inspire work in neuroscience and cognitive science so i'm looking forward to this talk and to a lot of inspiration nick thank you very much tommy uh it's a pleasure both to be part of cbmm and also to be giving a talk today um and uh as tommy said i'm going to talk about a few of the different things i'm also going to try and relate them to questions that i don't know the answers to that perhaps you all can help me figure out the answers to in terms of how natural intelligence exhibits some pretty phenomenal capabilities that seem to be important to robots and and uh chris mentioned the uh questions i'm happy to take questions during the talk i do have the um q a window open uh but chris you know please also if you see me steamrolling through a question please feel free to interrupt me so i can take it um so it's a super exciting time to be working in with robots and autonomous vehicles et you cetera when i first started at mit in 2003 these robots simply were not something that you saw as part of your everyday life but of course now we have self-driving cars that are you know present in boston certainly all over the place in the bay area singapore and other places on the right you see an autonomous drone delivery happening this is part of a project that i led at google a few years ago and so you know we're not just seeing robots being tested and evaluated in the real world we're actually seeing them become part of our everyday life you know we're taking the services that they promised to uh offer all starting to almost take them for granted which is super exciting for me um as somebody who wants to develop these systems and extend what they can do and understand what we need uh to develop further in terms of the technology to give us even better vehicles to understand what we need to do next it's useful to ask how do we get here so what's enabled current autonomy capabilities so a few things one is small uh computers um when i was a graduate student many years ago the total amount of computation on the surface of the earth was less than what's in the human brain and of course you know it's many thanks to moore's law that we're surpassed many many orders of magnitude relative to the human brain uh in terms of what exists on the surface of the earth but also we can now get them in really small form factors which is great for me as somebody developing small drones for instance we can put tremendous amounts of computation inside these vehicles another is just small-scale electronics of various kinds certainly the cell phone industry has given robots and drones um you know great enabling technologies another one is just a lot of the infrastructure we put up in the last 25 years or so so gpa this is a gps satellite so gps gives our systems the ability to know where they are it gives them great uh um or the telecommunications also gives us a great ability to communicate between the vehicles so these are all hugely uh enabling but i think most people in the field would agree that these have not been the biggest deals the thing that's really given us the most progress has been the development of highly accurate lightweight sensors one of the reasons why we can uh recognize self-driving cars as they drive around is almost all of them have some kind of spinning lidar on top of that basically gives the vehicle the ability to see where obstacles are recognize other cars driving around and those lidars also work far better than gps does for these vehicles to know where they are so that the laser rangefinder in particular many people have argued was the single biggest breakthrough for autonomous certainly ground robots on the right is another laser range finder this this is made by hokuya automation they thought they were making a factory safety lidar um but they were actually making a lidar for flying vehicles because this weighs only 160 grams it's about yay big if you can see my screen and it lets us do things like this so this is a video from a few years ago from my research group uh we put a lidar on this aircraft and had it fly around in the state of parking garage and in some sense this is just kind of a stupid robot trick we showed what could be done with autonomous flight um and but the reason i like this video is besides the fact it's kind of fun to look at and it was fun that we could do it it also is not a flight that a human pilot could have flown this is not to say that human pilots can't do this juma pods are extremely good um but the interesting thing is there's no place for a human person pilot to stand um so that they can actually see the vehicle all the way through the flight so if you want to keep the vehicle safe it puts a lot of pressure on the onboard sensing in order for the vehicle to know where it is and make appropriate control decisions now lighters have been great for self-driving cars but they're not especially biologically motivated they you know there are animals that have like echolocation bats uh dolphins uh etc um projected light tends to not be something that biology has done a lot of um the same students who uh put this vehicle into the air went off to found a startup called skydio which some of you may have heard of in full disclosure i'm an advisor to the company so they uh realized the limitations of lidar and it really focused on computer vision for enabling the same kind of flights so the intended application is among other things um a civilian uh infrastructure inspection so this is uh the study of vehicle inspecting a bridge i forget where the bridge is exactly i think it might be minneapolis but this is an extremely difficult operation for human pilot again to carry out because they don't have situational awareness where they're normally standing so this is entirely gps denied it's building a model of the bridge structure and it has some understanding of the kinds of operations that the human pilot wants in order to actually uh fly around it can fly you know through really complicated and tight spaces including this ver very narrow structure and the way that it works is it uses the navigation cameras the six of them uh i'll show a better picture in a second to basically build a 3d map of the environment and navigate through that and here's another video of the skydio vehicle flying relatively high speed building this three-dimensional map the voxels are false color by distance and um it does this all in real time entirely on board the vehicle so uh this is all preamble by way of saying that computer vision so lidar and sensing really got autonomous vehicles up and running um first in indoor robots and so-called service robots and then on the ground self-driving car robots and now computer vision really seems to be enabling aerial operations so so sensing hugely important but if computer vision seems to be crushing it right now how come we don't actually have more ubiquitous autonomy why don't we have robots in our houses and in our workplaces when we're in the workplaces and and why haven't the self-driving car uh companies really delivered on on the promise in like you know uh waymo uh went public in 2012 if i have that date correctly so it's been eight years and they've driven millions and millions of miles but they only operate in arizona and and the bay area so so why don't we have ubiquitous autonomy and so the rest of this talk i'm going to try and give a few reasons uh why we don't have ubiquitous autonomy and try and talk about some of the work that i and some others are doing to try and advance the state of the art um but uh here we go so i made the point that the skydio2 vehicle um you know is doing all these things but it's using six navigation cameras and as impressive as this vehicle is it can it can understand and it can process it as perception of the scene far less than what you or i could have with exactly one eye you and i can i can cover one eye and basically operate um just fine in the environment we might have a little bit of difficulty catching a baseball um that would you know it was that has traveled more than 10 or 15 meters but other than that we can do most things just fine but the skydio vehicle really has a tremendous pressure on its ability to have much much better imaging than what you or i have um and so uh that suggests that there's something about how we're using the camera imagery this probably doesn't match what biology does and that's possibly where some of some of the source of difficulty lies another issue is if you compare the amount of computation on board the vehicle it's a nvidia tiger x2 and it's a 15 watt computer and it's maxed out if you talk to the skydio folks they will tell you there's no not much room for any additional computation beyond what they're doing and if you compare that to the adult brain and sidebar it's interesting that certainly the internet and the popular literature has very little consensus on what how much power the adult brain actually consumes i thought it was about 20 watts i thought i learned that from jim dicarlo but if you go and ask you know even the literature um you get wildly different answers i think you know a doubling of possible power is wildly different it was interesting there was also jeff bezos who's been on the record multiple times saying the human brain consumes 60 watts which i'm pretty sure is not right um so it is interesting to me i couldn't find a definitive source that you know unequivocally defined how much power the brain consumes probably people's brains vary might be source of the issue but if somebody has a reference for this i would be grateful um but the point is is that the nvidia computer is about as powerful as the human brain but is producing far less uh capability out of that power and so the question is like why is it so badly misusing its computation so what's going on so it's worthwhile looking at the the structure of an autonomous system a control structure and some of you may have heard me talk about this in a program review before but you know there's parts of these vehicles that work really well and parts that don't the parts that really work so if we look at the control structure you'll notice that pretty much every single deployed and operational vehicle uh has a low level control loop that looks something like this you have sensor data that comes in it goes into some kind of probabilistic estimator like a common filter or something that says where the vehicle is in space and also it might say what's around it so you might have just a position estimator you might have a mapping solution that position estimate gets fed to a motion planner that generates a reference trajectory that gets fed to a controller um that then generates action commands to the motors and it runs at a fast duty cycle about a thousand hertz usually now this is fine but it isn't really good for anything except getting the vehicle from point a to point b so if you want to do a more complicated mission more complicated task of some kind then there's almost always a thing sitting on top of this control architecture that's very symbolic which is a very ill-defined term it's very discreet and it extracts some notion of the state estimate in a symbolic representation a discrete representation of the state estimate such as i'm i'm not at a x y position in the world but like i'm on a road a distinct road or if i'm an indoor robot i'm in this room or that room and then some symbolic planner operates in order to figure out what the high level sort of sub goal for the should be given to the motion planner like drive to the end of the road or exit the room or something like that and then this uh that gets converted into a level motion plan and so on and this is reasoning over many more things typically than the lower level and so but it good news it can run at a slower duty cycle and what you see in almost every operational autonomous vehicle one kind or another is that this lower level thing works really well very very unusual for failure in the system to result from failure and state estimator failure motion platter almost always a result of the vehicle being in some kind of operational condition that wasn't expected or some kind of mechanical or electrical failure what does fail is this this relationship between the low-level control of the robot and the higher level reasoning of what the robot's supposed to be doing almost always ends up with the robot getting stuck or not knowing what to do or doing the wrong thing so this is where the failures and autonomy come from this is why we don't have these highly capable robots around us right now so so why is it what is it about this thing that breaks it's not really this upper level high level reasoning piece that breaks it's the relationship between the two what do i mean by that well i'm going to notice i have these little arrows that represent an information or controlled flow and these little arrows typically represent information control flow condition on some kind of model so i need to figure out how my sensor behaves in order to make a state estimate i have to figure out how my planet behaves in order to give reasonable control signals those those estimates of how the sensor the plant behaves those models everywhere that i can learn that model from data i'm going to color it in green you know we have techniques like state estimation um or maybe you have reinforcement learning that's basically describing how to pick good symbolic actions based on some kind of symbolic state and this this notice that every arrow down here is colored in green that's i can learn basically everything i need to know about the low level operation of my robot from data but i can't color every arrow in green these two arrows here are red and the reason they're in red is because i can't learn them from data and inevitably what happens is that i get some engineer or some graduate student to write down a set of symbolic states some rules for how to extract them from the state estimate some rules for what my symbolic actions are like my behaviors for instance given to the motion platter and they're writing it down in python or c plus plus or something and inevitably the the engineer the grad student forgets some edge cases or fails to account for some combination conditions or somehow doesn't capture the richness of the problem the robot breaks people get a bit frustrated they come along they extend the finite state machine with extra states or extra transitions and they keep operation resumes and essentially we keep going until the next failure happens and essentially you're doing like ai or machine learning by grad student and that's not as scalable um just to make this super concrete you know this is not an artificial example imagine that you're a delivery drone company like i was running a delivery drone project at google and you might have a very simple sequence for uh delivering a package take off flight of destination address and hover lower winch etc very simple you're up and you know you're a part of this airspace where you're not going to encounter other aircraft there's no contingencies you have to worry about let's take one of these symbolic sequences fly to destination address you're a large internet company that happens to be developing this uh drone so you actually happen to you know your drone doesn't know about addresses it knows about gps but you've also got a large street view unit that's collecting street addresses and mapping the gps coordinates so why don't you ask your street view unit what is the mapping between addresses we're doing delivery and gps locations for delivery so this is an address in palo alto you look up the gps location and it's there this is a great statement of where that house is for that address it is a singularly poor location for where to deliver packages your vehicle has to understand that that's not this is a bad thing to do okay well maybe you're actually smarter than that and you also happen to have a self-driving car unit and so you asked the self-driving car unit where would you put the car well that's here on the street another really bad place to deliver packages okay maybe you've got somebody who's going like figure out where so the nearest sidewalk access is that's here it's under a tree you can't deliver that either you might actually want to deliver in the backyard except maybe there's kids playing there so i lied a little bit when i said there were no contingencies pretty much every opera every step of operating an autonomous vehicle of some kind requires understanding constantly what's happening in the environment around you the the models that we tend to operate with uh our times vehicles right now are so abstract that they really don't represent the real world and so my claim is that one of the things that's really holding back true autonomy is it requires true understanding of the environment and what do i mean by understanding of the environment um another thing that i've talked about in a program review before that some of you may have seen me mention is that this is really a question of how do we represent the environment if you went back in time uh many years ago to a robotics conference it says like the late 70s or early 80s well first of all you couldn't there were no robotic conferences in 70s or 80s all of robotics was inside ai and ai was all about logic this is rene descartes the father of logic and people were spending a huge amount of time writing down facts about how the world worked in order to get to what they thought that was the thing that they thought was important for ai and it turned out that logic was not a great way to represent the world because uh you had a trouble with reconciling inconsistent pieces of sensor information um if my claim is that the sensor was the thing that really enabled robotics then logic was poorly suited and has been poorly suited for deal handling the errors the inconsistencies and the partial observability that comes out of real sensors and robotics eventually moved to probability theory this is the reverend thomas bayes and probability was not well thought of but it was a roboticist a guy by the name of peter cheeseman who wrote in defense of probability that really argued that if you wanted to deal with the real world then you needed to be able to you needed probabilistic models that allowed us to represent uh the real world and so robotics actually you know these these kinds of models were all about what was or what robotics was all about for many years and you never really never see models like this in robotics conferences anymore you see probability distributions over and over and over again because they are how you deal with reconciling inconsistent pieces of the real world so that was a representational shift so the question then is how do what are the right representations how do we actually get to identifying the right representations for operation in in the real world so let's turn back to computer vision again uh many of you you know i'm sure have read the forsyth and ponce book and david forsyth in 96 observed that the world consisted of things and stuff so i made the claim a few minutes ago that computer vision seems to be crushing it computer vision is i don't want to take anything away from computer vision they've done a remarkable job at giving its tools work really well in in a lot of ways but i would say that right now computer vision is the best at object recognition we could argue about whether it's the best of this but it is pretty good at object recognition you can go to vision.google.com and give it an image and it will label 5000 different categories of things with very very high precision and many of you may have even better performance on a lot of these things and i'm going to observe that certainly knowing where cars and other distinct objects are in the world are but objects aren't necessarily the most useful thing for an embodied system the things that might actually be more useful are the stuff in the world stuff that aren't distinct locations of point objects in the world but terror spatially extended uh you know trees that deform and move around that really exist over not just a point in space but an extent of space even better than trees is being extract the roads so one of the things that i'd like to observe is that as we think about what are the representations that we need for true robots and embodied intelligence actually incorporating a model of the stuff in the world is is a crucial representational ability for acting uh in the world but again simply extracting the um so we could you know semantic segmentation has done remarkably well in the last few years you can actually get semantic segmentation to do up to 60 classes you know using uh less than 20 of a 1080 um at 15 hertz can't quite get that onto a drone but you can get about five hertz with fewer classes on a drone which is pretty good um and so we actually uh did this is that we put semantic segmentation on our drone and the first thing we're using an rgb d camera so an intel realsense and the first thing to notice is that if we just rely on the ranging ability of the depth camera we can't see very far we only see 10 meters maximum range if we start to extract semantic segmentations from the rgb camera image we can do a lot better in terms of understanding the scene around us and we're getting you know great uh sort of dense fill-in of the environment around us we can identify the roads uh which for a drone flying in an unknown environment are a great signal of what are good uh trajectories in the environment the um the other thing is that by having partial depth we can actually recover the depth of the those semantic segmentation pieces and actually build a three-dimensional model of the environment but reasoning about that three-dimensional model environment is complicated from a planning perspective so uh we also sparsify it as you see here um so what we do is we take the road segments and we do a graph retraction and then uh we can actually use that to fly through the environment at uh relatively high speeds here the vehicle is going nine meters per second as it flies through this is a medfield state hospital out in medfield it's no longer used as a hospital so it's a great place for us to go and fly and doing relatively low um uh uh frame rate semantic segmentation the scene and then retracting to a graph network the vehicle fly around um all of a sudden we can get you know very very uh very very good motion and we're using much less computational power than the skydio vehicle we obviously have much less detailed representation of the environment and i would argue that this is not the right representation if we're doing the kind of very careful flight in and amongst the steel bars of a bridge that the skydio vehicle was doing but for fast operation in outdoor environments maybe hybrid representations may be extremely useful and but what really want to do is actually understand everything that's around around the vehicle so if true autonomy requires understanding everything that's around the vehicle then i would love to understand how does the brain know what everything is around it um at such low computational power i don't have an answer but i'm you know within cbm i'd love to get uh a better answer to that now david forsyth said that there were things and stuff so we've talked about stuff what about things things are a bit problematic for uh autonomous vehicles right now so again object recognition uh object detection is a great technology we have it on our vehicles um but it's not perfect it's very good for recognizing the existence of an object on a flat 2d image but our robots and our vehicles exist in three dimensions they need to not only that there is a car roughly this part of the image but we need to know where it is how far it is away and how big it is for the purposes of safe operation around the vehicle but that's hard to do our object recognition is going to get it wrong a lot of the time um probably done a fine job with this car it's got the bounding box right around this car and a couple more images will probably give his ability to try uh trilaterate um on the position and uh no sorry triangulate on the position to trilaterate on the the size and distance um probably going to get this window wrong this pillar here is including the window and it's definitely going to get this window wrong here because the windows run off the edge of the frame so uh that that's problematic you know we often represent objects as point estimates we approximate the bounding box as a noisy point estimate and so we sort of take multiple measurements as sort of the centroid of the bounding box that corresponds to centroid of the object and if we have occlusions we're going to get it wrong so we need our perception system to be reasoning about not just you know spatially extended stuff in the world but we also need our perception system to be reasoning about spatially extended objects in the world so we need to represent objects as 3d volumes now we could represent them as discretized voxelized objects but there's a bunch of reasons why that's computationally not very efficient and we know that we don't reason about the world in terms of the very low level sort of voxelized representation of the world we think about objects as a single a lot of the time bounding uh solid and so an interesting bounding solid uh might be an uh ellipsoid and so if we uh reason about um sort of the nature of the edges of the bounding box with respect to the bounding ellipsoid we might do a much better job of fusing um fusing the bounding boxes so why ellipsoids um they're low-dimensional they allow a smooth closed form update as we get uh extra measurements turns out the the there's uh two 500 page uh books written simply about understanding how uh ellipsoids behave under different measurements etc really quadrix are really quite remarkable objects but highly useful for reasoning about low dimensional representations of space there is one problem is that we do have a problem of baseline so we if we start commit to instead of reasoning about uh say points of centers of objects from bounding boxes and we start reasoning about ellipsoids instead then uh we need to make sure we have enough measurements to make sure we have a well-posed problem if we don't have enough measurements to have a weld post problem then the following thing happens full disclosure this is a simulation uh but we're getting it working on the air vehicle soon we're taking bounding boxes of these objects and we are doing um data fusion for multiple views in order to extract ellipsoids that we then render here as bounding cuboids and you can see a lot of the bounding cuboids actually don't fit the um the ground truth which is the dark gray cuboids there very well that often times the ill-posedness of the solution results in poor estimates and poor estimates of the volume of free space around you is bad news for a vehicle because that's how you end up hitting things is you really want to know where um we really want to know where the objects are around you so what do you do typically what you do is you become super conservative and you put very large bounding volumes around your estimates of these things so you know there's something roughly there but you don't exactly know where it is you know uh that there's there's maybe there's a covariance attached and so uh you put a bounding uh cuboid around the covariance and and um you just give up on the fact that you don't have very good estimates and so you can't maybe perhaps operate in certain regimes but but that that seems sad that you committed to this representation of space but then you said but i'm really bad at getting that representation space what can we do about it maybe the thing to do is to actually recognize that you may have different ways of representing space so maybe you know the first time you see an object you're going to get relatively ill imposed views of that object and the best you can do is represented as a centroid of an object with some estimate of the overall um uh overall uh location um but then maybe as you get more views you get a much better estimate and i'm gonna go back there if you watch the estimate of the potted plant it actually transitions from a centroid to actually a volume around that potted planet as we got more views we're able to change the representation of space based on how much we know about the objects and so we can actually have a hierarchy of different representations we may have a very abstract representation initially which is just we know something's there um there's a bounding patch but we don't have we can't even tell where it is in space and as we get more measurements we get a point mass representation so we refine the representation and then perhaps we actually get to spheres ellipsoids or bounding cuboids as you show here as we get more and more measurements and so this is something that this kind of hierarchical hierarchical abstraction over representations is something that robotics just hasn't had um for most of its existence and i'm going to claim it's crucial for you know really understanding at the scene around you if i take the same problem that i showed on the previous slide and have the vehicle actually um move between different representations i'm going to pause here what you see is the vehicle got some initial estimates of the centroids of these objects but did not know how big they were exactly where they were so it actually put very large bounding spheres around these objects for the purposes of safe conservative flight whereas here this vehicle that's committed to the one representation of ellipsoids which again we're drawing here is bounding cuboids it's got it wrong and it's lucky that it didn't try and fly too close to the subject because it would have been wrong over here it knows it's wrong and so it's not going to try and fly too close to those objects but if i let the videos play a little longer you see that we're populating the scene in a very conservative manner and then as the vehicle turns around oh it actually already promoted that object there to something that it knows it understands well and you can see that the estimate corresponds to ground truth very very well and so uh the point here is to say that two things one is that for uh perception of the environment for a real robot we need a much better understanding of the environment than simply dense voxelized representations we need to know what things are and we need to know where they are in space and this requires us reasoning about spatially extended things like where the road network is and it requires us to reason about objects at multiple levels of representation for the purposes of putting them in the representation as accurately as possible and planning safe safe trajectories around uh the environment so this is something that i would love to understand is how does the brain reason at different levels of representation in order to actually capture everything that's around it you know when you don't have a lot of information you have to be super conservative and say that all the stuff in that general area is going to be obstacle and i'm not going to go near it and then at the same time if you um aren't at the same time as you get more information you want to refine that more and more and you need these hierarchical uh representations that you can move up and down on okay so i said the computer vision seem to be crushing it right now maybe for the purposes of image understanding but maybe not yet for the purposes of full vision on a fully autonomous vehicle um but i'm talking mostly about perception what about planning and uh okay what is uh let's try and think about what it means to have the same questions about planning you know the vehi typically for planning we understand everything about the um uh the what's what's in the model that planner is using but do we have the same issues of like hierarchical representations and what's in the model so let's choose a really really s a simple planning problem i'm going to call this the ring world imagine that we have two robots that are constrained to move on a ring and can't move through each other so i have an o robot and an r robot and um for those of you who know about configuration space i've drawn the configuration space um on the right here this is basically um so theta r is the angle that the r robot can move around the ring and theta o is the angle that the o robot comes around the ring and so any point in the space here corresponds to configuration of two robots and the gray bar right here is is where they must be an intersection and that's disallowed so you can't have the robots in in intersection so you can't be in this sort of gray stripe down the middle okay so suppose i want to put the uh the robots into a particular desired configuration um so i want to have them switch places for instance so this is a super easy motion planning problem you there's two two solutions one is that either the o robot moves this way and the r robot moves in the same direction swoops all around and that corresponds to a path and configuration space that looks roughly like this um you know the theta robot takes a long way around and the o robot takes the short way around um and you could give this to any motion planning problem they might find the other solution which is for the both robots to go the other way the o robot would go up and then wrap around and the r robot would move a little bit um but either solution is fine and you can give this to like a random sampling uh rapidly exploring random tree uh planner which you see on the left here um sirtash caramel and emilio frazolian or astro found an optimal motion planning algorithm called the rt start some years ago that's shown here and it finds the path there that's great simple simple planning problem uh unrelated anything i've been talking about before now let's make the problem just a tiny little bit harder let's imagine that the r robot is a true robot and it can move around but the o robot is actually an object and it can't move by itself it can move when pushed when the uh um when the robot is in contact and pushes it but otherwise it can't so now this is this is the uh optimal motion plan here you see the r robot only moving data o is not changing our robot moves around to come in contact with the o robot then it pushes the uh overall but notice they both move down the plane of this sort of uh uh blocked part of the configuration space and then moves off again that's the answer this problem is almost completely unsolvable as a stand-alone motion planning problem and that's super weird why would that simple change make it almost impossible for most motion planning algorithms to solve you basically have to break it into two problems one is where you figure out how to put the r the robot in contact with the object you figure out how to move the two of them together and then you figure out how to put the robot into the final configuration you break it into three separate motion planning problems and so this is basically the field of task and motion planning and and the reason why this is complicated is that there are three analytic dynamical systems there are three places where the dynamics of the system are smooth but they're smooth in different ways so you can have you have a patch where the um our art the robot is not in contact it's free to move around oh it's the middle one here excuse me it's free to move around in this orange space it can only move on horizontal stripes because it can't move the object while it's not in contact and then there's two other modes of operation one where the robot is on one side of the object and pushing it and they're both free to move in concert up and down this uh line and the one is where the robot and object are in contact on the other side of the object and they're free to move up and down this line and what's tricky for motion planning algorithms is finding the transitions between these different modes that uh notice that we only have transitions between these modes when the robot actually comes in contact and that state of being contact is a subset of measure zero if you're say sampling random states and so finding a subset of measure zero is uh essentially you know it was going to be vanishingly improbable and that's what makes this problem so hard so uh only some of the state variables can be trolled in each mode sampling from the orbits and the intersection between orbits that's what you need in order to find solutions and so by actually modeling the dynamic discontinuities this is now trivially solvable as a standard standalone motion planning problem so i made the case that for perception we needed representational ability to represent spatially extended parts of the environment around us and we needed to represent spatially extended objects around us now i'm making the case very briefly that to represent planning problems we need to represent uh we need to represent these very rare these very sparse intersections between dynamic modes of the things in our environment so we have two representational challenges we have to represent everything around us we have to represent be able to represent spatial extent we have to have a hierarchy and we have to find these very rare uh places where the dynamic modes of our interaction with environment change and that's really really hard to do at least roboticists haven't figured it out this is another example here of a robot oops planning a task of actually moving these objects from one location to another there's another task of planning problem we're not doing anything particularly fancy here but what happens when the robot is presented with an extra tray all of a sudden the planning problem changes dramatically in terms of the fact that it can actually do the planning uh solve the problem a lot faster by actually using the objects in order to move the uh or use the trade in order to move the objects a lot faster so think about what has to happen here is the robot has to reason about the tray as a spatial extended object capable of supporting multiple objects and has to reason about the uh fact that there's discontinuities in the dynamics between the objects in a nodding grasp of the robot manipulator objects ingress for the robot manipulator objects in contact with the tray um we figured out some of how to do this efficiently in terms of actually combining logical representations with um uh with the geometry and the low-level continuous dynamics of the scene but there's a lot more uh interesting research to be done here um so uh a question then i ask is like how does the brain maybe i wrote reviews here but maybe uses the wrong word or not sufficient maybe how does the brain find and use the very sparse representations that capture the the changes in the dynamic modes as we interact with with our environment there seems to be something special about discontinuities in the world for the purposes of planning let me give you another example so imagine that you're a robot faced with trying to get you're standing in an unknown environment um you can you can see what you see right here and you're told to go to a goal that's like 100 meters away in the direction of the green dash arrow and you know one thing you could do is you could build you know you've got a fancy laser you've got the skydio um uh raising system or maybe you have you know semantic segmentation that's extracting the the corridor in the hallway et cetera and building you a map but uh at the end of the day you've really only got two choices two distinct choices you can go down the hallway or you can go down the um or you can enter the into the classroom and thinking about the environment as anything other than those two choices is computationally really demanding so sparsity and discontinuity seem to be really important for driving down the computational cost of a lot of our planning algorithms and oh by the way knowing the fact that the goal is 100 meters away is super useful for choosing between these two actions um if 100 meters away it's unlikely to be inside this classroom because classrooms are not 100 meters across it's much more likely that you're going to go down this corridor and there's probably another corridor at the end that you should maybe turn left at in order to go there so we can use a lot of prior information to reason about these two distinct choices if you really were reasoning at the level of like you know optimized trajectory through this environment very hard to use that information in order to make decisions and if i tell you the goal is five meters away then you can still draw the same conclusions it's pretty likely the goal is inside that classroom and not at the not not inside the quarter going down the corner doesn't make any sense at all so how do you actually get at these distinct choices that you might reason about so uh there was some work about 15 years ago by a guy called stephen val at uiuc where he built a thing called gap tree navigation he demonstrated that if you had a robot that had a perfect sensor for sensing the discontinuities and range around the robot you could actually build a navigation strategy that had some nice properties in terms of completeness and optimality not a very practical thing but the key idea of actually sensing the range discontinuities around the robot actually turns out to be really really useful for building representations that allow us to plan efficient trajectories through the environment so this is simulation but we actually trained a gap sensor for image data and put on a real robot and this is our little rc car driving around the lobby i think this is of the sloan school e50 or e51 i forget and what you're seeing right below it is it's actually building a map by basically putting walls in between the gaps that it sees in the range uh from the camera it's doing a little bit more inference than that because also reasoning about the the whether the vertices that constitute the gaps or convex or concave and whether that constitutes a wall or not but um we actually do a pretty good job of building a uh representation of the environment that's very very efficient very very compact and is built entirely on this notion of discontinuities not in the dynamics of the environment but discontinuities in the geometric properties of the of the environment and uh you know if you can connect that with some of the ideas that i mentioned when we're looking at the classroom of the corridor is you can actually make reasonable guesses about whether to follow a you know a particular branch in uh the range discontinuity so this is an optimistic planner that's building a detailed map here and it's basically trying to get to some goal down here and then if we actually train a system to classify you know gaps as to whether or not they're likely to make progress towards the goal or not we can put that on a robot and um you see here that it does a much better job of actually getting to the goal without being distracted by um you know parts of the environment that are unlikely to correspond to things that the robot would use so i'm going to say that true autonomy will require true understanding of the environment uh bringing us know everything around it needs to reason different levels of representation and it needs to use very sparse representations as especially for um decision making now a lot of the systems that contain a learning component are increasingly important for complex autonomy and this is a problem for robotics because learning models need a lot of data from all operating conditions lighting and weather changes they're common in the real world and you know it's just data's very very expensive for um for robots to gather um i'm gonna i'm gonna skip a few slides and interests of time and just uh assert that robots right now are as data hungry as the rest of our learning algorithms um and the brain can learn from much from source corporate data that represent the real world very very efficiently i am by no means the first person to ask how the brain does this and you know would absolutely um uh love to have an answer uh to this because you know my group right now is struggling with like you know data collection for for robots um you know we got the bit i skipped to some work and sim to real but i'm not going to talk about it right now the other thing that really matters for robotics is of course uh safety so learn you know we've seen uh gen adversarial images we've even seen adversarial objects um we've seen a real failure uh on in the tesla autopilot where the the system was presented with a white tractor trailer against the bright sky that had never seen before did not recognize that as a tractor-trailer break was not applied and somebody died and uh one of the realities of learning in the real world that i i'm sure many of you you know very deeply appreciate uh as i do too is that the real world is not iid and so if you assume that the data is distributed like this um but it's actually distributed like this a lot of our current learning models have a really hard time with this and and people like alexander madrid are doing and others are doing great work in and really trying to understand how we might trust our uh representations and how we might trust our learn models um one thing that we have done is um we've taken advantage of auto encoders to at least try and do anomaly detection the idea here is that if as you're training your gap detector or object recognition system whatever you also learn to reconstruct the input through something like the information bottleneck then um we can actually recognize you know if the reconstruction is reasonably close to the input image then you feel like you might have seen this before and you can trust your system but if you um i haven't seen if the reconstruction is really terrible because this is a novel image that you've never seen before then perhaps you shouldn't trust your learn system and you need to back off to something much more conservative um i think this is just one step along the road and you know other people have have tried ideas like this before robotics doesn't really do a lot of this and i'm not quite sure why um and i think the answer might be because it's really hard to know what actually makes an uh image novel this is from some work my student charlie richter did a few years ago um and this is the basement of stata and he was actually driving a robot around and trying to have it predict trajectories using the auto encoder to determine when you couldn't trust the input image or say the classification input image and somebody stepped out of an open doorway here and the system actually characterized this as a novel image and didn't trust our prediction on it but is this a novel image you got most of the scene correct um this is a person not knowing what to do when new people show up show up seems like an important thing for safe operation and populated environments but at the same time the system was going to make an accurate prediction so there's a complicated question to be asked and answered about what truly makes an image novel and when do we truly trust our systems so questions that i would love to know the answer to is how do we represent everything around us how do we do so in a hierarchical way how does the brain use very sparse representations how does it actually infer them from rich representations how do we learn from sparse corpora and how do we trust our own perception and and our learned models and i really think that we do need new mathematical theories of representation they are you know theories of representation are very much you know uh a hot topic right now i don't want to pretend that i just discovered this there's an entire conference on learned representations but what i mean by representation i think is something very different and i'm trying to articulate requirements on our representations that are imposed by robots and operating in the real world that give us you know more autonomy they give us more complex operation that give us better safety and yes i need more data for my systems in order to um you know train up the models that we have uh so you know all of this of course is me just speaking for the work of tremendously great students um and also want to thank students even adam for the footage from skydio and been very fortunate to work with some some great sponsors 