 [Music] hello i am kobayashi from sony in this video i would like to cover some basics on recurring neural networks which are one type of neural network structure we have prepared an introduction to neural network playlist which can be found in the video description if you are new to neural networks we highly recommend that you also watch the videos in the playlist first let's discuss what exactly recurring neural networks are by comparing them to feed for neural networks which were discussed in previous videos as an image recognition solution with feed for neural networks data is processed in only way from input to output in this neural network an image for example is input and then processed in sequence the end result of the processing is the output of an image recognition result in contrast rnns are primarily used to process time series data this neural network includes a feedback loop that sends up the process information back as an input at the next time step in the sequence now that you have a basic idea of an rnn let's take a closer look at how this neural network with a feedback loop works with actual examples in this example multiple letters such as s o and n are input in word order and then processed with character recognition logic one by one the feed for neural networks we have already covered have a one-to-one configuration to produce one answer from one input if a person were to see this character's string of s o and n they would easily determine that the next letter could likely be a y to form the word sony a feed forward neural network can only produce a result based on an input of one liter and so cannot factor time series data if a neural network processes only the one letter in this particular example the image will likely be recognized as a t which will be incorrect in contrast rnas use a feedback loop to fit the output from the previous time step in the sequence as part of the input for the current time step in the sequence for example the output of the previous time step in the sequence is combined with the current time step in the sequence input via concatenate to be used in downstream in the neural network rnns are networks that have this feedback loop having feedback like this enables neural networks to use information about what data has been input or interpreted in previous recognition processes in each subsequent recognition process in the sequence in this example the information that the letters s o and n were input in this sequence before starting the process to recognize the final letter is used so that the neural network can correctly recognize this final character as a y using an rnn to recognize this time series data enables the data in this sequence to be factored into the processing we can expect better performance as a result using a recurring neural network sequentially in this way the flow of data is still actually a one-way flow this is the same as with regular phase 4 neural networks you can think of this simple rnn as a feed forward neural network with one layer added at each time step in the sequence for example each time step in the sequence is processed by a single neural network with the same weight if you look at the figure you can see that neural network parameters are common for each time step in the sequence this figure represents the processing of four neural networks being repeatedly performed by one network this neural network is trained by inputting data and then optimizing the time sequence parameters so that the output value of the loss function is minimized for each time step in the sequence as was discussed in the mechanism of neural network training video we use the back propagation technique to calculate a gradient that is used to optimize parameters back propagation is used in rnns to go back in time series order as such back propagation is referred to as bad propagation third time when used in reference to rnns this rnn can be configured in a variety of ways depending on the application taking the previous example we have a pattern in which both the input and the output are time series data because a recognition result is output after processing input data there are many multiple inputs and many multiple outputs so the configuration is called many too many in other scenarios time series data is only used as the input to produce a single output as an example we could have a pattern of using time series data of actual sentences to produce an output that simply identifies the sentence as positive or negative we could also have a pattern with a word as each time step in the sequence input so as to output a single answer that categorizes the input sentence in these scenarios we have a single output corresponding to multiple items of input this configuration is referred to as many to one conversely a one-to-many configuration would produce multiple outputs based on a single input an example of this would be taking an input of a single image and producing captions for the image captions are text consisting of multiple words that describe the image so it involves difficulty to output word sequence we can use this pattern to achieve this processing we also have many too many patterns in which both the input and output are time series data but the input and output timings are not necessarily related an example of this pattern would be a machine translation system when translating from one language to another especially ones with significantly different basic sentence structures such as with english into japanese both the placement of words within the sentence as well as the actual number of words will be different this configuration can be used for such a purpose features can be extracted from a sentence of input which are then used to produce time series data as the output in previous videos we have discussed how deep learning is a general purpose technology with rnns and enough layers and neurons we can achieve turing completeness in other words these neural networks enable us to achieve anything that can be accomplished using computers as such rnns can be applied to many different applications for example using video which is time series image data as the input and categories as the output we can use this configuration to create video recognition systems that categorize video we can make a stock price prediction system that takes past stock prices and time series data on daily stock price fluctuations to produce an output corresponding to the next stock price in the sequence we could create a speech recognition system configured to take audio which is a sequence of waves as an input to produce a sequence of text strings as briefly mentioned previously a machine translation system would use sequences of words as both input and output we can create different applications of this technology by changing inputs and outputs in different ways and creating different configurations of rnns i encourage you to think of new ways to utilize rnns obtaining practical performance and capability with the simple rnn configuration introduced in this video would actually be quite difficult to obtain practical levels of performance and capability we would need a more complex rnn configuration such as the long short term memory lstm configuration i will talk about lstm in more detail in a future video 