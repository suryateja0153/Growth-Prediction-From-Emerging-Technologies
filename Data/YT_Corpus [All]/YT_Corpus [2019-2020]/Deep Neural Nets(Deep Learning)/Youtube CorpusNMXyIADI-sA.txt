 So we had hoped to generate beautiful pictures of these different classes. In fact, all we found is something completely different, unexpected. Basically, our conclusion was deep neural networks are easily fooled. It's easy to find these types of crazy pictures, noisy pictures, they're now called fooling examples or adversarial examples that caused the network to have like, extreme responses in different ways. So we're gonna ask another question. So I've shown you something about what happens in the middle of this network, the firings going on? Let's ask another related question about this, maybe this gorilla picture that we're putting into the network, which is what makes a gorilla gorilla to the network? So I show you this picture of a gorilla kind of sideways. Sorry, I showed you this picture of the gorilla. You kind of know it's a gorilla. But what about it? Like what specifically about that picture makes it a gorilla to the network? Is it? Is it like the face of the gorilla or the further gorilla or is the background Like any animal on top of this specific type of grass, near gorilla habitats, it'll say is a gorilla. So we'd like to kind of get the network to, to draw us a picture of what it thinks a gorilla is. And this started like this whole other path of papers, which I thought was pretty fun. I'll show you a little bit about it now. Do that. So we want the network to paint this gorilla for us. So what we do is we actually give it we started with not a real picture of a gorilla, but we started with some noise. So we put some like pixely noise in, we do Ford prop. And rather than saying gorilla, because that's not a gorilla that ever tends to say, I'm not sure what that is. Lots of little neurons fire somewhat. And then we say, okay, you say the amount of gorilla there is like 1%. I'd like you to find more gorilla in this picture. So plus gorilla, please. We use something called backdrop to compute in pixel space, the little tiny pixel changes we would have to make to make the network fire more next time. So with this with these pixels that fires like say point one gorilla, if we change the pixels, we adjust this one a little brighter, a little less bright, a little more green, little more red. Maybe the next time, we can make it fire a little bit more. So we do that we take, we take this picture, we adjusted a tiny bit, and we try to get it to say gorilla a little bit more. We do that in like a four loop in Python. And we just like iterated 200 times to make it like to adjust the picture a little bit a little bit a little little bit, to hopefully make it fire more and more at the end, we output the picture and see what we see. And so here's what we saw. Here's the first, you know, run of the for loop 200 times. This is optimized for gorilla. So to us, we saw this we thought we were hoping for a beautiful gorilla or like an abstract art gorilla showing just the outline if that's what's important, but we don't see that at all. We see all these crazy pixels. So firstly thought maybe our code is broken. Maybe we start at point o one gorilla and we're basically getting to point oh to gorilla, we're not really getting anywhere. But actually we feed this network this image to the network. And it says this is point 9999 gorilla. So 99.99% The network is sure this is gorilla. That was pretty interesting. We tried the for loop again and ran many more times we can make many more kind of fake non gorilla gorillas. We tried it for different classes. So here's here's the network thinks this is a cliff dwelling or a parking meter or a Windsor tie all above 99.99%. So we had hoped to generate beautiful pictures of these different classes. In fact, all we found something completely different unexpected. Basically, our conclusion was deep neural networks are easily fooled. It's easy to find these types of crazy pictures, noisy pictures, they're now called fooling examples or adversarial examples that cause the network to have like, extreme responses in different ways. Okay, so this is this was like the first first paper in this direction. We tried this is without regularization, you get these crazy patterns, we tried adding more regularization, so like making pushing these pixels toward more realistic patches of images. Some other folks did this at the same time as well. So this is using l to regularization, this is keeping the images near the origin. This is a learned regularizer, which is a little bit more complicated. In both of these cases, you can kind of see a little bit of birds emerging or peacocks emerging. After playing around with it for quite a while, we found a different combination of regular risers, so ways of running this for loop in Python, that resulted in much more clear visualizations. so here we can now draw like this is what the network thinks that was a hartebeest or a school bus here. So now what other than this pixels we can kind of see the network things when when we say hartebeest, what the network thinks is a couple of features actually. So first, you can see these like curvy horns. So somehow hartebeest actually our animals with Kirby horns. The network thinks the horns are important makes sense because they're the Kirby horns are kind of a unique feature of hartebeest. It also knows that there's sort of like green in the background and that they tend to be kind of brown. On the other hand, you don't really see hartebeest hose, right? So the network doesn't really think of like the important part of hartebeest is having hooves, probably because it's trained to recognize so many other animals with homes. So homes are not really special to hartebeest. It's like we talked about before with the dogs, the different species of dogs. If you train the network to recognize 100 different species of dogs, it'll have to learn like these little features of the hair and the ears and the shapes and so on. If you just lump all the dogs together, though, any simple feature that makes it a dog instead of a cat will suffice. Cool, so we can try this for other things, too. If we visualize like the output layers, we visualize the last layer neurons, we can basically visualize the classes that the network knows about. Unfortunately, we know about those two. So here's what it thinks a pirate ship is or a teddy bear or a pitcher, we can also ask you to visualize like intermediate neurons. So if we visualize the two days before that six and seven, we see kind of a crazy amalgamation of different patterns, we actually don't really understand what we see here very well yet. However, if we visualize lower layers, so all the way down to the bottom, here's these patterns I showed you before. It's very simple, like edges, like things changing from light to dark green to red, something like that. One layer up, you can see it's combining these simple patterns in the slightly more complicated patterns. So here's, you see corners and edges and curved things. layer three, four or five, by layer five, we see some pretty cool stuff. We see for example, the face detector is not on here, but there was the face detector, we see also, here's the detector for like legs emanating from a central area. So maybe for octopus or, or spiders and thing we see here like a three dimensional corner, there's some water, we can Tech water. This is actually the text detector, it's pretty hard to see if you zoom in. This is like a big picture. If you zoom into the center, you'll see actually what it's looking for when it detects text isn't like abcdef. It's simply a relative like vertical and horizontal spatial frequencies that happened to correspond roughly to text. So it's good to know our network can't actually read can't read English or for that matter, other languages. But by knowing that it's looking for these simple patterns, we can actually kind of assume if we showed it Latin instead of English, it would respond to just about as much. So by taking all these approaches, all these things like looking at it from all these different angles, we can start to understand more and more about what neural networks are actually doing. 