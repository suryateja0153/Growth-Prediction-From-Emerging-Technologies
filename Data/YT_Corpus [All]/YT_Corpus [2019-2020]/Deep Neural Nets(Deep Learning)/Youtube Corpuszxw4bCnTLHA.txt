 Hey guys, welcome to Intellipaat. In today's session, we are going to learn all the fundamentals of one of the most interesting topic in artificial intelligence, that is neural networks. Neural networks are algorithms inspired by the biological neural network that is the human brain. Now before we start, do subscribe to Intellipaat's YouTube channel so that you never miss out on any of our upcoming videos. Now let's look at the agenda for today's class. At first, we'll start with a proper introduction to neural networks, and after that we will learn what is the activation function, and then we will learn about feed-forward neural networks. You will also learn about multi-layer Perceptrons. Also guys, if you are interested in becoming a certified AI professional, then do check out the Artificial Intelligence Course offered by Intellipaat. You can find the course link below in the description box. Now without any further delay, let's get started. So, a neural network is a computing model whose layered structure resembles the networked structure of neurons in the brain, with layers of connected nodes. So, it can learn from data which can be trained to recognize patterns, classify data, and forecast future events. So, the neural network is based on the biological neural network of our brain. So that is why it is given the name neural network. So, the layers are interconnected via nodes, or neurons, with each layer using the output of the previous layer as its input. So, its main function is to receive a set of inputs, perform calculations, and then use the output to solve the problem. Now, as I've already said, these artificial neural networks are based on something known as a biological neural network. So, our biological neural network has dendrites, cell body, and axon. So dendrites are where the input is taken, cell body is where the processing is done, and axon is where the message is transferred to other neurons. The same thing happens in artificial neural network as well. So first we give the data. That data is processed and then the final processed result is given out as the output. So, over here, let's say we train the data with images of a cat, and the labels would be either 'cat' or 'not cat.' After that, we give a new image of a cat and then we basically try to predict whether the model correctly classifies this as 'cat' or  or 'not cat.' Since the model has learned the data properly, it correctly classifies this image as cat. Now, there are many ways of knitting the nodes of a neural network together, and each way results in a more or less complex behavior. Possibly, the simplest of all topologies is the feed-forward network. So, feed-forward neural network signals flow in one direction without any loop in the signal paths, and typically artificial neural networks have a layered structure. The input layer picks up the input signals and passes them on to the next layer known as the hidden layer. There can be more than one hidden layer in a neural network. At last comes the output layer that delivers  the result. Now, the first question to pop into your head would be what is the inspiration behind these artificial neural networks? Well, the answer to that is the biological neural network of a brain. So, let us first understand the architecture of a biological neuron. So, as you can see in the slide, our biological neuron has these three main components: we have the dendrite, the cell body, and the axon. These dendrites receive the signals and the cell body processes these signals, and the axon finally sends out these signals to other neurons. So, just like the biological neuron, the artificial neuron has a number of input channels, a processing stage, and one output that can fan out to multiple other artificial neurons. So now let's understand these artificial neurons in detail. These artificial neurons are the most fundamental units of deep neural network. It takes an input, processes it, passes it through an activation function, and returns the output if the condition is met, or else, it'll process it again until you get the correct output.  Such type of artificial neuron is called as a Perceptron. So, they are basically like a linear model which is used for binary classification. So as the figure shows, we have X1, X2, X3, and going on till Xn as inputs in the input layer to which we add the weights and the Bias that are randomly selected. So here we have W1, W2, W3, going on till Wn as weights. So, we multiply these weights with the corresponding inputs and add all the values together, and finally we add Bias to that sum. So, this final sum is passed through an activation function which finally gives us the output. So, let us see this in detail. Just a quick info guys: If you want to become a certified AI professional, then check out the Artificial Intelligence Course offered by Intellipaat. You can find the link in the description box below. Now let's get back to the session. So, here we have three arrows which correspond to the three inputs coming into the network. Now for these three inputs, we have corresponding weights associated with them. So input 1 is associated with the weight of 0.7, input 2 is associated with the weight of 0.6, and input 3 is associated with the weight of 1.4. Now, these inputs are multiplied with their respective weights and their sum is taken. So, the three inputs are X1, X2, and X3. The sum would be X1 multiplied by 0.7 plus X2 multiplied by 0.6 plus X3 multiplied by 1.4, and to the sum, we add an offset which is called as Bias. So this Bias is just a constant which is used for scaling purpose. Now let's understand the concept behind these weights. So, these weights basically determine the relative importance of the inputs. So let's say we have two inputs 'humidity' and 'wearing a blue shirt.' So, here we can see that wearing a blue shirt has almost no correlation with the possibility of rainfall. That is why, the weight assigned to input x2 would be low in order to bring down its importance. Now let us see why do we need activation functions. So consider the scenario where you have two different classes: one class is represented with triangles and the other class is represented with circles. Now let's say I ask you guys to draw a linear decision boundary which can separate these two classes. So is that really possible? Can we draw a linear line which can segregate these two classes? Well, the answer is obviously no. So let me tell you guys how can we do this. So we'll have to add a third dimension to create a linearly separable model which is easy to deal with. So the logic is, when you're going from 2d to 3d, you are making your equation non-linear. So with the third dimension, I have introduced non-linearity in our data which helps in creating a linearly separable model. In real-world situations, you don't always get linear problems. So, you should know how to deal with nonlinear problems as well, and this is where activation functions help us to convert the linear equation to non-linear form. So these activation functions bring in non-linear functional mappings between the input and the response variable. Their main purpose is to convert an input signal of a node in an artificial neural network to an output signal, and if we do not apply an activation function, then the output signal would just be a simple linear function. Now, there are many types of activation functions, and today we'll be discussing some of the widely used ones. So let's start with the identity function. The identity function gives out the same output as the input. So no matter how many layers we have, if all the activations are identity functions, then the final output of the last layer would be the same as the input given to the first layer. The range of the identity function goes from minus infinity to plus infinity. So after that we have the binary step function. This binary step function is usually denoted by H or theta, and it is a discontinuous function. So if the input is less than 0, then the output would be 0, and if the input is equal to or greater than 0 then the output would be 1. This is why binary step function is used to solve a binary classification problem. After that we have the sigmoid function. So the formula for the sigmoid function is denoted by 1 upon 1 plus E power minus x. The sigmoid function basically scales the values between 0 and 1. So if the input is a large negative number, it is scaled towards 0, and similarly if the input is a large positive number, it is scaled towards 1. Then we have the tan h function. It is a hyperbolic trigonometric function which scales the values between minus 1 and plus 1. So, one advantage of tan h function over sigmoid is that it can deal more easily with negative numbers. After that, we have the ReLU function which stands for rectified linear unit. So, this function will give out 0 if input is less than 0 and on the other hand, if input is equal to or greater than 0, then it'll act as an identity function and give out the same value as the input. Thus, ReLU function is the most widely used activation function and is primarily implemented on the hidden layers of the neural network. Then we have the Leaky ReLU, which is just a modified version of ReLU. So the Leaky ReLU, instead of just completely removing the negative part, it just lowers the magnitude. Finally, we have the Softmax Function which is ideally used in the output layer for classification problems. So the Softmax Function basically gives a set of probability values for each class of the output and that particular class which would have the maximum probability will be our output class. So that was all about activation functions. Now, let us learn more about Perceptrons. So, we were taught how to behave in certain conditions. Perceptrons also required training, so they have a learning algorithm through which they produce the output. By training a Perceptron, we try to find a line, plane, or some hyperplane which can accurately separate these two classes by adjusting the weights and biases. So, consider this image where we give the dogs and horses as input. So here, after the first iteration, error value is 2 since the horse has been classified as dog and there is one dog which is placed in the horses class. In the second iteration, the error value is reduced to 1 as it is just a dog which is classified as a horse. Finally in the third iteration, we get the correct output as the Perceptron has been trained well with no error. So, all the dogs have been placed in one class, and all the horses have been placed in one class. Now let's understand the Perceptron training algorithm. So this Perceptron over here receives multiple inputs, and each input is initialized with a random weight. So after this we multiply these weights with their corresponding inputs and then we get the sum. Now this input is passed through the activation function which would give us a nonlinear output. So this process until here is known as feed forwarding. Now if the output which we get is not optimum, we calculate the error in prediction and then go back and then update the weights and bias. So this process where we go from output to the input layer is known as back propagation. We keep on back propagating until we get the desired output. So, that was the Perceptron training algorithm. Now let's have a look at the benefits of using artificial neural networks. So the artificial neural networks can learn organically, this means, an artificial neural network's outputs aren't limited entirely by inputs and results given to them initially by an expert system. So artificial neural networks have the ability to generalize their inputs. This ability is valuable for robotics and pattern recognition systems. Artificial neural networks also help in nonlinear data processing. So, nonlinear systems have the capability of finding shortcuts to reach computationally expensive solutions. These systems can also infer connections between data points rather than waiting for records in our data source to be explicitly linked. This non-linear shortcut mechanism is fed into artificial neural networking which makes it valuable in commercial Big Data analysis. Artificial neural networks also have high potential for fault tolerance. When these networks are skilled across multiple machines and multiple servers, they are able to route around missing data or servers and nodes that can't communicate. These artificial neural networks can also self-repair themselves. So, if they're asked to find out specific data that is no longer communicating, these artificial neural networks can regenerate large amounts of data by inference and help in determining the node that is not working. This trait is useful for networks that require informing their users about the current state of the network and effectively results in a self-debugging and diagnosing network. Just a quick info guys: If you want to become a certified AI professional, then check out the Artificial Intelligence Course offered by Intellipaat. You can find the link in the description box below. Now let's get back to the session. Now to implement these artificial neural networks, you would need the help of a deep learning framework. so the first question to pop into your head would be what are the different deep learning frameworks available so today when we cover just start so let's start with tons of look so tensor flow is arguably one of the best deep learning frameworks that we have today it is an open source software library developed by the researchers and engineers from the Google brain team for high-performance numerical computation one well-known use case of tensorflow is Google Translate so Google Translate is coupled with capabilities such as natural language processing text classification forecasting and tagging so tensorflow basically comes with two tools tents Abood and tensorflow serving so building massive deep neural networks could be complex and confusing this is where we can use 10 support to visualize our tensor flow graph and plot quantitative metrics and then we have tensorflow sewing which is a flexible high-performance serving system and can be used for rapid deployment of new algorithms while retaining the same server architecture and epi s-- so now let's look at the next deep learning framework which is chaos Shakira's is actually a high level api which can run on top of other deep learning libraries such as stands of lúthien o or c NT k and with the help of cara's you can implement both convolutional neural networks as well as recurrent neural networks and the best thing about Karis model building is extremely easy it's like stacking layers on top of each other so next we have PI dot which is our scientific computing framework developed by Facebook so we can get from the name itself that PI tortures pythonic in nature thatis it can leverage all the services and functionalities offered by the Python environment and also smoothly integrates with the Python data sine stack another great feature of pi dot is not it offers dynamic computational graphs which can be changed during runtime this is highly useful when we have no idea how much memory will be required for creating a neural network model and the next deep learning framework is DL for gyp so unlike deep learning frameworks which we saw till now which were all based on Python deep learning 4G is our deep learning programming library which is written for Java and the java virtual machine and the biggest advantage of DL Fujita's it includes inbuilt integration with Apache Hadoop and spark so it helps in getting state-of-the-art results on image recognition tasks so it shows matchless potential for image recognition fraud detection text mining parts of speech tagging and also natural language processing and finally we have a mix net so Amex net is a deep learning framework developed by Apache Software Foundation specifically for the purpose of high efficiency productivity and flexibility and the beauty of Emmet's net is that it gives users the ability to code in a variety of programming languages such as Python our Giulia and Scala this means that you can train your deep learning models with whichever language you're comfortable in without having to learn something new from scratch and this deep learning framework is known for its capabilities in imaging speech recognition forecasting and NLP so when you hear the term tensorflow the first question to pop into your head would be what exactly is a tensor so when tensor flow data is represented in the form of tensors simply put a tensor is a multi-dimensional array in which data is stored so you can consider these tensors to be the building blocks in tensorflow now these very tensors are given as the input for the neural network so as I've said the tensor has nothing but an N dimensional array so the number of dimensions used to represent the data is known as its Frank so if for a tensor has just one element in other words if it has just magnitude and no direction then it's rank will be zero if a tensor has magnitude and direction in one plane then its rank will be 1 similarly for tensor has magnitude and direction in two planes then its rank will be two and this goes on higher of the order now tensor flow has the name speed's as a combination of two words tensor and flow here the data is stored in tensors but the execution is done in the form of a graph so this is not like your traditional programming we just write a bunch of lines and everything gets executed in sequence so first you'd have to prepare this computational graph and then this computational graph is executed inside something known as a session now up in this computational graph all the mathematical operations are depicted inside the nodes and all the tensors are represented on the edges so the entire computation process is done in two stages in the first step the code is depicted on to the computational graph and in the second step a new session environment is started and the graph is executed inside this session so what that was all about the computational graph now let's look at the program elements in tensor flow so we have three program elements constant placeholder and variable so well let's start with constants so constants are program elements whose value does not change or in other words the value is fixed so let's head on to jupiter notebook and work with these constants right so my first task would be to import the tensor flow framework so I'll type input tensor flow as TF I'll click run so let me just wait till the import is done right so we have successfully imported the tensor flow framework now parallel set let's go ahead and start working with the constants so let me just type in constants over here so I create the first constant and name this constant as corn one now this is how we can create constants intensive flow so I will use this DF and then put in a dot and then type in constant now inside this I will give the value of the constant so let's say the value is 10 so this is an integer type constant now similarly I will also create a floating type constant and I love store this in corn too so I'll type TF dot constant and the floating value would be 3.14 now after this I love create a string type constant so again this would be D F dot constant and the string which I'd be giving would be this as Sparra and finally we have a boolean type constant and I'll stir this in corn for so this will be D F dot constant and let's say the value is false now I love run now let me print all of these values so I use the print function and then I'll go ahead and print all of these values con 3 corn 4 right so we see that this first constant is a tensor of type integer the second constant is a tensor of type float this third constant is a tensor of type string and this fourth constant is a tensor of type boolean now you see that we only have the data types of all of these tensors but we don't have their values this is because as I've already told you guys we have to create a computational graph and then execute that computational graph inside a session but till now we have not started our session so let's go ahead and start a session first so I'll rights s equals D F dot session and I'll hit run now I will run all of these inside this session so I will type cess dot run and let me go ahead and run all of these corn one corn to corn three and corn 4 so this time we have the values of all of these tensors so the value of constant one is stand the value of constant two is 3.14 the value of constant three as this is part up and finally the value of constant for is false right so first we'd have to create all of the constants then we'll have to create a session and inside this session we'd have to run all of these constants now let me go ahead and perform some simple operations on all of these constants so let me just type in operations over here so I'll do a simple addition operation so I'll type addition over here and let's see the value of the first constant s UND I'll put a plus symbol and then I'll take in the next constant and the value of the second constant would be 30 so I am basically adding two tensorflow constants the value of the first constant is 20 the value of the second constant is 30 and I'm storing that result in addition similarly I will multiply these two constant now so multiplication TF dot constant so I'll get the value of 20 now I'll put the asterisk symbol and then this would be d F dot constant of 30 so this time I'm multiplying these two values right so while hit on run so now again if I'd have to see the resultant addition and multiplication values I'd have to run these two inside a session so wild-eyed sirs dot run and then put in these two values over here addition and multiplication right so we see that 20 plus 30 gives us an addition value of 50 and similarly when we multiply 20 with 30 we get a result of 600 right now so this was BC cooperation with scalars and we already know that tensors can have higher dimensions just a quick info guys if you want to become a certified AI professional then check out the artificial intelligence course offered by intellibid you can find the link in the description box below now let's get back to the session so let's go ahead and perform addition and multiplication with these higher dimension tensors right so again I'll just put in addition over here and I will taken the first constant and inside this I will given a list of values so let's see I will take in one two three four and five and I will add this list with the next constant and this time the second constant has the values of five four three two and one similarly I'll also multiply so multiplication equals T F dot constant and thus will have values let's see the same values one two three four and five let me put our come over here right now I'll put the asterisk symbol again I'll type the F dot constant and I will give in the list of the values five four three two and one now I'll hit run again I need to run these two inside a session success dot run let me put in addition over here after that I would also need the multiplication value right so this is a result so when we add 1 plus 5 we get 6 when we are 2 plus 4 we get 6 similarly when we add each of the corresponding elements with these elements inside the list we get all of the successor video now let's take this multiplication results over here when we multiply one with 5 we get Phi when we multiply 2 with 4 we get 8 3 cross 3 gives us 9 4 cross 2 gives us 8 and again 5 cross 1 gives us a fight so this was addition and multiplication with respect to lists now let me also do a simple operation on strings so let me begin the four string and name it as str1 so this is a constant so TF dot constant and let's say I type over here I love and then I give us peace now I will take in the second string which would be STR two and inside this and again this would be a constant so TF dot constant and the value of this constant would be tensorflow right now I will run this and let me execute this inside a session so says dot run of str 1 plus STR 2 so the result which you get is I love tensorflow right so the 4 string is I love and the Nessus piece and the second string is tensorflow so when I add these two strings the resultant as I love tensorflow right so that was all about constants and then we have please holders so when it comes to please who will lose we don't have to provide an initial value and can specify it during the runtime so this allows us to build our computational graph without needing the data and this is how we can not create placeholders so TF dot placeholder is the syntax and inside that we just give the data type of the variable which we will substitute later on during execution so let's go to so let's head back to Jupiter and work with these placeholders so let me just type in placeholder for were here so let me create my first please hold up so I'll name the last E and the F dot please holder and this would be of integer type so TF dot and 32 now I will create another variable which would be B and the value of B would be actually a cross 2 so let me run this now I will run these two inside a session success dot run and I want to see the result of B so I'll put in B over here now since we know that a placeholder peeks in a value during runtime so this is one I'll feed the value to this placeholder a over here now to do that I would have to create something known as a free dictionary so feed that equals let me create a dictionary over here so it would be e and the value which I'll be giving to a would be let's say 5 now let me run this and let's see what do we get right so during the execution time I have assigned a value of Phi 2 e and when we multiply this Phi with 2 over here we get the value of B which is 10 right so all of this is happening during runtime because with the help of a placeholder we can assign it a value during execution now similarly let me go ahead and give a list of values over here so instead of 5 let me give the list of values 1 2 3 4 and 5 now I'll run this so here 1 cross 2 gives us 2 2 cross 2 gives us 4 3 cross 2 6 4 cross 2 is 8 and 5 cross 2 is 10 so you get an array of values during runtime now similarly let me also create our placeholder for Strings so while type string placeholder over here so now let me create this variable and name this as string name and this would be your placeholder so DF dot please holder and I am taking in a string so TF dot string right now let me create another string over here so the name of this string would be my name and let's say the value of the string as I am right now let me run this and I'll execute this inside a session says dot run and I own the result of my name when I added with respect to string name now let me also create a placeholder for Strings so I just type string placeholder and let me create the first place whole look so I'll name that as str1 name and since there's a placeholder D F dot placeholder and I will be giving in a string during execution time now I'll also create another string value over here and name that to be my name and the value of the string is I am and then I'll give us peace right so now I'll hit run and let me excute this inside a session now let me also create a placeholder for Strings so this will be string placeholder right so let me create this placeholder I'll name the sass STR name and since it's a placeholder I need to use TF dot placeholder and I will be assigning a string to this during execution time now I will also create another string which would be my name and this would be equal to PI and there's a space and I will be adding this with STR name so let me hit run right now I'll execute this inside a session so for that I'll type sess dot run and I want the result of my name so I'll just put in my Nemo here and then I'll try the feed dictionary feed direct equal to let me put in the dictionary over here and I will assign the values of STR name over here right so the values of STR name would be Sam Bob and Charlie now let me hit run and let's see what we get right so what we are basically doing is we are adding this with the placeholder value here and we are giving the values during the execution time so I am Sam I am Bob and I am charlie now these three values are coming from this feed dictionary during the runtime right so this is all about placeholders and finally we have variables so variable is just program element which allows us to add new trainable parameters to the graph and this is the syntax to create a variable TF dot variable and then we give the value or we initialize the value and then we specify the data type of that variable right so let's head back to Jupiter now I'll just type in variables over here right so let me create my first variable and the name of that variable would be fire 1 and we can create a variable like this TF dot variable so guys you need to keep in mind that this V over here is actually capital right so now after this I would have to assign a value to it so let's say I assign this variable a value of 20 and this is of integer type so TF dot and 32 I'll run this now another thing to be kept in mind us whenever we are declaring values in terms of low they have to be initialized so this is how we can initialize all of the variables so we have something known as global variable initializer and when we invoke this function all of the variables which we have declared would be initialized I'll hit run so now let me execute this inside a session sucess dot run off in it and I have initialized this variable over here now let me also go ahead and run that variable says dot run bar one right so we have the result of the r1 which is 20 now since this is a variable the value of a variable can be actually updated so let me go ahead and update the value of this so I will name this us updated bar 1 and the function would be T F dot assign and inside thus the first parameter would be the variable which I'd want to update and after that I need to give the value to which I'd want to update this so I want to make this value of twenty to twenty five right now I will run this so now this is actually a variable we can actually update the values so let me go ahead and do that so the name of the updated variable would be let's see updated who are one and the function for that would be DF dot a sign and the stakes in two parameters the first parameter would be the variable which were supposed to update and the second parameter would be the value to which we are updating it so I want to make this 20 to 25 I'll hit run right now let me run this inside a session says dot run and I need to pass in the variable which would be updated var one let me make it small V over here I'll run this right so initially the value of var one was 20 but we have updated it and made its value to be 25 now let's also go ahead and create a small linear model so let me just type in linear model over here and this is how our linear model would look like W X plus B where W and B would be variables and X would be a placeholder right right so let me start off by creating W so W is a variable so W would be equal to D F dot variable and I am initializing it with a value of let's say 10 and this is of integer type so this would be TF dot and 32 now similarly I will also assign the value for B so B is also a variable and its initial value would be Phi and this is also of integer type and finally we have X which as a placeholder so X is equal to TF dot placeholder and since please hold does not actually take an initial value it just takes a data type so the data type is again DF dot and 32 so I'll run this and now what I'll do is I will multiply W with X and add B to it so the equation would be W cross X plus B and I will store it in a variable and name that variable to be linear model right so this is W cross X plus B I'll run this now again if I have to execute this I have to run this inside a session and since also I have created two new variables I'd have to initialize them first so in it 1 equals D F dot global variables initializer I'll hit run so now I will create a session success dot run and I will execute in it one first so I have successfully initialized these two variables W and X now I can go ahead and run this linear model says dot run now I would want the result of linear model so linear model and then I will use the feed dictionary now inside this free dictionary I would have to assign a value to this placeholder X so X equal to let's say I give a list of values over here and the list of values would be 1 2 3 4 and 5 now I'll run this and let's see what we get right so the value of x is 1 we get 15 so this basically means that 10 into 1 plus 5 which is 15 now after that if the value of x is 2 so this would mean 10 into 2 plus 5 which is 25 similarly if X is 3 that would mean 10 into 3 which is 30 plus 5 is 35 right and same is the case for 4 & 5 so let's go back in 1980s there was this crazy guy called dr. Hilton he proposed that you know K we could mimic somehow the mind our human mind work and we can mimic the way it works so that we can also try to make a make a thing out of it so that to whatever our mind is doing artificially we can also do that thing so he proposed a paper and everybody said that little person you are doing something wrong this is not what is right and everybody you know didn't like the way things are going on but after 20 years almost in like to late 90s around 1989 he again came up to this paper now he brought in his paper but this time with a proof and he won a contest that is called imagenet in 2004 this time he came back and everybody had to listen so we were at in 2004 when he won the imagenet I have two people who I have like I've already told you no human it is imagenet is actually in a competition it is held by a joint dope joint division of Stanford Club Princeton University they these guys come up and they they did they downloaded a corpus of images now they have a very large collection of images around 1 million images around covering thousand categories they have you can just click on explore and you can see this tree around here so what you have to do is that you have to make a model out of it so that you can specify the images so people used to make you know hand-coded you know Cathy fired and they were working they were working but but at some stage you know they had to fail they call wake up now or let's say that fish you are trying to detect a fish but fish can not every ways the in the same manner in which you are building the hard coded thing right all it's said that you are trying to detect a cat and then you say okay top right if it is I a top left desire and then it's a squishing mouth and it's a cat right but now the cat face will not always be available to you in that particular fashion right it might be a top view a side view a bottom view any of you might come up and at that time people used to fail but in 2014 at 1:00 for when you know Joplin Hilton came weather with his paper his paper the competition he won was with 94% accuracy and that was never achieved never achieved and it surpassed the human accuracy also so that was though that was where you know when people started listening to him and he came out then he proposed slowly that okay that this is what I think that you know activation level network is nothing but it's time that we are trying to mimic what is happening in in our own mind right so if I say that what is the theory or what is the what joffrey said about the neural network is that he said that a that artificial neural network is a computational model that is inspired by the by the wave biological neural networks in our brain human information possesses right so I mean most of you know this thing that you know that our nervous system is is made up of exams and okay I will show you the photo that you know it is made up of exam then stove is to brain I show you the food and you might be able to you see this is actually how are you and human nervous system is basic most basic diagram and everybody of you might have seen this so how this works is that there is a exact replicate copy of this if I copy this you know and you can attach it back here again so these are the Exxon terminal that are connected to dendrites again and this is how it flows so let's imagine that your fingertips there you can have your dendrites and these are like small small particles again of your pure on your on your on your on your in connected to your brain so this is out there are like millions of these connected and they're connected I take this okay one more feature of Jupiter notebook that you already know that you can add a markdown language so what I do is I do escape em and then I'll write HTML inside it I write IMG SRC equal to right because all you can write you're in this you can add that this is right let's now look at it one by one slowly slowly okay so these are called dendrite Deaton right got the one who all except achill these are the one who are responsible for catching all the information and sending it forward to the nucleus now nucleus processes this thing and it sends to this Exxon's and again throws it out to the axon terminal now imagine again that this is this copy this is again replicated here back here so you can imagine that be turned right there connected to this exome right and then they're flowing flowing flowing flowing flowing currently now imagine let's say that you touched a very small or the very hot tub glass site the moment you touch the hot glass the dendrites at your drone fingertips they sense that something is very hot there so they send this information to your brain okay that is hot now imagine this dendrite is processing the information sending it to terminal terminally to dendrite and turn dendrite you terminal it goes on goes or goes under it goes to your brain now Billy now the brain feel that this is this is very hot you should remove your hand okay now this process is again flowing back to the back to the dendrite the original dendrite here let's say that this is the original dendrite here okay and the brain will say that this is really hot you should remove your remove the fingertips from the glass and now when I am saying that this information is going on believe with from your fingertip to your brain there are like millions of these androids you are connecting and flowing this information now again a process of reversal information flow is going on and this time your brain is sending a signal it remove your hand from the block and this is again going back to the fingertip and then you release your your hand from the glass but in real life when you touch the glass and when you remove it how much time do you take to remove this it's just some nanosecond right maybe less than your reflexes are very strong you just remove the hand out but imagine what is happening then you do this your brain is not your your your hand if not taking a decision of removing that you might think it okay my hand is taking the decision to remove that no the the hand is like a slave of brain so everybody part of art is like a slave of a plane like whatever we are doing it is send by the brain then function is send by the brain and then it's this is how your body works right so it's a slave of your brain and the brain sends information remove or attack whatever it is then it react but it's really phosphate similarly to this way only DAF resented okay this neural network this is how even I will also build on neural now I'll show you how a basic neuron movement here it is this is the image till I did basic knew the external network it's Esther neuron but this is artificial so Geoffrey's headed okay this is how also this is how I propose let's do the following let's say that your hand is not doing anything you're dead right so you know our system is not this basic neuron is not doing anything imagine that isn't doing nothing but you know it is reciprocating the information but how it is relative preventing the information by activating itself so when you cut a hot glass or something it activates itself and then only it's transmitting the the thing right now I imagine if this would have not activated it would have sent nothing right so this is also playing a very crucial route see brain is sitting somewhere like a million nerve neurons far right he doesn't know what is happening to your hand until and unless this neuron is sending the information so what information will the brain process if it doesn't have the information so these guys are very much responsible for sending the information and they send the information only when they're activated so this dendrite will become this this nucleus will become activated then only it will send up further information and there is also a basic kind of information processing is going on it doesn't mean that we detained writers are sending blank signals see I'll tell you more in depth inside see these dendrites they collect the information they generate they collect the information and they send it to the nucleus now nucleus think that should I process this and send it forward yes or no depending on that nuclear send it's further right now how is nucleus a nucleus ending is nucleus has a particular threshold value after processing it it says after if I am activated by that process value it will send it for them if not it will keep it to itself so this is also how a basic artificial neuron when I say artificial it means that the one that we are building right now in leap let me break this is also built now what is happening is that imagine this round thing that you are seeing is the nucleus this thing is a nucleus but this is a nucleus right and these are the dendrite food 1 this X 1 and X 2 this is the way of thinking how we are interpreting and this is the tail X in tail and it is again connected to again of neural net oh so it's a chain of multiple neurons going on now if we go mathematical what is happening inside this it is nothing but a linear combination of all the inputs with a bias I just like went to the very human biological way and I just send a sentence it's doing nothing what you're doing it and it collects in the input let's say that you have two input let's say that you have to input x1 and x2 these I have to input now to every input you will give some weight edge to it let's say our weight edge is here this weight edge is called W 1 for X 1 and W 2 for X 2 correct now what I said it is trying to build a linear regression right so what it does it multiplies the weight of the corresponding input to itself so W 1 X 1 plus W 2 X 2 plus it adds a bias to it now you will ask what is the bias I will say imagine that there is another input called X naught there is a another input corn X naught and the value of X naught is equal to 1 the value of X naught is equal to one and there is this weight associated to it that it's called W naught right so this bias it added to it and then you add that's it you plus all these values multi-purpose all these values and you give this output to something called by this is all our neuron does make sense in equations it ask very important could you build intuition at what is happening and so this is the most basic it's the smallest part of a neural network that you can pick and the smallest part of a neural network is doing this take it takes all the input that are coming to itself give them some weight it multiply the weight it to the corresponding input and then sum all these and then add a small bias to it also that's it oh how is this weighted and bias being introduced that took who is okay X 1 and X naught X 2 and X 1 I can say okay someone is giving to us the input that what is the temperature of the glass let's say it is coming from the outside environment X 1 and X 2 is coming out there but who is giving this W 1 and W 2 so okay so the answer is that you know you know artificial neural network environment at the very first instance at the very first instance they are randomly allocated to it so see you all guys have learned ml right so how does ml work ml says that I know the output and depending on the output I have to give some weight edge to these input so that they can give me the output now you know that the output will be Y you know that the output is going to be by right now you want to make some equation where Y is equal to X 1 plus X 2 okay and now this is not the correct answer right X 1 and X 2 will never give you bye-bye 1 so what you do you add some W 1 here some random W 1 and then you add random W to what it will do it will not give you the answer W 1 and W 2 will not give you down so you will do some Corrections again in W 1 W 2 and ultimately it will give you Y this is what is happening at the very first instance you will take W 1 and W 2's as random first L that does does the process is reverse yes it also reverses itself the the model will not only go from X went from X 1 X 2 to y but it can also come back from Y to X 1 X 2 and second let's say that he is asking a shaven does it process recursively recursively just imagine that there are like thousands of now I'll draw it again to show you just just a very basic one right now imagine what was going on is that this is this small that you saw right x1 x2 and it is giving someone now here it is again there is a x1 x2 and then this is giving me by now this Y and Y y1 will become a input for some other self then it is giving here so are you imagining how how will it look like please become this and this will become again and input for commander so these are like connected these are like throwing information from one to another one to another one to another right but this is how it works this is how small a network looks like and this this is the if I write down theory it says that the basic unit of computation and user network is a neuron or often called as a node it and came right in very right this is called a node or you can say inter unit also so it receives input from some other node from an external sources or external so that I was saying right all right he the x1 and x2 might be a input from another neuron or it might be given to you directly from the external environment right computes the object he is input each input has associated grade to it which is assigned on the basis of its relative importance to other the node applies a function f to the weighted sum of this input right we've been having the students data so whatever the marks based upon their marks so we have to decide the grade so we have to exist whether the student is passed out or pain so string the marks based upon the grade this is a very hard coded problem I think but let me take an example that you are working on a machine learning problem where you are trying to find out the loan default rate right in Bank you have multiple people who take the loan now the bank wants to know how will this person repay the loan or not right now you already have a historical data with you the bank will give the data to you okay let's say that this is the data with you so in that data you already know that if dishes the feature of a person this is the credit history of the person based on that did say they will also give that okay he paid he didn't paid he paid - it didn't paid right and based on that you had just your model right so there also you have action by Withey you will have your x and y's with you at that particular instance also okay so everybody gets this F dysfunction thing right that's how this function is working this function f is called f fine - the F function that you see this is called a fine a fine thing and it's doing nothing right you saw that this is it doing a linear thing here just multiplying the weights but corresponding X on xtube and you might now consider X 1 X 2 as the important feature so the more important x1 is the of the more higher the value of W 1 bin P right the more lower the value of W 2 is it means it doesn't make much sense to why so everything that we will now talk will be ready respect to Y so the X 2 is net not making much sense to Y so the value of W 2 will be very less let's say Y is dependent on X 1 the value of W 1 will go more higher right and then your bias you already have this is a static thing right because the X not associated to it is equal to 1 right let's now go back at what further happens here now you get the Y out of here right now this thing is a little you have got to answer and this might be greater than 1 or less than 1 also right so there is no measure of this answer it might happen that W 1 W 2 is very small let it up low n W are very small then the value of y which might be almost equal to 0 right or let's say that W 1 value is very high but very high so the value of y might also get very high right so now this might bring some bias you know this might bring some ambiguity to our model this Y might be sometimes you know next to infinite it might be equal to infinite or it might also be equal to 0 right depending on the variable and we don't want that we want our Y to be in some particular range right we want our Y to be in some bonded matrix then comes next guys whose names are activation function activation functions now we got to know at how this y is made right how this Y is the ultimately achieved now to adjust the value or squeeze the value of y we need some other other things right we need we need something that can control the value of y and these are called activation functions now what activation functions does is that they will take the value of y and they will squeeze the value of y in some certain range what they will do they will squeeze the value of y in from certain range so we have multiple you know let's say we have a step function we have a 10h function Shivam Sanjay I said can you please come again about this activation function not getting it properly you so let's come back what this activation function is so you understand that the value of y how it is calculated right it is now totally dependent on w1 and w2 if the value of w1 and w2 is very less I mean zero point zero zero zero zero or let's say 10 to the power minus 23 then the then the y is going to be very less right next to zero or let's say that the value of W 1 and W 2 is like 10 to the power positive 48 plus 48 so this might be very high the value of Y might be very high there might be no standard to control this value of pyrite mathematically there is no this value can go from negative infinity to positive infinity right and this is not a very good range to to get about a particular output right so so what we do is we introduce something that is called activation functions now what does activation function we'll do with activation function we'll make sure that your output that you have got why it should be in some particular range what it should be it should be in some particular range now how do you decide that who is going to win what range and how is is that ranger called see the activation function if I ask you mathematic in the mathematical purpose of introducing an activation function is you squeeze the value of Y between some variable called 0 to 1 right majority tank to the saw the value so the purpose of activation function is to bow into the why in some range right and this range might be -1 say 1 0 to 1 or sometimes 0 to infinity it may be different different things I was at this thing that mathematically the value is to be squeezed between zero to one but if I talk to you about statistically why I am doing this is now imagine till here in this particular range your model is a linear model everybody agrees with me this is a linear model nothing lawn janilla have been done till now right all the things that we have done till this step in the circle everything is linear there is no nonlinear thing right now linear things are are a little bad at classifying thing if there are more than three glasses right if I ask you can you if let's say there are three gases one is like crosses Hey one is said it goes right and one is some triangles now can you make a single line and can divide these three data no right if it would have been two classes you might have you know drawn a single line and you might have divided at a tie into the two classes right but if it is a three clouds later how will you divide this you have to ultimately make a triangle kind of a shape or this kind of a shape right then only you can divide the divide this this thing right so I will show you a boat also let's say let me give you an example then I will show you all this world okay can any one of you let me hide it you now this is this this thing in front of you everybody can see a data there is this red dots and there is a blue triangle in the screen right everybody can see this now can you make a linear model that can separate these taking this can only do enough to dimension right but if I fail you but okay see your magic anybody wants to try this thing how will you do no okay whatever you do it so what I did was I introduced a third dimension and now I could really separate by introducing a hyperplane here and then I can say it okay something that is above this hyperplane is read something else below the hyperplane blue make sense to everyone right so this is why we do activation function to make sure that we want to classify things more better to make things more clear we do activation function so mathematically why we are doing activation functions because we want to squeeze the value of why somewhere if but if I talk statistically what we are doing we are introducing non-linearity in a linear model because linear models are very much not capable of classifying things in when the number of classes increases so now we saw that as soon as we increase the linearity if we increase the dimension in the data if we introduce nonlinear in the data what it is doing it increases the chances of making a linear separable model right you right so if I say in three three times first mathematically squeezing the Y second if I say graphical thing why they're doing this because we want to introduce a new dimension and statistical is saying why we're doing this because we want to introduce non-linearity in fact now you might be asking what these function is - it's a very good question we have deviation functions gross see this is how our Sigma it looks like tan hats look like really looks like nc everybody is bounding so see it might bounding between 0 to 1 10 inches bounding between minus 1 say 1 Ray Lewis binding from 0 to infinite so it lets go everything that is going above yes we have step function although step function says that even go sum is something can we only have two things either 0 or 1 if something is negative it goes 0 if something is positive it goes 1 so step function Sigma in 10 its value so there are multiple things these are doing the same thing that we want to do for activation function but I'll you go you can understand that if something is negative it says it's a zero and if something is positive it lets go whatever the value of this is right right - maybe a builds in here this is what brings in Sigma it's this candidate if any relevance this like oh this is the maximum of 0 to 1 that that ever value it will take but this is what activation function this let's say that you have this thing called y1 and y2 right and now the value of y1 and y2 is very large very large right so this Y 3 will become ultimately also very large and this Y 4 will become also very large and we know this is not the exact thing we always expect our output to be in the form of 0 to 1 f it is a classification problem and if it is a aggression problem then it is it will always be in some range from 0 to 1 right you all agree with me right in machine learning also whenever you did a classification problem you one-hot encoded the problem so you never said that the output is going to be a CAT cat or Diggity Dog right you always said that ok if zero comes this is cat 1 comes into the dog right we did the same thing right a machine-learning also or if it was a regression problem then you always regress our value from 0 to 1 with a multiplicative factor right so this is what is happening here also the y3 and y4 well you can never go up beyond a limit to control that limit we have to squeeze the value and see when I'm saying squeezing the value that is in mathematical form on but if you see in statistically what we are doing is that we have to introduced non-linearity into the data right because linear data's are not linear equations are not good in separating nonlinear data right right this becomes separable you see now it is very much separable just by a simple hyperplane okay so coming back to a feed-forward neural network so see this was the input x1 this was input X - and now what a feed-forward neural network expects is that a hidden node should be connected to all the inputs what does it expects it should be connected to all the inputs that are in the previous layer so this is led this is called layer input layer wherever your hidden neurons this is called hidden lair and your outputs are here this is called output now when you are building this feed forward neural network your responsibility is just one that whichever neuron you're standing let's say we are standing on this particular neuron that's a distance needle this new one now the only thing that this okay this is northern Iran in 30 no sorry let's say we are standing on this neuron not the responsibility is justice that if you go to the previous layer the previous layer is the input layer all the neurons in this layer should be connected to this he didn't know he didn't move right similarly output 2 also expects the same you go to the previous layer so this is the output layer what is the previous layer fill in layer so all the neurons in the hidden layer should be connected to the output node to make sense what if I make it more easily let's say that whichever neuron you're standing just go to the previous layer and all the all the neurons in that previous layer should be connected to this particular neuron right what is this layer signifying here correct ready reflection so what you are doing it you have stacked all your new Don's one by one in some format right so all your input neuron so see whatever you made in this above statement here in this above thing here so this X 1 and X 2 this is X 1 X 2 this might be X 3 X 4 X 5 X 10 right so these X 1 and X 2 s are what inputs right so what you do is you collect all D and you just put it inside a layer here this becomes a hidden layer now you will not only have one neuron you will have multiple neurons right also what is happening is that you so you what you have done is rather than having just one neuron what you have done is you have made multi neuron system this is a feed-forward multi neuron model here so what you are seeing in the in the previous photo letters and image that I showed you that was just a multi input single neuron but now you have multiple neuron so what you do is you stack these neurons in this layer then you have you gonna so this is just a example what you can do is you can have a image like this one kid near okay so now if I if I if you see it around here so what is happening if these are your inputs X 1 X 2 X 3 X 4 right this is your input layer this is your hidden layer one where all your F fines and activations are happening right this is your lair to obtain all the F finds and you know your activations are happening and this is your output layer hidden layer hl2 this is Excel one and this is input layer so what is happening is that let's say this is called neuron one dead urdead y1 just month so let's say this is N 1 this is called neuron 1 now what is happening is that this neuron 1 is connected to X 1 also X 2 also X 3 also and some bias also so what is happening here this F fine will be happening and what will be happening it will be here that W 1 X 1 plus W 2 X 2 plus W 3 X 3 plus plus some bias B will give some output called Y and here you will have B having a very small activation function also residing right similarly for this n 2 also this n 2 will is connected to all the inputs right is n 2 is connected to all the inputs this n 2 is connected now it is connected to this to this this and this now n 2 n 2 will also do some affine and then bring some activation small activation function here and it will also give some output called by 2 similarly Y 3 will also behave now thus by 1 by 2 y 3 will become input for these new let's say we call this neuron as n foe now this info is connected to this this and this similarly again a fine and let's say it gives the output called oh one oh one is here similarly Oh two is here now this Oh 102 is all will also become input for this let's say we call it 201 by there right this is over Michael let me remove misogynist we were here that terrorist is some output called by four and there is some output called Wi-Fi right now what is happening why y4 and y5 will become output for this owner on and they will give some output oh one and for two similarly here and then you will have output right this is how a multi-layer neural network works how will we know that how many layers correct this is one of the very good questions that you know first of all how many layers do you want this is the layers that you have l1 and l2 now you might have another question that okay in in one layer how many neurons can effect so there are currently four I can fit five six ten also right I can fit 10 neurons although so who is going to tell me that how many neurons and how many layers this is what exactly what data finds you're going to do here I mean everything is built for you just this this neuron thing and this whole thing is every this is already build for you the only thing you have to do is that you have to come up with the optimal number of neurons and optimal number of layers for each neural network and now who is going to decide this very good question this is you who is going to decide this and how okay so first answer is that hit enter and that everybody will tell you and I am also telling the platter first step proaches hit enter let's say that you are doing a very new problem I mean you don't have any idea you are just doing so one thing is that you will go with the hidden trail model second is that as soon as you become a little bit expert you know now you start learning okay this is how my model is reacting to this particular neural network layer then you see the model capacity okay so let's say it so see this is a very good question but this is also a advanced question now see when this let's say this is called neuron one when this new Don 1 is connected to X 1 X 2 X 3 there is a weight called W 1 w2 on this layer and w3 here right so but when this n2 is connected this is not W 1 W 2 W there is w 4 WL w 6 so when this X 1 is connected to this n 2 here on this line there is another weight that is W 4 so imagine how many ways are there for 3 inputs 3 neurons there will be nine weights right now there are nine weights now imagine here again that you have three inputs and two outputs 3.6 then here are 6 so 9 plus 6 then again here two to the fourth so you will have two plus four so nine plus six plus four is 19 so 19 weights are there engine you will have three biases here right so 19 verse 3 is what 22 so for the small neural network you have 22 values that you have to predict right because Y 1 W 1 W 2 we have taken as as what as random right and biases are also random so you have to adjust them in a very optimal way so that you can get the output as Y so now imagine that you have 22 things that you have a pivot that you have to Train here 21 22 things that you have to or that you have to predict not predict but you have to optimize right and now let's say that you have a data only for like 10 rolls now does this justify that you will have 22 things to adjacent us 10 rolls of data so this is called capacity building so you what you have to do is that you have to check your model if your model is even capable if the data is even capable to adjust these values right let's say that you are just doing a simple addition right if you give you two numbers your model has to give some output right now let's say that you have thousand examples for it but now your building or likes a thousand layer and thousand you don but laird thousand neurons and here then you have thousand layers what does the model required a smooth operator no the data will be lost in some third third or fourth layer of me right and see what I just said in just last two three sentences this will come slowly slowly slowly with the experience and you want so what you read also whenever see whenever a new problem comes to me hi first of all read the research paper through it see because we are not research that may be frank with you we are not researchers who are going to make things we are deep learning engineer say right we have to implement the engineer and we have to solve a business problem right so what you do is first of all you read some papers or cut right then you get the intuition of how do you build a neural network around it then I do some hidden trial errors okay this is how better than this some changes and then slowly understand okay when I change this thing this thing is affected more then you reduce and more then you have some prior noise also like that thread that I told you keep capacity that what is the capacity of the model let's say that you have a very large data but you are just building one layer then the data is under fitted right all that so that you have very less data but you have made a very big neural network photos this is called overfitting right you have a very over number of neurons for your model to be Oh predictable right so slowly slowly you'll Bing player you know you will have intuition then this is I will tell you you know with your robe when you will be doing your first problem in the neural network you will understand slowly see you might be thinking that you know I am not taking to your practical right I am not telling you about anything well example but this theory is really important I might directly jump to the code also but that will make no sense to you after two or three weeks right because code anybody can write until unless you don't know okay what will happen in the next two after two or three months when you will be making your own neural net you will fail that okay which activation is once into use then this thing will happen okay so see this is what he is talking about to all the people this is what he's talking about that Tara do you want to add more layer so currently there is only two hidden layers you want to add more add more add more right if you want to increase per layer you want to increase the neuron you might go like this also okay you might click we reduce it to like let's say six this early reduce it to four listen to an assessment right this is how your noodle network will look like these are the two inputs now every input is connected to the every neuron in the first hidden layer right then second and third and fourth similarly and this is how the data is flowing inside you can see now slowly you then we put every X into each neuron does every neurons give same Y very good question but no C came if we if we if we come here it would have been the same if they would have the same W right so if X 1 is connected to N 1 here at this lip it has W 1 1 right but when X 1 is connected to n 2 here it has something called W 1 2 right so there are different weights so it is coming from x1 but but because it is connected to different neurons different input neurons they will have different waves also understandable so to other people what km is asking is that okay that you are doing a fine I understand that you are doing a fine but let's say that let's go back to a fine equation we call the value of x1 and x2 is constant the output will always be constant right but no they caused by 1 is made up of this this equation and this equation is also driven by W 1 and W 2 and the W 1 W 2 values are different the value of y will also be different in each case right make sense right and then we were asking about a real-world problem this is how a new dawn will work okay I have this also don't worry so you mentioned the overfitting and underfitting neurons like mom yes so how do we come to know that based on the Y or again a very good question so I took this to word called underfitting and overfitting this is how it actually work is intuitive gif okay okay so what salacious asking is that no that toe I took two words in overfitting and underfitting and how do we address this problem so what we do is that we divide our data into two parts and I think you may have also done this in ml also right training data and validation data right and not to does three perfect training validation and testing right so what you do is whenever you are training your model you just make a slice another slice for data and this is called validation data and you will never for this data to the model right so no you are never going to train your model on this particular data so what we're doing is you will never show the data go to the model the model will never come to know about it so it's it will never learn about it also now now once your model is made what you will do is you will test it on this data what you will do you will test this on this data and then the model will say okay this is how it looks like okay this is what the predictions are and then you'll see the then you'll see the predictions and then you'll compare the predictions okay okay this is not what I wanted so the model is underfitting on overfitting now how will you do under fitting and all fitting you will do a confusion matrix and then you'll see that if the model is just giving out you only one classes let's say that your data had ten classes and if the model is just pushing towards one class then this is overfitting the remodel is over a fitted or a very lot level okay then we have another question from came he says that it means now this dog image when we expand this image mathematically this will come out to be three things one is called X 1 X 2 and X 3 right now what we have to do is this X 1 X 2 X 3 is done towards from our end now this X 1 X 2 X 3 is connected to this neuron bun okay now X 1 is connected to n when this is called W 1 1 but when it is connected to n 2 it will have some another random bit called W 1 to W 1 to make sense so number of neurons you can increase as much as you want as much you can have n 3 and 4 and 5 and 7 7 doesn't matter but you have to make sure that this W 1 1 is not the same weight for all N 1 and 23 right good thank you so inside the matrix you can see there are multiple values the value ranges from 0 2 to 5 5 to 5 5 is 4 like utter black and zeroes for a white right the value ranges from 0 2 to 5 you can see the more black it is the more the valuated it has right so what is happening is that these are the multiple inputs that you will have inside your images multiple inputs and all these inputs will go in here so in this particular example you are seeing 3 but this time you will have more so these all these all the zeros in first all the values in second third fourth fifth row till the 28th throw you will have all the numbers inside this input layer stacked one by one so ultimately you will have 28 cross 28 is equal to 784 so you will have 784 inputs normally for this small image and this is you your guys we're all asking about me how this will work in the real world so ZL well may not say that you have a image of 500 plus 500 so that becomes around like 25 W double zero that means or 2.5 million inputs you will have four single image right and let's say that you have 10,000 images so this becomes x 10,000 right so these are number of inputs that will go inside one by one by one by one by one so one image will go one time to image will go to second time third time and then days on this the the output is the model is going to do some calculations in the hidden layer and then it will predict the output and it will give you some output it by ok so now we turn up and we turn the pages and we'll move to multi-layer perceptron but now we might have multi-layer perceptron architecture now a multi-layer perceptron I get it just simply means that you might have multiple hidden layers okay so apart from having an input layer and output layer you might have hidden layers in between of them also whereas a single perceptron was only a linear function a multi-layer perceptron will have multiple linear and nonlinear functions a combination of them I'm sure the tips you if I was not able to continue to do today also you will not be able to continue okay don't worry I'll push the notebook in there and then okay so I think you already thought it but I have an image of multi-layer perceptron also hey this is how multi-layer perceptron looks like I just minimize my screen a little bit so you can see it is know again just to memorize things up now this is X 1 this is what your input X 1 X 2 these are your two inputs right now you have a single hidden layer right now here you have this one neuron here now you already know that what is the calculation going on inside there is the affine function f I in function what it does it takes input from all the behind layer neurons right so if I say it takes input from all the behind there you know any a if this is a hidden layer h1 then it means that if you take inputs from all the all the neurons inside this input layer thing so this is external then this is connected this is X 2 this is also connected now whenever a new one is connected to another neuron it is connected through some weight gage corresponding to it right that's it it will W 1 this is w 2 right and then you might also have a X naught now the X naught is always equal to 1 so you might say this is just a bias bias as B now this B is like constant to this new don also and to this one ok so now now this is X 1 and X 2 for this when they are connected to this neuron the one it is downwards and this news on that are connected again to some weight it but this weight it might or might not be equal to the W 1 that is that is the one I described here the one described here this is this is never going to be equal to this one right so these are different different weights different but you might to make it more simple as you might think that whenever or whenever there is a string connecting to neurons it is connected through a village and this village is different for each one right this way edge is different for each one you might also see the equation here and down here the blue not in plus one plus w1 x1 w2 x2 now you know that W naught is equal to this one so we have replaced W naught is equal to 1 so X naught is equal to 1 so we have replaced X naught is 1 and the blue not W not in itself depends on W naught or what people like to say it's called as notation called B and this is called the bias of the neural network ok bias and death abuse are called of weightages right and then you do again a summation out here and just saying we do the final output and then you give the output right in right you also remember that after every neuron there is also activation function which is sitting here here and here they are also present on the on the on the outer section also outer section also this in this they are present here also so they and then you read that there are different different types of activation functions activation functions from sig from step function to sigmoid then to softmax we have different different I will slowly go through each of them and then you'll understand that how they how they interacted how you might be also changing them according to your needs visible now so now once you are done with this perceptron layer now okay so this is called a feed-forward neural network just one thing feedserver leavin it but when you when you come from here this neuron you go till here then there some calculations goes on then you come to here which is taking collections from days and just neuron and then you give the output right now let's say the output comes out to be some by one letter the actual output but let's save by right now there is the error associated to it so you say that you have a error and what is the error Y minus y1 this is their right why was the actual answer that should come right but you are saying answer is y1 then then Y minus y1 is actually the error what it is it is actually the error this is actually the error by - I won by a model you you say - a model that model you have done some kind of mistakes now if you go back in inside your session if you go back in back to your session you might see some errors inside it and that error might be equal to Y minus y1 so go and do the changes now if you see our function that we have done till now the result through which we have come till now here at this point y1 it is all how how much you know now dynamic things you have taken IRI mean non static things you have taken see x1 is always going to be static right this is this no one is going to change this x1 x2 because these are given by the data it says right to the data he'll never change itself it is you who has to change right and then what is the thing that we have taken as EOKA these are the values of W 1 and W W 1 W 2 and W naught these are the when similarly DW here I mean all the W that are here all the W there here these values or something that you have assumed right so now what the model say the model say that the value that you have assumed the blue and w 1 w w LW n these values might have some kind of a correction because the current values of W are presenta are giving some error that if Y minus y1 so to reduce this error you go back and check your values of W and switch the values of W to such an extent so that they minimize this error right whatever I fed just now statistically right make sense to everyone right what I just said here okay now this thing is called back propagation now this thing is called back propagation now back propagation if I like take the copy of can we try different activation functions - yes you might try whatever activation functions you want to try but there are some ok so oh man an out this question that you know that how do can we use different activation functions also yes they depends on the problem that what functions do you want to choose so I'll tell you my particular example so I was working on this library that is called D or D Phi here what we are trying to do that we are trying to restore colors into images that don't have color so I mean if you if you talk about images that are like from that are before 1924 so we have some German German arty art artifacts those are before 1924 and those are totally black and white images to us went so what we have to do is that we have to fill images inside it now filling images inside it is I'll tell you at the complex process but there is a form up for images that comes out I did called the LAV images so if you if someone of you who has an interest in photography or or if you understand what images are then you might have heard this term called la la be images to convert now the range of l-e-d images is from minus 128 to 128 so here my task so if generally if I if you see the activation function if you remember from the previous class all our activation function from ranging something from 0 to 1 or or like minus 1 to plus 1 right now I wanted something that has to go from minus 128 till plus 128 right and how will these things to go so accordingly you can change your activation function and you make your own function out of it right make sense yeah I forgot you in last lecture you said whenever you're trying to bound your values in a certain point that knew you had different activities to an activation function I just died just move out I this madman okay got a correct correct totally correct so this is all about the feed-forward now if you repeat forward if I just say in one line feed-forward is going from input-output now if you want to come back from output to input to make correction that is called back propagation now back propagation is one of the most used or unsafe okay now visible yeah now this is one of the most not used but this is one of the most obvious things in backpropagation this is in AI and why this is famous because this is the main backbone for yeah this is how a a learns from learn written right so people to that ai is learning from it don't know it's not learning from itself it's learning from the others whatever error it is making it goes back and it checks to see a human tendency of error taking is worth let's say that you are doing some calculation then you do some error what do you do you go back again to the calculations and you check okay I did this mistake and then you correct it right this is also the way how machine is also understanding so what machine does my machine does some calculation it calculates that there is some error then based on the assumptions that it took or based on the assumptions it took it thinks okay this the other things I assume and this is now the error so what it does it goes back and say okay I will change my assumptions I will tweak my assumptions and then I'll see what is the error again this is a hydrating process so again though let's say that in by provision what will what will it do it will try to change the values of W 1 and W 2 right it will try to change these values of W 1 and W 2 and then it will again go in the forward or forward direction and it will again see okay again there will be some error but this time the error will be less as corresponding it up egregious error right now again it will go back then come forward again go back forward go forward invert forward backward forward backward this will happen multiple times and at one point the error will saturate the error will look like this so error the error occurred graph will look almost like this here error is on the y-axis right and this is whatever you say time on the number of epochs or steps let's say as of now steps means that how many times you have gone a forward backward forward backward so you can easily see here that dot the error was very high at this particular point but then it slowly slowly slowly decreased it decreased still here but now it's almost saturated like almost saturating the trend at one tenseness it will be like parallel to the x-axis it will become parallel to the x-axis this is how it look right even so how this program will go back and forth this it's API on we have to manually do this every time we run the program each time and or run the program in the reverse order once I can you like it okay so so so the point is that first of all we do it on in our hands so we will do it manually first okay but good news for you is that no you don't have to do it manually all these things are all these things are like handled by the the framework itself he times we have to go back and forth for the the minimizer this you have to decide you will see the error graph continuously and then you'll decide okay now I want to stop or okay now I still I want to go for the this is up to your hands okay make sense right I mean see look what if if they can even tell this that you know if they if they can even tell it is that how to stop then I think the problem is done I mean then we will be doing nothing right we just open the and we just say this is the core read the code and do it it then read the problem and do it it tells right no the problem is that it will do everything all the calculations for you but but the mind where to stop the problem where to go ahead with the problem is still in your hands you have to still make sure that where do you want to stop the things or where do you want to again a bit of further out of sight makes sense right okay let's move forward I have fun more clearly from you let me paste it okay so don't read this I will tell you this is just for your your purpose so that it in the weekend when you be days when you go back and you want to do not have to take the visit the whole video don't roll do the video you can just read it through the notebooks only right that's why I am just copy pasting it theory here also so so what happens is I have already told you that you know all the edges that you were hated that you see here that all these lines that you sorry all these lines that you see here these lines these are called edges all the connecting things are colleges now for every corresponding edge you have a weighted rate for every corresponding else you have a weighted and these weight edges are by default randomly assigned to you right now they're randomly assigned a sign at the very first point when the problem start at that particular instance these are like a randomly assign to you now for every input in the training later the enn is activated and I Toton of that this is what I told you also right that once we'll do all the calculations we'll do the first feed-forward neural network we'll check what the output and then the output it has compared with the desired output that we already know and I already know this is also you might have heard this term again and again this is called ground soup right so see a deep learning may you might have heard this is not equal to what traditional technology is in traditional programming you have been given the input and people ask the output but here it's not like that here they will give you the input also in the output also and then you have to define some way to convert this input into output right this is how machine learning works right because they give you input also and then they give you output also so like say let's first on the first example if someone tells you that you have to do some image classification problem then this is not like they'll give you at the very first instant this is the image now tell you what it did no they'll give you a training it a set where they will give it out but also with the image they will also tell you that ok 9 is a 10 year 5 over 10 years I will written it so you have the ground roots with you also now what you will do you will compare your result with the ground output and then the error is propagated back to the previous layer if you read the line typically where the error is noted and the weights are adjusted accordingly this process is repeated till the output error is a predetermined threshold value or the way you think that it is being saturated right this is what I also told you now if I tell you that how it goes ok let's see one by one mathematically I will show you one by one also it's HBR Matt's okay just a little attention and then we'll be here let's say this thing is called okay or otherwise what I didn't do is I will just pick up the smaller problem and then I will show you also noticed okay just this is I can't let me search for the image that I have you okay sorry I cannot find it otherwise but okay let's go let's see that error is equal to I will write it down error is equal to let us save by actual minus y predicted right net is random error is equal to this e and I don't okay now if you see that how is this y1 calculated then let's say that we we had an input called we had an input we had an input called I 1 and let us say I to write down and let's say that we only have this one okay let's make it more simple right I will take just this image that will that will make you understand how things were correct because if I write this is the example right so now we have input input as X 1 and X 2 right and we have a output called by let's say that the actual what a desired desired output be why a right this is what we require from him this is how we have and the value of y is equal to the value of y is equal to w1 into w1 into x1 plus w2 into x2 right everybody agrees to this right it houses the a how is this calculated right now the kay and let's then define other errr error is equal to other a is equal to Y minus y a right or whatever you want to say Y a - why doesn't make much problem to us right this is how this is working so now if I say E is equal to Y minus y a then I can say e is equal to instead of Y I can write W 1 into X 1 plus W 2 into X 2 - why a right now if you open up this doesn't make much sense but let me write it down w1 into x1 plus top loop 2 into X 2 - ye right now sorry okay now the error is this right now what do you want to do now the final equation of error is in front of you right if I write it down the final equation is in front of you can now do you have the final equation in front of you now you have to see that how to minimize this equation right how will you minimize this equation this is the equation of error in front of you now what you can do is now you might be thinking that okay this edeka equation is just a simple linear equation I can solve it linearly now I imagine that instead of X 1 X 2 you have thousand X 10 right now this equation is not a 2d equation now this is a thousand dimension equation now you cannot just simply do things and just check it around right you cannot just do it like that you have to you cannot just plot a graph and you can look into that right this is a little more complex than we thought of it right if I show you how the graph might look like right the graph will look like don't just read this just for the time being forget this thing just don't read these things okay so what I can do is I said okay this is your error okay question well let's say error I can also say J of W you might see this term in logic a of W as error okay you should not do this this is not making it good okay then you plot this thing here now what you can see you see X 1 is X 1 is what X 1 is X 1 is a constant right X 2 is a constant and Y is a constant you all agree right that X 1 I will put a star and say X 1 comma X 2 and Y a are constant right you cannot change them these are constants right they make no effect to e the one that might change e is w1 or w2 okay for effect none of you told me but we missed a bias also bias be you miss this bias be here right say to see what this P is I mean it's not going to make some effect or but still just for the courtesy let's what is right to anyone any doubters still here please ask me questions right so now we can see that okay what is happening is that E is only dependent on W right E is only the error is only dependent on W 1 and W 2 and B no no none of the other people are changing this E so what we'll do is we'll try to plot this error with respect to the weight and we will see this is how the graph looks like and and what do we want to do with the error if error is by a - why - why a what is our aim our aim is to make why a is equal to equal to why a as equal to equal to Y right this is what our aim is right if y a is equal to equal to then we are saying that the error should be equal to zero right error should be equal to zero or tending towards zero right but if I say mathematically zero is I mean if I say statistically this error is never going to be a zero right this is never ever tending to zero so what our main focuses our main focuses that if e is here our main motto is to minimize the current error whatever the error is we have to minimize this error now the value of this error can be found here right you can simply see the graph and say okay this is the value of e just take the value of W and plot them right but as I told you now these errors are not just two just two inputs we have like thousand inputs come we might have thousand of w1 and then one B also so we cannot just plot it so it will become a thousand in one dimension graph you cannot just plot it so now we have to think mathematically that how can we how can we minimize how can we minimize duck error E now minimizing the error E but we also have the like the equation of a also we have the equation don't think that we have the graph just think that we have an equation of equation of e also right this is above write an equation of is somewhat like this I copied this from here this is somewhat like this right this is somewhat like this is what the equation of e is and this is how we have to minimize now in mathematics we have learned in inferential mathematics we have learned that if you want to minimize any curve if you want to minimize any polynomial curve of the order greater than 2 what is the process anyway tell me if you want to find out if you want to if you want to know that where is first everything I believe I am 30 what take the first derivative take the phone okay okay correct they take the further if that's it's very much correct so as amar said that you have to take so see what the first derivative tells you the first derivative tells you that wherever there is a change in the slope of the graph if I talk about what we learned in our higher education about differentiation what the differentiation tells you that point coordinate where the graph changes its slope from positive to negative or negative to positive so if you take the first derivative it will tell you this play see it was in negative slope that it came in positive slope now again here when it was in positive slope that had came into negative slope again positive negative positive negative positive negative or that is negative so it will tell you all the places where it has changed so change the slope from positive negative or negative to positive I suppose it's possible right yeah very like for global minimum and maximum you have to take the second derivative and second derivative correct correct correct correct correct so this was how how we talk about inferential mathematics but if you go in descriptive mathematics it says that what is a derivative of a function a derivative of a function when you have a tangent that just parallel to the x axis right when it is parallel to the x axis so how of where will be the tangent parallel to access it whenever there is a slope that is equal to zero so if the slope was from positive to going to negative or negative to positive so both the mathematical percepts tells you that there is a change in the slope and not right so similarly we can know that where are the positions where are the places where are where we can get a change in the slope now as Omar very very uh-oh he suggested that we will get all the points where it is maximum and minimum also and we just won the point where it is minimum I mean these points we are interested in the one right or by doctored by agree either this right this right this is right but now again there is a problem is that jeans are very higher error this this one that I am just marked as e1 this is a very higher level of error right we do not want a higher level of error we want a lower level this one we want the e3 we want a three to be our error so what we do we take double differentiation we first take differentiation then we take double differentiation now double differentiation then Omar can tell me what is the condition if it is less than equal to or equal to less than two Oh Carol now with the first derivative you can know that where the slope of change but if you want to know that which is what it is it is if it is a maxima or if it is a minima how do you know it you know by doing a double differentiation of that thing and then checking its dimension so if the double derivative if the double derivative of any of your function is greater than zero if the double derivation of a function is greater than 0 then it is a minima right and if the double derivation of any of your function is less than zero then that is maxima these are the conditions then you will check accordingly and eugen you will go for minima now again the problem is that ok now you will reject these these points you will reject all you okay so you might discard this you might discard this you might discard this you might also discard this right so you will you will you will remove all these blue points now you will only have the green points with you right now you want this et to be there right now we don't have any mechanism through which we know that we have reached global minima or not this is the dilemma in deep learning and for slowly what we do is now understand this mechanism this is called up this is this whole thing that tea that I am showing right now is called gradient descent or Gd right you might have also heard this s SGD stochastic gradient descent when you do a gradient descent in batches it is called stochastic gradient descent okay I'll take a pause here and to anyone who has not understood this thing please ask I know I might have taken a very very quickly this up and and again one more point to to other people also who are not math savvy see this is this Matz is just for your understanding you won't be doing all this math in your program no you won't be doing all this in your deep learning courses you but you will be just striking the netcode you won't be doing all this mathematics this is already done by the framework so don't worry about this but but again my point is please try to get this inside because this is necessary right this is necessary if you do not understand what is your frame what doing then there is no sense of a framework doing something right so just Pisa and if any doubt please ask please ask do not hesitate because I know it took me three days to understand this thing so I was okay so I am assuming that everybody understands still here okay so what I will do is I will take the first derivative of de sorry de now you know that E is equal to this let's write it down so now you have seen this one thing that the now the now the now the e is dependent on either w1 or w2 so what you will do is you will take the derivative with respect to with respect to tw1 so what will happen it will say x1 and then everything is equal to zero right because these are all constant director wrong what I am doing is I am trying to take a derivative with respect to w1 what will happen one into one totally correct plus w2 is again a constant plus constant plus constant makes no sense right there and if you want D a pi day w2 then what will happen it will give you X 2 right 2 into 1 again - okay it's a chat just a second again see someone is asking me this question that you know that is this really required no this is not required this is just for your reference don't worry you know my engineer a constant term to into the derivative oh yes yes yes so we need a a we add a constant term but here we we know that that constant term is are equal to zero so yes he is right we have some C 1 C 2 but these are constant as of now for us they don't bring much value to I put this picture now so see this was just for one right if you go from here till here this was just for adjusting the new values now what will be the adjusting values of w1 and w2 so now we already have the older values of W and W so the new value of W 1 W 2 will be te W is equal to W 1 minus D e by d w1 two will be equal to w2 minus d e by TW 2 right this will be equal to this and but there is a special term that is called alpha that is associated here it is multiplied I tell you what l5 here I do not know how to type l find like latex okay you okay so now you might ask that okay and addresses the new value of w1 and w2 right so now what will happen this this older value of W 1 and W 2 will be replaced by the new one and then you again go forward and then you again you have the error again combat change the value again go forward backward parole are covered but there is one thing that you might ask me that what is this a here this a is called a is called alpha colleges not air exactly alpha and this is also called as learning rate or you might see people calling us a large right now what learning rate is now just pay a little attention towards here right so let's say the previous value of W 1 and W 2 will give you some error that would be here at this point the one that I draw very small here right the next one will be here then here and here so it might it might vary that how gradually you are moving your W let us say that you are making very small change in W very small change in W so your error lets okay okay okay my rig so now let's say you're you have taken some you know initial value of W such that W one such that the error is here right at this point at this particular point now you make a very small change in your W one and make it to W one - so what will happen the F the error will shift very slowly also right your error is also going to shift very very slowly because you have made a very small change in W one all otherwise let assume that you might dare your W one originally was here and you made a very big change in W so your error might behave at this point new error might be here or it might also happen that it might come here also right it might change that how gradually you're moving your W so there is this terminal order it is that safe that you should very slowly move your W so that you don't miss your minima as you again let's assume that you slowly slowly moved your hair till here so you have come till here your W one is here now and all suddenly increase you did a change in W one so large that you came here you came at this point now this is bad because you missed the global minima in here right so what people say people say that you should very slowly move the value of W very slowly slowly so usually you should do all the changes very slowly and your de by DW your de by DW should be controlled by some another external factor and that factor is called alpha now because if you go down the screen if you see I have multiplied this de by DW term with a thing called alpha now this alpha is with you this is a constant and this is in your hands you can change it any time that you want right so what you will do is and move back up here so what you will do is that you will select the value of alpha such that it is moving very slowly right but now there is a disadvantage also because if you move very very very slowly then let us say you were here then you will come here at this point then here then here then here so you might reach your global minima at a very very larger time it may take a lot of time to you so you have to take a very critical value of alpha but that it moves very slowly I mean I will only be yeah how are you trying to like tune up your model this is not tuning it is just adjusting the thing that how the model is reacting to the changes that we are aware that we are doing so it's like we are controlling our own process of the flow of gradient and alpha keeping the value of alpha yes you can say this is one of the parameters of hyper parameter tuning Hyper parameter tuning HPT yes this is one of the small features that you're tuning so the value of alpha you should keep it 0.001 but again if someone says you just feel like like I just said you that you should keep your alpha no I am totally wrong he depends on your data that how slowly you see that your modulus moving this is just an initial value that you should take so if you if you will see all of your major frameworks like Tiras or like tensorflow you will see that they have put a value of 0.001 also by default make sense just a quick info guys if you want to become a certified a a professional then check out the artificial intelligence course offered by intellibid you can find the link in the description box below so this brings us to the end of this session if you have any queries please do put your comments in the comment box below and we'll reach out to you immediately also do subscribe to our channel to stay updated about our upcoming videos thank you for watching this video 