 Hi everyone, my name is Rangeet Pan, and I am  presenting our work with my advisor Hridesh Rajan.   Today, I am going to tell you about an approach  to decompose neural network into modules to enable   reusability and replaceability. Before we dig  into the decomposition of deep neural networks   or DNN into modules, we want to discuss briefly  about the decomposition in software development.   First, a monolithic software is a piece  of code without components or pieces,   whereas a decomposed software is with components  or pieces that can be utilized independently,   and the process that decomposes a monolithic  software into components or modules is called   the decomposition. There is a number  of benefits of decomposing software   into modules. One of them is enabling  reusabilities. In this example, we can   use small components to build new software  with the proper interface. Replaceability:   a component of the software can be replaced by  other components without changing other parts or   components. Independent testing, where it should  be possible to test each component separately.   Independent development: it should be possible  to make drastic changes to one module without   a need to change others. In this work, we will  show the first two aspects of the decomposition.   In the current state, a model can broadly be  built in two ways. First building from scratch.   As illustrated in this example, we  take a data set with English numeric.   First, we build a model structure based on  the developer's choice and then feed the data   into the model to train it. Once trained, the  values are associated with the nodes and edges,   and the same model would be used for the  prediction. However, if you want to change   something in the model, we either have to retrain  from scratch that increases the cost and the   effort significantly as training from scratch is  always very expensive or use transfer learning.   In this concept, a model trend for a dataset,  let's say X, can be applied to another dataset,   let's say Y. The most common changes involve the  replacement of the last layer and train with the   limited sets of the new data and push it to the  production. So in some sense, the reusability   can be achieved in this process but somehow  in a limited manner. The process increases   the complexity as a significant level of expertise  is needed to do so and decrease the scalability as   only if the model structure matches the operation  can be performed. Now, we will discuss a few   examples of reusability and replaceability  and compare how that can be achieved with   the monolithic design and the decomposed design.  First, let's assume a scenario where we have a DNN   model to recognize the handwritten digits (0-9)  and can you use the same model and build a DNN   model for binary digits without retraining the  model with the example 0 and 1. To realize the   reuse scenario, first, the developer will build a  model structure for a binary digit from scratch,   then the developer will take the training data set  for 0 to 9, and partition is to extract labeled   training samples for 0 and 1 for the scenario.  Then, with this new training dataset, the   model will be retrained. Whereas, in our approach,  we will decompose the DNN model into 10 modules   and reuse the module responsible for predicting  0 and 1 to build a binary classifier.   Now, if we have two DNN models  for recognizing 0-1 and A- J,   can we build a DNN model for recognizing  0-9AB, essentially a dual decimal classifier.   To realize the reuse scenario, the developer will  do the same. First, extract the data set from the   existing dataset and train a new model. Whereas,  in our case, we'll decompose two DNN models into   modules, where each module is responsible  for predicting a single class of output.   We will use 0-9 modules A B modules to build  a duodecimal classifier. Now, here, we will   discuss a scenario that needs replacing the part  of the DNN model. Let's assume we have a DNN model   that is doing a satisfactory job for classifying  0-4 and 6-9, could improve its performance for 5,   could we take the logic for classifying 5 from  another DNN model and replace the faulty 5 with   a new part. Realizing the replacement scenario  is more complicated. The developer might need   to change the model structure of the  model A to the structure of model B,   which also has the potential to change the model  A's effectiveness for 0 to 4 and 6 to 9. Then,   they can replace the training sample for 5 used  to train model A with those used to train model B.   Finally, model A would be trained with  the modified training data. However,   by decomposing the DNN model into modules, we can  simply replace module 5 in DNN model A with module   5 from the DNN model B. Now, we'll discuss the  approach to decompose the DNN model into modules.   This work takes the first step towards decomposing  a monolithic deep neural network into modules   so that the component of the deep neural network  can be reused to create another network. The   approach also takes the first step towards  enabling the replacement of features/concerns   within the DNN. Our approach decomposes a  trained DNN model into modules. In this work,   we focus on DNN models for multi-level  classification problems. We'll refer to   the single, black box, DNN model for all classes  as the monolithic model. Our approach decomposes   such models into DNN modules, one for each  label in the original monolithic model.   Concern Identification essentially identifies  those parts of the monolithic model that   contribute to a special functionality or concern.  To untangle the concern of the output level,   one could obtain the piece of the DN that  can perform a certain task, for example,   prediction of a single class and can hide the  non-dominant concern to separate that concern. Here, the monolithic model has three tangled  concerns 0, 1, and 2. The goal of the concern   identification step is to identify parts of  the DNN that are responsible for classifying   an image into 0, 1, and 2. Once we identify  the parts of the DNN related to the concern,   those parts still might contribute to other  concerns as well. We call those concerns   non-dominant concerns. Before we decompose the DNN  into modules, we need to identify the concern in a   monolithic DNN and separate them to build sub  network, responsible for individual concern.   In this example, for concern 0, concern 1  and 2 are non-dominant concerns. We identify   the part of the neural network that is  responsible for the dominant concern by   providing inputs related to the dominant concern.  Then we identify the active and inactive edges   based on the assumption of using ReLU as the  activation function, and any node value less   than equal to 0 is an inactive node. Then,  we identify edges incident to the inactive   nodes and remove them and identify edges coming  from the inactive nodes and remove them, too.   Tangling Identification recognizes the  part responsible for other concerns.   While concern identification can separate the  part of the network that contributes to a concern,   it may not be able to make the separate  part function. Using concern identification,   we identify the edges that are responsible for  particular concerns. However, the remaining   network can only classify a single concern as  all the edges corresponding to other concerns   are removed or updated, and the model predicts  the dominant concern irrespective of the input.   Thus, the resulting network becomes a single  class classifier. This is akin to removing a   conditional and a branch from the program, and  that results in a subprogram that performs the   functionality of the remaining part but does  so unconditionally. For example, the dominant   concern for 0 cannot be used as they cannot  distinguish between concerns 1 and 2. To solve   the problem, our insight is to identify some edges  and nodes back to the concern that helps us to   identify inputs that don't need classification by  the dominant concern. In our approach, we do so   by adding the part of the non-dominant concern.  Here, an imbalance is introduced while adding   a number of examples from the positive and the  negative output labels. First, the edges related   to the dominant constant added, and the edges  related to the non-dominant concern either added   or updated. Similar to the previous approach of  introducing imbalance to tackle the problem, here,   we punish the negative output labels by letting  the positive output label update the edges first.   Here, the negative output labels are assigned more  priority over the dominant concern by swapping the   order of the edge update. For this, we update the  edges correspond to a non-dominant concern first   and then update the edges responsible for the  non-dominant concern. Here, the edges related   to the positive labels are updated than the  negative ones. Furthermore, the strong negative   edges are added by introducing a validation.  To do so, the value of the probability has been   changed from 0.01% to 50% percent while keeping  the other parts of the algorithm unchanged.   Concern Modularization is a process of separating  the parts of the monolithic model belonging   to concern into its own DNN model. Concern  Modularization also involves concern channeling,   where the effect or the non-dominant concern  within the module is channeled appropriately.   Concern Modularization includes the  abstraction of the non-dominant concern.   In this approach, we propose a technique to  abstract the non-dominant concern by combining the   non-dominant nodes at the output level. First,  we select the non-dominant edges and replace that   with the single edge and remove the rest of the  nodes. Before applying the constant channeling,   we remove the irrelevant nodes at the last hidden  layer that only contributes to the non-dominant   concerns. The edges that are connected with the  negative output nodes and not connected with the   positive output nodes are combined into one node.  The outgoing nodes from the combined node to a   particular negative output node is updated  with the average of all the edges incident   to that negative output node from nodes that  are connected with the negative output labels.   In summary, we have decomposed a DNN  model that predicts three English   digits into three modules, where each module is  responsible for predicting a single digit. Here,   we will discuss the description of the dataset,  experimental settings, and the result based on   the three research questions: how efficient  is our decomposition, whether reusability   is possible or whether the replaceability is  possible, too. We have used four datasets. First,   MNIST is the dataset comprises of various  examples of the handwritten digit. Fashion MNIST:   this data set comprises our different clothes,  like, t-shirt/top, trouser, pullover, dress, etc.   The third one is KMNIST, it contains images from  the Japanese digits, and the fourth dataset is   Extended MNIST. The dataset has two-dimensional  images from the English alphabet A-Z.   As our approach is based on the dense hidden  layer, training a DNN model with only the dense   layer does not achieve high testing accuracy.  To remedy that problem and to fix the number   of output labels for all the data set under  experiment, A to J(10) are taken from the dataset.   To evaluate our approach, we built 16 different  handmade models. These models are trained with   corresponding datasets with 50 epochs, and  they have 1, 2, 3, and 4 hidden layers (each   of size 49). The name of the DNN model has  been represented by the dataset hyphen the   number of hidden layers. For example, MNIST-4, a  model created for MNIST dataset with four hidden   layers of size 49. Modularity has traditionally  been associated with some loss of performance.   Abstraction sometimes caused in performance.  Here, by answering this research question,   we want to find does decomposing DNN models  have performance cost also? If we find that   the decomposed modules are efficient, it tells us  that decomposition is not losing any performance.   To answer this research question, we evaluate the  four different proposed techniques to identify   tangling concern. We utilize the best approach  based on the accuracy and the Jaccard index   and use that technique to apply the concern  modularization to build modules. To do so,   we decompose the DNN models into modules before  channeling the non-dominant concern, and run   them in parallel and compute the accuracy based on  the voting rule. Finally, we compare the accuracy   among the different techniques and the pre-trained  DNN model created from the same training examples.   We found that in terms of accuracy, training only  with the strong negative edges performed the best   over all the tangling identification approach. We  measured the Jaccard index to identify tangling   concerns among modules. We found that utilizing  the TI-HP technique, where the higher priority   has been given to the negative example to update  the edges, the lowest accuracy can be achieved.   The average accuracy is being achieved by  using this particular technique is 22.93%, and   also for the 7 out of 16 scenarios, the accuracy  is 10% that means that the decomposed module   here is only able to identify its own classes not  able to identify the rest of the 9 output labels.   Now we applied the tangling identification  approach, which is the strong negative edges,   to the concern modularizing technique. Out of  two concern modularizing techniques, the one   with the removing irrelevant edges performs the  best. We found that the accuracy after decomposing   losses 0.01%, on average. Also, in 9 out of 16  cases, we are able to increase or able to get the   same accuracy as the trained model. We found that  the modularity does not lose much in performance,   and surprisingly, it does the same or the better  in more than half of the cases. Now, we want to   find whether these models can be used to build new  DNN models and how they can perform in comparison   to the model built from scratch. If we find  that the reuse can perform well, then it saves   a non-trivial amount of cost and effort to build a  new DNN model from the pool of available modules.   In this research question, we validate whether  fine-grained reuse can be achieved by utilizing   the decomposed modules. We have two scenarios;  first, intra dataset scenario. In this scenario,   we study two modules decomposed from the same  DNN model and execute them in parallel to build   a smaller problem and validate against a DNN model  built with the dominant examples from the picked   modules. Second, intra-dataset reuse. In the  scenario, we study modules decomposed from two   different DNN models. The intra-dataset reuse  evaluation has been carried on all the dataset,   and each of them has 45 scenarios where we choose  two concern from 10 concern from our data set that   comprises 45 examples (10 choose 2), and we found  that the 80%, 69%, 80%, and 51% cases for MNIST,   Fashion MNIST, EMNIST, and KMNIST scenarios,  our approach does the same or the better.   In overall, intra-dataset can help to gain 0.03%  accuracy, on average. For inter-dataset, reuse,   we have evaluated two scenarios, MNIST with the  Extended MNIST and MNIST with KMNIST. For MNIST   with Extended MNIST, found that the 5 out of 100  cases, decomposition works the better or the same.   In overall, the average loss is 5.36%, where  for MNIST versus KMNIST scenario, it is 8.28%. Now, similar to reusability, we want  to find how replacing a part of the   DNN model works in comparison  to the model built from scratch.   By enabling the replacement, we can replace  the faulty part of the DNN model or even build   a new model without making any changes  to the rest of the section of the model.   In this scenario, we replace a module from a set  of modules decomposed from the DNN model with a   module built from the same dataset with different  configurations. We evaluate this scenario   by replacing each module from the least complex  model (based on the number of hidden layers) from   each dataset and replace that with a module of  the same output label for a more complex model.   Finally, we computed the composed accuracy of  the modules and compared them with the accuracy   of the DNN model from which the module was  decomposed and the prior accuracy of the modules.   We found that there is an average 0.76% drop  in the accuracy when compared to the composed   accuracy of the modules before replacement. While  a module can be replaced with a module with the   same concern, there can be a situation that needs  a module to be replaced with a different concern.   Here, we replace one module from each dataset and  replace that with a module taken from a different   dataset. For example, the module responsible for  classifying English 1 replaced with the module   for classifying Japanese 1. We found that overall,  inter dataset replacement losses 5.44% accuracy,   on average. We have discussed three research  questions. First is how efficient the decomposed   models. We found that the decomposed modules were  significantly different, and they performed the   same or the better. For the reusability research  question, we found that reusability is possible   but inter dataset loses 8.28% accuracy, on  average. Whereas for the replacement scenarios,   replacement is possible, but both intra  and inter dataset scenarios lose accuracy.   In this talk, we have discussed about various  scenarios of reusability and replaceability and   compare how the developer will perform and  how our approach will work. Then, we talked   about our decomposition approach. Finally,  we discussed the three research questions,   and we briefly discussed about  all the results at a glance.   A more detailed description of the results and  more experiments can be found in our paper.   GitHub link has been given, where are all  the codes and that details have been present. 