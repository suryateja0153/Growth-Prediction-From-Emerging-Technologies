 [Music] hello welcome to the infidel online certification course on deep learning in our previous lecture we started discussions on the topic in neural network and we have talked about implementation of two of the logic functions the and logic and or logic using the neural network and we have seen that and logic and or logic both of them being linear functions they can be implemented very easily using a single neuron these single neurons are known as or a single near neuron they are known as single layer perceptron we'll see later that why they are single data cetera today we will talk about other implementations of neural network like X or logic we'll also talk about feed-forward neural network or a multi-layer perceptron and we will also talk about how the neural networks can be trained using learning mechanism known as back propagation learning so let me just recapitulate what we did in our previous class we have implemented two logic functions and logic and or logic and we said that both this and logic and all logic can be implemented using single neuron or single layered neural network so for implementation of an and logic what we have done is we have taken or input vector to be 1 X 1 and X 2 and the weight vectors were taken as minus 1 point 5 1 and 1 in the neuron I can consider that it has two functional parts in the first part it computes W transpose X so in this case it we'll compute W zero which is nothing but minus one point five minus one point five times one plus 1 into X 1 plus 1 into X 2 so effectively this function which is computed is X 1 plus X 2 minus one point five now this computed value is passed on to the second compartment of the neuron which computes the non-linearity and in this case the non-linearity is a threshold non-linearity so the nonlinear function to test that we have considered is a threshold or non-linearity so for X transpose W transpose X greater than zero the output is one for W transpose X less than or equal to 0 it is equal to zero so as a result at the output the function that I get is an ant function so this is what we get in case of an ant function when the input vectors are minus one point five one and one when I want to compute an or function I simply my input vectors remains the same that still is 1 X 1 and X 2 where as weight vectors are changed to minus 0.5 1 and 1 as before the first compartment computes W transpose X and the second compartment gives you the non-linearity which imposes no name threshold on any non-linearity in W transpose ax so as a result at the output what we get an or function or of x1 and x2 and here with these weight vector this weight vector the classifier that we are doing or the separating plane that we are designing is nothing but X 1 plus X 2 minus point five that is equal to 0 that is the separating plane between the two classes Omega 1 and Omega 2 and we have also said that we could implement and function and/or function using a simple single layer neuron or a single neuron because the classes in this case are linearly separable now what happens if the classes are not linearly separable so we have discussed earlier that in non linearly separable cases or linearly non-separable cases either I can have a nonlinear classifier that is the boundary which is which itself is nonlinear or the other approach can be that you can map the feature vectors using nonlinear functions to an intermediate feature domain where because of this nonlinear mapping the in the intermediate feature domain the feature vectors will be linearly separable and as in the intermediate which are domain the feature vectors are linearly separable so I can now have linear classifiers which classifies the features in the intermediate feature terminal so I have two levels of operation in the first level of operation the feature vectors will be non linearly mapped to an intermediate feature domain what they will be linearly separable and in the second step you these feature vectors in the intermediate feature space alright they are linearly separable in the second step I can design a linear separator or a linear classifier which classifies these vectors in the feature space ok so I have a non linear mapping of the feature vectors in the original space and finally a linear classifier so let us again see a very simple example what happens that in case of an instead of ant function or or function if I want to implement an XOR function so all of you know that in the in case of XOR function when the inputs are zero zero that is both X 1 and X 2 both of them are zeros the output is 0 if both of them are wants that is X 1 is equal to 1 and X 2 is equal to 1 then also the output is 0 only when the X 1 and X 2 are different that is in case of 0 1 or 1 0 the output will be 1 so that is what I have in case of XOR function and in the same manner as we have done before if I want to plot this in the feature space if I plot this feature space and plot all these feature vectors in X 1 X 2 that is 0 0 0 1 1 0 and 1 1 in this feature space then this is the situation that I have you find that when X 1 X 2 both of them 0 then the output is 0 if both of them are 1 then the output is 0 if they are 0 1 or 1 0 then the output is 1 and now find that I have a difficult situation in the earlier cases when he talked about our function or and function the classes are linearly separable on one side of the straight line of a straight line I had the value 0 on the other side the values were once but here you find that the values of zeros and ones cannot be separated by a single straight line rather I need two different straight lines and in between the straight lines the values are 1 and outside the state lines the values are zeros so this is a clear case where the problem is not a linear problem but it's a nonlinear problem so can i implement or can i implement this xor function using neural networks or neurons let us say so you know from your digital circuits course that an XOR function X 1 or X X 2 X 1 XOR X 2 can be broken into a multiple stay to function so what I can do is I can perform our operation of x1 and x2 and I can also perform NAND operation of x1 and x2 so here what I have this first i compute x1 or x2 and the second one is NAND operation of x1 x and x2 obviously this is not NAND operation because this is nothing but x1 and x2 complement of that which is nothing but x1 complement or x2 complement so I perform and operation of x1 x2 I perform I perform or operation of x1 x2 I perform NAND operation of x1 x2 and these two outputs the and output or output and and output I end them and them together and that gives me X 1 X or X 2 so effectively what I am doing is as shown in this particular table I am converting the input vectors x1 x2 to an intermediate vector age given the company even by components h1 h2 so here you find that if I consider that output of the or operation is h1 and output of NAND operation is h2 as we have shown here then when the input is 0 0 H 1 will be 0 and H 2 in p1 when the input is 0 1 H 1 will be 1 H 2 will also be 1 when the input is 1 0 H 1 will be 1 H 2 will also be 1 and when the input is 1 1 then H 1 will be 1 and H 2 will be 0 so as we have done before if I plot 8 1 and a - that is the intermediate feature space say this is h1 and this is h2 you find that when h1 and h2 they are having values 0 & 1 H 1 is equal to 0 H 2 is equal to 1 which is the map X as X 1 equal to 0 and X 2 is equal to 0 so I have h1 0 and H 2 1 this is where I want my X or output to be equal to 0 similarly when the values are 1 1 both h1 and h2 there 1 1 which is equivalent to X 1 is 0 and X 2 1 or X 2 X 1 is 1 X 2 is 0 in both these cases I want the XOR output to be 1 sorry I just made some mistake so over here when the h1 h2 both of them are 0 X 1 X 2 both of them are 0 that is H 1 is 0 and H 2 is 1 so this was correct rate so here I want the output to be 0 in the other case when X 1 is 0 X 2 is 1 or X 1 is 1 X 2 is 0 which are mapped to both H 1 and H 2 become becoming 1 1 I want the output to be 1 so here I want the output to be 1 which is the case when both X 1 and X 2 are either 0 1 or 1 0 and in the other case when X 1 X 2 or 1 1 I get H 1 to be 1 and H 2 to be 0 that is somewhere over here and here my X or output should also be 0 so you find that now I have this is the place where I have X or output 0 this is the place where I also have X or output 2 is 0 and this is the place where I want to have X or output to be 1 and now we find that Dell denier is a Bible so the first nonlinear mapping or the first step in the first step when I am computing X 1 or X 2 and X 1 and X 2 these two operations are transferring transforming my input vector to an intermediate vector given by H 1 H 2 H 2 and in this intermediate space in this intermediate feature space in the classes the problem is linearly separable so now I can have a linear classifier to classify the input vectors X 1 and X 2 by the linear classifier instead of operating on X 1 X 2 the in linear classifier operates on H 1 and H 2 so I can represent this again in matrix form so what I have is in the first level I have a set of matrices given by a set of weights given by the matrix W 1 so these weights are so if you look at the first one it is minus 0.5 1 1 which are the weight vectors we have used for an or operation the second one the second weight vector is given by 1.5 minus 1 minus 1 and if you remember from our previous discussion that minus 1.5 1 1 was the weight vector corresponding to and operation so if I negate it I make it plus 1.5 and this is minus 1 and this was minus 1 this is the weight vector which is used for NAND operation so using this weight vectors I compute W 1 transpose X where W 1 is actually the weight matrix consisting of minus 0.5 1 1 and 1 point 5 minus 1 minus 1 so this is my intermediate the matrix product that I get now if I apply the non-linearity which facial non-linearity I get 1 0 0 0 1 1 1 and 1 1 1 0 that is my intermediate set of vectors which is given by matrix H so this is my H 1 this first row corresponds to vector H 1 the second row corresponds to vectors H 2 so my final output if I put another weight vector now we find that this weight vector corresponds to an and operation that is minus 1 point 5 1 and 1 this was my intermediate matrix or set of intermediate feature vectors H 1 and H 2 which is given by H so if I compute H transpose W 2 now the output of this matrix multiplication becomes minus 0.5 then 0.5 0.5 and minus 0.5 again I pass this through this threshold non-linearity and I get the output as 0 1 1 0 so you find that when my H 1 H 2 are 0 1 or 1 0 the output becomes 0 and these are equivalent to the input vectors becoming being 0 0 or 1 1 and when my H vector that is the intermediate features are 1 1 that is this case the output is also 1 and this is the case when both x1 and x2 they're either 0 1 or 1 0 so I have implemented this X or function in two steps so I can consider this to be implemented using two layer neural networks in the first layer I'll have two neurons one giving an or operation and other one giving an NAND operation and you'll find that because and is a linear problem nande is also a linear problem because NAND is nothing but complement of and and that is what we have done over here that plus point one plus one point five minus one minus one this weight vector actually gives an NAND operation okay so I can implement this XOR operation which is linearly non-separable but for implementation of this as obvious from this operation that a single layer neural network or a single neuron is not sufficient rather I need three neurons and those three neurons are to be arranged in two layers one neuron in the first layer will compute or operation the other neuron in the first layer will compute NAND operation and that then the third neuron which is in the second layer will combine the outputs of these neurons in the first layer using an and operation to give you the final output so effectively the kind of neural network that I have is this so here I put my input to be again 1 X 1 X 2 suppose this first neuron computes or operation so we know that for or operation my weight vectors should be minus 0.5 1 and 1 and suppose the second neuron computes NAND operation for which I have to have a weight vector as plus 1.5 minus 1 and minus 1 so here what I get is X 1 or X 2 and here what I get is X 1 and X 2 so I use this as H 1 and this as H 2 and the second third neuron in the second layer this computes and operation so here I had to have the weight vectors as minus 1.5 1 1 so this performs an and operation and finally at the output what I get is XOR of X 1 X 2 so I get X 1 or X 2 at the output so through this discussion it is quite clear that if I have linearly separable problems single neuron for a 2 class problem is sufficient if I have multiple classes then I have to have multiple neurons so in case of multiple classes the way this has to be done is I can have a number of neurons but in a single layer here I have the weight vectors say something like this so you had I have X over here we have the weight vectors a state of weight vector for this out this neuron a state of weight vectors for this neuron oscillate upward victors in this one each of these neurons performs suppose the weight vector for this is a w1 weight vector for this neuron connecting to this neuron is say w2 weight vector connecting to this neuron is say W n if I have got n number of classes or in number of categories this neuron will compute W 1 transpose X and then a function of this this one will compute W 2 transpose X and then a function of this this one will compute W n transpose X and uh non-linearity on this so I'll get a number of outputs oh 102 and oh 3 which actually gives you the score functions or score values for different classes or different categories corresponding to input vector X and here you find that all these are linear combinations linear combinations of different components of the input vector so if my problem is a linear problem I can solve it using single a neuron or single a neural network but if it is not a linear problem if it is a nonlinear problem that as has been demonstrated with a very simple nonlinear function of X or function I need multiple levels of neurons or multi-layer neural network so the kind of neural network that I had to have over here is as given by this you find that every layer of the neural network computes a nonlinear function of the input feature vector of the feature vector inputted to that particular layer of neurons and the output of that layer of neurons is an intermediate feature vector may be of same dimension or may be of different dimension depending upon how many neurons I have in that particular layer we'll see that as we proceed through our discussions but effectively what I have is I have a multi-layer neural network and in every near the neurons compute a nonlinear function so when I have a cascade of this nonlinear functions as given by the multiple layer neural network as has been shown in this particular diagram so here you find that suppose each of these are neurons a layer of neurons and in every layer it computes a nonlinear functions in eyuth layer it computes a nonlinear function f I on the feature vectors which are computed by the previous layer given by the function f I my fi minus 1 and each of these are nonlinear function so I can cascade all of them together to give you an overall cost edit function as given by this so if my input vector is X the first layer computes f1 of X this f1 of X is inputted to the next layer so which compares f2 on this f1 of X that means this layer computes f2 of f1 of X and this way it continues when it comes over here say I have a set of map feature vectors over here which is given by say H K minus 1 which is a map feature vector mapped from this vector X through a series of nonlinear mapping but each of these layers of neurons gives some sort of long linear mapping and then I have a fine lair of neuron which is the cave lair which when I come to this HK minus 1 these which are vectors are non linearly mapped from X and as these are non linearly mapped so this in in this feature space as given by this vector HK minus 1 these feature vectors are the problem that is posed is a linearly separable broken and once I have such a linear problem the final layer or the Keith's layer can solve this using a linear discriminator so effectively I can put all of them in a cross kid of operation something like this ok so the first layer you compute F 1 of X which gives you say state of feature vectors H 1 in the second layer you compute F 2 of F 1 of X which gives you a set of which a vector say H 2 so you find that each of them are non linear mappings over here to discuss scaling operations I get a set of each a vector say H K minus 1 and when I have this H K minus 1 in this feature space as these are nonlinear mapping of X I can have a set of nonlinear mapping such that H K minus 1 will lead to a linear problem or a linearly separable problem and once I have that the final one that is FK can be a linear machine or it might be a linear classifier so I have a set of operations which Maps an input vector non linearly into a space where they are linearly separable and once they are linearly separable then in the final step I can have a linear classifier that is what a multi-layer feed-forward neural network does and why is it feed-forward because we find that the info mission is always flowing from input to output right the information does not flow in the reverse way of course there are other variants of the neural network known as the current a neural network where the information can also be fed back we'll come to that later and here you find that in the number of such cascading stages here the number of cascading stages which is equal to P this tells you what is the depth of the network so in this particular case the depth is K and as the value of K increases that is the number of layers increases that depth also increases and this is this term that is the depth of the network which actually is converted to the concept of deep learning or deep neural network okay so the more the depth is you are moving deeper into the problem so the deep neural network or deep learning concept actually comes from the depth of the network or the depth of the network giving you the nonlinear transformation it is related to right so I'll stop here today and I will continue with a discussion in our next class thank you 