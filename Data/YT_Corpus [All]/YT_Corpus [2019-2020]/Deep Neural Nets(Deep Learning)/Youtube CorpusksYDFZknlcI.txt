 tim obtained his phd from the university of osnabrook uh where he was and he was also a visiting student with trang at vanderbilt uh he was sent a professor a postdoc with professor konig at the university of ostenbrook before becoming a research associate at the mrc cognition and brain sciences unit at the university of cambridge and he's currently an assistant professor at the donders institute for brain cognition and behavior at radboud university his work sits nicely at the nexus of human and machine visual intelligence we are excited to hear all about this and this talk it's all about time modeling human visual inference with deep recurrent dual networks great yeah thanks for thanks for the introduction thanks for the for the invitation um it's about time i mean the talk is about time uh and about how we use recurrent connectivity in deep neural networks to kind of better understand um visual inference in the human brain so when i start so i i should say that i i thought a lot back and forth gave arturo a hard time not telling him what the title of the talk was because i went back and forth between whether i should talk about eco set which is a new data set to train deep nets on which are new object categories uh or or this now given the last two talks being about data sets maybe ecoset would have been a good a good choice but now it's something something a bit different maybe maybe that's good too so when i show you this image almost immediately everyone is able to extract meaning from it right you see the pixels but your brain within a fraction of a second can tell that this is a little girl holding maybe her her newborn sibling and that she has maybe mixed feelings about holding a baby or having a sibling and that's remarkable and our visual system is just generally very good at these visual tasks it's very fast it's very versatile and this is the community that i think appreciates this most of all communities so this is kind of a uh maybe a bit boring introduction but okay so it's it's the visual system is fast and versatile it's very reliable it's very energy efficient and it's very data efficient so what in in my lab what we try to do or try to understand like many of us here is how does the brain do that right how can it be so quick and yet so robust and so efficient and uh in addition to trying to understand how the brain does that um we also would like to know can we learn from looking at the brain and improve machine vision in the same time which means that we're in in this you know midway between computer vision and and cognitive computational neuroscience where we try to you know improve our understanding about one hand and on the other hand try to improve machine vision and um deep nets really offer themselves for this sort of task between the two or dance between the two worlds and because they can work as computational models of inference in the brain and at the same time many of them are image computable which means they give us a handle on computing representations from pixels which is what machine vision is ultimately about um so we think that deep nets are really ideal for this sort of dual task um i think many of us have started using deep nets since you know com they basically revolutionized computer vision having been around for a long time but you know alex and 2012 coming onto the scene gave a lot of people left a lot of computer vision researchers in shock that they could do this and it didn't take long for neuroscientists to also appreciate deep nets as models of brain function what i'm showing you here is um data from humans and macaques i don't want to go into details but it basically says that that computer vision models trained on the task of object categorization happen to be quite good models of what the human brain is doing if we look at neuroimaging data or also at macaques and and firing rates so today dnns are really the best image computable models that we have for predicting prime adventure stream which doesn't say they're the best it doesn't say they're perfect right it just says currently these are the avenue that we take at understanding understanding human vision and they're currently the the best models for doing so didn't take long from these these early papers who started to to show that you know there are in fact some similarities between deep nets and how the brain solves the task of object recognition for many other labs and many many people to to join in and now you know the field is full of computer vision dnns and the reason for that is in part that um it's computational convenience we use these models these feed forward models because they're there we can download lxnet we can download vgg we can run our models on it and compare it and that's great and it's very useful in in terms of co2 emission that makes a lot of sense but we must be aware that these most many of them most of them are feed forward architectures that are trained to categorize and we often predict time average data so one thing that we think is missing from the equation here for understanding the brain and maybe improving computer vision is the temporal domain or recurrence so if you want to understand the brain ideally you would like to these three things which i took from marika's and nico's paper in 2014 we'd like to know what when and where which means we'd like to know for the ventral stream or basically everywhere in the brain we'd like to know what is the distinction that is currently being made what is the representation like um how does it change across time and how does it change across space and so ideally we would like to have models that do the same we'd like models that don't only predict temporal average data because the code can change through time we want models that you know basically do this we want models that reproduce the whole representational trajectory as information enters the retina and goes from one ventral stream area to the next uh but also within a given ventral stream area or visual area we'd like to know how the code changes through time in that very area right so we need both we need variance in space and we need variance and time and all of that ideally should be in one big deep neural network model that then kind of is a good model of what what the brain does if it follows through the same trajectories representational trajectories as as the brain so the problem is with many of the models that that we took and that we took out of convenience you know that i said already their feed forward so dynamics are kind of out of the question there but the problem is that a lot of these models that we typically use to make inference about the brain are doing two steps at a time basically they they assume a given architecture that could be a feed-forward architecture and they assume a given task let's say object categorization but that's you know two assumptions at a time and in this project that's that's published last year we kind of took a step back and said well let's try and let's let's play with different architectures and let's try to enforce the architectures to be as brain-like as we could possibly get them and let's not even train them to be you know doing a given task let's train them to be brain-like and then there will be some architectures who do really well in this task and some architectures who are not very good at this task and the ones who are not even good at this task if we force them to are maybe not good candidates for being a good model of the brain right that's the rationale basically so how do we do that how do we how do we try to enforce you know brain like representations in deep nets um this is an extension to what's known as representational distance learning which we call dynamic representation the representation of distance learning and the idea is super simple the idea is that you show objects to a brain and you extract for a given region at a given point in time the representational pattern uh let's say for seeing an elephant or a pineapple uh here to the left can you see my cursor actually um can you move it okay now now yeah we can see here we can see okay good so uh basically you show an elephant and a pineapple to um to a brain and you extract a distance and and the pattern activation at a given point in time and and space and let's assume that that distance would be 0.8 you can do the same with the deep net and let's assume uh the distance was 0.2 and the realization is that you can treat this as an error you could say if this were a model that's more brain like it should be 0.8 it shouldn't be 0.2 and you can use this to drive learning or to define a loss function and to drive learning in these networks the point is that if you have enough images in this case we had 92 and you define uh you can compute these distances for all pairs of objects so in this case we get about five a bit less than five thousand distances you can enforce this whole geometry of what things are treated the same way or differently in the brain and you can force the whole thing that you enforce on the brain you can do the same in the deep net or what you observe in the brain you can enforce in the deep net right and that's um that's uh that's really promising because it it doesn't only allow us to test different neural network structures for the ability to mirror the brain it also enables us to directly inject if you want neural data into these networks and maybe that will help us get the more robust who knows it's one of the avenues of research in the lab okay so let's do this let's look at meg data where the code changes in different regions across time and let's take two networks and enforce them this way to be brain-like and see which one is better and we'll we'll do two things so we'll we'll the first model will be is known as ramping feed forward or we call we call it's not known for it we called it ramping feedforward um and it's a feedforward network but each unit has a connection to itself which means it can slowly integrate evidence over time it can ramp up its activity over time and this gives feedforward network some nonlinear dynamics but still information is only flowing from the bottom to the top of the network and then we have what what nicoterum the blt models it's not baking lettuce and tomato but bottom-up lateral and top-down connectivity um and those are basically unrolled recurrent neural networks unrolled convolutional networks um i have so much to talk about today so i i'll really just go through this very very briefly and and i'm happy to answer questions and everything else also in the paper what you see here is basically is going to be a movie um this is v4 and lo this is what we extract from the human brain this is going to be a movie that shows you how the representations change across time how distances between different objects that you see in the left change over time and this is what the recurrent model predicts the dynamics should be the model has never none of the models that we test here have seen these stimuli but they've seen these categories of stimuli before during training but this is their prediction of what the response to these stimuli should be and we train them on you know we do that we do our homework we train them one half of the data we test them on the and a half and so on and so forth okay so but together get a visual idea of which model is good at doing this task which is not i'll just play this movie and what you'll see is that there's a lot of stuff happening uh in the in the brain you know even though you're within one region the code changes quite dramatically over time and this loops around and um you can see that the recurrent model here in the center is it's not perfect but it's it's it's quite closely able to at least track uh the large-scale organizational changes in b4 whereas the feed-forward model even though it has non-linear dynamics that it can learn so it can learn the ramp-up parameter uh only very initially there's there's a bit of nonlinear change and then it kind of settles on a good average and we can of course quantize this and um for all regions that we tested across the ventral stream the recurrent models are very much better able to kind of follow the dynamics that we observe in the brain compared to these parameter matched feed forward models so um really from this what we take is that even if you want to you know this is you know milliseconds already so even in the earliest process parts of the of the response recurrent models will be better able to capture what's happening in in the brain what i promised you in the start was though that maybe if we understand something about the brain we can get models to be better at the task so let's look at that next let's let's um look at computational benefits of of recurrence and this is work that's that was led by a phd student courtney spora and it's it's published in plus cb um so let's assume we have a feed forward model this is now a computer vision type task this is trained on imagenet actually um let's assume you have um a feed-forward model uh just a convolutional network and now we can add lateral connectivity to it so units get we unroll it through time and units can now get information from surrounding units adding lateral connections adds a whole lot of parameters so if that model were better than the base model you could say well it's just more parameters so really it's not that surprising so we enter three more control bottles bk bfbd bk is just larger kernel sizes bf is more feature maps per layer and bd is just more layers the point is that um b has 11 million parameters bk and bf have 40 million parameters and bd and bl have 30 million parameters so these these model are in the same order of magnitude in terms of parameters or quite closely match what bl has in terms of parameters so what we'll do now is we'll take these models for a spin we'll train them on imagenet and this is the the top one accuracy that you see here and you can see that bl um is is note that this doesn't start at zero um but so bl is better able at doing this task compared to bd bf bk seems to even over fit to the task because it has more parameters but it does worse than b so overall we think that you know adding this lateral connectivity helps solve solve uh imagenet or or perform better at imagenet now what people tell us when they see this is like yeah that's great but now you know if this expands through time i have to wait a bit longer for my results maybe i don't want that and so what we plot here in the right is um we can cut the recurrent model short we can after each each time point we can ask for a response right and now what we do is we compute the entropy of the probability distribution um at the output of the model at every time point and if the entropy dips below a threshold we we think the model is certain enough and then it gives a response and that way we can get reaction times from the model right if it's certain enough it gets the response and now what we can do is we can change this threshold and going from four down to zero you can see that if we change if we lower the threshold the model will get better and better and better but it'll also take longer to compute the interesting point here is that these three models which is b b f and bd in terms of floating point operations they're round about the same as the recurrent model so the recurrent model isn't slower it's just as good as their free forward models at a fixed number of floating point operations you can just choose to let it run for longer and then it gets better that's the point here so what what we can do in addition with this sort of um model that now gives us reaction times true reaction times so it you know the the time steps of the model are now the reaction times is we can compare to human uh reaction time data this is uh data collected by yam charles who's in birmingham is collected at the cbu still in cambridge and now we can we can ask okay how well can if we show these stimuli and this is an animate sorry i should have said this is an animate this is a speeded animate classification task which means you see an image and you need to say is it or not as fast as possible now we can train our models on the same task we train them on it let's say imagenet and then we train a new readout and that says is it animate or not we can now compare the model reaction times to human reaction times this is what i'm showing you here in the top row you have how well human reaction hand patterns correlate with human reaction time patterns so this we call human consistency and in red you see how well our recurrent models work um again not having seen or being fit to these data but being trained on a separate set of data to do this nmc detection task and we do cross-validate to to fit the threshold um the entropy threshold to get reaction times out and um compared to all the models that we tested vgg resnet densnet exception and so on and so forth and our control models um the recurrent models clearly outperformed these sometimes very deep feed forward networks in their ability to predict human data you may ask how did you even how did we even get feedforward models to give us reaction times because they basically compute once and then you get that we basically treated each layer as a time step so we retrain readouts for each layer and then we do the same entropy game as with a recurrent model but you go different depths into the network we call that a reaction time okay so um i'm aware that this is a bit of a firehose talk but i want to talk about this third project too which is why um i'll take a quick break here and and kind of reminisce on what we've what we've done so far so we've we've trained recurrent models and we enforced them to be brain-like and we found that adding ladder on top down connectivity made these recurrent models a bit more brain-like in terms of following the same representational dynamics as what we observed in human observers in met and then i've shown you that adding lateral connections to feed-forward models increase their performance on on imagenet and ecosed which is a different dataset this similar to to imagenet and scale and we've shown that we can we can use this entropy trick to get reaction times out of these recurrent models and it turns out that if you train these recurrent models on on imagenet or ecoset then their predicted reaction times are currently the best models that we have for predicting this this behavioral data set by jancharist okay so now the remaining minutes i want to talk about this new project which isn't published anywhere so this is uh i wouldn't you know i wish i could say it's hot off the press it's not even impress anywhere it's just the the sort of thing we're doing right now um so the the i call it closing the loop because what i haven't done yet is is following the normative approach in in terms of modeling neuroimaging data right either we enforce models to be like the brain but we haven't trained models on a given task and then tested how brain-like they were which is what you know brain score and these things these these data sets uh do as well uh benchmarks do so the rationale is really we train a deep recurrent network to categorize visual input and then we test how well the internal representations agree with with neuroimaging data and we use rsa for that now this data set that we're testing on is peculiar it's it's i i think that you know the the greatest neuro imaging data set out there it's the natural scenes data set um uh which is uh spearheaded by thomas nesolaris and kendrick k it's 77 tesla um fmri data it's eight participants and 73 000 stimuli different stimuli that they show to these participants over tons of um sessions and it's 10 000 images with three repetitions per participant and so this is super high as an r and you get you know rdms that are basically 10 000 by 10 000 they give you a huge variety of different images and brain responses um to to this and um this project is is headed by jan charist who who's putting all the pieces together taking the nsd data set and um giving it another twist that i'm going to talk about in a second and what i'm going to what we're going to test is the bl network that i just showed you a bit earlier it's a it's a model that has default lateral connectivity and we'll train it on ecosed which i would have loved to talk about but didn't have time today it's a new data set that'll hopefully come out soon it's 1.5 million images mirroring 565 most concrete and most frequent basic level categories in the english language okay so uh what are we going to do with all this we're going to take these thousands of images we're going to show it to participants in the 7 tesla and we'll we'll extract rdms from from their conjugate responses we'll do show the same images to a pre-trained model and we'll get rdms since this is a recurrent model we'll get different time points and different layers and what i'm going to show you next is the results of a searchlight approach where we took the each time point in each layer and the model and we search through the whole brain and mark how well the rdms of the model agree with what we found in the brain so i hope this wasn't too fast so i'm going to show you a movie now um so this is uh the representational agreement between the recurrent cnn and the seven tesla data on this nsd data set and it's going to show eight time points per layer before jumping onto the next layer and you can see in marked in gray what the what the current layer is so let's let's look at this movie so you can see that um across time you know the spatial arrangement is sort of similar but it it you know kind of changes in intensity but as we go across the layers i hope it will be clear that different brain regions will light up it was early brain regions in the early few layers now we're on to high level regions and um in the final layer of the model we end up with this this beautiful pattern um where the last time step of the model last time step in the last layer of the model agrees well with the regions highlighted in red here which are clearly not early visual areas but they're higher level visual areas the question is now what are these areas and what are they doing like how can we understand this i mean it just shows you that you know the later layers in the network agree well with these regions but we don't know why so here comes here comes jan with this this great idea um so this is what we've done so far this is just basically a snapshot of the final image that i showed you in the movie this is the agreement of the recurrent model and the nsd data set at the final time step and final layer so we basically showed an image to the model and now this is the agreement what we can do in addition is we can get an image caption so for this image i don't know how visible it is for you but it's basically a young boy sitting on a bed with a lamp beside it this is an image caption and this gives you an idea of the semantics this isn't talking about low level elements in the scene this isn't saying there's a bright spot to the top right and a red uh curtain in the top right that has certain vertical patterns also this is a very high level semantic description language or linguistic description of what happens in this scene and what jan did was he took it and threw it into google the google universal sentence encoder which is a linguistic embedding space called queues and we can do this with all of the nsd data set scenes which also have captions and we can get an idea of where in this linguistic embedding space these different concepts are and we can again compute an rdm a dissimilarity matrix and go searching for these regions in the brain as we just did with our deep neural network representations for the visual case right and interestingly enough if you do that if you use use and run the same analysis on the brain data pretty much the same areas um light up which tells you that and this is interesting right because this is data that's collected from people seeing these these images in the scanner so it's a visual paradigm but it's a prediction to the right it's a prediction that's derived from image captions that someone else somewhere wrote about this very image so it goes across observers it goes across modalities because it's from image to language and it happens to to agree well with the same regions that we observe to agree well with the image computable uh recurrent deep net okay i'll i'll wrap up here uh we at least that many of the projects in the lab deal with recurrence and we think recurrent connectivity is key for modeling dynamics uh neurodynamics for modeling behavior for better usage of parameters and computer vision tasks and and maybe what i showed you last for mapping from pixels to actual uh cross-domain semantic information with that i thank the members of the lab and my collaborators and every one of you for your attention thanks 