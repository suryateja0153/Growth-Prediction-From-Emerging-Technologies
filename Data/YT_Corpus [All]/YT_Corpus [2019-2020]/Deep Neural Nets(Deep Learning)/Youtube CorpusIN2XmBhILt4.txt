 back propagation is a really big word but it's not a really big deal stat quest hello i'm josh starmer and welcome to statquest today we're going to talk about neural networks part 2 back propagation main ideas note this stat quest assumes that you are already familiar with neural networks the chain rule and gradient descent if not check out the quests the links are in the description below in the stat quest on neural networks part 1 inside the black box we started with a simple data set that showed whether or not different drug dosages were effective against a virus the low and high dosages were not effective but the medium dosage was effective then we talked about how a neural network like this one fits a green squiggle to this data set remember the neural network starts with identical activation functions but using different weights and biases on the connections it flips and stretches the activation functions into new shapes which are then added together to get a squiggle that is shifted to fit the data however we did not talk about how to estimate the weights and biases so let's talk about how back propagation optimizes the weights and biases in this and other neural networks note back propagation is relatively simple but there are a ton of details so i split it up into bite-sized pieces in this part we talk about the main ideas of back propagation one using the chain rule to calculate derivatives and two plugging the derivatives into gradient descent to optimize parameters in the next part we'll talk about how the chain rule and gradient ascent apply to multiple parameters simultaneously and introduce some fancy notation then we will go completely bonkers with the chain rule and show how to optimize all seven parameters simultaneously in this neural network bam first so we can be clear about which specific weights we are talking about let's give each one a name we have w sub 1 w sub 2 w sub 3 and w sub 4 and let's name each bias b sub 1 b sub 2 and b sub 3. note conceptually back propagation starts with the last parameter and works its way backwards to estimate all of the other parameters however we can discuss all of the main ideas behind a back propagation by just estimating the last bias b sub 3. so in order to start from the back let's assume that we already have optimal values for all of the parameters except for the last bias term b sub 3 note throughout this and the next stat quests i'll make the parameter values that have already been optimized green and unoptimized parameters will be red also note to keep the math simple let's assume dosages go from 0 for low to 1 for high now if we run dosages from 0 to 1 through the connection to the top node in the hidden layer then we get the x-axis coordinates for the activation function that are all inside this red box and when we plug the x-axis coordinates into the activation function which in this example is the soft plus activation function we get the corresponding y-axis coordinates and this blue curve then we multiply the y-axis coordinates on the blue curve by negative 1.22 and we get the final blue curve bam now if we run dosages from zero to one through the connection to the bottom node in the hidden layer then we get x-axis coordinates inside this red box now we plug those x axis coordinates into the activation function to get the corresponding y axis coordinates for this orange curve now we multiply the y-axis coordinates on the orange curve by negative 2.3 and we end up with this final orange curve bam now we add the blue and orange curves together to get this green squiggle now we are ready to add the final bias b sub 3 to the green squiggle because we don't yet know the optimal value for b sub 3 we have to give it an initial value and because bias terms are frequently initialized to 0 we will set b sub 3 equal to 0. now adding zero to all of the y-axis coordinates on the green squiggle leaves it right where it is however that means the green squiggle is pretty far from the data that we observed we can quantify how good the green squiggle fits the data by calculating the sum of the squared residuals a residual is the difference between the observed and predicted values for example this residual is the observed value 0 minus the predicted value from the green squiggle negative 2.6 this residual is the observed value 1 minus the predicted value from the green squiggle negative 1.61 lastly this residual is the observed value 0 minus the predicted value from the green squiggle negative 2.61 now we square each residual and add them all together to get 20.4 for the sum of the squared residuals so when b sub 3 equals 0 the sum of the squared residuals equals 20.4 and that corresponds to this location on this graph that has the sum of the squared residuals on the y axis and the bias b sub 3 on the x axis now if we increase b sub 3 to 1 then we would add one to the y-axis coordinates on the green squiggle and shift the green squiggle up one and we end up with shorter residuals when we do the math the sum of the squared residuals equals 7.8 and that corresponds to this point on our graph if we increase b sub 3 to 2 then the sum of the squared residuals equals 1.11 and if we increase b sub 3 to 3 then the sum of the squared residuals equals 0.46 and if we had time to plug in tons of values for b sub 3 we would get this pink curve and we could find the lowest point which corresponds to the value for b sub 3 that results in the lowest sum of the squared residuals here however instead of plugging in tons of values to find the lowest point in the pink curve we use gradient ascent to find it relatively quickly and that means we need to find the derivative of the sum of the squared residuals with respect to b sub 3. now remember the sum of the squared residuals equals the first residual squared plus all of the other squared residuals now because this equation takes up a lot of space we can make it smaller by using summation notation the greek symbol sigma tells us to sum things together and i is an index for the observed and predicted values that starts at one and the index goes from one to the number of values n which in this case is set to three so when i equals one we're talking about the first residual when i equals two we're talking about the second residual and when i equals three we are talking about the third residual now let's talk a little bit more about the predicted values predicted value comes from the green squiggle and the green squiggle comes from the last part of the neural network in other words the green squiggle is the sum of the blue and orange curves plus b sub 3. now remember we want to use gradient ascent to optimize b sub 3 and that means we need to take the derivative of the sum of the squared residuals with respect to b sub 3. and because the sum of the squared residuals are linked to b sub 3 by the predicted values we can use the chain rule to solve for the derivative of the sum of the squared residuals with respect to b sub 3. the chain rule says that the derivative of the sum of the squared residuals with respect to b sub 3 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to b sub 3. now before we calculate the derivative of the sum of the squared residuals with respect to the predicted values let's clean up our workspace and move these equations out of the way now we can solve for the derivative of the sum of the squared residuals with respect to the predicted values by first substituting in the equation and then use the chain rule to move the square to the front and then we multiply that by the derivative of the stuff inside the parentheses with respect to the predicted values negative one now we simplify by multiplying two by negative 1 and we have the derivative of the sum of the squared residuals with respect to the predicted values so let's move that up here and now we are done with the first part now let's solve for the second part the derivative of the predicted values with respect to b sub 3. we start by plugging in the equation for the predicted values remember the blue and orange curves were created before we got to b sub 3. so the derivative of the blue curve with respect to b sub 3 is 0 because the blue curve is independent of b sub 3. and the derivative of the orange curve with respect to b sub 3 is also 0. lastly the derivative of b sub 3 with respect to b sub 3 is 1. now we just add everything up and the derivative of the predicted values with respect to b sub 3 is one so we multiply the derivative of the sum of the squared residuals with respect to the predicted values by 1. note this times 1 part in the equation doesn't do anything but i'm leaving it in to remind us that the derivative of the sum of the squared residuals with respect to b sub 3 consists of two parts the derivative of the sum of the squared residuals with respect to the predicted values and the derivative of the predicted values with respect to b sub 3. bam and at long last we have the derivative of the sum of the squared residuals with respect to b sub 3. and that means we can plug this derivative into gradient descent to find the optimal value for b sub 3. so let's move this equation up and show how we can use this equation with gradient descent note if you're not familiar with gradient descent check out the quest the link is in the description below anyway first we expand the summation then we plug in the observed values and the values predicted by the green squiggle remember we get the predicted values on the green squiggle by running the dosages through the neural network now we just do the math and get negative 15.7 and that corresponds to the slope for when b sub 3 equals zero now we plug the slope into the gradient descent equation for step size and in this example we'll set the learning rate to 0.1 and that means the step size is negative 1.57 now we use the step size to calculate the new value for b sub 3 by plugging in the current value for b sub 3 0 and the step size negative 1.57 and the new value for b sub 3 is 1.57 changing b sub 3 to 1.57 shifts the green squiggle up and that shrinks the residuals now plugging in the new predicted values and doing the math gives us negative 6.26 which corresponds to the slope when b sub 3 equals 1.57 then we calculate the step size and the new value for b sub 3 which is 2.19 changing b sub 3 to 2.19 shifts the green squiggle up further and that shrinks the residuals even more now we just keep taking steps until the step size is close to zero and because the step size is close to 0 when b sub 3 equals 2.61 we decide that 2.61 is the optimal value for b sub 3. double bam so the main ideas for back propagation are that when a parameter is unknown like b sub 3 we use the chain rule to calculate the derivative of the sum of the squared residuals with respect to the unknown parameter which in this case was b sub 3. then we initialized the unknown parameter with a number and in this case we set b sub 3 equal to 0 and used gradient ascent to optimize the unknown parameter triple bam in the next stat quest we'll show how these ideas can be used to optimize all of the parameters in a neural network now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on 