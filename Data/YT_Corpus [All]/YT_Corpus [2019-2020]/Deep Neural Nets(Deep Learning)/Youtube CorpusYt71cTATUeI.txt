 hey everyone my name is fabrice and this is the quick summary video for the paper is neuron coverage a meaningful measure for testing deep neural networks now for the most part we approach this question empirically first we introduce a novel diversity promoting regularizer that extends adversarial attacks to also increase neuron coverage we then use this regularizer to create over 2 000 test suites representing a broad cross-section of ml projects these suites are then evaluated against three key criteria that we argue are important for evaluating the meaningfulness of any test metric the first of which is defect detection which measures the number of defects found by a test scheme second is naturalness which measures how realistic the tests are and third is output impartiality which measures the degree to which a test generation technique biases certain classes over others during suite creation we then investigate whether there exists strong positive correlations between neuron coverage and these three criteria overall no more than five percent of our experimental results support the hypothesis that neuron coverage is strongly and positively correlated with defect detection naturalness and output diversity and this invokes skepticism that neuron coverage is a meaningful test metric our paper also includes a smaller scale comparison with other tools designed to generate tests while incorporating neuron neuron coverage maximization as a guiding objective the triangulated agreement in the empirical findings reduce the likelihood of our results being deterministically affected by our choice of coverage inducing technique and increases the generalizability of our main finding that neuron coverage exhibits limited correlation with the desirable test criteria of defect detection naturalness and output impartiality thank you 