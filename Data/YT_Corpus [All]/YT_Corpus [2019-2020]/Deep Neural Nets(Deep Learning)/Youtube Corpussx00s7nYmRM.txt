 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu ok so this is an important day and Friday was an important day I I hope you enjoyed professor's Fras terrific lecture as much as I did you probably saw me taking notes like mad for the section that's now to be written about stochastic gradient descent so and he he promised a theorem if you remember and there wasn't time and so he was going to send it to me or still is going to send it to me I'll report I haven't got it yet so I'll but I'll bring it to class wait and see hopefully about and that'll give us a chance to review stochastic gradient descent the central algorithm of deep learning and then this today is about the central structure of deep neural nets and some of you will know already what they how they're connected what what the what the function f the learning function it could call it the learning function that that's constructed the whole system is aiming at constructing this function f which learns the training data and then applying it to the test data and the miracle is that it does so well in in practice that's what has transformed deep learning into such a important application so so so this is this though Chapter seven has been up for months on the mascot mit.edu slash learning from data sight and I'll add it to stellar because that's where you'll be looking for it okay and then the second the backpropagation the way to compute the gradient I'll probably reached that idea today and you'll see what it's the chain rule but how is it organized okay so so what's the structure what's the plan for deep neural nets good start starting here so what we have is training data so we have vectors x1 to X what should I use for the number of training of samples that we have in the training the training data well Sadie for data ok and each vector those are called feature vectors so equals feature vectors so each one each X is uh has like m features so maybe it maybe I my notation isms isn't so hot here I have a whole lot of vectors let me not use the subscript for those right away so vectors feature vectors and each vector has got maybe so we say m features like if we were measuring height and age and weight and so on those would be features so the job is of the the job of the neural network is to create a and and we're going to classify maybe we're going to classify men and women or boys and girls so our let's make it a classification problem a just a binary so the classification problem is what should we say minus one or one which is sort of or zero or one or boy or girl or cat or dog or truck or car or anyway just two classes to two class so I'm just gonna do two class classification so the we know which class the training data is in for each vector X we know the right answer so we want to create a function that gives the right answer and then we'll use that function on other data people know so let me write that down create a function f of X that so that that gets most of gets the class correct in other words f of X should be negative for when when the classification is minus 1 and f of X should be positive when the classification is plus 1 and as we know we don't necessarily have to get every X every sample right that may be overfitting if there's some sample that's just truly weird by getting that right we're going to be looking for truly weird data in the test set and and that's not a good idea we want we want the rule that we're trying to discover the rule the covers almost all cases but not every crazy weird case okay so that's our job to create a function f of X that is correct on almost all of the training data yeah so before I draw the picture of the network let me just remember remember to mention the site playground I don't know if you've looked at that so I'm gonna ask you playground at tensorflow dot org how many know that site or have messed with it just just a few okay okay so it's not a very sophisticated site it's got only four examples for examples and and the yeah so the so one example is a whole lot of points there are blue B for blue inside a bunch of points that are another set that are old for orange orange but okay so those are the two classes orange and blue so the points X and the the feature vector is is the feature vector is here is just the X what the coordinates features are the X Y coordinates of the point and our job is to find a function that's positive on these points and negative on those points so there's a simple model problem and I recommend well just partly it's if you're a expert in deep learning this is for children but but morally here I certainly learned from playing in this playground so you you set the you you set the step size do you set it or does it say no yeah yeah I guess you can change it I haven't I don't think I've changed it what else do you said oh you you set the the nonlinear activation the nonlinear activation function active I'll say function and let me just go over here and say what functions people now mostly use the the activation function is called riilu pronounced different ways I don't know how we got into that crazy thing for this function that is zero and X so the function riilu as a function of X is the maximum the larger of 0 and X so it produces the point is it's not linear and the point is that if we didn't allow non-linearity in here somewhere we couldn't even solve this playground problem because if our classifiers were all linear classifiers like support vector machines I couldn't separate the blue from the orange with a plane I it's got to somehow create some nonlinear function which is maybe the function is trying to be a good a good function would be a function of r and theta maybe maybe r minus 5 so maybe the distance out to the that let's suppose that distance is 5 then r minus 5 will be negative on the Blues because R is small and our minus 5 will be positive on the oranges because R is bigger and therefore we will have the right signs less than 0 greater than 0 and it'll classify this the this data this training data yeah so so it has to do that this is not a hard one to do there are four examples as I say two are trivial it solves it finds a good function well yeah I've forgotten they're so trivial they've they don't shouldn't be mentioned and then this is the medium test and then the hard test is when you have oranges you have a sort of spiral of oranges and inside you have a spiral of blue satin that was a cooked up by a feed so you would so that the system is trying to find a function that's positive on one spiral and negative on the other spiral and that takes quite a bit of time many many epochs I learned what an epoch is did you know what an epoch is I I didn't know whether it was just a fancy word for counting the steps in gradient descent but it counts it counts the steps all right but one epoch is the number of steps that matches the size of the training data so if you have a million samples we're grading ordinary gradient descent you would be doing a million you have a million by in problem per step of course stochastic gradient descent just does a mini batch of 1 or 32 or something but anyway if we had yeah so so it's the number of you have to do enough mini batches so that the total number you've covered is the equivalent of one full run through the training data and that was interesting point did you pick up that point that in stochastic gradient descent you could either do it you could either do a mini batch and then put them back in the soup so with replacement or you could just put your data in some order from wonder zillion so here's here's of a first X and then more more X's and then just and randomize the order so you'd have to randomize the order for for stochastic gradient descent to be reasonable and then take a mini batch and the mini batch in a mini batch in a mini batch and when you get to the bottom you've finished one epoch and then you'd probably randomize again maybe if you were if you were doing it if you wanted to live right and go through the mini batches again and probably do a thousand times yeah anyway so I haven't said yet what you do what this f of X is like but you can sort of see it on the screen because as it creates this function f it kind of plots it and what you see on the screen is the is the zero set for that function so it perfect would be for it to go through zero if I had another color oh I do have another look this is the first time the whole semester blue has appeared okay so so if it if the function was positive there in the in this part on the Blues and negative outside that region for the oranges that would be just what we want right that would be what this little playground site is creating and on the screen you'll see it you'll see this this curve where it crosses zero so that curve where it crosses zero is supposed to separate the two sets one set is positive one set is negative where it's zero is in between and the point is it's not a straight line because we've got this nonlinear function this this this is nonlinear and it allows us to to have functions like our minus five and so at at five that's where the function would be zero and that you'll see that on the screen you might just go to tents or playground at tensorflow of course tensorflow is a big big system this is the their child's department but it's I thought it was pretty good and then on this site you you decide how many layers there will be how many neurons in each layer so you create the structure that I'm about to draw and and you won't be able to get to solve this two problem to find a function f that learns that data without a number of layers and a number of neurons if you don't give it enough you'll see it struggling but it won't it just it's creating that the the zero set tries to follow this but it but it has it gives up at some point this one doesn't take too many layers and the two trivial examples just are just a few neurons do the job okay so now so that's a little comment on on one website if others father if you know other websites that I should know and should call attention to could you send me an email I you know I'm just not aware of everything but out there or if you know a good a good convolutional neural net CNN that is available to practice on where you could give it the training set that's what I'm talking about here I'd be glad to know because I I just don't know all that that I should okay so what does the function look like well as I say linear isn't going to do it but linear is a very important part of it of the of this function f so the function f really has the form well so we've got we start here with a vector of one two three four M is five this is the this is the vector X five components okay so let me erase that now okay so then we have layer one with some number of points let's say N one is six neurons let me make this simple I'll just have that one layer and then I'll have the output this will be the output layer and it's just gonna be one number so each of these so I'm gonna have a matrix a one that takes me from this a one will be this will be 5 by no six by five because I want six outputs and five inputs six by five matrix so I have 30 weights to choose there and so the the the y that comes out is gonna be y 1 will be a 1 times X 0 so X 0 is the is the feature vector with five components so it's so that's a purely linear thing but we also want a an offset function offset vector so that's a vector this is then this is that was a y that's coming out has six components the a one is six by five the X naught was five by one and then of course this is six by one so these are the these are the weights well these are these yeah I'll call them all weight weights to compute so in this so these are connected usual picture is to show these all these connections I'll just put in some of them so in here we have thirty plus six parameters thirty-six parameters and then I'm going to close this it's gonna be a very shallow thing so that will be just just one by six yeah just okay right so we're just getting one output so it's that's just a vector at this final point but of course the the whole idea of deep neural Nets is that you have many layers so you have that 36 more realistically as in the tens of thousands and you have it multiple times and the idea seems to be that the first layer that you can sort of separate what layer one learns about the data and from what layer two learns about the data layer one there's a one apparently by just looking after the computation this learned some some basic facts about the data this learns the next next a to which would go in here would learn more detail and then a three would learn more details so we would have a number of layers and it's that construction that has made neural net successful but I haven't finished because right now it's only linear right now I just have alcohol at a two in here right now I would just have a matrix multiplication apply a 1 and then apply a 2 but in between there's a is a one by one action on each by this function so that function acts on that number to give that number back again or to give zero so in in there in there is real ooh right in there in this it becomes real ooh on each six six copies of reacting on each of those six numbers right so that really x1 comes from y1 by applying riilu to it then that gives the X so here are the Y's from the linear part and here are the X that's why ones that's a vector y1 the linear from just a linear plus an affine map linear plus constant that's a fine and then the next step is component by component we apply this function and we get X - X what yeah and then do it again and again and again so do you see that the function how do i describe now the function f of x so the learning function which depends on the on the weights on the a's and b's is so I start with an X I apply F 1 a 1 to it yeah let me do this so what what this is the function f of X f of X is going to be F 3 let's say of F 2 of F 1 of X 1 2 3 parentheses right okay so it's a chain you could say F is a what's the right word for a chain of functions if I take a function of a function which is the reason I use the word chain the chain rule gives the derivative so a function of a function of a function that's called composition composing function so this is a composition I don't know if there's a standard symbol for starting with F 1 and do some composition and do some composition and now what what are those separate F those separate FS are the or the so the separate FS are the F 1 of a vector would be in tunes the real part the nonlinear part of a 1 X 0 plus B 1 so 2 parts you do the linear or affine map on your feature vector and then you component by component you apply that nonlinear function and it took some years before that nonlinear function became a big favorite people imagine that it was better it was important to have a smooth function so the original functions were sigmoids like s curves but but of course it turned out that experiment showed that this worked even better yes so that would be F 1 and then F 2 would have the same form and F 3 would have the same form so maybe this had 36 weights and the next one would have another number and the next another number so we're getting you get quite complicated functions by composition by you know like e to the sine of X or e to the sine of the logarithm of X or things like that pure masses asked could you get what functions can you get you know try to think of them all now what kind of functions do we have here what can I say about f of X as a function as a math person what what kind of a function is it so it's created out of matrices and vectors out of a linear or affine map followed by a nonlinear by that particular nonlinear function so what kind of a function is it well I've written though with those words down up here at f of X is going to be a continuous piecewise linear function because every step is continuous that's a continuous function linear functions are continuous function so we certainly have we're taking a composition of continuous function so it's continuous and it's piecewise linear because part of it is linear and part of it is piecewise linear so so this is some continuous piecewise linear function function from from of X X and M dimensions okay so one one little math question which I think helps to understand to Mike like to swallow the idea of a chain of of the kind of chain we have here of linear followed by riilu so so here's my question this is a question I'm going to ask and by the way back propagation is certainly going to come when Z rather than today that's a major topic in itself so let me keep going with this function could you get any function whatsoever this way well no you only get continuous piecewise linear functions that's a it's an interesting case let me just ask you well one of the exercises says if if I took two continuous piecewise linear functions do you have a the next 20 minutes are an attempt to give us a picture of the graph of a piecewise linear function so what what say in say a function of two variables so I have M equal to two and I draw its graph okay help me to draw its graph so this would be a graph of f of x1 x2 and it's going to be continuous and piecewise linear so what does its graph look like that's that's the question what's the graph of a piecewise then your function looks like well it's got flat pieces right in between the change from a Medusa piecewise that means it's got different pieces but within a piece it's linear and the pieces fit next to fit with each other because it's continuous so I visually is that the well it's like origami this is the theory of origami almost so right origami you take a flat thing and you fold it along straight folds so so what's different from origami maybe not much well maybe origami allows more than we allow here origami would allow you to fold it up and over so origami would give you a multivalued thing because you know it's got a top and a bottom and other folds this is just going out to infinity in flat pieces and the question will be how many pieces so let me ask you that question how many pieces do I have you see what I mean by a piece so so I'm thinking of a graph that has these flat pieces and they're connected along straight straight edges and those straight edges come from from the real ooh operation that well that's got two pieces actually we could do in one day and one day we could count the number of pieces pretty easily so what would what would be a piecewise linear let me put it over here on this side and erase it - okay so here's a maple one a continuous piecewise linear f I'll just draw its graph so okay so it's got straight pieces straight pieces like so yeah you've got the idea it's a broken line type sometimes people say broken line but that I'm never sure that's a good description of this piecewise linear continuous so it's continuous because the pieces meet and it's piecewise linear obviously okay so that's the kind of picture I have for a function of one variable now that my question is as an aid to try to visualize this function in 2d is to see if we can count the pieces see if we can count the pieces yes and that for that so that's in this notes I found it in a paper by five authors for a from for a meeting and so actually the whole world of neural nets it's it's the conferences every couple of years that that everybody prepares for submits more than one paper so it's kind of a piecewise linear conference and those are the the big conferences okay so this is the back propagation section and I want to look at this at the okay so I'm gonna show this paper by Kleinberg and for others Kleinberg is a he's a computer science guide Cornell he was a PhD from here in math and he's a very and significant person on not on not so much on neural networks as just the whole this whole part of computer science right so anyway they and other people to have have asked this same problem suppose you have suppose I'm in two variables and suppose so what are you imagining now for the surface of the graph of this of f of X and why it has these it has these lines fold lines right I'm thinking it has fold lines so I start could start with a complete plane and I fold it along along one line so now it's like Ray Lu is one play half plane they're going to a different half plane there everybody with it and now I take that function that surface which just has two parts and I put in another fold okay how many parts have I got now I think four am i right four parts yes because this will be different from this because it was folded along that line so these will be four different pieces they they have the same value at the center there and this and they match along the lines but there's so the number of flat pieces is four for this so that's with two folds and now I just want to ask you with M folds how many pieces are there can I get up to three folds so I'm gonna look for the number of folds so let me let me just use a get a notation maybe R R is the number of number of flat pieces and M is the dimension of X in my picture it's 2 and n is the number of folds so let me say it again I'm taking a plane I'll fold it plane because the dimension was 2 I'll fold it fold it n times how many pieces how many flat pieces so this would be this would be a central step in understanding how close their function you know what freedom you have in the function f for example can you approximate any continuous function by one of these functions f by taking enough folds seems like the answer should be yes and it is yes so that's the that's one big for pure math that's one question is the is this class of functions universal so the universality theorem would be to say that any any function sine X whatever could be approximated as close as you like by one of these guys with enough fold and over here we're kind of making it more numerical we're going to count the number of pieces to just to see how quickly do they grow so what happens here so I have four folds now this right now I have n equal to I mm is two here in this in this picture and I'm trying to draw the surface and here I've put in two did I take in yeah two folds and now I'm gonna go up to three folds okay so let me fold it along that line how many pieces have I got now let's see can I count those pieces is it seven so what is a formula what if I do another fold what happens what end we do another folder yeah yeah maybe that's gonna be it it's a kind of nice question because it is care to visualize this thing okay so what happened how many how many of those lines will be if I put in a fourth line how many yeah how many new folds do I create that's kind of the question and I'm assuming that fourth line doesn't go through any of these points it's sort of in general position yeah so I put in a fourth line done it there it is okay so what happened here how many new ones did it create how many new ones did it create let me make that one green because I'm distinguishing that's the guy that's added after the original we had we had seven we had seven pieces and now we now we've got more was it seven it was wasn't it one two three four five six seven but now how many pieces have I got or how many how many pieces that did this did this new line create that's what we want to build it up use a recursion how many pieces did this new new well this new line folded created one new piece there right one new piece there one new piece there one new piece there so there are four new pieces okay yes so there's some formula right that's gonna tell us that and now what would the next one create well I've got now I have one two three four lines so now I'm gonna put through a fifth line and that will create a whole bunch of pieces I'm losing that losing the thread of this argument but you're onto it right yeah so any any suggestions yeah okay yes there that's right so there's a recursion formula that I want to know and I learned it from Kleinberg paper and then and then we have an addition to do so the recursion will tell me how much it goes up each with each new function and then we have to add okay so the recursion formula let me write that down so this is our of and and M now I'd like to find a formula for it's the number of flat pieces in M with an M dimensional surface well we're taking em to be two and n folds and let's just so an equal 1 2 3 let's let's write down the what the numbers we know with with one fold how many pieces too good so far so good with one fold there were two pieces so this is the count R and then with two folds how many we've gone past that point so can we get back to just those two was it for okay thanks now when I put in that third fold how many did I have without the green line yet seven was it seven and when the fourth window went in that green went how many have I got in this picture so the question is how many new ones did I create I guess so that that that line got chopped into that piece that piece that piece that piece four pieces for the new line four pieces for the new line and then each of those pieces like added added a flat bit because it that piece from here to here separated these two which were previously just one piece one flat piece they followed it on that line I folded on this I folded there I think it went up by four to eleven so now we just have to guess a formula that gives those matches those numbers and then of course we really should guess it for any M and any N and I'll write down the formula that they found it's it involves binomial numbers every everything in the world involves binomial numbers because they satisfy every identity you could think of so here's their formula R of with enfolds and we're in M dimensions so we believe early and our thinking had m equal to 2 but we should you know grow up and get em to be 5 dimensional so we have a 5 dimensional let's not think about that ok so so it turns out it's binomial numbers and 0 & 1 up 2 and M so so for m equal to 2 which is my picture it's ends and 0 plus N 1 plus n 2 and and and what are these what is that into mean for example what's that that's a binomial number I don't know if you're keen on binomial numbers some people their whole lives go into binomial numbers so so it's something like is it is it n factorial divided by n minus 2 factorial + 2 factorial I think that's what that number means that's the binomial number so this would be I go up to so at this point I'm hoping to get the answer 7 I think right now I've put in I've gone up to am equal to I mean Emmy put that I've gone up to - so I've yeah so I think yeah I've obviously allowed for 3 3 cuts and they are when we had just 3 was 7 so is that is this so so this is now I'm taking em to b3 and I'm hoping for hope answer is seven so what is 3 what is 3 so I add these 3 things so what is 3 the binomial number 3 with 2 forgotten how to say that I'm ashamed to admit 3 choose 2 thanks I knew there was a good way so what is 3 choose 2 well put it in 3 and 2 it's in there already so it would be 6 over 1 times 2 this would be 3 would that be 3 and what is 3 choose 1 how do you know that you probably right mm-hm 3 I think yeah oh yeah probably a theorem that that if these add to 3 yeah so so this I'm doing N equals 3 okay so we yeah I agree that's 3 and what about n to 0 what's that you have to live with 0 factorial by 0 factorial is by no means 0 what so what is 0 factorial 1 yeah I remember when I was an undergraduate having a bet or math I won but he didn't pay off so yeah so so 3 this is 3 factorial over 3 factorial times zero fighters so it's 6 over 6 times 1 so isn't one yeah so 1 in 3 and 3 makes 7 so it proves their formula well done quite proved the formula but their way to prove it is by an induction and I'll just if you like this this stuff the the recursion that you use induction on which is just what we did now what we did here here comes in at number four and it cuts through oh yeah and then we just counted the the four pieces there so yeah so they let me just tell you what they are of N and M the number we're looking for is the number that we had with one less cut so that's the previous count of flat pieces plus the number of the plus the number that was here was for the number of pieces that cut that and that's our of n minus 1 M minus 1 yeah and yeah I won't go go further times up but this the and that rule for recursion is proved in the in the section 7.1 taken from the paper by Kleinberg and others yeah so ok I think this is I don't know what you feel this for me this like gave me a better feeling that I was understanding what kind of functions we had here and so then the question is to with this family of functions we want to choose two we want to choose the A's and the weights the A's and the B's to give to to match the training data and so that we have a problem in in minimizing the total loss and we have a we have a gradient descent problem so we have to find the gradient so that's Wednesdays job Wednesdays job is to find the gradient of F okay and that's backpropagation good thank you very much that's that's 7.1 is done 