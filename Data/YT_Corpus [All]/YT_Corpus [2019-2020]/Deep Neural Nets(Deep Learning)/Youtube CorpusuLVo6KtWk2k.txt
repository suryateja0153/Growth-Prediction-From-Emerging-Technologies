 [Music] today we will talk about auto regressive models and this is continuing on our journey towards deep generative models we've already seen a few our beams and not few - actually our beams and various new autoencoders today we look at one more family of deep generative models known as auto regressive models and in particular we look at two architectures made and made I'll tell you what they mean they're not just made up terms and so let's start with the first one which is neural auto regressive density estimator okay so let's look at each of these terms I hope the first term is obvious to everyone by now I guess density estimator what does that mean probably good not physics okay so this just means that we want a neural model for estimating probability densities okay and what does auto regressive mean of course you don't know right if you knew all these and why am I here so I will we look at that and see what auto regressive means right so so far we have seen a few latent variable models fix our beams and variational auto-encoders and these models make certain independence assumptions by relying on the latent variables so for example both in the case of our beams and the A's we made some assumptions in the case of our beams we said that given the latent variables the visible variables are independent of each other so once you bring in this latent variables and the idea is that the number of latent variables is actually much smaller than your total visible variables and you get rid of the dependencies between the visible variables by just assuming that given the latent variables all of these are independent so that largely simplify is your factorization and reduces the number of parameters in your model and in fact even though it was not so obvious in the case of variation autoencoders we made a similar assumption there also we very smartly said that P of X given Z actually comes from a normal distribution whose covariance matrix is identity that's the same as saying that given the Z the X's are independent of each other right hence the Sigma is equal to I it so we brought in these latent variable models and they had in our eventual goal which was towards having a tractable model of course even then there were issues it's not that just bringing in the latent variable model solved everything despite having latent variables we still had to do this expensive MCMC sampling in the case of our beams and even in the case of VA s we had to get rid of this integral or summation or expectation by resolving to this variational inference rate which tries to maximize the lower lower bound instead of actually working with the original objective function right so just having latent variables is not enough on top of that also we had to do certain things now we look at Auto regressive models which do not contain any latent variables okay the name the aim of course remains the same that we are still given a bunch of variables X all right so this is x1 to xn and in our example this has been explained to x1 0 to 4 because you have 32 cos 32 images and you want to learn a Joint Distribution of that and as usual for ease of illustration we'll just assume that this X belongs all these X's are binary right so your X comes from 0 comma 1 raise to N instead of R is 2 n okay that's easier for us to illustrate it now ER models actually do something very bold they assume that there are no independence assumptions that's a really kind of eluted way of seeing it so they assume no independence is so the what they assume instead is the natural factorization and that's not an assumption they just break down this probability distribution using the chain rule and you saw the original chain rule had this X 1 priority of X 1 into X 2 given X 1 into X 3 given X 1 X 2 and so on so in general one of the factors in this joint in this factorization is probability of X i given X 1 to X minus 1 right and how would you represent that as a graphical model what are the nodes in your graphical model x1 Direction what are the edges for X I what are the edges it's connected to everything from x1 to X I minus one and it connects to everything that comes after it it's a parent of everything that comes after it and a child of everything that comes before it right that's what this figure is trying to illustrate okay so every node depends on all its previous notes and all these subsequent nodes are dependent on this no that's very simple a that's very straight for just convert this factorization into a graphical model okay but this is like really frustrating right we had this entire saga of three weeks where we said that we cannot work with this joint factorization because it has an exponential number of parameters and after doing all that epic saga and back to the basic factorization and seeing that auto regressive model spoke with that right so this is really expensive in fact the total number of parameters that you have in this factorization is order tourists to in there were some of the factors have a large number of parameters right so everyone gets that so this have the first factor has one parameter the second has two four and so on right so it's a exponential number of factors that you get so what does I mean I started with saying that this is bad this is bad this is bad we did a lot of stuff along the way to explicitly avoid this kind of factorization and now we are back to this so how does this make sense so this is infeasible but the way auto regressive models work around this is that they use a neural network to parameterize these factors and then learn the parameters of this neutral network again nothing new what does this sentence actually say it is giving you the - for learning a joint distribution the standard recipe right this was the recipe that we discussed that if you want to learn a joint distribution you can say that there are certain parameters associated with the distribution the parameters in turn can be functions of some other parameters and then you learn the those parameters and that's exactly what this sentence is saying in short right so again that does not really explain how we are going to get rid of this exponential number of parameters so let us see what this exactly means and how it allows us to bypass this problem okay so in terms of neural network right so for the time being just ignore this part actually we can remove this from this slightly so what do I want I want a neural network where I have n outputs each of these outputs predicts one of the conditional probability distributions that I am interested in okay there are n conditional probability distributions in my factorization I will have a neural network which has n outputs each of these outputs will predict P of X i given X 1 to X I minus 1 okay without telling you anything else can you tell me what is the function that you will use for each of these output nodes right so remember we used to do this right what's the function for input it was sigmoid tan it and so on then what's the output function then what's the loss function right so I'm just bringing that back without me telling you anything else I'm just telling you that the job of the neural network is to predict these conditional probability distributions and further we are assuming that all these variables are binary okay so now can you tell me what is the output function that you use for the output layer what do you expect the first node to predict a value between 0 and 1 the second node so softmax we need a probability distribution right so we should have a soft match why not these nodes are all independent of each other these are all separate factors there some need not be one so we don't need us off max yeah what do we need we need n sigmoid functions each of these sigmoid functions will predict a value between 0 to 1 which will tell us what's the probability of X I taking on the value 1 given X 1 to X I minus 1 so is the output layer clear to everyone is that fine please raise your hands if it is clear ok so you should understand why we don't need a salt max function and Y will have n independent sigmoid functions ok fine now at the input again you are given these x1 to xn right so this is the pixels that are given to you and what you want to predict is 12 X i given X I - one okay but the catch is that the NH output should see only inputs round 1 to n minus 1 why is it so I am saying that the net output or the I at output should only see inputs from 1 to I minus 1 why is it so because that's the given part right the AI at output is predicting P of X i given X 1 to X I minus 1 so given means what was the input given to me so the input was only X 1 to X I minus 1 so when I am predicting this probability I should only be looking at the inputs X 1 to X I minus 1 does that make sense in particular see if I give you X I also then what's the point of predicting what is P of X say that I have already seen that what's the what am I trying to predict does that make sense right so the given part tells you what is the input given for this computation so you should only be looking at X 1 to X I minus 1 okay now if I were to use a fully connected neural network that means I start with the input layer I have some hidden layers and then I go to the output layer will that be okay I'm saying if I had a fully connected layer I start with the input layer think of the multi-layer perceptron that we did right so we have the input layer we have a bunch of hidden layers and then the output layer and everything is fully connected will that be okay for the solution yes or no everyone why yes so by definition it's fully connected so every guy in the hidden layer sees all the inputs which in turn means every guy in the next hidden layer has seen all the inputs and you can argue this up to the output layer every guy in the output layer has seen all the inputs and that's exactly the problem that I want to avoid I don't want every guy in the output so to see all the inputs I just wanted to see the relevant inputs okay so that's why a fully connected network does not work so this neural autoregressive density estimator has a simple solution for this for a three output unit we compute a hidden representation using only the relevant inputs okay so let's look at x3 this is the output unit that so I want to first compute a hidden representation okay I'll have a matrix W so think of this W as the fully connected matrix okay but out of those entire W that entire W I'm going to only look at the first less than K columns okay and from the input I'm also going to look at only the first less than K columns does that make sense okay so that means what does that mean my H K only depends on X 1 to XK minus 1 so I have ensured that the hidden representation which I am computing has only seen a selected number of inputs and in particular only those inputs which I was allowed to see because this is a k-8 output I should have seen only inputs up to K minus 1 does this computation make sense to you right it does I'm just looking at the first K entries of the input ok fine and now we can compute the output so I'm interested in the Kieth output which actually gives me the priority of XK given X 1 to K minus 1 and I'm going to compute it as a function of H K which in turn is only a function of X 1 to XK minus 1 so everything kosher up to this point I have not seen anything that I was not supposed to see and VK and CK are just parameters so that doesn't matter right so I if I do this kind of a computation so what I'm doing in effect is for every output I am first computing a hidden representation which looks only at the relevant inputs right and once I make sure of that whatever I do with the hidden representation is going to be all fine because the hidden representation has seen only the relevant inputs hence the output computed using the certain representation will only see the relevant inputs is that fine with everyone how many few clear with this please raise your hands top and I okay good so now let us look at this equation carefully so you have h k is equal to first K columns of the W matrix multiplied by the first K entries of the X vector and then you have this prediction ok how many parameters does this model have notice something there is no suffix associated with wnb which means they are shared across all the outputs but there is a subscript associated with we can see k okay and I've asked you the question what's the number of parameters I also need to tell you something about the input and the output so we will assume that X belongs to 0 1 raise to n that means X is n dimensional right it does not matter whether 0 1 or what but it's n dimensional and I will assume that H belongs to our D so it's a d-dimensional vector okay now you need to tell me how many parameters does this model have to start counting and why am I asking you this question what will I have to prove to you that this does not have exponential number of parameters I've somehow got rid of exponential number of parameters right so how many parameters does this model have what is the size of W D cross n what's the size of B D everyone please there's a second last lecture of the course I expect you to do this how many parameters is WI everyone should answer everyone n cos T how many parameters does behalf be okay so you have D cross N and D cross 1 okay and the same W and B are used for all the outputs okay how many parameters does the output layer have what's the size of VK what's the size of VK a be one please how many forget that it's 1 cos D these are hands up and I okay how many such V's do you have and what's the size of CK okay it's 1 sorry yeah so what's the total number of parameters here and into and into D plus one okay so that's number of parameters that you have in the output layer is there any other parameter obvious from the figure how do you compute H - how do you compute H - you take the first column of W the reason I'm asking you this question is because next I'm going to ask you how do you compute H one how do you compute H two is clear to you how do you compute H one does this start H one remind you of anything else that you have seen before where have you seen a such a similar thing are n ends and n is teams you had this edge zero state what did we do with the zero state we made it our parameter right so similarly H one is going to be a parameter here what's the size of H 1 D so what's the total number of parameters that you have I think I've goofed up here yeah so you can so okay let's just do it right so we have n plus 1 into D here okay so n plus 1 into D in the input layer then what was the next thing okay so let's just call it n D plus D and the next one was nd plus n and then another D right so this is what you had in the input layer this is what you had in the output layer and this is your h1 okay so then in effect how many parameters do you have 2 ND + 2 D + n right so it's order n so do you have an exponential number of parameters no so how did this happen we started with a factorization which is an explicit factorization and we have been arguing since the beginning of this section that such a factorization will have an exponential number of parameters so why don't you have an exponential number of parameters here we shared the parameters right that's one reason I don't have exponential number of parameters okay so now let's just do one more thing right before we move so I just wanted me make sure that all of you are clear with this computation so I had this W less than K into X less than K plus B okay so let's actually see what that means so what's the size of this output going to be this belongs to what Rd okay fine so now when you are doing say let's just this is x1 x2 x3 okay and then you have this W 1 1 W 1 2 W 1 3 okay now I said that you just take the first K columns right so this would be if K is equal to two then you will take the first column and you'll also take the first row from here what will this multiplication give you a scalar a vector a tensor a vector right so that's exactly what is happening here but the right way to implement this is actually the following that you take your input and you mask it okay then whatever vector you get which is essentially going to be x1 0 0 so then you can directly do this matrix vector multiplication and that's the same as this operation but that's the more neater way of doing it take your input mask it and then multiply by your weight matrix how many of you get this okay so that's what W less than K and 2x less than K meats so just get familiar with this concept of masking we'll be using it soon again ok fine how will you train this network how will you train this network second last lecture of the course Yunus I mean don't pretend you just know one algorithm that you can use for training and we don't think as if you have 100 algorithms on which again choose right there's only one algorithm that you know what is that back propagation well after all this is a neural network right what else are you going to use to train a neural network so if I say back propagation what do I need to define loss function what's the loss function what are you trying to predict 12 distribution so what's the loss function going to be cross-entropy sum of course enterpise third option is cross entropy of sums what is the loss function going to be I don't have the problem with the answer I have a problem with the confidence associated with the answer what's the loss function going to be still I have a problem with the confidence associated with the answer only one contouring game it's going to be some of the cross entropy why hesitate so let's look at one of these guys okay now this you need to understand how everything falls in place so what is your training instance it's x1 to xn given to you okay that means you know the true probability distribution that you need to predict at x3 suppose X 3 is equal to 1 at the input okay what's the you said cross entropy so for cross entropy you need the true distribution and the predicted distribution all I'm asking you is what's the true distribution if X 3 is equal to 1 then what's your true distribution what kind of a random variable is X 3 a binary random variable if I ask you the distribution of a random value what do you need to give me how many values do you need to give me to probability of it taking on the value 1 taking on the value 0 I am telling you that it was one so what's the two priority distribution zero one the same as the two labels right we have seen this a million times is that fine and what's the predicted probability distribution it's whatever you get here right so here you're going to predict some probability and one minus that property right so let's say it's 0.7 and point three now what's the loss function going to be cause entropy between P and Q how many such loss functions will you have this is the one associated with this prediction how many such loss functions do you have n of those so a total loss function is going to be sum of these n loss functions is that clear was it so hard say at this point I expect you to come up with these answers I mean it's like very straightforward and it's it's not satisfying if at this point you need to me to answer these questions okay so that's exactly what's written on this slide so the last function is well defined we know how to deal with the cross entropy loss function we know back propagation the only catch here is that during back propagation the gradient should flow only to the only to the only to those connections which were active this is similar to something that you have seen in in loaded someone is seeing a list geum's something similar to what you have seen in dropouts right so in dropouts also you remove some connections and then you just make sure that the gradients flow through those connections and in practice all while implementing the way you will achieve this is is always define this mask matrix so during forward propagation also the mass would be active and during backward propagation also the mask would be active is that clear okay so just need to make sure that the gradients only flow along these two paths and not to any of these other guys okay is that fine okay now let's ask a few questions about this model so whenever you are talking about generative models what are the two things that we have been interested in every one abstraction and generation so let's ask the first question can this model do abstraction that is once the model is strained I have learned W B all the v case and all the C case and I know I can do that because this is a neural network I have defined the loss function I can do back propagation once I do that if I give it a new X can I ask you to compute a hidden representation what's the hidden representation of X in each forward pass how many hidden representations does this network compute in each forward pass how many Aryan representations does it compute n right H 0 2 HN minus 1 or H 1 2 HN whichever you want to look at it so given an X you are computing n hidden representations now I'm asking you give me a hidden representation which one will give me this is where you have to start bringing everything together right so the idea of hidden representations the first time we looked at it was in the context of auto-encoders ok the idea always behind and hidden representation is that it should capture the semantics of the input ok now which of these n hidden representations that you have computed captures the semantics of the input semantics of the input the way we have semantics is of course very vague we have never defined it but at least in terms of auto encoder you know what we meant or it given that hidden representation we could reconstruct the output so which of these n hidden representations is the abstract representation of your input the last one the first one all of them none of them last one what is the last representation actually capturing what is the job of the last representation given H 1 to H n minus-1 reconstruct sorry given X 1 to X n minus 1 reconstruct X n is that good enough for you what did the hidden representation in auto-encoder do reconstruct X 1 to X n each of these hidden representations that you see here is only capable of constructing one of the inputs is that good enough so these models are not latent variable models by design they are not meant for abstraction right you can of course come up with arbitrary things that you could take a concatenation of all these and these are but that's going to be a very large vector n in 2d or you could take a sum of these are an average of these and so on but the fact is that these are not latent variable models they do compute some hidden representation but by design they are not meant for abstraction ok you could come up with ways of constructing hidden representations from this H 1 to H n but that's not very natural in the sense of how we did it in the case of auto-encoders how we did it in the case of our beams and how we did it in the case of vs does that distinction make sense everyone raise your hands if you make sense ok so that's why these are not latent variable models and they are not designed for abstraction what's the second question I'm going to ask can we do generation what does it mean to do generation no I just want to generate samples how will you do generation so you're saying you have an independent binary random variables you could a sample independently from them so why do you need training and all that compute the probabilities give me a procedure to sample from this distribution I have actually given you an explicit distribution right given okay let before I talk about sampling if I give you an X right so I had asked you to train this on m-miss digits and at test time I give you a new image and I ask you to compute P of X can you compute it can you compute P of X where X is a vector actually or a matrix right it's an image can you compute the probability of X using this model yes no everyone how you will compute all the will compute all the all the n outputs these are factors in your Joint Distribution you just take the product of them and that gives you the total probability or the probability of the entire thing is that fine how many if you get that okay okay that was just a separate question now my other question is that how do you sample from this distribution how do you generate a new image first question can you generate the entire image at one go yes no no what's the generative process going to be think in terms of graphical models first find X 1 given X 1 find given X 1 and X 2 ok now tell me how will you do it not randomly initialize H 1 was a parameter of the model its trained sample from px1 but how do you compute px 1 H 1 is remember X 1 only takes H 1 as the input and what was H 1 a - of the model a parameter of the model that means we have done training right I am assuming that the training is done so what's the first thing I'm going to do I'm going to compute the priority of x1 equal to 1 using this formula is everything here known V 1 is a parameter H 1 is a parameter C 1 is a parameter ok I have computed this probability now what do I do sample from what does that mean how will you sample what kind of a distribution is this Poisson distribution richly distribution beta distribution normal distribution everyone Bernoulli distribution how do you sample from a Bernoulli distribution this will give you a value right say let's value is 0.4 allows this a million times that ask is this another million times if it's required how will you sample from this you will sample from a uniform distribution you'll get a value between 0 to 1 if the value is less than 0.4 you will set it to one else to 0 so now you have x1 generate you do you have the first pixel generated now what will you do you will set x1 to that value and what will you do next now you can compute h2 using X 1 you have a value of x when you have sampled it once you have X 1 you can compute H 2 because X 2 only depends on X 1 and the parameters of the model which you have already learned so again you will get some value say 0.6 again you will sample from this Bernoulli distribution and what will you get you'll get a value of 0 or 1 and that you will put it into X 2 you will continue this process one random variable at a time or one pixel at a time and generate the whole image or set the configuration for all the random does that make sense is everyone clear about how you will sample from this distribution right sample one value at a time obviously this is very efficient inefficient inefficient and is going to be very slow again nothing wrong with it all these models come with their own pros and cons so this is one disadvantage of this model the sampling is going to be very very slow you'll have to do a lot of computations for generating one sample from this distribution okay what's the advantage you are working with an explicit or exact factorization you're not making any independence assumptions okay fine now notice that this model requires many many computations right because every time you have to first compute a hidden representation and then compute this YK right okay but the good thing is that if I want to compute this for the K plus 1 at unit I can just use whatever I'd done for the Kieth unit and just add one simple thing to it so it should be no actually right does that make sense beside these three columns here and these three rows here so the first guy only depends on this product the second guy is actually a sum of this plus this second product right so once I have computed the first guy I can just reuse that computation right this is this follows from basic linear algebra or matrix multiplication everyone gets that right how many if you don't get this how many if you get this so you can just go back and look at it so you can do some of these computations efficiently it does not take care of the entire problem but at least something can be done efficiently okay okay so the things that you need to remember about need are that it uses an explicit factorization for the joint probability distribution each node in the output layer corresponds to one factor in this explicit representation it reduces the number of parameters which would otherwise have been exponential by sharing the parameters wnb right you have shared the parameters in the neural network there's it is not designed for abstraction it does compute some hidden representations but it's not clear whether they are the idiot representations that you really want the Guru's you could do something on top of that and try to get a hidden representation but by design it's not meant for abstraction and generation using the model is going to be very slow because it's going to generate one pixel at a time and it's possible to speed up this computation by using the previously done computation that's what I saw Ronnie last right so that's in a nutshell noodle autoregressive density estimator what's auto regressive noodle and density estimator is fine Auto regressive it's using the previous prediction to predict the next prediction right regression is fine auto comes from the previous query is that fine okay [Music] [Music] 