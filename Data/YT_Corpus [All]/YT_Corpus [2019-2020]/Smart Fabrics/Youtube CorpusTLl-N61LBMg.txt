 where a person is looking is an important social cue in human human interaction allowing someone to address a particular person in a conversation or specify a focus of interest prior computing systems that have used gaze location have either used sensors worn on the head unlikely for consumer adoption or cameras which have significant privacy implications in settings such as the home in this research we explored whether speech could be used as a directional communication channel much like visual gaze what is the weather? instead of a device's microphone simply receiving and processing spoken content the direction of voice is also inferred note this is different from existing direction of arrival algorithms which calculate from where a voice originated in contrast our approach calculates the direction along which a voice was projected this innately enables voice commands with addressability in much the same way as gaze but without the need for cameras this not only allows users to dispense with wake words but also more readily and naturally interact with increasingly dense ecosystems of smart devices our approach relies on two fundamental properties first built environments introduce characteristic multi-path effects when a user speaks towards a device the first loudest and least distorted signal to arrive is the original sound which took a direct path all other sounds which have echoed and scattered off of various surfaces in the environment are delayed quieter and more distorted this effect is apparent to the naked ear what is the weather? what is the weather? what is the weather? what is the weather? what is the weather? what is the weather the second property on which our approach is built is that the distribution of human speech frequencies varies by spoken angle what's the temperature? what's the temperature? in particular higher frequencies are more directional and also tend to be absorbed by the environment leaving the more omnidirectional lower frequencies to reach the microphone this creates characteristic ratios between higher and lower speech frequencies our machine learning model leverages features derived from these phenomena to predict direction of speech when does my bus come? when does my bus come? how is it outside? how is it outside? what's the weather outside? and more coarsely if a user is facing or not facing a device how's the weather? how is the weather? our model can run on a variety of devices and offers a new rapid and intuitive way to address distributed voice first smart devices please see our paper for full details 