 Without further ado I'd like to introduce  you to our speaker Duncan Curtis who's VP   of products at Samasource. Duncan brings nearly  15 years of experience in product development   most recently acting as a product management  lead for autonomous vehicle startup Zoox.   In addition, during his time at Google Curtis  impacted the gaming experience of over 1 billion   active daily users with his work on Google  Play games. As a product leader at Samasource,   Curtis leverages his expertise in computer  vision and autonomous vehicles to supercharge   the summer hub annotation platform with deeper  incorporation of machine learning. This evening   Duncan will be presenting on the challenges  with 3D LiDAR annotation and 3D datasets. Over to you Duncan, if you'd like to share  your screen. Yeah I'd love to thanks.   Of course now I can't find them, there we go   perfect. Thanks very much for the  introduction and welcome folks! All right as Luke said yeah, I'm Duncan, I'm  the VP of product here at Samasource and I've   got some background in autonomous vehicles and  data labelling so I'm hopefully going to share   some insights that are useful to you tonight and  then answer some of your questions at the end. So how I’m going to address this is we're  gonna talk about your data collection options   building a 3D quality rubric and then picking an  annotation partner. So let's start with creating   a data strategy, so the key takeaway here is  you want to match your data set size to your   development size so you want to return results  as efficiently as possible at each stage and   the real reason for this (and I kind of show here  some of the data types and i'll walk through them   in a second) but the key takeaway is that early on  in your development phase, you're going to change   your sensor configuration many times throughout  your development process, it’s very highly likely,   I’m yet to see an AV partner that had a sensor  configuration at the very beginning that was the   same when they were further along in the project.  So if you picture like ‘hey I could collect   the perfect data set, I could create absolutely  everything I need’ - if you collected that at the   beginning, and then you had it annotated, you may  actually need to collect that same data all over   again and go through the process again. So really  try to like minimize the data set to where you're   at in your development phase. So for example if  you're in the exploratory research phase at the   beginning, you should consider open data sets or  enriched open data sets. And i'll talk about the   pros and cons of each of these types of data in  a moment. Once you move into research testing,   continuing on with those open data sets and  enriched ones, you may start moving into   some custom data collection where you collect your  own data. As you move into simple driving, such   as like on a test track you will still continue  through with custom develop data collection, and   then as you start moving into real world testing  you'll probably want to be adding synthetic data   into your mix to gain more edge cases, and that  continues on throughout your development process.   An example here why I have edge case management on  the end here, as a one of your more final stages,   is as you've got a a setup that actually does  the vast majority of core driving for you,   you'll then consider ‘hey how do we make this  a practical product for our end consumer?’,   and that's when you may, depending on whether  you're looking at building an adas or say like   an l2adas system or an l4 system is you may want  to add elements such as parking, or you may want   to build in as you're trying to prove out your  safety case you may want to add an entirely   separately developed and separate set of sensors  as a backup safety system, and those generally get   added later in the development process as you  have more confidence in your driving system. So let's talk a little bit about how you jump  start your ML development and the pros and cons   of each of these different types of data sets.  So, open data sets - they’re obviously fast   and they're free (mostly). On the downside, you’re  locked into that third-party sensor configuration,   and at the beginning that's probably not too  much of a problem but it'll become more of a   problem for you later. Most of the data sets are  relatively small, I know Lyft's level five data   set is actually pretty comprehensive, but  it's certainly not enough to solve for the   full autonomy. You'll also get a predefined  ontology, so it's not just the sensors that you're   taking, you're also going to get the ontology that  that data set provider actually created. Now those   ontologies aren't often similar between open data  sets, so you may actually lock into one of them,   but this is going to be really important  later in your development cycle as you create   different predictive models and different ways  of managing different types of objects that   you actually encounter. Let's take for example  maybe in the open data set both cars and buses   are considered vehicles, but later in your  development cycle you actually know that   well actually buses behave slightly differently  than cars do and I want to have those labeled   separately so I can detect them and I can actually  then manage them in a different way. Another good   example here is like emergency vehicles - while  they're both normal vehicles they actually do   some very different things. Some examples here  Kitti, Coco for more 2d stuff and then level 5. Next is this concept of enriched open data sets.  This is where you might take an open data set that   already exists and have it further annotated or  annotated again to match your ontology. So data   collection is actually quite expensive, but you  could actually go ahead and take an open data set   and then have it re-labeled or re-annotated.  This is relatively quick, it’s relatively cheap,   you can define the ontology and the annotation  instructions. The cons are that you're still   locked into their sensor configuration, and there  may be licensing considerations - some open data   sets may require that if you're going to use  it for commercial purposes that you may need   to republish that data set, which could be a great  thing for the online research and development and   PhD community, so that might be something you  want to consider from that perspective as well. Next we move on to manual data collection and so  this is where you actually, either you yourselves   or with a partner company, you get some sensors,  you actually put them on vehicles and you get them   out there driving, obviously with a human behind  the wheel. This is quite expensive, as well as   the fact that you're going to need to redo this  but most of your sensor configuration changes.   Some sensor configuration changes,  you may find don't require you to   recollect all of your data but for the most part,  as you make pretty large sensor configuration   changes that will be the case. Some programs that  are much more further along in their development   you can build your system to be more  robust to sensor configuration changes   but that's a much more advanced capability. Next we move to synthetic data creation,   it's relatively quick, it's cheap per hour of  data to create, you can have your custom sensor   configuration as well as use this for edge case  coverage. So a great example here is you don't   want to be like seeing accidents or being in  accidents or near misses when you're doing manual   data collection, and this a really good way to be  able to model that out. It does have a relatively   high setup cost so getting a 3D platform actually  running as well as the scenarios that you want to   be generating is actually a fair bit of upfront  work and ongoing work. Don't ever consider your   synthetic data creation or simulation teams work  ever done - you’re going to always want more   out of it. There’s also the question accuracy of  data - how accurate is that 3D data to your real   world data? Some good examples of this include  like do your LiDAR returns behave the same way   with different materials on objects in your  3d simulation as they do in the real world?   And this can be, you know, across all of your  different sensors, you know, there's lens flares   for cameras - most 3D products have pretty  good controls for a 2D camera but LiDAR and   radar you may need to build your own models for  how reflections and data returns actually work.   It's also quite hard to model human reactions in a  purely synthetic world. So, 2 platforms (and there   are obviously many others) that color and unreal  are good places to start here, and I'm going to   show a slightly different variation on that with  the final one which is synthetic augmentation   of manually collected data. So this is where  you have your real world scenarios that you've   collected, you then turn them into synthetic  scenarios so you actually have the human behavior   already built in and then you can vary it from  there - you may want to change the scenarios,   or vary them slightly. The pros of that  gets you the custom sensor configuration   edge case coverage and you get to extrapolate from  real world examples that you know have occurred.   So this helps a bit with the human reaction,  although obviously you still need to be careful   as to how far you project into the future  as to the accuracy that you still gain.   This has the same high setup cost but it also has  the downside of the manual data collection cost,   and you've still got the accuracy of  data in terms of the modeling of your   sensor returns. You do then have your real world  sensor returns that you can compare against, so   if you recreate the experiences you've seen in  the real world and your sensor returns then you   can more accurately review any discrepancies there  and adjust your models to make them as close as   possible to your tolerance levels. So, that's just  a few ways to jump start your model development. So we're going to move into creating your quality  rubric for 3D LiDAR annotations. I'm going to   spend a fair bit of this presentation on this  area here because once you've collected your data,   getting the right annotations for your model   is going to really define how  far you can push your models. So step one is you want to define the limits of  your system, so this is understanding like how   tight do my boxes need to be? How far away do I  want to actually detect things? It really lets you   figure out the limits of it. So, things that you'd  want to consider here is what's my localization   accuracy? Like how well do I position myself in  the real world? What’s my system performance? So   from detection, through to processing, through to  actuation of the vehicle, what's the time that it   actually takes me to respond to anything? The big  consideration here may also be whether you've got   early or late centrifusion but I'll talk about  that a little bit more later. You need to consider   your speed in your worst case scenarios. Obviously  when defining limits it's easiest to look at   the extremities, so you want take your list of  scenarios that you're trying to address and look   at the worst case scenarios there to define  that. You then also want to consider their,   it’s pretty simple math, your time to collision in  the worst case scenarios and then you also want to   consider the accuracy of your hd maps - like how  well does it represent the real world? If there   are any discrepancies there? So that once you do  the math there you can calculate ‘okay well I need   to be able to take things so far away and I need  them to be this level of accuracy because let's   say for example that you're driving at 60km/h down  quite tight streets with parked vehicles beside,   how tight do those boxes and detections need  to be so that you're not clipping mirrors off   parked vehicles? Or obviously all of the safety  cases that are standard with av development. Next you want to define your quality scoring,  and what I mean here is you want consistent   scoring across your annotations. You want to  try to remove the, well often human labeling   is a core part of getting to quality and labeling,  you want to try to get to as consistent a method   for scoring so that if you have two people  looking at it, it gets the same results.   So the way you approach this is you look  at each class of annotation, you define   the core attributes your model requires - is this  direction of travel? How tight the box should be?   What class attributes about them do I need  to include? Some considerations here are,   how small an object is too small? Is it, you know,  at 20cm by 20cm by 20cm? And your scenario should   be able to help you define this. And then also  you want to say how far away from you is too far. The next one is about sizing of your cuboids.  Often in with LiDAR, if you get a vehicle that's   quite far away and then comes past you, at some  point during that set of frames you'll actually   have a really nice picture of the vehicle in  LiDAR and you'll have a really good idea of   the full extent of it. Do you want to choose the  size then? And then make sure that you extrapolate   back to earlier detection frames where maybe you  only get the front bumper bar, do you still want   that vehicle to be its full length? Or do you  want to just capture the points and say that's   part of this vehicle there? Some of the reasons  you may want to actually choose the full extent   or estimate the extent if you only get  to see a small part of the vehicle,   is this can be very useful for later development  of autonomous vehicles when you're also managing   occlusions. So if you have a vehicle that drives  behind a van or something coming out of a driveway   and you want to understand where should  I expect the rest of that vehicle to be. Next is what's the penalty for missed objects?  No system is going to be 100% perfect on finding   them so you need to have an agreed upon scoring  penalty for missed objects. And then also for   incorrect labels. This is actually an important  one - it can be very difficult when going through   your very very large data sets to get the labels  right perfectly every time. Lots of tools to help   with it, but you need to understand what the  penalty score is for a incorrect label. Now,   I want to just take a second to say that each of  these considerations I’m discussing you should   consider for every class. Some classes may not be  as important as others, or certain attributes of   them may not be as important as others. So for  example when we talked about size of object,   I’m not aware of any vehicles that I would  care about that are 20cm by 20cm by 20cm,   and you may have attributes about  say a vehicle where you want to know   turning indicator signals and things like that,  that wouldn't necessarily apply to a pedestrian.   Then the final consideration is when you're  thinking about how you score a set of labeled data   is do you score per frame or do you score  across the whole video? Do you have a score   given to each attribute class that applies to  the whole video and orders just on each frame   that you're looking at. Just to call it out again  the thing that's really important here is that a   good quality rubric means that two people with  the same information would score the annotation   in the same way. That's what you're looking for  is reproducibility and making it consistent. I just want to include a couple of fun ones here,  hopefully these will be useful for you is that   when automating checks or working with your  labelers whether they're internal or a third   party, you may have some problems if you do  automatic checks or questions may come up.   So look at these two adorable puppers here  - what’s their direction of travel? So if   I normally, you know, annotate dogs and say  ‘hey I want to know which direction they're   going so I can watch out if a dog looks  like it's going to cross into the street,   this dog here is what direction of travel is up/  down/ not important, something you probably want   to consider there. But what I also want to point  out here is that visual examples can really help   convey the intent of your rubric, so when you're  building up your labeling instructions that   accompany a quality rubric, visuals are often  an extremely good way to help with that. Another part of automating quality scoring for  consideration here is 3D gold tasks. So gold tasks   are pre-scored tasks that new annotations are  compared against. So think if you take a video and   you perfectly annotate it, and then you introduce  piece of that video, maybe frames of that video   depending on how you're doing annotation to your  labelers, and then you actually get the results   of what they do, and you compare it against  your ground truth that you originally did.   That allows you to run through the quality rubric  that I was mentioning you should create above,   and get a certain score as to where your labeler  is at. This can be really good for training and   getting consistency out of your partners, as  well as like being able to look at your own   labeling guidelines and quality rubric and  seeing if you need to make any adjustments,   because don't forget, going way back in the  presentation the goal is to get quality data as   fast as you can for each phase of development  you're at, so if you can make annotation   instruction changes to get quality data faster,  then that's only going to help you to to achieve   your goals. So areas to consider for gold  task is - can you can you actually create a   perfect task or a ground truth task by annotating  manually? It's actually quite a difficult thing to   do and there may be some variation there. Also  consider the fact that the amount of time it   should take or the amount of times you'd want to  rework a single task to say ‘hey we're confident   that this is as near to perfect as we need it to  be’. If you do use synthetic solutions, so one   of the advantages of using synthetic data is that  you can say ‘hey I perfectly know what everything   is’, if you're introducing that into a set  of annotations that are of real world data,   is that that can firstly signal to your labeller  ‘hey, this is a gold task, pay more attention’,   secondly you need to make sure that labelling  that type of data is actually representative   of how they're labelling real world data. Then you  need to consider, so you've got this great perfect   data set, you can introduce it to your labellers,  you're confident about that, how are you actually   going to score variation in what the annotators  give you versus what you did originally? Will you   score the points contained within the cuboid? So  you look at all the LiDAR points and you say ‘hey,   these are the ones that are contained, if you  contain them all perfectly then that's a complete   score’ or there's a penalty for points that  were missed.What about missed nearby points? So   let's say you missed a wing mirror on a vehicle  - does it actually matter to you or does it not?   What if you included nearby points that aren't  part of what you were trying to annotate? So if   you had a 3D cuboid around a vehicle and you've  got the wing mirror of another vehicle in it, or   part of a pedestrian around say the back corner  of a vehicle, how are you going to score that?   And then, tightness of cuboid is actually also  a really important one here. I mentioned earlier   that in some cases or for later development  purposes when dealing with object occlusion,   you actually may want to keep the size of the  cuboid you're annotating to the full extent of   a vehicle. So how are you going to deal with that  when in different frames you're only seeing say   the front bumper bar in LiDAR? Are you going to  look for the closest face of the cuboid and the   distance to the point? So you could imagine a  point cloud being in a cuboid and you want to   find the closest extent - is that one of the  ways you want to do it? Are you going to do   it per frame? Are you going to do it across the  whole video? Lots of things to consider there. So back to making our quality rubric,  you want to consider trade-offs,   and I’ve mentioned this a little bit. If you're  in the early stages of developing your solution,   speed may be a higher focus as you have  more limited use cases and maybe you're   only testing on closed testing grounds such  as like Gomentum or something like that.   Define the different quality variables that can  be adjusted for your model. So some examples here   include if when you're earlier in development  is, do you want to have a smaller ontology? It   can be faster to annotate and be less error prone,  which can get your data back faster and cheaper.   What about your label accuracy - is 5cm okay?  Is 1cm okay? There’s a big difference in price   for getting to 1cm of accuracy and it's a lot more  time to do so. So consider where you are and your   system's capabilities as to whether at an earlier  stage you can you know maybe set a lower maximum   speed threshold for your vehicle or not tackle the  hardest scenarios at this stage of development. And then label attributes, so how many additional  attributes do you really need to know about each   class? Do you need turning signals defined  for all vehicles at this stage of development?   Obviously you're going to want them later  on when you're looking for whatever cues   that you can get to be an amazing l4 system but  consider where you are in your development stage.   And then this one might seem a little obvious  but experiment with your labeling team, whether   they're internal or external is try out variations  that are acceptable to your stage of development,   and then you can optimize for speed. Remember the  goal here is to help your development program move   from start to production, and so at each stage  you want to experiment with your team what tools   they have, what tools they may want to build,  there's a lot of variability here and if you just   throw the instructions over the fence you're  not going to be getting the most out of   your labeling team. So just something  important to consider there. I'm just going to talk a little bit about  the near infinite edge case dilemma, so   as you move along that development  phase and you get towards the end,   at a certain point, especially for l4  development, you're going to come across the   100 trillion dollar question- do you have enough  edge cases to claim your solution is safe?   Your data strategy needs to account for it. It’s  highly impractical to collect enough raw data,   even if you had the number of miles which you  know I think it was about 10 billion miles   you would need, miles are not the same. Highway  driving is not the same as dense urban driving,   is not the same as suburban driving. And then  you need to also consider all of the variations   in weather conditions; rain, snow, all night  / day, overcast, cloudy, all these different   things that you're going to need to do, and  so you'll need to turn the synthetic data and   simulation and scenario variation to retrieve  your results. So it's good to understand that   early in your development phase, that doesn't mean  that real world data isn't a core part of your   development strategy but do know that that's going  to be a big investment as you go down the line. Finally who should annotate your data? I'm going  to say something a little controversial here,   working for a data labeling platform company here  is that your goal is to get high quality data as   fast as possible and agility is king. So I've used  the same development phases that I showed above,   but I'm going to suggest how you should do  labeling. I actually think in those earliest   stages, is that for a lot of teams, you won't  have a very large data set size or you may have a   pre-labeled data, focus on an internal development  team. You can get a few people internally,   you can iterate on your instructions, learn a lot  more about what your quality rubric needs to be   and I do recommend you, even if you're doing it  internally that you treat your internal labeling   team the same way you would in a third party, is  that you do create a quality rubric, you do create   labeling instructions and you use that as a really  great learning experience, and that as you move up   the development phases - that’s when you may want  to look at a third party labeling partner. Reasons   for this really include like the capabilities.  Internal labeling is fast, it's easy to do, it’s   sometimes even easier to get those resources made  available to you or have some interns help you   out in those early stages, but once you want to  move to having some of the more advanced tooling,   you’re going to need a lot more engineering  resources to be able to achieve the quality   you need in your platform, whether it's machine  learning assisted annotation, task management,   I mean it's relatively easy when you've got a  few thousand frames or a few thousand videos to   make sure that your labelers do the right thing,  but when you want to start having 3D gold sets,   you want to be managing you know terabytes of data  and you want to be getting annotation across many   different workflows you want to be able to change  the priority of what is being labeled on the fly,   this is when a third-party labeling partner  can really help you because you know,   we have teams dedicated to building  these tools and running these processes   across many different partners and you  get the advantage of that you know,   only having to pay that amortized  cost across the multiple partners. So obviously we're a third-party labeling provider  and so I want to just talk a little bit about our   automotive and 3D annotation expertise. So, we  have experience with all relevant players in the   autonomous navigation value chain from OEMs  to AI startups tier 1 and tier 2 suppliers,   hardware companies making LiDAR  sensors, the top companies in the space,   we've worked with almost all of them and we  can't name customers due to confidentiality but   we're also really good at working  on highly sensitive projects,   so if you have one of those and you're looking  for an annotation partner please do reach out   and you know we can certainly help you - even  if you're at the earlier phases of wanting to do   internal labeling yourself, we can certainly  help with giving some advice along the way. We've worked with 25% of the fortune 50, and they  trust some source to solve their training data   challenges and that's a really good way to call  it is its challenges - data labeling is something   that is a really really core part of getting  your ML models developed and out into production   to provide value to you. We’ve seen a lot of the  issues that people encounter, and we're really a   partner we're not just a service provider - we're  someone who'll let you learn from our expertise   and really help you avoid those pit traps and  pitfalls along the way. But Samasource is a little   bit different than other labeling providers, we  actually were founded in 2008 as a not-for-profit,   with the purpose of impact sourcing. We founded  this concept of impact sourcing, where we believe   in providing good meaningful work to people,  to women and marginalized youth in places   where there are high levels of unemployment like  east Africa or southeast Asia or the Americas,   we give meaningful work for these people, we train  them, we give them life skills, they're full-time   employees with benefits. We really care  about making a positive impact on the world,   and so if you're considering you know ‘hey do  I want to get labeling done with this partner,   or with someone who's making a really big  difference in the world, this is something that   we're also doing and actually I'm really happy to  be able to say this year we're also recognized and   certified as a B Corp, we're the first AI company  to actually be certified that way, and so it's a   really good way for for you to have impact with  the work that you're going to be doing anyway. So I think I’ve run a little bit  earlier than I was planning to do,   could we start a little early  Luke on the questions because   I ran a little bit quick because I saw  some of the questions were more around   sensor fusion topics or things like that and I  wanted to make sure I had time to cover those. Yeah absolutely, so we’ve got one there from Remi   to start saying ‘you mentioned that 3D  party labeling partner might bring ML   assisted annotations- could you just  develop a little bit more on that? Sure yeah great, so there's many different ways  to do ML assisted annotation, common ways you   may have heard are pre-annotation, or assisted  annotation, it's basically we develop models that   help humans in the loop achieve the best quality  at speed that we can. So some third-party labeling   companies are actually trying to essentially  replicate what you're trying to achieve,   which is you know sort of real-time pre-labeling,  we actually focus on working with our annotators   as well as our R&D team to make sure that  we're building highly efficient ML models that   consider having the human in the loop to get to  quality. So let me explain maybe the nuance of   difference there. We truly believe that having AI  plus the human is how you're going to get to that   quality bar that's needed for your data. We don't  want to be providing you with lower quality data   that is just super cheap because we're able to  just pre-annotate everything, we believe that with   ML models we can get very close, and that we need  those humans in the loop to actually get to that   quality bar. It's also, you know, just a logically  speaking, if you could partner with someone who   can already annotate your data at the quality  you need your model to do, maybe you should just   buy their model I mean that's just a logical  fallacy that i'm still yet to see play out. Another one that came in: so someone's asked ‘what  about scenario generation and applications on HIL,   how to compare the quality of the  scenarios to real world annotations? Yeah so that’s a really interesting one, and  so, once you've got your synthetic data or your   mix real world synthetic data and you're doing  variations and you're running hardware in the   loop, it's a really big meaty topic to look at  - how do I say that this is realistic? There's   different approaches and there's some good data  to draw upon from different iso standards and   the airline industry as well. When it comes  down to it, you're going to need to satisfy   your internal company that the results you have  are valid and the way I personally thought about   it when I was in the autonomous vehicle space was  I just pictured okay I'm gonna have to explain it   someday in front of a legal body somewhere,  why did I think this was safe? Personally I   believe that being able to prove with as much real  world data to synthetic data that the variation is   very small and is applicable is actually a key  part there, you may also want to investigate,   depending on how much your variation of your  synthetic hardware in the loop run scenarios how   much they deviate from your your collected data,  you may then want to also look at certifying your   models for human movement and human behavior  or sorry any agent in the world’s behavior,   whether it's you know humans, bikes, cars,  animals whatever it is you're solving. Sure, on that note, if you have any questions  for Duncan and you're on the webinar right now,   make sure to get those in but we'll move  on to some of the pre-submitted questions   from before the webinar. So someone  said ‘If you could highlight some   of the biggest challenges you're  facing at the moment in your work’ Yeah no I think that's fair - some of the  biggest challenges that we're facing is just   making sure that we're working closely  with our partners to give them success.   A lot of our different partners are in different  stages of development and so ensuring that we have   the right tools, the right processes, and the  right advice that we can give at each stage,   it’s a big challenge, it's a logistical  challenge to make sure that we're providing   the level of support that we want to all of our  different partners. From a technical perspective,   I'd say is that getting our machine learning  to apply to all of the different variations   in cases that we want it to yeah I mean that  takes a lot of time to develop and so we're   making really great progress but it's one of  those areas where it's something that takes time. Yeah absolutely, another has asked, ‘when data  labeling, how do you differentiate between   objects if the resolution is really quite poor?’ That’s a great question - in those cases I would  recommend, so there's a few things you can do,   one is you can actually use machine learning  too or not just machine learning but you can use   common filters to clear up images. So one of the  advantages of having access to the data offline   is that you can spend more resources on actually  clearing up data, you can also look forward and   back so you can see like ‘oh can i see that  object later or earlier’ and then be able to   be like ‘oh okay well that blurry thing in this  frame is actually this object there’. I'd also   mentioned that sensor fusion is a really good  way to consider it at this stage too because you   may have a poorer detection or poorer sensor  return, you know whether that's from a camera or   light or a radar in one sensor but looking through  the other sensors at the same time or around the   same time period is another common method to  be able to to identify the object correctly. Another live question that’s just come in, I  think they mean - ‘how do you accelerate the   production mostly in LiDAR annotations and how  that works with kind of the cost effectiveness’ Yeah so what we do is we work with our clients  and look at their quality rubric. We do what I   was suggesting at step four which is experiment  with your labeling partner. So what we do is we've   had a lot of experience labelling other people's  data, and so we look at where can efficiencies be   gained that we know have been true in the past, as  well as experimenting on other ideas that we have.   So our goal, as I said, is to be a partner - it's  not just to be like ‘hey you told us to do this   we go do this and everything's done’, our goal is  to make active recommendations. With our partners   we also have a great A B testing framework where  we can actually run data through different groups   of annotators with different instructions as one  example to look at both instructions as well as   different tool sets, so one of the things we  sometimes also do is look at ‘would there be   any unique technologies that we could introduce  UI changes, workflow changes, that would actually   assist with a particular problem that we are  maybe seeing that's more common in that particular   partners data that we haven't experienced before  or is different to what we've seen in the past. Sure, another from Remy, he asked: ‘how  do you deal with uncertainty in the data,   so for instance cases where a human  can't really decide - do you explicitly   label that or would you remove it  completely from the ground truth?’ Yeah so that's a really good question and  it depends on how you want to handle it so,   let me give a couple of, ( and I hate giving  'it depends’ answers but it really depends   on what makes sense for your model) so when I  mention the different attributes you want, so   and the different labeling instructions, so there  are different ways you can handle this - you could   not label it and say ‘hey we're not sure’, I  could label it and say ‘I’m unsure or I can   actually provide different classifications of why  I'm unsure, so depending on where you are in your   development phase, if you're looking for, say  common use cases where your sensor configuration   may not be being successful and that may just be  from a ‘hey I can't actually annotate this’ so   that thus is not a successful configuration  we could annotate that for you and provide   human level reasoning as to you know, maybe we  choose 4 or 5 common different reasons as to you   know, maybe it's blurry, maybe it's too dark,  maybe it's you know, whatever the particular   issue is, we could annotate that for you as well  and then you could choose to either put that   information back into your model or you could at  least then do data analytics or run an analysis on   what are the trends we're seeing through your  data in terms of those issues cropping up. We've had another question around kind of the cost  implications as well, so someone’s just asked:   ‘when you run sort of through the  same data with many annotators   how does that affect the cost of it and the cost  effectiveness of the kind of process in general?’ Sure so there's a few different methodologies  there, one common methodology is called multiple   submissions where we see this a lot more in  crowdsourcing, at Samasource we have what's called   a fully vertically integrated managed team and  workforce, so they actually work for us full-time,   so as they gain experience on your projects or  through other autonomous vehicle projects like   a large percentage of our workforce have, they  actually gain those skills and apply them in   the future and so that leads you to generally an  annotation format where you have trusted quality   processes to be able to identify ‘hey, yes these  people are doing good work’ and then you have   sampling of those results internally for us as  well as the client may sample the data as well   to confirm that scoring from the quality rubric  we defined above, and then the other way that I   started mentioning was multiple submission, we see  this a lot more with crowd work and it's generally   more common with, there are two cases where it's  more common is in the crowd case it's generally   more common when the work is very simple and  you're just looking or it is very subjective   where someone's like ‘oh is this good, is this  bad, is this, what's the intent someone's trying   to do here?’ And so that can be a good way where  you have 3 or 4 different annotators annotate the   exact same thing and you compare their results and  once you get you know more agreement, sorry if you   get no agreement you can add more annotators to  get to agreement or if you get agreement very   early on then you can say ‘hey this is a good  annotation’. It can be used even for some of the   more complex or cases where there's disagreement  in those edge cases, you mentioned earlier that   there was this problem with, you know if you  couldn't identify an object, that's the case   where you want to introduce another person to  take a look at that and you could either highlight   that through our system for our quality assurance  team to take a look at it which is our people who   sample our data, or you could or you could build  that into your workflow if it's common enough   so that it's actually part of the workflow where  another person would actually check that same one. Okay brilliant, Herman had asked: ‘can  you give kind of more examples of where   3D data sets are used other than sort  of the autonomous driving setting?’ Yeah no I absolutely, can I kind of just given  my background I tried to pick a use case that   was relatively easy to explain but we actually  see 3D data sets across a range of industries so   anything that has a LiDAR in it and for anyone who  has a new Iphone, you've got LiDAR in your phone   so we actually see it across more mobile  applications are starting to come out,   we see it more common in AR/VR setups where  you're trying to understand what's the spatial   recognition of the room. Some are doing it camera  only but we're seeing LiDAR more common as LiDAR   is becoming cheaper and cheaper and making its way  into consumer electronic devices that's becoming   more of a common use case. We also see it in  some drone and mapping applications as well,   those are the major ones we're seeing it in  and then all the variations on autonomous   vehicles such as we do see it in robotics, we  do see it in agriculture for similar use cases. Sure, one of the pre-supported  questions we had was:   ‘what are the benefits of using  LiDAR over radar for remote sensing?’ There's pros and cons. So each sensor, and that's  why I kind of go back to autonomous vehicles   because it's a great place to show the different  pros and cons of the different sensor types.   The way I like to think about it is that, each  sensor is good at some things and bad at others,   so LiDAR can be quite good at  getting a shape of an object   with enough points as well as the distance away  from you that vehicle is at. Radar is actually   far better at getting you a general size and  the velocity. The velocity is a much better   out of radar. I would also mention that radar is  a technology that's been around for a long time   so there's actually a lot more research and a  lot more off-the-shelf parts that you can get   that are more certified and that can  speed up your development in that way.   Given my background was trying to solve fully  autonomous driving I would say you need both   but it depends on your use cases and your  scenarios that you're trying to solve for but   that's the kind of thing I'd be you know, happy  to talk through with the clients if they were   looking for a certain solution and we  could go in depth on their use case and   why a particular sense is good or not. Also you  have to consider your LiDAR, the failure cases,   radar can get poor returns if it's near a large  metal object. LiDAR may not perform so well during   rain or through steam. Actually a very interesting  edge case there could be to look at, compare say,   camera versus LiDAR is that if you think of those  iconic scenes where you see the steam coming up   out of the streets as it's getting into the colder  months then your LiDAR may actually say ‘hey,   there's an object in front of me, I can't possibly  move ahead there’ and whereas your cameras are   saying ‘no, we're totally clear what are you  talking about’ and so you may need to train   a model specifically for steam detection. Steam  does actually have a shape and a set of attributes   about it that can be identified so you can say  ‘hey, I can trust this for now and maybe apply a   different driving model for that particular time  while you're getting the driver through that. Sure, another one kind of  asked more on the data side   from one of the pre-submitted questions that  said: ‘Do you have any top tips for ensuring   data quality or sort of having  the highest quality of data?’ Yeah so I would recommend 3D gold sets, I do  think it's a really valuable resource, I would   also recommend introducing something that we call  autoqa. So autoqa is basically the introduction   of programmatic error checking so there are  things that humans are really good at which is   why we want humans in their labeling and there are  things that computers are really good at and those   are generally two pretty different things, so if  I'm say, annotating 200 vehicles in a particular,   you know, I'm on a busy street and I'm looking  out and I'm annotating a full scene and the client   has said ‘hey, every vehicle needs to have its  own unique ID and that ID cannot be the same as   another vehicle’, if I'm going through and okay  I'm up to my 153rd vehicle I'm like okay it's   vehicle 152. Oh wait it's very hard for a human  to know that they have perhaps labeled it with   the same idea as another object and it would also  be very hard to catch in sampling if another human   was to look through an entire list and to catch  a single small error like that, however you could   build a programmatic rule that we can build, in  that we actually do this in our hub as a standard   offering is that we can build the programmatic  rules relevant to your quality rubric and your   annotation instructions to check for errors like  that because a computer would just instantly,   you know, if a label has gone through and said  ‘hey, this looks great, they go to hit the submit   button and we actually can do a lot of checks this  way and instantly give them feedback like hey hold   on I see that you've annotated these exactly the  same. Or another example when I was showing the   dogs laying on their back in a street is that  a common way for us to check is, do we have the   direction of travel correct? It is very uncommon  for you to have a direction of travel that is up   or down as a pedestrian or as an animal however  we can provide a programmatic way to say ‘hey,   check this this doesn't seem like it's correct’  and then the annotator could say ‘hey, no that   is correct or let me go fix that’ so those are a  couple of ways I would also say understanding, I’m   just trying to think about the other ones that  I recommend to ensure uh high quality data,   having a really good feedback loop with your  labeling partner whether it's internal or external   is there is going to be edge cases and making  sure you build out over time that set of edge   cases so that the annotators can just like  say ‘hey i've seen a weird thing how do I   deal with that’, if you've answered  that question before making sure that's   like more on a knowledge base  level can be addressed easily,  and then I would say…I’d say those are the  key ones that really help with making sure   that you hit high quality. I personally am biased  because we have a managed workforce and years of   experience they've got keeping people consistently  labeling on your project really does drive quality   up over time. Tthere’s an interesting study by  Hive Mind with cloud factory where they compared   a managed workforce versus a crowd workforce and  they saw the annotation result differences. Now   I'm not saying crowd providers don't provide  the same people working on it at all times but   our system is inherently built that way  so the people annotating for you stay with   your project and so that they build up that  experience with your work in that quality bar. Sure, another question saying: ‘can LiDAR, I  know you kind of already touched on this roughly,   but can LiDAR be used during bad weather  conditions such as heavy rain that will cause   refraction of light? Also LiDAR uses powerful  lasers - will that potentially affect humans?’ Sure, so almost all LiDAR, so let me address  the latter piece first, almost all LiDARs   are eye-safe, no one’s releasing products out on  the street that does not conform with eye safety   standards, so not really an issue there. There are  maybe some more interesting issues about potential   threat issues of people shining LiDAR  lights or lasers back at your LiDARs,   but there are methods where you can actually   detect if your LiDAR is either suffering  internal problems or issues from an attack.   LiDAR does have weather issues, all sensor types  have strengths and weaknesses and heavy rain is a   problem, I would mention that there is also both  LiDAR and radar in different ways but in this way   LiDAR also gets reflections of different  surfaces so you get what we call a ghost image,   so you might be like there's a car here so if  you look at a street there's a car in this lane,   a car in that lane, and there's a whole  other car in between those two cars in   the same space - clearly logically something that  doesn't exist but it could be a reflection from a   different sensor or from a different location, so  that can occur as well. When we're talking about   LiDAR and weather conditions specifically  and I recommend this as you go through your   development phases you're going to need to build  detectors into your system that say ‘hey, this is   a condition where I can't trust this sensor or I  need to have a lower level of confidence in this   particular census return and that also will play  into your sense of fusion choice of whether it's   early or late sensor fusion or both and how much  you trust individual sensors at a given time. Sure, a slightly longer question here, somebody's  asked: ‘when trying to understand a scene,   switching modality can be really helpful  in brackets (LiDAR/ camera for instance)   as well as going back and forward in time  and going across data (data recorded on   different days) - do you provide these  kind of options for your annotators?’ Yes so sense of fusion is a capability we  have, so the idea of you're absolutely right   that jumping between modalities can be really  helpful as well as you know projecting those   annotations that you have made across those  different sensors so if you're if you're looking   at your 3D scene and you've got a bunch of cars  there that are already labeled and you're trying   to figure out something else going to look at the  2D image it's really helpful if you have those   boxes actually projected across as well because  it gives you a much better orientation like ‘oh   there's those 3 parked cars oh okay that's what  the thing I'm looking for is’, so it helps you   orient yourself. There is actually an advantage  in looking at the LiDAR data for an orient,   for a 3D perspective but then when you go back  to 2D sometimes it's quite hard to be like okay,   but where is this? And it's really helpful if  you you know, you could drop a cuboid where the   object that you're not sure about is and see where  that is in 2D, but yes we offer the going back   and forth in time as well as a different sensor  looking at multiple sensors for your annotation. Okay brilliant, another pretty  surprising question somebody had asked   if you've seen any trends emerging  recently or if you have any predictions   of potentially future trends that might  be coming into this sort of scene for you? Okay I could either take that as autonomous  vehicles in general or in LiDAR- I'm gonna take   autonomous vehicles first. The trends  I'm seeing is we're starting to see   the more advanced like l2 systems where we've now  just recently seen the gm Tesla competitor come   out and I think we're going to see that become  much more standard across auto manufacturers as   as lot of them have their own programs and  are initially trying to solve for the more   Tesla solution rather than the sort of true  l4 development. We still have not yet seen,   despite small public trials, we've still not seen  a mass deployed sort of true l4 system at speed,   I think we will continue to also see more niche  use cases actually starting to be deployed whether   it's you know slower buses or fixed routes  or other things like that, that’s a natural   progression of how this kind of technology  will roll out but that doesn't necessarily   define that you know l4 is  like you know here next week. Okay brilliant. Herman had asked: ‘when you  say LiDARs as a standalone, would you replace   cameras or would they augment the accuracy of  existing models that rely on camera feeds?’ Sorry could you just repeat that? Sure so it said ‘would you say LiDAR s  as a standalone would replace cameras   or would they augment the accuracy of  existing models that rely on camera feeds?’ Definitely the latter, I mean I've seen  some very advanced LiDARs that, I mean   we're getting pretty close to it looks like a  video like I can read signs that are just, you   know, so if you think of a movie theater where,  or a shop that actually has its name written   in like stenciled letters out the front or  something - you can start reading that but   LiDAR doesn't necessarily return a whole load of  the rich information that you get out of camera,   so I think we're going to see  more and more of those of the   sensor fusion occurrences and I think that  that's not just for autonomous vehicles but   even when we talk about like AR/VR or even drones  or other use cases it's the combination of sensors   that really get you much more rich results even  though it it is a more difficult development part. Absolutely. Another question from  somebody watching right now has asked:   ‘what is the most challenging area of  annotators in 3D keyboard annotations?   Do you have any of your own  tools for 3D annotations?’ Yes we've got our own tool for 3D annotations,  you can check out sammersource.com and we've got a   automotive area on that site and that you  can go check out what we do. I would say the   most challenging area is   fatigue, I mean our annotators are amazing people  but they're still people and so when you get   through your you know 8th hour of annotating the  same scene it can be quite difficult, there’s ways   to help manage that and we do that so that's from  a human management perspective. I would say from   a technical perspective, it's generally around  the very edges of annotations, so if we were to   annotate vehicles and we wanted to annotate that  same vehicle through the entire scene, relatively   easy task to do, you could either pre-annotate  or we you know interpolate across frames,   when you get to the very edges in what the LiDAR  can pick up and there's maybe two points left,   two like you literally only have two dots of that  vehicle and then accurately putting it in the   right place, you know there's there's challenges  there - how good is the sense of calibration when   we're swapping between 2D and 3D that the client  has provided us, that can be a challenge if   their calibrations aren't right so you're trying  to put an entire vehicle in exactly the right   place with two LiDAR points on the entire thing  because you see it later and you definitely know   it's that vehicle, but you know did you 100%  accurately place it there especially when you   project into 2D and there may be some variation in  the sensor calibration information passed through. Sure, we have another question saying  with Tesla starts on LiDARs do you think   point cloud annotation will stay relevant for  autonomous vehicles in the next decade or so? Yes. I think Elon and Tesla have done an amazing  job building the system they have, I think that   Elon is probably, personally I think Elon is  probably right that we'll be able to do camera   or camera radar sometime in the future, it's a  long way up and it's a little, it's surprising   to me that you wouldn't want to give yourselves  as many tools to solve the problem as possible   especially now I understand when Tesla was first  coming out with their vehicles that LiDARs were   extremely expensive, but as we're seeing the cost  of LiDARs come down as production ramps up on new   types of LiDAR and greater manufacturing volumes  that LiDAR will be here to stay for a while,   and then I would ask you the later question when  you potentially could solve with cameras only,   as cameras improve in quality as well as computer  power becomes more available and cheaper.   If you've got your LiDAR down to roughly the  same price that you're paying for cameras,   I mean don't forget at automotive volumes  you're talking about a relatively decent like   megapixel camera is like less than 10 bucks so  if you get LiDARs down into this kind of range   why would you not have it when it when it does  things that cameras don't and it gives you   added safety and I think it's going to be the  safety element that really drives its inclusion   for the future, I mean Tesla has a really strong  advantage with their offering in that it's a   backup system, it's a driver assist technology,  you're supposed to always be paying attention,   and so there's a very big leap  from that to you know level 4. Okay, Dale has asked if you have any experience  with synthetic scene generation tools? Scene generation tools, yes I do, my  recommendation would be like it's an entire topic   in and of itself. Thing I would recommend  looking at is - how do you want to run this   at scale? So it's quite easy to get started  with a GTA type experience, relatively easy   things you want to consider is how are  you going to create the worlds that   you want to have generated is that are they  going to be randomly generated? Or are you   going to use specific locations? You know going  back to my comment earlier about fixed routes   it's quite easy to very highly accurately map  out an area with you know if you drive through   and you collect your LiDAR data you can then 3D  model on top of that to get an accurate world map   that you're going to simulate in but if you want  to do generic random parts of worlds. And then   the other areas I would consider for it is, do  you have access to or can you create your own   models for sensor return types? That's the other  other big area there and then how are you going   to do scenario generation and take that in from  a scalable perspective and say like ‘here is my   language or definition language of all the  scenarios I want to run and how am I going to   ensure they get run through my simulation and  look at the results in a human readable way’ I realized we're at time so sorry  for a quick answer on that last one. I will just say for anyone that actually has  a question that hasn't been answered and we've   put down it through quite a few questions  now, make sure to connect with him on Linkedin   and also with Samasource make sure  you’re following all the brilliant   developments and everything coming next year but  I do want to just close with one question that   I do like to ask everybody - what are you  most excited to work on next year in 2021? I'm most excited about continuing to  increase our impact - we’ve got some really   cool technology pieces coming out that I can't  talk about yet that would be my other answer,   really product launches that I’m super excited  for but you know it's the reason I joined   Samasource being able to work on the cutting  edge of AI with a great research team that   really world class and amazing and getting to  know that every day I'm like helping bring about   a like new way of working and helping people  out of poverty who really need that help. Sure absolutely, with that I just want to thank  you for your time and as I just said make sure   you're connecting with Duncan on Linkedin,  if you have any further questions I'm sure   you'd be happy to go through them there as well as  following Samasource, but thank you for your time. No problem, thanks Luke, thanks, cheers. 