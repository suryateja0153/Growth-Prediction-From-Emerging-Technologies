 [Music] thanks for joining our session I'm Adam Michaelson I'm a product manager here at Google on the IOT team joining me shortly will be a couple of folks from our partner cog night gear n'gou and sundry ham Erland so they'll be coming up in a minute and describing in detail their expertise around IOT and industrial but before we dive in on that I wanted to just do a quick informal survey so for the folks in the room around IOT Internet of Things how many folks in the room have deployed an IOT implementation just by show of hands or are planning to do so say the next 12 months wow that's that's a lot of people so that's exciting to hear what we've seen from analysts is there's basically a claim that somewhere between 8 billion and 14 billion IOT devices exist out there today and that's a lot of devices it's also a big range 8 to 14 billion I think one of the reasons might be when you think about IOT a lot of the implementations we see use a gateway right look at think of this room and if you want to control all the lights via IOT then a lot of the implementations we see is will be a central box somewhere some Linux machine that all these lights are talking to on-premises and then that gateway will connect to a cloud so what is it that is the counting as the 8 billion is it the one gateway or is it each individual light I don't know I feel sorry for the poor analyst who's going around counting those 8 billion devices but it's a lot of devices more and more all the time and all of those devices are generating incredible amounts of data and dealing with the intake of all that data is one of the challenges when it comes to IOT even though there are so many devices that are online there's another statistic which isn't so great and I don't know what the experience of the audience is that we've seen that people claim that around 1% of the data that's actually collected for IOT is actually used to generate some sort of a positive outcome business outcome and so what's happening to that other 99% of the data that is not generating that outcome there's an opportunity there for everyone to say how are we going to collect this data and make sure that where the data were collecting has maximal value so that's one of the purposes where we could say see googles mission come in so googles mission is to organize the world's information and make it universally useful so when it comes to say Google search that's easy to think about pulling information from lots of web sites organizing it so that it makes sense providing a nice handy search bar that you could type into and then at the end giving you useful results so you tie that chain together IOT has the same opportunity where we're collecting all this data it needs to be organized in a uniform way and then made useful so we think about the lights in the room are collecting ones type of IOT data or telemetric data the HVAC system might be another one the alarm system is another one how is all this data being collected organized to make sense and then produce a useful outcome that has business value so some of that some of the topics we want to talk about today but before diving right into the industrial use cases I just want to spend a couple of minutes in baseline us on what are Google's IOT offerings so we know what the terms are and all of a common understanding before we dive deep into the industrial use cases so this is an image of Google's IOT platform for those who haven't seen it it's called IOT core IOT core is a component that exists within Google Cloud Google cloud services IOT core has a few major components built into it one component is this protocol bridge that is what will enable us to take these edge devices and safely and securely send that data into the cloud we often will talk HTTP or a more popular protocol for IOT is MQTT which is very efficient for IOT use cases the other component of IOT core is this device manager which allows us to understand each and every device both in like example we were talking with before with a gateway like in this room if we had gate with lights we would under we will run or understand the Gateway that's talking to us but also each device that's connected to that gateway would want to be known to the device manager so as data is being sent from these devices into the cloud we can say oh that's a light and that's an HVAC end and that's an alarm or whatever it may be another component of IOT core are some edge modules where we have libraries that you can take embed into your edge devices that have pre-built connections to IOT core you don't have to use those edge components you can use the raw API of the IOT core cloud modules but those components are there and extensible for you just for a quick start so you can get your IOT journey going a little faster now behind IOT core is the rest of Google cloud the first module we connect to is pub/sub it's a large message bus that's used across many Google components to allow services to interoperate once the data gets the pub/sub then it can be sent to other Google services commonly we see services such as data flow that can transform data so for example if you have a data coming in from a light and a late data coming in from an HVAC and they're in different formats we have to make them look common cloud data flow will help us do that we can also call cloud functions if we want to run business logic on real-time data that's coming in in a streaming way and then many services behind that to store the data we have multiple storage options that you can choose based on what type of data it is how frequently it's coming in how frequently you want to do analytics on that data and then there are analysis tools that we have such as data studio as well as a suite of machine learning tools so this is a this is all the Google components clearly but it's ones that we typically see on an IOT implementation you heard potentially yesterday in the keynote about multi cloud and hybrid cloud and a lot of the implementations we see aren't quite as clean as every single component up here is one of these Google components which we'd love to see but we have many partners in there are many players in IOT so oftentimes this architectures a suite of tools that we see between Google components partner components to come together for an architecture so one way to sort of think about the various components that are involved in an IOT architecture is this simple three buckets on the Left we see ingestion to gather the data so IOT core is there to get the data sometimes you have data that doesn't need an IOT broker it's already ready to be ingested so for that you can use something like pub/sub just to take the data ingest it in the middle we need to clean up that data process it in its the organized step in the Google mission here we see data flow functions data storage various components on the far right is where we have to do analysis and understand this data and if we don't see Google components on all of these that's fine what's more important is for the folks in the room implementing this they have a way of laying out the components you're gonna have any Ryo T and know what functions they're going to operate on so before when we were seeing 99% of the data in IOT is not reaching its full potential a lot of the reason why is because the process and clean step isn't cleaning up the data as much as we need so that data is looking like it's in data silo from the lights in the HVAC and etc and the other reason is because there's so much of this data it's not hard for a single device to generate gigabytes of data in a single year if that device is talking multiple times per minute or multiple times per second and then multiply that times all the devices and we're talking about an enormous data load to be able to understand that data so we just don't have the human capital to go through all that data and that's where tools like machine like like let me go back when the machine learning tools come in that will help us understand over here on the right to review all that data so that let the Machine look at the data and figure out how to find the insights rather than the people and that's why ml and IOT go together so well so on the next slide I wanted to talk about machine learning where machine learning also has lots of tools you could use so what's a framework that we can think about in terms of which ml components we want to use and this is another block of three so here on the Left we have Google components that are pre-built ready to use our data our model meaning for example vision you we have a Google vision machine learning component that you can upload an image pizza here's a pizza and Google will say I think that's a pizza and then you can go ahead and use that data or whatever it's a building or it's a dog that sort of thing in these components on the Left there are more and more of them all the time very simple to use a great place to start your ml journey with all this data but oftentimes you'll say ok I know that's a pizza but I run a pizza company and I want it to know that it's one of my pizzas because I have all these different pizzas that I make so how can i train that model and that's where the middle components the retrain models Auto ml will come in so like Auto ml vision is a tool you can use and the models are pre-built but the training is not in here you'll upload an image and say here's pizza and it's my supreme pizza and here's another one and it's my whatever deep-dish and these are the others so that Google will learn those images so when it sees images of those in the future maybe as the pizzas are coming out of the oven and you want to make sure it's matching what the customer ordered something could take a picture of it match the point of sale and say yep that's the supreme pizza just like the customer ordered with extra mushrooms and that's where tools like Ottawa ml can come in any of that training you do is your data none of it would filter into the left none of it filters into Google it's all your data and then sometimes customers need even more they need not only custom trained models but they need to build their own models and that's where there's you'll the data scientists define the rules you need define the data sets you can create great tools like tensorflow either the open source or the hosted version on Google and have models that are custom to your business and customers will often start on the left use the out-of-the-box ones and work their way to the right as they need to because the more you go to the right the more coding you have to do and the more the more coding you have to do it unless you're worrying about your core business and so that's why we offer this suite of tools so with that we have a baseline of IOT core in the components so now what I want to do is invite gear up and he's gonna describe to you how cognate has taken these tools assembled them use in an industrial IOT use case and brought them into the real world and they'll share with us some of the lessons they've learned in some of the tips for all of us to take advantage of here thank you thank you hello everyone my name is gear I'm CTO and co-founder of cog night which is a software company the works with a set Heavy Industries what that means is it's basically industry that has big machines that cost lots of money so we work mainly with oil and gas also in power and utilities and shipping now this talk will focus on shipping because it is the most challenging edge environment that we've encountered but before we dive into that I I want to make sure that you are all awake so quick show of hands how many of you have had a cup of coffee this morning yes that's what I thought as we all know coffee is one of the primary or most important inputs to writing a good software called but what are what are the inputs to coffee it turns out that coffee sits at the top of a very long complex industrial value chain and you don't have to backtrack very far to understand this because the beans before they came to most only they were on a truck and that truck needed fuel hence the oil and gas industry or in where I'm from in Norway I believe 60% of new vehicles sold last month for electric so there you have power and utilities - and the truck needed to be manufactured from steel so you are mining steel mills and before the beans got onto the truck they were probably on a ship so it's but there's this huge value chain which touches all the industries that we are in and it's not just coffee it's everything that we surround ourselves with that kind of make our lives comfortable and convenient and that's why I think it's such a privilege to use data and algorithms to drive industry to be more efficient produce more for less energy input make it safer yeah it's it's really quite quite the potential there so when we started called night we set out and we had this kind of naive belief that we would use sensor data and we do ml on that and things would be great now it turns out industry is a little bit different from consumer in that there are so many different data silos different protocols you know different systems that you have to integrate with and in order to understand the sensor data it's you need a lot of context around it so maybe you need the engineering data maybe you need a CAD model to figure out where the sensor actually is located or you need the topology of how how stuff flows through a power plant and so we were very lucky to start out with a large industrial customer very early to be exposed to this reality of industry I think the core problem is that the life cycle of industrial equipment is so much longer than the life cycle of software you know a plant that's five years old is new software that's five years old is old so then with this data complexity that we witness our mission quickly became to liberate all this data from the industrial silos move it to somewhere where it's always available and clean it so that it becomes understandable so that it's easy to build value from it as you can see it's quite a complex reality out there as Adam was saying the the scale of data is enormous a single sensor is sending one value per second will produce one gigabyte of data every two years and that doesn't sound like a lot but typical ship has 5000 of these an oil platform can have a hundred thousand so it becomes a lot of data and that actually informed our choice of Google Cloud as the vendor because we believe in the ability of Google to run distributed systems at a scale that's one step ahead of everyone else so we want to reuse the same infrastructure that's powering YouTube Google search and Gmail to store all this data for instance we're storing all the IOT data all the sensor data time series in system called BigTable which is a ridiculously distributed key-value store and how we structure data into big BigTable to be able to query it efficiently and you know store data efficiently could fill an entire talk and in fact it does I did a talk on that last year up next so if you're interested in the details of that you can google code night next BigTable and you'll find out talk as someone who loves technology it's really easy to get kind of sidetracked into building technology for technology's sake and that usually doesn't lead to great outcomes in the end and I've been guilty of that in my past so it makes sense to sit down with the people in industry figure out what the real valuable use cases are and some of these use cases will span we see in every vertical every industry that we are in such a fuel efficiency or energy and fishin see in more in general nobody wants to waste energy energy is a cost in fact in shipping it is the number-one cost a single product percent reduction in fuel costs for a you know an average fried chip is a 50k per ship annual savings and for a fleet of 100 ships that's 5 million and by the way we're not targeting 1% so our customers estimate anywhere from 5 to 15% savings depending on the type of ship and the age of the ship and then there are other use cases which are specific to each industry such a weather impact on cargo you want to be able to tell your customer that you didn't damage the goods and you can measure the motion of the ship at any time so you can even say hey look I didn't go through two rough seas this is what a data silo looks like we have this great piece of equipment a sensor that is using lasers to measure the torque on the propeller axle on a ship so it's like the laser underwater sending a beam along the axle it's reflected it's detected it measures the microscopic deformation of the axle due to the force and acted on it now contrast that with the way that this data is sent to the cloud because what happens is every day this guy walks down to the engine control room and writes down the value on a piece of paper and it goes into a report but that's not like the reason this particular measure is important is if you want to solve the energy efficiency use case for a ship you need to be able to break down the fuel used to ship motion into its two main parts that's the engine efficiency so how much force do you get onto the water via the propeller for each unit of fuel used and the whole performance so for the force that the propeller acts on the the water how far do you go and a once per day resolution is nowhere near what you need to solve the use case because the engine settings change too often weather changes all the time so here to get the data out we have to install an adapter and this is a pretty common pattern so the data sets in a separate network you can't talk to it on the ship's network which converts the data and sends it onto the ship Ethernet where it can be picked up by an edge box and you have many of these systems that you need to liberate data from here's another issue connectivity the good news is that you can be connected virtually anywhere on the planet the bad news it's it's kind of expensive and the bandwidth is not great one of the things that surprised me about this is that you actually get quite decent latency your data can go from the laser to the cloud in 200 milliseconds I find that pretty pretty awesome but we have too much data to send over this connection and most of the data is not that useful for this use case anyway so given those constraints here's how we approached solving the situation first given the bandwidth constraints there is a need for an edge component it needs to manage the bandwidth so some of the data is high-priority and should be sent directly to the cloud some of the data needs to be buffered on the edge stored there until connectivity is better when the ship is close to shore it will get cellular coverage which is much cheaper and much wider when you can bulk upload that data you also may want to do some processing on the data down sampling or compressing the data in other ways for instance vibration sensors on rotating equipment will often send you data at 50 kilohertz so that's 50,000 data points per second that's like 10 times more than the rest of the sensors on the ship you can send all that data and you don't need to because if you do a Fourier transform convert the signal into the frequency space and send the coefficients of each frequency that constit constitutes the signal then you get almost exactly the same curve with one thousandth of the data transmitted which is also a very cool stat and you may also want to use it as a cache if you have applications on the ship if you have like a digital worker application running on android phones where the workers on the ship can get all the sensor data from the ship at any time they may not want to go via the cloud for that so you can ask the hedge box directly and there are also more interesting use cases which we'll get into which does more interesting compute and predictions on the edge also this is not your typical IOT consumer scenario where you have to have very very light Hardware it actually makes sense to invest in a rugged Hardware that won't break in this case and it also needs a bit of storage because it needs to buffer all this data it pretty like an average ship will produce something like 500 megabytes per day of data and it can be at sea for up to 2 months at a time so you need some storage there and you also don't want to lose that data if one disk dies so it's a raid configuration has a rugged PC with no moving parts so here's what the architecture looks like you have all these silos at the bottom the green boxes they're typically on their own networks you can't access them via the ship Network so you need a physical adapter and in our case those are always set up to be read-only which makes the whole situation less scary from a security point of view then it's picked up by the edge box what it needs to do is translate from all these different industrial protocols the Modbus the OPC OPC UA MQTT and a wide variety of other dialects that you need to talk once you've translated the data into into the cab same format put it on another cue and then it's processed it's matched against a list of high-priority tags and some is sent directly over the satellite link which is managed by cloud IOT core and the other data is stored and sent over a cellphone connection one of the things that we didn't do what we are considering doing is to move to EDX EDX is a framework for fore-edge solutions which will do some of the translation for you and it has a lot of components that you can put together to solve for from any of these scenarios one of the things so cognate is almost 200 people and it's just over 2 years old one of the things that we value very highly is speed and one of the things that we to get speed is to always try to use managed solutions where we can you don't want to reinvent the wheel and when you're looking at this next slide you might wonder if we do anything at all because it looks like Google is doing all the work but there is some stuff there like cloud functions we actually write the stuff that's in the cloud function and kubernetes engine where we have our business logic so on the cloud side basically what i io T core gives us is a managed connection we don't need to scale that thing and it has encryption authentication is taken care of and it conveniently puts the data on the pop sub-q which we use for the rest of our system so it's to us it's we just interact with the pub sub q which triggers cloud function which translates the data on the Q into API calls to our API gateway so our our solution is used by many customers in many verticals many industries which have slightly different ways of sending data so not all of it is going through IOT core and that's why the API kind of access to the one gate which is the common denominator there that's why you need the translation same thing with the daily logs the the backfilled data it's uploaded GCP no Google Cloud Storage I mean and that triggers a cloud function whenever a new file is uploaded which does the same translation so it's the same protocol buffer based format now when we get to the yellow box we have a pipeline for processing the data structuring it so that it doesn't use a lot of space and computing roll ups so that you can get you know do advanced queries with millisecond the latency and this pipeline we've run that ten million data points per second so every component there the kubernetes engine autoscaler pops up skeltox incredibly well and BigTable so it all scales horizontally it's very fast and then when we do run predictions on this data we use ml engine to host those models and we use cloud scheduler to do periodic predictions no no so we don't write back to the control systems so the use cases that we have are about advicing say the captain you know you need to slow down a bit because you're using too much fuel or you need to clean the hull because there's marine growth which is slowing you down it's not it's something that you can do parallel periodically maybe you do a prediction every minute but we do there you can think of cases where you'd want to kind of do streaming analytics when it comes in but right now it's not necessary security so the edge environment is kind of scary the control systems that you deal with there are set up they usually don't have the concept of authentication so if you can talk to them you can do whatever you want with them a couple of principles that we implied we don't want to invent our own encryption scheme without Google IOT core handle that we have no inbound routes to the edge and there's a physically separated network so we only read to these sensors so that makes it a lot less scary to be on the edge but security is definitely a big concern here so bring that together here we have one of the dashboards which will show you and verify that was not damaged this is the ship's motion in all the different directions axles 10 Hertz with 200 milliseconds latency so if you're shipping cars for instance you could share this with a car manufacturer and really you know assert that you didn't damage any other goods it's also very important input to the energy efficiency use case because if you have rough seas your engine will have to work more in fact very rough seas can make the engine shut down entirely which is what happened with the cruise ship Vikings guy a couple of weeks ago outside the coast of Norway rough seas and the engines tripped as a result of that leaving the ship in a very precarious situation and then when we move on to the more advanced use cases that involve machine learning you have an of course anomaly detection which is very much used in predictive analytics the the models they're usually based on clustering or forecasting so you're looking for data that's that you haven't seen before basically and then you're alerting someone inspection we have a lot of sensors that are not just you know pressure temperature flow we have a sensor in in our pocket which is a camera and you can use that to extract loads of interesting information using image recognition so inspection and we with trained for instance last year we trained an auto ml model which detected damaged wires and you can imagine having drones to fly around and detect these these faulty things or corrosion which is already being done by some companies and in those cases you definitely do not want to send all the footage to the cloud if you're on a satellite connection yes so guide I heard some rumors about your demo last year that you did some had a windmill on stage and did some machine learning as well that must have been pretty terrifying yes a good demo is exciting to the audience and very scary to the presenter so I think you know especially when you are moving parts and you're doing machine learning which is never really 100 percent that's that's always scary but you know the one trick is you can always blame the Wi-Fi but yeah so I think we should which would try to top that and do something even scarier this year yeah definitely and that's why we brought this tiny model of a chemical plant it continues the streams live sensor data to cognise data fusion via Google IOT core so typically you use this sensor data to visualize visualize it in charts like you can see soon on this screen so here you can see the temperature it's about 27 degrees that's pretty hot but there are other ways to visualize your data as well in cognate we see the power of using 3d models as a tool to visualize your data and that is definitely I use a tool for the future I see that but what what if you don't have a 3d model that's definitely a big issue and a challenge for the industry because either you don't have a 3d model or perhaps the 3d model already got got all from the day of assembly and was has become kind of useless so that's why we have developed an application that helps you generate this up-to-date 3d models in order to visualize your data in a better way you're not connected I think okay the way it's the Wi-Fi yeah definitely the Wi-Fi so what we can see here is the model on screen I take a single picture and I detect features on it I could take one more and then it guides me through the entire process so this is something that I do now on my phone but this is something that you can do with any kind of camera and it's not something that you have to do yourself either okay so you just need the photos for this to work you don't need a special camera you might wonder why there's a helipad here that's because sometimes when we run this we use a drone unfortunately if you look closely on the sign outside the door here it says no drones are allowed and yeah that complexity of getting that permission in time was unfortunately too high for us but it doesn't matter you can use anything like your mobile phone or drone or you know GoPro on someone's helmet it doesn't really matter yeah so what's happening to those pictures right now yeah now that I'm finished taking pictures I just upload them to cognise data fusion where the magic happens and right now it's processing all the pictures and should we take a look at the final result all right let's try that nah Wi-Fi has failed and here it looks like we've got a nice model to it so suddenly we have a 3d model that's up to date with the exact same dimension the exact same proportion and also you get a good overview of the texture on the model as well but if you want to view the time series in their right context here I guess you have to do some kind of manual yeah yeah so the thing is that this is definitely a really good tool for the for the future of the industry and but the thing is that wouldn't it be cool if we could link this model with the data that we have inside the cognate data fusion platform and that's actually something that we can do so if we take a close look on the tags that are located all around this model you can see tio2 tier one those kinds of tags is something that is present on all platforms or in ships as well and identify the different assets that identifier is something that we also have in cognize data fusion so what we can do is to use the same pictures that we took here and try to detect all the different tags using Google vision and when we have detected them we store the location of all the different assets and also which asset it is shall we see if it got them in there and the end result is here so what was happening behind the scene there is we used a model called Yolo you only look once which is an object detection model and that thing is trained to look for this this mission here right target's tear the tags but it it can identify other stuff too like rust or faulty wires etc and place it right there in in the 3d model yeah that's instant situational awareness there Andy this room is too hot it's so we can demonstrate this for you let's see if I'm hotter or cooler than twenty eight point two degrees so I'm cooler yeah I'm almost dead I guess maybe it maybe you have some cold feet yeah but then we're the inside operational limit again and that's good because it turns red when it's above 27 yeah you're actually cooling the room down yeah but all right so yeah and this is the model that you can update 24/7 at least when you have a robot to take the pictures for you and that's really convenient when it comes to detect when corrosion when corrosion started you can actually travel back in time you can check whether the corrosion mark was there only two weeks ago or perhaps two years ago so to complete five minutes ago we only had a chemical plant that streamed live sensor data but now we also have an up-to-date 3d model that displays our sensor values in a really intuitive way thank you [Applause] you can you can do this with real industrial equipment - all right this is data now streaming in from the North Sea and it identified the plates on that equipment - and also here you can find some things that shouldn't be here for instance these ladders they're not supposed to be in this zone so it's a health safety environment issue right there then let's wrap up here's my checklist I wish this checklist was shorter but it's not I mean this is the reality of why it's hard to create value in industry so you have to start with the valuable use cases start with that and talk to the people in the business because you will depend on them to implement it later so it better be something that they think is valuable - then you need to figure out if you have the data that's necessary to solve those use cases usually you have the sensors but you may have to do some work to liberate the data and then you need to clean the data make it so that you can actually solve your use cases for instance linking it to a 3d model it may seem trivial when you have kind of a plant with four sensors but if you have a hundred thousand sensors and you send the data scientist in to find the right sensors to do predictive maintenance on your one compressor or whatever it's going to take months if you don't have good tools to figure out what sensor is is where and which and relevant to the use case then you need to do stuff with the data so if that's algorithms usually you don't build your own you merely apply you know other people's work most companies don't do machine learning research they do application of known research to new problems now once you're you're satisfied with those results you need to monitor the quality of the data because your model probably won't handle you know garbage data coming in and these systems these sensors will give you a lot of crap data from time to time there will be connectivity issues there will be you know flipping bits here and there it's it's it's a quite challenging thing to do so you have to build that trust in the data and then you have to change the way that people work in the company there it's very useful if you kind of introduce your use case early on so they know this is not something you made up in worst case you just you've done a lot of work and you get to this stage and it's like way this isn't useful that sucks so and then last that's when you've done all those things you profit and that's what LT is really about otherwise it's not sustainable here are some pointers to get you started so for IOT core all the AI solutions that Google has to offer we have a booth down in the lower level and there are industry solutions so it's on the left after taking the escalator down contact us there if you have questions thank you and also don't forget to rate this talk we definitely appreciate your feedback so that you know next year we can have an even better talk and even scare your demo thank you [Music] 