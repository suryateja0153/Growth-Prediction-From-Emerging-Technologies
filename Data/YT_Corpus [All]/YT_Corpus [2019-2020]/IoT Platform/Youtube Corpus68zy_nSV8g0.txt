 thanks everyone for joining us this is our online Tech Talk predictive maintenance on IOT data for early fault detection with Delta Lake just a few housekeeping notes chat we're kind of using that if there's any you know audio issue is anything I can help with I'll drop some links in there and then the Q&A is gonna be where you're gonna want to submit your questions so hopefully we'll have some time at the end and our speakers will will answer some questions and then subscribe today so quick call-out for our youtube so it's linked there and then I'll drop the link in the chat in just a moment but we we record all of our Tech Talks and we post them to YouTube so that's the link to access our playlists of one of our dev rel managed online on my meetups Tech Talks and workshops that we run so you're you're free to revisit the content at a later date so we'd love for you to subscribe and check it out there and then so I would like to introduce our our first speaker so I'll dive right into it so in a detail would you like to take it away sure hi everyone I'm on in the time a Solutions Architect of data breaks I love working with data good data by itself is great to have it's even better and you can reap the valuable insights from it look forward to our session today and hi there thanks very much my name is Danny Lee I'm a developer advocate here at data bricks I'm working with Apache sparks since 0.6 so I guess I told you a little context but how long I've been doing this for Malaya was a senior director of data science engineering it cooker and I'm also a former Microsoft involved in projects like cosmos dB the earlier hdinsight of the precursor to HT insight called project isotope and yeah there we go so that's a little bit about ourselves so hey before we dive into him we're actually gonna be shifting if you've been watching some of our sessions from the past you'll notice that typically we always go with slides and so don't worry we still have slides so if any of you are folks our slide junk junkies we still have that but indeed it has really great mind master that actually allows us to talk about some of the concepts that we're going to go through today so I'm sort of teasing up the context that we're gonna be first talking about predictive maintenance in the IOT scenario and the use cases that go with it and then we're gonna go ahead and talk about specifically healthcare and in the case of healthcare we're gonna focus on one scenario simplify things and talk about EKG machines okay about getting sensor data i/o T sensor data from the DK machines in order to be able to make sense of that now all this is within that context of building your modern data platform using the fundamentals of Delta Lake and EMA flow for us to do that and then ultimately what India's belief is calling us out here is that with your Cloud Data Platform we're altima lis gonna need to build a multi hop streaming pipeline to be able to achieve the goals of being able to analyze this data so without further ado let's go to the real star of the show so today we are going to talk about how Delta enables predictive maintenance use cases using IOT data and so why is this important so we have we want to increase the reliability and extend the useful life of all these expensive equipment and at the end of the day it translates directly into cost savings and how are we going to do it just like a human has got some vital as parameters you know said a machine also has got some parameters as well so what are examples of such sensors you have temperature variations you've got a vibrational analysis you have got oil viscosity all of these if we were to analyze would give us indications of failure you can diagnosis failure from there you can classify failures into different categories and at the end of the day you want to be able to recommend relevant maintenance actions at the time in a timely manner so a key enabler of PDM that's short for predictive maintenance is continuous monitoring and that why do we need that because it establishes baselines for what is a normal running condition and there have been a lot of idea in the IOT section there are lots of industries that have contributed to it and so now that there is easier integration with the sensor data allowing the health of the equipment along with this lifecycle to be transmitted to systems where we can start to baseline and do this analysis again there are parallel advances in many fields including advances in say sensing technologies better connectivity options and breakthroughs and ml and AI one reason why this is different from traditional or routine time-based maintenance is because the repairs are done only when it is truly warranted and you would ask what are the markets where this is particularly useful these are the top verticals leading the IOT and predictive maintenance error so there is a manufacturer right on top and then we have transportation we have energy and logistics wick we have aerospace but there's no reason why it should be restricted just to these industries these are the verticals leading the sector but all other industries can benefit from this as well and why should we care about it let's look at what is the growth projected for these use cases there is a compounded annual growth rate of more than 39 percent for the US market itself and a little over 28 percent for global market if you were to take it and that's huge we definitely want to understand where this industry is going and where we can make better utilization of resources and technology as Dan mentioned today we are going to talk about just the healthcare sector but everything that we are talking about can apply to all these different verticals so the scenario that we are going to consider is how can we predict the next repair to an EKG machine so what are some of the challenges that we would face one is around scalability so we are talking about big data here there is a variety of sensor information it is being ingested at high velocity and it needs to be stored over a prolonged period of time so that you can understand what is normal working conditions and all of this data sometimes can be extremely sensitive like to fluctuations these machines are highly calibrated so the accuracy of the data is important you know sometimes you might have late-arriving data sometimes you may have data which is wrong which needs to be corrected at scale and these these work loads do not operate in isolation you would have to combine it with other data sources and you know if all of this was possible then you need to be able to do it in a cost-effective manner to handle all these little files coming in in a continuous stream managing the end-to-end latency dealing with streaming challenges and you would have to work with folks who work with a different variety of tools so some of them could be just bi reporting some of them could be you know machine learning at different levels of sophistication and as you start to build you not only have to manage the data but you need to manage the ML artifacts that go along with it and have their have a lifecycle of its own so who are the folks who typically deal with it there are domain level SMEs for sure but there are data engineers there are ml practitioners there are analyst all of whom needs to work with this data to make sense of it and you know start to understand when the machine is expected to go back because it's far cheaper to predict it and repair it early on than to let it fail take it offline and deal with all the mess that follows as you are going to build a modern data platform there are certain characteristics that folks look for first is you want the speed and low latency of a streaming platform you need the scale and affordability of a data Lake and you need the reliability and performance of a data warehouse of course there are trade-offs like so for instance latency and cost there would always be a trade-off but at the back of our mind these are the three primary characteristics of a modern data platform to be able to handle into n scenarios and in that context we will see how Delta aids the scenario how it addresses most of these challenges that we have talked about today's talk is around Delta lake so what is the Delta lake it is nothing but a combination of Delta tables again what you know what is the Delta table it's a set of data files if you've been working with spark bar K is a format that you would be exceedingly familiar with park' is highly columnar it's optimized and it has open source and open format so there is no locking that comes along with it Delta is nothing but parking along with a transactional file what do we mean by that so if you look at this little picture here this is what you might be already familiar with you have partitions let's say we are dealing with time series data here so you have a date partition and you have these party files this is where your data resides alongside with it in the same directory as your table say my table is your reference table Delta table that we are talking about you have something called a underscore Delta log where you can see a series of JSON files this is where each of your transactions on the data are logged and why is this important it's important because this is what brings in your asset transaction capability so when you want to do updates and deletes and merges at scale this is what helps you it helps you with time travel and all of us are guilty of occasionally messing up a pipeline making a human error or who knows there are sometimes upstream processes which are beyond our control which may have come down we've already ingested it and you know introduced some deficit C with the data so if you have a version and a timestamp associated with every ingest that is coming along you would be able to roll back in time you can control which version of the data set you want to work with this is especially true of ml practitioners who want to compare experiments so for them to have a consistent view of the data that every one of them is working off for their experiments to be able to compare and contrast which one is a better model where time travel and this versioning capability gives us the ability to do so there is schema enforcement as well so it is great that you have a lake a data like where things can be dumped into but if you do not do it in a disciplined manner in the sense you want it to grow you want people to be able to put in there it's like variations of their data sources but at the same time if you give them too much control then instead of pristine data Lake it can turn into a swamp so with schema enforcement you can control and discipline in a discipline way an evolution of the schema as opposed to just a dumpy out of everybody putting in whatever they feel like these tables are of course registered with the meta stir so people can look at the schema and understand you know what it is that it contains we are talking about cloud here so the storage is important and we have storage so is suppose you're in the AWS ecosystem then it is s3 if you're in the OSHA or eco system it's going to be blob storage or adls or whatever it is these have become exceedingly popular because of some advantages they offer primarily around affordability scalability and high availability people will be able to dump files in there and have direct access to it but there are a few shortcomings of it also as the number of files starts to grow listing this directory this you know folder and bucket starts to get exceedingly long and so there are some alternatives around it which is what we are going to see in the demo today primarily around notification so there is an s3 sqs connector for instance that you can listen into only when a new file comes then you're going to be notified about it and that's when you pick it up so you do not have to list over and over again and there's a new feature called Auto loader that was introduced in very a you know you can avoid having to set up that infrastructure on your own so you if you use a slightly different way of offer in Jes then you can escape this infrastructure part as well so enough on storage why are we doing all this because we want a single source of truth for all our downstream apps and all the inside generation that folks are doing can be done off a single source of truth and there are advantages to it primarily around the fact that there is compute and storage separation spark inherently supports it and now we are just enforcing that it is important storage is far more cheaper than compute and if you can put it in inexpensive storage your storage can grow independently and you would use your computer only when you truly need it there's increased reliability and performance so we'll talk about the architecture shortly but Delta by itself will help provide higher reliability to your data through those as the transactions with your ability to do reader/writer isolation so that at no point in time you see bad data and with multi-hop we'll see how you can avoid you know propagating your fault throughout your pipeline so there's better fault isolation easier debugging and there are different sls on your consumption pattern depending on your appetite to consume the data and we'll see what those quality guidelines are and then structured streaming is a very very important part of Delta as well those of us from their big data architecture and Hadoop systems and all must be familiar with lambda where you need two ways of handling data one through a batch method and one through a streaming method and over time it's difficult to maintain there are data reconciliation issues with Delta you can have batch and streaming together which is again going to be very powerful and so with that with that let's go directly into the architectural blueprint the bronze silver gold is the multi hop pipeline that we are going to talk about and let's go straight into what we mean by that so this is your platform this is your s3 bucket we are talking in the context of sensor data coming from your EKG machines so as they land on the s3 bucket you can ingest it directly or you can have this sqs connector only you will be notified only when a new file comes in as the file gets dropped it goes into a brown zone and then there might be further enrichments and refinement to the data there might be a need to join against a reference table so that is when this is going to become a silver table that's enriched there are further aggregations and roll-ups and that's when this is going to be considered as a gold table now remember these concepts of bronze silver and gold are quality guidelines only so if your use case demands two sets of silver tables or maybe you want you do not need a gold table that is perfectly fine it's just a blueprint and you should not consider that to be like set in stone let's let's talk about those personas who are going to be using it let's say you have a business analyst who is interested in aggregating data from the gold zone which uses a window function of the last few minutes now his use case should be allowed to progress in parallel to an m/l practitioner who might be more interested in tapping into the silvers own to repeat an experiment from data produced a week back and at the same time a data engineer may want to look into the brown zone to reapply a different set of business transformations on that data that may have arrived maybe two weeks back so these are different timelines we are talking about we're talking about different SLS we are talking about people with different wearing different hats performing different tasks using different tools and they should all coexist and be able to have a single view of the data here we have a the notebook in two different sections to correspond to these activities of ingesting and then read from bronze refinement and then read from silver and then finally doing the roll-ups to have their gold tables and tapping into it so you can think of like a bi analyst using an external tool like something like a tableau or a power bi and sitting on their desktop and being able to tap into this ecosystem as well there's a notebook which has all the includes us all the constants there's a set up and then these are the six steps that we are going to go through so actually so let's scroll back up real quick if you don't mind and zoom in a little bit just to show the notebooks real quick you know mind mm-hm so I want to call out Andes has create this awesome demo for all of you so these notebooks will just like the recording for this session these notebooks will be available for you to download and test out yourself actually ok so that's an important call so we're not just showcasing like you know how to do it good luck she's actually created a really cool set of notebooks that you can go ahead and you run your own multi-hop pipeline here ok and so each one of these call-outs you know the one data ingest one Avery bronze and so forth and so forth these are associate to a different notebook that she's about to show and they are all like I said no before they're all going to be available in our data Brooks tech talks session and and github repository there we go and that you can then go ahead and download and run yourself and just as Karen noted in the chat you're more than welcome we'll send you the link as soon as it's available by email and also it'll be connected to the youtube description as well so just as a quick call out so I just want to call that out sorry apologies apologies for that so this is the s3 bucket and we're going to show how you would have to do it directly consuming from the bucket or let's say you have an sqs stream which you have given privileges to receive events from this particular folder of the bucket let's say we have this folder sensor and then this is where the magic happens of joining the two you're saying that every time a new event or a new message comes into this bucket into this folder sensor then start an sqs notification we'll come back to this picture a little later but let's take a quick look at the data set that we are going to use there is this public data set from kaggle it's a multiple multivariate time series and you can you can see what it looks like there is a machine number it's a time series so instead of having an actual time one two three four says that okay one is coming before the other there are three operational settings of the machine and there are 21 different sensor readings associated with it we have a training and we have a test and of course there's a ground truth which corresponds to how much is the remaining useful life of that particular machine so when we are going to do some ml on it that will become handy there's a little setup that is needed so you're going to do a streaming use case and streaming not only do you account for data path where your data is going to reside in those three zones that we talked about but you need a checkpoint location also and wise checkpoint directory particularly important in the context of streaming is because if for whatever reason you have a hiccup in the set up then you do not want to start to reinvest from the beginning or you do not want to lose data that checkpoint will help you rewind the system to that period of time so that you neither have drop data not you have to placate data and you're going to have a checkpoint for each of those zones that we are talking about you're doing a quick check we ensure that those directories have been created and you know as we noted earlier no no data works in isolation at some point you would have to combine it with some other useful data it might be yet another stream it might be a batch table that does not change as frequently that is commonly referred to as a slowly changing dimension in the industry so there might be a need to join against it against such tables and this is just an example of what it looks like right so for example when we're talking about the stream of EKG data that Andy has been talking about right so we have all the different operational metrics and the sensors that are coming in from the KPG machine that tells us it's time to live or time to failure right because we're trying to basically use the sensor information to be able to predict okay this this component of the EKGs the sensor rod is gonna fail at this particular point in time the the key context though is that each machine is gonna have a slightly different model right so they have a model number that model number associated to when they were created basically how old machine is what components does it have things of that nature so the this reference table that he's referring to is basically an example of a very slowly changing type two dimension or the main table that contains the EKG model information so that way you still have that info and combine that with the stream of sensor data as it's coming in and together you're actually gonna you your need to join those two sets of data together to be able to figure out okay what ultimately will be the time to failure and just as we've noted right from the beginning we're trying to go ahead and understand what is the time to failure and trying to get ahead of the curve so we're not waiting for the machine to fail we're gonna predict its failure rate it will predict when it's gonna fail so that way we can repair it before we actually have to go to something a lot worse after all when it comes to EKGs and other medical equipment takes a long time for supplies to come in to fix it and also it takes a long time to fix it as well so these are things that we're if we can get ahead of the curve it actually will allow smoother operation of the data of the excuse of the machines that we have as we noted there is an include directory which holds mostly constants that can be included in all the other notebooks so these are the examples of like where are the pots and the schema of course so we talked about the each of those machines having a time series associated with it three operational settings and 21 different readings with God let me go and start up the streams and load some data while we talk so so as is currently working on loading the streams remember the context here is that she's dropping the data some sample files as an example into sqs right into s3 excuse me and so by dropping the data inside there what we have in the notebook is an example of a stream that's constantly running and processing this data so as the data is coming in and being written into the file system the data books notebook in this case or basically your structure stream if you're not using data Bex but you're just using a spark structure streaming it's going in and actually processing this so right now it's just a simple example the files are being dropped directly into the s3 folder and then for sensor data and then when we flip back to the notebook you'll be able to see the the data being processed as you live basically there we go again as you notice the data frame talks about the same schema they ancient number the cycle time and the the operations and we should start seeing data come in right we might need to drop it again this because we forgot to load it first so let's see if it let's see we might need a drop did the files again just in case yeah or a different set of files but nevertheless the context is that this is what's cool about this auto loader mechanism as the data is being dropped into the file system just like that basically you'll see the data being processed automatically being picked up by source for streaming right from the get-go and by the way there are multiple mechanisms to that so what's really cool is included in the notebook the different options of how to process this data and here we go right but we're just specifically using the auto loader for this particular scenario so you can flip between the dashboard view of the live graph as well as the raw grass so you begin to see the spike and now you have the sliver of information that is coming in and you'll continue to see it because you can either trigger it on a certain micro batch interval like a three second or five second or one hour whatever you consider necessary to control the ingestion of inflow of the data and as you're as we speak you can start to see the data go in you can flip into this view and say okay this is the second batch since so there are two batches the first batch did come through and this is the second batch in how many rows it has and what what is going on at the same time we have a right stream operation and notice that the format here is Delta we're using a checkpoint location and you're beginning to see this data files streaming into the into the right part of the string so we broken it into a read stream and a write stream both of them are working in tandem and once this is over the very first sign that you do this you need to create a table and point to that location subsequently you do not have to do so so we have our stream going on let's go to this was one data ingest let's go to the second notebook which is one a three brands here we are going to read from the branch table and you will notice that as we are pumping data into our stream we are still able to read data which has already landed so you don't have to wait you do not have to pause the system while the ingestion is going on in parallel you can continue to work off it the thing to emphasize here is that no point you see bad data like say for whatever reason if there is a failure in the system it's not that a few nodes have bad data and so the analysis that you have done further out stream is now completely invalidated because you weren't sure what is the state of the system and this is what we refer to as the reader/writer isolation and if you were on a system like hi for instance you would have had to do something like ms ck repair table just so that the statistics can be updated so that you can read from the table all of that is not necessary there is no never any partial data nor any bad data that is presented to the consumption layer we can do other things like counting from from the data that has just landed and some other things are like you can do a describe history on the table so this is where you begin to see something new there are these versions so which was not not available in a party file or any other data files that you were working off so here you have an automatic version number and a timestamp for you and if required I can roll back to a different day or a different version number the metadata as before if you do a describe detail on the table you will see that the format is Delta that's the name that's the data part who created and all the other details but it is important to know that these are very small details around the data that is stored in the metadata with streaming and then the volume of you know changes that that is possible on every ingest of the data we store the actual schema in the underscore delta underscore log directory and let's take a look at that directory and see what it looks like as you can see we saw the JSON files and we will see them the party files in a in a parallel directory let's take a peek at one of these JSON files to see what it looks like and sure enough you see the schema you see the CMAs string of having a name of a unit number of integer the metadata the cycle time and each of those sensors an operational settings so this was just to you know emphasize that it is not stored in the hide meta store for instance it is stored in the Delta transactional log and that is how it is able to know when a version changes when an update or delete happens it can track the operation so right here in your operation this is just streaming data coming in but if somebody were to delete something like accidentally and it was a delete operation that would be detected here this is especially important for gdpr requirements when somebody may need to may want their data to be removed from a system in the context of personal information safeguards and with that I think we have covered some of the basic aspects of what a Delta table looks like how the history can be viewed what where is the metadata store let's move on to the next one so the next table here is - which is data refinement and as before we are going to have a different mindset a different hat a different set of rules playing in here this is where we want to join against a a new reference table we want to apply some business transformation we want to cleanse the data and then persist it back into the silver zone so like before we read from the bronze this looks very similar to the data that was coming in P because we did not make any transformations major transformations on the bronze table this is the reference table that Danny was talking about earlier which has got information about serial numbers specific to a machine so that when you're joining you know in what context you are looking at the data and that's the joint right here you've done some basic filtering and joining against it it's the ancient number or the machine ID against which you have joined so you come up with a data frame which is yet another data frame but it has got these additional information associated with it and this is what we persist in to the silver zone like thank the goals on this is what is gets persist into the silver zone let's move on to the next one and this is an ml practitioner for instance we would want to come and do some analysis on the data so this is exploratory data analysis you might want you I see what are the correlation between the different sensor readings you might want to use a sequel to do this I mean you might be a Python programmer who wants to do this or it might be an hour library that you want to use so all of these different languages is easily supported and the results are just the same so if we were doing something very simple like a count in sequel versus in our versus in Python they're all going to be the same so what it's trying to say is at this point you have a lot of tools at your disposal you have certain frameworks and languages that you're comfortable with and you can use any one of them to work off the same data you do not have to copy it anywhere else and now that we have our data well taken care of it's reliable it is has landed now we can do some modeling activities on on the data we had earlier referred to the fact that while you are doing your your experiments all the practitioners should be looking at a consistent view of the data and that is why it is important to see say decide which is the version of the data that you want to work with so that you can get a deterministic training set you see this command would have said select star from your table but we have just added version as of X so this means that everyone who's running this irrespective of when they are running it are going to look at the same view of the data and that is what we mean by a deterministic view of the training data set we have our we have our fields already defined what we are trying to do here is have an additional column called the time to failure which we determine by finding out what is the maximum cycle time that a machine like that usually encounters and figuring out in what transition period of that machine cycle we are in and doing a very simple calculation to find out what is the remaining cycles left let's say we are considering this analysis for the next 30 days so our period here is 30 days and we want to see whether in this 30 day period we are going to have a TT F of 0 and 1 and that is what is going to determine our boolean value that is the label that they are going to attach to the data so at the end of this period we have got two additional columns this is the T TF and the label we do the same thing with the trink data set and right now we have our data ready for analysis the usual thing would be let's establish a baseline model let's do a very very simple regression model to determine 1 or 0 of failure so that then we can add on to it other models and compare and contrast as - are we getting better with our prediction is one algorithm more suited than the other for this particular scenario again if you have done these type of scenarios there are fairly boilerplate code you have a set of features you have an assembler where you are saying okay these are my input features that I'm going to consider as part of my experiment you have a scalar so that all the features are within a certain range you apply the logistic regression and there are certain how-to parameters to it let's say we have established this to be ten and point five and you scale down some of these features you build a pipeline but then you decide maybe that's not going to be right maybe I want to experiment with some other set of hyper parameters and that's when you introduce a cross grid across validator and you say okay over five feelings I want to experiment using a pattern grid of a few parameters this is just a demo so these are very small examples small ranges but you get the drift you would be experimenting with hundreds of these and not just across a single model but across several models but it's important to establish a baseline and so yeah we go ahead and we do a fit with the training data set we find the best model of all these iterations we have run and now we are displaying the summary ROC we have a baseline model so we can stop here or we can look to see what are some of the other techniques that are other architectures that might be more suitable for this type of dime series example so LST M stands for a long short-term memory it is in the family of noodle nets and sometimes it works very well for this type of use cases why because it is time series and if you take consecutive time series and you study it to say that when you have previous values like this it there is a chance that a machine fails but when you have previous values like this the Machine does not fail so it's not a single value a single row by itself that you are considering but you are considering a group of data points all consecutive in time to help make that decision while be doing this because it has shown that these sometimes work better and so as an ml practitioner you'll have to do your due diligence to see does it truly work or not work and that's where you have set it up like before you have the TTFN and the label column you have your validate data set which you use to see how well your training was it an overfit or an under fit and then you do similarly you do your feature scaling you reshape it because then lsdm has to match you come up with your network architecture so in this case we have so many units so many epochs your drop out whatever layers and then you start with the Train so these are Findlay boiler plate code but you have to do it over and over and over again and that is where the rigor that you put in the number of experiments that you do you to figure out which is your best model matters so we come up with an with a set of parameters now we have we have given it its training data set and it has gone through the epochs it has come up with an accuracy and a confusion matrix so what do we want to do we want to write a very small function to use this model and be able to predict the weather that machine is going to fail in the next 30 days or not because that is where we set out and so let's say we want machine ID 16 so you're going to call this function pass in the ID and it's going to tell you whether it's going to fail in the next 30 days or not and looking at this value you'll determine should I schedule a maintenance for that machine right now or should it be next week or should it be in the next month or maybe it's perfectly healthy and I do not even have to worry about this while you're doing all of this let's show you what is happening with ml flow you have all these runs recorded here and I'm going to switch back to my mind map here to show you the companion to Delta Lake which is the ml flow and the analytics that we have done so far you might have a reporting use case right you have you will come to the gold tables and that is where this would become more relevant you can do a lot of ML you can do single node using psychic that we saw you could do Banda's you could use you could use extra boost you could use distribute if your data set is large you might use the spark ml libraries you can use deep learning so these options are completely at your disposal and depending on your use case depending on your appetite depending on your skill set you might be using a combination of them as well so what is ml flow ml flow is needed for the end-to-end lifecycle management of the artifact itself and if we look at this little chart here there are four components to it tracking projects model and registry so tracking is what people often go to because in the past the first step because this is what helps them record their experiments so we just ran through a few of them now can we remember a week from now as to what we learn and how we land it and what was the code behind it what was the configuration what did the model produce can we can we do that it's actually difficult unless you have a very very disciplined way of doing it and when you start to do hundreds of these it is going to become extinct exceedingly difficult so tracking server is constantly recording all of this for you so that you can go back in time and search and say okay I want to find what was the best model that had produced a week back because now I am actually regressing and I want to understand what changed did i do - cause that regression to happen or I'm done and I can justify it I can show it to somebody else and say why I have chosen this to be my best model under the condition under present circumstances so you choose your model you know the best one and now you have the task of taking this to another environment because you may be built it in a dev setup on your laptop now you have to take it into a staging environment so being able to package the model for it to be reproducible in many platform becomes your next challenge say you already did it now the next step would be there are different consumption patterns like maybe somebody expects this to be a simple Python function that should be exposed somebody wants it to be a batch job somebody wants it to be a streaming one somebody wants it as a tensor flow model so you have a lot of deployment options which is completely orthogonal to how the model itself was built and that is why it is necessary to make a distinction between the two the last one is around registry so in the coding world we are all familiar with Gibbs as like a source of truth we go there to see who did what what is the latest version and if you make it make a bad judgment call you can always go back and to a different version you know who did what if somebody breaks the nightly regression series then you can go back and pinpoint exactly what happened and you can recover from it the same thing is needed for the ml world as well there's a need for a centralized location for this artifact that is precious to you because these artifacts are living breathing things they you know once it is set in motion and it is taken into production somebody needs to be accountable for it there is auditing around it there's governess oscars around it and then every morning will become stale at some point so you need to be able to understand when a model is not as useful as it used to be to put the next best model out there and having this lifecycle management is what model registry is going to help us with now let's go back to this and in that context look at let's pull one of these experiments and see what does it look like so ml tracking server is automatically registering some of these for us so there's an experiment under the experiment you've got these multiple runs it tells us when it was created any additional notes what were the parameters used what's the metrics a unique ID for each one and and at the at the same time if you're not happy with it you can come back and and try something else but you have the option of tracking it as well as going back to it at a future period of time let's move on we have gone through our silver model a silver zone which is where ml practitioners usually come then comes the gold this is where data roll-up happens for bi reporting purposes so you can do a read stream and a write string together this is just an example of it and at this point if the table is already created then you can go and query it so this is your gold stable this is simulating an analyst coming in and reading curated data using sequel which is typical tooth for analyst and you can you can see the unit number in the count and just for fun sake let me go to my tableau dashboard here you see this is the cluster that I'm working off right now it says field engineering cloud data breaks blah blah and that's the connection that I have from my local tableau setup and I'm pulling in different types of tables so this is the schema these are the tables I see the data I can refresh it and from this point onwards you know if you are into reporting then you would know you know what are the best charts that you want to draw out of these data points how you want to refresh it and so on so forth but the thing to point out is you are actually looking at all the data that you would want it's sort of like you know pulling a sliver of data somewhere else pulling it down this is as quickly and to end as you can possibly get to so we will go back here I'm just taking a quick look at a time yep and let's close out on this to summarize we had the use cases we concentrated on a single one we saw what were the unique requirements of these are the advantages of Delta and along with the Delta you have ml flow which helps you manage your artifacts so it would be fair to say that there are several advantages to materializing data frames in this multi-hop pipeline and that includes fault recovery easy troubleshooting and ability to cater to consumers with a different access pattern SLS so we Delta and structure streaming it gives you a very clean simple architecture blueprint and that can be applied in very similar use cases we saw the loading and how incremental ingestion at scale with Delta can be achieved and Delta can works well as a source as well as a sink and so we saw how we can change different stages of the pipeline the other thing to consider is your jobs sometimes start out as batch your ended but if you design it this way then you can easily transition from a batch job to a streaming job with just few configuration changes as opposed to real designing recoding all of that there is always trade-offs between cost and latency so you have the levers in place in the count in the in the injection and the read stream constructs to use something like trigger once so if you say trigger once and give an interval of 12 hours that essentially becomes your batch job you have processing interval which you can control your cost also so you can have a processing interval of 5 seconds versus 15 seconds and so on so delta lake is indeed the next generation data lake for building your pipelines and the analytic offerings as it combines all the desirable characteristics from all the three different platform types namely the low latency of streaming the scalability and affordability of data lakes and their reliability and performance of warehouses we also saw ml flow how it offers several building blocks to manage the analytic asset lifecycle tracking hydrations of model training model selection packaging and subsequent deployment and promotion to higher environments so with that we can see if there are questions that we can address actually do me a small favor switch back to the slide so I'm gonna go ahead and just finish it up a little bit and then also I will go ahead and either answer the questions directly myself or I'll ask you to answer the questions if that's okay sure alright cool so justly yeah let's start with the dock like containers if you can click on present so we can show the full screen thank you very much thank you very much perfect so don't worry I'm going to answer some questions and there's a very good good important call-out from in the zoom webinar chat that actually I'd like to go ahead and dress to because it's actually a very good point so for starters just to give you some context there are Delta Lake connectors that actually will talk to not just spark as you've been seeing here but will also talk to hive presto and other systems as well I flipped the next slide please there are many partners and providers that are already working with Delta link so this is the reason why we talk about doesn't like as much as we do and also if we can jump to the next screen where again please there are many users of Delta like okay and yeah just flip the next screen and that's not for the market feel okay so again if you want to provide some comments after the sessions over we just would love you to describe to the data books YouTube channel so you can keep the conversation going so now looks let's address because we actually have two very different views both valid okay one view was for example that there's a wide range of topics that's really interesting lots of food for thought that's pretty cool yet at the exact same time there's some other feedback which is also valid you know that oh the stitches too densely packed either make it you know 90 minutes long or cover fewer topics um in fact this is the exact debate in DITA and I were talking about it could should we shrink this thing down or make it a two part or a three parter um the reason we actually decided that it made sense to still keep it this place because you as the data engineer data scientist you're gonna have to do all this right this is what you just saw here today and indeed of went ahead and had to do this in order to be able to build up for her set of customers okay i OT customers and all of those components are there now don't get me wrong youyou could just easily turn out said hey but dude you could have just went ahead and made this into a multi-part the part of the reason why we did not do is multi parts because we're in realistic as we dive into each one of these issues what we're planning we had actually need five different sessions we did we thought none of you would actually be tolerant of that so what we're doing instead actually is we're gonna be reading a blog post to Oh up this session that actually will contain not just the full notebooks in the full context but also links to all the various different videos either already pre-existing ones or some new ones that allow you to actually fall through and build this entire system but what ultimately comes down to is that it is densely packed because you as a date an engineer a data scientist when it comes to building these multi-hop pipelines with IOT you're gonna actually have to be familiar with each and every one of those those concepts so again great feedback from everybody we really enjoy it it's so it's not like we're saying what's right or what's wrong it's just that we were trying to find that right balance of how to make sure that at least the context or the primer is there and again that's why the notebooks are going to be there right away for you to download as we're writing the blog and once the blog goes out we'll make sure to update everybody accordingly so that information is there for so you can run it yourself okay all right so there's also an excellent kind of EDA that an all credit to you for this one some folks really love your mindmap yeah I absolutely agree that was pretty that that is absolutely sweet okay all right so let's see okay well let's we've got about two minutes left okay and so like I said before subscribe to the YouTube channel so that way you can go ahead and we can continue the conversation there because this video will go up there and we'd love to actually continue talking to folks this way all right um I guess the the first question would go to you and indeed it was just like is Delta Lake capable of low latency queries IE fast queries you know using spark sequel for you know your time series iot data that you're currently doing yes you have a lot of luck in any system you have a lot of options for querying faster like Delta has things like file level operations Z ordering compacting optimized auto optimize bin packing all of these are geared towards your use case and so you have to understand exactly what your access and make sure that you are following you know alongside it so your question is definitely very loaded in terms of is it geared for a fast access it you know if you do not use the right partitioning for instance if you do not use the right Z ordering in your where clause and you're doing a full table scans of course it can be extremely expensive but that is why you have all these options at your disposal to be able to you know make the most out of it then you do you want to add to it no actually I don't but I do want to answer one more question before can we're gonna switch right back to you and and then the session because I want to make sure we're cognizant everybody's time there's a quick question about how do we connect a real-time application that renders predictive maintenance in the real time right where the calls can be a sickness okay and so the the quick context that I would basically go ahead and call out and then Nvidia I would love you to add to it it's basically that is the most the context of structure streaming right the the idea is that with Delta Lake and we actually had processions in the data Burke YouTube channel which talked about how you can have instead of the lambda architecture we can have the Delta architecture which allows us to deal with both streaming data and also with batch data at the exact same time in fact during today's session that's exactly what was shown which is that there were some queries that of the IOT data that you were literally reading the data as it was coming in live well in some cases we were going ahead and looking at historical data or joining the historical data to provide that context so anything else do you think I that I miss then Anita that should be called out oh cool all right well then hey Karen I think we're good to go I in terms of the session and before we close up I just want to say I apologize for anybody else we were not able to answer the questions but please do ping us on the YouTube channel because we are active there to answer questions okay and back to you okay angst any thanks in edita does a great presentation like Danny mentioned this talk is recorded and it's gonna be uploaded to YouTube and we'll also make sure you have the to the notebooks so watch out for your emails if you're with us on zoom' you'll get any more than 24 hours with all the the links as a follow-up email so thanks everyone for joining us and have a great rest of your day take care of bye 