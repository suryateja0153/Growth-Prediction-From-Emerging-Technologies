 hi everyone I'm Daniel and I'm Billy and we're from the Google cloud developer relations team our focus is on storage and especially cloud-based storage we want to help you make informed decisions about what storage solution to use for your iot project we're going to start off by discussing a little bit about the shape size and scale of the data every industry has unique data problems and we're going to focus on the kinds of problems that you might see when building IOT apps at scale then we'll see why the cloud is a great fit for data of this shape and scale and we'll look at how to ingest your data into the cloud we'll akeem we look into a number of storage characteristics that you need to take into consideration when reasoning about storage solutions and finally we'll examine a few common use cases and real-world scenarios that businesses encounter or map those business needs to technical storage requirements and then select a database based on that information and we've got a cool we've got a few cool demos for you to check out today so this is a storage talk although with a little bit of a cloud twist and one of the first things to come to mind that comes to mind when we think about storage is how much data can I store this is a relevant question especially in the with an IOT use case in mind connected devices today produce massive amounts of data according to statistics a billion connected devices by 2023 so this is a map of oil and gas fields in the Gulf of Mexico let's do a thought experiment let's say that we have 1300 oil and gas fields and each oil and gas field has a thousand sensors streaming data even if those devices is only streaming a hundred bytes per second that still makes 137 megabytes per second in total or eight gigabytes per minute or 11.5 terabytes a day you could try to manage that infrastructure on your own but there's a reason the cloud exists all the infrastructure headaches that comes with managing massive amounts of data is minimized by using the cloud so let's make sure we are on the same page here when we talk about IOT data what we really talk what we really mean when we talk about IOT data is time series data some of the common requirements for storing time series data include robustness for some applications data loss simply cannot happen we want high volumes reads and writes and we want low latency to go with these high volumes of reads and writes we want we want to have efficient ways of backfill historical data and we want efficient sequential reads they connect the devices the producers of the data we are talking about could be anything from smart home appliances to public transit cards whether you're starting with 10 devices or 10,000 using the cloud to manage your devices storage and analytical tooling provides security reliability and scale I will now hand it over to Billy and he will talk a little bit about cloud IOT core cool thanks Daniel so yeah our cloud-based solution for this is IOT core and it's you get secure connections easy ways to manage your devices and all of this is at global scale we've got IOT core and then this breaks down into registries which are the buckets that hold your individual devices from there the registries will automatically connect and publish messages to a cloud pub/sub topic and cloud pub/sub is our event stream manager that will allow your iot pipelines to scale you get fine-grained access controls and to end encryption at at least once delivery to ensure reliable and secure streaming of data in a fully managed service and all this is global by default publish from anywhere in the world can consume from anywhere but consistent latency no replication is necessary from there pub/sub integrates perfectly with cloud dataflow our fully managed stream and batch and batch data processing tool and you can use this with the open source Apache beam SDK you can subscribe to the pub sub topic and then run your ETL operations on these messages so this is our IOT pipeline so far setup your devices on IOT core they'll send messages to pub/sub and then your dataflow job will process it it works great with a handful of devices and scales up to handle a thousand megabytes per second we think back to the example Daniel just showed of those oil and gas fields around the Gulf of Mexico those are only adone ly produce around 15% of what this can handle so this pipeline can really handle a lot just to kind of give you that idea for a scale so we'll take the standard IOT pipeline and then this is the part where you of need to make a decision choosing a storage solution for your data every database has strings and you have to make trade-offs to ensure you're using the database best fit for your use case Daniel's gonna talk about the features we'll find in various databases and what kinds of limitations you might run into as well Thank You Billy as Billy showed in the previous slide the ingestion pipeline is pretty standardized the moving part here is storage so in this set section we'll talk a little bit about the different characteristics that the databases might have databases that on the surface will look pretty similar so if this is too basic for some of you we apologize for that but we think it's important that everybody is on the same page before moving on so we try to group to deciding factors for your storage solution into three pillars functionality sonship such as data model query languages partitioning etc scalability which in the IOT we'll always will be a deciding factor and lastly cost let's see if this makes sense to you so there are many types of data models just look at like how many books there are on data normalization or stuff like that and every data model comes with assumptions about how it's going to be used since the data model has such a profound effect on what the application can do or cannot do it's super important to choose one that is appropriate for your use case a good practice is to lay out the queries that you want to be able to perform on your data ahead of time that will give you great insights in what data model to use the relational model does a very good job of hiding implementation details behind the clean interface however the use cases we are looking at other data models might be a much better fit let me give you a few examples so we usually need greater scalability than what relational databases easily can achieve especially when it comes to very large data sets or ha very high very high right throughput a desire for more flexible schema design or a more expressive data model for example scheming read instead of scheming right so many people feel that a declarative language like sequel is much easier to work with than an imperative one like JavaScript and as we discussed previously the relational model and subsequently sequel does a really good job of hiding implementation details of the database engine so that means that a database vendor can introduce performance improvements under the hood without you having to change any of your queries on the other hand many known relational databases support specialized query operations that might fit our use case much much better one example would be efficient sequential reads and it is in a distributed system or first we might just address the terminological conclusion that this is all the same things it's just what different databases called shorting or partitioning or regions or tablets and whatnot but in a distributed system data will always be partitioned so when you have a system with data partitioned over several nodes how will you go about scaling your cluster up and down as more nodes get added to the cluster when the de database help you reassign data to new nodes or is that something that you have to manage yourself and if you have to manage it yourself will that make you have to compromise on your scalability goals in some systems assigning ownership of a partition to a new node is a online metadata operation that doesn't require any online or sorry any data motion spoiler alert I think Billy will talk about one of these systems in a little bit so replication replication basically means that we are keeping a copy of the same data on multiple nodes or multiple machines there are several reasons why you might like your data replicated so for once you maybe want to make sure that your data is geographically close to your users another one could be for high availability reasons maybe you want to add increase your throughput by adding additional read replicas or maybe you want to do it for workload separation you might have one cluster serving data and an identical cluster for your analytical workloads and lastly cost so how is the database license is it open source does the provider charge you for the amount of data that you store order for the or for the throughput that you get or something else is it a managed service do you need to take on management and monitoring out of the database and if so how do you quantify that cost what does support cost do you have the competence for this technology in-house or do you need to train people on it we did it hopefully we were on the same page now and have a common language so if there's one thing we want you to take away from this session it is don't treat storage as an afterthought so we've identified two different IOT use cases the first one we call complex application logic this is where the read to write read to write ratio is balanced these applications are not just about receiving data from devices but also controlling the devices so the complex application logic is used to determine some kind of change that needs to be performed on the device the other type of application that we've identified is what's something we called data ingestion in intensive here we have many devices sending a lot of data upstream these applications are write intensive as opposed to read intensive and they are usually used for monitoring or analytics with that said let's look at a few scenarios the first scenario we look at a newly founded IOT startup the IOT startup is still in the beginning of their journey so the team is still small they haven't developed a platform for smart home appliances like light bulbs thermostats smoke detectors etc the data from each smart home appliance is very suitable to be stored as a self-contained the document and the smart home appliances are being controlled by mobile apps so a family for example can have several mobile apps that are controlling these appliances so let's map these two to two technical features then small team and a limited resources most company would probably benefit from server less today but for a small small startup it might be absolutely crucial if you have a small team of developers letting them focus on your core business instead of managing servers could be could make all the difference self-contained the documents sounds like something that pretty good would be a good fit for a document database the hierarchical rupture lends itself to good data locality and the fact that there is mobile apps that are being used to control the appliances give us a hint that this is a complex complex application logic case so this would be closer to the first of the two years case as we discussed earlier so this startup they don't they don't only face the challenges of IOT they also have to manage mobile apps so let's have a quick look what it would take to build a mobile database the hard way first you need a database servers like my sequel Postgres or and you got to manage and scale that database yourself then you want to query the database from afar so you need an API layer and that API layer needs to be able to authenticate and authorize your queries and finally you need to keep state locally and that requires yet another database and that database has to be kept in sync with a remote one so that's a lot of stuff to do and that's only to fetch some simple data and I mean don't let's not even get started on what it would would take to make that a real-time system instead of a polling one let's see how we can simplify this enter firestore how many in here are familiar with firestore that's pretty good so firestore is what we call a server less database so a server list doesn't mean that there are no servers what it means is that everything from capacity planning upgrades maintenance all of that is handled seamlessly by the system without scheduled downtime multi-region replication will keep your app and user data safe available and available across regions in case of disaster without any configuration on your side fire store is a globally distributed database that will scale based on your usage cloud fire store lets us structure our data as collection and documents which makes queries much easier we can store related data together in hierarchies we based this model on what we learned from firebase real time database developers we studied how people wanted to model their data supporting the most common use cases with patterns that most developers designed intuitively so far we noticed that this reduces the learning curve and that the data model is designed to scale let's look at firestore in action so can we switch to the demo please this is actually super good this is a live demo but we have the creator of the demo in the audience so if if stuff goes sideways you know so we have four quadrants here forming the firebase logo and then we have 40,000 clients that we have simulated and all these four to ten 10,000 clients are assigned to each of the quadrants so what we'll do here is that all these 10,000 clients they are listening to one of these inputs so and of course so that no one will get will get upset we do cero based indexing here so this is 0 this is 1 this is 2 and this is 3 so 10000 is listening to each of these 4 documents when they see a change they will then write that change back into this collection which is called pixels and these are than 40,000 different documents so when we do a change on one of the documents over here that will that will trigger 10000 reads and ten thousand thousand reads from one document and ten thousand rights to different documents and hopefully it will also change the color of this so if we take this one and we change the color like this to and now it should there you go so we can change it back like this boom and if you think this is fake then we can actually look at the different pixels here so let's switch it back to red again and now you have to be focused because it will just flicker there we go thank you thank you thank you we can go back to the presentation thank you and with that I'll wait so this sounds amazing right firestore so why would anyone choose another database over firestorm maybe Billie can help us answer that yeah all right thanks Daniel really good job glad it worked so yeah all this does look really good but if you are in so that's kind of the first case where it's complex logic but if you're maybe more in the data ingestion intensive examples of a use case for IOT you might need a different solution so I've got a few examples Russ to look at that have massive QPS requirements and solutions that are going to work for them the first we're going to take a look at is for fleet monitoring using IOT so let's say we are an enterprise scale company that wants to put some kind of monitoring device in a fleet of cars or trucks we'd like to have a dashboard to monitor our fleet in real time so it gets good could detect anomalies and provide feedback to the drivers we'd also like to do the same with the historical data store that in the database and potentially do machine learning on it and maybe use that to optimize the fleet or use that to Train our data so if we translate these needs into the technical or this business Easons of the technical requirements we need the horizontal scalability massive scale low latency and high throughput and this is a perfect fit for cloud BigTable our massively scalable high throughput globally distributed no sequel database service and this is the same technology that powers a range of billion user Google services we've proven this to work in search maps and Gmail and these are applications that are getting millions of database accesses per second and it's all happening with low latency and if you're thinking a million QPS is a slow day for you then you can just increase the performance by adding more cloud BigTable nodes throughput will scale linearly based on the number of nodes so 30,000 QPS for three nodes or three million QPS for 300 nodes based on your performance and cost requirements you can easily scale with no downtime you don't have to make a sacrifice for consistent throughput or latency as your data size grows so we're going to take a look at two examples here it's a examples of fleet monitoring I've had a chance to work with that I think are pretty cool the first we're gonna use the standard BigTable API and then the other leaves the open-source geo Mesa API so cloud BigTable provides a ton of raw speed and scale but does it come with the trade-off this is a no sequel solution and with that there's a very minimal query interface you can do row level lookups or row range scans and so you must create a row queue that's gonna group data together based on the kinds of queries you want to run for this example we're looking at monitoring the New York City bus system and we want to focus on individual bus lines over time so we're gonna create a key that's a concatenation of the bus line the time stamp rounded down to the hour and then the vehicle ID and for every data point within that hour we can store them in the same row using big tables versioning feature so with that when we perform a get row we're getting at all the information for a vehicle over the hour and you can see it kinda going across Central Park but more commonly would probably use a scan and here we're getting the entire data set for this for this m86 select service bus line I'm looking at the heat map we can already get a bit of data we can see where the starts and stops of the bus are and maybe if there are any SLO points but if we want to get even more information we could just append we could change our query to be half the bus line and maybe the timestamp rounded up to the month and then see if that changes over time we've got a data we've we have a code lab with all this data that works with the big table API so and we'll have a link for that at the end if you want to play around with this a little bit more so now I've seen the cloud big table API but big table also has Apache HBase api compatibility so you get to use that API if you're more familiar with HBase and more importantly this opens up a ton of additional capabilities for streaming time series analysis and geospatial analysis for some of these the additional API will set up this row key based schema in a way that will maximize cloud BigTable speed so you don't have to do that so and we're gonna take a look at geo Mesa and this creates a row key using the latitude longitude and timestamp to allow for performant geospatial queries to use it we change our data flow job to write features using the geo Mesa API and then we're able to perform more sophisticated queries of easily here I've got a query that's selecting a bounding box on a map and a time frame and I'm able to do this with just a few lines of code so Daniel and I or could this team at Google that would put air quality sensors in the back of Google Street View cars and we were able to stream their data through our standard IOT pipeline and set up our dataflow job to write to BigTable using the Geo Mesa API we were able to create a speedy dashboard to view the cars as they drove around San Francisco in real-time and viewed three and batch and batch process in three years of historical data so we're seeing some of that historical data now and these are just some of the ways you can use cloud BigTable as part of your IOT pipeline now we're going to take a look at one more IOT scenario and a database to help out with it here we've got a large scale or another large scale organization that wants to create public transit cards so users can tap their card on the transit entrance which will remove money from their card and let them enter and then if they run out they'll have to go and refill that so this is a combination of these IOT use cases we've our complex logic in here and this is also very ingestion intensive so for the business for the technical requirements for this we need the horizontals we need a horizontally scalable system to handle this massive scale of a public transit system we need high availability so no one's gonna have an issue trying to get to their train and most importantly we need strong consistency since we're dealing with monetary transactions and Google's offering of cloud spanner is great fit for these it's the first horizontally scalable strongly consistent relational database service daniel spoke a little bit earlier about consistency availability partition tolerance as some of the things you might have to choose between when picking a database now with most databases you can only have two but Claude spanner uses special hardware to allow for all three and Claude spanner scales similarly per node like cloud BigTable however to support transactions with strong consistency rights are going to be more computationally expensive than writes without strong consistency since BigTable doesn't have strong consistency with amongst rows so cloud spanner can provide up to 2,000 QPS of rights whereas BigTable comprise up to 10,000 QPS for writes but really strong consistency is why that might be a deciding factor of using spanner over a big table when you do decide to go with cloud spanner though you get the benefits of a relational and non-relational database on the relational side you get structure schema and enforced data types as well as asset transactions across rows regions and continents and on the non-relational side you get the horizontal scalability of a no sequel database if you're used to using a single node solution with a master replicas setup when that system needs a software update security patch or any kind of maintenance you may have to take that system down and have your users experience downtime we know your users we know your users and your services can't have scheduled or unscheduled downtime so because spanner is distributed we can move your data around to perform routine maintenance under the hood without any visible changes to you or your users let's take a look at how we might use spanner for this public transit example will define a table that with using schema should be pretty familiar to any sequel users and then we can easily update this schema with no downtime here we're creating a table of commuters and their card balances when we want to perform a transaction on this we can first get the balance then check it meets the condition of being greater than the cost of the fare and then deduct the cost of the fare from the balance we've now taken a look at three IOT scenarios and found databases with particular strengths that complement the needs of these applications but these examples could generate massive amounts of data so how do you perform analytics across these ever-increasing data sets and Daniel's got the answer for that thank you so we usually divide databases into OLTP or OLAP systems analytical queries you usually need to scan over huge number of Records and calculate aggregate statistics whereas transactional queries usually such as a small number of Records fetched by key our go-to solution for ad-hoc analytical workloads is basic bigquery so anyone in here familiar with bigquery who so before we do anything else let's do a super quick bigquery demo can we switch back today thank you so here we have one of the public data sets that are published to bigquery it's the all the taxi trips in New York well there's several years but this particular year is 2015 and here you can see the different fields that we have and their data types so I'm just going to do a super let's say I'm a data scientist at the city of New York and I want to just calculate the number of taxi trips per month during this year so this is how I would do that I'll use a time step function that will I mean it will focus in with granularity on this is the time stamp and this the granularity so we want to pick month from the time stamp and then we just count the number of trips and then we group it by month so let's run this query this is not very big query it's probably like around 150 million records and here you see that you get it's roughly around 12 million 13 million taxi trips per month and if you want to visualize this it's super simple everything is a click away by just importing the data into Data studio so can we go back to the presentation things so bigquery serverless the underlying infrastructure is being seamlessly managed for you you just upload your data to bigquery or stream it into bigquery and bigquery then provides you with a standard compliant sequel interface and off you go it also comes with built-in support for geospatial and machine learning capabilities even those capabilities are expressed through sequel bigquery also have as a new feature called bi engine which is a fast in-memory analysis service traditionally when you use bigquery you get stable response times between three to five seconds regardless almost of how big how many data behave how many how much data your queries processing but four so three to five seconds that's great for a data scientist but for a dashboard it could be a little bit much so that's what BA 4 - would be made my want to have sub-second latency and that's what bi an engine gives us all right thanks Daniel so today our intention isn't to give you all the answers we want to empower you to know how to choose a storage system so we looked at complex logic applications data ingestion intensive applications and Daniel just showed us how to perform and basically analytics on all of those so take this knowledge and use it to make an informed decision don't treat storage as an afterthought Thank You Jillian I will be around if you want to chat with us or about this or any other storage related topics yeah thanks [Music] 