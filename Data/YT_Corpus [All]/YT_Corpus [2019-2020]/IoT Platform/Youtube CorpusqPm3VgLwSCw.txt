 hi folks welcome to build and welcome to our session on Azure time series insights titled to make your IOT data useful with an end-to-end analytics platform I'm Diego V so I'm a program manager here on the time series insights team hey team welcome to burn in this session we'll cover the following topics an overview of time series insights key capabilities and features customer scenarios the architecture for time series insights we'll also go through a demo and we'll finish off with our roadmap so let's start with what time series insights is time series insights is an end-to-end analytics platform tailored towards the unique needs of industrial IT so what does this exactly mean it means that we're flexible and built for scale we ingest billions of events across hundreds and thousands of data points giving you the ability to tap into your massive volumes of data to unlock insights not previously possible let's talk about the attributes of IOT data which help guide our design principles so firstly IT data lacks structural consistency we typically see unstructured or semi structured data which has very little context about what the actual data means second point to that is IOT data needs contextualization in order to derive insights out of this data we need to better understand how this data relates to each other for example what are the relationships between the sensors how far is sensor a from sensor B these are the types of questions that we need to be able to answer thirdly IT data requires near-infinite retention so in order to gain operational intelligence it's well known that the more historical data you have the better your insights will beam and finally IT data is useful when combined with other related datasets to ensure an organization has a holistic view of their data landscape so with this in mind how does time Suze insight solve these challenges let's start by sharing that time series insights is a fully managed past platform built for IOT because we are a past service you can build applications on top of us or you can use this as a turnkey solution by leveraging our out-of-the-box TSI user experience now let's break down our key of value pillows here first we ingest store processed query and visualize data at scale we're talking about billions of events across hundreds and thousands of devices second we are open and flexible we allow you to connect your data with other analytic solutions by leveraging the open source format of Apache park' and storing this data in an additive storage account that you own third we have a rich set of api's and a UX experience that is first-class it allows you to derive insights very quickly and in fact everything that's done in the UX can be done by our API is public API is there's no private facing API so that we leverage for our UX and finally we have an open source JavaScript SDK library which allows us to create the TSI look and feel within your own applications it provides components such as charts and function stacks just access the time series data so you can tailor an application and experience for your particular use case this is built on top of a scalable platform that it shows that ensures that when you grow the service grows with you and we're also pay-as-you-go and surely ensuring that you only pay for what you use next I wanted to spend a little time to highlight some of the key capabilities of agile time series insights firstly we provide multi-layered storage which allows you to have a Ted storage experience warm for highly interactive recent data and cold for historical data that spans decades giving you which is great for a great advanced analytics scenarios second we have a flexible platform as I mentioned we write in open source efficient Apache Park a file format and we write this in your own storage account allowing you to connect to other analytics the tools of your choice we will over time provide native connectors to platforms such as Park to make the integration experience seamless next as I mentioned we have a rich set of query api's and a UX which is a feature rich and includes things such as support for interpolation scalar aggregate functions categorical variable scatter plots it's truly a first-class citizen for TSI next where enterprise-grade we're ensuring that we meet the high scale demands of our customers and finally we have rich extensibility we have a power bi connector which allows customers to combine related datasets to get a better understanding of their data landscape what I wanted to do now is share two real-world use cases with you to give an insight on how our customers deployed TSI in an end-to-end scenario we are used across a variety of industries including manufacturing power and utilities oil and gas automotive and agriculture in this example I wanted to show you a manufacturing and a smart buildings use case so starting off with manufacturing and manufacturing customers are generally trying to gain operational intelligence and improve their products this is all about high fidelity analysis of the manufacturing process comparing results across batches the aim is for a manufacturer to figure out what makes a golden batch and what is affecting product quality and how do I make better products so in this particular architecture starting from the left we can see we have a bunch of industrial IOT assets these are IT assets usually speak multiple protocols we have protocols such as Modbus df1 PROFINET OPC da and what we aim to do is unify those protocols in a way that we can actually ingest them so what we ask customers to do is deploy a gateway generally it's an OPC UA gateway that we ask them to deploy and from there we ask them to deploy an IOT edge server on the plant floor the IOT edge server will actually take data work in conjunction with the OPC UA server to do the translation and create the JSON files that we actually consume as part of our platform this goes to IOT hub and typically what we see is two parts here some customers will actually build a hot path to do advanced stream processing these are things such as doing derived time series taking two particular pieces of Time series combining them together that derive time series will drop into event hub and then being consumed by time series insights the other scenario we see is customers will run some sort of conditional logic here and based on the result they'll actually trigger an Asia function to drive some other action from IOT hub then we have the warm path which is where time series insights consumes that data and then using hierarchies types and instances we can start to visualize that data and derive some insights everything is built on top of a query API so customers can then visualize that data within their own custom applications they can use TSO Explorer or they can go ahead and use power bi the other extreme here is customers can take that data as we discussed before everything's stored in Apache Parque and use their advanced analytics tool to do things such as building predictive dashboards the next case I wanted to share with you is a smart buildings use case one of the goals of smarts buildings is to derive to drive energy efficiency and promote sustainability a key contributor to energy inefficiency of faults so the quicker these faults are detected the greater the efficiency hence why being able to detect anomalies and faults within the building are critical for energy efficiency so in this particular architecture on the left hand side we have a building that has multiple assets including things such as H facts Vav z-- boilers and chillers all of this equipment generates telemetry what's typical to see in a smart building is a building management system which aggregates all of this data we then have a gateway in this case it's a back net gateway bacnet is the typical protocol used in facilities management and this gateway does the translation from BACnet to something we ingest which is adjacent payload again it goes on top of IIT edge IOT edge sends that up to IIT hub and what we sometimes see within smart building deployments is customers will re-enrich this data with some additional data about that telemetry and it can be information around the equipment itself or it could be for example the building that this this telemetry is originated from those is an enrichment step there that's delivered through an azure function this data is then ingested into event hub and two paths again this hot path and warm path where the hot path we see telemetry being run through a rules engine that based on a particular condition will trigger a function and then in the warm path the data will be ingested in time series insights to drive that intelligence and to do some trending and anomaly detection and things like this finally I just quickly wanted to talk about our customers we have many customers across different industries including a manufacturing oil and gas automotive and energy and we're keen to work with our customers to see other requirements that we need to make in the platform to ensure their businesses are successful so definitely get in touch with us we're here to partner with you to ensure success and with that said I'm going to hand it over to Deepak to cover the product in a little more detail thanks Diego now let's look at the time citizens architecture so if you look at you know all the components that make up the product the first thing if you notice is you know hey how will we get the data coming from the edge there are multiple ways through which you know customers choose to ingest data into TSI one of which is coming through IOT hub and one method through which you know they choose to ingest is why event hub and in future once ATT goes live that is another method through which you know we will be getting telemetry and contact information coming into TSI so from from the perspective of like all these core components from IOT hub what we do we do get your device telemetry and you know we process that will sing over a stream processing engine and then depending on the customer configuration we either route it to BOM store or cold store and our cold stone is based on our data leak and we support Apache Park a file format in which we write the files and you know customers can even store it once and reuse it across you know the have ecosystem and we have this notion of modeling within TSI what it does it helps us yet context as well as you know it helps you know sync the context coming from the upstream systems so essentially time series model has three components you have this notion of instances this which representation of your time series telemetry as well as time series themselves then you have this notion of variables a the notion of four variables are part of types which helps you classify all these telemetry items that are coming and then you have hierarchies which helps you organize all these instances in a meaningful way that way you can triage it so in the long run you know what we want to do is we want to have this model Auto populate and looking at you know what you get from model information from hub as well as from ADT that's the reason you see from ADT you have a digital print model that gets Auto populated here and from IOT hub you have but device 20 mortal that gets populated and there comes you know the query layer that Diego was talking about the query layer tries to give um a unified experience for our customers when they try to query data stored in WAMP store or cold store warm store for us he's based on edx and you know in one store customers can choose to retain data of by X number of days like hey if you have the need of storing the data for 30 days you can store and all the data that comes in into one store and then you know rest about everything to cold store so in path I mean today when the data gets into hop and when you know our process processing in your process the data its store is in both warm and cold so that you know cold you master copy of all things you know that you have so far collected or accumulated in the environment versus form still being you know last X number of days depending on your configuration and query what we what it does is you know it tries to you know go into monster and cold store depending on the II I request the body arm they reconcile that you have and you know it tries to retrieve the information and similarly in our tsq even helps you query the data coming from time series model that way you know you can query context as well as telemetry that is coming from these stores and then you know our query is today used in our TSI Explorer or later in this presentation will go through you know some of the just that we have available there as well as you know TS q is used through you know our end points in third-party applications customers can take the goodness that we have to offer through these api's and it did them directly with their line of business applications and they say hey how can I enable crud directly from there that way right you know they can integrate the EPI layer within their operation system then you have power bi connector that we have which will let you take the data coming out of tsq and integrate that with the rest of your ecosystem so customers can say hey now I have this time series data stored in TSI now I want to take this information and connect that with maybe you know ERP information or CRM information they can do that you know seamlessly using power bi then we have this a whole notion of you know things that we are working on like for example chalcogen and job engine if you look at it you know a job in them helps us do operations batch operations essentially loading the data in on the historical data essentially bringing it from you know the lake and making it useful within the context of TSI as well as you know it in future it will help in batch queering because right now it is interactive tsq you know in synchronously it girls and increase the data but in future we will stop support scenarios we end up enabling batch query scenarios for our customers and we do hammer spark connector through which customers can take the data that is stored in cold store and integrate that with you know the line of business applications they have are like getting downstream upstream analytic solutions they have so in a nutshell this is what you know architectural ETSI animals now quickly jumping on to you know the queries I spoke about from queering perspective we have a queries classified into three categories environment epi is essentially lets you go and do operations on the environment like hey how can I understand what all of the environments have access to or hey what is the distribution of for the data within the environment and even schema of you know the data that is there these environments so all the things that has to do with environments are for under environment EP ice then you have time series modeling api's so types release model EP ice enable crud on the model itself so customers can use these EP ice and directly now update the model without coming to our you acts where they can say hey how can i go and update you know the types all the variables that are stored within the times how can I update you know metadata related to the instances that way I can provide more context to the instances and how can I go and you know organize my data in a hierarchical manner that way I know I can use this API and seamlessly do it so this is the second category through which you know customers can go and create the data or update you know the model then you know comes the time series query API this is where you know the most of the data is computed and is retrieved if you look at it you know today we have three API skint events essentially is used to get the Ravens it takes you know every API again ena has a different payload depending on the configurations that you provide to the pale to the request the response you know you can do response accordingly so from a get events perspective it is just a raw data telemetry API and gives series and aggregate series on our aggregation API so you know this essentially takes set of points and you know for interval it tries to reduce the point depending on the formula that you have stored in the notion of variable as we go to the demo I can show you some of the variable definitions that way this becomes a lot more clearer so if you look at this this is our landing you know and actually since as preview if you click on the environment once it is provisioned you land on our Explorer so what's the environment loads it shows you a list of all the environments you have access to depending on you know what you want for this Jamo purpose let's say you know let's use this environment contoso free time since analytics this is an environment that we have set up in which you know contours of fleet management company is using this TSI product to manage the fleets that they have leased to all the customers so in here if you see this experience you have two paints one is the model section where you know you go to author of the model for edema the other one is analyzed will you come and analyze the data that you have modelled so now quit jumping on to model in model you you have three categories you one start let's start with the instances if senses essentially are you know your virtual representation of time series within TSI so anytime there is a new instance that is discovered on the hub that is configured for TSI we automatically create an entry here and here customers can come and customize the data and provide more context to Italy for example they can come and say hey I have this instance for which I want to add you know instance fields these are standing properties and you know this will be used in order to explore the data and understand like you know what are the characteristics of this data going by you know the value that is provided for this instance of these so in future what we aspire to do is to connect this directly to you know the property metadata that we get from hub and digital twins so as part of our evolution these will get synchronized for product experience now instances again you know are the elementary themselves where you know you can use them and say hey how can I explore this particular tag and derive meaning out of it then what you do you take those instance fields that we spoke about and you create hierarchy here if you see you have we have we have created a hierarchical delivery truck operations ops where you have the first level to be country leased to manufacturer year and make so all of these are coming from the instance fields if I go back here for one quick minute and show you the instances these are the telemetry feels like you know you see here here you have made here manufacturer country so what our UX does is you know it leverages the power of TSM q TR x respond query and it burns this hierarchy for customers to understand how they have organized the data that way exploration becomes seamless so hierarchy helps you think about hierarchies as you know helper which helps you organize your instances then comes you know types this is where you know you class by your sensors let's say you take all your temperature sensors and you can say hey there is a time for temperature and you define in your computation around temperatures for that in this case what I have done is you know we have created two types one is performance analytics type in which you know I have much of calculations that we are doing leveraging the incoming telemetry and there is another one which is default type in which I have a bunch of calculations as well now quickly looking at you know what we do here let's take you know a simple example let's take average speed if you see in a variable definition what we do with average speed is we select hey what is the name of the variable and what is the value that I should refer to which is coming from the chellamma tree that is that the hub is you know giving you time series insights and you know the type of aggregation operations that I want to do on that and then I'm say hey you know what I just need to do this and save it so that is your average speed well what it does is it stores the formula and it just you know leaves it there and then when you run this on the runtime that is when you know this gets calculated pointing to the telemetry that is stored in our storage we will show that you know in a minute and we will quickly look at another example let's say a road slope this is a complicated you know expression or like you know it's a complex expression when you have trigonometric functions that you have used to create the road slope again it uses the notion of the event and the altitude that is coming from the event and the longitude and the latitude coming from the event and it calculates the road slope on how the truck was I mean when the truck was driving on the road what was the road slope and you know try to compare that with the speed of the truck now what we want you to take away with this is like you know types help you classify your tags or time series instances and it helps you define computations on them that way right you don't have to repeat that so the thing that we try to achieve with GSI is we don't do any pre compute we always look at the data and use the definitions that we have stored in variables and explore the data now here right now we created two hierarchies anything that is not associated to these hierarchies will go under unassigned time series instances that way our customers can come and even model those now let's pick up you know delivery truck ops in which I'll go to Brazil I'll go to hey let's look at all the trucks that have a leased out to adventure world's transport and within that I'll go Falcon make a year mono and these are the trucks we have let's say now for these the strut I wanna see average speed at this point what happens our query thinks they did the the the time you know you go and click show average speed at this point our query takes this request goes to the backend gets the data uses the formula that is told in average field that we looked at and it tries to calculate and give it to our Explorer that way it gets displayed here so quick thing here on our Explorer if you see anything that is in this orange range is the warm store and anything that is beyond the orange range is essentially a cold store so you you can you know easily switch between warm and cold store using this option as well here or you can say hey you know what I want to use warm store as much as I can and only for like additional analytics I want to go to cold store so what we did here is we plotted the variable and because the search span was within you know the moms to range the query came back as soon as it could so if you see let's say you know I want to go and say I want to say hey show me the road slope and now let me see okay this seems interesting I can zoom into the section and say how can I ask questions around okay when what was my speed of the truck when the road slope was varying is there any correlation that I can do and then you can start adding additional things like you know friction force you can say hey all of these are again you know coming from the definition of variables and these are all runtime compute we have calculating these formulas at the runtime in determining you know what to display that way we don't pre calculate anything in store you know within our data stores and you know you can say hey now this is interesting but let me do one thing let me go and use these three variables try to explore data for last almost a year now let's say you know if I want to do that operation you can easily go and issue a query something like this and what at this point it does is it takes these three variables it goes to our cold store and it tries to gain all the data compute it and retrieve you know signals for us that way you can go and plot reduce if you see you know in a couple of seconds and I think you know seven seconds this data came back now we can see the trend off like you know these three variables over the last three hundred twenty days so in a nutshell you know there are like bunch of features that are available from our UX perspective you can convert this into a heat map and you can visualize that you can change the intervals to whatever interval you have instead of twelve hour interval let's say you know you want to do eight hours in trouble you can definitely go and do that and then you know you can also take this data and you can say hey I want to download this and send it to somebody within the team that way right you know I ask a specific question that I need to answer all you can you know even take this data that is available and integrate that with you know power bi that way when you click on this option it gives you and it gives you the ability to say hey do you want to take the aggregated data or do you want to take the raw telemetry that is coming out of it and then you know connect it to power bi so these are the things are you know that we are planning to release as part of our GA and you know there are like bunch of other things which you can try by going to you know the link that will be available in our documentation and you know if you just add slash demo to it this is the demo environment where you can go and get your hands now that was the demo that we wanted to show you and now let's quickly look at the product roadmap we will talk about you know what we are working on right now as we speak warm and cold based analytics that we just reviewed that is being worked on and it is all ready for our GA then you know we have times it is model and time series query API additional capabilities that we are adding to it from scale as well as from additional aggregate and scalar functions that our customers can use we are revamping our user experience and we are providing additional capabilities around okay how can you go and do Auto refresh of the data that way you know anytime there is new data you don't have to do anything the charts that are already there and gets automatically referenced you have the notion of categorical variable where you can go and explore categorical signals you know that is something that we are working on and that is available as of today in our demo environments as well as it will be power for GA then power bi connected we briefly review that that is something that we are super excited about because that ends that opens up a lot of avenues for data integrations for our customers then we are working diligently on Azure data like a storage end to support in public preview again you know we want to support this in a preview format and then go from there then the next thing we are adding is a ad group support that way you know the ackles are managed via ad groups making it easy to manage them the next thing is enterprise readiness we are going to new regions we are scaling the product to support you know enterprise load as well as security and you know we are working on a framework to import data such that you know customers can bring their existing data and you know start using the power of time series insights in con in parallel to you know the new data that is being streamed and the last thing is essentially the spark connection for advanced analytics where you store the data once in CSI and then use the spark connector and integrate that with all the downstream as we go system for analytics and from what is coming next perspective there are tons of things that we are working on next and you know we are super excited for where we are taking the product some of those things include how can we get better at improving the latency lag and the transforms for the data that comes into TSI today we support you know basic processing of data but we want to go and double click on that to say hey how can we enable customers to go and do meaningful transforms that way they cleanse the data and store it in a meaning for way as well as you know improving the latency and lag for the data the second thing and the most the even of the important things for us is how can we get better at integrating with the ecosystem like you know how can TSM get better at understanding PNP and ADT natively that way customers don't have to reconfigure anything they configure it once and all this product they understand each other and things start populating automatically in these experiences OPC um binary and CDM support is another thing that we'll be working on just to complete the story around analytics and like you know how can we have all of these supported end-to-end there are a bunch of query updates that we are working on like how can you enable composition that way you take a bunch of time series and try to combine them into one and derive metric for it and new scalar functions and aggregate functions we are thinking of exposing who's the query language on our monster that wave customers who are familiar with the language can easily use the power that kql provides data amputation and data deletion is the top of our mind because that is one of the highest ask customer requests and that is being worked on next connectors how can we you know add more things to our connectors and how can we expand in our integration stories with connectors with EML spark India rubrics there are a bunch of new analytics features we are adding like process graphics - coding and you know all the SDK updates so one thing you know what I noticed that we did cover as part of the architecture is what power you know the SDK JavaScript SDK that we have bring to the table our JavaScript SDK he can be used and you know it can be integrated with any of our customers line of business application that way whatever charting features you have that can be used you know within these SDKs and we'll be adding as and when we'll be making new improvements to our user experience we will be adding you know those features to our SDK that care that way it can be leveraged by all our customers and then you know we'll be adding new security and compliance features for role based access control things like hey how can we have tag level security will be exploding that next migration of TSI gen 1 to Gen 2 essentially you know all the customers who are on generation 1 how can we give them a meaningful path through which you know they can migrate to Gen 2 and start using the power of the payers Egoscue that we have here and TSI on MG is another important thing that we have been focusing on detailing out so that is another thing that is that is being worked on next anomaly detection forecasting and predictive maintenance are our advanced analytics scenarios this is something that we want to make natively available within TSI that way customers can use this and do you know complex even processing or like you know the actual processing depending on their scenarios and disaster recovery I mean it goes back to like hey how can we support you know be CDR for our customers business continuity and disaster recovery for our customers and bunch of updates to our SDKs like hey how can we build new SDKs that way it is easy for our customers to take these and integrate that with their applications as well as you know their CI CD pipelines so these these are the these are some of the things that we are thinking of doing next and as we proceed with the roadmap we will keep you guys posted on how we are making progress and quickly jumping under the last thing you know these are the resources that you can use in order to learn more about our product and you know we look forward to hearing back from you on like you know we can you take this next um next steps perspective these are the list of resources that are available we think you know will help you as you traverse through Bell to become in a July already certified training you have we have trainings as well as exams and you know these hyperlinks will help you you know get those and we do have I already shows as well as you know how to get started with the azure IOT that details are like you know what our products can enable and scenarios can you go and solve and you can join our tech community as well as you know attend other build sessions that we have that way you know you get to understand like you know what is that we are thinking from azure IOT perspective so Microsoft IOT vision and roadmap is something that is there on May 19th as well as you know IOT live coding session that is on thank you so much for participating in our session make your IOT data useful with an end-to-end analytics platform we hope you've enjoyed the overview and the deep dive of azure time series insights and we hope you enjoy the rest of the build session and please get in touch with us thank you thank you so much for attending our session we are looking forward to seeing what you can do with this product and what scenarios you solve for yourself as well as the customers thank you [Music] you 