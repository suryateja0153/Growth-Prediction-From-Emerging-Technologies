 Hello! Welcome back to the Rasa Masterclass. This episode is a part one of our deep dive into the training of the NLU models. In this episode, you will learn what is a processing pipeline, which pre-configured pipelines you can choose from to get started, how to train your first NLU model and how to interpret the output. To enable our assistant to understand the intents and extract the entities which we define an our NLU your training data file, we have to build the model. This is done by defining a processing pipeline. A pipeline can be understood as a sequence of processing steps which are used to extract specific text features and train certain components which allow the model to learn the underlying patterns from the provided examples. With Rasa you can either build a custom pipeline or use one of the pre-configured pipelines. If you are just getting started, using a pre-configured pipeline makes it very easy and quick for you to train your first NLU model and get a taste of how it works. There are two pre-configured pipelines you can choose from. One is called pretrained_embeddings_spacy pipeline. This pipeline uses the library called SpaCy which loads the pre train language models which then are used to represent each word in a sentence as word embeddings. World embeddings are vector representations of the words which means that each word in a user message is converted into a dense numeric vector. Word vectors capture the semantic and syntactic aspect of the words meaning that similar words should be represented by similar vectors. So what's the advantage of this? The known semantic and syntactic aspects of the words will boost the performance of your models even if you have a very small training data sample which in fact is a very common situation when you're just getting started with building your contextual AI assistant. In addition to this, since the training doesn't start completely from scratch the training of your NLU models will be very fast which will give you fast iteration times. While using pre-trained embeddings comes with a lot of advantages there are some shortcomings too which you should keep in mind. First, is that good word embeddings are not available for all languages because they are usually trained on publicly available datasets which most often are in English. This, unfortunately, limits your choices in which languages you can build your assistants in. Another thing is that word embeddings are usually trained on rather generic training data sets for example Wikipedia articles and similar. This means that they don't cover domain specific words like product names, acronyms, etc. In situations where you want to go around these problems you might consider using the second pre-configured Rasa pipeline which is called supervised_embeddings pipeline. The main difference of this pipeline is that instead of using pretrained world  embeddings it learns everything from scratch using the examples you provide in your NLU training data file. Since your models are not using any pre-learned knowledge you will naturally need more training examples for your models to really learn and to start to generalize on unseen user inputs. However, there are a lot of advantages of using this approach. First, since your models are learning from your data it will adapt to your domain specific messages as there are no empty word embeddings. In addition to this ,this approach is language agnostic which allows you to build AI assistants in any language that can be tokenized and you are not dependent on pre-trained word vectors which are available for specific languages only. And last but not least this pipeline allows you to build NLU models which can handle advanced cases like messages with multiple intents. We will cover that in the next episodes of the Rasa Masterclass, but overall supervised embeddings pipeline is a flexible way to handle advanced cases. So let's recap a little bit. How should you choose which pre-configured pipeline to use? Here is a little rule of thumb for you - if you are building your assistant in a language which doesn't have pretrained word embeddings, go with a supervised_embeddings pipeline. Otherwise, check if your assistant will likely be handling advanced cases like messages with multiple intents. If yes, supervised embeddings pipeline is the way to go but if you are going to stick to simpler inputs then the last two things you should check are if your assistant will have to handle domain-specific vocabulary and how much training data you have. Supervised_embeddings pipeline will give you better results for domain-specific messages but to train good models with this pipeline you will need more training examples than with pretrained_embedding_spacy_pipeline. While there is no strict number for how much training data you need, a recommended amount of examples for using supervised_embeddings_pipeline is a thousand labeled examples or more. Both of these pipelines are pre-configured to enable your NLU models to handle both intent classification and entity extraction tasks. Now, let's take a look at how it all works in practice. A processing pipeline configuration is defined in a config.yml file of your project. This file is automatically created for you when you run the rasa init function. If you open the file you can find an example configuration of the supervised_embeddings pipeline. It consists of the language indicator which corresponds to the language which you are building your assistant in and a pipeline name. You can use this configuration to train the first NLU model of your Medicare Locator. To do that, use the Rasa CLI function rasa  train nlu which will train the model using the pipeline configuration and the data we defined in the previous episode. It will save the model in a directory called models once the model is trained we can test it by running a Rasa CLI function rasa shell nlu which will load the most recently trained NLU model and allow us to test it on a command line. After the model is loaded we can test the model on various inputs and see how the model performs. For example, let's see how the output of the model looks like for a message "Hello there". And here you have it - the output of the model. It consists of the most probable intent which was predicted for this input message, in this case it's greet, alongside the confidence of how confident the model was to predict this intent also it contains the list of extracted entities if there are any and the results of the intent classification for all other intents in our training data. We can test the model on other inputs like "I am looking for a hospital" and again the model provides the most probable intent which was predicted for the input message, extracted entities and the rest of the intent classification results. Would the pipeline configuration be very different if you wanted to get started with pretrainedn_embeddings_spacy  pipeline? Not that much. You would only have to change the language parameter to match the SpaCy language model you're going to utilise and the pipeline name. After updating the pipeline configuration you can retain the model using the rasa train nlu function and see how it performs. Once the model is trained use rasa shell nlu command, load the new model and tested again. As you can see, a confidences of the models changed because we are using a different pipeline. Using the pre-configured pipelines it's a very good way to get a taste of how the NLU model works and it's a great quick starter for your project which can take you a long way. However as your project grows you will very likely want to change specific aspects of your model and this is where you will want to dig into certain components of your processing pipeline. Even the pre-configured pipelines overall are just the shortcuts for a full list of different components which are responsible for specific processing steps. Understanding how each of these components work and why they are necessary is crucial for you to better understand why your models are behaving in a certain way and how to design your training data to achieve the best performance. This is what we will do in the part 2 of our deep dive into the training of NLU models. You will understand what each processing component is, what it does and why it is important which will enable you to configure your own custom pipelines. In addition to this, you will get the answers to the questions like "Does the punctuation in your training examples affect the performance of your NLU models?", "Does the class imbalance matter?" and "How to deal with very similar intents?" and much more. I'll see you then.  