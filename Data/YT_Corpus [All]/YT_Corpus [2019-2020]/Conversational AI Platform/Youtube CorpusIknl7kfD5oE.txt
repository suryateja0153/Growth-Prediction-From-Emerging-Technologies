 (pulsing music) - All right, welcome on in everybody, welcome in, great to have you joining us, welcome in. This is Thinkful's Webinar Speaker Series, How to Leverage Big Data and AI. We'll start off with some very brief introductions. My name is Dodge McIntosh, I'll be leading us through our webinar session today, or rather moderating our conversation. I'm a data science mentor with Thinkful based here in San Diego, California, and I've been teaching data science and machine learning courses, and kind of working in that space for about a year and a half now, and I've been with Thinkful since the beginning of this year. Before getting into the data science field I had pretty much zero programming experience, pretty much none, so for those of you out there in the polls who also put that you have little to no coding experience, maybe you're looking to make that big career change or learn a new skill, just know that it's possible coming from whatever background. I always like to tell people, other humans have learned how to use these tools and techniques, if you're a human and you're willing to work hard and apply yourself, you can learn them too. I also am a former bootcamp graduate so happy to answer any questions about that bootcamp experience from a personal perspective. It can be a challenging environment at times, it's a lot of information compressed in a pretty short timeframe, but I am a big fan of the structured environment for learning that a bootcamp can give you. One last brief intro for myself is I'll go ahead and put just a couple of quick ways you can connect with me. We've got Robert's connections listed there in the reference sheet, but if you have any questions for me about Thinkful or any of our programs, always happy to continue that conversation, the best way is to do that via LinkedIn and via email. So that's me, I will turn our time over to Robert here real quick so I'll give a brief intro for him and I'll let him expand on that. So I've known Robert for a couple of years now. Robert graduated from UC Santa Barbara, both a Bachelor's in Chemistry followed by a Master's in the Material Science and Engineering program. After working as an engineer for several years he made the jump into the data science field. So he's held multiple data scientist positions now and currently works as a data engineer for Tectonic, a Denver company dedicated to big data analytics, marketing automation, customer relationship management, and cloud consulting services. You can connect with him on LinkedIn, his LinkedIn is linked there in the chat, but with that, I'd like to give a big welcome to Robert here. - Hey everybody, thank you for the great introduction Dodge. And yes, I'm Robert Manriquez and I'm based out of Denver, Colorado, and I'll expand a little bit about how I got into data science, also being a bootcamp graduate. A couple of years after I finished my master's degree I worked as an engineer in more of the traditional kind of engineering and research engineering roles, and after a while I started noticing I was much more interested in learning about the hardware and the software and saw a lot of the value that it brought into our own lab and our own research. And after a couple of years I decided to make the jump into going to a coding bootcamp, I did a combination of self-teaching through the bootcamp, and I also taught with a bootcamp for a while. And I landed my first data scientist position, and another data scientist position and now I was poached by this consulting firm called Tectonic and, as Dodge mentioned before, we do everything, cloud computing, consulting, everything from setting up modern data analytics platforms and setting up our clients to have their information on AWS, Google Cloud Platform or Azure, and giving them all the tools that we need to make modern analytics applications possible such as dashboarding with Tableau and Looker, managing their data and all their content with Salesforce, and even making machine learning and AI solutions for our customers to introduce automation, save some labor, save some costs, and improve whatever kind of processes they have in place there now. And yeah, I've only been in this space for a couple of years and I'm really happy, I'm really excited to talk about some experiences that I've had working with AI and machine learning at scale and lots of the fun things I've learned along the way. - Awesome, yeah, and got a chance to talk about kind of a lot of the different breadth of material that you've covered in those years working as a data scientist, working kind of in the field out there, so I'm really excited to continue to unpack more of that during our conversation. But I was also wondering if you could touch a little bit on kind of that career transition piece that you're making, and how you kind of knew, or when was that turning point when you wanted to, really you felt like all right, I'm gonna get out of engineering specifically and I really want to make that jump into into data science? - Well for me, it wasn't really a novel idea. It's become a really common career path for people that end up majoring in the traditional sciences and engineering. About a third of my graduating class ended up learning software engineering or data science or went into some kind of tech role instead of working in those traditional roles, so that was something that I started noticing a lot of my colleagues doing in the mid 2000s, I mean in the mid 2010s, I'm not that old, and I started thinking, you know what, I started thinking if this is somewhere I wanted to go. I felt like at the position I had before, I was having a hard time growing that company and I just had interest elsewhere, and I was trying to think, okay, if I'm gonna switch industries or try something different, I'd better pick one that is growing the most. And what attracted me the most about data science is the word science in there, data science is very much still about your core programming, machine learning, statistics and math knowledge, but what really makes data science interesting for me and why I wanted to switch my career into it was that it still requires you to think like a scientist. So all the things about designing experiments, controlling variables, enforcing a scientific process, and ultimately making reproducible products and methods is what really excited me. And you know, at heart I'm still a chemist so I love everything procedural, I love rules and structure, so that's what I really liked about the process in developing a data science product. And in congruence to that is I feel like there's lots of room to be creative with data science, there is never a cookie-cutter answer to solve any problem, and there's also never just a cookie-cutter problem that you're going to encounter in this field, so it's great that you also have a chance to explore ideas, do some research, really express user creativity to solve a problem, whether that's selecting the right model or the model that fits for your purpose, or even deciding that you don't need a model and analysis and reporting is enough, or developing an elegant solution with code or a clever algorithm, those are all things that really attracted me to join the field of data science and what's ultimately gonna, hopefully keep me to stay. - Now, I love that point that you brought up too about that creativity piece where, you know, there is, data science does have these really unique intersections where you get to, if you do have that love of structure and that love of process, well when you start to tackle a new project, a lot of times there isn't that prescribed process or project or that structure, that framework around it, and so we have like kind of these steps that we make sure that we work through as part of that scientific, more scientific pipeline, but there's also, with each one, a tremendous amount of creativity in creating that structure, whether that's, you know, custom metrics for performance or the way that you're building that predictive modeling piece, yeah, you have that balance of structure and framework but also being forced to think critically and think creatively too. - Yeah, and then touching a bit more about how I transitioned to this field and how others without data science or computer science backgrounds can transition is, the greatest skill that you have to bring is your skills of problem solving, critical thinking, creativity, and just the motivation to break into this new field because once you get here, surprise, no one else knows as much as you think they do. Technology's changing so much, job roles change every few years, you're really, once you enter the field, you're going to quickly find out that you are on the same page as everyone else, things are changing, you're keeping up and adapting. So, and then just-- - You're writing the script too as you go along the way. - Yeah so, and that's kind of just reflective of how the workforce has changed in the last five years, it really rewards adaptability, so data science really is about adapting to new tools, new methods and just breaking out of your comfort zone. And also, before I became a data scientist and before I went to that bootcamp, I had very little coding experience. Even with a background in science and engineering, lots of things that you learn are really basic, C++, MATLAB, Mathematica, things that are only good in the research lab, things that don't let you build products or actually teach you good coding practices, like you're never gonna see a GitHub repo that has, you know, MATLAB in it. - Yeah. - You know, even though you can do machine learning in MATLAB, no one really does that, that's our, like, very niche research groups. So, I really-- - And I liked one of the things that you just said right there too, we're talking about the workforce and the way the world works is changing, especially with tech roles in general and like data science, specifically. You have a really cool opportunity to, along with building off of that self-teaching foundation that you did of actually, like you're saying, create real projects and these real material things that you can point to, just going, all right, like I've actually worked with this tool, I've developed solutions, I can show you proof that, you know, I'm not just saying that I can do this thing, I have a little bit of proof about what I'm saying I can do, right, we can talk about this, we can like objectively verify whether or not we know what I'm talking about. - Yeah, and then just kind of like the last comment I want to make about that is, getting into this field, you can really get in with any background as long as you're bringing those skills and pretty much the desire to keep learning and the motivation to keep learning with you. And, just a testament to that, most of the people that I work for, or a lot of my bosses, don't have backgrounds in computer science or engineering or science, and they've just had years of experience and they've been able to self-teach themselves and really get into the field and make names for themselves that way, so-- - And that's not to say that if you do have that CS background or anything like that in a related field that that's a bad thing, right, that's still providing a tremendous foundation to jump off from, yeah. - It directly helps, but here's the big but, lots of computer science programs at universities I feel do lag behind industry a lot, so they're, oftentimes if you do come out with a computer science degree, you still find yourself having to learn all kinds of things that you didn't get a chance to, you find out that your two courses in assembly aren't helping, you're finding out that your circuits courses aren't helping, or you find out that, you know, your four years of, I don't know, whatever outdated language isn't helping, so that's definitely a leg up, but it's not everything. So the kind of takeaway is, the amount of work required to get into the field is more or less the same for everybody, the great equalizer. - Yeah, no, that's great. Awesome, well yeah, thanks for that additional context kind of around your transition and just general transitions into the field and getting a little bit more of that motivation to break in. Yeah, so we've thrown around quite a few terms already, and I know in the title of our event we have some very kind of buzzwordy words that we're gonna to be unpacking and that we're gonna be getting into what it actually means to work with some of these frameworks. So I want to start to kind of take that overview here, as we zoom into some of the specific examples and those workflows that you have for us to talk about. So I guess we'll start at the highest level, talking about just when we're having this conversation about data science and about machine learning, what are we really talking about with data science or with machine learning? - Is that a question for me or is that just introducing the topic? - That's kind of for all of us to think about but I'd love to hear your thoughts too, specifically, on that. - So my opinion on what data science is, it's really just using code, math, statistics, and machine learning to really solve problems with data. It's really, at its core, solving problems with data, and that can range anything from making a report, doing a statistical analysis like a traditional regression analysis, making a statistics-based simulation, or everything up to a modern decision like building a machine learning platform or building a big data workflow to aggregate your data and deliver it to some kind of reporting mechanism. That's what I feel data science is, and even on the other end of that, being like a researcher and developing new algorithms or new methods of novel automation, that's also data science too. So that's just my opinion of it, and then you'll see that the definition is changing like every year or so, and then it'll change from job post to job post if you're looking for jobs or positions to fill. - Yeah, and then where does machine learning fit into that overall data science umbrella, right, 'cause like you're saying, data science still is this really broad, really encompassing term that covers a lot of different specializations and different contexts, where does ML fit into this? - Machine learning, I feel, fits in kind of two places. One, being a kind of like a software product that you need specialized knowledge and expertise to really leverage, because if you just try to follow a recipe or just throw random algorithms at whatever data source you have, it's really hard to be successful with that because you really need to, one, understand your data, two, understand the statistics underlying how these models work and why you need to know the statistics, and three, basically how to wield machine learning to actually accomplish your goals, so it's very difficult to just kind of jump in and make a machine learning model for some application and getting results right away. So that's kind of where machine learning fit into data science because it requires the skill set of understanding how to work with data, how to use programming tools, having to know statistics, ultimately to build this kind of product which is pretty much automated decision making. Two, the other side of that is machine learning itself is, in my opinion, an analytical tool. If you make, you know, a supervised learning algorithm, one that basically looks at your data and tags, yes or no, hotdog, not hotdog, it will tell you, it basically, those are the kind of decisions that you can make on your own if you know the data well enough, but it does it for you and it helps you figure out things that you missed or it helps you discover trends that you didn't understand, so it's almost like an automated way of, pretty much, a regression variable analysis. And also on the other end of that is, unsupervised learning, where the specialized algorithms are meant just to find patterns and unseen trends in your data that are really difficult to discern with just using Excel or coding things by hand. So that's where machine learning fits in data science, in my opinion, automation and analysis. - Awesome, yeah, no I think that's a really, yeah, that's a really good way of thinking about that framework for where that fits in. And then so what about our other kind of two big pieces here for coming in from our our kind of event title here? Artificial intelligence, let's tackle that one first because I know that's another one that people out there may have heard some different definitions, and maybe it is something that has those shifting definitions still. For the context of everything that we're gonna be talking about, how are we thinking about AI? - So AI is definitely one of those big, kind of like almost a buzzword nowadays, AI is everywhere you see, in every kind of business article or some company's trying to build some kind of AI platform, and its definition is a bit nuanced because it's very similar to machine learning where you have these two sets of tools that are meant for ultimately automating decisions whereas automating estimating the price of a house or identifying spam messages or, really anything that you have to do manually, you can have a machine learning algorithm, AI do. But the way that I define the difference there is AI distinctly is the advanced application of machine learning, there's a whole suite of traditional machine learning algorithms that you can use to accomplish most automation tasks such as random forests, decision trees, all your flavors of regression models, K-nearest neighbors, all of those, but AI is something that you need, you really need an advanced architecture to do where you'll definitely need neural networks, deep neural networks, convolutional neural networks for something like image classification, LSTM, long short-term memory cells, which are meant for predicting frequencies and doing time series analysis. And then even things like, you know, GANs and RNNs, all these other kinds of acronyms, things that your traditional machine learning algorithms can't do, so it's applications for generating things like song lyrics or doing language translation you need to use AI for. So the key of that is, something is a machine learning algorithm that is trying to mimic human behavior, something that a human would do, because us intuitively as humans, we can look at data, analyze it and understand it and then just based on a couple of variables we can start thinking, okay, maybe I can kind of pick out which ones are going to get the result that I want. For example, if you're a sales guy and you have your Salesforce open and you see all of your leads showing up, yeah, you can probably just know from intuition what you know about your position before, which leads are gonna be dry, which leads could land you a deal, well that's something an AI can do but you have to really use the big guns and make an advanced architecture if you want to do something sophisticated like that. - Awesome, awesome, yeah. And then even kind of within that scope of AI, there's, like you said, there's probably some different nuanced ways that we can think about that, right, all the way from totally mimicking something that is like a human way of learning and executing on these actions, to maybe some more specific, narrow scope, a focus or purpose, right? - Right. So things that are simple like estimating a number, like estimating the value of whatever product that your company is trying to sell, or trying to estimate if a lead is going to be, you know, profitable or not, or basic classification regression tasks, that can be done with traditional machine learning, but for advanced applications, has to be AI. And another reason for that is just the computation power required to get that desired result, so you really need to start thinking about how these advanced architectures, well, why you actually need to use them. - Yeah, awesome. All right, cool, so we're starting to, we've got our foundation being built here for these different pieces coming together, let's introduce our last one. The last piece of our title for today, big data, and that is, for sure, one that gets thrown out a lot that I'm sure everybody, all of our attendees have seen some article headline that has big data in it or how some company is starting to implement big data, what are we talking about with big data, why is it big data compared to whatever other information that a company is already collecting? - So lots of times, the difference between your normal data and big data is really the scale of it. Working in megabytes and gigabytes is much different than working in hundreds of gigabytes and terabytes and petabytes of data. There are some tools that you would use as a software engineer or a data scientist where you simply have too much data and you're bottlenecked by computational power and how many computing resources you have, because you have too much data. So if you try to do an aggregation or if you try do a process on one gigabyte of data with using traditional tools like Python, yeah sure, it'll work, but if you try to do it on a terabyte of data, you'll be waiting the rest of your life for that to run. So, and the difference in the tooling is, the types of databases that you use and the kinds of programming languages that you use. So lots of traditional data work has been done using flat file systems and relational databases and REDs, so that is literally, whenever you write a process, if you're going down a table, under the hood your computer is looking row by row, one, you know, it's kind of one leading head at a time to do aggregation or to collect this data, so that works fine when you have a small amount of data but when you're analyzing hundreds of gigabytes, that is too slow. Or if you have just enormous tables, that's too slow and relies on your system's memory to really do all the work there. So when we're working with big data, we have to use a network architecture called distributed computing, so you'll hear things like Spark-- - Distributed computing, yeah. - And GANs and Hadoop, and all those technologies, what they do is instead of having basically one leading head scanning the databases on like a flat file, what it'll do is it'll take your data tables, it will split them up into partitions, and it'll break up the table into smaller pieces and then it'll start scanning all of them concurrently, and then once it's done with the aggregation or whatever process that you have, it stitches them back together and gives you the result. And what's powerful about that is, using a traditional relational database, flat file computing, if you scale your computing resources, if you use like as much computing resources that you want, your bottleneck is still just how fast your code is going, but if you use a distributed framework, you can put it on, instead of just one computer, you can put it on a cluster, which is multiple computers of multiple nodes with multiple executors all working together on several slices of this data. So that approach is what is really scalable when you use big data workflows, and anything from just moving one table from one database into another, or supplying a dashboard or training a model, lots of those, you can really gain performance if you switch to some kind of distributed computing framework. - Nice, yeah. - That's the big difference. - We're like, and when we're talking about this, this means we're well beyond the point of being able to throw this into Excel and analyze it line by line, right, or even being able to throw it into a Jupiter notebook with Python and even then with a programming language like that, in that environment on a local machine, it's probably not gonna be too good. - Yeah, and then just to give our viewers an idea of the scale of what big data is versus small data, Excel caps out at a million rows, so if you have more than a million rows, you can't analyze it in one Excel sheet. - I actually didn't know what that Excel cap was, so that's-- - Yeah, it's only a million rows. So for lots of organizations, that's still a lot of data, but if you're, you know, a gigantic ad tech company or if you're an Internet of Things company, if you're collecting lots of data, or if you're Twitter collecting, you know, millions of tweets per second, that tool's gonna be working, that's gonna work. And even using like Python in your Jupiter notebook, you start seeing huge performance losses once you start seeding a few million rows or if you crack over a gigabyte of data, that's when you really start hitting the wall on how you can process that data with your standard tools. But if you use-- - And you don't, yeah, and for a lot of companies out there, right, you don't have to be like a Twitter-sized company to be generating that amount of data, or you don't have to have that kind of name recognition, that kind of like clout to be generating, like you said, if you're an IoT company, if you have connected devices, those connected devices per person are generating tremendous amounts of data compared to the data that we were looking at a decade ago or two decades ago. - Yeah absolutely, and the case that we're going to be talking about later in our webinar today, is an AI that I worked on at a company that I worked at previously. They specialize in everything email, and what their AI is, the data that they use was the event-level email data, so opens, clicks, views, bounces, deletes, deletes without opens, basically just address, where it came from, and then a number. But, and it seems like that's pretty trivial on its own but when we had this AI platform, when we started onboarding more and more clients and more and more clients that had enormous email lists, and when they're sending emails, you know, essentially millions per day, then you can start seeing how that adds up. So this product, within the span of the year, the database swelled from a couple of hundred gigabytes to five terabytes, so then, that's really where you have to start thinking about how you can analyze that much data with the tools you have. - Awesome, yeah, no that's amazing, and that definitely queues up our conversation for later that we'll continue to build off of there. Before we get into that example and start to really pick your brain about those implementations and everything, I wonder if we just have some cool ideas or some other cool examples of AI or big data being used by companies out there today, or if you've seen, if you've run across any other interesting use cases out there in the field? - Yeah, by far one of the most interesting use cases that I've seen is this company called The Trade Desk. A friend of mine from college works there and I met one of the data scientists based out of Denver who gave a talk on it, and The Trade Desk is an ad tech company that's all about selling services to marketing firms and digital ad tech companies to service their ads, whether it's as simple as page ads, or banner ads, pretty much anywhere that you see an ad on a web page, they are helping service it. So an AI that one of the data scientists did was, he was able to make a neural network that was capable of estimating how much an ad space would be worth, and so you pretty much bid a dollar amount or an amount in the cents to buy that ad space, and the way that that industry works is, any kind of avenue that is selling ad space, it's almost like an auction where you have different kinds of companies want to pretty much bid for that ad space, right, and what's hard about that is, if you're spending, you know, a couple of bucks for thousands of ads per day, that's gonna add up a lot so you always want to, one, increase your win rate on those auctions and two, minimize, you know, you don't want to keep over-bidding every time. So what he made was, it was really crazy, it was a zero by inflation model and it was something I'd never seen before but he had that on top of a deep neural network, and it was able to bid a winning amount to all these auctions in milliseconds. And the reason why they needed to use an advanced architecture was they needed a way to kind of tackle a problem they had with bias with the traditional algorithms, so what this AI did was, when it was deployed, it's able to participate in these auctions millions of times per second, like millions per second, so it's able to-- - Yeah, that's wild. - Accurately finds ads, place a bid, and then record if it wins or not, all in a matter of fractions of a second. - Yeah, where in a matter of time where it feels almost instantaneous, right, almost live, that it's generating these predictions and executing on them. - Right, and if it's too slow by like a fraction of a fraction of a fraction of a second then it loses the auction, or if it's at such a tremendous scale of trying to buy ad space millions of times per second, it just simply will not perform fast enough, so that is why they had to use AI to kind of augment all the products that they had. Another good example is what Airbnb is doing, they've built an AI that can automatically tag the pictures in rental listings. So intuitively, this is the part of where AI, you need to mimic human behavior or use an advanced architecture like a convolutional neural network. So they have this AI that can go through each post and identify, is this picture a living room, is this picture a bathroom, is this picture a backyard? So instead of having someone go in there and check on their own, because we know what living rooms look like and what a backyard looks like versus a kitchen. - Hopefully. - It's not always, depending on where you live. But AI can do that, so that's saving you lots of labor, that's saving lots of costs, and it's ultimately helping improve the listings because listings that are more populated and have better information filled out, typically, the better they do. - Yeah, you're not gonna go on and book something, like if there's, right, the listings that are kind of sparse or missing stuff, as a user that feels kind of like a red flag, right, like well what's, is this a real place, what are some of these issues that are coming up about this, right, and so yeah, that's pretty awesome, I was reading that article after you sent it to me, I was like, yeah, this is crazy. - Yeah, and it seems to be so simple, right, you know, image classification can solve all our problems but it's also kind of hard to find problems to solve now because that technology is so ubiquitous, so I thought that was a really interesting application and I think Dodge will be supplying links for that one. - Yeah, yeah, and for a couple of these ones that we're talking about, make sure you go over and you check out that reference sheet, the reference sheet is linked at the top of, or towards the top of our chat here, but I'll also make sure as we're talking about different terms or we're referencing different articles or different things that we've talked about before, I'll make sure that I add those to the chat too. - Yeah, and then the last example of application AI that I really liked is, something that we use everyday probably, it's Gmail spam filter, and previously they've been able to have a very high detection rate, it's typically 99.9% accurate of them being able to correctly identify spam messages. - That's crazy. - And what's really interesting about that is, Gmail has about, estimated a billion accounts that it services, and there are 281 billion emails sent per day, so having something that performant, at that scale, is really difficult, and even though 99.9% sounds like a very impressive number, 0.1% of, you know, 281 billion emails, that's still like over 100 million spams that can get through every day and that will affect people. So to kind of crank that accuracy even further they had to use TensorFlow and they made a really specialized deep neural network that can crank up that performance and it can additionally block 100 million additional messages per day using TensorFlow. - Yeah, especially when we talked before about what's that threshold for big data, and we're talking about exceeding a million lines in Excel, this definitely kind of just totally, it totally dwarfs that by an amount that we can't even really meaningfully comprehend as humans, right? - Yeah, it's really difficult to just truly understand the scope of how much this thing works every single day, and what's difficult about that is, you know, email is a pretty old industry and spam filters have been around for a long time and that's, you know, more or less been solved so being able to improve something that has been figured out for the last 10 years is really difficult. - Yeah, those kind of, once you do get to that threshold of high performance, going from mediocre to that high threshold, cool, lots of room for improvement, but pushing it past that last level, like you're talking about, yeah, that's pretty crazy. And then you threw out one term there that I just want to make sure everybody's aware of and that was TensorFlow, so just a real quick, and we can get into some of these different tools in more detail in just a little bit but a real quick, while we mentioned it, for what TensorFlow is. - Yeah, so TensorFlow is Google's open source machine learning framework, and it's really a bare bones tool that allows you to build, everything is simple, from just a one-neuron neural, well, it's not really a network, a one-neuron neural network or even a simple linear regression to something as complex as a convolutional neural network or this enormous network that Google uses to do the spam filtering. So, and TensorFlow is essentially the industry standard right now for developing AI frameworks, and that's because it's highly tuneable, it's very robust in what it can do, far more robust than other tools like scikit-learn which is something we'll talk about later, so that's what TensorFlow is. So lots of times when you hear about AI you're going to hear about TensorFlow as well. - Awesome. So I guess before we get into some of those more specific languages and frameworks of working with big data, working with AI, what were some of the things that you really built up as a foundation before you added those on top? So what things from data science and machine learning, kind of what were those really key building blocks and maybe ones that you continued to develop in some of those early roles or ones that you still find important today? - Okay, so when I first started working, you know, helping build and maintain a AI platform, I came up with the foundation of the traditional data science toolset, Python, knowing how to use Jupiter notebook, knowing how to do modular code and how to use your various engineering tools such as GitHub and some basics with Amazon Web Services, and also knowing my basic math and statistics required for that kind of stuff, and just your standard library of Python tools like scikit-learn for machine learning and doing analysis with it, Pandas for working with data, and things like that. But really the jump that came for me was learning how to use these distributed computing frameworks, mostly Spark, which is also an industry standard tool for handling big data. So everything from basic data aggregations and doing data analysis, if you're using large amounts of data or silver gigabytes or if you're trying to pick, you're trying to filter out data from enormous databases, then yeah, you're going to need to use something like Spark. And also just getting comfortable with working kind of more closely with non-data scientists, so working with software engineers and product managers and other kinds of, you know, data analysts, that was also a skill that I had to learn as I went too, because you'll find, once you enter this industry, data scientists like to do things a bit differently than software engineers, and kind of learning how to, you know, code switch between the two is really important. - Yeah, I was wondering if you could also just kind of keep keep building on that, that idea of this kind of collaborative piece of that data ecosystem, right, where it's not, although I'm sure a lot of times it feels like it, it's not just the data scientist who's doing that workflow. - Yeah, so really, if you are trying to set up an AI project for success, you're going to need more than just a handful of data scientists, because you'll need data scientists to pretty much prove the concept, work with the data to understand the data and identify the problem that you're trying to solve with AI and ultimately make something that can do that, but you're also going to need software engineers to build the deployment infrastructure, how to help you do the best practices with working with data and doing your data workflows and, another word you'll learn is ETL, your extract, test, load. And so you need to work together with them because lots of times data scientists, you know, some of them will have a strong background in DevOps and infrastructure and being able to put things in production themselves, so build infrastructure themselves, but lots of times, if your company is spending resources and relying your data scientists to do that kind of stuff, it may not be the best idea, so having the engineers build those things and kind of work with the data scientists closely is really the way to go. And then also, another part to that is really understanding the business, understanding your customers, and understanding really the business use case of what is AI supposed to do, and that's when you're going to need product managers to kind of come in and help mediate between both the teams, mediate between the technical team and customers and other parts of the company. So the role of the prod manager is they are the advocate for the customers, you know, how's the customer going to understand this, what do they want, does this approach solve their problem, and ultimately will it help our business succeed, will it help us make more money, will it help our clients save money or labor or costs, or anything like that. So really, when you think of AI, if you want to do AI at scale and in production, you really need to have all three disciplines. - And so for you, yeah, you'd say that one of those big, really important pieces is being able to look at, to be able to tie all of the work that you're doing at whatever step kind of back to that business objective, back to that strategy, right? - Yeah, even before your hands touch the keyboard and you start trying to prototype your first machine learning model to prove this concept, you have to really understand what the problem is that you're trying to solve, because if you make an AI that does all these wonderful things but your customers don't get any value from it then the AI itself isn't very useful. So this is a term that we'll talk about some more, it's called a data strategy. You really need to have the strategy laid out clearly amongst all the people in your team and across the leaders of the business to really make it successful, because no matter how fancy or cool or interesting of an AI or machine learning model that you make-- - And there's lots of them, right, you could make a super cool one. - There's lots of really cool stuff you can make out there, but if it doesn't really help your business's bottom line, if it doesn't provide any value beyond just being cool, it's not going to be very helpful. - Yeah, yeah. So with, yeah I mean, right, when we talk about kind of, and a term that gets thrown around a lot is like data driven decision making, right, or data driven companies and everything and that, I feel like a lot of times that is something that gets used as a buzzword, right, where you can talk about being a data driven company, you could talk about having that data culture, but it's one thing to say that and it's a different thing to implement it, right? - Yeah, absolutely. One thing to really make any kind of data science effort succeed is you really need to have that buy-in from your company's leadership, like, your company needs to understand the value of data science, the value of AI and machine learning, and being able to set up the environment for data scientists to iteratively make these products for them, you really need that, and you also really need to understand your data and what it's telling you to really succeed with it. There's always a quality versus quantity trade-off when it comes to trying to be data driven, it's not enough to just have data, it's not enough to have a lot of data, you need to have good data that you can trust, that you can rely, and you have to have a team that can understand the data and actually really intimately understand how it's going to affect your business, because if you're trying to make decisions off data and you don't understand it, or if it's not good or if you don't have enough, then it's not really going to help your business. So that's really paramount when it comes to, if you really want to be data driven, if your company needs to have data culture, then to enforce best practices, have people that understand the data, let people take time to understand the data, and really listen to what it's telling you because another pitfall of data science is looking for the answer you want in the data. So, you'll see in popular literature, lots of popular articles that kind of poke fun at that, is, "What does the data scientists do at your company?", "Oh, they tell me I'm right". - Yeah, or in other cases, right, p-value hacking or different, you know, I'm like a more scaled-down sense, but yeah, you've got to be careful that you're not just using the data to confirm your assumptions in a vacuum. - Right, because-- - And that's a little bit more where the science piece comes back in, right? - Yeah, like just using data to confirm your own biases is not going to be helpful. Because the whole goal of having a data driven enterprise is to break convention, you know, if you have processes that your company's doing that are out of convention, "Oh this is how we've always done it", if there aren't good reasons beyond that, then it's a good idea to start using data and start trying to understand how to improve those processes. - Yeah. And then one thing too that you were talking about with kind of that overall larger data strategy is having that understanding of your data, like that intimate, collecting the right data, and I'm wondering, you know, if going just back to the basics of data wrangling and data processing, having those kinds of foundations under your belt, if that's still helpful in the day-to-day work that you do? - Absolutely, because very rarely can you plug in a raw data stream or just pull a raw data table from a database and make that into a good product or a good machine learning model or making a good report, you're going to ultimately always have to do some kind of post processing of that data, whether it's your data munging, data cleaning, anything as simple as pulling out null values or imputing null values or changing data types, to things more complex as trying to do pivots or trying to do, you know, multi-tiered goodbyes and things like that. So that's like, that's the most important part about being a data scientist is being comfortable to work with data and being able to essentially mold it into the format that you need for your application. 80% of your job is going to be working with data and then 20% is everything else, and that includes modeling. - Yeah, and that is something that, yeah, is kind of an important disclaimer to have for people out there, right, is data scientists, you see the article headlines of the, you know, the sexiest job of the 21st century but a big part of that is this inherently not sexy data cleaning, data wrangling and all of that. And a lot of that does get kind of, I feel like maybe a little bit worse tag than it should, because really, when you're doing that data cleaning, that data wrangling, if you're doing it right, the lines start to get blurred between just pure data cleaning and also doing some of that exploring your data, finding those different trends and patterns and insights and being a little bit of that data detective, right? - Yeah, those are essentially two parts of the data cleaning process, and those are two things that are really hard to, you know, replace someone that knows the data and knows how to work with it, because the hardest part of making these products is making sure the data is right and in a format they can use. There are lots of technologies out there like DataRobot and Alteryx, even Amazon SageMaker that can automate a lot of the machine learning and data science process but that's all well and good for really basic stuff but if you're trying to make something advanced like AI at scale, you really need to know how to get into that data and mold it into the way that you want. Because sometimes just rows and columns of just basic entries will be enough, but if you're using an advanced architecture like image classification, you need to start working with images and you need to compress them into these complex matrices, or working with sequences, you need to figure out how to build the sequences, what to put in them, and how to build this array, and how to feed that into the model, or if you're trying to, you know, predict sequences with a recurrent neural network, then you have a whole different approach, especially if you're doing things like natural language processing where you're trying to work with text data. - Yeah. - It's not enough just to throw a tweet into a machine learning model or throwing in a paragraph into a machine learning model for it to figure out. - Yeah, so in theory, it's not gonna run, right, if you just throw that raw data in there, it's gonna go, well, cool. - Yeah, so that's what's really important about being comfortable with knowing how to do data cleaning and how to do data aggregations, and in lots of times, if you're working in things like ad tech or IoT or things that work with lots of streams of data, lots of data that you get is gonna be raw event-level data. That still doesn't tell you anything, so you have to figure out ways to intelligently group it, roll it up, what you are capturing, what size windows of, you know, the differences, that's-- - And getting back to that creativity piece, right, being able to look at this, sometimes really like nebulous, or really initially mundane-seeming event-level data and go, all right, how do I actually create some structure and some value out of this because that data by itself, that data in a vacuum just sitting there doesn't have, you know, it has potential value but it doesn't have that already present value in itself. - Yeah, and to kind of speak to that some more, one format's not gonna fit every algorithm you want to use or every technique that you want to use, so if you spend a little time setting up all of your x's and your y, and you throw a model at it, it may not work the way that it would for a different type of model, or if you want to do a different kind of analysis, like you want to do a statistics-style regression analysis it has to be a different format, if you want to do a clustering analysis it has to be a different format, if you want to do an advanced neural network architecture it might have to be a different format, so yeah, 80% of your job is going to be thinking about those kinds of things. - Yeah, so having, yeah, so that's a good baseline to kind of, a great understanding to get that baseline that you started with, right, to know that that data cleaning, that data processing, working with, being comfortable making a problem or a project into those different types of, you know, whether it's a regression run, a classification run, a clustering, or being creative enough to figure out how to translate those from one to the other, all of those are kind of that core work that you need to be doing. And then from there, what are just some of those, I'm wondering if you could just list out a couple of the different tools beyond that, to make the jump to working with big data or that AI at scale. Some other tools beyond just Python and scikit-learn that people should be starting to look at. - So what helped me the most in learning how to work with big data, and also lots of companies that are working with large data sources already have people that can do this, is working with Spark, and Spark is this distributed computing framework that you can use to do that partitioned style database table transformations and data processing that we talked about earlier. And it's its own language and it comes in different flavors too, you can code in PySpark which is the Pythonic version of Spark, Scala, which is kind of like Java and the native language of Spark, and even R, which is a popular analytical tool that's been used by statisticians for a long time, and even data scientists today. So that is probably the most important part, because going from a function that scans a table, you know, in a flat file kind of sense using Python, if you try to use a Pandas function to do that on a large set of data, it's not gonna be performant, your computer's probably going to crash or your cluster's probably gonna run out of memory before it can do it all, whereas if you do a Spark, you can actually do that aggregation more efficiently, and you also have to learn a different way of thinking with how you code with Spark. Spark is pretty difficult for the first-time programmer in my opinion, so once you have lots of the basics then with Python or Java, whatever language that you're using in your role, then you can start learning Spark, because something as simple as doing a group by or even selecting a column, or doing a value counts kind of aggregation with Pandas is very simple, it's only, you know, one function. With Spark it's different, you have to, before you even get to start coding you have to configure a cluster, you have to pick how many executors you have so basically how many nodes are working at a time, how much memory each one has, and then how many partitions to slice up your data into. Because there is what's kind of called a computational overhead, like your computer has to do enough work first to split up and shuffle that database before it can actually start doing a function on it, so if that overhead exceeds the computational cost of just using Python, then you don't need to use it, but once you have that set up then you can start actually doing these aggregations and moving data to one place or another or transforming it into a format for your machine learning model. - Nice, awesome - And, yeah. - So you would say, like Spark is really that, that first big kind of pool that people should jump into for making that step from kind of the base data science and machine learning to working with that computer or with that distributed framework? - Yeah, that is exactly, that's what I really recommend for our viewers and just anyone in general that's trying to work with larger amounts of data and that, you really need to start thinking about the efficiency and computational cost and how fast it needs to work, then you need to start using Spark. And what's great about Spark is, you know, the two flavors that are most commonly used are PySpark, which is used more by data scientists because it's greater for its Pythonic syntax and better for analytic work, and Scala is a bit more performant and it's a fan favorite of software engineers who already have experience using Java, so lots of times you can use Scala on PySpark to do all of your data management and data moving, and then you can use PySpark for all of your analysis and developing models and training models. And then even beyond just your standard data transformation functions, there's even a machine learning library that can do distributed model training, and it's called Spark Machine Learning Library, so Spark MLLib. So once you have all of your data set up in the way that you want and split up in the partitions that you want, then you can throw a couple of models at it and it will train them all together and spit out your composite model that you can, you know, put somewhere like on a virtual machine and have it give your predictions. So that's, I feel like the most important, learning Spark and just learning how to do the aggregations that you're comfortable with within Python, but just in Spark. - Yeah, yeah, nice, awesome. So we've got all our, or at least most of our tools under our belt that we need to be successful to start working with that big data or that AI project. I want to shift gears here because I do want to make sure that we have at least kind of five to 10 minutes for an open Q and A session, so where we get everybody, and hopefully we do get some more questions, I know we haven't had too many in the question bar there, but yeah, if you do have any questions on anything that we're talking about so far, please please post them, I want this to be as engaged, as interactive as possible. So I do want to make sure that we carve out some time at the end that we're able to get to that, so with that, I do want to switch gears and talk about maybe an applied example that you have worked on, kind of asked in that real-world work environment, so yeah, with that, we're gonna talk about this really interesting dynamic optimization problem, so I'm wondering if you could kind of cue that up, talk a little bit about the context of it. - Yeah, so a company I used to work for, I worked for previously was called Return Path, and they were definitely one of the ad tech industry's authority on anything email, everything as simple as figuring out your inbox placement rate for whatever email campaigns that your marketing department is doing, to something as advanced as using AI, that's what Return Path did. And Return Path was around for 20 years before they got acquired, and they knew like a mind-boggling amount of knowledge about email infrastructure and how email works across the internet ecosystem, and the product that I worked on for them was this Dynamic Optimization service that we were selling to our customers. So there were lots of tools that we had provided, like things that monitored how well your campaigns did as an email marketer, how many times, and all kinds of reporting tools, it had a platform that gave you essentially a dashboard that showed you every email campaign you have and your various metrics that you watch, such as how many times the email is deleted or opened, how long it sat in the inbox, and ways to kind of score, something we called the sender score, how well your IP addresses send emails to your customers. So there were lots of tools that Return Path sold, and one of them was Dynamic Optimization. This is one of the few-- - So with, yeah, so I was just gonna say, so with this Dynamic Optimization, what was like the key, the key problem or that key solution that it's focused on? - Yeah, so one product that wasn't really on the market was something that email marketers and anyone that works in the email space kind of intuitively knows is, if you send too many emails to a subscriber, there's a chance that they're going to unsubscribe because you're sending them too many emails or you're not sending them at the right times, so Dynamic Optimization had two products to help really combat that. One was called, it was called Sending Priority, where it could pretty much tell you as an email marketer which emails to send, in what order, to which subscribers. That way, the spam filter will have a lower chance of marking it as spam, because you know, it's not spam but the way that lots of email programs mark spam is engagement, so if you send emails that are spammy at first, then it's gonna start tagging the rest of them as spam too. And then the product that I worked on was called Engagement Assurance, that was really, for every subscriber that our clients have on their subscriber list, how many, if we keep sending them emails, when will they unsubscribe? And basically what the AI did was it looked at your subscriber's event history, and then based on the sequence it could determine, is that person going to unsubscribe if I send them email? So what it would do is, every day, it would train this model for a client, service this list of their subscribers, and then basically give them a very simple classification, send this person or don't send this person. So the idea behind that was, if you have bad email sending practices, if you're sending too many emails to certain subscribers, if they start unsubscribing because of that, you'll start experiencing customer churn and then you're gonna start losing engagements and impressions and you're gonna start, ultimately, being able to sell fewer goods and services. So the real idea behind Engagement Assurance was to use this AI to help email marketers figure out when they should slow down sending emails and ultimately over the time span of their email campaign, retain more subscribers and get more of their products and services sold or their brand awareness increased or whatever goal they're trying to accomplish with their email campaigns, so-- - Awesome, thanks, sounds like it was a pretty ambitious, a pretty ambitious undertaking there for, I guess, like as you're starting to tackle that problem and you're starting to frame it out, what kinds of data did you need to collect for it? - Lots of it was just that raw level, that raw event-level data. So basically we could use that data to make, to pretty much, you know, capture profiles and start understanding individual subscriber behaviors based on how often they interact with your emails and how they interact with those emails, so-- - And really quick, when we're talking about event-level data, because I know all of a sudden I'm hearing that in my head and we've used that term a couple of times here, I don't think we've actually offered like even a brief definition for it, so when we're talking about event-level data, what are we really working with? - Really it's the most granular, you know, temporal way to express data that you can collect from something that happens on the internet. So event-level literally means, here's what the event was, it was a send or a click or a delete, here's the time it happened, here's who it went to and where it came from, you know, things like that, so event-level data is how lots of ads collect their data. For example, if you ever use like Google Analytics to track the activity on your websites, all that data is going to be at that level, it's going to be this page got this many clicks, and then aggregate to clicks per hour, clicks per day, views per day, views per month, so it's the most granular level of data that you can roll into some kind of temporal framework that makes more sense, like seconds, minutes per day, per year. - Cool, yeah, no that's perfect. I realized you had used it a couple of times so I wanted to make sure we were all on the same page about that one. - Yeah, I'm glad we needed a little, I think we needed a little clarification on that. So that's what this algorithm did, Engagement Assurance, and it was part of Dynamic Optimization, and yeah, so it was born about this need for email marketers to know when they should stop sending emails to people and pretty much maintain a healthy subscriber list, so that's what that AI was all about. - Yeah, and so as you're collecting all of this event-level data and other data too, I'm sure it's not just that event-level data but was there anything weird or interesting that you found during that data processing phase, right, we talked about kind of how important that is for the overall process, was there stuff that was wrong with this as you were collecting it? - Yeah so, well the first thing that was wrong was how much of it there was, this was where we really needed to really start thinking about our data infrastructure and how to use tools like Spark to aggregate all of the data and process it, and that's when we need to think about what kind of cloud computing resources that we want to use, what kind of databases to use, and how to efficiently program ways to save that data as it comes in and to aggregate it and then, pretty much on a schedule, make training sets for our models to start training on and then delivering that data, so that was the one of the biggest challenges. And then with the data itself, when you're working with raw event-level data, it's not always correct. It really depends on where it came from, the email ecosystem is so, so complex, every inbox provider, every type of service, every IP address, every domain has its own, and every telecom company that services the internet has different ways of handling emails, so it's really-- - That's really crazy, right, to think about all of that stuff behind the hood of an email that we use all the time, every day. - Yeah, so it's really easy to get outliers in your datasets or getting duplicate emails or emails that don't make sense, or just kind of like anomalous pieces of data that's just part of the stream, and it's really hard to pinpoint where that comes from, did that come from the source of the data or did it come from something in my code and my architecture? And also trying to figure out how much of it to collect and how much, and which samples from which subscribers to use, that can be difficult too because, you know, if your email client is doing what they're supposed to do there shouldn't be any bots in their subscriber list or there shouldn't be any nefarious activities on their email campaigns, right? So it also, not every subscriber or email user's gonna behave the same. We can do as much as we'd like to try to quantify and, you know, segment those customers into different profiles, but you're always gonna have, you know, one day a subscriber decided to delete everything in their inbox, or one day that person just stopped opening emails, things like that, and when you're trying to collect millions and millions of rows, you know, gigabytes of data to make this training set, then you start having to think, all right, how can I make a data workflow that can pretty much be called as a validation, pretty much validate if this data is what I want? Also it can be called a unit test, just pretty much a reproducible way of verifying that this data is going to be in the format that you need, and it contains the right kind of samples that you need from your data. And kind of the last difficult part about working with this kind of data is most of it is just sends and opens because not everyone's going to be deleting their emails, not everyone's going to be interacting with their emails, and not everyone is going to be unsubscribing, so then we start getting into an issue of, if we just pull all the data in, well, all of it, are we going to get-- - As much as, yeah. - And that's a concept of machine learning where if you have a classification algorithm, if you're trying to predict a one or a zero, you know, a hot dog or not a hot dog, if you have a dataset that's mostly hot dogs, then your AI is gonna think everything's a hot dog, so trying to deal with issues like a class imbalance can be really difficult as well, especially at that scale. If you're ingesting, you know, billions of emails per week and 99% of them are all the same, how is that going to be helpful? So that makes it really difficult. I see you're throwing out some keywords there, false negative rate. - Oh just the ones as, yeah, as we're working through those, yeah. - Yeah, so that was one of the big, those were some of the big challenges of working with that data source for that AI. - Nice, and so after you've kind of worked through some of those challenges, worked through some of those processing, but how did you actually start to model that, like what kind of frameworks did you use, what kind of model did you use? - Well I can't really say exactly what kind of model but there was a lot of TensorFlow and Keras involved, which is, Keras is another flavor of TensorFlow, it's a simpler way to work with it. It's great for prototyping, and then you can use TensorFlow to really tune the performance out of it. So we used a lot of TensorFlow, we used Spark and PySpark and Scala to do most of the data infrastructure, and when it came to actually developing this model, we used the TensorFlow library to prototype the models, tune the model, do parameter optimization, and really track the performance and establish, is this a good model or not, is this model going to help us or not? So those are the two main tools to make this AI work. - Nice, awesome. And then how, once you started some of that modeling and you had kind of models that you felt pretty good about, when did you start to move that into full production or actually let it start executing, making those decisions based off the predictions, right, when did you let that AI kind of loose in the wild? - Okay, so, it kind of depends and it's different from application to application, but what we did was when, or the data scientists that worked on it before I did, because I came in when this AI was in beta, so there was an alpha and a prototype before I came in, and what was done before was they had approved that it performs well on this training set, it performs well on the testing set and it performs well on a hold-out set, so they're able to train this model up, get really good performance metrics out of it, so high accuracies, high true positive rate, low false negative rate, and then they're able to reproduce that on real data or data that the model has not seen, and they were able to pretty much convince themselves all right, we were able to set up enough experiments and enough tests in our development environment, and now we want to start putting it out at scale. So lots of times if you want to start selling an AI product or a service that comes from an AI, you need to bring in your beta testers, so when your product's in beta, you bring in some testers, so they were able to get some of the clients that they've already, were selling products to like, you know, Return Path Platform and other tools that they would use for their email campaigns, they would come up and, well, the product managers or client relations specialists would come up and say, "Hey, we have this new product "that you might be interested in helping us develop "and if it works well "then we can kind of cut you a deal on it "and if it get a lot of value on it then we win too "so we can keep giving you the service "and then we start selling to other people too". So once it got into the testing phase and it passed the threshold of, okay, we have enough buy-in from our customers, enough buy-in from leadership, and the team itself is confident in the product, then we're able to bring in more customers and start servicing more customers and that's when it really went from beta to full-fledged product, or full-fledged service, that Return Path was selling to customers. - Awesome thing, nice, yeah. And so if everybody wants to kind of jump into even more of those specifics, definitely check out that link that I shared there. I'm just looking at our time here, I want to make sure we've got enough for those questions, but as we start to think about wrapping up from that project, and as you're looking ahead to like new challenges, new takeaways from that, what are some of the exciting things that you're continuing to work on today or some other advancements that you're seeing in big data and AI? - Well what I think is most exciting is just lots of, you know, lots of AI has been, AI development is kind of like hitting a wall to computation, so seeing lots of advancements in quantum computing is really interesting, that's kind of most exciting for me because that's, you know, in the next five, 10, 15 years when that really starts taking off, that's when AI can start doing really incredible things that have previously been limited by just raw computing power. Things that also excite me in what I'm working on is seeing other types of distributed computing frameworks come out. A new one is called Dask, and there's another one that NVIDIA has developed called RAPIDS, so trying to get away from the hard Spark syntax, what they're doing is they're trying to use GPU compute to do basic data aggregations with a familiar Python, Pandas syntax. So in the next couple of years it'll be great to see a simpler way to work with big data than having to learn this enormous library of stuff, so that's really exciting. And then me in my day-to-day work as a data engineer, what excites me is seeing all of the tools that, you know, Google, Amazon, all the big tech companies are making, even things like, you know, DataRobot and Alteryx that they're making to enable data science and to really promote data culture. For example, Google Cloud Platform has BigQuery where it's its own partitioned, you know, micro-cluster based database system that's great for analytics where you can just hop in, use the editor and start pulling data from it and doing data analysis, whereas it's more difficult to do that with a traditional database technology like PostgreSQL or MySQL. Snowflake is also becoming a great database technology for doing those kinds of analytics too and servicing dashboards and being able to form training sets and doing machine learning at scale, so those are some exciting technologies and advancements in the field that aren't necessarily AI itself, but things that really enable it and promote data culture. And then especially me working in a consulting firm, you'd be surprised at how, well, the industry as a whole, knows big data, data science AI is hot, but there's still, the majority of the consensus out there in the real world is, what's data science, what's AI, what's a dashboard, what's, well, what's analytics? So those kind of things are still not very well understood out there in the real world, so one thing that's always exciting me is when people, company products, make data and analytics more accessible, because ultimately, every company is going to be a data company in the future so better that we can start letting people do that, the better those industries will become. - Awesome, yeah, I love that, every company, it's like every company is a tech company, right, like it's moving away from just big tech to actually being able to leverage these tools, these techniques, these approaches, every company's a tech company, every company is gonna be becoming a data company to really stay successful and stay relevant in their market, awesome. Well yeah, so looking at the time, I want to encourage people to start posting some of those questions in our chat, in our Q and A, and as we're kind of waiting for some of those questions to roll in, I just want to echo some of those thoughts that you had shared earlier about what makes somebody good at working with AI and big data, data science and machine learning, and it's that it really is about being, having that combination of a couple of different things, that scientific approach, but also being able to think creatively, think critically, and then keeping that business sense, that tying it back to the real-world work that you're doing. Yeah, and that kind of successful professionals in any of those data-related professions, whether it's the data scientist ones that you've worked in or data engineering that you've transitioned into more, that you have to be comfortable getting intimately acquainted with that data, right, you've got to be willing to get your hands dirty with that data, explore it and really be that data detective. And kind of like you were saying before, to not feel like you have to have all of the answers, right, so much of this is continually changing, so many of these approaches are being modified all the time, that you just have to be more comfortable with knowing where to learn, where to ask how to get those resources and how to adapt them to your workflow too. - Yeah, and then, really for any data professional, whether you're making an AI product or making a process or just trying to build whatever kind of piece of software or data tool that you're trying to build, a lesson that we learned at Return Path was to kind of understand where you are now and where you need to go in terms of that product. So when it came to this AI that we had, we had all of our standard classification metrics and ways to identify yes, the model's working great, but then we started getting questions like, okay, as we bring in these clients, we need to start semantically understanding if this AI is working for them, if it's really helping them move the needle, push the bottom line, if it's really providing value for them, and that's a valuable, so that's a lot of work that I did was I had to help develop these new data workflows and new kinds of custom metrics and ways to kind of identify is this product working for people? And then we can go back and be able to tune the model, we're able to make modifications to the model, and really get into the nitty gritty of improving that model. And that lesson applies to anything, even if you're, you know, back when I was one of the guys in the chemistry lab, you always need to keep in mind, what am I working on, why is it important, what value does it bring, and that's true of lots of professions even beyond, you know, working in data. - Oh yeah, I love it, that's, yeah, I definitely couldn't have said it better myself and thankfully, that's why you're here joining us today, awesome. So as we've talked about a lot of these tools, a lot of these frameworks, hopefully our conversation today has people out there feeling more excited about jumping into that data science space. It can seem a little bit daunting with all of the tools that are out there but the good news is is that there's also tons of great resources out there to continue learning, and like I was saying before too, and like Robert has said so many times in this conversation, regardless of your background, there's great resources to get into this space. A couple of different ways that you can do that, on the reference sheet that we have linked there, and I'll share my screen real quick for, to take a look at that, so on that reference sheet that we have linked there, down at the bottom we've got some more information about ways that you can learn data science and machine learning with Thinkful, specifically the one that I would encourage everybody to check out is that two-week trial piece, so let me go ahead and also link out a, that same link for the two-week trial, you can check that out, right there in the slide, that offer and as well as pinned to the very top of the chat. So in that two-week trial you get access to about the first 100 hours of the actual course curriculum. You won't probably, in this two weeks, get all the way up to the Specializations piece, but in the scope of our data science program, at the end of it, you also get to do some work with some different specializations in data science like working with big data, starting to build out those big data and AI considerations as well. So I'd shared those, we also, Robert and I, have a former coworker and colleague of ours who started this really excellent podcast, so I'll make sure that I link that right here in the chat as well. So that's the HumAIn, I never really know how to pronounce it, podcast, it's human and AI together, so this is a really cool listen to, you know, while you're on your way to work if you're starting to think about some of these other, you know, other projects, other cool use cases and applications that are being done in the space, definitely give this a listen and give that Thinkful two-week trial a quick look over. Yeah, with that, I want to really open up the floor here, kind of continue encouraging any of those questions. So far, looking through the chat, just making sure I didn't miss any there in the chat, and I'm not seeing any in the Q and A tab there, but yeah, I want to make sure that you're leaving here with more information than you came in with, so definitely post some of those questions, we really, really would love to hear from you. And as just kind of we're waiting for those questions to roll in, I definitely want to thank everybody for joining in today, I know people probably had all kinds of different ways that they could spend their time and we're both thrilled that you chose to spend it here with us. We always enjoy getting a chance to catch up and talk about the work that we've been doing, talk about advancements in the space, different things going on, but it really is kind of that extra level of special when we get to share that conversation with everybody here. So yeah, big thanks to everybody for joining in, oh awesome, so we've got in the chat there, "No questions", okay, "But great insights so thank you all", well, thank you for the thanks but really, both of us would turn that around and thank you for joining in and participating in our conversation here. - Yeah, and while we're giving thanks, thanks for hosting me today, Dodge, and thank you everybody who's listening to my ramblings this evening, I appreciate you all attending because what's the point of a webinar when no one's around to learn and listen from it? - Awesome, yeah, well and a spoiler alert but we are gonna give one real big thanks to Robert here, our special guest speaker for taking some time out of his day to share, you know, tons and tons of knowledge, tons of practical applications, tons of what it's like to actually be out there in the field working with these tools and techniques, kind of peeking behind the veil of, you know, not just these buzzwords but what's it like to actually work with them, so Robert, huge thanks to you. And it does look like we've got some questions rolling in so we'll start to kind of tackle some of those before we wrap up our time. So Leonard asks, "Great webinar, what would you recommend "a high school kid consider taking at college?" So if they were going to continue down that college or kind of university path, maybe what are a couple of good courses to study, or a couple of different majors or areas of study that would set you up well for a data science career? - I'd like to say hard majors, majors that make you think a lot, so lots of critical thinking skills would be great, as well as some hard skills would be useful too. So lots of people I see enter the field come from some kind of quantitative background such as math, economics, science and engineering, or even majors that make you think really hard such as philosophy, or even majors like English that you wouldn't expect to get into data science. Part of being a data scientist is being adaptable, knowing how to think about a problem and to set out a solution, and also being an effective communicator. One harsh lesson that I learned in my first foray as a data scientist is sometimes the story matters more than the data, so that part of data storytelling is really important, and being able to communicate these technical concepts or communicating a value proposition from data science or AI or whatever you're working on, is going to be really useful. So I feel like what you major in matters less than what you get out of the whole college experience or what you learn while you're there, so just being able, learning things that improve your problem solving, critical thinking, communication skills would be great. And then if you want to throw some hard skills in there, taking some introductory computer science courses, introductory programming, taking a couple advanced math courses or-- - Or statistics courses, yeah. - That would be great, statistics is gonna be important too, so if you can get some of those under your belt, that is a great foundation to start building off of. - And kind of in a similar vein too, just to echo that, if you are transitioning into the data science space from a previous career already, is being able to think about, all right, well data science has this crazy intersection of a lot of these different skills, that stats background, right, that computer science, a lot of those different critical thinking pieces, and that communication piece, right? So whatever career or area of expertise that you're transferring from, or area of study that you were an expert in, knowing how to identify, all right, I'm really strong in this area but how can I add some of these other pieces, right, like for me, the transition was, you know, more front-facing roles at companies, so sales, those marketing roles, so I had a good understanding of kind of some of those business objectives already, and I had a little bit of statistics background in college, but really picking up that programming side and being able to work with those tools and those actual, right, the hard day-to-day tools that you're working with, that was the big piece for me. So being able to identify where you're strong and those other areas that you can really fortify to step into this unique space. - Yeah, and we're talking about how we made our transitions. I came in kind of like a, you know, I felt like I was really comfortable with doing research as like a research scientist, research engineer, learning how to craft scientific experiments, and knowing how to read technical literature, how to make presentations, how to make data visualizations and understand them. The hardest part for me was just learning how to program because sometimes in, you know, even though I had a background in science and engineering, I didn't really know how to program, like I didn't know programming best practices, I didn't know anything more beyond how to do like a for loop and how to, you know, make an equation, so I think that's one of the most important skills you can kind of get out of that too is just being comfortable with programming and basic Python, working with the command line, working, because that is the most foundational, because once you get that programming language under your belt, everything just becomes easier because you have that programmatic thinking in your head. - Yeah, definitely. Got another, yeah, so great question Leonard, thanks for, yeah, thanks for writing in with that one. We've also got another one over here in the Q and A tab from Vincent, so Vincent says, "Hi, when looking to switch "into that big data industry or the data science space, "what's the most important thing to keep in mind "when trying to get that first interview, "or when you're looking for the right "first company to apply to? "Is it knowing the basics, "regression, basic probability, et cetera, "or is it having some more of that prior work experience "and getting a higher education master's or PhD?" - You're not gonna like this answer, though. So it kind of depends on the position that you're going for, the company that you're working on, but in my opinion, some of the best data scientists out there are the ones that know the basics, like you laid out Vincent, some of the basics that you want, if you master the basics, that will allow you to ace these interviews and how to kind of pick up and learn things as you go and give you that confidence to do well in these interviews and to convince your interviewer that you can do this job. It's not as important as knowing the answer to all the hardest questions they have, is demonstrating that you can be someone that they can work with, no matter what your skill level is. So things like regression and basic probability, as you mentioned, machine learning and programming tools, those will all be useful, but most places are going to rather, they'd rather hire somebody that they can work with and know how to work with infrastructure, know how to program, know how to do the basics of data science, rather than a guy who spent, you know, six months in his basement winning a coding competition, that's impressive in its own right but being someone that can join a team, learn these things and contribute with the data science process and workflow is really what's gonna get you into those positions. And when it comes to having a higher education, data science is surprisingly a very credentialing field, (dog barking) sorry but you can hear my dog in the background, very credentialing field, there's some kind of this, not really irrational but kind of like this weird preference for just having, if he has a master's degree or a PhD in anything, they automatically think it's more of a qualification to be a data scientist, and sometimes it's not always true. Even if you have a bachelor's degree, if you can demonstrate that you know the basics of data science and that you can, you know, work in a team, then that speaks volumes. - Awesome, yeah, yeah, fantastic. And yeah, great question so thank you Vincent. Yeah, and then this last one here, because I know we're a little bit over our scheduled time and it sounds like you're got a furry friend who's ready for dinner or ready for you to wrap up as well, so we'll do this last one and then we'll do the full wrap-up. But Esk asks, "Can someone move from sales to data science? "What would be positives "and what would be some of those barriers?" - Absolutely, go ahead Dodge. - No, I was gonna say, I don't know if you wanted to talk a little bit more about this or if, I'm also happy to share kind of some of my experiences with a similar transition. - Well, kind of what I mentioned before is, most people have the same kind of block of work in front of them to get into this field, so even if you're moving from sales to data science, it's still totally possible. Some of the positives that I would see is having those communication skills, being able to understand the business and being able to understand how to work with customers and what your customers and clients are going to want, because on the opposite end of that spectrum, I've seen lots of people that are really strong with the technical skills and the technical ability but they're really poor at working with others, working with clients, or they're not great managers or any of that, so having, you know, being personable and being able to communicate is probably going to be one of your biggest strengths coming in from a sales role. But then the barrier would be how much technical expertise that you have under your belt already. If you're like a technical salesperson or like an application salesperson, then maybe that will be a lower barrier because you're using technical concepts and communicating numbers and figures and things like that, but if it's a sales position where you haven't had the opportunity to flex those critical thinking muscles when it comes to programming or math or statistics then that might be a challenge I foresee, so that's just, again, every person coming into data science is gonna be proficient in a few areas and deficient in others, like I've very rarely met people that just pick up and go, so that's kind of my advice for that, Dodge? - No, that's perfect, and then just to kind of tack on to that a little bit is, is having that sales background, like Robert had mentioned multiple times during the conversation, being able to tie whatever your work, whatever the work is that you're doing back to that company-level perspective, right back to getting that buy-in from leadership, to getting that corporate-level vision of what you're working on, having that different perspective from the sales background can help give you some additional insight there, right, you can go, okay, well if I'm working on this model, right, I need to get it to a certain point of accuracy or performance, right, or I don't necessarily need it to be perfect but I need to have a sense of where it's gonna fit into our overall company offering, so having that additional perspective can be really helpful and really valuable. And then yeah, just kind of tying it back to I think what we've both said is, is knowing where your strengths are coming in, and being able to go, okay, yeah, I'm strong in these areas, during an interview I can really kind of highlight some of those strengths. And then from maybe that sales background, one of the biggest barriers, I think, would just be starting to build out a larger portfolio or a larger body of work, and that's kind of one of the great things about the space today is that those roles are more accessible than ever before because you have the opportunity to do that, right, so you go, okay, yeah, I have this background that's not directly related to this, but I've built this project and this problem and I've made this predictive model that works with a dataset that's similar to that same industry that you're breaking into, right, maybe you're going, okay, I want to transition to a role in healthcare as a data scientist, well yeah, I've actually made a bunch of these different projects that are working with healthcare datasets, making those predictive models, so yeah, I can actually get my foot in the door with these, and then from there, in that interview process, you'll be able to talk about those more, you'll be able to flex, like Robert said, be able to flex that knowledge a little bit more. So yeah, and all of that's not to say that there's not gonna be, you know, bumps along the way, that it's not still a good amount of hard work and applied effort there, but that it is possible, right, that these doors can be opened with that right mentality and the right body of work. - Yeah, and just expanding on that a little bit, just knowing yourself is, this sounds cheesy, but that's really the best advice I can give you, once you've built your portfolio and started getting your data science experience and knowledge together, when you start interviewing, don't try to mislead interviewers into knowing, letting them think that you know more than you do or don't. You know, being kind of upfront and being aware of what your deficiencies are, what you do and don't know is really gonna be a strong indicator of are you hireable or not, because one of the big red flags a lot of employers have when hiring data scientists or software engineers is where if they get a hard question and they try to wing an answer or they try to gaslight the employer or try to basically make it look like they know what they're talking about when they don't, lots of people can pick up on that. I like to call that the academic smokescreen, don't do that, if you don't know an answer or if you can't do a question they ask you, say "Hey, this is what I know about it, "here's how I would start "or here's what I know how to fix this "but I would need to get some help "or I would need more clarification "or I would need to do some research", because that is the strongest indicator of, yeah, we're not expecting this person to know everything right off the bat, but we need to know if they hit a bind, can they ask for help and get it and fix the problem and learn how to do that and then will they be able to do that with others? So that's, it's what's most important in interviewing. - And with every new project that you get handed, you're not gonna know everything right off the bat, right, it's another way for you to demonstrate that, yeah, I can get this new problem, this new project that I'm working on, go out, get the right resources, start to formulate this overall strategy and plan of thinking about it to tackle it successfully. Awesome, well thank you guys for those questions, those were fantastic questions, certainly glad we got a chance to talk about those and kind of add that extra layer of depth to our discussion for today. All right, well with that, we're gonna wrap up our session. Once again, thanks to everybody for joining, thanks to Robert for taking some time out of your day. This has been Thinkful's Webinar Speaker Series, How to Leverage Big Data and AI. I hope you all have a fantastic rest of your night, fantastic rest of the week ahead, and I hope to see you all online and maybe even on our Student Slack real soon. All right, goodbye everybody. - Thanks everybody, thanks for having me. (pulsing music) 