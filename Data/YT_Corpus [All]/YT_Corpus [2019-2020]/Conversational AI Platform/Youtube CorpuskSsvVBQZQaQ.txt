 - [Abbie] Hello, and welcome to our webinar, Using Artificial Intelligence to Thrive in Challenging Times. I'm Abbie Lundberg, I'm a business technology researcher and writer and president of Lundberg Media. I'll be moderating today's discussion. Before we get started, just a couple of housekeeping notes. This webinar will be recorded and available to all of you within three to four business days. We welcome your questions for our speakers. You can enter them anytime in the questions module on the go to webinar control panel, or you can submit questions on Twitter using the hashtag #MITSMREvent. We'll answer as many questions as possible after the presentations and before we wrap up an hour from now. You can also use the Questions module to get help if you have any trouble with the audio or other technical issues. So just about every organization is looking for ways to hold fast in today's challenging and uncertain economic environment, AI can help on lots of dimensions from reducing costs to enhancing customer engagement, to better managing business operations. But a lot of struggle to take AI from proof of concept to enterprise to production. There are some really good reasons for that. However, companies had better figure out how to overcome those challenges or risk getting left behind. In today's webinar, we'll explore a coordinated approach to infusing AI throughout your business. Having the right platform is important, but so is having the right structures and teams and addressing deployment issues from the start. Our speakers today are well versed in this topic. Justin Emerson is a modern data architect at Pure Storage. He's worked in the data center for more than 15 years, helping customers with the journey to virtualization and converged infrastructure, and more recently guiding them through the machine learning revolution. Prior to joining Pure Storage, he led the AI strategy for a national solution provider, working with partners such as NVIDIA and Pure Storage. Tony Paikeday is director of product marketing for artificial intelligence and deep learning at NVIDIA. He runs go-to-market for NVIDIA's portfolio of AI supercomputers, and its accelerated machine learning platform for enterprises. In a recent blog post, Tony wrote, "the process of going from a concept to a model prototype, to production training and inference has largely been hand-guided and artisanal in nature." Organizations are now focusing intently on how to mechanize this workflow while enabling better specialization of AI roles that are part of it. Both Justin and Tony help business and technology leaders understand how to do just that, and that's what they're going to talk about today. Welcome to you, both Justin, it's all yours. - [Justin] Abbie, thank you very much. Good morning, everybody. As Abbie mentioned, my name's Justin Emerson, I'm joined here by my good friend, Tony, and we're here to talk to you today about how to use artificial intelligence to thrive in these challenging times. And every business has real three imperatives that they need to address. Tony? - [Tony] Great, thanks very much, Justin, pleasure to be here. So to start things off, I wanna recognize that clearly, we are in a challenging environment. Many of you today might find yourself facing unprecedented pressures and even some maybe existential threats that you see on the horizon. But for seasoned leaders, this may not be that new. If you look back many decades, you'll see similar inflection points that challenged businesses during these kinds of environments. Well-managed organizations did things consistently well in any of these prior episodes, if you will. And they were, they intently drew customers closer and solidified relationships, customer experience and loyalty, and the intently focused inwards on ways to streamline processes and save costs everywhere possible. And third, they positioned themselves for the upturn using the current situation to invest in enabling better agility to outmaneuver the competition and create an economic moat. Let's go to the next slide, please. So the remarkable thing here is that AI is actually well poised to help in all three of these areas. And many of you have probably been inundated with the AI hype cycle. Stories of robotic butlers and flying cars and whatnot. But the reality is that there are a lot of practical AI use cases that are perfect for surviving and even thriving in turbulent times. Now is a great time to implement recommender systems, to deliver more personalized experiences for your customers and tailored support with natural language processing and chatbots. Now is a great time to reduce inventory levels and improve forecast accuracy to save money. And if you're a manufacturer with large industrial facilities, you might be amongst those saving potentially hundreds of millions of dollars every year in site inspection costs by switching to AI guided drones. So now's a great time to also anticipate customer behavior and intercept churn. So in reality, AI is adept at finding the needle in the haystack and distilling oceans of data into actionable insights that can put you ahead of your competitors. So let's dive into some examples of what we just discussed here, for example, with natural language processing and chatbots. If your organization runs a customer service operation, you know how important your agents are to protecting customer goodwill and loyalty. If you've got a support number, if you've called a support number since the pandemic started, you've probably noticed increasingly longer hold times. So AI trained on advanced language models that deliver super human levels of language understanding can help you scale the number of inbound questions that you can address coming from your customers. We see it happening very successfully in every sector, especially personal banking. Companies like Clinc shown here are helping financial service providers like Turkey's Isbank reach more customers faster through mobile interface, which means their human agents are able to spend more time on more complex issues with their clients. Other companies like Deepgram are using AI-based call transcription to convert 100% of their recorded call center audio with over 99% accuracy. And they're using this data to help train agents for better outcomes where they previously could only transcribe maybe 4 to 5% of the calls with at best 20% accuracy. Now we talked about manufacturing environments. BMW is no stranger to automation. I've actually been to their plant in Munich, which is probably about 99% automated today. BMW is getting their facilities to 100% automation by focusing on the logistical process within the factory. So they've trained their AI models to handle everything from transporting materials to organizing parts. This helps ensure that the right materials get to the right part of the plant just in time, ensuring a more streamlined operation that results in higher production volumes and lower downtime, ultimately helping drive down the cost per finished vehicle. So we wanna dive into a couple more examples like this and for that, let me hand it over to Justin. - [Justin] Thanks, Tony. Another great example that touches on one of three pillars Tony talked about is Walmart. Walmart has been using machine learning for some time now to manage inventory levels, to estimate demand forecast, and we all know in the current time that we're in, it's extremely important to be keeping track of inventory levels, be keeping track of demand. We've all seen instances where we've gone out of the door in our mask and gloves and the thing that we always buy just isn't on the shelf, and that can have a major impact on customer loyalty, it can have an impact on customer satisfaction but moreover, it also has an impact on the bottom line. So supply chain optimization is extremely important in this current environment where we wanna make sure that we are handling strained supply lines, we're handling challenges around just-in-time delivery. So using machine learning for these kinds of activities was important before the pandemic and is even more important in the current environment. Similar to the natural language processing, examples that Tony outlined, another company, Global Response, who runs contact centers for large organizations, they have been implementing their own brand new contact center software and really infusing it from the ground up with AI. Everything from things like Tony said -- call transcription to sentiment analysis -- I'm sure we've all been on a phone call with a customer service representative, and we've been asked to rate how we felt about the call on a scale of one to five, and answer these six questions. And I can tell you that the response rate for those is very bad, most people don't bother to answer them. I can say I'm guilty of that as well. But with sentiment analysis, you can actually, using those call recordings, make very good estimates of how effective that customer service rep was -- did the person have their problem solved? How did they feel about the call just from the tone of their voice? So that really revolutionary thought that's going on in this page, and it's super important because more and more interactions between organizations and the people that they serve are moving online or over the phone. Whereas before, it might've been a minority or a very slim majority of customer interactions, obviously now it's moved into almost exclusively not in person interactions with our customers. And we wanna make sure that customers don't leave and go somewhere else because this other company has much better tools at their disposal to serve the needs of their customers. So while all these things were important before the current situation, the current situation has emphasized how important these are to business. - [Tony] So Justin, if you think about all these great tenable AI use cases, why aren't more enterprises immediately successful in AI? Clearly there are some great examples that we just covered today. So why isn't this the norm? So let's break this apart a little bit. First off, AI models for those who are not familiar, which are designed to let's say, solve a business problem, aren't built or deployed like conventional software. The team building them includes individuals whose expertise is in algorithms and running experiments and certainly not in engineering code, and they're not experts in things like scaling or security or IT disciplines. With machine learning, there isn't a simple process of taking a finished AI model and deploying it in production, it's resource intensive work done in a highly iterative manner by data science artisans, as I mentioned in that blog. People who are really hard to find, expensive to hire, difficult to retain, and whose output is largely opaque to anyone who wants to explain how these AI models actually work. So when you look at the sweat equity that's plowed into all that work, organizations are incurring a growing amount of what I'd call model debt in terms of the investment and resources sunk into undeployed or even under-deployed models. Model development involves a complex process that has multiple pipelines for data preparation, model prototyping, training and inference, and you're not just building a single app, it's a model, it's a web service, and it's the integration of these things. And assessing it in production, isn't a simple pass or fail determination, it needs a data scientist involved to continually evaluate its performance, which can degrade more rapidly than conventional software. So monitoring and retraining the model on a continual basis is key. So what we're doing is we're trying to improve the end-to-end AI life cycle from development to deployment, from something that as I mentioned is highly artisanal today to one that's fairly industrialized or mechanized and accelerated and integrated into a more familiar standard enterprise IT operation. To get there, we need some teams to work together. Ultimately, in order to overcome these problems, you need to bring together two worlds that are currently very much disconnected and worlds apart. This is really about enabling data scientists to focus on what they're great at and enabling IT and data engineering teams to focus on the right platform that supports the end-to-end life cycle of AI from develop to deploy. We do not want data science teams wrestling with code building platforms or infrastructure, we want them creatively experimenting, iterating as fast as possible to get to a model that offers the highest predictive accuracy for a business problem to be solved. Conversely, we want data engineers who can build pipelines that enable effortless mobility of datasets across the development workflow and working with their infrastructure team to deliver the resources and performance that speeds iteration cycles and are manageable within an IT DevOps kind of setting. Now, the happy confluence of these worlds is what is collectively known as MLOps. And it will ultimately enable more of those great models to get realized in production versus stalled at a pilot stage. MLOps includes the right hardware infrastructure that delivers the right resources for each job, whether that's data ingest, manipulation, training or inference, and it speeds the iteration cycle, so data scientists aren't waiting for results from an experiment. MLOps also include workflow management tools that let enterprises manage users, datasets and experiments such that a standard process can be implemented, that takes models from prototype to production. The net effect of this is the ability to streamline the handoff between these teams while ensuring manageability and accountability and creating a cyclical life cycle on which models can be continually evaluated for drift and retrained with new data on an ongoing basis. Now, with this in mind, it's probably helpful to do a walkthrough of what deployment looks like in practice and where they tend to land. For that, I'll hand it back to Justin. - [Justin] Thanks, Tony. So a common question that I get is "if I want to implement MLOps or if I want to build an infrastructure to support these kinds of workloads, where should I build it? Should I build it in the cloud? Should I build it in my own data center? Should I build it in someone else's data center?" And the answer is really all of the above. In many instances, different parts of that MLOps pipeline may run in different places. If your needs are extremely elastic, maybe say your inference workloads are very bursty, will happen only at certain times of the day, those might be great workloads for the public cloud. But the experiments that your data scientists run, the opportunity cost of an experiment not running because of every time they run that experiment, they're spending money, that might be more suited to on-premises infrastructure. Things that are ongoing versus ephemeral, obviously you would want to own and manage those environments, but the truth is that most customers and most companies will have infrastructure in both places, which is true of utmost things really. So that doesn't make this unique. Similarly, it's great to get started in terms of experimentation, because obviously capital requirements for the public cloud are extremely low but as companies begin to build out more and more models and operationalize more and more different algorithms, they may find that it starts to make more sense to own rather than rent. But the one constant across all of these is, where is your data? I like to say that data has a gravity, and it takes a lot of the data to escape that gravity. If your dataset is living in the public cloud, it can be very helpful to do that training in the public cloud. So you don't have to lift and shift or maintain multiple copies of your data. Similarly, if you have your data on-prem that you want to train off of, maybe it's there not only for legacy reasons, but maybe there for compliance or governance reasons, then it makes a lot more sense to have it on prem. But the goal is to enable all of those individuals who are part of that MLOps workflow to work as quickly and as effectively as possible, and to reduce the speed bumps between the handoffs from each of those different individuals. And what's important to understand is you need to make these decisions as a business leader now, because if you don't, they will be made for you. So I'm sure we all remember the stories of procurement departments going and asking business units, "Hey, why are you buying so many books at Amazon?" When it turns out that they were really just sprinting cloud infrastructure, right? The anecdote from maybe almost a decade ago now. And we're seeing similar things today in the space of machine learning and artificial intelligence, why? Because AI infrastructure is fundamentally different, the applications and algorithms are fundamentally different like Tony said. And so if IT does not evolve to support the needs of the business, the business will go around them, and this is not a good thing. It creates silos of infrastructure that are outside of IT's purview, that's bad for data governance, that's bad for compliance, that's bad for security. Moreover, it's a waste of budget because if one line of business is doing their own thing and another line of businesses doing their own thing, there's a common set of services and infrastructure that they're probably not sharing and are duplicating somewhere. And it also creates silos of knowledge within an organization because let's say one line of business has figured it out, but the other is still struggling, if they're not aware of each other's efforts or they're not sharing information and infrastructure to help solve these problems together, then you end up with speed bumps or slowdowns that could have been avoided if we addressed these challenges centrally. So what are some of these speeds? So Tony talked about all the different points in which you have to consider what goes into actually not only building, but also training, deploying and operationalizing a machine learning or deep learning algorithm. There may be areas where you're ingesting data, there may be areas where you're storing metadata about the algorithms that you're training, there may be databases where you store performance data, there may be monitoring information that IT needs to determine how healthy the entire pipeline is, and that's before we even get to the datasets that you're working on, which are inputs, the data that you're curating in order to train a model, the process dataset, so this was after going through data hygiene processing, other kinds of transformation to prepare it for training, and then finally, the actual results of inference, which are the profits of introducing new data to these algorithms. Each of these different steps along the pipeline carry its own set of data. And if you're not careful, what happens is you end up passing data from part to part to part to part and creating multiple copies, creating time delays as these massive datasets can move throughout the system. And then that only gets exacerbated the more of these models you deploy 'cause each model is gonna contain this pipeline. And one of the common things here is the storage. So looking at not only how do I make sure across my entire enterprise, that I'm reducing duplication of effort, how am I avoiding shadow AI, but also within the context of each individual model or project, that I'm reducing duplication of effort, both from a storage standpoint and from a processing standpoint. So it's imperative to build infrastructure, to support these algorithms that doesn't create all of these silos of data because data itself takes time to move, it takes time to process and the more we can do to centralize that workload, the faster everything is going to be from end to end. But it's also not just about the storage, it's also about the computer. - [Tony] Great, thanks Justin. So what Justin's shared with you is the basis on which to build the right platform for AI at scale. Now in tandem with this, I wanna share with you how IT teams need to think about infrastructure for AI. And if you took a look at a typical data center built on what I'd call legacy, compute or traditional CPU-based computing, you'll easily find three silos of servers, each silo designed in scale to attack one kind of computational problem, whether that's analytics or training or inference, for example. Three different kinds of servers, each with their own architecture, resources, planning considerations, and scaled independently of each other, each silo capable of running only what it was intended to run and nothing else. This inflexibility is driving up capital and operating costs inside the enterprise data center. And so we've worked intently on solving this inflexibility with a new platform in architecture that delivers all of these things, offering enterprises a universal building block for the new AI data center, fully optimized for the end-to-end lifecycle of AI, from prototyping to development, to deployment, from analytics to training to inference, it does it all. So now, enterprises can enjoy a single universal platform for homogenous infrastructure that supports heterogeneous workload, run any AI job as it were in the lifecycle, on any system, at any time. And every system can be tasked to tackle the workload required and run that workload with the right amount of compute resources, right size to the job at hand. And with one system type for the entire data center, we've not only consolidated the silos, we've made capacity planning incredibly simple. So now, building your AI infrastructure on an optimized platform means having, if you will, your own elastic AI infrastructure, your own private AI cloud. And at the end, we'll share with you how you can explore this further. So where do you take this? Forward-leaning organizations and enterprises that wanna infuse AI into their endeavors are taking a new look at infrastructure. They realize that running up OpX in the cloud and shadow AI is not really making sense. And they need to understand that the right infrastructure strategy can be an enabler for consolidating AI expertise across the organization, standardizing best practices and accelerating the time to solution on the most pressing AI opportunities. This is what we call the AI center of excellence and many teams maybe like yours have used it to democratize the access to AI development resources. Centralizing development lets your team pool previously disparate resources, platform and personnel such that you can eliminate innovation silos and streamline and accelerate AI development workflow. And remember when I mentioned earlier on the artisanal nature of data science development and how it's hard to attract and retain talent? Well, when you build a centralized shared infrastructure for development, you're also creating a platform that benefits your people in two ways. First, your center of excellence or CoE attracts the world's best talent. We've seen it happen over and over again in businesses like yours. They all want to do these data science artisans, they all wanna do their life's most important work using the best tools that are out there, that's what they've been used to. The CoE demonstrates that you're a forward-leaning organization with respect to AI and you have the tools and resources that they can use to build incredible things. Second, the experts who can build your best AI applications are maybe already working for you. They're inside your business units and they know your problems and data better than anyone. Many of them want to evolve into data scientists, but they need mentoring and an environment where they can learn valuable skills while shadowing other experts in your organization. The center of excellence creates that environment and lets you groom and scale citizen data science expertise from within, saving you a lot of money, versus hiring from outside. So let's bring all this back together. IT decision-making around platforms and standards starts with the CIO and their infrastructure team. And if you're the CIO, you may be facing an existential crisis of sorts. In tough times, nobody wants to be seen as purely a cost center, but rather they wanna be seen as an enabler of business transformation. The reality is that IT can actually lead the strategy to accelerate business transformation with the power of AI. But first, you need an infrastructure standard that can actually industrialize modern AI development workflow, instead of letting researchers run rampant, running up OpX and doing do-it-yourself platforms on shadow AI, as we've been calling it. CIOs need to get in front of all of that and drive architectural choices that will centralize AI compute infrastructure and consolidate people, process and technology. The result will be amplifying the expertise that exists within your organization while enabling a robust talent development pipeline. And it'll shorten the deployment timeframe and let your business see more of their innovative models, make it from prototype to production. And certainly in challenging economic times, it'll help your organization save costs by consolidating infrastructure and reducing spend. Thanks to higher, more efficient utilization of resources. All of this is possible with the right game plan and Pure Storage and NVIDIA can help with AIRI, AI-Ready Infrastructure. AIRI from Pure Storage and NVIDIA offers IT a proven enterprise great approach that can become the standard platform for industrialized AI from the development to deployment. - [Justin] If you'd like to know more about AI-Ready Infrastructure, there is an on-demand webinar that Tony and my colleague, Emily Potyraj presented just a couple of weeks ago about simplifying the AI transformation. There's also white papers and other collateral available as well, and we'll provide those links as part of the webinar. - Great. Well thank you to both of you, Tony and Justin, thank you so much for that presentation. And we're going to take some questions now and we've got a lot of great questions already coming in from the audience and if anybody else has questions, just put them into the Question module there. We'll continue to take the questions for the remainder of the hour, you can submit these in the Question module. Just to kick things off though, I was really interested in a couple of the points that you made, one was about the MLOps and that sort of cyclical nature of going, "How the teams are working together?" And the other one was about your sort of that process of the whole flow of AI. And my question is, are those teams and is that process addressing data accessibility issues when models are brought into production? So models are drawing on data, they're getting data from a variety of sources. Once those models get into production, is that being thought about upfront? What are you seeing? - [Tony] Yeah definitely, organizations that do this well are truly looking at it from the very end points of the entire workflow from even the upfront data preparation side of it. Because obviously, your problems begin when you're essentially trying to find the right datasets within which an answer may lie to the business problem you're trying to solve. So the workflow and the MLOps platform and approach, if you will, extends right from the earliest start of curating data, manipulating that data, bringing it into a prototyping process, the whole iterative nature of that through to pushing through a winning prototype into production training, and then inference. But the folks who do MLOps well, from what we've seen, truly look at this from the end-to-end. And similarly, they have a mindset around integrating that into their existing dev ops practices. Justin, I don't know if you wanna color on that as well. - [Justin] Yeah. - [Abbie] And Justin by the way, before you do Justin, just for the audience's benefit, Tony was having some technical difficulties with his camera, so we just have to suffice with his picture there. - [Tony] My apologies, yeah. - [Justin] So I agree 100%, Tony, and part of that data rationalization or identifying where all those datasets lie, a lot of that ties back into the whole on-prem versus in the cloud discussion as well because if you find out that half of my datasets are here and half of my datasets are here, that presents an upfront challenge that you have to solve for going in, because the best algorithms use all of the data that's available and you don't want to hamper or handicap the capabilities of what you're building because you didn't have the right data locality strategy. - [Abbie] Excellent. One of the questions came from, let's see. So the question from the audience is, this sounds really good for big companies, how can you, what can you rescale for smaller organizations? So, given some of the complexities and the investments that are being made, can this work for smaller organizations? - [Tony] Justin, do you want me to go first? - [Justin] Go for it. - [Tony] Yeah, absolutely. And I would say runs the gamut from we've seen even startups organizations with maybe a dozen people or less, take the same kind of mindset and approach as well as obviously ultra-large organizations. So it does run the gamut, the architecture that we're prescribing and what you'll learn more about through the links that Justin shared is very modular in nature. So there's definitely what I would call starter or entry level positions that let you begin with just the right amount of compute and storage resources that you need that fit the amount of model development work and training work that you need to support. The good thing about these approaches is they lay a foundation that you can grow into such that if your business grows or the volume of production training grows over time, you can add capacity and capability like your own private AI cloud, if you will, without having to forklift a ton of iron in the process or see a big step curve in terms of your capital spent. So there's definitely that aspect of linearly predictable performance at scale, such that you can choose the entry point, which makes sense for your business. I'll also just say that there are additional form factors that if you are purely anchored in a prototyping phase, we also have like workstations as an example, built on NVIDIA GPUs that can let you purely just iterate at a data scientist's desk and then push them into, for instance, an area infrastructure when you're ready and ready to scale. So there's definitely more appropriately sized entry points depending on what your business needs. - [Justin] And I think also a challenge small companies may say, "Well, I don't have the capital to outlay on a large AI center of excellence." The good news is that both for the cloud and the on-prem case, there are operational cost models available today which allow you to start small and grow and you can maintain that OpX model in perpetuity or you can reach a certain critical mass and then say, "Okay, now's the time to make those capital investments." So regardless of whether you're talking about on-prem or cloud infrastructure, the ability to start small with low operational cost, as opposed to high capital investment, those options exist. And those are ideal for smaller organizations that want the infrastructure to grow with them. - [Abbie] So we had a couple of questions that address the cost issue, the cost-benefit equation, and one had to do with, and you gave some great use cases. One had to do with the investment which you just addressed, but how are you seeing companies way that, we know we have to invest X. How are they targeting then the applications and the use cases that can get them the biggest bang for their buck in terms of does it have to do with customer experience or does it have to do with productivity and efficiency? - [Justin] I think there's certainly the three examples or the three pillars that we talked about at the beginning. I think Tony and I both feel that those are low-hanging fruit to a degree. There are things that are clearly challenges in the current environment, they're are things that probably every company has to deal with. If you have customers, you need a good customer experience. And so, those I think are good starting points, there are big advantages there as well because they're well understood use cases, they're ones that are not science projects. A lot of people think, well, I'm getting started with AI and I'm gonna do something that no one has ever done before. The truth is most businesses don't want to do that, most people don't want to be the first person to ever walk that path. And all of the use cases that we outlined, not only is there one marquee example, but there's dozens of them, and there are ones that are good places to get started that have a high return on investment and a relatively low barrier to entry. - [Tony] The other thing I'd just add on there is, if you were to try and undertake even the ones that we showed at the beginning, those use cases a couple of years ago, you'd probably be putting a lot more sweat equity in terms of the model development process and a fair amount of software engineering within let's say the AI framework that your developer team prefers. But it definitely would have been some work, let's put it that way. Today, you can literally go to places like our NDIVIA GPU cloud or NGC, and all these models are prebuilt. They're already basically assembled and pre optimized for the hardware such that you just pick and choose in a grocery card almost. I don't mean to trivialize it, but essentially, you wanna recommend your system, we've got a model for that, if you wanna do anomaly detection, there's a model for that, you wanna do a language model, NLP, there's a model for that. Essentially the models are already done and they require just light customization, based on maybe the kind of datasets you're presenting it or other nuances. But essentially, 99% of the work's already been done, you can literally drag and drop the models and use them, also scripting, a lot of that stuff's been done too. So, just a long way of saying essentially the amount of lift that you have to do or money that you might spend to operationalize those use cases is dramatically less than it was years ago. - [Justin] And I think- - [Abbie] That's a great one. - [Justin] One of the big developments is that the concept of transfer learning has gotten so much better. So before it might be, you need to, like Tony was saying, two years ago, you might need to build something from scratch. But now, the actual techniques and the training has gotten really good in terms of saying, "I wanna start here and I wanna train this natural language processing algorithm on my particular kind of language." And I don't mean language necessarily, like English versus Chinese versus German, but in my line of business, these are the terms that I use that no one else uses. Maybe it's technical terms or it's medical terms. But all of those are things that wouldn't be in your run of the mill off-the-shelf algorithm, but the techniques around transfer learning have gotten significantly better, such that it's possible to take these mostly complete sort of pieces of play and just finish off the edges as it were to make it look like what you want. - [Abbie] Great. We have a question about the center of excellence. Can you give more details about the roles inside of the CoE? And also, I'm going to add to that question, how does it then interact and engage with the bigger enterprise? - [Tony] This is one of those things where essentially, this is a team sport, and yes, the CoE would be under the purview of typically like the CIO or chief technology officer, but if you look at it, all the most important work done there is a multidisciplinary thing in terms of, it's the coming together of not just data science expertise, and let's say data engineering teams and infrastructure teams, but it's also for instance, application developers, it's also business analysts that sit within business units, people who are very close to the data and the problem trying to be solved. So the CoE on a technology basis is definitely like in IT infrastructure that simply creates an environment around which all these teams can pool resources and expertise and actually collaborate on projects. And when it works well, essentially, the way it starts is I think you attack low-hanging fruit. You'd find a tenable use case within which to quickly prove success, maybe along the lines, the ones that we shared as the opener today. And when your team quickly goes through the motions of prototyping, going to approve a concept to a pilot deployment, to full scale production, they gain muscle memory around how to do this well and what works and what doesn't and they share best practices. And what that happens is they prove success quickly, their executive sponsors essentially then can use this proof point as a way to secure more funds and more resources and more support and this thing becomes like a flywheel because it gains momentum and now this team and the CoE is able to attack bigger and bigger projects, and things that are much more complex in nature. But that's a very typical trajectory of how we see personnel and resources coming together in enterprises that decide to embrace that idea. - [Justin] And I think a lot of the same way that the cloud center of excellence, the idea was to take different people who would be champions from each different part of the business. I think that's another really key part, just to emphasize what Tony said, is that it's important to have different stakeholders from different groups, because truly, AI is about solving business problems and then the technology to support that comes afterwards in terms of workflow. So what you wanna have is you wanna have a strong understanding at the business level of the problems you're trying to solve. You don't want to spend months trying to solve a problem and figure out that you solved it the wrong way, or you solved it in a way that doesn't actually help the business because you need data that they don't have on a real time basis or something like that. So having investment from all different parts of the organization, not just IT, not just data science, not just the business, but even more than that, going all the way through to depending on what your business is, people that are in charge of product development or things like that. You wanna have all those people at the table so that you can make the smartest decisions and you can build the best internal process to support the needs. And you can't really do that unless you understand what those needs are. - [Abbie] We have an interesting question about -- it's sort of a opportunity question. Given the current environment, and obviously the economy's taken a hit, but AI has a really critical role to play in helping companies through that. The question is, I expect that there should be a demand for senior AI leadership roles in businesses. Can you please share your view about how you see the situation on the job market? Who are the potential candidates for these AI management roles? - [Tony] I think what we see happening is, I don't see necessarily a persona emerging that has like AI leadership or AI executive title per se, but I do see specific individuals within business units and IT teams becoming executive sponsors for this. On the IT side at the practitioner level, we definitely see titles emerging, like machine learning engineer, even MLOps engineer, I think we've started to see those kinds of titles surface. I think the right way to look at this is not necessarily trying to reach for an AI title or mantle if you will, but rather looking at within your organization or other organizations that you might be studying from outside in, do they have a culture and mindset around AI and especially if they have a focus on infrastructure and platform, are there roles within their IT team that would offer you the opportunity to for instance, bring your skills and help them build the right kind of purpose-built architecture for AI development? So obviously there's differences between organizations that have a forward-leaning stance in AI development and they're making investments versus ones that are still on the sidelines and still trying to decide if this is something they wanna pursue. So I think you obviously want to look for the former versus the latter. - [Justin] And you don't need to understand neural network and gradient descent in order to be able to articulate the value that AI has to the business and coordinate those things. There's an AI as a tool and there are people within the organization who have to understand how to wheel that tool, but you don't have to be the expert in deep learning or machine learning in order to understand how to leverage it in your organization. - [Abbie] There's the question about data. What is typically the biggest data problem that you run into and what is your advice on how to fix that in a very large global enterprise from a data management point of view? - [Tony] Justin, you wanna grab that one, or? - [Justin] Yeah, I think two big things that come up is like I mentioned where the data is, and that isn't necessarily always cloud versus on-prem, it could be, well, this data's here and this data is here in these different data centers or these different continents, or they're in these completely different security contexts. This is data that has personally identifiable information, this is data that doesn't. How do I bring those together or otherwise leverage those in a way that maintains my compliance requirements? And the other thing I would say is data cleanliness. What a lot of people find is that they've been collecting data potentially for a long time and it turns out that there's problems with that data or that this particular field we wish we were collecting, but we weren't, or sometimes you have instances where we've ruined a lot of that data and we've put it in our data warehouse, but we're throwing out a lot of the ancillary data around it. So now, we're missing these pieces. So figuring out what you have and where you have it is probably the largest challenge. - [Tony] I think another aspect that lends to what you just said, Justin is, if you don't look at it truly from the end-to-end, namely from that initial ingest and manipulation of the data, cleaning the data, transforming it through to prototyping and into production, then you start to see speed bumps in your iteration speed 'cause the name of the game with AI model of development is speed of iteration. It's a recursive process that after many, many turns of it, ultimately results in a model that has high predictive accuracy. The problem starts to become is that if a lot of your data exists in different places, you now are faced with having to move very large datasets from one stage of the pipeline to the next. So if you don't have the right underlying data platform that enables effortless mobility of these very large datasets from one stage to the next, you are gonna incur speed bumps along that pipeline, the iteration speed will slow down and as a result, your time to insights will drag on. So it'll definitely not be a streamlined process. So I think you have to be thinking about all the stuff Justin talked about and as well, do you have the right data platform underneath it that can seamlessly move that data from one point to the next? - [Abbie] Great. We have a number of questions from perspectives of different industries, different functional areas, so I'm gonna throw out a few of those that we have, and you can jump on whichever one you feel you can speak best to. So one was, could you highlight a couple of examples of AI use and benefits shown in the procurement and logistics processes? There was one from manufacturing and there was one, is, does this really apply within education? So I'm gonna throw all that at you and you can pick one of those. - [Justin] Well, I'll start with the manufacturing piece. Something that is very relevant right now, we're all trying to practice social distancing. If you have a facility, you're trying to figure out, okay, should I have people come back into the facility? If I do, what are the guidelines around how they need to conduct themselves? So manufacturing, automation and other kinds of sensor networks within manufacturing become even more critical now, because you may say, well, I used to have a person at each part of this chain, but now I can't, I need to have them further apart. So how do I take this piece and how do I replace it with, say, computer vision or some other kind of automatic system so that I need less people physically in wherever I'm doing my manufacturing, because for safety reasons and for public health reasons, that's now mandatory. So it's not just about, is it going to save money, but also is it going to be the right thing to do from a health stand? - [Tony] Yeah, and definitely we've seen a lot of focus on AI in education higher ed, especially, there's many organizations, institutions across the globe that have doubled down on building their own versions of AI centers of excellence. You think about MIT and many other organizations that are building world-class facilities for a number of reasons. One is to attract students but also, it's to actually build the next generation of AI enabled apps. So what you find is, many higher ed organizations see this as a very important place to attract the best talent, garner more funding for world-class research, evolve their curriculums to speak to data science and data science related disciplines, and also enable a workforce that can ultimately be more equipped for this new industrial revolution that we're finding. So definitely, in education, especially higher ed, you'll see this proliferation of AI. - [Abbie] Go ahead. - [Tony] What was the other one that, there was another one, I just missed it- - [Abbie] It was supply chain logistics? - [Tony] Yeah. Well with supply chain, definitely we see opportunities for, for instance route optimization. And there's organizations like USPS, the folks who bring us our mail, as well as Domino's Pizza. I did not expect to be hearing about an incredible use case from Domino's on how to optimize route delivery. And if you look inside the store, how they built Pizza, they focused intently on things like order accuracy and forecasting when your pie is gonna be ready based on a combination of factors, including the state of the operation at the store, number of orders in queue, complexity of the orders, time to deliver to where the customer's situated relative to the store and to put a thought something like pizzas can be... But essentially, it's a supply chain, it's logistics, it's route optimization, it's all of these things. So if you can find, and I'd encourage you to Google the Domino's story, we've written it up on the NVIDIA sites. If you go Google it, I'm sure you'll find it. But if you can find stories that are as impacted and infused with AI in the fabrication and delivery of pizza, you can pretty well find it anywhere, I would say. - [Abbie] Great, we have time for- - [Justin] (murmurs) very seriously. - [Abbie] We have just time for one more short question. And I know asking for a short answer when we're talking about this topic is asking a lot. The question is, what is a fair timeline for an AI project to be industrialized? So for critical scale, what is the timeline to scale? And I know that's very dependent on- - [Tony] Yeah, it, it runs the gamut. We've seen, I mean, the thing is the state of the art of this infrastructure is you can now build a world-class infrastructure in just a couple of weeks, stuff that used to take six months or eight months. So building a platform is not as complex or as intensive as it might've been before, but we've seen organizations that have stood up incredible results in just a few months going from concept to operationalized and deployed model, Justin, I don't know if you wanna throw some additional color on that. - [Justin] Yeah, and just like in one of the earlier slides, when it's not done well, it can be on the scale of months. And if you are going at it without the underlying process, well-defined with doing it in an intentional way, you can build a model and not have it in production months later. But if you are mindful and have a good, strong plan and strategy, what you can do is enable that to become a much shorter window and the faster time to value, the faster you're starting to see benefits from the investments that you make. - [Abbie] Great. Well, we are just about out of time, so thank you, Tony and Justin, so much for sharing your insights and your experience with us, and I also thank you all in the audience for your attention and your great questions. There's a lot that we weren't able to get to. Over the next few days, please look for a feedback survey we'll send via email, and we really do appreciate your thoughts and opinions. A recording of this program will be available within three to four business days, and we'll let you know how to access that. We'll send the link to the recording as well to the PDF of the slides. So final thank you to our sponsors, Pure Storage and NVIDIA. This is a timely and important topic, and we appreciate your sharing, your insights and perspectives. Thank you so much. 