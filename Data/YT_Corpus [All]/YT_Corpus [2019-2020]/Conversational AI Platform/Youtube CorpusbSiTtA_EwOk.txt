 [Music] hello everyone welcome to Google AI huddle thank you so much for for being here and thank you to everyone who volunteered earlier to rearrange all the chairs we had a different set up which wouldn't have worked out so everyone who did that and really appreciate that thanks a lot how many of you are here for the first time okay roughly half of the crowd so right I mean basically you know googly-eye huddle is an open collaborative and developer first AI forum and our goal is it driven by AI Googler expertise and the goal is to have AI conversations out in the open right no strings attached you can come in have you know pizza and how the AI conversations talk through your problems and discuss you know things you know which you have blocked on how do you see we can help you with your AI workload and you know sort of enable enable AI in your business and whatnot additionally we also have what we are calling a huddle office hours and office hours are away for it's still in pilot but it's a way for you to literally hop on a hangout VC conversation and talk to a Googler right so I think I think that's that's a great thing and again you know no strings attached it's all is free of course and our goal really used to learn as much as we can from you to really understand what your pain points are and and how AI can enable you and how we can enable you with that I'll hand it over to Shantanu hey all I'm Shanthi no I'll eat that cloud AI user research team and also responsible for data analytics and IOT so one of the missions that we are on in the user research so basically what we do is we bring insights from the field back to the product teams and so this channel is a great source for us because you guys you can connect with us tell us about your problems so basically we'll get into details about your problems through office hours through forums like this and also we have like an hour or so before and after this session for you to network with us so my goal personally selfish goal here is to gather all the pinpoints that you have hopefully we can turn those pain points into product features and eventually solve your problems so that's how I'm involved in this effort how many of you use notebooks and curious okay great awesome fantastic so Slava who I work closely with is gonna be covering notebooks today specifically with DCP in mind oh okay you want me to do the intro okay okay all right okay yourself about this okay so I'll do this quickly so slow is a Google cloud AI tech lead with seven plus years of experience working on deep learning images for Google Cloud II i teaching java courses blogging about deep learning and tensorflow basically making things work he also has a link on the meetup event page so feel free to check out his link and about his work more than anything is just a great guy that's why we partnered with him and focus on his team and we saw a lot of interesting problems every day order you thank you everyone yeah my short name is Slava that is my full name over there yeah and before we begin there was already several questions asked but they still would like to ask several more just understand what the what you guys are using day-to-day basis so you already guys answer about notebook now the question who is using Google cloud okay some part of the audience who have heard about our product is the data lab or deep learning VMs okay one hand only one hand okay but someone okay now this talk is about so yeah why I'm going to just discuss with the main audience while you guys have time to open Twitter and just follow me with that particular tag but while you guys doing this I'm going to discuss coda one is for this talk so there are many audiences people who using Jupiter lab on day-to-day basis with a big in in the team not just by yourself as one one team member per team if you if you're using Jupiter notebook for a small project that you're working on your own as a scribe pook probably these practices not going to resonate with you but if you use it if you have tried to use notebook with a bigger team with virtual control system you probably know how painful is so we're going to start the talk with Jupiter the notebook development manifesto manifesto is basically a collection of small theoretical rules or some suggestions that you guys can use but the thing is the key part here that this is just the statement there is no easy recipe how to convert this statement to a nice nice best practice that you can apply in this topic I'm going to show some of them some recipes that you can come home and actually try to apply with your team but the key here that this is more concept something that you need to try to aim and several ideas how you can do some of the things already today there is no silver bullet exists right now on the market different players trying to tackle the solution for nice collaborative notebooks in their in their ways and we're going to see how Google actually solving this problem this talk effectively featuring one article that was published with effectively this manifesto and github repository github repository is the source that not only has manifesto but also has some tools that already been in the developed that can help to enforce some of the ideas behind the manifesto so let's force first walk step by step which each of the statement and then we can jump to actual implementation so statement number one Jupiter notebook is effectively just the tool and there's any tool you as a software engineer do need to use all the best practices that applies to software develop the process this is a simple statement but it's actually been ignored on the in a wild drastically and we will try to figure out why today second one Jupiter notebook needs a version control system integration it's not so right now if you've tried to put your notebook at the gate you know probably how horrible disease how to do a code reviews for Jupiter notebooks horrible to the knowledge conflicts it just does not work the serpent gbto notebook should be absolutely self 100% self-contained we will see why and what this can give us there should be a way to pass input arguments to the jupiter notebooks and this is actually give us a powerful way to create things from the item number five each jupiter notebook should be checked by Cugini system integration and the last one before we're going to dive each notebook effectively should be plugged into a continuous development system and each artifacts produced by the notebook if it's passing particular minimum goal of your team or your product should be deployed to the production the same as you will do with any other software now all this effectively these on the github you can go and check the tools that already exist you can actually try to propose some changes if you see a typo or tarawa now in this talk we're going to cover the first three item more or less in depth with some what we'll call double cover item number four and number five and unfortunately it's completely out of scope number six as of today conversation there are actually PZero priority zero that is not there formally but it should be there should be a way for the very easy way to use jupiter not this is probably the first that we're going to cover and then we'll jump the first one two and three so if you on the google cloud I'm going to stab to walk you through the solution that we have and how we tackling this particular problem how to make use of the Jupiter lab very simple for our customers how many of you guys have heard about Google co-op okay slightly more than about deep learning via that is mmm not a surprising useful but as ATL of deep learning VMs not a happy news as well okay so let me speak about this the solution and how they are different we effectively already have co-op which is really simple way to start using notebooks and if you haven't sync up let me try really quick just open it co-op oh I don't see my my my mother's okay never mind I've tried I've tried so let me compare this the institution if you've never seen co-op before just I would encourage you to go and try and search just Google co-op co-op collaboratory Google so what are the difference of activity between these these two tools now let's first speak about the collaboratory the nice part about coop is first to start you're actively getting absolutely 100% managed environment that can be start within a second you just opening a browser you can start using your your notebook you're getting for free one GPU which is nice you have k80 you're getting for free one GPUs yes it's not the latest TPU but still it's free and it's a CPU you now have amazing collaboration experience where several people can edit the same notebook a kind of thing the way how would you envision the Google Documents work now this is a process obviously is any product there are some cons and collaboratory doesn't give you any persistence so if you working with a notebook your VM going to be recycled each 12 hours or some amount of hours this effectively means that if install your customer packages is going away you're also not able to start any long term training because your VM going to be recycled it's not possible these more resources whatever we give it to you that's it you cannot attach 96 skylight cores or two GPUs or a GPUs or whatever there is no versioning support which means you cannot say I need ten the flow one month ten or I don't so I don't know pi torched 0.4 there is no integration with version control system and no nice integration with GCB and last but not least raw right now in the production support 2 main framework tender for Python and the factory that is it now if all these cons acceptable for you let's say you are one one person T that develops using notebook as a scratch pad you probably do need to stick to club I would highly recommend that indeed if the collapse is a tool that you can use you should be using now the second product that we have is effectively product several products built around the planet VM which is when your Jupiter lab on the Google cloud pro so this stop stop persistence you're getting exactly what you're paying for if you if you're creating it VM it will save whatever we have that if you're starting packages it's not going to be recycle everything at that it has integration with it flexibility of the research resources we will see how you can easily edit it at a GPU idiot a GPU do whatever you want that's nice GCP integration and finally we're showing you can specify which particular version turns off already now the cons it's not free you need to pay like we cannot give you a to be 100 GPU for free even for a short period of time now collaboration feature is not supported that is also something that Jupiter lab as a community trying to solve but it's not there and finally time to start the slightly hard because you actually need to stop VM now among this product we're going to focus on the Jupiter lab mostly because it's very simple to integrate different best practices to this particular solution and Before we jump into different other item from manifesto let me show how quickly and simple is actually to create one of the VM because all other examples will be on the DL so this is how creation process looks like did you do let me play a quick video this is effectively cloud marketplace one of let me open cloud marketplace one of our products on the Google cloud if you type there deep learning VM you will find with this this solution on the marketplace and spam the VM is simple you click in lunch then you effectively specifying all the configuration that you want configuration the name just put something meaningful for you something that you will know why you created this VM one VM is effectively you can think of one VM is one Jupiter lab instance that allows you to run many many different notebooks okay this would be PI touch playground you specifically you can specify zone it should be obviously as close to you as possible and the Machine type with the amount of CPUs on top of this you can specify how many and what type of GPUs you can use let's let's pick P 100 and the framework I will pause here just for a moment so these are the framework that we're supporting that's a flow several addition tells a fall to the toy is just experimental mode inside will ready pre-baked several interesting deep learning courses that been built around and the flow to the doll actually was surprised that there are actual deep learning courses already around tensorflow to the top we do have pi touch with past the eye with pasta occurs pre-built by actually supporting students of past the eye we have change extra boost MX net and for base images base images is effectively just in images that has everything you might need to start using GPU if you have I don't know open CV compiled in the way that capability lies in GPU and you want to run it somewhere you need Nvidia driver if you need cooter you need to do an end or maybe coup blocks or something else there is not much reason not to use base image if you're planning to use GPU just because you will have to install everything that there yourself okay so let's pick something in this particular example PI torch now this is a feature that we just have announced effectively because of the DM as soon as you create a VM you need to be able to access it to run your to see a Jupiter lock this better feature effectively will give you a link nice-nice link that you can open in browser that link would check your identity whether you have access to this particular GPS all up or not and if you do it will proxy all the traffic to actual vm that you have created the just a nice touch that all of you to access vm after the creation and really easy way and obviously need to install the driver okay so this is effectively the more or less the creation part then you need to wait and this is reason why it's video because i [Music] make it I think 20 feet for this creation because it's it's slow unfortunately yes it would be it would be really really slow to show it real-time in the end of the creation you will get one particular comment executing which will give you a link as soon as you have this link now you no longer need to do anything you can just add it to favorites or whatever so here is a link and just the showcase that we do have running Jupiter I'm going to open this link this is your Jupiter so that is it you have fully worked Jupiter on the cloud with P 100 GPU under the hood there is some tutorials already pre-baked in this particular case this is Python so there is a pass the eye and just to make sure that GPU is actually accessible media semi to show that we have T 100 so everything works as expected video driver there okay so this is simplicity this is how you actually can I'm going to run the whole notebook how you can actually access Jupiter notebook on the GCP so we have covered this part priorities you let's speak about the tooling the item number one will be more conversational and rest of the item will be more practical so just the two what does it mean let's say you're speaking with Android developer and you ask an Android developer who came to you on the interview who wants a position in team you're asking what are the best software engineering practice like CI and CD or something on base that you have used on your project and suddenly the person tells you oh no no no that is different like we Android developers we don't using all this like what are you going to tell such person probably okay it's nice answer will call you by right but suddenly this strange conversation somehow makes sense for Jupiter love everyone whom you asking about Jupiter notebook will tell you no we're not using it like we're not using sorry I'm not using CD we don't have tests around our notebook we don't have any right because we are different in see this is just the tool there are some practices did a specific form droid lab or Android studio that is for sure because it's XML because it's UI but similarly there are some best practices for Jupiter laughs it doesn't mean that if they have custom practices you cannot apply generic software developer practices any Android app should have the ICD was test any Jupiter love that you committing to the source code system should also have this now unfortunately there is no best practices tooling around on the market that is why a lot of the people are forced to use Jupiter lab or Jupiter notebooks at a scratch pad I've hearing this again and again from many developers when they prototyping they having a notebook they prototyping in a notebook but then because notebook as a tool is not ready for production or use they rewriting the code usually when you're speaking when you're diving deeper to the problem you'll see that no one wants to rewrite the code from notebook to actual Python because it's consumes a lot of time and it's actually something that you don't need to do you have to do because you don't have tooling around now there are some cases when you actually just using Jupiter notebook as scratch pad that is for sure the same how I for instance have many pets projects on the Android and a lot of Python on my local laptop that I not covering with the I not covering with tooling for that particular cases you can use call up collabo is perfect solution for that use cases if you don't need anything except one notebook with Google Drive for just experimenting probably also don't need too much ressources under the hood co-op is a deal to so this is the idea on the high level behind I would hide the mentality that this is just a tool now the second part effectively all at the part if you look on this this is just extensions of the item number one second part is version control system usually when we're saying now the expression control system was speaking about get not many people using mercurial and I hope almost no one using SVN nowadays or or before so let me show you exact problem and what what the solution might be for this particular political element so let me start with the problem here is a notebook notebook on Japan aquarium notebook inside of the get I'm going to execute two cells and from my perspective I have not added this notebook it's perfectly fine right now there is I have not changed anything but the notebook by itself adjacent file that has some metadata inside under the hood a lot of stuff have changed for instance you now have this number one you now have star on the second cell then you will have number two the second cell and if you're actually going to go and get client we do have some good client integration you will see that this file has been modified already okay let's assume you're coming back to this file after after week after a vacation you see that this file being modified you want to see if you need to check in to commit this file right you see that somehow be modified and normal reaction to go and see it if let let me see it see food what have I done with that particular file what have I changed do I need to commit it or maybe not it says that file that modified and now it's now let's see what's what's D for showing us and here is the DV from my perspective actually they cannot change anything in the notebook but from perspective for the notebook there are a lot of stuff that have changed and if you know how internally everything works maybe you can guess but maybe not and this is just a small change if I would do if I were to execute several more cells you will have a bigger bigger and more horrible deep lying around okay so how this can be improved inside of the github repository that I showed in the beginning when I mentioned that Manifesta actually have some of the tools we already have some preset of the tools that help to improve deep learning vm experience we're going to work on the planning maybe someone want to to improve it and adopt some other some other frame environments but as of now it works only our diploma creams so it's effectively a small script I go on the github repository that comes with this talk you will have the link video slides I'm going to run just a small scrip scrip that enables get integration again it's now has turned 10x speed because it will take time to integrate everything in dip learning vm that we need as soon as done it's going to reboot Jupiter lab this is okay now let's see what have happened in the end now who opening this same notebook and what have changes is to small icons these to small icons on top one with small get icon and clock like icon effectively it's one of the plugins that allow you to do a really nice and amazing things all of you to do a normal difficulty of the notebooks here is a normal deep which effectively shows on the left hand side or the same cell that in was empty now in this one then the second cell the same image to nothing have changed it has this blue item showing the deep of the metadata but that's it it's actually normal deep we do have also integration of even margining and conflict resolution for the different notebooks for the different notebooks one more example of the d4 with more complex situation can look like this so this is just a teaching of the notebook again on top of did the justin normal teaching tool executed on deep learning vm that shows you normal normal different okay so this is how we can enforce one of the item from best practices this is again just link for the github with manifest and tools now search sort item do be the notebook should be self-contained and first let me show what possible to do if your notebooks is self-contained so let's say yeah let's let's start it usually with the exam let's say you prototyping your notebook suddenly you have decided to do a GPU because you don't want to pay money for GPU so you want to detach it right now it looks like this you're going to be an instances all your instances are there and there are probably many of them I pick one of the chunk who is a lot a lot of instances now in order to find it you probably need to go back to your deployment to figure out the name of your instance by touch one playground find it here you're ready spending some time now you can edit instance but in order to edit instance you need to stop it stopping text time again I have 10x rewind here after style after full stop you finally can edit it you can edit them you need to find where is the GPU for me it always takes time to find where exactly in the Edit this GPU are yeah after the editing I will scroll this because it's not too fun you can actually start it again after the editing and the start process actually takes also some time I have 10 10x here for starting because it's also take time and after your start that you have a GP so this video by itself is one and a half minutes in reality I actually timed it and I have it the next slide in reality takes 4 minutes approximately 4 minute to stop edit start now let's say you attach GPU you executing your training finally you see a back you see barking when you need to debug this bug while you debugging you don't want to pay for your GPU so we need to detach you need to prototype you need to attach back and start the training so effectively one buck requires two more edits which is plus eight minutes so you can see it's actually waste of time especially if you have different box you need to iterate I mean it's good if we if the company pays for your resources to don't care you can just attach a GPU and forget about it but if you actually paint from your pocket this might be a nightmare now what would be really nice to have let's assume that you prototyping or you deploy um that is laptop with the user behind this laptop I don't know let's call him Bob so the Bob prototyping country's laptop using deep learning VM under the hood would be nice when needed when you actually want to train to spin up another something another instance like deep learning VM that actually has GPUs that small tiny icons that looks like exactly like a DM icons it's official icon of GPU yeah and it looks exactly the same as icon for 40 VM itself but trust me GP ok so now you want to spin up a GPU machine execute the training there and then give give result back and destroyed that back-end machine that look really nice you you paint only for train a but in order to do so because that second entity if even if it's deep learning VM it doesn't have any knowledge about your environment your environment that you have used for prototyping might be completely different you might have done something something strange there in order for this unit of work you need the notebook that actually self-contained and even better if you if you have a self-contained notebook it might work through the control source controls source repository system like it you can push it and then deploy Nick VM can pull it to build it test it and destroy it so let me actually show how it works already again you can find the tooling for this on the same wrapper so let's say I have some some notebook I'm going to actually download notebook with our git integration from from just github just the random tensorflow notebooks I spent some time trying to find here a meanest amidst example let me scroll this because I wasn't sure which folder it is yeah so here is a nice notebook that that I finally finally have found and I want to take this they want to train this with GPU my local machine is CPU and I want to send it on the background training while I can continue time in order to do so I'm going to install the second script from the same repository it's again we'll do a special magic yeah we're going to the same repo it showed the showed before going to planet I mean the qualms not going to folder responsible for self contained non books and there is a script enable notebook submission again as of now it's working on your deep learning VM not on all the Jupiter instances but this is yeah this can be changed now now let's go to the folder that contains my notebook that I actually want to execute I know the folder I just copy it and if everything is done right you now have two CLI commands that are available to your just to your to your usage let me show you that the comments yeah these are the two commands execute notebooks with CPU or with GPU there are four input arguments to this comment one is name of your notebook another one is Google Cloud storage temporary bucket and for CPU data zero that is all that is all you need to provide for GPU comments you also need to specify type of GPU and amount of GB in my case as I said I want to train this notebook and in the background with GPU so I'm using execute notebook with GPU then as I just mentioned they provided name of the notebook then I have some pre-populated Google Cloud storage you need to be sure that your VM has access to that storage this is storage V 101 so I do want to train with one v 100 that is effectively what is happening right now it's uploading everything to GCS starting p.m. starting the training this is again fast reboot each 10 seconds I'll show you there is a small error in there and just because script as of today just checking if this background VM actually exists and the error means that it's no longer exist mean it means that trainings have succeeded and the VM has self-terminate itself and the result being applauded to GCS as of today alt notebook has named surprise surprise notebook dot I PI and D it's always notebook I pine B and this is a notebook you can see on the Left panel that it's created a second ago because it was downloaded from our background background VM and it's fully populated so all the cells are there is actually have executed everything and this is effectively one the order of the recognition and the training actually happened to live a hundred on the background so you're going to pay only for a time of women hundred that you have used for training if something has crashed you will see crash here you can restart your notebook here with CPU you can prototype again when you're ready you can go to your console you can submit your notebook to the background train but as I said it should be self-contained okay so we have discard this and just to to show on the on the diagram what has happened we have one VM we effectively applauded our notebook to cloud storage we have spin up deploy nerium everything script is doing for you that deep learning vamp has downloaded your script then we're using technology that has named paper mill that was developed by Netflix paper mill is basically special open-source tool that is in charge of executing notebooks locally without any any UI interface attached and as soon as these succeeded we've done the hooks little through cloud storage back and destroyed the backend bail that is pretty simple a pretty important work now we always were saying that you need self-contained notebooks let's speak about what exactly this means what self-contained means several things same number one you need to have notion about which deep learning VM like 10 0 M 20 you have used for creating notebook our deep learning VM has already tons of different different packages very likely you will not need to install anything else you can use CENTAC a non pipeline that started learn tons of them already there but they have different so you need to make sure that the notebook has information that it was created with M 19 M 18 in this way we can spin up exactly the one declaring me M that has exactly the same set of packages and the version this is number one number two is simple if you do need any dependency please use magical cell inside of the notebook that install them that's it I mean I don't understand that there are some cases when it's actually not practical there are tons of dependency and magic cell could be a huge in this case we do have several other ways but in many cases you would be surprised deep learning VM already have majority majority of the things you need you can just install two packages I don't know maybe you need MX net for some reason you can install it already just use the magic so that is it it's actually pretty simple now limitations current limitations of this particular script the solution itself can be extend all these limitations can be solved but current limitations becoming home if you're going to actually use the street with a do planning M there are some of current limitations and actually many other right now only works with telephone vm's we do have PI torch memes but this this is not going to work yet only works on CLI I personally would be would be thrilled to see a nice button you pressing the button it magically got there and then magically come back but for now it's on the CLI third one always uses latest answer flow yes I know I have told you like moment ago you need to know which particular version but for now it's ignore it it always will use terms of call a test even if it were created not with the latest for now it's not capable of catching files around the notebook again it's not a problem you can upload your local folder to Google Cloud storage and downloaded and for the backend VM it's just current implementation is not capable using files and artifacts lying around not yet possible to pass any input arguments and this is actually very important but here is a link to github please do feel free to contribute fix like pick any of these and solve it if you you happens if one of them actually creates more pain than another for you now next item that we will just going to cover so so without diving too much its input argument and why this is important and this is almost almost last of my slides this is example from one of previous notebooks we actually don't care what notebooks is doing but it has number of steps number of steps is something that reasonably you want to change you might want to test with much different batch sizes different number of steps different learning rates it's something that you want dynamically to adjust when I say that notebook should have ability to process input I mean that you should be able to override these variables from outside of the notebooks without changing not this is powerful concept that again paper mill and Netflix has implemented already that script that I that I have used doesn't have the support but with this particular ability to write these variables you can actually implement through continuous integration system imagine notebook but when you sending this notebook to the get continuous integration system pulls this notebook oval writes variables for example it's a writes name of the table and instead of the production table it's using table with mock small amount of data and executing everything with that small amount of data just to verify that the notebook is working completely working and if something is broken its pages whoever is on call or whoever's charge of the build oh this is possible if you do integrate this capability of input arguments so this is effectively covers the last two and why it's important to have a deal easy to pass input argument to notebooks and this guy effectively if you're solving number four number five with technicality this is actually wall so this is five elements or if we're counting zero six elements we have not covered the last one as I said this part the otoscope just to recap just to mention this is more an item that you should aim for github has script that will help you already to import them but don't think about that it hop is a production of solution that you can just you there will be a lot of problem but it will give you some flavor of how future the notebooks might look like and that is all thank you very much guys let's move to a question question QA create part if there is any question thank you oh we have a question let can can someone help with passing the mic so I guess yeah it could be a silly question but can you compose notebooks can you include one notebook in another and then run run it capability okay yeah just just want to mention there is no like silly question there's only silly answers but yes so question was if I can use one book and another one somehow yes okay I mean you cannot with vanilla notebook you do need to install several plugins of paper meal or something that allows this capability but there are several solution on the market already that effectively allows you to say something I need variable a from the notebook blah and it will execute notebook blah until a is populated and then will import a so that is already possible yeah that is actually there are even though technically solvable it's open question about best practices whether this is recommended way or not my personal take on this that if you using generalized logic it's better generic logic to be just a pure Python if you want with just imports but technically is used to can import one or another thank you any other questions yep can you please close the mic yes sorry because with microphone you guys will be recorded so you are talking about tests so you so you are saying that if we can input parameters like we can just change the table name or some part of the table name and then we can read like Mach table names and so it's your vision that there's those table the test tables yeah will be sort of automatically generated or you are envisioning something that you have to create a table and then the test case is you for the rows that you want to test you how to write it because that's pretty painful and if you have a lot if you have two tables you just can create the more table table otherwise it will be kind of hard to create tables like with all the edge cases what's your original testing because in general testing programs were used to it yep bar testing that process data the logic the data logic we are not used to it in the data industry right kind of yes so how do you ambition that so okay your questions actually are absolutely right if you're going to manually mock the database it might be kind of painful so there are two things here first of all for notebooks usually the simple space let me first the say one more thing so you have usually most popular you need tests and functionalists right for notebooks is harder to do unit tests simpler to the function test when I ran the whole notebook now in this particular example that I show to you just writing input variable will force you to create yourself manually a table which is which is horrible but this is just the first step the second step is you can can have a normal Python test that will generate data then it that will call training on that data and will and will check the result I have not showcase here the actual CI but CI that we that we have for demo actually doing like this so because it's it's not just deployed deploying background training kits for generated data deploys background training and checks that the artifact that notebooks submitting actually has a results from that thing just to make sure that that is working what is we right now definitely missing is ability to go even deeper and do for instance unit tests like particular style that you can start that part correct yes that part will be harder but but yes it's possible any other questions so you the winning of the talk you were mentioning date Allah but you didn't show it oh yeah I asked if someone knows build-up so effectively yeah indeed I have to show it and one of the reasons because you can see you can think about depending a.m. solution is data latitude okay so it's that's all right there somewhere deep learning vm's know they they allow me to know the solution deep learning is is by itself a data lab with you so that's reincarnation it's not yet as parity in the features that's why you will not find this statement yet that's a question okay good but at some point it would have parity of the features plus other features great thank you anything else okay then thank you guys have a nice evening [Applause] [Music] 