 Hello everyone Thanks for joining IEI Mustang AI accelerator cards 2020 Embedded World presentation My name is Malcolm responsible for AI product line In this video you will learn what IEI can offer to your future AI deep learning task AI application are growing fast from smart healthcare and retail to smart factory Almost every industry implement AI into their applications nowadays The reason is the storage capacity and the memory speed got great improvement breaking through deep learning algorithm optimized rapidly And the most important is the AI accelerator which can help edge system to reduce CPU loading lower system total power consumption There are so many AI accelerators in the market But why IEI choose to work tightening with Intel solution That's because Intel solutions have many advantages First of all the cross platform Intel's accelerators support both Linux and Windows OS it's easy to be implemented in the user's existing projects And secondly heterogeneous integrate all Intel's accelerators from CPU integrate GPU, VPU to FPGA The third one is fast integration OpenVINO is a free inference SDK provided by Intel It's easy to install and convert a pre-trained model to Intel platform Compact size and power efficiency Small size and scalable FPGA it's only 45-watt power consumption and VPU by eight PCIe card is only 25-watt power consumption so for Intel solution you're gonna have better performance per dollar per watt and you must have those questions when you get started your AI applications Let's say if you not sure the support frameworks and the topology of the AI accelerator and also you may not know the performance and the performance may not meet your expectation when you finish your project and your system got space limitation or you cannot find the right form factor and also you don't know where can get to ask some experts when you meet problems So there are many deep learning frameworks and users may have different preference Let's say some prefer Tensorflow others prefer Caffe the OpenVINO supports Caffe, MXNet, Tensorflow and Kaldi and if users prefer other frameworks it can also convert their model to ONNX to run on the OpenVINO toolkit and the OpenVINO toolkit supports most of the popular topologies such as ResNet, YOLO, MobileNet and Faster RCNN and for more supported topologies you may refer to Intel official website. and you can visit Intel official website to understand the performance of different accelerators in different popular topologies which can help user to know what type of the accelerator and what quantity of the accelerator can fulfill their future applications So in IEI Mustang series accelerators the card size is half-height, half-length which is much smaller than the general-purpose GPU and that's the reason why it is the ideal device for the AI inference system Power consumption is also a key feature for AI inference systems Mustang accelerators' power consumption is from 5-watt to 45-watt and depends on different form factor However, it's still much lower than the general-purpose GPU so Intel official website has many demo programs and pre-trained AI models for you to reference so the demos include python and C++ those are very popular programming languages for the deep learning users. and If you want to learn more about OpenVINO you can subscribe Intel OpenVINO on the YouTube channel to get step-by-step tutorial and if you meet questions during your project there's also a official forums for you to file a question This page is the IEI Mustang accelerators series The silver one is FPGA accelerator and the black series are VPU accelerators This is the FPGA accelerator Mustang-F100-A10 F stands for FPGA A10 stands for Intel Altera Arria10 FPGA The features include compact size supporting multiple cards and low power consumption as we mention it's only 45-watt typically and the low latency so the greatest feature of FPGA is high flexibility so we can upload different optimized bitstreams for different topologies for your different AI tasks The OpenVINO toolkit will be released every quarter so unlike typical ASIC you can upload the latest or optimized topology periodically to get the best performance And these are the VPU accelerators Mustang-V100-MX8 and MX4 V stands for VPU MX stands for Intel Movidius MyriadX VPU MX8 means there are 8 Myriad X VPUs in this PCIe card and the features include compact size and supporting multiple cards and extremely low power consumption For the MX8 the power consumption is only 25 watts for MX4 is only 50 watts So by this block diagram you can see there are eight myriad X VPUs inside this PCIe card so you can assign different AI tasks to each VPU to achieve multitask and distribute computing For example you can decide VPU NO.1 to NO.8 to do different AI tasks such as face detection face recognition object detection or even a license-plate recognition at the same time So sometimes your system may not insert PCIe accelerator cards you can have other options such as mini PCIe and M.2 Mustang mini cards to execute AI inference in your compact edge device Age-related Macular Degeneration (ArMD) is an eye disease which happens in senior citizens For medical AI application IEI group and QNAP co-work with Taiwan local hospital to train ArMD model to assist Medical professionals to pre-check the patient's OCT image in different symptoms In this slide the architecture illustrates how IEI cooperates with Intel and Microsoft to complete a total AI ecosystem In the right hand edge side IEI inference scripts AI model which was trained by QNAP it's executed by Microsoft ONNX runtime which has an OpenVINO EP to drive Intel device such as CPU integrated GPU Mustang-F100 FPGA card or Mustang-V100 VPU card to execute AI inference From the left-hand cloud Azure side Azure can collect edge inference data from Azure IoT hub With collected images and inference data model can be retrained by this ecosystem This is the demo video to assist Medical professionals to pre-check the patient's OCT image in different symptoms By the ecosystem co-worked by IEI Intel and Microsoft In this smart city case for this application it's using Geovision’s object detection topology and combines with IEI Mustang-V100 Movidius VPU accelerator card to achieve different smart city task for the left top corner channel it's doing the pedestrian crosswalk violation detection For the right top corner channel it's doing the bus stop restriction area parking violation detection For the left bottom channel it's doing vehicle counting for traffic condition monitoring it can send alarm to local law enforcement if too many traffic conditions and for the right bottom corner side it's doing the license-plate recognition for the parking lot So for the license-plate recognition case this application is using AlphaInfo’s LPR so-called license plate recognition topology to detect a license plate which we cooperate with a local law enforcement in Taiwan and you can check suspicious vehicle and send alarm to the local law enforcement and this application can offload officer’s workload avoid the conditions caused by human eyes fatigue and misjudgments and AlphaInfo also implements Mustang-VPU accelerators into their end customer's parking lot system to recognize license plate with very high accuracy In this driver behavior monitoring case the application is using Intel Open Model Zoo to demo the topology with Mustang mini PCIe cards to achieve two applications For the right-hand side screen it's driver fatigue detection actually there are two workloads including head pose which can detect drivers head position in case the driver doesn't look forward and another workload is face grid it can detect driver's eyes blink frequency in case the driver falls asleep and for the left hand side screen it's doing road condition detection it can detect such as vehicles scooters pedestrians to assist driver to know the road condition and this demo is using IEI small form factor IPC named ITG-100AI which is a fanless system embedded Myriad X mini PCIe card inside so you can achieve operating temperature between minus 20 to 50 Celsius so it's an ideal edge application AI device So this is the face recognition case in this demonstration you can see the facial recognition, gender, age and emotion detection by the CyberLink’s FaceMe® software which is a very high ranking and the most accurate AI facial recognition engines in the world so recently IEI cooperates with CyberLink’s FaceMe® by leveraging the computation capability of IEI Mustang-VPU accelerators so you can speed up 20 times of performance than the Celeron CPU Okay so that's today's video hope you can have more idea about IEI AI accelerators for your future AI applications if you have more questions please visit our website don't forget to press like in our social media such as Twitter, Facebook and LinkedIn and please subscribe our YouTube channel to get our latest product information thank you for watching bye bye 