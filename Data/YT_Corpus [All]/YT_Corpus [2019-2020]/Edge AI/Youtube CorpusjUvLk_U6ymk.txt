 This presentation is  the ‘Full use of Rust on Edge and Cloud: AI and IoT Use  Cases’ by tkato. Hello. I’m Tomohiro KATO. I work online with the ID, tkato. I’m a software engineer at the Japanese Company, Mobility  Technologies. At my job I use Rust to develop a machine learning system with an Edge device. I hope you enjoy this talk. In this presentation, I will go through product development examples and introduce design techniques developed with Rust on an Edge device or Cloud. This is my agenda. I’ll first introduce Drive Chart, our product, and then developing Rust on Edge and Cloud. First is an introduction to our product. We are developing a product called Drive Chart. AI Drive Chart is a service to help reduce traffic accidents using an AI for taxis and commercial vehicles. It uses the drive recorder and Cloud to move the AI algorithm in relation to camera image and sensor data and creates a safe driving report. I’ll introduce the part of Drive Chart developed with Rust. This is computer vision processing with production on Edge and an evaluation simulation with development on Cloud. We implement a computer vision algorithm with real-time deep learning on the drive recorder using Edge. We slightly alter the Edge implementation on Cloud and operate it on Kubernetes. Next, I’ll explain how this development progressed by  adopting Rust. We initially launched using C++. We considered switching to Rust due to the development time and quality  issues. However, I’m not saying that C++ has issues. Just in our case with limited time and manpower to develop Rust was more suitable. The first issue is the build system and test framework. These aren’t built into C++, so we had to select our own technology and write most of the script for it. We wanted to simplify the non-code script to focus on substantial system design and coding. Rust prepares the build, test, and documentation from the project template via Cargo. That solved this issue. The second is safety. The drive recorder requires high-quality software so it won’t crash even if there are issues during operation. Rust mostly removed the the memory operation bugs that we had during the compilation time. Since we switched to Rust, no bugs causing it to crash have been imbedded even during its development. Even Rust can’t prevent the logic bug, so be careful. Because it had these advantages and we expected the same execution speed and memory as on C++ we actively switched to Rust. In about two weeks, we created a prototype to make this  switch. The core was essentially the same as on C++. We implemented the application on Rust and checked the drive recorder’s operational performance. We didn’t rewrite everything all at once. We slowly switched what worked from C++ to Rust. Only then did we decide to develop a product on Rust. Afterward, we progressed while improving our knowledge of Rust and launched without any issues. Part of this is still on C++, but it was mostly implemented  on Rust. Next, I’ll introduce the development flow. We develop the AI algorithm on Python, reimplement it on Rust, and operate it in the production environment. Finally, an AI function was supplied as an AI library and incorporated into the drive recorder application. This is an example of how we add a new function. First, we do R&D on Python and verify the algorithm. This is because Python is the main ecosystem to develop deep learning. Next, we verify this while integrating the Python module and  Edge AI library. Once the deep learning module is finished, we can verify it by adding it on the drive recorder’s production pipeline. In this development on Cloud, we use massive test data including videos and sensor data. We evaluate the overall algorithm and machine learning  system. Next, we reimplement and verify Python on the AI library  with Rust. Finally, we incorporate it into the drive recorder, repeatedly verify the recording, and go live to release it. We can efficiently develop new functions by seamlessly linking R&D with production. We brainstorm software designs to facilitate this. Let’s get into the design and implementation. First, I’ll explain how to handle deep learning and the computer vision processing on the drive recorder with Rust. We write using a standard library on Rust and an image  processing crate. We wrap the C++ library with Rust as needed for performance. We use our successful C++ library for deep learning  processing, but I won’t go into too much detail. We create our own API on C for the C++ library and wrap it with Bindgen. But we use CPP when we want to quickly call C++ from Rust. For reference, we shared the information on the Rust machine  learning ecosystem. There is already Rust binding for major frameworks like TensorFlow or Pitorch. Also, this runs on the hardware of the common Rasberry Pi and Smartphone, so TensorFlow and TVM can be used on Rust. In addition to this, I recommend binding the C or C++ library on Rust like we did. Let’s go onto the unit test. Rust is particularly reliant on third-party crates, and they sometimes begin to degrade because of this. I’ll introduce several measures to prevent this. Firstly, I recommend a snapshot testing method using a crate called Intsa. This crate stores the execution results of the test and can easily compare results while running the test. We use this to check that the system detection for videos is consistent. The automatic test runs on GitHub Actions and includes snapshot testing. This is useful, because there are many useable commands on GitHub Actions in Actions RS. The Dependency version recorded on Cargo.toml goes up to the patched version. The GitHub dependabot will take care of updates. The dependabot creates a pull request for any crate updates, automatically test it, and verify if it’s degraded. This makes operation a breeze. Let’s continue onto development on Cloud. We are constructing a simulator to evaluate Edge  implementation on Cloud. As shown in the diagram, we join multiple crates in the application on Edge as a monolithic library. On Cloud, we deploy crates as containers and structure it as a distribution system. There are two reasons for this. The first is it accelerates building. It splits containers to hasten updates and deployment per function with differing development cycles. The second is it integrates R&D and production. In one use case, we wanted to evaluate the deep learning  module within the current pipeline with Python implementation  during R&D. In brief, we selected an architecture to create a mixed system with Rust and Python. You can consider other methods to wrap Rust as a Python library and structure the overall system on  Python. We selected this structure for its loosely-coupled, simple  structure. We collaborate on shared code for Edge and Cloud within the  crate and give it a design with hexagonal architecture to continue developing it on Edge and Cloud. Edge and Cloud have a common core logic. But the implementation selection depends on the application entry point, the hardware, and the use case. For example, during Edge production, we incorporate Rust implementation in the application as C API. When we set a device benchmark from a PC or call from other applications on Kubernetes, we use the gRPC or CLI interface. The inference function allows us to select implementation to call the R&D Python model with gRPC, a deep learning framework for the device, and a mock test framework for X86. This seamlessly links from the R&D on Python to the production development on Rust. Switching over functions is implemented with Rust traits as needed for each use case. This slide shows implementation examples. The code is abridged but sufficient to get the gist. This allows the module inference to be switched to three different types: gRPC, Edge, or mock. First, it implements the trait module service. Consider the input image and inference results for the  execute method. Next, we implement each trait. The gPRC module service uses the gRPC client, like Tonic to access the external API. The code for the Edge module is abridged. This is implemented through a deep learning framework for Edge. The mock module service is a unit test implementation that responds when there’s an instance. This is how you create multiple implementations from one  trait. I’ll explain the part of the code used in the application. This helps to switch the implementation when starting the  application. This uses the trait object to reset different  implementations based on the given module type variation. You just input Model.Execute to call it. This abstracts the implementation and lets it switch  depending on the use case. We used these techniques to facilitate an improved  testability and integration with systems other than Rust. Our final technical topic is logging. We often use log to debug and profile at the application level. Naturally, we use a benchmark like Cargo bench during development at the crate level. We often use JSON structure logging, then parse and visualize it with a tool like Chrome tracing. This lets you see processing with multiple processes and  threads. This is a reasonable method to get an initial indication of bottlenecks. The log base doesn’t rely on language as it’s on a system constructed with multiple languages, like Rust and Python. You use a s-log crate on Rust to do structured logging. You can output a JSON log with a macro, like in the  diagram. This shows each inference begin and end event in the log. Let’s discuss system changes within half a year of  development. In a few words, the system changed from a monolithic crate to a micro crate. In our initial development with Rust, we often stuffed multiple functions into a crate, but overtime split those  up. There are several reasons for this. First is the build time. Differentiating crates accelerates  this since we don’t build all functions at once during  development. One advantage is how easily it combines with other languages once you can make a container per function and crate. In our use case, we wanted to make a system that mixed Python and Rust. One benefit of employing a microservice is how flexibly it selects technology per function. I recommend the option that doesn’t force you to use Rust for several lines written on a shell script. Our goal for this microservice was to improve maintenance. Currently our team has 4 people writing Rust, and is a small part of the overall company. We value making it simple so that any new participating members can quickly catch up. Lastly, I’ll give my impression of using Rust in product  development for half a year. I felt Rust was a good choice, because its team development is good and production would develop quickly. Since I proposed switching to Rust, the team has grown to its current size of 4 people. Rust was the first programming language any of us  experienced. Familiarity is the key. We got used to Rust through the compile error messages and the Clippy messages. Earlier I spoke about system transitions. We were able to work on stress-free incremental development because even when we conducted major refactoring to differentiate crates, we could check for slipups while  compiling. I spoke about our initial motivation to use Rust. I think it has an enhanced ecosystem. In product development, our team focused on the design and coding without overwhelming the repository with script on build and test documentation. Another positive is that the role of our team expanded. Rust can be developed on a variety of platforms and domains. We mainly worked on the development on devices. But we could sufficiently develop on Cloud too. To conclude Today, I went through product development examples and how we used Rust on Edge and Cloud. We devised a software architecture on Edge and Cloud to integrate R&D on Python to the production on Rust. This approach and implementation were based on our use case and may not be applicable to everyone. Still I’m please so many joined this session. Let’s all try out product development on Rust. That’s it. Thank you for listening. 