 In this special Build edition of the AI Show, we hear from Phani Mutyala, Senior Program Manager on the Azure Cognitive Services team. Phani will explore Cognitive Services containers and how we can get started. Make sure you tune in. [MUSIC]  So welcome to the Microsoft Build. Today, we're going to talk about the Cognitive Services containers. I'm Phani Mutyala. I'm working as a Senior Program Manager as part of Azure Cognitive Services. Primary objectives of this session are: we're going to talk a bit of Azure Cognitive Services, and we'll do a deep dive on Cognitive Services containers, in other words, Cognitive Services at the Edge, and we'll have some session takeaways, which I'm going to share some information on how you can get started using the containers. So with Cognitive Services, our investment principles, we really think about the developers. We are always thinking about how we can make any developers productively working with Cognitive Services. From that perspective of APIs, we always think about, they should be flexible and available for any developer that comes with any skill set. These Cognitive Services APIs are built and meant for enterprises, and they're optimized to work for any skill that our customers are coming with, and they're trusted, which means customers can own and control your data. So with Cognitive Services, we have four product families: Vision, which is packaged with face detection, face recognition, and computer vision, and custom vision; Speech, having speech-to-text and text-to-speech and speaker recognition, and also we have Language, which is having the text analytics and language understanding and text analytics with the key phrase sentiment analysis and also the language detection. With the Decision, we have a widely-recognized Anomaly Detector service also available. Our motive is to bring more flexibility for our customers to deploy Cognitive Services in their own terms, meaning the customers have a need to talk to the business applications, has in need with an AI model. We have the APIs that are running on the Cloud where our customers want to take the AI close to the data. For example, in few cases, customers cannot talk to the Cloud. So in those cases, what we are doing is we are making the Cognitive Services available for our customers so that they can take it and run anywhere in their own terms. So it's the same set of API models, it's the same set of powerful APIs we are running on the Cloud, at the same time, we are making it available for our customers to run them at the Edge. Why we are thinking about bringing Cognitive Services for the customers to run at the Edge? Let's talk about few enterprise pain points. Some of the customers, here's some of the use cases that we learned, the pain points are some of the customer segments. They are unable to upload all of the data to the Cloud. In some cases, they have a very strong security and privacy requirements where they cannot put their content on the Cloud, or where they cannot talk to the Cloud always. In some cases, customers are dealing with low bandwidth and intermittent connection. By providing the APIs in form of Docker containers, the benefits that customers receive are the packaged APIs in the Docker images, they can take them and run it anywhere. With that, they can have full control over the data. Also, with the containers running close to the data, they can meet all the regulatory requirements as demanded, or met with their organizational standards. It's also great for high throughput, faster, or the high bandwidth operations. Let's look at some of the Edge scenarios. Today, we have tens and hundreds of customers already using containers for some of the user scenarios that are related to the robotic process automation, and the combination of language understanding and speech-to-text and text-to-speech. They're running containers for intelligent customer support, and we have our customers using document understanding such as read for text extraction from the forms, and we have customers running text sentiment analysis for their documents. Also, Face API is one of the widely recognized service from Cognitive Services, and Face is available containers, our customers are using for face detection and face verification for some of their Edge use cases. Also, with the combination of language understanding and some of the Text Analytics containers customers also deploy multiple solutions, the bots, and also their intelligent agents. Today, we'll view, these are our top partners and the customers already leveraging containers for their Edge users scenarios. So how we do it? Today, containers, this is not separated from existing Cognitive Services. Containers bring flexibility, meaning they're an extended feature of the existing Cognitive Services. We give flexibility for our customers as I mentioned before, if customers want to talk to the Cloud, we have the web APIs, or they want to take the AI for the Edge use cases, we have the container. So they're just an extended feature from the existing service and nothing new. So we are taking the same model code and routing up using our service code, and we're building a Docker image. We make it available for our customers to deploy either on a data center, or they can even use it through the widely-recognized orchestrators like Kubernetes or the OpenShift, and also they could even take it to any of the Edge devices which can be managed by Azure IoT, or their own IoT. Let's talk about some customer stories. This is a strategic Microsoft partner and the systems integrator that does tons of work in the legal space. They're really unique ISV that has a customer relationships with a majority of the biggest law firms in the US and also in the UK. Because they are the legal industry, most of the data must stay local. They have a product called Proforma Tracker that the law firms use to recognize the text, optimize their legal forms, and process them from billing. So picture-wise, Wilson Allen is using the container to bring form-based data into the firm data ecosystem. Also, they're using it to create the metadata from the text-based data, establish the patterns, and render it in a Common Data Model, with all the traditional data warehouse visualized through the Power BI reporting. So using the Form Recognizer container, the extract that historic set of the HR PDF forms, such as an intake, change, or any other forms, and ingest the output resource into database for master data and analytics purpose. Insight, a Microsoft partner, it's a global payments technology company working to enable the consumers, businesses or the banks, and governments to use the digital currency. So they built an exciting self-service kiosk offering the robust user experience in an efficient and secure environment. So the solutions hosted with Azure IoT Edge, Azure Cognitive Services, with the mix of face recognition, speech-to-text and language understanding container along with SQL Server as a container. So providing a full customer experience, this solution is integrated with customers digital issuance technology, so that the customers can start using their credit cards without any [inaudible] times . Also, the current solution implements some of the interactions, like requesting a new credit card, or requesting to meet banking manager, or request to pay a bill. So this helps reducing the wait times for customers, and ultimately bring the customer satisfaction. I have our customer already on the call. So Vaibhav, is a member for data science team at TIBCO, Vaibhav?  Thank you, Phani. I'm just going to go over who we are. TIBCO really makes it possible to unlock the potential of real-time data for making faster and smarter decisions, and we do this with our TIBCO Connected Intelligence platform, which basically consists of three pillars: That's connect, we connect to anything and any source; Unify, we unify your data coming from any source, and Predict, and push these predict back into your systems. For today's talk, we'll be focusing on our Predict pillar, which is the strength in our combination of Visual Analytics backed by data science at scale, deploying to any operational system. One of the areas where we had a lot of success in is anomaly detection across all manufacturing, energy, equipment surveillance, financial fraud, risks, and healthcare. So I would recommend everyone to check out the URL to learn more about anomaly detection solution provided by TIBCO. On to next slide, please. So here, we're going to talk a little bit about what a business problem is, which is here to detect anomalies in power and manufacturing plant equipment. Once we have these anomalies, we really want to identify the key driving factors to determine the root cause of the anomalies. From this point on, we really want to dig a level deeper to analyze the maintenance log files to finally generate a recommended action. So some of the services that we're utilizing from Microsoft include Microsoft Azure Anomaly Detection Service, which you'll be using our typical data science platform. Then we'll also be utilizing the containerized Cognitive Services for anomaly detection and Key Phrase Extraction at the local, or Edge scenarios for a specific site location. So what makes this solution unique is actually leveraging the services from Microsoft and TIBCO together. We're really using two platforms here. The first one is the TIBCO Data Science, which will be using the Azure ML services. TIBCO Data Science integrates at-scale data science operations with Microsoft Azure ML services. It also enables data science backed visual analysis using second platform which is TIBCO Spotfire. The second platform which we'll be making use of would be TIBCO Spotfire for the equipment maintenance dashboard. So TIBCO Spotfire, basically, we add the root cause analysis for Anomaly Detection Containerized Service, and then we recommend the action from TIBCO Analytics with input from Microsoft Key Phrase Extraction Containerized Service. Onto the next slide. So here, I'm going to walk you over the Reference Architecture. So the solution is really developed in two phases. We have a company who has power plants scattered across the country and has its headquarters based off a city, where the central data science team sits. So the analyst at the headquarter uses the TIBCO Data Science platform to detect anomalies on historical data from all sites. Next please. So the visual-backed data science at-scale workflow produces infrastructure which serves as an artifact for the maintenance engineer to carry along to the remote site. The containerized services are called in TIBCO Spotfire the Python Data function feature and transformed into reusable artifact. Next. So once the maintenance engineer is at the site, the maintenance engineer detects anomalies using the containerized cognitive services from Microsoft and leverages decision intelligence embedded into TIBCO Spotfire to perform root cause analysis. So once we have the anomalies detected, we identify the key drivers in a specific time Window as a part of the root cause analysis. Next. From here on, we dig a level deeper and the results of the key phrase extraction containerized services, we use that to determine recommended action through further processing of the key phrases. We can think of this as the root cause analysis which is provided by the TIBCO analytics stack sitting on the top of the containerized services provided by Microsoft. We can move on. Next, please. I'm going to show a live demo of our solution. So here we see the TIBCO Data Science platform which is the visual data science back analysis at-scale, and I'm going to walk you over the first phase of our solution which is building and inferring the data at headquarters. We can see here we have the power plant data and we are interested in finding anomalies in production per minute. We go through a series of data pre-processing steps which includes the transformation of the time stamp, filtering and finally, calling the anomaly detection custom operator, which is built in the TIBCO Data Science platform. Once we've detected the anomalies for all the sites, this data is then exported to a Spotfire binary data format which enables faster loading in the TIBCO Spotfire platform. At this point, we move over to the second phase where the maintenance engineer goes to the remote location. So once the maintenance engineer is at the remote location, the first step is actually to perform anomaly detection at the Edge. We're interested in learning more about anomalies in production per minute. So this is the equipment maintenance dashboard. As you can see here on the top left, you can select the feature that you want to detect anomalies on which is production per minute here. The granularity level is by minute, and we can go ahead and detect the anomalies. So on the bottom, we see the difference which is nothing but the difference between the original values that are supplied through the data and the expected values that are given by the container. So on the bottom, we can see that the red markers are the anomalies that are detected by the anomaly detection container. At this point, really, the site engineer would want to learn more about certain anomalies and certain time windows. Let's say the site engineer is interested in learning more about the nature of anomalies in this window. So we select this window of interest, and we hit the "Root Cause Analysis." Which bring us to the next phase, the Root Cause Analysis." On the bottom left, we can see the key driving factors for anomalies in production per minute for that specific time window and from this, we can see that the waste gas, power pressure, and loss efficiency are some of the key driving factors. At this point, the site engineer would really want to dig a little bit deeper and understand what were some of the maintenance actions performed prior to these anomalies. So we can go ahead and actually call the Key Phrase Extraction Container which basically generates the key phrases and from here on, the decision-backed intelligence in Spotfire generates a recommended action. So we can see here on the top right is the chart of the production per minute within that specific time window and at this point, really, we can see here the key phrases that are generated by Microsoft Key Phrase Extraction Container service and from here, we can really see that based on further free processing of these key phrases, we have a generated recommendation which is to change the faulty baro pressure meter reading. We also see that this sensor was restarted five times, whereas it was only recommended for replacement once. Looking below, we can also see the site engineer might be interested in just knowing the distribution of these key driving factors within that specific time window. So we can toggle between that. If you want to know how the waste gas measurements were looking like, we can have a look at that. Same applies for baro pressure reading and loss of efficiency. One thing to note here is that we can see that one of the complaints that were logged were for a faulty baro pressure meter reading, which was a high baro pressure meter reading. So if you click in that and look at the chart below, you can see that there is a high reading from the graph. That is it.  Cool. Thanks, Vaibhav. Just going back to the slides. So with that said, containers are so far in the public preview for the last 18 months. Starting May 19, containers are generally available to start with Sentiment Analysis 3.0 Container and Language Understanding. Remaining other containers are currently in public preview; continued to be in the preview for the next few more weeks and we definitely keep our announcements posted when these containers are ready to go GA. But from May 19th, it's just only Sentiment Analysis 3.0 from text analytics and Language Understanding. The rest free tiers are available for developers to get started with the containers and there are some resources, feel free to take a picture of these slides. We have developer samples, and we have great documentation, how to work with each container and also, if you have any questions or any comments or any help needed, you can always reach out to the e-mail given below. So with that, we hope you have a great understanding about Cognitive Services Product strategy. Definitely, we hope you have a good understanding about the Cognitive Services containers and how to get started working with the containers. Thank you and we hope you'll have a good day. [MUSIC] 