 and welcome to our webinar today we're going to be talking about graphs and how they continue to revolutionize prevention of financial crime and protein real time your host George and abilities from now connect the data London and we have the pleasure of having a few select guests with us today so some guests from taiga graph and some guests from opencorporates but before we get the specifics let's just start by giving you an outline of well what we're going to be talking about today so first we're going to start with an introduction to graph databases and you know where they come from and why they are useful and what they have to do with finances crime prevention and then we're going to be talking more specifically about customer use cases and case studies and the real-time aspect so this is quite important and then we're going to be into the second part of the web which is more about graph query languages and well we're actually going to make the connection to the first button then everything becomes more obvious but before we get to that part let's start by introducing well first myself then my select guest so as I said you may know him me from connected data London with some organizing I've been into graphs since well almost 15 years now in many capacities as a researcher in consultant and analyst and so on and I'm very glad to have Rebecca Lee today with us Rebecca is the chief data officer from opencorporates and Rebecca it's over to you thank you yes my name is Rebecca and I spent longer working with dates than I'd like to mention I started off building databases for and financial clients and the United States levels and before transferring over to Deloitte and PwC to investigate data winning companies themselves and that's for everyone's different reasons from financial crime to building great thanks Rebecca then moving on to our next guest Victor and Pauline from TigerGraph Thank You George this is Victor Lee I'm the product manager for Tiger graph I started out my career working as a hardware engineer doing silicon in Silicon Valley but I got interested in databases and when I went back to grad school then I found about found out about graphs and got really interested in those so I combined them and did graph data mining so I've worked for a little while as an academic but I've returned to industry now that graph databases are becoming a up-and-coming product so it's really exciting to be in this industry as it's growing and making really interesting contributions to what you can do with connected data last but not least professor alin deutsch thank you John George hi everyone yes I am a professor at the University of California San Diego specializing in research on data management database technologies and I've had a soft spot for graph technology ever since my first research days my thesis was actually on the design and optimization of languages for graph databases when this was just a toy academic exercise over 20 years ago in the meantime I'm very happy that the technology has so much industrial backing and this is so relevant today certainly does yes you did so that's a good bridge actually to get started so why graph? that's a common question I guess all of us get to hear from from people when we first started talking about this topic and you know again the common answer to that question is that graphs are everywhere the most common example we can probably think of is the social graph so which really helped a lot popularized the notion of graph and how everything is connected as we say so graphs are about relations and it's very very obvious in social media but that's obviously not the only place where graphs actually exist so another very obvious example would be Google which is actually the one who popularized the whole term knowledge graph so what you probably know PageRank is a graph algorithm and it's used by by Google search to to rank and to be able to achieve all those great things that Google does at sea but I would say that moving moving forward from the kind of obvious examples there's many many places in Greece graphs argue so product recommendations is another one that many people don't realize is powered by by graphs so we have again kind of obvious example in Amazon then there's also wish that you customer 360 vision graph so the bottom line here is that graphs really are everywhere naturally let's say it's a kind of natural way of modeling the world and as such that kind of found the way into databases as well so here we see like a kind of the evolution let's say of databases and the ones that most people are familiar with relational databases that pretty much everywhere and we know them and will to a certain extent we can say we love them they were but you know they're not the best mats for everything they they do have the downside so rigid skin eyes you know one something that characterizes relation or a sequel databases they're really good transactional application applications but they are not as good when having to do what we call deep analytics which we'll get to explain a bit more about later another kind of database that's been really popular with no sequel explosion these key value databases and these are at different different based let's see so the highly fluid they don't really have a schema are the really good at single transactions but again they kind of hit a wall when they have to do deep analytics and that brings us to the graph databases so again contrary to what many people may think graph databases are really not that new that they've been around for a while in one form or another for many years actually but what's really helped them take office well on the one hand Hardware evolution really so performance is much better these days and on the other hand well they're the kind of virtualization that we talked about so cookie graph databases can have flexible schema and they can do pretty much everything that the other databases can but while they were there in Excel is when do you have to do deep analytics and with that I'll hand over to volume again to talk a little bit about data path which is specific internation of graph database product and Alain over T I guess Island sounds maybe walking can you hear me now yes No thank you thank you Tiger grass was founded about five years ago by dr. yu Shu who received incidentally his PhD from the University of California San Diego I had the pleasure at the time to work with him mrs. research advisor our mission is to unleash the power of interconnected data for deeper insights and better outcomes and to do so we exploit the fact that graphs are naturally storage and computational model for interconnected data and by exploiting this insight we invented the first technology for massively parallel processing queries over graphs and we we are providing these days the only scalable and we're proud to report the fastest graph database for the enterprise we are based in Redwood City in the San Francisco Bay Area and we are working with a wonderful engineering team with experience from database and system companies like Oracle Google Microsoft sequel server Teradata and IBM most with PhDs and master degrees could I have the next slide please I guess where there are the customers one the taiga graph customers are using our technology to for competitive advantage to find deeper insights in their data and they come from a spectrum of applications being really the the top players in a payment credit card mobile e-commerce financial tech industry large pharma Healthcare even power grid companies okay thank you over to you yes Thank You Alan and so we're moving then forward to to Victor who can actually get to the bottom of how graph databases can power data science so poverty thank you so we've talked about how graphs are representing connected data so once you have the data it's it's a great way to represent the knowledge but once you have those individual facts about connected data and remember that in the financial world every transaction is a connection from one a from one account to another account all of that information you can then perform analytics so you know starting I've we've got seven different types I'll go through them quickly and then as we go through today we'll also talk about some of the applications of each of these so deep link analytics not looking at just who you are connected to but or or what is connected to what but looking through a long series of things to see is there any connection at all if so how close how broad are there multiple connections to get from A to B and sometimes that can signal things like do these parties actually have a hidden relationship which then becomes visible in the graph moving over to number two you then once you have this data you can look for similarity or frequent patterns so for example I have this this node of interest I know that it if I follow up to its parent of parent there's no day if I go down to its child of child there is a node B is there anyone else that's similarly situated so if I do my analysis I see that C and D are similarly situated is that a frequent pattern like so I can analyze what what is a common occurrence within the data knowing what commonly occurs is understanding trends and every business person knows trend analysis is important and three I can have a target pattern that I'm looking for and see does that occur in my graph because those little graph patterns they actually have meaning you can say you know I sent money to three other persons within a certain amount of time of sizeable amount does that occur anywhere in the graph for example for looking for communities and hubs of influence the the density of connections varies so what you end up with is you get natural communities there are algorithms that will help you find those algorithms you can also see running algorithms like PageRank who are the most influential where most important knows within a graph and combining those two both communities section influence even with the others the the paths and deep link analysis you can get a very rich understanding of your data both for general exploration and targeted analytics the last three are general capabilities I'll go those very quickly geospatial where are things happening temporal when are things happening and using all these building block analytical techniques you can then use them to improve the the training that is going on these days for machine learning and AI so and a great advantage of graph is that people intuitively understand relationships and so when you're AI model comes up with a result you can explain to yourself and to others why it came up with that result because you can see the connections next next slide please here's just an example looking at anti money laundering very very important thing in the financial industry obviously on the Left I show a money laundering ring where you're passing money around in a circle and what happens if you're just passing money around in a circle that doesn't seem to serve much real business interest what you're doing is laundering it on the right it's showing the technique which is called layering where you start with a lot of so-called dirty money which was gotten through illegal means and it's usually in cash and then so to disguise it you split it up and send it in smaller bits to several other parties and this goes through a couple stages you eventually merge it back again so having these the individual transactions look innocuous but when you gather them together and represent it in a graph you can see the the layering happening next slide so yes thank you thank you Victor for actually introducing us to the connection between graph databases and financial crime prevention and let's actually hear it from someone who is doing that in there every times of now so Rebecca over to you and you can tell us a bit about how that works in opencorporates thank you very much so we're talking today about so for any company and person to know who they are dealing with is critical and you have a net funded and criminal actors employ a large range of techniques to obscure both who they are and the beneficial ownership of both assets and transactions they use complex ownership and control structures across jurisdictions using the unrestricted use of legal persons as directors whether that's informal unverified or undisclosed or even trust and the use of professional means mediator many of these complex control and ownership structures are legitimate but we do identify them in in nearly all financial crimes would try and covered and these range from avoiding taxation and concealing weight wealth to Phoenix activity and of course we've already come of it Adri but why do I say it's going to be difficult today and tomorrow being even harder what we see is the use of corporations being created digitally so they exist briefly they just disappear both for legitimate and illegitimate reasons and these are obviously being registered and result by machines and this is going to lead to an exponential data form that we're going to have to deal with in terms of understand acute companies are and we'll need algorithmic ways to monitor both that activity and just what good and perfect companies so it's against this backdrop that open corporate was founded as a social enterprise enterprise we have a public benefit mission and are later has been involved in some really groundbreaking investigations things like the ICIJ Panama papers which more I'm sure you're all aware of and transparency international's London property markets around their money laundering and currently the twenty laundromat investigations next slide please though we describe ourselves as the largest open database of companies in the world we collect data from official registry sources and our mission is to map every single legal identity entity in the world and the public data that's relating to them so currently we have about 170 million companies and that's from about 130 different jurisdictions and those numbers are increasing as we add more data so in the past five months we valued Brazil, Cambodia, Iran, Germany and Belize but quizzically our data is global open and white box so global as we mentioned is critical today even as individuals we don't interact with companies just within our local UK companies web open this data is available to everyone including you by the websites we provide our API both to commercial companies as well as academics and public benefit missions and we also provide both data to organizations who use us as a foundational data source so in terms of kyc type applications or even to build it into external commercial platforms but I've used the term white box and I think most people understand the opposite terms so black box in this case organizations providing company data have of historically being black box so that data is is really well opaque in terms of its provenance its terms of its freshness it's generally not very well defined and because it's in-house to these individual business intelligence companies have called it quality feedback loops and most of them have proprietary numbers you've probably heard the dunk summer office on the opposite side white box dates are devised a defined transparent data model which is we clear about what it represents and how fresh its waters identify as it uses and its provenance of which sauce on when it was connected and because it's used by hundreds and thousands of users within the public domain it's got a really high-quality feedback loop so where the true crime have become much more stringent and regulators expect financial services companies to provide an audit trail and prove they've done all they can to establish the facts about a customer so where they're only using proprietary information going forward this is not only enough to satisfy regulatory expectations and therefore white box data and is really becoming critical so if I move on to the next slide we'll talk a little bit about exactly what we doing without graph database so we initially implemented it in about 2016 we hit some scaling problems and didn't do a lot up until the end of last year and we're moving forward from that we now have a large number of nodes probably not for us I mean 30 million nodes with a lot 22 million edges and all those these are expanding quickly it's not necessarily of the scale of those who have building graph databases on billions of transactions a day but what we do have is really complex interconnected graphs rather than just hierarchies and we want to query these through you know who have hops and taking to it account attributes on the edges such just ownership percentages and time dimension which really had sales nesting as you can see on the slide the network based comes in from a number of data sets there is weirdly shortage of public data out there which is important to realize so where other organizations have gathered this data how do you know it's also into duty and where it was deflected so how can you trust it so on the data is really a state of statement about one or more companies and each with a provenance and exists at a certain time so it's tie box and as you can see it comes from things like US Securities and Exchange Commission the Federal Reserve System in u.s. people with significant control from Companies House and a number of local corporate registries and there also as a small number of manual corporate structures and technically when we and relationships into a lot making database or Ruby BOTS then white or graph database with sickness method to upset and importantly this method updates the pirate ships so for example if the shareholding edge is changed from 10% to 15% the far edge will have its updated two before the date your shareholding is time bountiful so for you move on what I'm showing on the next slide is a d3 visual representation of a New Zealand company and in this case is a corporate control network of Chartered Accountants made in Ingham Mora Limited New Zealand is a great company to investigate all of its shareholding information is open data and it provides not much parental control but really down to the granular level of shareholders and but what's really important to realize is that there are different types of folks who can not go to chest so equity control is usually exercised through all these shares of common stock and in this case the decisions can be carried with a simple majority of 50 percent of the votes so if you look at that threshold there are 32 relationships control but there are multiple different ways to look at voting shares and who has control and if you look at a threshold of 25 percent as based around UK money laundering regulations we then upgrade to 14 if we look at some of the US banking codes we got 112 and if you look at one vote one share model with ended 163 nodes in the network this particular diagram just looks at control and not all relationships and at the moment this shows that come pointer time but where we add unloaded complexity is where we add apparel and theme time aspect so how does that change over the course of their premises in terms of if we go to the next slide so what we're dealing with is a is a highly interconnected relationship-based dataset and we need real-time performance for this module up analysis and those who absolutely critical for API and website performance as I said we need the ability to support multiple degrees the separate between entities I need to build up the picture of and networks and connections that I would see otherwise I want to look up the training to understand who's ultimately in control or down the control or down the Train to see how different entities connect someone I'm interested so siblings as I've mentioned before a temporal graph search it's really important for us to ascertain if a relationship existed at a certain point in time and I need to look at active and dead relationships so for us the next steps are to vastly improve the quality quality and quantity of the network data that we import so the number of nodes increases a rapid rate while we ingest more sources of relationship information from registers and there is going to be over the course the next few years a large increase in the public record information that's available we also need to model out individuals so these can be officers or company or other persons with significant control and as well as incorporating other node types things like addresses and really what we need to do is improve the performance of scale of our networks so showing a network for three levels people at a time is what we do currently but wait we need to resend to that from just doing a nice entry of doors to move through the network to actually expanding that to go to the the multiple hops through so that's a real quick summary of what we are doing and what we are looking to do with our time of our implementation and please send me overs Westers are not happy to pick them up at the end of the world thank you very much Rebecca well that's quite the sloping yeast there and actually I would say that an important part of that has to do with that temporal aspect and fold that I know that Victor has done it's something that Victor has done quite some work on so let's hear it from him their relationship basically between the temporal aspect and finding patterns in ground okay thanks George so in in this illustration what we're showing is three different snapshots of a database but realize actually all of this data is stored in the database at once these are just the output of queries to see who what are the subsets subsidiaries of company a at a certain time or within a certain time window if we change the time window to July we see that there are two additional subsidiaries and then we see by August those two subsidiaries have disappeared and that goes back to one of the use cases that Rebecca mentioned earlier about corporations being created digitally and they come into existence and disappear in a flash and and that can be a signal of some type of questionable activity at least somebody perhaps doesn't want to get caught doing whatever they're doing so we're seeing again with for graph databases institutions like open corporates are really finding traction where the way that you can explore graph data is really enabling their users to do new things financial institutions retailers everybody handles money whether they are helps service the transactions or whether they are you know a vendor everybody has an interest every every industry we discovered has fraud and in the financial institutions in particular are interested in money laundering but everybody looks at fraud in the the next slide we go into one way that a connectional view of the data is is really helping people to financial institutions improve their anti money laundering this is a highly regulated area where a lot of what they do is the not just trying to catch it but trying to report to government agencies how well they're trying to catch anti money laundering or ham l so here's a traditional view on the left we have a new transaction a customer has is connected to some account and that account has been flagged we see the little star due to previous activity in previous activity we saw that this account was associated with some some other parties and then some of those generated alerts and after some investigation some of those alerts seem to be substantiated so they generated suspicious activity reports which went to the government and so for that reason that account has now been flagged as suspicious and in a typical real-time analysis you can go to hops to go from the activity customer account you see the account is flagged so future activity you flagged in any any such transaction however in the next slide we show that if you can go deeper you have a much better view of the data and remember there were there were multiple past actions with this account some of them were ok and some of them were not counterparty one was the one that had the suspicious activity reports counterparty to after investigation they decided they closed the investigations there was nothing wrong so when we look at this new activity we see it's actually of counterparty - so it's actually fine so by doing the deeper analysis in the graph in real time we're able to see that there's no significant suspicions suspicions we don't need to alert this at all that reduces the amount of investigative work which is very manual currently that banks have to do so anything they can do to reduce the amount of manual investigation is a great benefit to to the financial industry that's great Thank You Victor for for breaking that down for us that actually brings us to the next one so one last conclusion let's say for for this part would be mentioning one of your other clients or while eBay in this case yeah Olli pay is a sister company of Alibaba a lot of people have heard of Alibaba big shopping website in China Ali pay is is a financial sister company that helps to handle the transactions so they have a huge customer base over 500 million users and handling over 250,000 payments per second it's you can imagine what that is per day and per year so they again have to try to detect fraud and anti money laundering Tiger graph has worked with Ali paid to develop a real-time fraud detection solution so they import each transaction in real time into a graph so the graph is changing constantly it's new interactions new transactions are being added thousands per second not just the transactions but you know new members of the network etc they're also recording what credit card was used if people are often making mobile payments so those their phone devices have IDs so what device is being used where is it occurring that geolocation data of course when is occurring through their experience they have detected past fraud cases that becomes training data for machine learning so they develop models to to detect fraud but then they have to apply it in real time to catch it so once they've developed this graph based machine learning model for detecting fraud they can then apply in real-time to see is this a suspicious pattern again as we looked at the anti money laundering there's a pattern which you can see visually or described in a using a query language right as it as a query to say is this occurring in the graph and and so some of these queries actually up to 11 hops deep that's it looks easy on paper but in fact they're it's it's basically a very elite number of graph databases that can perform that sort of analytics in real time so obviously all you pay is by able to detect even a fraction of the fraud and increasing their ability to catch and prevent fraudulent transactions that is improving their bottom line Thank You Victor by the way we have a live question for you but let's which a week kind of answered in that chat but let's save it and deal with that in more detail at the end of and for now once you have already drawn the connection to the graph query languages and how they can help do everything that we refer to so far so all those deep hope analytics it's actually important you to see what graph will how graph query languages can help us do that what differences there are among them what query graph query languages are around where they're coming from and worth it going and if in case you know the connection is not always what I would say is that basically everything that we've just talked about becomes much much easier if it's supported in the query language otherwise it's still possible to do that but it means you have to write lots of code basically and we stopped to hand over to Alan who's experiencing the past and present and future grigory language this is going to be quite useful for everyone it's over things thank you George could we go to the next slide for those of you who are new to the domain it may come as a surprise that this is not the first time when graph technology is in the focus of attention this is really the second coming where the first was sometime in the early to mid 90s under at the time the term of semi or unstructured data where that research was motivated by the fact that data was logically structured as a graph when people were interested in just querying world wide web pages that were the nodes of the graph and the links between them which were the edges in the graph after that there were about two decades of continued interest in graph technology but with a much more restricted focus on special graphs that corresponds to hierarchically structured data and these graphs in computer science are called tree shaped graphs but you will all have heard of the XML data model which was a special graph data model and then they get later Jason and variants leading to such notable systems as MongoDB college-based etc but in the last decade people returns to being interested in unrestricted graphs because initially they were motivated by writing analytic tasks on social networks where the nodes were the persons and the edges were the friendship relationships between them and other such relationships and of course nowadays graphs find the universal use because after all most interesting data is linked could I have the next page please ok so just to be on the same page the growth data model is really very simple and natural you have a bunch of real world entities that you're trying to model and you model them as nodes and these entities are all involved in relationships with each other which model is the edges in the graph but they can be directed for example to so an asymmetric relationships such as I follow you or this account has been debited in order to credit the funds to this other account or they can be symmetric relationships such as friendship relationships nodes and edges may carry information typically in the form of key value pairs and they typically are also attached a label that gives us the type as I'm going to show you in a moment on the next slide it would be an example graph where we have two vertex types the type of product vertices that have for example name category and price attributes attached to them and the type of customers who have a social security number a name and an address and then the edge types reflect such relationships as the fact that the customer bought a product attaching for example the discount at which this sale happened and the quantity of the both products for example the edge on the bottom shows the fact that Mercy bought product P at the discount of 5% the quantity being a hundred items could we have the next slide now the key language ingredients that we inherit from past development of the technology Center around the very important notion of path expressions which basically allow you in the query language to specify very concisely how you would like to connect various nodes in the graph you're seeking paths that connect them and these paths satisfy certain properties such as these are only transfer fund edges between accounts then there are other ingredients such as introducing variables that manipulate the data that is found during the traversal stitching multiple such path expressions together into complex navigation patterns just like the ones that you have seen drawn in Victor slides and constructing new nodes and edges okay alright so Thank You Alin for giving us like a very very quick crash course in graphing languages and where they come from so it's time to see where we are today basically so that's that's all fine and well and you know that gives us kind of common ground you know the graph query model and how that how things work let's see where we are today thank you George let me briefly mention some of the reference query languages that you'll find out there today such as the w3c standardized SparQL for ontologies and Semantic Web cipher from the Neo4j system which essentially inherits the syntactic style of the old query languages that I just mentioned before Gremlin and Apache project also supported by a various commercial products that has dataflow programming model and of course GSQL which is TigerGraph's own language the only one in the spectrum that is compatible with SQL and which supports massively parallel graph analytics we have the next slide some of the key language ingredients that are required by modern graph analytic applications involve of course all the primitives we have inherited from the past but in addition they require a bunch of other functionality is the most important ones being the ability to aggregate the data encountered during the traversal and control flow that allows one to express classic iterative algorithms that converge in multiple rounds I'm showing in red how each of these primitives is supported by existing languages I will in the interest of time not list each of them and just go to the next slide again thank you for for being so efficient and indeed as someone who has used quite the number I would say pretty much all of those it's it's quite a nuanced topic to do to go into the specifics so you know how how they differ and partially that's because there's this quite some overlap but I'll just give you the flow for the last part action which is highlighting the SQL and the specifics of what make it different pasting so again over to you thank you on the next slide we we highlight one of the really important salient features of the SQL namely the ability to aggregate data encountered during the traversal of the graph and while existing languages do this more or less in the spirit of SQL using a group by clause GSQL certainly do that but it has an addition a very strictly more versatile and very specific way of aggregating in the form of the notion of accumulators next slide please we're accumulators are simply containers think of them as holding a data value and accepting inputs that we repeatedly write into these containers as we traverse the graph and these inputs are aggregating using a binary operation such as plus minus times etc which immediately provides support for the standard built-in aggregations from various query languages such as some max min etc but also allows you to write your own customized aggregations which come in very handy for more sophisticated applications accumulators may be global in the sense that they have a single instance and the query writes in to it from all parts of the graphs during the traversal or they can be vertex attached which means that every vertex has its own copy and there's the traversal reaches a vertex you can write into the accumulator right then and there on the next slide I show you the one example of mystical that I wanted to include here we have we start with a declaration of to vertex attached accumulators that are meant to aggregate inside the revenue per customer and the revenue for product on a graph such as the one that I Illustrated before in which customers buy products and we know exactly about this counter etc the from clause introduces a little pattern telling us that customers see both product P and we introduced there will be there to refer to the bought edge that connects this customer and this product and we use B to extract from it the quantity that was sold and the discount at which it was sold and notice how this way we can compute the variable this sale revenue that gives us the revenue for the particular sale and then then this is the interesting part we can write the same value both into the customers accumulator for aggregated customer sales and into the products accumulator that aggregates the product sales and this way we we compute two aggregations in a single pass over the data and of course we could have arbitrarily many depending on how many accumulators we define and more moreover all of this aggregation can happen in parallel naturally as the aggregates the accumulators are computed at the vertices where they are located and vertices of course are distributed over multiple machines that all work in parallel okay could have the next slide I would like just to point out here a slide that I don't mean for you to try to follow the syntax of what matters is that in this single slide without any omissions we have coded in this equal more interesting application in which we are ranking a particular kind of product namely Toys to be recommended to a customer based on the taste and prior purchases of other customers just like this customer and similarity between customers here is computed using the standard law cosine similarity measure all of which can be encoded in these just few lines of just equal and then on the next slide let me point out one last important salient feature of GSQL namely the ability for the developer to to control the flow of the traversal in particular to write loops that allow one to do program iterative algorithms such as the PageRank algorithm that runs in Google search engine and that Victor had mentioned before and many other interesting algorithms typically from machine learning applications these heathyr ative algorithm synergize extremely well with the notion of accumulators because they allow a unique combination that concisely expresses sophisticated graph analytics and just one last slide I would like to flash again I don't mean for you to read but the point is here again on this single slide without any omissions we are implementing the the famous PageRank algorithm in which the relevance rank of pages on the web is determined from the connection between pages and the idea here is that the rank of the page is aggregated inside an accumulator that is placed on that particular node that represents the page and these are cumulative values keeps being updated iteratively by the while loop that you see prominently displayed there that's all I wanted to to mention in terms of syntax thank you George Thank You Alan for taking us on to a very brief one I may add for certain in both topic of the SQL top abilities as well as the history and and the present of graphical languages show with that we can actually switch over to answering questions and we have quite a few of those from people attending the webinar so let's start with the more general ones so one question we have is why use a graph database rather than a NoSQL such as Cassandra for example for retail product documentation would like to take this one I'll take a stab at it a graph database in a one important question is is your graph database a so-called native graph or not you can construct a representation of a graph on top of a key value or other types of a column based no sequel and there are products that do that and there are also open source tools you can buy that will build a graph on top of a no sequel the difference is in the performance and in some of the functionality first of all some of those structures are some of those no sequels are designed for good read performance but but not update performance so if if right is important to you that may not be the solution more importantly it's how do they handle those those deep link multi-hop traversals you can do it logically the the the tool will let you construct the query but it will perform slower on a no sequel because they are not designed to optimize the performance of following those links so without going into a lot of the technical detail is basically native graph databases like Tiger graph are designed to traverse links very quickly because that's their storage and engine are designed that way from the ground up just just to add to that because I would say that you know you could even do some of those things so let's let's basically say that the problem with using relational databases in specific is will a formulating queries and be executing the quiz so you may even use a kind of graft layer let's say on top of the relational database and that will help mitigate we formulate in queries part but it won't really soak the execute in place so that's the sort answer basically to that so the other question which is Tiger graph specific actually is well why use tiger graft on for example new we talked about native graph databases versus non native and well I guess neo is is an 81 and I guess this is a question you get quite often as well so I'll let you take this one so we we think neo4j does have a great great little product and what people often come to us when they have started out with neo4j and find that their needs have increased Tiger graph is scalable and distributed and massively parallel in its processing so we can expand to multiple server nodes to distribute a really large database across multiple nodes that's something that that neo does not support that's probably the most obvious difference there when you get into the details there there are other differences in performance we have a little just because we're a more modern design their design originated you know 12 years ago and of course they've updated it but they're they're found their starting point was different than our starting point okay so now we for a change we have a question which is actually know if not graph database specific so the question is that it may be difficult to detect layering even with a graph database because some transactions can take place outside of your network while both endpoints are still in your network so the question is if there are any tips regarding how to deal with a situation like that link prediction or something more advanced HD kind of supplement to the question I guess maybe Rebecca maybe there'll be much presence you explain how how how you can use things like machine learning and its implementation okay so you know I think the the question is is is a great one because what I showed was if you assembled all the transactional data and and those transactions are often between one bank and another and so no one institution has the full graph it only maybe if all the data was shared with the government an anonymous third party would anybody be able to assemble at all and that is what happens sometimes in the investigation after there is suspicion so after their suspicion the government can gather the data actually different banks subscribe to third-party services where they send anonymize data to a third party in the third party can assemble the graph but what an individual institution can do is they do their investigations from the clues they know and then from from past investigations where they found out AHA this actually was a case of money-laundering or some other malfeasance they then develop a model that they can train so it's is based on from past experience my partial view of the data these cases of the partial view turned out to be you know 80% of them turned out to be actual malfeasance so that's that's how you score those so it is related to what you're saying link link prediction is more broadly it's it's kind of machine learning in an even more general just predictive modeling ok Thank You Victor well we're actually almost out of time but maybe we can wrap up this there's a couple of more specific and actually quick questions for you so diagraph initialization and then this diagraph transactional or analytical yes from here both obviously we talked about the analytical part but we offer acid compliant transactions high concurrency distributed systems so we can handle both transactional and analytical workloads with strong consistency there's a graph visualization we have our our products includes graphs studio which is a full-featured visual web-based drag-and-drop menu driven tool not just for visualizing graphs and exploring graphs but also for designing your data models so you can design your database you know to figure out what sort of entities do I want a model you can do that graphically you can get graphical assistance in importing data and in writing queries or running ad-hoc analyses and then visualizing the results so one of the screenshots I showed earlier the fraud ring is actually from graph studio okay that's great Thank You Victor and by the way since you mentioned the slides let me just briefly say because we wrap up that what both webinar and the slides will be made available so we'll keep you in the loop about that and yeah with that I'll just like to thank everyone for for joining and obviously our guests so thank you Victor Thank You Arlene and thank you Rebecca we hope you had a good time like like we did with with this and yeah thank you and see you again in one of our next webinars maybe thank you thank you George thank you everyone thank you you 