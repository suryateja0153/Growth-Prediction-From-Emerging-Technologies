 so I could not have asked for a better leaden for what I'm gonna be talking about today so we talked about how do you get your data into a graph shape how do you have your knowledge graph now what I'm gonna be talking about is once you have your knowledge graph what do you do with that from a data science perspective to start generating new insights and so I'm the senior data scientist at neo I sit on the analytics team so we have both graph algorithms as well as the SPARC integration so I'll be talking a little bit about those and I think the like TLDR if you don't want to listen to the rest of this presentation earlier this week someone said to me why should I care about this Alicia like you really like graphs that's cool whatever why do I care tell me in one fen and the one sentence like hey Frew ISM we fall back on is it's not what you know it's who you know and I think we all can think of examples of times this is really mattered in your life so you really wanted a job what was important was not where you went to school what your degree was in how many years of experience it was hey your college roommate could recommend you to the hiring manager right so we've all had that experience that relationships can often be the strongest predictors of behavior and that's a hand wavy example but some real-world examples are the single biggest predictor of whether or not you are a smoker is whether your friends and family smoke if we talk about elections do you know what the biggest predictor of whether you show up to vote is it's whether your friends have voted and it's that peer pressure and why when you log into Facebook on Election Day it says click here to show you voted so that you know all the people you're connected to are voting so when we talk about why should I be using a graph and a model it's because relationships can be really important predictors and there's a lot of different ways to include those so one thing I like to do is kind of level set on vocabulary when I talk so I am a super technical data scientist I use weird technical words for all different things in this room I bet we have 50 different words for the same five concepts so to kind of frame everything today when I talk about artificial intelligence what I mean is when you have a computer process that is learned to solve tasks in a way that mimics human decisions so this could be anything from a heuristic to deep learning this is the most general kind of concept that we're using machines to make better decisions machine learning is a subset of AI machine learning is when you have an algorithm or a model that you can train to solve a specific task so you train your model to solve this task you give it some new data and make some predictions within machine learning you have deep learning this is a specific class of models using neural networks multiple hidden layers to make predictions with using gradient descent and so we're gonna be talking about kind of going from this broad AI all the way down to deep learning and at neo you've had guys have you've ever seen any other neo slides before about this you've probably seen these four pillars of AI so you start with a knowledge graph you need some data to do data science right that's a prerequisite this gives you your context for your decisions from there you can kind of go into two directions you have graph accelerated AI which are which is basically you have your knowledge graph it's really easy to pull out the relevant sub graphs or subsets of data from that graph to solve the problem you want to solve so you don't want to throw the kitchen sink in you want to say ah for this problem I want this sub graph and I want these connections I want to build these features graphs make it easier to do this on the more technical side you have connected features where basically you're looking at using the graphs to create new features that you can feed into your machine learning pipeline and make better predictions and that's really going to be the focus of this talk and then finally and this is kind of future directions you have graph native learning which is basically we're gonna be talking about graphing are all networks and that is when you are learning natively within the graph you have a graph as your input you make predictions about changes in graph structure and your output is an updated graph so I've said a lot of words about data science and I'm sure some of you guys are like I don't care do I even have a graph problem so we've kind of called out eight potential use cases these are eight out of many these are just one set I've worked on in the past that are coming up pretty frequently and we're kind of doing a really high-level overview of where graphs touch these different problems so financial services is something that I've spent a lot of time on recently working with our early adopters for graph algorithms looking at how do you use kind of the inherent graph data when you look at finances right you have accounts who are making transactions with businesses those accounts have identifier z' there's a native graph there and you can use information from that graph who's connected to who what are common flows and patterns to identify things like first party fraud or money laundering and I will go into a lot of depth on that in about 20 minutes one thing that's kind of near and dear to my heart where I spend a lot of my career is drug discovery so biology and chemistry are kind of inherently graphi you have protein-protein interaction networks you have chemicals as graphs one thing that's very you know growing in popularity is if you think about graphs of genes chemicals and diseases how do you find new treatments based on those interactions can you use wink prediction or relational inference to develop new insights recommendations or something that has come up again and again neo4j has done a lot in the space of collaborative filtering where I have a knowledge graph and I basically have some rules about the recommendations I want to make right I want to recommend to you something that your friend liked building on top of that you can start using algorithms like PageRank to find influential people you want to make recommendations on you can use tools like modified community detection algorithms to find recommendations that form specific groups or projects or you can use sort of graph neural network approaches to actually predict novel paths based on graph structure customers segmentation is kind of more an example of that graph accelerated machine learning we talked about so when we talk about customer segmentation you know in tabular data world you're gonna think about I want to segment my customers based on their demographics show me all white college-educated women between the ages of 25 and 40 and I am going to market them this thing when we talk about steamer segmentation on a graph your segmenting your customers based on who do they interact with and the people I interact with are probably going to be better predictors of my behavior than kind of a generic term when we talk about cybersecurity this is there is a really cool recent paper by Fujitsu and deep tenser looking at if you have networks of interacting IP addresses connect identifying changes in that graph structure can identify intrusions into your network and possible security breaches when we talk about term prediction you know we say you know how do you retain customers how long is this customer been with this company how many times have they called tech support one of the best predictors is actually how many of your friends have left this network and so huawei recently published a paper where they looked at their internal data in China and they said what are the biggest predictors of someone leaving this mobile network and what they found was that not only was it whether your friends had left whether the people you call frequently was also the page rank or how influential your friend who had left the network was so when your friend who's always on the phone calls everyone all the time leave that Network they better worry about those people you were calling search and MDM is kind of where a lot of this got started we take PageRank for granted now but I remember the days before Google you want to use Alta Vista but you know PageRank is kind of this foundational algorithm of how do I traverse all this connected data find what's important now we have a lot more ways of building on top of that to recommend the right search results context dependent relevant information and then finally predictive maintenance you want to talk about you have a network of interacting parts in some kind of system you want to identify which parts should I be looking at where should I be preventing failures so you could be looking at which parts are sort of connectors between different hubs which parts have sort of the highest connectivity which parts if they fail will cause a cascade across your graph and you can use all the different kinds of graph algorithms to figure out what do I want to hone in on the bottom line of all of this is I talked about eight different examples but the concept behind them is the same we're talking about making new and better predictions with data that you already have we're just putting that data into a graph shape and then letting you get more information out of it so your typical tabular data science model you're gonna have rows and columns and you're either ignoring network structure in relationships or your going to a lot of effort to pull those out where you have your column of how many friends does this person have when your data is in a graph it's much simpler to get those out and into your model graphs also can add really new predictive features based on graph algorithms which are unsupervised features instead of me saying I think this is important I'm gonna go find it and put it in I can run something like connected components and see which isolated sub graph is this individual in maybe that's predictive and then finally when you have graphs you can start predicting things about relationships so I think this is one thing that we sometimes overlook we say hey you can make better predictions about financial crime because you know where money is flying but once you have graph data you can start predicting things like who's going to interact with who you have a network of interactions and you can say I want to predict who's at risk for joining a game a gang so I want to look at who of their friends are already involved in gangs and how influential are those friends in the network so because you've got your data in a relational graph you can start making new predictions about relationships so there's a lot of exciting ways we can make use of this and in case you're like maybe this is just neo4j making up stuff to mark it to me it's not just us so there was a really awesome paper from google MIT and deep mind that came out in November it's I think it's called like relational inductive biases on graphs I call it the deep net paper but basically what they said is that graphs bring an ability to generalize about structure that no existing modeling framework gives you so by using graphs you can make better more broadly applicable generalizations and for people who spend their time reading these journals this was like super exciting game changer but you get to the conclusion of the paper and I say there's one lingering question you know this is the right framework this is the algorithm to rule all algorithms where did the graphs come from the graph networks operate over and I think this is a really nice you know why neo4j because we put your graph data in a graph shape that lets you use these graph native algorithms so it starts you off on the path to success and now I've said a lot of words and it's like how do I actually do this right and so I like to see code I'm not going to bore you with that so I have some workflow models you want to talk about how do I get started so you start with your data sources you pull those together we can talk about spark CSV sequel you pull all that information into your native graph platform in neo4j once you're in new 4j you can modify your graph architecture you can do some feature engineering and then you can pull that new data out and add it back to your tabular data and push that through to your normal machine learning pipeline because my team works with spark graph and Morpheus we have examples throughout this deck specifically talking about spark so in spark you have your data frames using spark graph which will be part of spark 3.0 we have rewritten graph frames to use our property graph model you can take your data frame and you can map it to a graph structure this lets you start to experiment and say do I have a graph problem what does my graph look like and what can I get out of it in spark these graphs are immutable this is also a projection where you're running a cipher query on something that's really mapped back to a table so it's not as efficient as it could be but it gets you started once you've figured out hey this is my graph shape I have a graph problem I've taken my 10 terabytes of data down to two I'm ready to do some data science you can use Morpheus to subset that graph into your relevant sub graph move it into neo4j where you can store your graph in a persistent way that you can update right through and run your native graph algorithms write your predictions etc from neo4j you can either move back to spark and use ml live to make predictions or you can use some other package wherever you're comfortable but sort of this workflow is neo4j sits in the middle is the place where your data lives and where you can run graph native algorithms and kind of conceptually here what we're talking about is spark is a really great place to explore and say do I have a graph problem it's really scalable it can deal with large data quantities it's very powerful for data pipelining and what it gives you our non persistent non-native graphs so we've had some people say I tried this out and my for hop query just choked and so that's when you want to move to neo4j when you really want to build and persist so once you're a new for J you have persistent dynamic graphs you graph native queries and algorithms that are specifically designed to run on an in-memory graph so things run more quickly you can store your data you can make modifications and we have a constantly growing list of graph algorithms and embeddings that you can make use of putting it all together so this is kind of intimidating right I'm like you guys should all just get started data sciencing with graphs let's go we're how do I start sign me up so the way I like to visit visualize this is kind of the steps of graph data science so on the x-axis we have enterprise maturity and this is kind of inverse so on the left hand side you have kind of the most mature you've got knowledge graphs neo4j has been doing knowledge graphs since 2012 for a long time moving over to the right you have kind of things that we've developed things that are in experimentation and where we want to go in the future so we have query based feature engineering we have graphic algorithm feature engineering we have experimental graph embeddings and we're moving towards graph neural networks on the y-axis you have kind of data science complexity so you're like how do I get started the easy place to start is with your knowledge graph once you have your knowledge graph you can incrementally build on it and at each step add value and so what I'm going to do for the rest of this talk is we're gonna talk about each step and go through a real-world use case where some one has done something in this space made predictions and use neo4j so starting off with query based knowledge graphs we're not gonna spend a long time on this because we've had a whole morning about how awesome knowledge graphs are but I think we kind of under sell them right a knowledge graph lets you answer the questions that you know you have the answers to you just couldn't get the answers out before so a really nice example is the ref in of dashboard which is made for Thomson Reuters and it's in neo4j and what they did is they made a knowledge graph connecting data sources across lots of different siloed data sets so they have corporate data about which companies interact with each other and how this company has a contract with this other company this company is a supplier this company has invested in this other company they have additional data about external news so what articles were published about this company in the last week is there something happening in the press that we want to be concerned about good news or bad news and then on top of that they have customized waiting that they've put in when they've built their knowledge graph and this is kind of intimidating right you've seen browser you've seen Bloom what if you're just an analyst and you kind of wanna you know the canned queries you want to use what we've built for them is a dashboard where they can look at the knowledge graph and kind of get the information out that they expect to get so an analyst might look at 200 companies they can pull up a dashboard for those companies and use pre executed queries to look at things like credit risk so are there have companies associated with my company of interest had financial trouble lately have companies have there been bad news articles about my company are there been good news articles so you can kind of quickly pull together the information you know is out there kind of it one touch because it's unified in a single data source so once you've got that knowledge graph you can start doing even cooler things so that was I just I know this information is in there I want to get it out this is query based feature engineering where you know some information in your graph is relevant to a prediction you want to make so you're a domain expert you have your knowledge graph and you have a model you want to build let's talk about financial crimes so I want to predict whether an account holder is likely to commit fraud maybe in my model I want to put in something like how many how many accounts have they interacted with that have already been labeled for fraud within one hop two hops or three hops of the knowledge graph and all I'm doing is I'm saying I know something about this problem and I can use the graph to get that relationship data back out a really cool example that I love about basically domain based feature engineering is something called head net or head iana this is online at hat dot IO everyone should check it out it's a knowledge graph that integrates over 50 years of biomedical data looking at genes compounds or drugs diseases and kind of biological processes like tissues symptoms side effects all into a single knowledge graph and what they use this knowledge graph for is predicting new uses for drugs that are already on the market or drug repurposing and so the idea is what if there's a drug that is approved for this disease but it would actually be relevant to treat this other disease maybe because it's targeting the same genes and so the way this works is here's kind of a sub graph around multiple sclerosis so you can see in this example it might be a little hard to read depending on where you're sitting multiple sclerosis is in brown the gene IRF one is in blue we want to know if the gene IRF one is actually relevant for multiple sclerosis and you can see there's no direct connection but you can see some multi-hop connections in the graph what they do is they use queries to develop some features based on those graphed reversals so they look in and they say for IRF one and multiple sclerosis how can i connect it at you what are the different ways I can get between them so they found two ways for that toy example IRF one is connected via leukocytes to multiple sclerosis and there's three potential genes reversals to multiple sclerosis via different genes and for each of these potential meta paths they have weights based on kind of domain expertise of how much do I trust this many data sources reinforce this idea and they come up with meta path scores so given the number of paths between this gene of interest in this disease and how long those paths are how much we trust those paths you get a probability score that then they can plug into a link prediction model this is really powerful if you have a subject matter expert who knows the data and who knows what they're after and this is a really good way of pulling in relevant information about relationships and putting it into what would previously be kind of a standard tabular prediction model the kind of limitations there are you really need to know what you're after right I've said I think this is relevant no one is coming up with new ideas so the next chunk we're going to talk about is graph algorithm feature engineering which is unsupervised instead so if you want to do this we're back to this how do you get started in spark again you can merge all your data together you can create your relevant sub graph you push it into neo4j and that's where you're building these queries of how many hops between a and B and how do I weight them or what do I think is relevant and how do I get that information to come up with some kind of metric that I'm gonna push into my machine learning model so it's not as intimidating as it sounds once we're comfortable with this you can move into graph algorithm feature engineering and so the pivot point here is instead of saying I think this is relevant we have different categories of algorithms that tell you different features about the topology or connectivity of your graph and so what we're talking about here is feature engineering where you're just adding new more meaningful features and we're not saying to throw out what you already have we're saying take your graph and move that data into new columns so this is a toy example it is Game of Thrones again of course why not it's really pretty to look at you have a graph of who interacts with who and what is their page rank and how many people do they interact with in communities they've been assigned to and I have my toy little excel tap table I guess because we all sometimes use Excel for our models and I have my tabular data where I have you know how old is ever what house do they belong to what's their gender are they alive or dead to that model you can enrich it adding what's the PageRank of this individual how influential are they how many people are they connected with what community do they belong to and you start to add new features that are probably much more predictive than how old someone is right so this is just taking the data you have reshaping it and making it more informative when we talk about graph features there's sort of six categories and embeddings we're gonna talk about last cuz they're a little bit different we start with community detection and this is basically clustering or partitioning your graph into groups of individuals or nodes that interact more frequently with each other than other groups and so tabular data you might think about you know doing k-means based on various metadata for your entities this is clustering nodes in your graph based on who they interact with not what their attributes are we have centrality and importance algorithms which basically tell you how important or how connected a node is in the graph and this is where PageRank fits in we have pathfinding and search algorithms and these are basically how do you find the shortest path the least cost path the best path between a node and a source and where this comes in to you machine learning is back to that example of fraud maybe I want to say what's the shortest path between me and a fraudulent account and maybe that's going to be a predictive feature we have heuristic link prediction and I really want to call out this is link prediction based on the likelihood of a relationship being formed based on nodes in common we have several presentations out there you've probably encountered or if you've read our graph algorithms book machine learning for link prediction using classifier models what's built into our graph algorithms library is sort of rule-based link prediction so how many neighbors do two nodes share tells you how likely they are to be connected we have similarity algorithms which basically tell you based on what nodes are connected to how similar are two nodes so are they connected to the same or different things and you come up with a similarity score and then kind of on the on the bleeding edge of our algorithms we have embeddings and these are learned representations of the connectivity or topology of the graph they're designed for certain use cases and we'll talk about those later but they're experimental right now so one use case for using graph algorithms to create features is in financial crime the reason that we focused a lot on this is large financial institutions already have existing pipelines to identify fraud either a heuristic where if you violate these four rules I'm going to flag your account and someone is going to review it or models you know here's the classifier model if this model predicts this is likely to be a fraudulent transaction it's not going to go through so this already exists and this is our sweet spot of we want to use graph algorithms to add features that give you more accuracy and I'm currently working on a number of different projects in this space looking at different kinds of fraud and financial crimes so I kind of pulled out relevant algorithms for different use cases to give you a sense of what can I do here so you could look at something like connected components which finds disjointed sub graphs in your knowledge graph just identify things with shared identifiers so if you're talking about first person fraud where you have a real identity and you're daisy chaining synthetic identities off of that real identity and maybe they're sharing something like an IP address or an address or a social security number when you use connected components you find these clusters where everyone is sharing common identifiers and you can prioritize large clusters to review for fraud or you can use the size of those clusters as a predictive element in your modeling pipeline we can talk about PageRank which measures influence and transaction volumes so this is if you run PageRank on a transactional graph so you have different accounts making transactions with each other you use the volume of the transaction size and the transaction frequency as your weights you assign page ranks to nodes in your graph the higher your PageRank kind of the more money and transactions are flowing through that node and some of those are gonna be spurious maybe it's fidelity bank right that's not a problem but sometimes those are gonna identify problematic nodes either individuals committing fraud with high volumes and money or you know it could be something you want to flag because they are the center of a fraud ring you can use something like blue vein to identify communities that frequently interact with each other so the main application we've seen of this is if you want to find money laundering rings so if you look at transactions as connections between accounts levain will identify clusters in your graph where money is frequently moving between those clusters versus across the rest of your graph if you see those clusters with known criminals or use it in a machine learning pipeline you can identify communities where there is money flowing between different accounts in order to obfuscate the source and finally we've looked at jacquard which is a similarity measure where if you have a known fraudulent account you can take another account and say how similar are these two accounts are they interacting with the same individuals do I think that this account is unlabeled is fraudulent and this is not something like that I thought of or that neo4j thought of if you look at Google patents and you type in graph fraud or anomaly detection there are 48 thousand US patents in the last 10 years and these are big companies like IBM Intel Cisco Palantir has a bunch of them so this is a hot area of research and something where graphs really make it much more tractable to predict what's going on the relationship structures are really predictive so going back to how do I do this so again we start with spark to bring our data together and the difference is now once we're in neo4j we run those graph native algorithms to start pulling out features that we're gonna add as new columns to our tables and so depending on your use case you're gonna run a different set of algorithms pull those out and then those go into your machine learning pipeline in terms of what graph algorithms do we offer we have a lot of them I think it's 45 and Counting in those five broad categories if you're kind of doing this exploring spark build and Neill there are seven algorithms you can run in spark and they're a great way to kind of get started and say what does this look like and then when you really want to kind of build out and say I actually want to run a couple different algorithms and then use variable selection see which one is most predictive that's when you come to me oh and you can start having all the fun question so the question was they're using spark to build their graph right now and how do we recommend doing that and is that the right thing to do right so what we have is in spark 3.0 they've rewritten a spark graph and the graph frames using the neo4j property graph model you'll also have cyber 9 which will enable you to run cipher queries against those graph frames these are a projection that basically maps your table into a graph shape but your query is still against the underlying table so we really position this as I have 10 terabytes of data I want to make it into a graph shape now I want to subset it into the relevant graph and when you start doing heavy queries you run into some performance issues and that's when you want to move into neo4j and we have a tool called Morpheus please talk to me afterwards if this is exciting I worked on this team and Morpheus is what lets you take your your graph and you can create sub graphs from it and then move those into neo4j natively and so I'm super happy to talk about this I'm a little crunched on time and I think people want to have lunch but so moving on to graph embedding so I love graph embeddings because they're this sweet spot between feature engineering and deep learning and so everyone's like oh we should do deep learning and I think awesome betting's are really the nice place to get started because you can use an embedding either as a feature in your traditional machine learning pipeline where you've generated this embedding and you treat it just like any other categorical variable you can use graph embeddings to look at similarity where you can compare two embeddings using like euclidean distance and see how similar are these nodes or you can use your embeddings is your input into your graph neural network so they're kind of this nice transition point and we actually just brought someone onto my team to be developing graph embed for us and in case I got really excited and talked about Graff embeddings and you were like that's cool what on earth is that stepping back and embedding is basically how you take some complex data and transform it into a vector so probably you guys are familiar with something like word Tyvek where you take some text and you want to transform that into a bit string a graph embedding transforms some aspect of the graph into a vector or set of vectors that describes the topology connectivity or attributes of nodes or edges in the graph and different embedding serve different purposes so you wouldn't kind of have one Universal embedding to rule them all generally speaking you can have vertex embeddings which describe the connectivity of a node so if you've ever seen graph sage it's a vertex embedding um you can have path embeddings which is the example I'm gonna walk through for recommendations or you have traversals across the graph and you translate those into bit strings and then you can have a graph embedding where you encode your entire graph as a vector and this is when you have multiple graphs that you're making predictions about which is relevant kind of in the chemistry space and so at the bottom of this slide we have a figure from deep walk which was one of the first and kind of most influential graph embedding papers to be published and the way they create their embeddings is you take a node in your graph you do multiple random walks across your graph from that node and then you take each of those random walks just like you would in Word so back and you create a skip ground model so you look at co-occurrence and then you can use that to learn weights using gradient descent of how influential how much should I wait the presence or absence of this node in the path so you take your graph you take this kind of amorphous connected data structure and you transform it into a bit string which is much easier to work with in machine learning so one example that I really love that was published about a year ago and eBay is paper explainable reasoning over knowledge graphs for recommendation we all want to use knowledge graphs for recommendation and we always talk about collaborate collaborative filtering but if that that's kind of what we can do now what can we do in the future how can we predict new applications new unseen in the graph and so what a base paper did is they kind of looked at the idea of how do you recommend a song to someone and so Alex or Alice likes the shape of you by ed sheeran how do we identify new songs she might like well luckily the shape of you sits inside a knowledge graph that connects all of this other data about Alice and the shape of you so shape of you is sung by ed sheeran who sings some other songs that genre that song is pop someone named Toni has also listened to this song and he's listened to icy fire as well so you have all of this connected data that lives around this kind of source of Alice and the target of what song she likes and you can actually traverse this graph to create embedding this to make better predictions and so what that looks like is this paper took what I would call a path embedding where you take they extracted every potential path up to six hops in the knowledge graph between the source and target node and they looked at what were the nodes traversed so Alice shape of you ed sheeran I see fire and what were the relationships Traverse so interacts sung by produced contains right so you have kind of two sets of information you have all of these paths up to six hubs and then you take each of those each of those individual paths you create an embedding for those paths and then you use an LS TM model to pool those and come up with a single path embedding that says this is what the traversal between Alice and a song she likes looks like and then you can use those embeddings in a deep learning model to say well what-what would Tony like here's how Tony is connected in the graph and make a prediction based on path embeddings about what someone is supposed to like next and what's really powerful about this example is these embeddings are interpretable so you can use a decoder and you can make a prediction and say okay I think Tony is gonna like Taylor Swift to me and you'll say well why and you can decode that embedding and come up with the probabilities that you Traverse to come up with that new prediction so it's moving away from this black box model of deep learning into something that's a little more interpretable so I think this is exciting if this is something you want to get started with our labs team has implemented two prototype embeddings we have deep walk and deep GL these are experimental not supported but cool to play around with we've just brought on someone for this summer he's actually the PhD student of Steve Skinner who came up with deep walk and we're gonna be spending the next few months kind of doing a review of relevant embeddings figuring out where we go next so if embedding sound exciting come talk to me and so kind of the holy grail of graphic data science at the end of our ladder is graph neural networks and there's a lot of hype about this but not a lot of understanding of what we're talking about so when we talk about deep learning this is training multi-layer neural networks using gradient descent you might have seen RNs or recurrent neural networks and CNN which are convolutional neural networks RNs are used on sequential data CNN's are often used on images where you have a matrix the idea of a graph neural network is you have a more generalizable structure so the definition that the deep net paper put forward is the graph native learning is a deep learning model that takes a graph as an input performs computations on that graph you update your graph and then you return a graph as your output and so the kind of the figure underneath walks you through that so you have an edge in your graph you update it the update of that edge changed the property of a node and then once that has happened you update the state of your graph and this is a little hard to wrap your head around but you can think of it like going back to how do I know who smokes so we have a graph of people who interact and you want to know do the majority of people in this population smoke and is that gonna change over time and you could start with doing link prediction who knows who so we start with an edge update Joe becomes friends with John Joe is a nonsmoker John is a smoker so we've added a new edge now we want to update the John node to say now that John knows one more nonsmoker is he still a smoker we say no nothing changed in our graph then we repeat this this is iterative so now we have a relationship between John and Mary John becomes friends with Mary Mary is also a nonsmoker so we've done an edge update and then a node update so now that John knows to non-smokers maybe he's like I'm tired of standing outside by myself I'm done so now we can update the status of our total graph and we say ok now the fraction of this graphs that are smokers is one smaller so it's kind of a toy problem to help you wrap around what on earth is she talking about moving on to a real-world problem kind of the most advanced use of graph native learning that we see is really in the chemical space so you can represent any chemical as a connected graph the atoms in your molecule our edge our nodes in your graph the bonds or relationships there's a lot of research and how do you encode those as vectors what you can do is you can represent each molecule as a graph you can take a data set of molecules and reagents where you know what they form and you can train and embedding in a neural network model to predict given two new chemicals in a reagent what are they going to make and so there's kind of two outputs from this paper product prediction so you take two input chemicals in a reagent what do they form and because this is a graph neural network where you are updating your graph iteratively you can answer the question of how was it formed remember we talked about node and edge updates and then global State so this is kind of cool in that it's really transparent and if you're like how does this work kind of here's a walkthrough you start with your two molecules you identify a node in your molecule that is likely to be updated and then when you find that node you say are the relationships around that node going to change and then you say once that's changed is the molecule stable or do we keep going and then you update changing those nodes and the bonds in your molecule until you reach something stable and then you not only have your new product you have the steps in between so this is the reason I'm citing this is this is kind of an in production example of graph native learning and what's super cool is I actually know one of the authors of this paper I was talking to him last week and he was like I am really excited to try to get this running in neo4j so this is close to my heart I think it's super awesome I love talking about grass native learning so going back to progressing and graph data science the take-home message here is it's not hard to get started once you have your data in a graph the barrier for entry is low there is value to be gained at each step you can use domain expertise and cipher to come up with relative really relevant expertise driven features that you can put in your model building on that you can run graph algorithms to create unsupervised features that tell you about the topology becoming more sophisticated you can train graph embeddings using supervised embedding models to tell you relevant features of your graph in generalizable ways and then ultimately you know our end goal is enabling graph neural networks this is kind of where we're heading but the steps along the way each one shows value and depending on what you're doing you may not want to go beyond graph based feature engineering right that might meet your model so the goal here is it's not that hard to do it's awesome to get started I'm really excited about it so if you have any questions we do have a graph algorithms book which we have told you about many times you should go download it it's super awesome there's online materials and also websites where you can kind of play around with stuff and feel free to reach out to me Alicia frame or bother me on Twitter or in person at lunch thank you you 