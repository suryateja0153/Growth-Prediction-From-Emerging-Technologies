  Hello. My name is Beth Ebersole. And I'm going to talk about SAS Visual Forecasting 8.4. Specifically, we're going to use the SAS Studio Interface. We're going to create some graphs and interpret our results. Here I am in the SAS Studio Five Interface. You can see there are three horizontal lines here. When I click on that, I get the side menu. I selected develop SAS code. And that brought me to SAS Studio Five. I've precreated a snippet so that I can get my data where I can see them. I'll highlight by CAS statement, my LIBNAME statement, and submit that. When I open SAS tasks, I see forecasting here. This would operate on the compute server. This will not operate in CAS. This is actually SAS 9 code behind the scenes of this task. What I want to use is SAS Viya Forecasting. I open that, and I see Time Series Exploration. That's the one I'll click on. The data I want are the VF price data. And I can browse here to find those. I can go to VF and price data. The roles I'm going to set is sale as the dependent variable or that's the item that I'm forecasting. For my time ID, I'll select order date. I'll also add an independent variable. Let me add price as my independent variable. So I have my PROC TSMODEL, my ID, date statement, and two variables, the dependent variable sale and the independent variable price. I see here, require TSA. So I'll do a time series analysis. And here, between my Submit and end Submit, I have my user defined code. You can see that in this PROC TSMODEL, I'll create the information I need for my autocorrelation plots. But it will not actually create those plots. I'll need to do that with PROC SGRENDER below. Now I'll go to my analysis tab. I see that my time series plot is already selected. I see that my perform autocorrelation analysis is already selected. I'll deselect my autocorrelation analysis panel, and I'll individually select what I want. I'll take my autocorrelation function, my partial autocorrelation function, inverse autocorrelation function, and my white noise probability test on a log scale. When I look at the code over here, I can see that the code has been generated. You can see here is my white noise log probability. This PROC SGRENDER will plot the white noise probabilities on a log scale. I'm using a template to make a nice pretty graph. You can see when I deselect, that will disappear. When I reselect, that appears. So I'm just creating code using the task. I'll select Submit to run this. Here are my results, which I can look at in a larger window by going to this snowman or three dots above each other. And I'll open a browser tab for my results. The first thing I see is a table, for example, the number of analysis variables. I had one dependent and one independent variable, the number of rows read, things like the minimum time ID and the maximum time ID. So you see I have quite a bit of information here. Next, I have my series values. And you can see this runs from January 1998 to January 2003. And I can see these little circles are my individual data points. Here's my autocorrelations for my sale variable, my dependent variable. And as I scroll down, I can see my partial autocorrelations, my inverse autocorrelations, and my white noise probabilities. Let's look at each of these in a little bit more detail. So we're going to look at four different types of plots, autocorrelation function plots, inverse autocorrelation function, partial autocorrelation function, and white noise plots. So why do we need these plots? How can we use these plots? Let's remind ourselves a little bit about ARIMAs. Remember for an ARIMA, we're going to set our autoregressive terms, our integration term, our moving average term. So we call these the P, D, Qs. So we're going to set P equal to something, D equal to something, Q equal to something. Remember that our autoregressive term or our P, we're looking at setting lags on our observations. For our moving average term or Qs, we're looking at setting lags on our error terms. So we find the significant lags, and these graphs help us find them. Also, for our integration, we're looking at our differencing term, which is D. So back to our autocorrelation function graph. We notice we have a series of bars here and a shaded blue area. The shaded blue area is two standard errors. So we are going to be most interested in those bars that stick out beyond that shaded blue area of two standard errors. The first block is a correlation of yt with itself. So the observation at time t is obviously always going to be 100% correlated with itself. So we have a autocorrelation function of 1. The next bar we see is at lag time 1. So we see a correlation of yt with yt minus 1. That's the observation at the current time with the observation at the time before or the observation, actually, at any time with the observation of the time before. And we see what appears to be a significant correlation. Here, we also see, which we commonly see in seasonal data where we have monthly data and we have a seasonal annual effect, we see a correlation at time t with 12 months prior. So we see that December is correlated with the previous December. January is correlated with the previous January. In this graph, we see that, of course, January is correlated with itself. January, for example, of 2010 is correlate with January of 2010. So we might suspect that February of 2010 is correlated with January 2010. That's called serial correlation. And we also see that January of 2010 is correlated with January of 2009. So each month is correlated with the month 12 months prior. Here are a couple of things to know about interpreting autocorrelation function. Let's recall the importance of stationarity in time series. A stationary time series is one whose statistical properties, such as the mean, variance, autocorrelation, are constant over time. So time series with trends or seasonality are not stationary. So we want to model out those components. We want to model out trend. We want to model out seasons by using lags. One of the things that we look at when interpreting an autocorrelation function is the decay. So here in this first graph to my left, you see the rapid decay indicates stationarity as opposed to the graph on the right where slow decay indicates non-stationarity. Remember we ultimately want stationarity. We also note that if we have significant spikes in the autocorrelation function, there is still variation that can be extracted, that can be modeled. So we need to modify our forecasting model to improve it to capture that information. If we have no significant lags, no meaning in this visual no spikes outside of the blue shaded area, then we would know that this series is white noise. And a model will not help us extract useful information. So here's a little rule of thumb. The autocorrelation function helps you diagnose moving average terms. It can also support our diagnosis of a need for AR terms. If the ACF bars drop after 0, then try, for example, a moving average 1 model. Remember our moving average term is also called our Q. So we would set Q equal to 1. Then we can try that model and see how it works. The beauty of SAS Visual Forecasting is that you can easily try a model. You can set your MA to 1. You can try that. You can set your AR to 1. You can try different models and see which give you the best results. Now let's consider our partial autocorrelation function. Unlike the autocorrelation function, the partial autocorrelation function adjusts for all of the previous lags. So for example, we see we have what appears to be a significant lag here at four intervals. In this case, that's four months. And that has been adjusted by removing the effect of the lags at 1, 2, and 3. So rule of thumb for the partial autocorrelation function is to help us set autoregressive terms. So for example, if the partial autocorrelation function bars drop off after 1, then we would try an AR 1 model. That means we would set P equal to 1 and try that model. Also, partial autocorrelation function spikes indicate lags to try for a pure AR model. And here, we see the inverse autocorrelation function. This also, in conjunction with the other two that we saw, helps us diagnose AR and MA terms. It commonly suggests subset and seasonal autoregressive models better than the partial autocorrelation function. And it can be used for detecting over differencing. If the data have been over differenced, the inverse autocorrelation function looks like an autocorrelation function from a non-stationary process. That is, it exhibits slow decay. So our rule of thumb here is if the IACF dies off exponentially, try setting an MA term of 1. That is, Q equal to 1. Finally, let's look at our white noise plots. Here, we're plotting the white noise probability. And you can see it goes from 1 down at the horizontal axis up to 0.001. So higher is a lower number. And you can see that it's on a log scale. The graph on the left indicates white noise. The graph on the right indicates that we do not have white noise. If the series is not white noise, you can try an autocorrelation model, such as an AR 1, and MA 1, or a low order mixed ARMA model. You can also use the ACF, PACF, and ICF plots to help you figure this out. Assigning your P, D, and Q can be sometimes more of an art than a science. You know, there are rules of thumb, but it can take a lot of training to really understand what's going on. And also, you want to use your domain expertise. You want to select lags that make sense logically as you understand your data thoroughly. But hopefully, you can now better understand these graphs and use them. Remember with Visual Forecasting, because it's so quick, you can easily try different ARIMA models and see which one of them work the best with your data. Remember you always want to use a hold out sample or an honest assessment to make sure that you're validating your results. You don't want to fit your model exactly to the data and then select a model that won't generalize. I hope this information on SAS Visual Forecasting using the SAS Studio interface and interpreting your results graphs was helpful. 