 [Music] hi I'm Arjun gopalan and welcome to episode 2 of neural structured learning in the previous episode you learned about what neural structured learning is and how this new learning paradigm can be used to improve model accuracy and robustness in this episode we'll discuss how neural structured learning can be used to train neural networks with natural graphs before we begin let's define what a natural graph is essentially it's a set of data points that have an inherent relationship with each other the nature of this relationship can vary based on the context social networks and the world wide web are classic examples that we interact with on a daily basis beyond these examples they also occur in data that is commonly used for many machine learning tasks for instance if we are trying to capture user behavior based on their interactions with data it might make sense to model the data as a co-occurrence graph alternatively if we are working with articles or documents that contain references or citations to other documents or articles then we can model the data set as a citation graph finally for natural language applications we can define a text graph where nodes represent entities and edges represent relationships between pairs of entities now that we understand what natural graphs are let's look at how we can use them to train a neural network consider the task of document classification this is a problem that frequently occurs in a multitude of contexts as an example machine learning practitioners might be interested in categorizing machine learning papers based on a specific topic such as computer vision or natural language processing or even reinforcement learning and often we have a lot of these documents or papers to classify but very few of them have labels so how can we use neural structure learning to accurately classify them the key idea is you citation information whose existence is what makes the data set a natural graph what this means is that if one paper cites another paper then both papers likely share the same label using such relational information from the citation graph leverage is both labeled as well as unlabeled examples this can help compensate for the insufficiency of labels in the training data you might be wondering well all this sounds great but what does it take to actually build a neural structured learning model for this task let's look at a concrete example since we are dealing with natural graphs here we expect a graph to already exist in the input the first step then is to augment the training data to include graph neighbors this involves combining the input citation graph and the features of the documents to produce an Augmented training data set the PAC neighbors API in neuro structured learning handles this and notice that it allows you to specify the number of neighbors used for augmentation in this example we use up to 3 neighbors the next step is to define a base model in this example we have used Kerris for illustration but neural structured learning also supports the use of estimators the base model can be any type of Carus model whether it's a sequential model a functional API based model or a subclass model it can also have an arbitrary architecture once we have a base model we define a graph regularization configuration object which allows you to specify various hyper parameters in this example we use 3 neighbors for graph regularization once this configuration object is created you can wrap the base model with the graph regularization wrapper class this will create a new graph chaos model whose training loss includes a graph regularization term you can then compile train and evaluate the graph Kerris model just as you would with any other chaos model as you can see creating a graph Kerris model is really simple it requires just a few extra lines of code a collab based tutorial the demonstrates document classification also exists on our website you can find that in the Crypton below feel free to check it out an experiment with it in summary we looked at how we can use natural graphs for document classification using noodle structured learning the same technique can also be applied to other machine learning tasks in the next video we'll see how we can apply the technique of graph regularization when the input data does not form a natural graph that's it for this video there's more information in the description below and before we get to the next video don't forget to hit that subscribe button thank you [Music] 