 so we spend the last few lectures discussing similarities regularity lemma so we saw that this is an important tool with important applications allowing you to do things like a proof of roster via graph theory one of the concepts that came up when we were discussing this statement of summary destroy clarity lemma it's that of pseudo randomness so the statement of some Redis graph regularity lemma is that you can partition an arbitrary graph into a bounded number of pieces so that the graph looks random like as we call it between most pairs of parts okay so what does random like me so that's something that I want to discuss the next couple of lectures and this is the idea of pseudo randomness which is a it's a concept that it's really prevalent in combinatorics in the radical computer science and in many different areas and what pseudo randomness tries to capture is in what ways can a non random object look random alright so before diving into some specific mathematics I want to offer some philosophical remarks and so you might know that you know income on a computer you want to generate a random number while you type in a Rand and it gives you a random number but of course you know that it's not necessarily true randomness it came from some pseudo-random generator probably there's some seed and some complex looking function and output something that you couldn't distinguish from random but it might not actually be random it's just something that looks in many different ways like random so there is this concept of randomness you can think about a random graph right generate this address writing a random graph every edge occurs independently with some probability but I can also show you some graph some specific graph which I say well it's just for all intents and purposes just as good as a random graph so in what ways can we capture that concept so that's what I want to discuss and that's the topic of pseudo randomness and of course well you know this idea extends to in the many areas number theory and whatnot but we'll stick with graph theory in particular I want to explore today just one specific notion of pseudo randomness and this is comes from an important paper called quasi random graphs and this concept is due to Chung Graham and Wilson back in the late 80s so they defined various notions of pseudo randomness and I want to state them and what it turns out and the surprising part is that these notions these definitions although they look superficially different they're actually only equivalent to each other so let's see what the theorem says so the set up of this theorem is that you have some fixed real P between 0 & 1 and this is going to be your graph edge density so for any sequence of graphs GN so with so from now I'm going to drop the subscript G so subscript n so G will just be G n such that the number of vertices so to genius and vertex with extensity basically peep okay so this is your sequence of graphs and the claim is that we're going to state some set of properties and these properties are all going to be equivalent to each other [Applause] okay so all of these properties capture some notion of pseudo randomness in what ways does this graph G or really a sequence of graphs or you can talk about a specific graph and have some error parameters and error balance they're all roughly the same ideas so in what ways can we talk about this graph G being random like well we already saw one notion when we discussed similarities regularity lemma and let's see that here so this notion is known as discrepancy and it says that if I restrict my graph to looking only at edges between some pair of vertex sets then the number of edges should be roughly what you expect based on density along so this is basically the notion that came up in epsilon regularity this is essentially the same as saying that G is epsilon regular with itself for this epsilon now hidden in this little old parameter so that's one notion of pseudo randomness okay so here's another notion which is very similar so it's almost just a semantic difference okay so I have to do a little bit of work so let me call this this prime so it says that if you look at only edges within a set so instead of taking two sets I only look at one set and then look at how many edges are in there versus how many you should expect based on density alone these two numbers are also very similar to each other so let's get to something that looks dramatically different the next one I'm going to call count so count says that for every graph H the number of labeled copies of H in G okay so labeled copies I mean that the vertices of H are labeled so for every triangle there are six labeled triangles that correspond to that triangle in the graph the number of labeled copies of H is okay so what should you expect if this graph will truly random you expect P raised to the number of edges of h plus small error times n number raised to the number of vertices of H and just as a remark this little old term one term may depend on H so this condition count says for every graph H this is true and by that I mean for every H there's some sequence of decaying errors but that sequence of the K nares may depend on your graph H the next one is almost a special case of count it's called c4 and it says that the number of labelled copies of c-4 to the four cycle yes at most P raised to power 4 ok so again what you should expect in a random setting just for cycle count alone I see already some of you are surprised and we'll discuss that you know this is an important constraint it turns out that alone implies everything just having the correct c4 count the next one we'll call Co degree and the co degree condition says that if you look at a pair of vertices and look at their number of common neighbors in other words their Co degree then what should you expect this quantity to be and vertices that possibly could be common neighbors and each one of them if this were a random graph with edge probability P then you expect the number of common neighbors to be around pqube a p squared n so the co degree condition is that this sum is small so most pairs of vertices have roughly the correct number of common neighbors degree yes number of common neighbors next in the last one certainly not the least is I can value condition so here we are going to denote by lambda 1 through lambda G the eigenvalues of the adjacency matrix of G okay so we saw this object in the last lecture I include multiplicities if some eigenvalue occurs with multiple times that included multiple times so the eigenvalue condition says that the top eigenvalue is around pn and that more importantly the other eigenvalues are all quite small now for D regular graph the top I can value it's fine to think about D regular graphs you know if you want to get some intuition out of this term for D regular graph the top I can value is equal to D because the top eigenvector is there's the old one vector so top eigenvector is a one vector which has eigen value T and what I can value condition says is that all the other eigen values are much smaller right so here I'm thinking of des on the same order of SN okay so this is the theorem so that's what we'll do today well prove that all of these properties are equivalent to each other and all of these properties you should think of as characterizations of pseudo randomness and of course this theorem guarantees us that it doesn't matter which one you use their own equivalent to each other and you know our proofs are actually going to be I mean I'm going to try to do everything fairly slowly but none of these proofs are difficult we're not going to use any fancy tools like some radix regularity lemma and in particular all of these quantitative errors are reasonably dependent on each other so I've stated this theorem so far in this form where there's a little one error but equivalent ly so can equivalently state them State theorem for example disk with an epsilon error which is that some inequality is true with a most epsilon error instead of oh and you have a different epsilon for each one of them and the theorem turns out that okay so the proof of this theorem will be that these conditions are true so all equivalent up to at most a polynomial change in the epsilon in other words so property one is true for epsilon implies that property two is true for some epsilon raised to a constant so the changes in parameters are quite reasonable and we'll see this from the proof but I won't say it again explicitly any questions so far about the statement of this theorem so as I mentioned just now the most surprising part of this theorem and the one that I want you to pay the most attention to is the c4 condition this seems at least at face value the weakest condition among all of them it just says the correct c4 count but it turns out to be equivalent to everything else and there's something special about c-4 right if I replace c-4 by c3 by just triangles then that's not true so I want you to think about you know where the c-4 played this important role how does it play this important role okay so let's get started with the proof but before that let me so in this proof one recurring theme is that we're going to be using the Cauchy Schwarz inequality many times and I want to just begin with an exercise that gets you some familiarity with applying the Cauchy Schwarz inequality and this simple tool but it's extremely powerful and worthwhile to you know master how to use the Cauchy Schwarz inequality okay so let's let's get some practice let me prove a claim which is not directly related to the proof of the theorem but it's indirectly in that explains somewhat the c-4 condition why we have less than or equal to over there okay so the lemma is that if you have a graph on n vertices such that the number of edges is at least P and squared over 2 so edge density basically P then the number of labeled copies of c-4 is at least e to the 4 - little one and so if you have a graph with edge density P P is a constant then the number of C forces are at least roughly what you would expect in a random graph alright so let's see how to do this and I want to show this inequality as a me I'll show you how to prove this inequality but I also want to draw sequence of pictures that at least to explain how I think about applications of the Cauchy Schwarz inequality ok so the first thing is that we are counting labelled copies of c4 and this is basically but not exactly the same as number of homomorphic copies of c-4 in g so by this guy here i really just mean you are mapping vertices of c-4 to g so that the edges all map to edges but we are allowing not necessarily injective maps c4 to 2g but that's okay so the number of non injective maps yes most cubic so we're not really affecting our account so it's enough to think about homomorphic copies okay so what's going on here so let me draw a sequence of pictures illustrating this calculation so first we are thinking about counting c4 c4 I can rewrite the c-4 count as a sum over pairs of vertices of G as the squared Co degree and what happens here so this is true mean it's not hard to see why this is true but I want to draw this in pictures because you know we have larger and bigger graphs may be more difficult to think about the algebra unless you have some visualization so what happens here is that I noticed that this c4 has a certain reflection namely it has a reflection along this horizontal line and so if I put these two vertices as U and D then this reflection tells you that you can write this homomorphic number who more copies as the sum of squares but once you have this reflection and reflections are super useful because they allow us to get something into a square and then right after apply the Cauchy Schwarz inequality so we apply Cauchy Schwarz here and we obtain that this sum is at most where I can pull the square out and I need to think about what is the correct factor to put out here and that should be okay so what's the correct factor that I should put out there okay so 1 over N squared so I don't actually like doing this kind of calculations which sums because then you have to keep keep track of these normalizing factors one of the upcoming chapters when we discuss graph limits or in fact you can even do this instead of taking sums if you take an average if you take an expectation then it turns out you never have to worry about these normalizing factors and so on somehow normalizing factor should never bother you if you do it correctly but just to make sure things are correct this keep me in check all right so what happened in this step in this step we pulled out that square and pictorially what happens is that we got rid of half of this picture so we used cauchy-schwarz and we wiped out half of the picture and now what we can do is while we're counting these guys these paths of length two but I can reprioritize this picture so that it looks like that and now I notice that there is one more reflection all right so there's one more reflection and that's the reflection around the vertical axis so let me call this top vertex X and I can rewrite the sum okay so once more with your cauchy-schwarz which allows us to get rid of half of the picture and now I'm going to draw the picture first Stan you see that what we should be left with it's just a single edge and then you write down the correct sum making sure that all the parentheses and normalizations are correct but somehow that doesn't worry me so much because I know this what that means the but whatever it is were just summing the number of edges so that's just a number of edges and okay so we put everything in and we find that the final quantity is at least P raised to 4 and 2 4 ok so I did this quite slowly but and also emphasizing the sequence of pictures on and party to tell you like how I think about these inequalities because for other similar looking inequalities in fact there is something called sidorenko's conjecture which I may discuss more in a future lecture that says that this kind of inequality should be true whenever you replace c4 by any bipartite graph and that's a major open problem in combinatorics it's kind of hard to keep track of these calculations unless you have a visual anchor and this is my visual anchor which I'm trying to explain of course it's down-to-earth it's just the sequence of inequalities and this is also some practice with cauchy-schwarz alright any questions so but one thing in this calculation told us is that if you have edge density P then you necessarily have c4 density at least P to the fourth so that party explains why you have at most n here so you always know that it is at least this quantity so the c-4 quasi randomness condition it's really the equivalent to replacing this less than or equal to u by an equal sign ok so let's get started with proving the chunk um Wilson theorem so the first place that we'll look at is the two versions of disk disk stands for discrepancy okay so first fact that disk implies this prime I mean this is pretty easy you take Y 2 equal to X be slightly careful about the definitions but you're okay not much to do that the other direction where you only have discrepancies for single sets and you want to produce discrepancies four pairs of sets ok so this is actually a fairly common technique in algebra that allows you to go from bilinear forms to quadratic forms and vice versa it's that kind of calculation so let me do it here concretely in this setting so here what you should think of is that you have two sets x and y and they might overlap and what they correspond to in the and you think about the corresponding Venn diagram where I am looking at ways that a vertex and fall in pair of vertices can fall in X and/or Y so if you have x and y and so it's useful to keep track of which vertices are in which set but what the thing finally comes down to is that the number of edges with one vertex in X on one vertex in Y I can write this bilinear form type quantity as a course as an appropriate some just number of edges in single sets okay so there are several ways to check that this is true one way is to just tally and keep track of how many edges are you counting in each step so if you are trying to count the number of edges in yeah so let's say if you're trying to count the number of edges in with one vertex in x one vertex in Y then what this corresponds to is that count but let me do a reflection and then you see that you can write this some say alternating sum of principal squares so this one big square plus the middle square and minus the 2 side squares which is what that sum comes to alright so if we assume this prime then I know that all of these individual sets have roughly the correct number of edges up to a little o of n squared error and again like I don't have to do this calculation again because it's the same calculation so the final thing should be P times the sizes of x and y together Plus this same error ok so that shows you this prime implies disk so the self version of discrepancy implies the pair version and discrepancy so let's move on to count to show that disk implies count actually we already did this so this is the counting lemma so the counting lemma tells us how to count labelled copies if you have these epsilon regularity conditions which is exactly what this case so so count is good another easy implication yes can implies c4 or this is actually just tautological c4 condition is a special case of of the count hypothesis all right so let's move on to some additional implications that require a bit more work so what about c4 implies Co degree so this is where we need to do this kind of Cauchy Schwarz exercise so let's start with c4 so assume the c-4 condition and suppose you have this okay so let's so I want to deduce that the co degree condition is true but first let's think about just what is the sum of these Co degrees as I vary U and V over all pairs of vertices so this is the a picture so that is equal to the sums of degrees squared which now by cauchy-schwarz you can reduce to be at least n times 2 raised to the number of edges namely the sum of the degrees I think squared so now we assumed the c-4 condition actually we assume that G has the density yes written up there so this quantity is P squared plus Pluto 1 times n which is what you should expect in a random graph of GMP okay so that's but that's not quite what we're looking for okay so this is just the sum of the co degrees what we actually want is the deviation of co degrees from its expectation so to speak you know here's a important technique from probabilistic component works is that if you want to control the deviation of a random variable one thing should look at is the variance so if you can control the variance then you can control the deviation and this is a method known as the second moment method and that's what we're going to do here so what we'll try to show is that the second moment of this code agrees a meter sum of their squares it's also what you should expect as if the random setting and then you can put them together to show what you want so this quantity here well what is this we just saw see up there it's also called degree square squared so this quantity is also the number of labeled copies of c4 not quite because you might have two vertices and the amber at the same vertex so I incorporate a small error so it's really a sub it's a cubic error but certainly sub n to the fourth and we assume that the number of labeled copies of c-4 by the support condition is no more than basically P to the 4 times and raise to the power 4 okay so now you have a first moment for some average and you have some control in the second moment I can put them together to bound the deviation using this idea of controlling variance code degree deviation is upper bounded by okay so here using Cauchy Schwartz it's upper bounded by basically the same sum except I want to square the summoned this also gets rid of the pesky absolute value sign which is not nicely algebraically behaved now at the square and I can expand the square okay so I expand the square into these terms and the final term here it's P 2 4 & 6 all right but I have controlled individual terms from the calculations above so I can upper bound this expression by what I'm writing down now and basically you should expect that everything should cancel out because they do cancel all in a random case but of course it's a sin if you check it's it's important to write down this calculation so if everything works all right everything should cancel out and indeed it you cancel out and you get this so everything should cancel out and you get a little o of n cubed I have to summarize in this implication from c42 to Co degree what we're doing is were controlling the variance of Co degrees using the c4 condition and a second moment bound showing that the c-4 condition you know trumps over to a col degree condition any questions so far okay so I'll let you ponder in this calculation the next one that will do is Co degree implies disk okay and that will be a calculation in a very similar flavor but it will be a slightly longer by a similar flavor calculation so let me do that after the break alright so what we've done so far so let's summarize the chain of implications that we have already proved so first we started with showing that the two versions of disk are equivalent and then we also noticed that disk implies count through the counting lemma so we also observe that count implies c4 tautologically and c-4 implies code agree okay so the next natural thing to do is to complete this circuit and show that the code agree condition implies that the discrepancy condition so that's what we do next and in some sense you know these two steps you should think of them as going in this natural chain where C 4 so C 4 is 4 like this C 4 code agree condition is really about that and disk it's really about single edges as you can go from top if you you have you you get much more power so it's going in the right direction going downstream so to speak so that's what we're doing now going downstream and then you go upstream via the counting number all right let's do code degree implies disk so we want to show the discrepancy condition which is one written up there but before that let me first show you that the degrees do not vary too much so that the degrees are fairly well distributed which is what you should expect in a pseudo random graph right so you don't expect a half the vertices half into Greece twice the other half so that's the first thing I want to establish if you look at degrees this variant this this deviation is it's not too big okay so like before we've seen absolute value sign we see some so what do you coffee shorts coffee Schwartz allows us to bound this quantity replacing the summoned by a sum of squared a square so I can expand the square so let me expand the square I get that so just expanding this square inside and you see this degree squared is that picture so that's some of code degrees and some of the degrees is just a number of edges okay but we now assume the code agree condition which in particular implies that the sum the code agrees is roughly what you expect so the sum of the code agrees should be P Square and cubed plus a little ol and cube error at the end likewise the number of edges is by assumption what you would expect in a random graph and then a final term and like before of course it's good to do it since then if you check everything should cancel out so what you end up with is little o of N squared showing that the degrees do not vary too much and once you have that promise then we move on to the actual discrepancy condition so this discrepancy is can be rewritten as the sum over vertices little X in big x degree from little X to Y minus key times the size of y okay so rewriting this sum of course what should we do next Cosi sorts great so what do you call shorts okay so here's a here's a important step or a trick if you will so what do you coach is shorts and something very nice happens we need your coffee shorts here instead okay so you can write down the expression that you obtain when you do coffee shorts so we do that first okay so here's a step which is very easy to cross over but I want to pause and emphasize this step because this is actually really important what I'm going to do now is to observe that the summon is always non-negative therefore I can enlarge the sum from just little X in X to the entire vertex set and this is important right so this is important that we had to do coffee shorts first to get a non negative someone you couldn't do this in the beginning so you do that and okay so I have this sum of squares so I expand expand I write out all these expressions and now the little x-range over the entire vertex set all right so what was the point of all of that so you see this expression here the degree of from little X to big Y squared okay so what is that how can we rewrite this expression honey little X Y squared yeah so some of code degree of two vertices in Y so Y Y prime in Y code degree of little Y little Y Prime and likewise the next expression can be written as the sum of the degrees of vertices in Y and the third term I leave unchanged so now we've gotten rid of these funny expressions we're just degree from the vertex to a set and we could do this because of this relaxation up here so that was that was the point we had to use this relaxation so now we get these Co degree terms but now because you have the Co degree terms and we assume the Co degree hypothesis we obtained that the sum is roughly what you expect as a random case because all the individual deviations do not add up to more than little and cubed that could agree some so what you expect and the next term is some of the grease it's also by what we did up there what you expect and finally third term and as earlier if you did everything correctly everything should cancel and they do and so what you get at the end is total of n square okay this completes this 4-cycle any questions so far okay so we're missing one more condition and that's the eigenvalue condition so so far everything had to deal with counting various things all right so what does eigenvalue have to do with anything so the eigenvalue condition is actually a particularly important one and we'll see more of this in the next lecture but let me first show you the equivalent implications so what we'll show is that the eigen gallery condition is the equivalent to the c-4 condition so that's that's the goal I'll show you covalent between I and c4 so first implies the c-4 condition because up to so instead of counting cforce which is a little bit actually not it's been annoying to do actual cforce just like earlier we want to consider homomorphic copies which are also labeled walks so closed walks of length 4 so up to a cubic error the number of labeled C force is given by the number of closed walks of length 4 which is equal to the trace of the fourth power of the adjacency matrix of this graph okay and the next thing is super important so the next thing is this is sometimes called the trace method that one important way that the eigenvalues of the spectrum of a graph or matrix relates to other combinatorial quantities is via this trace so we know that the trace of the fourth power is equal to the fourth moment of the eigenvalues okay so if you haven't seen a proof of this before I encourage you to go home and think about it so this is this is an important identity of course 4 can be replaced by any number up here and well now you have the eigenvalue condition so i can estimate this sum there's a principal term namely lambda 1 right so that's the big term everything else is small and the smallness is supposed to capture pseudo randomness but the big term you have to analyze separately so the big term okay so let me write it out so the big term you know that it is P to the 4n to the 4 plus total of 4 ok so the next thing is what to do with the middle terms so we want to show that the contribution in total is not too big so what can we do well so let me first try something so first well see that each one of these guys is not too big so maybe let's pound each one of them by go to all of em so raise to 4 but then there n of them so you have to multiply by an extra n and that's too much that's not good enough so we cannot individually pound each one of them and this is a this is a novice mistake so this is something that will actually we'll see this type of calculation later on in the term when we discussed Roth theorem but you're not supposed to bound these terms individual the better way to do this or the correct way to do this is to pull out just a couple you know some but not all of these factors so it is upper bounded by you take max off in this case you can take out with one or two but you take out let's say in two factors and then you leave the remaining sum intact in fact I can leave I can even put lambda one back into the remaining so so so that is true so what I've written down is just true as inequality and now I apply the hypothesis on the sizes of the other lambdas so one I pulled out is little Olaf N squared and now what's the second sum that sum is the trace of a squared which is just the twice the number of edges of the graph so that's also at most N squared so combining everything you have the desired bound on the C for count of course this gives you an upper bound but we also did a calculation before the break that shows you that the C for bound is a lower bound as well so really having the correct I can value this already already shows you that the C for bound is correct in both directions but because we this is the main term and then everything else is small okay the final implication is C for implies eigen value okay for this one I need to explore this following important property of the top eigenvalue so this is something that we also saw last time which is the interpretation of the top eigen value of a matrix interpreted as so this is sometimes called the Kuran Fisher criterion or there actually this is a special case of Kuran Fisher okay this is a basic linear algebra fact if you are not familiar with it I recommend looking it up that the top eigen value of a matrix of a real symmetric matrix is characterized by the maximum value of this quadratic form let's say if X is a non zero vector so in particular if I set X to be a specific vector I can lower bound lambda 1 so if we set this boldface one to be the oh one vector in R raised to the number of vertices of G then the lambda one of the graph is at least this quantity over here the numerator and denominators are all easy things to evaluate the numerator is just twice the number of edges because you are summing up all the entries of the matrix and the new denominator is just n so the top eigenvalue is at least roughly PM okay so what about the other eigenvalues or the other eigenvalues I can again refer back to this moment formula relating a trace and closed walks it is at most the trace of the fourth power minus the top eigenvalue raised to the fourth power some of the other eigenvalue raised to the fourth power and for here we're using the force an even number okay so you have this over here so having a c4 hypothesis and also knowing what lambda one is allows you to control the other lambdas she lambda one cannot be greater than much greater than PN also comes out at the same calculation yep thank you yeah so there's a correction so lambda one is so so in other words the little always always respect you the constant density okay yep okay so the question is in I can value implies C for said something about the lower bound so I'm not saying that so as written over here what this is what we have proved I saw him but when you think about the pseudo randomness condition for C for it shouldn't be just that the number C for count is and most something should be that in close to that which would be implied by the c4 condition itself because we know always it is the case that the c-4 count is at least compare what it is compared to the random case one more thing I said was that lambda one you also know that it is at most P and plus n right because so this finishes the proof of the Chong Graham Wilson theorem on quasi random graphs and we stated all of these hypotheses and they're all equivalent to each other and I want to emphasize again the most surprising one is that c4 implies everything else they're fairly weak seeming condition seemingly weak condition this just having the correct number of copies of labeled C force is enough to guarantee all of these other much more complicated looking conditions and in particular just having the C for count correct implies that the counts of every other graph H is correct now one thing I want to stress is that the Chun Graham Wilson theorem it's really about dense graphs and by dense here I mean P constant of course the theorem as stated is true if you let P equals to zero so there I said P strictly between 0 and 1 but it's also ok if you let P equal to 0 you don't get such interesting theorems but but it is still true but for sparse graphs what you really want to care about is approximations at the correct order of magnitude so what I mean is that you can write down some sparse analogues for P going to 0 so P as a function of n going to 0 as n goes to infinity ok so let me just write down a couple of examples but I won't do all of them and you can imagine what they should look like so disk should say this quantity over here the discrepancy condition is not all of P M squared because P N squared is the edge density overall so that's the quantity you should compare against and not N squared if you're compare N squared you know you're cheating because N squared is much bigger than the actual edge density likewise the number of labeled copies of age yes I want to put the little ol 1 plus little in front right so instead of plus little ol end to the H at the end as you understand the difference there so for sparse this is the correct normalization that you should have when P is allowed to go to 0 as a function of N and you can write down all of these conditions right I'm not saying there's a theorem you can write down all these conditions and you can ask is there also some notion of equivalence I'm sorry are these corresponding conditions also equivalent to each other and the answer is emphatically no absolutely not so all of these equivalent tests fail for sparse some of them are still true in some of the easier ones that we did for example the two versions of discs are equivalent that's the okay and some of these calculations involving cauchy-schwarz or you know mostly still ok but the one that really fails is the counting lemma and let me explain why with an example so I want to give you an example of a graph which looks pseudo random in the sense of disk but has no C for let's say C 3 count also has no C for Campbell just Xu has no triangle as the Keene correct number of triangles alright so what's this example so let P be some number which is no off 1 over root n so some decaying quantity with N and let's consider GMP well how many triangles do we expect in GMP so let's think of Pierre just slightly below 1 over root n so the number of triangles in GMP in expectation so that's that's the expected number and you should expect the actual number to be roughly around that but on the other hand the number of edges it's also expected to be okay so this quantity here and you expect that the actual number of edges to be very close to it but P is chosen so that the number of triangles is significantly smaller than the number of edges so asymptotically smaller fewer copies of triangles than edges so what we can do now is remove an edge from each copy of a triangle in this GNP we removed a tiny fraction of edges because the number of triangles is much less than number of edges we remove the tiny fraction of edges and as a result we do not change the discrepancy condition up to a small there error so the discrepancy condition still holds however the graph has normal triangles so you have this pseudo random graph in one sense namely of having discrepancy but fails to be pseudo random in a different sense namely has no triangles yep we're question to the condition c4 and coda we still hold here basically downstream is okay but upstream is not okay so you can go from Seaforth code agree to disk but you can't go upward and understanding how to rectify the situation may perhaps adding additional hypotheses to make this true so that you could have counting llamas for triangles and other graphs in sparser graphs that's an important topic and this is something that we'll discuss at greater length in non-black not next lecture but the one after that and this is in fact related to the green-tailed theorem which allows you to prove similarities term among the primes the primes contain arbitrarily long arithmetic progressions because the primes are also a sparse set so as density going to zero density that came like 1 over log and according to prime number theorem but you want to do regularity method so you have to face this kind of issues so just okay so we'll discuss that more length in a couple of lectures but for now just a warning that everything here it's really about dense graphs the next thing I want to discuss is an elaboration of what happens to these eigenvalue conditions [Applause] so for dense graphs in some sense everything is very clear from this theorem like once you have this theorem they're all equivalent you can go back and forth and you know you lose a little bit like epsilon here and there but everything is more or less the same but if you go to sparser world then you really need to be much more careful we need to think about other tools and so the remainder of today I want to just discuss one pretty simple but powerful tool relating I can values on one hand and a discrepancy condition on the other hand right so you can go from eigenvalue to discrepancy by going down this chain but it's actually there's a much quicker route and this is known as the expander mixing them for simplicity and really will make our life much simpler we're only going to consider T regular graphs here T regular means every vertex s degree T same word but different meaning from epsilon regular and unfortunately that's just the way this so T regular and we're going to have n vertices and the adjacency matrix has eigenvalues lambda 1 lambda 2 and so on arranged in decreasing order let me write lambda as the maximum in absolute value of the eigenvalues except for the top one in particular this is either the absolute value of the second one or the last one as I mentioned earlier at the top eigenvalue is necessarily deep because you have all ones vector as an eigenvector so the expander mixing lemma says that if I look at to vertex subsets the number of edges between them compared to what you would expect in a random case so just like in a disk setting but here the correct density I should put is d over N this quantity is upper bounded by lambda times the root of the product of x and y so in particular if this lambda so everything except the top eigenvalue is small then this discrepancy should be small and you can verify with what we did that it's consistent what we put what we just did all right so let's prove the expander mixing lemma which is pretty simple given what we've discussed so far relating so there was this spectral characterization up there of the top I can value so we can let J be B or once matrix let J be the old ones matrix and we know that the all one's vector is an eigenvector of the adjacency matrix of G with eigen value D so the eigen decomposition of J is also the ovince vector and it's complement so we now see that a sub G minus T over and J has the same I can vectors as AG so you can choose the eigenvectors for that it's the same set of eigenvectors of course we consider this quantity here because this is exactly the quality that comes up in this expression once we hit it by characteristic vectors of subsets from left and right okay so so what are the eigenvalues okay so the eigenvalues so a previously had eigenvalues blend the 1 through lambda and but now the top one gets chopped down to 0 so you can check this explicitly so you can check this explicitly by checking that if you take this matrix multiplied by the or ones vector you get 0 and if you have an eigenvector eigenvalue pair then hitting this by any of the other ones gets you the same as in a because now the condition or the other eigen vectors are orthogonal to the or once vector all right so now we apply the quran fisher criteria which tells us that the number in this discrepancy quantity which we can write in terms of this matrix it is upper bounded by the product of the lengths of these two vectors x and y multiplied by the spectral norm not quite using the version up there but I'm using the spectral norm version which we discussed last time it's essentially one up there but you allow not just single x / x and y and that corresponds to the largest eigen value in absolute value which we see that at most lambda so it's the most lambda times size of x size of y okay nothing is the proof of the expander mixing lemma so the moral here is that just like what we saw earlier when the in the tense cadiz but for any parameters so here there's no it's a very clean statement you can even have found the degree graphs he could be constant if lambda is small compared to D then you have this discrepancy condition and the reason why this is called an expander mixing lemma is that there's this notion of expanders which is not quite the same but very intimately related to pseudo-random grass so one property of pseudo-random graphs that is quite useful in particular in computer science is that if you take a small subset of vertices it has lots of neighbors so the graph is now somehow in a cluster into a few local pieces so there's lots of expansion and and and that's something that you can guarantee using in the expand your mixing lemma that you have lots of you take a small set of vertices you can expand outward so graphs with that specific property taking small subset of vertices always gets you loss of neighbors are called expand their graphs and these graphs play important role in in particular in computer science in designing algorithms improving complexity results and so on but also play important roles in graph theory and combinatorics well next time we'll address a few questions which are along the lines of one how small can lambda B as a function of T right so here it's as if lambda is small compared to D then you have this discrepancy but can it be you know if D is let's say am Union how small can lambda be that's one question another question is is considering you know everything that we've said so far what can we say about let's say that the relationship between some of these conditions for sparse graphs but that are somewhat special for example KD graphs or vertex-transitive graphs and turns out some of these relations are also equivalent to each other 