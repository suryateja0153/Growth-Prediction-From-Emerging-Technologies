 [Music] thank you my name is Kevin Sat singer and I'm here today to share with you our latest results on quantum supremacy benchmarking the sycamore processor the promise of quantum computing is that it can solve certain useful problems that are simply beyond the reach of classical computing and most of those applications are here in the useful error corrected machine which is still years of research in the future we also hope that in the near term in this blue region we will be able to find useful applications but of course before a quantum computer can do something useful that is intractable for classical computing it must first do anything at all that is intractable for a classical computer and so a real question that has been on our minds in this whole industry for the last decade is can we cross this line and this is something that our group has been focusing on for the past several years it's something that was given a name by john prescott in 2012 he called it quantum supremacy to perform tasks with controlled quantum systems going beyond what can be accomplished with ordinary digital computers he went on to ask is controlling large-scale quantum systems merely really really hard or is it ridiculously hard well I am pleased to say that it is merely really really hard and we demonstrated this with the paper that we published last fall where we showed crossing this line for the first time and looking at this slide I see there's a lot of words on there so maybe a picture would be a better way to reflect this and I want to emphasize that this is a huge team effort and I'm very thankful to each and every person each and every member of this team who contributed to this work and I'm honored to be here today representing them you can find our paper on nature it's open access and also at this archive link we have updated versions of our supplementary information and the centerpiece of this paper was a new processor called Sycamore and it's positioned here right at this boundary between classically simulate Abel and beyond classically simulate Abel and that's what I'm going to be presenting about today first the march from graduate school in the upper left toward Industrial Technology that can take on a supercomputer and then how that plays into the research direction towards a useful error corrected machine for the rest of this talk I'll be following this outline so we'll look first at the Sycamore processor second how to calibrate and benchmark Sycamore and finally quantum supremacy itself so let's get started with Sycamore and before we get to Sycamore I want to flash back a few years to what the state of the art was in 2015 this is a chip from the Santa Barbara group it's about one centimeter in size and it has nine transman qubits in a linear array and what I'd like you to observe is that the control wiring takes up about half of the area of this chip and when you look at this device it is not at all obvious how we could scale it up to a two dimensional array with 50-plus qubits it's not a matter of copy/paste we need to really re-engineer the system in order to make it scalable and one of the key technologies that made it possible to make such a two-dimensional array was moving to a scalable flip-chip design where we have two chips instead of one so one chip this top chip will be solely responsible for a two-dimensional array of qubits and the other chip will take care of all the readout and all the control wiring and by dividing the responsibilities like this we'll be able to have a more scalable design one of the key technologies that makes as possible is these indium bumps which provide a superconducting interconnect and a mechanical connection between the two chips this is a photograph showing a small processor prototype that demonstrated this flip chip technology so there are four chips in this photograph at the bottom is a qubit chip that has a small two by three array of superconducting qubits in the center and the rest of the chip is just covered in those indium bumps in the center is a separate chip that's responsible for readout and control and interfacing with the outside world and what we do is we take the qubit chip and flip it over on top of the control chip align them and press them together and that completed assembly is what's at the top of the photograph where we have two chips stacked together and ready to use now this is how you can make a quantum processor but one of the lessons that I want to share with you is that there's a lot of hardware infrastructure that goes into making one of these experiments actually work so let me share with you a couple of highlights one is packaging this is basically anything that goes between the quantum processor and our dilution refrigerator and in our case we have the processor embedded in a circuit board with superconducting metal and the circuit board interfaces with about 150 coaxial connectors around the perimeter we also in case the processor in electromagnetic shielding to protect it from the outside world we then take this package and bolt it to the bottom of a dilution refrigerator which will cool it down to about 10 millikelvin and also is responsible for delivering signals through a couple hundred coaxial cables that go from room temperature down into the device another key piece of hardware infrastructure to give these experiments to work is our electronics these are room temperature electronics that generate the control signals for our processors and we use custom scalable electronics here's an example of one of our boards in the center is a field programmable gate array that's responsible for driving eight digital to analog converters circled around it which then drive these eight channels these can output arbitrary waveforms from 0 to 400 megahertz and we can also up convert those with mixers to microwave frequencies like around 6 gigahertz this is just one card we can have many of these cards in a crate like depicted here and then several of these crates in racks working together in concert to control one of our processors now I'd like to turn to Sycamore itself one of the key advances of Sycamore is that it's a new tunable coupling architecture this is a new feature where we're able to turn the qubit interactions on and off at will so the qubits can be independent of each other most of the time but then when we want them to interact and get entanglement we can turn on the coupling for a brief period of time this it turns out was immensely helpful to making the full system work and was really a key breakthrough in order to get this processor to perform we did this with a scalable two dimensional architecture where we introduce an extra coupler transmen between each pair of qubits where each qubit itself is also a transman and this is depicted in this device schematic here where we have 54 qubits and 88 couplers one between each pair of qubits I want to share with you a little bit of data showing how these couplers really work so let's look at a simple experiment where we have two qubits next to each other at the same frequency what we're going to do is excite one qubit and then have the two qubits interact for a period of time the vertical axis subject to a certain coupler bias the horizontal axis and let's look at the center first so in the center here the coupler is at its maximum frequency and so there is a few megahertz of coupling between the two qubits and what happens is this photon leisurely swaps back and forth between the two qubits that's what these oscillations are but as we march to the right as the couplers frequency comes down there's this divergence where there is no more swapping this is where the coupling is turned off and the two qubits can act independently this is a very valuable place this is where we we operate ordinarily but sometimes you want the qubits to interact and to do that we'll post the coupler a little bit further to the right so that we have very strong coupling for a brief period of time between the two qubits I'll end this section with this nice photograph of our Sycamore processor and a nice symmetry that I could highlight is that the center chip here with the qubits is about one centimeter in size which is the same size of the chip we looked at at the beginning of this section so now let's move on to calibration and benchmarking suppose that I handed you all these electronics the fridge and cables the packaging and the processor it is not a trivial matter to turn all that stuff into a quantum computer and calibration is the process of learning how to execute quantum logic with our hardware so that we can go from all of our stuff to a system that can run an arbitrary quantum algorithm in the same way that you could play music on a finely tuned piano but this is not a trivial task there are around 100 parameters for each qubit we need to choose a frequency for every qubit and coupler and there are strong interactions between those frequencies and biases and then we need to tune up all of the gates and read out for each qubit and pair of qubits now if you have just a few qubits you can park a graduate student in front of a computer for a couple of days and they'll be able to work it out but if you have 50-plus qubits you need a much more scalable solution and in order to solve this problem we encode our calibration routines into a graph and this allows us to solve the problem using standard graph traversal algorithms pictured here is an example calibration sequence for two qubits and this network is really a graph distilling decades of research from groups all around the world learning how to make these processors work in this graph each node represents a physics experiment where we acquire data and then the analysis that we use in order to figure out what the data says and decide what to do next this is an example for two qubits but there are literally thousands of these nodes when we want to calibrate Sycamore so it's crucial that we use algorithms to work through this graph to calibrate the device and then maintain the calibrations give you a flavor of how this works we start on the Left learning some device parameters and as we work to the right we iterate back and forth between single qubit gates and readout until eventually we get around to two qubit to qubit gates at the very end a key step in setting up our device is choosing a frequency for each of the qubits and we're going to kind of follow a two step program here first we're going to measure the qubit lifetime as a function of frequency this is an example of data set for one qubit from Sycamore and then we're going to use that data and all the information we know about our device and how it performs in order to choose optimal frequencies for each qubit this is a pretty rich subject but let me just give you a flavor for how this proceeds for example each qubit is least sensitive to control noise at its maximum frequency so we might want to park all the qubits at their maximum frequencies although there is some variation in those frequencies across the device but then if we consider pulse distortions for our two qubit gates it's actually nicer if the qubits are close to their neighbors so they don't have to move very far in frequency in order to interact so this will kind of smooth out this shape here so that the qubits would be close to their neighbors but you don't want to be too close because there can be stray interactions parasitic couplings between qubits and their neighbors and next nearest neighbors and this might suggest kind of a checkerboard type pattern like we see here and finally we include the information from this qubit lifetime versus frequency data to find an optimum where we expect each qubit to have good performance in solving this problem is not a trivial matter it's very difficult for a human to solve it but we have developed good heuristic algorithms find us good solutions to this problem now once we've set up all of our qubits at their desired frequencies we need to actually carry out the calibrations and I'm just going to share with you a couple of nice snapshots of what that calibration data looks like throughout the process and an early crucial qubit experiment is Rabi oscillations where we oscillate back and forth between the qubit zero state and one state and this is very nice because it allows us to choose the initial amplitude for a PI pulse for our qubits this is an example for one qubit and here we've plotted the data for all 53 qubits that we're measuring on this device and we can see although there's some diversity in these data sets our algorithms can readily pick out this first oscillation that gives us our one of our earliest calibrations another critical calibration is readout so an example experiment here is readout clouds where we prepare each qubit in the zero state and then measure the readout signal that's the blue cloud and then we do the same preparing the qubit in the one state using our PI pulse and that's the red cloud and we want the separation between these two clouds in order to achieve single shot readout of the qubit state this is an example for one qubit and across the full device I'll mention I'm not going to go deeper into readout but this is quite a sophisticated subject and managing to achieve good readout on all the qubits simultaneously is very challenging and even just benchmarking the readout for 50 cubits at the same time is not a trivial task either the final calibration data I'll share with you is for calibrating two qubit gates and I'll talk a little more about two qubit gates in a moment but basically one an eye swap between the two qubits where we have a resonant exchange of the qubits photons so what we do is for each pair we tune the coupler amplitude so that we have the correct amount of coupling between the two qubits about 20 megahertz in this case so that they'll completely swap a single photon and that's that first maximum there and here we've plotted 86 of these plots it's one for each pair of qubits across the device now I said I wanted to talk a little bit more about two qubit gates on Sycamore our basic idea is to use fast resonant gates where we pulse the coupling on for around 10 to 20 nanoseconds so that the two qubits can interact and for two transmen qubits the natural interactions that will take place are an I swap interaction and a conditional phase and we can map the gates onto this two-dimensional space here with those two axes and you can see some of your favorite familiar gates like I swap square root I swap and see Z on this plot now for quantum supremacy we did something a little unusual we did what we would call an eye swap like gate on all of our pairs this is the data that I was just showing you and we actually determined the unitary specifically for each pair of qubits but if you're not although this is a little unorthodox you can take two of those I swap like gates and compile them into a C naught which classic textbook gate along with single qubit rotations I'll mention as well as a part of his PhD thesis Brooks Foxen actually filled in this whole space for a pair of qubits and was able to execute an arbitrary gate in this space and you can read about that at his paper here now suppose we've tuned up all of our two qubit gates and we want to benchmark them to figure out how well we're performing there are many different techniques to do this but I'm going to focus on one of those which is cross entropy benchmarking because it closely relates to what we do in the quantum supremacy experiment the first step is to choose a bunch of sequences that we're going to measure and these are sequences of our two qubit gate interleaved with randomly selected single qubit gates like depicted here we'll then take each of these sequences and run them on our quantum processor maybe a few thousand times each in order to compute these experimental probabilities associated with each of these sequences now on the other hand we can also use a classical computer in order to basically run the same circuit and come up with the theoretical expected probabilities and those depend on the unitary that we're trying to run but say we have an idea of what unitary we mean to run with our two cubic gate so we have two probability distributions one that we measured with our experiment and one that is computed and we can compare these two probability distributions to estimate the fidelity which is like how well are we executing the expected quantum gate and this is where cross entropy actually comes in because that's the metric we use to compare the two probability distributions now there's something special here we can have a feedback loop between these two sections in order to find the unitary model that maximizes this fidelity and when we do that we're finding the two qubit unitary that most precisely expresses what exactly we're doing in our two qubit gate so we're basically learning what is the two qubit gate that we're actually doing this is very useful because it helps us identify coherent errors for example suppose you are trying to do a CZ gate and it turns out that the optimal unitary actually has a little bit of swapping in it this is telling you where your errors are coming from so that you can learn from that and make your calibrations better in the case of quantum Supremacy we use this tool to identify what is the unitary that each qubit pair is actually running we're going to use that information when we do that experiment there's another important detail here which is simultaneous operations and so in addition to benchmarking each qubit one at a time or each pair one at a time we want to benchmark all of the qubits operating at the same time and all of the pair's operating simultaneously as well in this kind of simultaneous operation is essential to near-term applications because because of noise we have limited algorithm depth it's also a necessity for our long term plans for quantum error correction and the key question here is does the processor work when we're doing operations simultaneously there's one other detail that I want to highlight here you can't actually run all the pair's simultaneously because each qubit participates in more than one pair so we divvy them up into different layers that we can benchmark simultaneously so let's look at some benchmarking results from sycamore and I'll start with the single qubit gates and we're plotting the distributions of errors across the device for 53 qubits on the left so if we do the single qubits one at a time their gates have an error of about 0.15 percent which is pretty good and if we do them all simultaneously it only modestly increases to 0.16 percent this is great because it suggests that there are not stray interactions between the two qubits or crosstalk that would cause these gates to mess up as we turn to the two qubit gates in an isolated case we see an error of 0.36 percent which is really outstanding and in the simultaneous case this modestly increases to about 0.6% which are really still quite proud of and this increase in error could be attributed for example to stray interactions between pairs of qubits that are becoming slightly entangled with each other we also have readout which I won't delve into too deeply here where we have a few percent error per qubit I'll make a note for this simultaneous two qubit gate benchmarks the unit aries of the two qubit gates actually changed slightly when we run them all once and we can measure that and take it into account in our experiments instead of plotting the distributions we can also plot these single and two qubit gate errors on the device layout itself and we were really excited when we got this got this to work and got to see this for the first time these are the errors associated with the simultaneous operations for each qubit and also each coupler is colored in with the simultaneous error for each two qubit gate now this was basically looking at pairs one at a time not having entanglement across the whole device but an important thing that we want to do is to evaluate the performance of the full processor doing a realistic algorithm and once you have 50-plus qubits this technique of cross entropy benchmarking we were using where we were comparing comparing probability distributions is no longer tractable and that's because of the simple fact that two to the 53 is about 10 to the 16 different amplitudes or probabilities that we would be trying to deal with and it is simply not possible to resolve all those probabilities so instead we're going to reframe the question of cross country benchmarking to Kinley sample from the expected distribution so we'll get samples bitstrings and we'll compare them to the probabilities that we're expecting based on what we thought we were doing in this case we'll make some observations like maybe a million or so bit strings for a circuit and then we can take the bit strings that we actually observed and compute the expected probability the theoretical probability that's associated with each one and this allows us to estimate the fidelity using linear cross entropy that's what this equation is here this is the only equation in our paper I think and this fidelity stated plainly is basically how often we sample high probability bitstrings with that let's move on to the final part of my presentation which is quantum supremacy itself so what we need if we're trying to pit our quantum processor against classical supercomputer is a well-defined computational tasks and the task that we chose is to sample the output of a pseudo-random quantum circuit and the point here is that for a sufficiently large circuit the classical computing cost of performing this task becomes prohibitively large and to give you an idea of what these circuits look like this is very similar to what we were doing before but now we're getting entanglement across the whole device so we have layers of randomly selected single qubit gates and then layers of two qubit gates there's a bit of a problem here that you might be picking up on based on something that I said a couple minutes ago I said that we're going to observe a bunch of bit strings and then compute the ideal or expected theoretical probability associated with each one so on the one hand we want to beat classical computers but on the other hand this test seems to need them to check our fidelity to see if our quantum computer is doing the right thing and we can look at that schematically with this cartoon where we plot fidelity as a function of the computational difficulty for our quantum processor you can think of that like the number of qubits for example and as we add in more qubits this fidelity goes down because there are more opportunities for errors to happen but we can still check our results and see what the fidelity is using this technique but at some point we reach this boundary where we're no longer able to check check our work so this is a problem but we have a solution to it and that is to control the difficulty of the circuits that we're executing the most natural and obvious way to do this is by changing the size of the circuit so a smaller circuit will be much easier to run classically but there are some more subtle techniques that allow us to run large circuits and still check our work one of them is to remove a few gates instead of running every single gate in the circuit we take a cut down the middle of the device and remove just a handful of gates from the very beginning of the computation this decreases the entanglement between the two halves of the device and makes it much easier to simulate or compute with a classical computer but it's still essentially the same for a quantum processor because we've just removed a few gates from the beginning another even more subtle way to change this difficulty is by changing the order in which we execute the gates it turns out if instead of doing this hard sequence we do this very subtly different sequence of gates there is a slight simplification in a way that the entanglement propagates across the device that can be exploited in order to perform the classical computation more efficiently so the idea is to use these circuits at the top as a proxy for our hardest circuits which have all the gates running in the difficult order and what we'll be able to do is evaluate our performance with these circuits and then infer what our performance is on these which are very similar from the point of view of the processor at the quantum processor now let's look at what this data should look like so before we run the experiment let's make a model for what we expect to happen and we'll make a very simple model here where we say our fidelity at the end is going to be the product of the fidelity of each and every gate that we're going to execute times the fidelity of the readout at the end so this is using the numbers that we were looking at in the previous section on benchmarking in order to estimate how well we expect the device to do now on the left of this plot we have a dozen qubits we're going around 14 cycles so there are a couple hundred operations that we're doing and the fidelity of the full circuit on the full system is about 40 percent is what we expect and that's actually really good for hundreds of operations as we move to the right we add more and more qubits so there are more opportunities for errors and this fidelity will decrease for example at 50 cubits it's at about one percent which is still large enough that we can measure it reliably so this is our model this is what we expect to happen with a very simple model now let's look at some results from experimental data and there are two data sets that I'm plotting here one uses the simplified circuits where we've removed a few gates from the very beginning of the circuit and another uses the full circuit where all the gates are present but in this data we're using the easy ordering of the gates so that there's kind of a back door so that the classical computer can still run this computation what we see is a remarkable agreement between all three of these first of all our prediction our very simple model matches the data extremely well and this is great because it means that our very simple error model is able to reflect that system performance going from a dozen qubits up to more than 50 cubits another crucial lesson here is that the simplified circuits and full it's are also consistent with each other and this makes sense because for the quantum processor they're only slightly different there's just a few gates missing for the simplified circuits out of craps a thousand or more but it's great to see experimentally that these agree with each other one last note here although these are the easy circuits once we get to fifty-three qubits it still becomes pretty computationally intensive for the classical computer to check our work for example that last read data point took about five hours to verify with a million cores on a supercomputer but let's pull out all the stops now and use our hard circuits and this is what that data looks like you'll notice that there's something missing here we no longer have the full circuits represented because although we took that data we cannot process it so we have our model prediction where as we increase the number of cycles how many operations we do the fidelity decreases and we have the data points from our simplified circuits each of those data points represents 10 circuits a total of 10 million samples and took our quantum processor about 200 seconds in order to execute and I can summarize the last data point it has 53 qubits 20 cycles over a thousand single qubit gates and 402 qubit gates we predict from our model a fidelity of 0.2% which is remarkably consistent with what we observe for our simplified circuit I'll mention as well that this data is all freely available online at this URL the last chapter in this story is the classical competition and an important lesson here is that all known classical algorithms to perform this task require memory or runtime that is exponential in the number of qubits in fact they all require exponential run time and in order to test this and see how well we're performing compared to classical supercomputers we actually ran benchmarks on several supercomputers like these pictured here there are a couple of algorithms that I want to highlight that we considered one is the Schrodinger algorithm this is what you might come up with if you understand quantum mechanics and want to run a simulation of a quantum circuit basically you put the full wave function into the memory of your computer and then do some linear algebra to do it's unitary evolution and this is pretty rough because it has a memory requirement that is exponential in the number of qubits in this case it would hire about 65 petabytes in order to run this computation on 53 qubits which exceeds the memory of any computer in the world I also mention that the run time is exponential in the number of qubits although it is linear in the number of cycles another algorithm that we explored is the Schrodinger Fineman algorithm and this is a really nice algorithm because it allows us to trade-off between the memory requirements space and the run time and instead of having the full system in memory we just have subsystems in memory at any given time and basically what we do is we see how much memory we have access to and use as much of it as we can in order to make the runtime as short as we can and in the case of solving this particular computational problem we're using around a petabyte of memory which is a huge amount but something that's totally achievable for a supercomputer but this trades off with the runtime we estimate that to solve the same problem that our Sycamore processor did in 200 seconds would take thousands of years and this runtime is also exponential now in the number of cycles that we perform and let me emphasize here that these numbers these estimates are based on real benchmarks we performed using our quantum circuits doing subsets of them on the summit supercomputer so let me conclude now we looked first at the Sycamore processor and the crucial hardware infrastructure that makes these experiments possible we then reviewed how to calibrate and benchmark this processor and compared it to the world's fastest supercomputers to demonstrate quantum supremacy I want to thank one more time this this huge team that worked so hard in order to make these results possible and I'm really honored to be a member of this team and to share these results with you today thank you [Music] 