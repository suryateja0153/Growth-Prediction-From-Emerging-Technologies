 Welcome back to another AI 101. This month we are focusing on quantum machine learning, including building our own quantum machine learning model in TensorFlow. If you haven't seen my quantum computing video already, I will leave a link in the description so that you can check it out, but we'll also be giving an overview in this video so you can get up to speed. If you're new here, I'm Jordan and I'm a PhD student who is fascinated by the ways that we interact with artificial intelligence and algorithms on a daily basis. So if you want to keep learning with me, you can subscribe to my channel. Also, I'll be doing a belated live Q&A for reaching 10,000 subscribers this Sunday at 2:00 PM Eastern. So you can leave your questions in the comments or you can add me on Twitter. Let's get into it starting with a crash course in quantum computing. Actually, let's start with normal computing. In order for computers to communicate with other computers and within themselves, they use bits, which are long strings of ones and zeros. This is called binary code. These bits are usually encoded in electric or light-based pulses, and everything that is on your computer and everything that your computer does can eventually be reduced down to these bits. The important part of this design is that a bit must be one or zero at any given time. It can't be both, and it can't be neither, and it can't be anything else. This design is important because it means that we can write explicit instructions for computers and know that they will do the things that we're asking them to do. Things are a bit different in the quantum realm due to one of the fundamental principles of quantum mechanics, superposition. Superposition states that you can add quantum states together and break them apart into distinct other quantum states. The second part is particularly useful for quantum computing because it means that a quantum bit or a qubit can be broken down into multiple quantum states. That is, it can be both one and zero or neither at any given time. This property lets qubit store and process information much faster than a bitcoin. In conjunction with another quantum property called entanglement which lets us connect two qubits together such that we can alter one and it will change the other in a predictable way, qubits can be used to solve mathematical problems or train machine learning algorithms that might otherwise take hundreds or thousands of years using classical computing in seconds. Demonstrating this is actually possible. It's called quantum supremacy, and Google actually did it in 2019. Once our problem is solved and our algorithm is trained, quantum computing lets us turn qubits back into bits and vice versa by periodically observing the qubits to see what the output is. Now, we're not going to be achieving quantum supremacy in this video primarily because most of us don't have access to quantum computers. They're very hard to create because qubits actually lose their quantum properties when we observe them repeatedly due to something called decoherence. This can happen mid-training, which will result in errors in whatever you're trying to compute. Decoherence can be mitigated by keeping qubits extremely cold or by protecting them from vibrations. But researchers are still working on ways to create quantum computers that can stay quantum for long periods of time. In particular, consumer quantum computing would be very difficult to achieve because I think I can speak for most of us when I say that I don't usually keep my laptop or my phone in freezing cold storage with no vibrations. However, the expansion of quantum computing is particularly interesting for the field of machine learning because it would allow us to train larger and more complex algorithms than we ever have before. This might approach something like artificial general intelligence, although it's a bit hard to say because we don't fully understand human intelligence, and so there's no clear benchmark for when we get there. As an aside, if you'd ever like to see a video on artificial general intelligence, let me know in the comments. In the short term meaning in this video, we will focus on simulated quantum computing. Now, Amazon is actually preparing a service that will allow you to use quantum hardware and do simulated quantum computing. However, it's not currently available to the public. Instead, we will use TensorFlow's new code library for quantum computing to develop our own quantum machine learning model using one of the tutorials that they provide on the website with a few modifications for me. I'll be using Google Colab for this. So if you'd like to follow along, the Colab link is in the description of the video. Before I get too far into it, as I mentioned earlier, this tutorial is primarily from the TensorFlow tutorial website, so all credit to the original developers for creating this. My goal is to walk us through this so that we understand what we're doing and to perform a couple extra experiments at the end so that we can extend this problem further. As usual, we'll start by loading our code libraries into Colab. While we do this, as I mentioned earlier, a lot of this is from the TF quantum website, and I actually highly recommend looking through some of the other examples that they have, including classifying MNIST using quantum computing, which we've actually done using classical computing in an earlier video, as well as developing a quantum convolutional neural network. The second example in particular is interesting because you'll see that the quantum network performs worse than the classical one does, and that's because to run the images through the quantum network, you have to downscale them so much that there's not as much useful information as the classical network gets to use to do its training. Also say upfront that this is not an intro to quantum physics video, so if you're looking to learn more about the fundamentals of quantum physics that I'd highly recommend taking a course on the topic potentially brilliant. This tutorial focuses on a simpler problem which is developing a neural network that can calibrate a simulated qubit so that it outputs the correct value of zero or one. The qubit will be instantiated in a way that it initially results in errors. Our goal is to train a neural network to create the parameters for the qubit that fix those errors. This is called hybrid quantum classical computing and solves a simple, but fairly useful problem in quantum computing, which is making sure that your qubit eventually outputs the correct number given a particular input. Now that everything is loaded in the Colab notebook, we will start off by defining our quantum circuit. Quantum circuits allow us to perform quantum computations by inputting some classical number, mapping it to a quantum state, performing that computation in the quantum space, and then measuring or observing the output of the quantum computation in order to get the classical result. Again, this isn't a quantum physics video, so we will leave it at that. We can define our quantum circuit using Cirq, which is a code library designed for quantum computations, and we can see what it looks like here. So now that we have our quantum circuit, we want to develop the neural network that will control it. This is a pretty simple neural network that consists of an input, a hidden layer of size 10, and then an output of size three. The three outputs that this network provides correspond to the three parameters that we set in the original circuit. These are the values that we're looking to optimize using our neural network. So we have a controller and a quantum circuit, and now we need to connect them. To do this, we'll create inputs for both the controller, which is the command input here, as well as for the circuit, which is the circuit input here. We can then connect these two models by creating layers for both the neural network itself as well as the quantum circuit here. We can combine them using the traditional way that you do it in Keras. So if you're familiar with that, then you can see it here. We connect the two models by creating a layer for the controller, which you can see here. Another layer for the quantum circuit input and then connecting them using a parameterized quantum circuit, which trains the quantum circuit we developed earlier, and you can see that here. To summarize in this useful picture from the original tutorial, we have a command input here that sends the value that we would like to see into our neural network, as well as the circuit input here, and we're looking to optimize this output so that we get the correct output here. Now that we have our model, it's time to create our data as well as miscalibrate our circuit input. Both of these are pretty simple. All you have to do is create two data points here so that we can map our input values to our outputs. Similarly, miscalibrating our circuit input just involves setting the Theta values to a random number between zero and two Pi. We also want to make sure we have two copies of this circuit so that we can create Theta values for both inputs, and we're all set to turn our model. So if you've used Keras before, if you've seen some of my other tutorials, then training and plotting the loss works the same way. So we can see how well the model has converged down here. But, does our qubit produce the correct output for our input data? This trial actually comes with a function that can check that for you, which is super useful. Here you can see that the qubit comes pretty close to matching what we expect it to get based on our desired input and output data. It doesn't exactly match the output values that we wanted, which is the expectation, because the quantum circuit is modeling a probability distribution. So there's always some chance that the value is something else. But what if we wanted to train this qubit to learn more than two data points, what if we wanted to train something in between one and negative one or in between one and zero. Well, I do this here, which you can try yourself by changing some of the values that are in the training data. You can see that it actually learns this pretty well too. So that's your intro to quantum machine learning. Hopefully, this was useful to all of you all, and if you'd like to see me do more videos on quantum computing, then you can let me know by tagging me on Twitter. Otherwise, thank you so much for supporting me on Nebula, and if you'd like to catch up with more in my PhD life, you can follow me on Instagram or Twitter, and I'll see you guys next Friday. Bye. 