 so i want to introduce uh three uh great friends and scientists uh um uh professor nancy kanosher professor tomasser and professor uh josh tenenbaum we're going to conduct this in a similar fashion to our panel on monday so each of the panelists will have a very short answer to this question uh first and then they will uh initiate the conversation and the dialogue with so without further ado uh thanks again nancy toma and josh for participating here and uh the podium is all yours so i thought i'd just start with a couple minutes on my uh immediate take on this question and then we can get lots of other perspectives in your questions so starting with a question of what is the relationship between biological brains and ai um it strikes me that really we need to ask the relationship at what level uh and what i'm referring to is mars levels of analysis which probably many of you already know about but in case you don't uh mar made the very important point that when we want to understand a complex system like a brain it's very helpful to think of the question of how it works at three different levels the computational theory which really considers the nature of the problem that it solves what are the inputs what are the outputs could you get those outputs from those inputs is it an ill-posed problem that kind of thing the actual series of computations that happen step-by-step computations algorithm that runs the representations that are extracted uh as at the second level and the third level is the actual physical hardware of the system so i think if we think of this question at these three levels my perhaps idiosyncratic take on this focused overly on trying to understand uh brains by using ai is that at the level of computational theory really the questions are the same the whole point of computational theory is it's characterizing the problem to be solved and it doesn't matter if that problem is being solved by a human brain or an insect or a deep net um but so by definition the computational theory of a given task or problem is the same for all these systems but that enables us to ask this cool question which is when an ai system and a brain does the same task will they necessarily be running similar algorithms or when will they be running similar algorithms um and um and so then you can ask at this level of the the algorithm representation level we can ask well okay are they running similar algorithms and the cool thing about that is that is now a really tractable in some cases empirical question so we have all these nice methods now we can compare representations in brains and representations in deep nets using representational similarity analysis or regression methods or lots of things you've probably heard about in previous lectures to ask how similar are these things and there are already some really impressive findings like the paper that i'm sure jim dicarlo told you about the yemen santa carlo 2014 paper um showing that when you train deep nets to do visual object recognition tasks you find a remarkable similarity you know to the at the to the degree of different levels of the deep net corresponding to different stages of processing in the private primate brain and that's quite astonishing it very much didn't have to be that the deep net and the brain would come up with at least somewhat similar um kind of strategies for visual recognition i think this is a really important paper but it's not just vision where we see this uh josh mcdermott's lab has been looking at um optimized solutions in deep nets for auditory perception problems like speech and music and also finding similarities to brains and in a paper that from ed fedorenko's lab that josh is also involved in they've been looking at natural language processing models optimized just for doing various language predicting task prediction tasks and finding a remarkable match between um the behavior of those the representations in those systems and the brain's response to language so in at least some cases it seems like there is a real similarity here uh and of course those systems don't match perfectly okay this is more than one minute but i'm not gonna be much longer uh they don't match perfectly but that's not a problem that's an opportunity it tells us okay there's a mismatch here there's something different and that starts to give us clues about how to think about how they differ how to make them more similar uh etc and then at the level of hardware well obviously ai systems and brains are totally different in many ways at the level of hardware but now is that a problem does that mean that these systems are not useful well no i think it's actually a strength in many ways if you find that despite this very different kind of hardware between say a deep net and some a brain solving a similar problem uh if you find nonetheless that the representations involved are very similar that tells you that actually for this set of computations this is solving this problem maybe the hardware doesn't matter or sorry if they're different maybe it says they do matter and if they're similar maybe it says that you can get the very same uh or you know very similar algorithms running um despite very different hardware and i think that's a really deep idea and i think that's really the idea that mar was talking about or a version of it when he said and now my screen is occluded just a minute he's he said the nature of the computations that underlie perception depends more on the computational problem than on the particular hardware in which their solutions are implemented so i think you know the the translation to to kind of modern ai modeling here is that an ai system optimized for uh for doing a given task will necessarily have some similarity in how it approaches that task to help brains approach it and i think uh ai systems enable us to test that very interesting hypothesis okay i've talked too long i'm done now i would agree with almost everything i think nancy said i could add a couple of remarks maybe and then if you want to go tama and then we could just all uh not that you need to respond to that that was just what i had to say no no i wouldn't know but i mean i i i share a lot of those basic uh ideas i guess i would just say that um you know i think maybe implicit in what you were saying nancy was you know you pointed to three examples of ai models which are all a certain class of deep neural networks and i think implicit or maybe even explicit is that what what makes these models particularly exciting is that there's some architectural and maybe kind of you know circuit level motifs that they have in common with the brain that's certainly true in the ventral stream models for the yemen's de carlo at all models and maybe in the mcdermott models for auditory cortex but like the work with um martin shrimp and federenko and language models which which nancy is also part of both she and i are very small parts of that bigger collaboration i'm the cheerleading section for that project okay but anyway like so they're just um i don't know if anybody's gonna talk about that work but you can if in case nobody does it you can read that paper on bioarchive or a version of it but that's a really interesting kind of challenging case because what we show there is that the the kind of predictive neural language models that have been all the rage in nlp specifically the gpt2 class of models which are these generative pre-trained transformers from open ai provide really remarkably good models of the language processing in the human brain in in terms of being able to like explain variants in fmri and ecog signals e cod being intracranial recordings in humans okay um so that's very cool but as a number of people have pointed out um and for those again probably somebody will talk about these class of transformer models for language if they haven't already um in the summer school many of you might know about them but you know they the relation between what goes on there in terms of the quote neurons of the neural network and what goes on in neurons in the brain is much less clear right many people would say they're obviously not like what the brain is doing there you can call them units in a neural network but the way the architecture works is obviously different from the brain to me i don't think it's so obvious it's just not so clear right so it just raises open questions like how brain-like are these models or maybe they raise questions that like we don't really you know i would say and i think tomorrow might comment on this too i know he's worked on this um we have a certain kind of canonical standard model of what neurons do and what networks of neurons do in the brain and how that plays out in math but like that is by no means true necessary like we don't know that that's that's that's what neural computation is really like there could be all sorts of aspects of neural computation that that standard model doesn't take into account whether it has to do with what goes on inside dendrites or in the temporal properties of of how networks of neurons fire together or just other aspects of neural computation that we don't know and maybe the fact that these models let's say these very large transformer models which some would say don't look like what the brain does maybe there is a sense that they are more like what the brain does and you know i'm not saying we know that i'm just saying we should be asking those questions and then one and then beyond that i would say and the people who don't know the work that i do will see more of this when i present on friday but you know there's important aspects of cognition to me many of the most important that there are no neural network models of at this point there are other kinds of ai models that involve symbolic and probabilistic generative models defined on symbolic representations which fit very well what the brain does at the software level but we don't understand and they predict behavior very well and they perform very well in an ai setting um better than neural network models but they don't we don't know how they could be implemented in neural network terms they're you know the brain is a neural network and they work if we think they're right they work in the brain but then you know that it's just coming from a different direction raising this question that we still don't know how key aspects of computation like a representation of an object or a symbol of a concept we don't know how those things are represented in the brain so we we you know to me that's just a big open question right is we can capture those we're making progress on certain kinds of things on the ai side but not in neural network form so it raises the question of what those look like in the brainstorm networks yeah so i i agree with obviously what nancy and josh have said uh maybe if i had uh to expand a little bit on on one aspect i would you know my view on uh the relationship between biological brains and ai is is uh relatively complex and i think if we look at the history of in the past that neuroscience and ai have followed um there has been some point where uh ai and cybernetics uh somewhat uh joint forces with neuroscience and there has been times where the two have diverged um so i think the the the question is still uh worth asking um uh uh i i think uh part of the answer will depend on on on the you know essentially the goal or goal so i think as a as a neuroscientist um it is uh it seems impossible for for us to understand the brain without any form of mathematical theory or ai as a guideline and a way to to formalize um a question to be asked about brains um so to me there is no question that you know neuroscience cannot proceed without ai and computer science and and math uh i think the reverse is not necessarily true uh uh i think it's possible that ai will uh you know mostly diverge from neuroscience and that's probably okay but i think this is one of the points i think we're going to get back to someone was asking about other forms of intelligence the the the challenge we have i think here is that at least to my knowledge i can i was trying to come up with an alternative form of intelligence besides brains and and i couldn't come up with uh with any uh because i think grounded in the idea of artificial intelligence is is uh biology biological like intelligence as a goal um and and so uh it could be that the two are so intertwined that ultimately i will have to follow somewhat of a biological path and as you know nancy pointed out there is uh already overlap obviously complete overlap at the computational level uh there's partial overlap and potentially interesting differences at the algorithmic level and so the question will be uh how much overlap will we ultimately end up with towards adi what do you think about the role of inductive biases in a ns based on brain architectures i mean it seems this is from miguel nunez i mean it seems that the architecture of a cnn is closely related with the architecture of the visual perceptual stream and those models have fairly good performance in visual tasks but do you think we should be looking in those inductive biases to produce new models so so meaning like should we be looking for the brain to give new inductive biases for a ns or i guess we could also be looking at inductive biases that have proven valuable in a ns and see if those are in the brain but it sounds like it's going more from the brain to providing inductive biases for a m yeah so i like this question a lot um and i think um you know i think we can all agree that uh you know very wide very deep fit for neural networks are universal approximators um so with enough units and enough training data they'll be able to learn any arbitrary input to output mapping so in theory there's no no visual so i'm in vision by the way so when i when i speak of ai everything should be taken with a grain of salt and you should you know replace ai by vision because this is all really all i know um but when we when we so going back to our feed forward neural networks there is no task that they wouldn't be able to learn um uh and and and so i guess uh uh wait i have to interrupt do you mean no classification task or there's lots of visual tasks that are very subtle and sophisticated and that that the kind of thing that josh did you just mean like a universal approximator you're saying because in the universe right so i guess it's hard for me to think of a task that would then be formalizable as a input to output mapping uh now this is what i would also refer to as the brute force approach uh and and and i think to me what's really fascinating about brains is their ability to learn so efficiently and so um you know in that sense i think about and you know josh was talking about transformers early on um if we take a black box very deep neural network there is no necessary something like attention there is not something maybe explicit as working memory there is not necessarily something as explicit as uh um you know psychedelic system and so on and so forth and so uh what becomes interesting is whether uh and and we know that many of those tasks uh potentially require some form of memory some form of attention at least as we've studied it from the neuroscience perspective and so to me what's fascinating is whether when we train those neural networks those those functions or those neural computation will emerge spontaneously uh and and perhaps if we had you know enough samples and enough uh you know human-like or biological like visual type of diets or experiences maybe they would but it's very likely that given the ecologically invalid ways in which we train those network it's it's unlikely that those uh functions will arise just through training and so i think going back to the question uh by miguel i think this is one one uh potentially interesting involvement of neuroscience in that some of those functions that we know are relevant uh at least from the biological perspective for solving those tasks can potentially be thought as inductive biases uh and and it's very likely that if we want those networks to uh solve tasks much more efficiently and in particular learn to solve novel tasks much more efficiently those types of function will certainly those functions that i think of as the building blocks of neural computation will become uh quite uh useful now again not to say that this is the only games in town but this is certainly one solution that we know works at least for for biology yeah but i just add that you know there are lots of different kinds of things that we might mean by inductive biases and lots of different kinds of things would end up being biased but here's another example you mentioned attention and working memory and stuff like that but um in my lab my postdoc katarina dobbs has been looking at this question of why human brains have functional specialization at all yeah sorry hey nancy someone has a really good question which is on the chat can someone please define inductive bias i think we just skipped over that yeah um you want to basically we mean something like in a learning system something you do that biases the function that is learned in a certain way that isn't just in the data so it could be like the architecture or it could be a prior if you're a bayesian um it could be some pre-training on a different data set that isn't the thing that you think you're learning on for this task those those are the kinds of things yeah exactly okay so the case i'm talking about here is like or some extra loss function smoothness criterion yeah okay good yep yep so you know braids have specialized regions for face processing i and lots of other people have been studying this for a long time the question is why do they have them and how do they arise and it's very tempting to say look this is an important thing it's been important throughout human evolution maybe you have to build in something a special learning mechanism we have to do something which you might call an inductive bias something that you build in to make the system set that up so what katarina finds is when she trains vgg on both face recognition and object recognition has it manages to learn both tasks well but when you look inside you see that what it has done is spontaneously segregate the network increasingly at later stages into basically a face processing pathway and an object processing pathway so lots of people formerly including me who might have thought look if you have these separate systems and adult brains there must be some bias that tells the brain to treat them separately but this suggests that in principle at least it's possible that a kind of very kind of generic learning system that learns to categorize is going to spontaneously segregate its machinery so i think there are many versions of this question but one kind of thing that you might have thought would require an inductive bias maybe doesn't i think there are there are other cases um yeah i mean other so so yeah other kinds of inductive biases that where we by studying the brain were gain insight for ai models i mean i think i can point to two one is um that several other people have already asked about in the q a you know i guess what you could call either feedback or recurrence or top-down connections those aren't the same things but they're sort of related it's things that are not just a feed-forward classifier basically and you know i think that we we are far from understanding how that works in the brain we know there's a lot of top-down connections we know in various ways that feedback and recurrence is important leads to better models both functionally and physiologically and you know i mean i think ai people sort of figured that out too i don't know that they necessarily were inspired by the brain but there's all sorts of ways in which you know by incorporating either it could be either by incorporating for example generative models in the loop for a vision system or by having recurrent things for example an architecture that judge and wu and colleagues i was one of them had a couple of years ago it was a neural net that that learns to go from 2d images to 3d volumetric reconstructions it was called mar-net named after the same mar that nancy was talking about and in the spirit of mar it had a kind of intermediate representation of what mar called the two and a half d sketch but like a a in between a 2d image and a 3d volumetric reconstruction it had something like a map of surface normals and that by putting that in there that inductive bias it made the system better at 3d reconstruction but also there were feedback connections in which the 3d volume was was forced to uh to produce shape whose visible surfaces were compatible with an estimate of the surface normals and that that loop made the system better also so that's an example of something that for us was inspired from the biological side and helped to make the the system better and one other example which is not which is um which comes from behavioral studies is the idea of objects right so again you'll hear more about this probably in other parts of the the um the summer school and certainly in my lecture on friday but the idea that from very early age even very you know babies who are just a couple of months old if not much younger um see the world not just in terms of patterns in pixels but actually physical objects like we sometimes talk about in object permanence but like a thing right like even if you don't if you've never seen a glasses case before you know what this is like i'm holding a thing and it's in one hand and now i toss it to the other hand and when i drop it it goes out of sight of the screen but you know it's still there it didn't just disappear right i can put it behind my head and it's gonna still be there and there it is again okay so the idea that the world is made of objects like that this is different from how we recognize objects and classify them which is a lot of what we were talking about before but the idea that the world is made of objects that seems to be something that humans and many other brains are built with and on the ai side putting in taking neural networks and get making them object-centric where basically they don't they analyze complex scenes or physical dynamics not just holistically but by having separate representations of the objects and their interactions has made them much better and it was definitely inspired by our understanding of the object-centric nature of human scene understanding and intuitive physics so that's i think another great place where biology influenced ai but it wasn't via the architecture because we still don't know how those object representations work in biological neurons rather it was based on behavioral studies and computational cognitive models which elucidated the importance of those representations and then that's what the ai was building on yeah i mean to me with a you know following up on on on josh uh to me the real question is whether you know um these inductive biases need to be kind of baked in or you know somewhat they would be discovered with the proper kind of visual diet right so one of the issue that i see in modern ai especially in vision is you know we know that uh cnn strain on image net learn mostly textures whether it's texture of the objects the background and to some extent um it's not surprising given that they are trained on you know iid samples independent images and so on and so forth there is very little information and besides you know statistical irregularities that can be uh learned by these these architectures and um in particular for the case of foreground versus background the only way this networks would be able to properly learn a model for the foreground versus background would be if the foreground and background were independent that is that you know objects would appear on a completely independent background and we know that's not the case we know that objects are especially on the internet images are presented in context and so there is no way in the world that these networks would be able to learn any kind of notion of object based on the the images that are used to train them um so one option as you pointed out josh would be that yeah maybe we can just bake in an explicit representation and the sense of objects yeah but you know babies also learn very differently and and we know that there are you know babies learning a much richer visual environments uh uh babies have access to stereo you know disparity information we know that motion information gives a uh amazing cure for uh for one versus background right in fact even worked by palancina at mit has shown that uh uh uh adult uh uh patients who have uh recovered uh uh recently recovered from impaired vision uh seem to be recovering primarily uh the ability to recognize moving objects as opposed to static objects and the the assumption is that that has to do with the fact that moving objects are much easier to segment out from their background and so i guess to me with the interesting question and i know that there's a lot of work uh in this area a lot of work motivated and uh arising from uh developmental uh cognitive science labs with more realistic data sets that assets involving maybe video sequences of uh either object manipulation or observers uh embedded in in the environment uh maybe even stereo uh that to me will become an interesting kind of testbed to study how those networks are able to uh whether they do learn some of those uh inductive biases um from data because those biases are are inherently in the data or whether they need additional kind of help whether it's the evolution that our brains had or any kind of uh yeah yeah i mean what i love about this is that that that deep nets give us a kind of empirical way to test in principle what you have to build in to learn something right so so you know when i was in grad school chomsky was carrying on about the poverty of the stimulus and you know everybody believed him because chomsky posited it chomsky said okay there's not enough information there to extract the the unique syntax of your language therefore there must be stuff that's innate never said okay chomsky says true end of story but now that has become an empirical question you can train deep nets on different kinds of inputs with different kinds of architecture and ask the question of what those things can learn and people like tallins and others are doing this kind of stuff nancy i mean i know you're in love with deep nets and it's great but like that's also true for just machine learning in general people have been doing that for decades and um i think the difference that you didn't used to work i think that's the difference that's a lie that's a lie i'm sorry i mean that no i mean that's ridiculous as well well certain things didn't used to work as well certain things work much better language processing i mean language processing josh i mean like 10 years ago language processing yeah well i mean again i just i think that um it's just important for for people to understand that uh there are certain kinds of i mean again there are certain kinds of things in language processing um i i'll i'll i'm not i wasn't planning to talk about language processing but i gave a keynote at this year's acl conference which talks about this there's certain aspects of language processing which models like gpt2 do better than anything but there are other aspects of language processing that engage more with meaning and where you um you know you have to understand like um the the content of verbs and how they ground and perception that you know our best models and don't involve i mean they they might use neural networks in some ways but they're not the gp gbt class of models they're ones where key things are done by other technologies that that existed well before so this isn't a language panel so i'm not going to go into it here but i just think it is important for students who are new and especially given that there's a strong vision focus in this group at cbmm that there's a lot of there's a lot of things in intelligence and ai besides vision and visual object recognition and a lot of ways to ask those questions so anyway um yeah i mean it wasn't yeah my point was just that if you have a system where you have an opportunity to use kind of brute force training from a you know a big diet of input it enables you now to ask for at least that set of systems whether it is ever possible to learn x from input y and that's cool like it used to be just speculation before now it's something that you can ask sure sure but i just i guess again i i'm not just trying to stand up for other um machine learning methods it's just that if those systems there are some things those systems just don't learn like they don't i mean the neural language models you're talking about they don't actually reason they don't form a semantic representation okay they form some kind of statistical summary of a semantic representation but like they don't understand the concept of negation i was just playing with jesus earlier today and so no but it's just don't like they're not it it would be a false it's it it you don't want to be constrained like maybe what i'm trying to say is the point you're making is even more powerful than i think you're you're casting it as because if you want to ask what can be learned from data versus has to be built in then you should just be willing to like you should recognize that there's ways of learning and things from data that deep networks are not very good at but other methods are good at and so it's important to be able to ask those questions with the full tool set and not just be constrained totally to one of them okay um yeah so um there were a few questions that were that were in here and we don't we could spend the entire time just um talking about uh one thing but um well let's see so okay so one has now they were there when i glanced a minute to go well the order has changed but let's go with the one that's on top if that's good for people yeah um the biological brain can perform an increase from carolyn malin mayer the biological brain can perform an incredibly large number of tasks while current neural networks are often trained for one or two specific purposes how close or far to the biological brain is the idea of creating separate networks for separate tasks and linking them with some sort of scheduler how can we take inspiration from biology to make an artificial system that can perform many different tasks good question yes well by task caroline i don't know whether you meant uh so you mean task uh very general tasks spanning multiple modalities or within i might add it to different visual tasks because i think the answer might be slightly different yeah i mean and i i you know i would say for you know for the visual system there's clearly one visual system and one neural networks right so there are different streams there are different you know subtle but uh it's hard for me to imagine and i think that's that's probably one of the issue we've had in the field is that we've been separating all of this different visual recognition problems somewhat separately and uh both from a neuroscience and from a computer vision perspective and you know clearly we have one visual system that is capable to solve you know uh almost any random or arbitrary task that that we threw at it and you know josh at those uh amazing experiments from you know early early experiments that were conducted in his lab showing how quickly people can learn novel object categories for instance and that's uh um that's in shock contrast with eva right okay visual tasks segmentation classification but now i'm curious about both um but tomorrow i want to take issue with that i think yeah sure there's one visual system but you know it's a whole back third of the head right and if you look at it there's lots of different bits that are doing wildly different things like processing visual motion is just a very different kind of computation from extracting the shape of objects and it turns out empirically if you look at it in human brains and if you look at it in deep nets trained on lots of things face recognition turns out to be just a very different task from object recognition and so i think that actually both in the structure of the task itself and in the organization of the brain and i'm guessing in the structure of any system that's going to be able to do really sophisticated vision you're going to find quite a striking degree of segregation of function where different parts of the network are going to be specialized for different aspects of the tasks i don't know if you're disagreeing with that no i i i i do agree and maybe i should have been clear i you know segregation of functions to me is not incompatible with the fact that those algorithms run essentially more or less the same neural computations and i think you know what's at least squinting our eyes on on on the rough architecture of the visual system you'll find that whether you look at the the ventral stream for shape processing or the dorsal stream for motion processing you know uh you know almost any anywhere you look in the visual system you'll find at least a course uh anatomical and functional uh hierarchy if you look at the mechanisms that have been discovered in the past 50 years again across streams across task you know a lot of it boils down to some form of of uh of um you know a dinner non-linear model of a neuron there will be some form of divisive normalization there's a lot of overlap between you know the the models involved in motion processing stereo processing and you know contrast processing again that doesn't that that is not to say that we have fully under we understood those functions there's nothing to say that we've solved the problems but i wouldn't be surprised if we find that there is a small number of uh canonical operations uh that that serves you know uh uh similar uh uh goals and importance across across tasks and functions it seems like there's a difference in levels here right at the level of you know what is a non-linear function computed by some unit in a neuron or a network a unit in a deep net um that's one level of computation and sure at that level they're probably shared across lots of things neurons are basically neurons right they they differ in various ways but you know they're very similar um to one another but the representations extracted in a motion processing system and a shape system and an intuitive physics system these are just qualitatively very different so when we talk about same computations we need a little more precision about the kind of levels issue here yeah yeah and i think again i think you are bringing an excellent point and i you know i didn't want to trivialize it by stating that you know it's all going to be you know dinner dinner filtering followed by rectification and normalization i think again if you look at the you know both from a computational and a kind of an algorithmic uh level especially in motion and stereo it's to me it has always been fascinating to realize how much overlap there is between the models at the end of the day whether you're trying to solve the stereo correspondence most of those problems are correspondence problems whether it's finding correspondence in time or correspondences between you know retinal inputs and so again not to say i agree with you i don't want to you know trivialize uh and and say that you know all there is to vision is uh you know three computations but again i think we we might uh superficially underestimate the amount of overlap there is between visual recognition tasks a lot of it is measuring similarities and i hope we'll go back to that because i'll contradict myself probably 15 minutes from now um uh by arguing that there is much more than you know similarity and template matching to vision but at least for low and mid-level vision i think a lot of it will find uh is is based on a small set of of canonical circuits that solve computationally very similar similar tasks that run very similar algorithms uh that yes might be uh uh you know carried in in separate streams uh but to me that's uh i think that the correspondence um yeah the correspondence problem in both motion and stereo was a particularly interesting one to me because those are the two things that are computed in visual motion area mt it does both of those things and i've often thought that was so interesting is that because they're both correspondence problems albeit very different ones i don't i don't know the answer to that right that's true that's a good point yeah yeah i think so i mean yeah so i think yeah sorry go ahead josh no no go ahead i was going to go back to the question but um no i mean well i just i think it's that you know the word task means so many different things uh well the question just went away but um oh no a different question got upvoted beyond it but sorry um but when caroline asks the says the biological brain can perform an incredibly large number of quote tasks yeah i mean i guess i i can't help but thinking about different kinds of cognitive tasks or then when you say how how close or far to the brain is the idea of creating separate networks for separate tests and linking them with some sort of scheduler yeah i mean i i think that the notion of task is a is sort of a machine learning construction right it's a very important idea in machine learning a task is defined usually by a data set and a lost function and a training and testing regime but a lot of what human cognition does isn't doesn't really fit into that right we solve problems we have goals those are kind of like tasks but often we don't learn them we make them up or somebody suggests them right many of the tasks that i perform i perform because somebody asked me to perform them or suggest that i might or maybe they didn't i just thought about it right so you know i could set myself the task of um cleaning up my yard or i could set myself a task of seeing if i could dig a really deep hole in my backyard i don't know why i want to do that but somebody might do that right or seeing you know how many small holes could i dig in my backyard i mean it's too hot right now to do any of those things but those are you know like i can do that though if if i said okay well i've decided to um to see how many holes i can dig in one day in my backyard i could set that task for myself but and somehow i would i would compose together various perception action planning capabilities in order to achieve it so i think it's it's really very interesting to try to understand um how brains are able to compose together the different things that they have learned how to do to do an a really quite you know endless open ended set of uh what chomsky i think very rightly called the you know the infinite use of finite means not just in language but in behavior more generally i think that uh you know neural networks or any kind of machine learning approach has not really taken us very far to understanding how the brain is able to do that maybe we could go to the question the the one that's uploaded sharon chen's question it's a good question yeah that looks great um so sharon says just like how neural networks are trained analogously to how humans are educated throughout their lifetime can training of neural networks mimic evolution what are some strategies that have been used to mimic this evolution to build inductive biases and then there's a clarifying remark from sharon isn't the goal of ai to be able to learn some form of intelligence from nothing okay yeah so what do you guys think well let's take the first cause just like how neural networks are trained analogously to how humans are educated throughout their lifetime i think a lot of us here would sort of disagree with that at least the standard current uh you know supervised training with millions of labels we might say that today's neural network learning is more like actually like evolution right i mean when pressed a lot of people who use neural networks to study um where inductive biases come from and to try to learn something from nothing would say that it's it's maybe more like evolution than it is like biological learning maybe not so much in vision but especially like when people are doing deep reinforcement learning a lot of those kinds of things are even almost explicitly evolutionary in that you create lots of agents they sort of all go in different directions then you see what was the best that you kind of create a new generation of agents and you do that many times over for example it's not necessarily the way people approach like deep learning and vision but a lot of the ways that people have tried to use ai learning systems to learn something from nothing have looked at least as much like evolution as like learning that's like what biologists would call learning and where do you guys what do you guys think about these new um unsupervised models like the the recent yemen's paper um which suggests that you can get a lot of the same uh performance uh in in visual recognition using uh much more generic learning rules like basically just trying to what do you mean like self-supervised contrastive losses you mean like yeah like yeah yeah yeah i mean there's i mean it seems well it just seems a little bit a little bit closer to to the kind of learning regime of of infants right we're not every damn because it's not supervised right yeah yeah but you know we were just talking about task i think so so again i think this is fascinating and and again i you know this goes way beyond anything that i would have imagined if you had asked me just a few years ago that said i think uh going back to our tasks i think those uh kind of problems and learning regimes are tailored for image categorization for you know pattern matching type of algorithms um you know when i think back about you know woman and you know the some of the point that josh was just making about the the visual routines and how we are able to compose and learn you know on the fly an infinite number of visual reasoning problems it's very hard for me to imagine how we can kind of continue to scale up uh self-supervision and things of that sort to get to systems that will really learn you know from a handful of training examples i mean maybe just to to to mention some of our own work some of the work we've done as uh and related to uh uh uh uh inductive biases and and recruit neural networks uh our work a few years ago started building computational neuroscience models of classical and extra classical receptor fields phenomena very well described in electrophysiology the classical receptor field is the part of the visual field that if properly stimulated will typically see the response from the neuron the extra classical receptor field is typically a region sitting right outside of the classical receptor field by itself is stimulated it will not elicit a response but stimulated in combination with the classical receptive field it will have a drastic effect on the neural response and we're working on the circuits because we're interested in modeling and understanding visual illusions now there's a prior work both anatomical and electrophysiology calls suggesting that uh recurrent connections feedback connections uh potentially observe some of these classical extra classical receptive interactions so our work took us with very classical kind of computational neural science modeling trying to model how neurons communicate within a cortical column and across a cortical column now this is very different from kind of standard cnns that only communicate going you know from lower stages of processing towards higher stages of processing because here neurons are allowed to exchange information horizontally or so within a processing stage or from a high level stage onto a lower level stage um so we we built those those circuits uh and and found that they were able you know we could uh uh work with constraints from the anatomy and physiology and get uh models that would exhibit uh and explain most of this electrophysiology studies and then if we literally simulate you know kind of scale up these circuits into models that could uh process fulfilled stimuli they were consistent with a host of visual illusions things like color induction the fact that our perception of a particular color patch is highly dependent on the context or the tilt illusion the fact that that our uh perception of orientation is highly context dependent you know similar effects in motion stereo etc etc and so anyway without going too far of a tangent where this work took us is perhaps to now wonder why is it you know if the visual system why would it be that the visual system seems to be going into so much trouble through those recurrent circuits to uh uh implement strategies that seems to be flawed after all uh visual illusions are kind of edge cases where literally the visual system fails you know uh the chelsea illusion is a deviation thomas you're muted no already no i can hear you oh sorry i guess my connection went out sorry okay sorry um so yeah so these visual illusions are kind of uh edge cases for our visual system there are failure cases and so why would it be that the visual system goes into so much trouble and so the our assumption has been that of course you know uh the visual system doesn't do anything by chance and so perhaps many of these uh visual illusions are byproduct of strategies to build uh and to solve uh uh object uh perception and object constancy so that perhaps you know the same mechanisms that would uh yield uh color illusions would be responsible for ability to solve color constancy seeing colors irrespective of the illuminate that maybe uh the tilt illusion would be a byproduct of strategies that are optimal for detecting edges in natural scenes where there are you know uh all kinds of inductive biases that might actually help the system to solve those tasks um so anyway the going back to the the question what i think is uh i think this was one of the questions how to discover this inductive biases i think neuroscience at the mechanism level might be able to uh and extra physiology might be able to allow us to decipher maybe basic mechanisms that will be able to help us solve more complex uh uh visual tasks solving much more efficiently with fewer training examples uh and so on and so forth yeah i think so do we do we think we've covered most of the ground on the evolution so i mean there's there's a lot to it like i mean again sorry so maybe if i was not clear the idea of evolution is that perhaps neurophysiology might not tell us necessarily something about evolution but the mechanisms that are at play could be deciphered in your physiology deep neural networks and i think yeah i mean i think i think the kind of study that nancy was talking about that katarina did with um you know showing that a deep network kind of spontaneously develops a face and object that could as much be a model of what happened at evolution right as in what happens in biological learning especially given some of the evidence that like some of that stuff is in there in very young babies brains but it could still be compatible with that right yeah so um i think it's also the i mean another another um challenge but really interesting open work area i think is that if you want to make artificial evolution and learn about where inductive biases come from and how my intelligence arises from nothing you know um there's a sense in which deep learning in networks and deep rl is kind of like evolution but there are some important differences the mechanisms evolution involve in some ways much deeper kinds of structural modification right like most ways of training a neural network don't change the fundamental architecture that much like you have the kind of result nancy sited where you have one big network and it kind of carves out two channels through it but that's different than like introducing recurrence or introducing like 100 new layers or introducing a self-attention mechanism or any number or or a memory buffer or any number of other things that evolution might have introduced into brains yes that well but let me just finish those those things are always introduced through cultural evolution if you like like human researchers say oh i wonder if i could do this what about this like we don't have algorithms that do that and they involve and that's partly because they involve actually like writing code like humans have to come up with the idea and then they have to write new code and we don't have the kind of algorithms that are used for artificial evolution even things that are sometimes called neural architecture search they basically search within a set of existing kind of architecture motifs turn switches on and off they don't like fundamentally write new code but for human and for biology it's all about code the code starts with your dna and then how that's interpreted and executed through the course of development and all throughout your life and evolution can modify all those different chunks of code that give it they give it a much much richer space i mean almost a universal space to be able to explore so i think if we want to pursue ai simulated evolution we're going to have to be pursuing approaches that are much more able to like kind of write new code basically than just explore the weight space of existing architectures okay i think that's super interesting but and i don't write code so i don't i don't know but it seems to me that you know i write papers and often what you do is you don't just start from scratch and then something new you copy and paste and edit right and i'm guessing coders do a lot of that too and i think that is one of the main mechanisms of evolution so yes evolution does invent cool new qualitatively different things but most biologists think that the the way it does that is to copy paste and differentiate as one of the very basic mechanisms so it could be the actual mechanisms of evolution are not you know they're they're not the same as just training a network and changing the weights but there may be a relatively small space of ways to twiddle with things involved oh there may there may be uh yeah and you know i mean the the the analogs between what evolution does and what coding does are very rich um so um that that is certainly one of the mechanisms by which new code is written and new code and biology's written another one is that a lot of um coding is actually writing programming languages right you write a new programming language in which you can then write other things much more quickly and biology does that too right you could say i mean i'm not an expert by any means i'm sure others out on this call are much more experts but what you know my understanding is that for example a lot of evolution in um mammals and other species of you know complex organism of life happens not by like actually um making new genes but turnips are sort of turning on the switches of gene regulation it's all regulation it's all regulations exactly so you can think of that as evolution basically created a new language in which evolution itself is much more efficient it's kind of like we talk about learning to learn sometimes it's like evolution evolves to evolve more effectively and that's that's an that's an analog of like how in programming you have a low-level language and then you write a high-level language like humans don't write in machine code and evolution doesn't work in machine code either machine code is dna evolution created these high-level languages with with primitives that then gene regulation switches on up which is like a human writing in you know python right so there are lots of analogs i think and there's a there's a really interesting space of ai techniques that have yet to be really connected with neuroscience that involve really algorithms for writing code exploring the space of code which includes copy paste modify which includes write new languages and then write code within those and i think those will be increasingly interesting to bring into contact with the biological side so daario de matisse reyes says what are your thoughts about the existence of a backpropagation like or any optimization principle such as gradient descent in cortex could that be possible no so i was gonna say you know i was uh trained with uh i trained with tommy pojo at mit and uh tommy was a close friend of francis creek who wrote this uh very well cited and known paper in nature in the 80s criticizing back propagation and neural networks saying there's no way on hell that this thing will ever be implemented in brains um i think by today's standard there's probably at least three dozens paper describing uh ways to implement backprop various approximations of backdrop in in brains um so i think you know there is there any evidence that that any of those are good are actually right yeah there is no but i think you know to be honest uh you know we know so little about uh the underlying learning we all learn about uh plasticity in their brains we know very little about uh the underlying learning rules um and so i think you know uh all of this many of those uh implementations are fair game not to say that you know the the brain necessarily does gradient descent uh but you know i i think there is no reason why it could not if that's uh indeed uh uh the the best game in town yeah i was gonna say i mean nancy the um self-supervised learning you were talking about i don't know if that's really an alternative to back propagation or not i mean in some ways no it's still used i think it still uses backprop doesn't it they still use backprop yes yes yeah yeah yeah i mean it doesn't it doesn't have to use the same kind of labels but so the error is more plausible form of error yeah but yeah but the updating mechanisms yeah yeah like like tomah i think it's it seems like it makes sense that the brain should have something like backprop or some kind of optimization um but but it is interesting that we haven't really found it too much i mean we have local heavy and learning rules which can implement some of the local kinds of things that backprop needs but the idea that you can really back propagate errors through an arbitrary network and over long ranges there hasn't been much evidence of that that i know of i think it will be very hard to find right i mean well what would the tool be you know you have to calculate some error up here and propagate it down and change this weight i mean so so andrew sacks for example has has some ideas about how to how to do this and you could set up cases where for a certain kind of network maybe for a not terribly deep network under certain situations like like the main error signal would kind of be in one layer or another layer like so so one of the things that backprop does is it doesn't just allocate error or or sort of error signals and tell you where to update synapse by synopse i mean it does do that but for certain kinds of controlled situations in relatively simple networks it will say well you should change mostly you're going to change the last layer or the or the first layer or some intermediate layer and that would be a way to test back prop type ideas if you could set up those situations biologically because then you could go and test for whether plasticity tended to happen in this part of the brain or this part of this part for example it's like you could set up some problems in vision for which backprop might predict the plasticity should happen primarily in early visual cortex middle or late and andrew sachs whose ideas i'm channeling i think he has some ideas along these lines i'm not sure he would he was um he was part of cbmm in the early days and i think he's now a professor in oxford is that right i haven't been in touch with him recently but um his work would be interesting to check out in that in that um direction just to be clear josh there's a lot of proposals i mean prediction networks are another you know you can explicitly have neural network that encode and pass error local years and uh anyway that yeah you mean like a predator resilience yeah no no i'm not citing his work as i'm citing his at his work as an example of trying to propose realistic ways you could test these models but i'm sure there are other proposals for that too um but yeah okay sure should we take one more question depending on how quickly we answer them we can take or many many all right okay i was going to go on to this question from quan juan yes yes who says to be completely hypothetical so possible the computational problems we're solving we're trying to solve are different than what the brain is trying to solve say when we search for the neural substrates of face recognition language processing or intuitive physics separately i don't know who would do that just kidding that's what i do is it possible uh that the brain is actually using different computations for the same problem of ours or use the same computations for desperate uh cognitive problems uh well yeah i think that um absolutely i mean i think what you know when i started off saying i think we can we're starting to get methods to empirically test the degree to which the brain is engaging similar computations to what whatever model is doing and that's exciting and cool and fun and engaging right and so then we can ask empirically are they similar but as for the question of whether the tasks that we're uh looking for in the brain are different than the tasks that we're modeling with ai i think that's just a point where we need to be clear about what we're doing right like we need to just you know not just say uh face perception we just say okay what exactly is the the face task we're talking about before we go comparing them to ai models to add to nancy's point i would i would say you know i think you're you're asking whether it's different computations for the same problems or same computation for different problems i would say there's no question that we're solving different problems i think if there is one thing uh uh you know i i've been i studied image categorization since i was a graduate student and i'm deeply convinced that there is nothing that is ecologically valid about image categorization that's the last thing our friend does is categorizing images frame by frame um and i think if if there's something that neuroscience has been teaching us in the past uh i don't know five six years all the rodent all the rodent work beautiful radon work that has done and kind of all brain uh imaging of of uh rodents literally you can record in the visual cortex and in almost any kind of task you find a marked effects and measure of behavior all the way down to the lower areas of the visual cortex which to me suggests that you know we are not you know our brains do not solve image categorization we are involved in everyday tasks whether it's looking for food whether it's navigating to a point whether and and um so behavior will impact that that goal that cognitive uh problem will impact you know the entire brain and and you know obviously i think the way we've been carving out those problems is to be honest mostly for convenience i mean this has been to a large extent uh guided by by ai computer vision because those are problems that are uh uh easily uh uh measurable we can measure progress in image categorization because it's easy to get class labels for images and figure out how well we are doing at that task and so i think yeah a lot of it has been convenient and i think very little of it actually is is necessary too closely related to uh the kinds of cognitive tasks and problems that our brains are solving on a daily basis i think this i think this is a place where you know again i the the cognitive science part of brain science the people who study behavior and models and algorithms at the software level have often been in a good position to provide guidance for both the neuroscience and the machine learning people or at least personally as someone who specializes in that that's the one of the main things that neuroscience people and ai people come to me for is they want guidance on what tasks should we be trying to solve i just think cognitive science has really specialized for many decades in trying to really identify like what are the core tasks um and um you know and it's not like everybody's agreed on that but i think by studying behavior and by trying to come up with models that can explain many different kinds of behaviors and figure out what are the core tasks that identify that that kind of method has been very has been very valuable and has helped us like we don't know quote what are the what are the core tasks that the brain solves but we've made progress on that and it's often come from cognitive science methods and maybe you'll see some of that in nancy's talks um because you know nancy in her work in brain imaging as well as students and collaborators of hers um you know like like federenko and language rebecca sax in uh social cognitive neuroscience and many others you know that's kind of the approach that they've developed is to try to use a combination of behavioral methods and classic cognitive psychology to try to figure out how to sort of jointly understand the tasks and the brain systems that fit together so it's i think that's that is interesting to explore yeah and another parallel field just to say another parallel field that does that very effectively is to just look at development and say what are the core cognitive abilities that are present very early that's another way to kind of figure out what are the tasks that are really yeah that's definitely cognition yeah yeah and right i'll talk more about that in my talk and i'm sure there'll be others on that but yeah to me i'm especially influenced and here i'm channeling insights of nancy's as well as liz belke and others in cbmm is like when you find some kind of task that is both early emerging in babies in infants and seems to be like present very early on some competent some ability to do something and it's supported by a dedicated brain system so it seems or that at least when you can study in in adults and maybe even increasingly now in babies see yes there's a part of the brain that seems to do that and you also have analogs or homologs in other species then that seems like it's telling you something like ah that's a fundamental thing right totally so and and some of those you know like face recognition intuitive physics language processing are all examples of those but sort of different right because language processing is one that isn't shared with other animals and is dependent on culture and experience even though remarkably it does when anybody who learns language tends to come out in basically exactly the same interesting network in the brain so there is i think a great deal to be learned from the combination of development brain behavioral studies and and when and how and when those and comparative studies across species how the when it when and how those converge on some kind of uh decomposition of systems that's telling us something if it's okay with others can we go to the there's one there's a good one that's the next highly range which yes it's very good and it's also actively spawned a whole sub discussion so probably everybody's been following that but the gist of it is why is there no theoretical neuroscience in the brain like the way there is theoretical physics in physics do we have to rely on experimental methods or neuroanatomy and then miguel says that was laha ale i'm not sure if i'm pronouncing that right but laha says that miguel says there is theoretical neuroscience cites the book by diana abbott um laha and others say well but has it really had impact varun wadia says well i think the claim is that there isn't quote theory in the same way there is in physics for example we don't have laws of the brain in the same way we have laws of motion or electromagnetism essentially physicists build a theoretical framework and then all experiment is to test the predictions ligo to find gravitational waves and so on many years after they were predicted in neuroscience we seem to do experiments first and then try to stitch it together and there's more and more debate but i think i would add into this that there's also on ai like at different at different points in time neuroscience has been more driven by sort of grant proposals for grand laws and then experiments to test them and and right now maybe it seems more in experimental phase but ai also has kind of gone back and forth between um you know i mean especially a lot of the neural network research it doesn't seem very theory driven and yet in other parts of the field or in other times like several people here have cited tommy pojo's work and you know he's the founding director of cbmm i'm sure you're all familiar with his work and you've even seen him talk um you know he's definitely been an advocate for theory in both machine learning ai and in the brain so there's definitely all of that but it's yeah we should comment on like what do we make of you of the current moment or the longer sweep of the field in terms of the right role that theory is playing or should play i mean there's there's a huge role of of theories but maybe not a big overarching theory like you have in in physics right so you know it's a more complicated system right um you know you don't you don't uh well i guess i was gonna say you don't have broad overarching theories in biology either but you do you have evolution um and that accounts for a lot and there and maybe there's a bit of an analog in terms of you know i don't know optimized computation or something like that right if they you know i if there's if there's something really uh you know most exciting to me about the use of deep nets is it's a way to say um you know can we use them to figure out what is it what what would an optimized system do right so it's like we used to yeah yeah i mean you could say that there's a broad theoretical idea which includes deep networks but it also includes like all these ideal observer models and yes right yeah yeah and fristan and kind of almost everybody i mean like which basically it's sort of the kind of engineering optimization toolkit it says and you know it includes like expected utility theory and information theory a set of tools on you know i guess to me it's maybe that the the the overwhelming or the if i were to say the single big idea is something like expected utility theory honestly some idea that there's like a cost function or a loss function or utility function and then there's some probabilities on how the world can shake out and how your actions can unplay or play out and you're trying to optimize you know intelligent behavior is trying to optimize for good expected utility outcomes maybe to to try to maximize for those or something like that and it could do that in many different ways and it's it wants to do it on not just you know it wants to do it in the future like expected expected utility into the future because it doesn't really matter about the past what matters is you know your whole life that's that's one way to put a broad theoretical idea that is that has been you know continually influential in the field and just kind of takes different guises when you train a neural network with backdrop to minimize a squared error loss that's one version of it if you make a hierarchical bayesian model that's another version of it a reinforcement learning model is another version of it and broadly those approaches can do what i think is one of the most exciting things and that is in principle sometimes they can answer why questions like why do we solve this task this way why is the brain organized that way right so if you use any of these optimizing systems and you find that an optimized system has these properties and then you find those properties in the brain then that's pretty damn cool it tells you okay and maybe we're starting to understand why the brain does that that's what an optimized system would do right it can answer why in a way that also feels general and explanatory because sometimes the answer is well just because right i mean there are there are some things like you know like for example biology is full of famous examples of things that it doesn't seem like any optimized system would do like you know have the retina turned inside out or whatever it is um but you know maybe someone could come up with an optimization explanation but yeah but when you when you explain something in terms of that kind of you know optimization account then it often feels like it's going oh that's gonna then have lots of other predictions or it's going to unify with other things because that same principle is probably you know important in other settings i don't know if that the people who are engaging in that debate does this uh is this um get to what you are in your sciences messiers and physics arguably yeah yeah and attempts you know attempts to make it as clean as physics are not honestly all that satisfying to many people like like so carl fristan has has pretty prominently basically taken tried to take some ideas from physics you know in terms of like um free the free energy principle which is another version of the same idea we're talking about but explicitly um framed in theoretical physics terms and it's it's elegant and inspiring in many ways and also like you know doesn't necessarily really tell us anything specifically about how the brain does i mean carl himself said the same theory applies to slime molds and the universe and brains and so it's you know it's a very general theory but it's not necessarily telling us anything specifically about brains or anything like the kind of flexible adaptive intelligence that characterizes humans and some other animals but not slime molds and the universe so you know in the quest for super grand theory sometimes you you uh you lose what we're most interested in but i but that's why i feel like that the kind of theory that we were talking about as nancy was calling this sort of it's sort of optimization taking into account probabilities and utilities really that is sort of as close as we come to a standard model or standard theoretical paradigm and then we explore it in different ways depending on the kind of aspect of behavior that we're studying and whether we're more you know working at the at the neural circuit level or more at the behavioral level and so on or the time scale are we looking at you know perception in the moment or learning over a lifetime the same math can apply in all those different settings but the particular computational toolkit by which you explore that map might be best suited differently in the different cases 