 [Music] return in the audio group this summer he is a third year phd student in the department of computer science engineering in university of california in san diego his research focuses on developing transfer learning algorithms to improve the robustness of bci systems and he also works on developing practical applications using different bci paradigms and in he is going to present to us the results from his three months long remote internship titled a closed loop adaptive brain computer interface framework without further ado go and jump you have the floor is the meeting recorded right now i just started it yeah okay so uh hello everyone i'm khun jung i'm a some summer internship uh in the audio and acoustics research group working on a brain computer interface project with my mentor ivan tashit and the title of our project is called a closed-loop adaptive brand computer interface framework and here's the outline for today i'm going to give some introduction about what is a eeg based uh brain computer interface and then our proposed framework then how we set up our experiments to study this topic and then talk about how we implement the two main components which are the erp classifier and the ssvp classifier in our framework then what we got in the summer and then the conclusion so first a brain computer interface or bci is a system that can translate users brain activities or users cognitive cognitive space into comments for machines or for computers and there are multiple brand imaging methods to implement a bci system for example there are some invasive methods that can measure the e-cog or the local field potential or the single unit activities of our brand but these kind of methods are invasive and they're unlikely to be used by general public and there are also methods that measures the change of the blood oxygen in our brain tissue and so the nearest and the fmi uh however these kind of method have slow reaction time because um because the reaction time of our blood oxygen is relatively slow so they're unlikely to it's unlikely to build an inter interactive bci to have an inter efficient interaction between the user and the system using this kind of method and not to mention the mfmri requires a huge magnetics so which is usually only be able to be done in a laboratory so in this study we focus on using the eeg to decode the brain so the eeg measures the voltage punctuation from our from the the ion current within the neuron of the brain and we can measure the eeg simply by putting electrodes on our scope to measure the change of the electric field between these electrons so we believe the eeg is the most suitable brain imaging methods to implement a real world bci because first it has um relative higher temporal resolution to to give uh the possibility of having an efficient interaction between the user and the system and also it has relative uh better portability so um to to allow user to actually wear it during their daily life so for example people have already [Music] tried to combine those eeg intellectuals with some hardwares uh people have already been wearing like glasses or vr headsets uh so there are actually multiple types of eeg based bci there's something called passive bci basically monitor users cognitive states like workload or sleep stages etc and there are also something called interactive bcis that is basically a bci system that allows users to voluntarily control the bci system and in this study we focus on bci season which is based on something called steady state visual effort potentials so uh there's a the steady state visual evo potential or ssvp are basically the brain responses to some repetitive visual stimulate stimulation for example we can build a virtual dialing keyboard that uh each digit in this keyboard is flickering at different frequencies uh if for example the digit 4 is flickering at 10 hertz and if a user using this dialing board intend to intend to dial number four then he or she can stare at this digit and the 10 hertz flickering can induce certain responses basically the 10 hertz or its harmonic responses in this user's visual area so a bci system could detect that these kind of brand responses and know that this user is intending to bow for in this case so uh although eeg based bci is the most suitable for real world application it still suffers some problem and one of the biggest problem is that eg has a large variability either especially in the cross-subject scenario so different subject or different users have uh very unique uh unique patterns of in their eg signals it could be due to different users have different cortex shapes or the scope shade and different users can have different background activities because those things in their mind are very different and eeg also has relative smaller procession variability even within a single individual and it's mainly due to there's there's always a displacement between each time the when the headset is set up also the scope conductance will be different every day depends on the condition of our scope so in order to build a robust bci most bci system required users to go through a calibration process every time before use so that the statistical model in this system can be optimized to have a robust performance and the problem of having this calibration process is this process is usually very time consuming which will greatly [Music] drive people from using this kind of system in their life so people have also tried to propose using transfer learning techniques to reduce the calibration process so the main idea of using this kind of transforming techniques is that we can utilize the some existing data either from old users or from the same users but um in pre from previous days or previous sessions so that we can trend the model based on these existing data sets and then we only need a small amount of additional calibration trials before use to fine-tune this model but uh the problem is these kind of systems still require some collaboration trials so it's still not fully plug and play and uh that's why uh we are proposing to use some adaptive models that that allow the bci system to start from using an initial model that is trend uh with some existing data and then this model can be and improved during the time the system is being used so the season can be plug and play and during the time the user is using this system with the mod the psi system can collect new testing trials to update the model and the big advantage of this kind of adaptive model is that you can make the system become plug and play and also you can capture the changes of the signals for example over time this user can be get can get fatigues or their intrinsic intrinsic brain activities could change over time and by including the new testing trials we could be able to capture these kind of changes but the problem is how can we obtain those labels for for those incoming like new testing trials to actually fine-tune the discriminative models used in the bci system so it leads to our proposed framework the closed loop adapted bci framework and so first i'm going to introduce there's a kind of eeg response called error-related potential so error error-related potentials or erp happen when a user observe some unexpected outcome uh so in this study we are focusing on interaction erp because in our targeting scenario the user is controlling a certain type of bci and the bci will generate some output to the user so the use [Music] the user will observe those output and if the output does not min does not match the user's uh expectation or intention there will be a erp induced in in the user's brain so if there's no erp detected when when the outcome is shown to a user it means that that predicted label is very likely to be correct so we can include that predicted label in the corresponding trial back to the classifier to improve the the original bci classifier so here's the um the full star system diagram of our framework uh we have a user interacting with the b7 mainly with the uh with his or her ssd piece and we have a ssv classifier decode the ssvp and generate some result and then these results will be displayed to the user to further um inter induce the error related potential of this user so uh we can have an error detector to observe the the eg response during the period of the user observing these results and to see whether there's a erp detected and then feedback these pseudo label back into the ssvp classifier to induce to improve or you know adaptively fine-tune this classifier so as you can see there are two main components in our framework the first one will be the ssd classifier and the reason we chose this bc because it's a well studied in the classic one so that we can focus more on the the effect of the whole system instead and uh we we chose to use a three-way classifier to me the gener generality also uh studies have shown that to in order to have a best performance to obtain the best performance of a of an ssvp classifier a calibration process is usually required so it means that we can use our adaptive framework to reduce these this kind of calibration process and the second component will be the erp detector it tells whether the predicted ssdb label is correct to the user or not and a key feature of this erp detector is that it should take short or even zero calibration process because if it takes a long calibration process it kind of lose the original purpose of the adaptive model because we want to make the system plug and play unfortunately decent performance of the cross section or cross subject scenario prediction have been reported so which means we can have a we don't need the coverage process for erp detector but we can still able be able to obtain a decent performance and in temporal our system will look uh work as uh following so it will start with the queue to to let the user know the user can start to interact with with a pci system so you'll mainly uh the b-side system will have some flickering stimuli to the user and the user interacted interact with with his or her ssvp responses in the ssv classifier generate some prediction of these eeg signals and then these predicted labels will be displayed to the user to further potentially induce the error-related potential so the erp detector can tell us whether the predicted label is correct or not and then feedback this information to the sscp classifier and so we are expecting that with our adaptive model the bci system can have a pre-trained model first so the model is protected from some exchange existing data and then this model can be fine-tuned and the performance of the model will grow over the time the system is being used so traditionally if an adaptive framework is used so a bci system will require the user to go through a carburetor process and during this time the system is totally not usable and after that will be calibrated and then fixed so it should have a fixed performance after the calibration process and however in practice the performance could decay because uh there will be changes in the quality of the eeg signals due to you know the users getting fatigue or some other issue so we're expecting that with our adaptive model uh the b6 system can start from the picture model and then the performance of this model will grow over along the time this the system is being used and then eventually become having comparable performance as the fully calibrated model so next i'm going to talk about how we set up our experiment so in this study we use the eeg headset called epicap express twist from brim products it has 32 channels plus the ground and the reference channel and we also added one auxiliary channel for a photo sensor to synchronize the timing between the timing between the eeg signals and the actual stimulation and this eeg headset use active dry electrodes the amplifier used uses a bluetooth connection with to communicate with the laptop and as for a software uh we're using the recorder from the brain product to uh to communicate with the amplifier and we and we have a unity program that actually display the visual stimuli and there's a python script that actually record those signals and save into files or process those signals during the experiment and we also use the internet protocol called last streaming layer to broadcast the eg signals from the recorder and also to fulfill the inter process communication between the unity program and the python scripts and here's a short demo video of how what our experiment process is so later on you'll see three visual stimuli down here and each target will flicker in different frequency and in the beginning of each trial there will be a red arrow pointing one of one of the three targets and the you the the participant is asked to gaze at the pointed target during the following period and after the flickering duration at the flickering period they'll be the the system will process those the ssvp signals in real time and then it's an output so the output will display in the middle and also the predicted target will be highlighted in green color and you'll you'll also notice that there's a square at the top here changing between black and white so this is the trigger for the photo sensor for timing synchronization so you see the red arrow it means the user as is there at this one and there's a result so that trial is correct and the result left so this chart is correct as well and the final one is the incorrect one so arrow pointing this but the output the classifier predict the the label as the left one and so in the last round when the user is staring at the visual stimulation the erp response should be induced in this user's brain just one more comment to the audience in reality the flickering of the buttons is much faster it's not that slow but this is actually the beating between the frame rate and of teams and this is why it looks very slow yeah yeah i'll talk about the actual flickering frequency later on can i ask a question real quick are you taking questions yeah please um yeah um i was just curious if the error the error response that you see that because the the green highlighting is not where they're looking is it a visual thing or is it more like a mental thing where it's incorrect and that causes a response that you can measure no matter how the ui was shown uh yeah yeah it could it could be a mix um so i mean even uh even um the the potential is induced by the visual effect i mean the system could still work because you can always have this kind of feedback and official stimulation to the user but um but we during experiment the uh the the person is not asked to you know to really stare at the target so we believe their erp could be actually induced by the the fact that the result is is not as they're expecting okay great thank you okay so um so the data set we have uh so due to the the kobe 19 i'm i'm mainly the participant that is for for the study and i have eight uh i have a cross-section recording in total so during these sessions that the ssvp is was classified during the experiment so for each session here um there there was a ssvp model trend by the previous trend with the data in the previous session and during the experiment the the ssvp in the current session uh were classified by this pressure model and then so so that the error related potential could be induced during the experiment and both of the ssd trials and the erp trials were recorded and but the erp trials were processed in offline and uh i also have i also had two two more session that was uh that were fully online and which means both of the ssv trials and the erp trials were processed and classified during the experiment so that the adaptive model could be finding during the experiment and yeah i would like to highlight that the simulated erp here means that we had some pilot session that we use the fake ssvp classifier instead so the fake ssv classifier basically output the true label with the with 0.7 probability and 0 3 0.37 probability the fake classifier returned the false label and i also have the second subject which is my roommate so we are to experiment in the same household to avoid you know the infection of the kova 19 so for this subject the first session will be a cross under cross subject scenario that um because by that time the data i had was was only were only from myself so there was a model pretend with my data and then classify my roommates ssvp during the experiment and after that the third and sec the second the third session the pre-trib model uh was trend on his data so uh it will be it was cross-session um in these two in these two sessions okay so now i'm going to talk about the erp classifier how we process it and how we build the classifier so first uh from literature we know that the error aerated potential mainly locate in around the cz area so we were extracting the channels from uh this channel basically around the cz and some back uh some backward yeah and we did also uh includes the channel around the visual area here and these signals were referenced to the the channel behind the ear to reduce some intrinsic brain activities and then the signal we apply 1 to 10 hertz band pass filter as you can see the the erp responses are relatively slow and finally the signals were downsampled to 250 hertz and here here are some plot example plot of the average trials within a session so add cz and fc so you can we can see that for every trial we can easily distinguish them visually and uh so after after the the pre-processing the the erp trials were further applied with some spatial filters to enhance the signal to noise ratio so a spatial filter is basically some linear coefficient for each channel so if we write the we write the matrix of the data of the the sample of each trial as the matrix with the dimension number of channel times number of time we can calculate the summation of the covariance matrix of all the arrow channel all the arrow trials and all the trials no matter label and then after this we can write the optimization problem to to obtain the best uh weight factor to to optimize the the variance after projecting the signal into some uh spaces and uh so this up either problem is very similar to like uh pca or the linear discriminant analysis uh and so the answer of this problem is uh very well known so the optimal weight factor will be the eigen factors of these matrices so the final spatial filter will be the eigen factor uh we preserve and then concatenate concatenate them together depending on how many components we want to preserve in this study we preserve the first four eigenfactors of this matrix so after appending this spatial filter each trout is then projected into the component spaces as the this form so we multiply the transpose of w with each trial to project project this trial into this the dimension of number of component times number of time stamps and after obtaining the time series in the component space these these components were further divided into two windows to capture the two unique two signature peaks and [Music] within one window so all the windows of all the trials were fed into the linear discriminant analysis and then so the window of each child is projected further to uh to the lba space so to obtain one uh one feature value in one window so so there so the first component will generate two feature values for two windows and we are since we are preserving four components so we have eight feature values in total and these feature value then will be fed into the logis the classic logistic regression classifier to um to generate the final prediction and then uh i'm going to talk about how we process and build a model for ssdp signals so instead of having a channel around a cz area we we extract the channel around the occipital area because uh from literature we know that official the physical responses signals are uh are mainly processed in the occipital area so uh a 6 to 40 hertz band pass filter was applied because our targeting frequency were 7.5 hertz 10 hertz and 12 hertz and we want to preserve up to the third harmonics of the of these frequency so so the maximum one will be 12 times 3 will be 36 that's why we cut at around 40. and these two plots are the average of the spectrum within each label uh within an example section and we can see a clear peak at 7.5 hertz for for a 7.5 hertz label also we can see the second harmonic for example the 12 hertz has a has a unique peak here around 24. and uh so for the ssd classifier we use the convolution neural network to build a classifier so for each trial we have a number of channel times number of time stands and then the fear in both channel domain the temporal domain were extracted by two convolution layer which uh which was in a previous work and then after a feature after features were extracted there further flatten into a single dimension and then connect it with the fully connected linear layer then finally project it projected into the class spaces which is the dimension of three because we have uh we we're using a three-way classifier and a lib a little more detail about how we design the convolution layer so we use the first convolution layer to find the special filter as as similar as we want to have some special filter in processing the you know the erp trials so we also want to find some linear coefficient for each channel in the ssvb data so the kernel size was set to number of channel times one so that we can focus in on the linear coefficient of for each channel and then after you convolute it with these kernel the signals were projected from the channel spaces to component spaces and then the second layer we we have the height of the kernel as the number of components to no to preserve all the components and then in temporal domain we had a 0.5 second window to extract the temporal signatures for targeting frequency and then i'm going to show the results we got quan jung can i can i ask a good question the input features to your cnn yes are those the components by time or the channels by time the input the yeah the input and beginning will be channeled by time yeah but yeah but the channel here is actually um different from the channel in image processing so uh huge processing we we have rgb channel but instead here uh it's more similar to the the width of the image or the height the height of the image and the the rgb channel the the idea of rgb channel here will be on the uh single channel that'll be because we only have you know one trial for for for for each for each sample so it'll be more comparable as the the height of the image and the the width of the image is the uh the timestamps so uh is that is that care yeah thank you okay then uh so before i show some results um this allow me to introduce some the evaluation flow used in our study so the first evaluation flow we were trying to compare the calibration scheme with our adaptive scheme so here um for each targeting session for example if this session two we use session the previous session which is section one in this example as the pre-training session so we can obtain a pre-trained model using the data within this session and then for a starting session we further split the data in this session into two uh two parts the first part will be the calibration data so assimilate the traditional calibration process that the user has to go through this carburetion process then after that the model the system can be actually used so with the carburetion data here we can trim we can obtain the fully calibrated model by fine-tuning the pre-trained model here and uh as for our adaptive model we further split the collision data into four blocks so because we want to want to observe how our authentic the pro how the performance of our adaptive model grow as a new you know new calibration trial or new online testing trial coming in so we split this carbohydrate data into four blocks and the adapted data was that data pool was updated block by block so whenever there is a block coming in we had a policy to decide whether trials in this block can be edited can be added into the adapted pool and then so the size of the adaptive pool will grow as more blocks coming in so the adaptive model can be obtained by fine-tuning the preacher model with the adapted data and the second evaluation flow is to simulating the online performance so in this case we no longer have the the fully calibrated model because uh we want to simulate that the system right now is a plug and play we we split the target session into 16 blocks and so each block is evaluated first as the plug-and-play system that no the trial is immediately evaluated and after evaluate the model with these trials then we can put uh we can make these trout you know put them into the our adapted pool to fine-tune our to obtain our adaptive model and then this updated model could be used for the next block so as you can see the the adaptive model is always update here and then seeing the new block and then the trials in this block is were further used to to fine tune the model so uh the policy of how we add the ssdb trials into the into the adapted mod port data pool is that shown here so whenever an ssv trial and it's pretty labeled and the corresponding erp trial come in we'll first see whether the erp detector can confidently classify in this erp trial so if if so then we'll see whether the erp detected is the erp is detected in this erp trial uh so if the erp is detected then we know we can say that the predicted label to this ssvb trial is probably wrong so we'll discard the ssv trial in its label because we are having a three-way classifier so we know the label is not correct but we will have no clue about the actual label of that but if the erp is not detected then we have a conf that this predicted label is correct so we we can further add that uh ssv trial and the pseudo label to the adaptive pool uh to fine-tune the the model and here's the performance are of our erp detector we can see that after applying the confidence threshold we can further boost the performance of our erp detector while still preserving a decent amount of the trials and the good thing of this model is that we have a very low false rate here which means when we when the erp detector tells us that the predicted label is correct it is very likely that it is true so so it means that we are not adding uh trials with wrong label in our training data so so if yeah if if we're adding round label into the training pool we can easily ruin the model but with a relative lower false false negative rate although we although we're missing some trials because of the false positive here but we can avoid we can avoid the risk of ruling the model and here's the result of the the classification of the ssv trials uh of the the cross section recording subject one um we had a multiple uh simulated model to compare so the protrude model we talked about that uh this is just trend with the data from previous session and fully calibrated one is further fine-tune the preacher model with all the carbon trials and labels and the curve with is similar to the fully calibrated one but instead of adding all the carbon trials and labels right like all together uh these trials are progressively adding to the training pool uh following the no the block design we have and the perfect erp is assuming the erp detector is perfect having the 100 accuracy and the semi supervised here means we're adding those trials in the pseudo label the slow label is not was were not determined by the erp detector but uh determined simply by by its predicted uh classes is that and the adaptive model is our proposed method and we can see that in the evaluation one the the performance of of our adaptive model um is as what we are expecting that it grows from the pre-trained one to the like gradually converge to the fully calibrated one and the block bar plot here shows the statistics of the accuracy at the last block last block and the p-value here are the wilcoxon sign rank test between you know between the adaptive model and the pre-trained model here over eight sessions and the p-value is uh smaller than 0.05 which means that our data model kind of consistently outperformed the pre-chain one over these eight sessions and if in the evaluation two we can see that the adapting model can outperform the pre-trained model especially at later blocks so especially when the this the that model is finding with a certain amount of blocks also uh yeah you can see that we with the true label the problem the performance is mostly the best one across time and here's here are the results of the cross-subject scenario of subject to so in this one the the growth of the our adaptive model is less observable like in uh here if we only have four blocks to adapt it's hard to tell the trend of our whether our adaptive model can actually outperform the the pre-trained one but if we have longer um if we have larger number of blocks to adapt we can kind of see that this delta model gradually diverged with the preacher model although in the last block it kind of bounced back here but we we think that this phenomenon is basically due to that we have far worse performance in our erp detector in this cross-subject scenario so it means that we are adding some incorrect label to our trials so that in average if we are adding more correct label into a denti pool uh i mean in average the accuracy can still grow but it will be less efficient and uh here are the cross section result of the subject 2. since we only have two sessions we can't do the statistical tasks and but we can see that in this two session especially when we have larger number of blocks to adapt the performance of the adaptive model clearly diverge and outperform the pre-treatment model and also note that for subject to the subject to had worse performance than the subject one in general but the trend remain similar so that we can we can say that we can know that the effect of our adaptive framework um can work uh no matter the starting performance of the picture model i mean at least in these two subjects we we can see this effect and finally here's the fully online recording of the subject one so in this setting the erp trials were classified and the model was fine-tuned during the experiment online and so we have we had the labels during the experiment recorded and also we are comparing our adaptive model to the case that when the pre-trim model were used instead and we can see that in the first session here both models perform very well so they kind of saturated and actually much room to improve but for us the second session we can see that the pressure model has worse performance and the adaptive model can can correct this problem and to have a better performance and especially at the later half of the experiment that the model managed to maintain a decent performance here and uh just um we have a question yeah mail uh yes sorry just quick question for clarification so in this plot here green is showing the actual results from the adaptive online adaptation yeah and red is showing uh kind of simulated performance based on using the predicted of the um pre-trained model yes yes okay great thank you yeah okay finally here's a the short demo video of how these uh online fully online session go went and uh so you can see this is me wearing the eg cap the photo sensor the extending box and the amplifier here and here are the three targets and the arrow pointing the middle one as a queue and due to the color effect here it's kind of hard to see the predicted result but i think it's a bit leggy but you can see that the result it will be highlighted in green color the protective button is green yeah the green one here the green one okay so in the video yeah these are uh 11.11 correct prediction in streak don't wait can you still see my screen it's black corn joke you still see the video yeah so finally the conclusion so we showed that our adaptive framework can really make the model improve over time also so it works well it worked well in a cross-section scenario within the same subject but the performance in the cross-subject scenario was kind of limited by the erp detector and we also showed the demo video and you know that we can successfully implemented this framework online end-to-end and i think the immediate immediate future work will be record recording more data to validate the consistency of our work and so that's uh that's my uh intern project and i'd like to give a special thanks to my mentor ivan tashif also thanks to the members in the msrbc team andy dimitra david at hannah's mihai and the other intern winco that you you gave me very useful feedback either during the meeting or sharing ideas with email or in the in the team's chat yeah uh it's very nice my pleasure to work with you thanks for all your listening thank you quan jung thank you let's give you my prop questions colleagues now uh yes great presentation so i just wanted to ask how much of this work is related to your phd work going forward so it seems you're in year three i was wondering kind of what your next steps were oh yeah so in my lab i like even introduced uh i focus on building the transfer learning techniques so uh well the next step could be you know combining the transfer learning techniques i already have with this framework so in in here we use um well the model is more is based on the com neural network and he has some gradient descent to to adapt uh we we had another kind of another timeless types of model which was based on like near nearest neighbor classifier something like that so yeah i think you will be interested to look into like how we can combine those two awesome thank you and i don't want to take up a lot of time but i just want to mention um funny enough this presentation or your internship presentation is very similar to my phd um so if you have any questions or would like feedback on specific things let me know oh sorry yes i'll i'll message you after uh okay yeah thank you leave your email or something i i mean or now talk with me and i'll connect you with conjunct with his university email perfect thank you my email is shown here so yeah but i mean i don't know your contact information so that's right i'll reach out then thank you thank you more questions um i have a few questions ask one and see if anyone else has a question in the meantime if you go back to maybe slide 38 towards the end um the calibration time you mentioned you you have blocks four blocks or eight blocks or 16 blocks how much time is that roughly for calibration uh so each block is uh 18 trials though and each trial is around uh four four seconds in total so so that'll be that's it so one uh one block eighteen twelve times four like one minute around one in one minute and 12 seconds for for each block so it'll be like four minutes here aim it here but uh yeah uh kind of so the actual time can be first shrinkage if you know uh because i live some space for you know after after the the erp response i add some blank for the subject to have a creep to have a quick rest um yeah thank you but you also increase if you have more number of targets so it kind of depends on what kind of pci you're building so if you compare this slide with i believe was the next one subject to um right does that show that subject to when you train on subject two's data the the accuracy wasn't as high as when you trained on your data and tested on subject to um so basically you were able to compensate for subject to low accuracy by training on on a different subject subject one so uh so i believe there's um uh the variation here every time the when the headset is put on this subject um so when i i myself was the participant i mean i can better control the the easy headset so because well i guess they'll be more robust when i put on myself that i know i know the limitation of how much pain i can like i can endure and like how deep the electro can be put but uh uh yeah for for my sk yeah forever for information the eg headset we use uh does not tell the impedance of the electrode so i kind of have to visually observe whether the signal quality were was good enough and so when setting up on my roommate i guess there's more variation when putting on devices also i myself it's more has a lot more visa experience so the b side thing is actually a skill you have to learn how to if once you are more familiar with using this kind of system you can have better accuracy and um so for him his he's a non-experienced person and kind of uh i think he's his signal quality or the the the capability of him getting induced by the visuals flickering kind of varies a lot more than me so we yeah i can't draw the conclusion like the digital conclusion that trend on my the trend with the data myself rather than train the data of himself can really bring better accuracy it's uh this variability can be even from insufficient data uh we'll have to get 10 15 20 subjects to make some more definite conclusions i think more questions colleagues no more questions last chance okay i'll take one oh yeah um so you've um after all you've done with with erp um erp do you think that it's the right the right approach do you think that of all the things that we could sense you think that that's the one to go with so i believe uh you'll be one of the good reimbursement signals for building this kind of framework and i think it's actually wait did i lose my sharing no we see it but uh let me do so uh so here like there's a semi-supervised thing here i believe it's actually better to combine these two so sometimes you have very strong confidence uh just simply from your original classifier that your result is correct and so i believe so that kind of gave us in in independent uh information so that we can say that the er the this the information we can obtain from the erp is kind of indeed independent from the information we can obtain from the classifier because the confidence of these two models are are independent and i think it kind of leads to a more fundamental learning problem that so if you have a new trial coming in if you originally i mean without the erp detector with your if you are building a semi-supervised model you originally have a very strong confidence that your label is correct so maybe adding this neutral will now be very helpful because you are not pushing any your classification margin further so with this erp response you can have some you can add some child that originally your model is not so sure about the correctness of this label so adding this but if your erp trials for your erp detector is very confident then you can kind of uh greatly [Music] move your uh move your margin or your you know like classification hyperplane or something like that to to a better place so i mean i believe it's better to combine these two and also uh there well i think it will be a good first future topic that if you are confident in your like first in your classified first stage then you can probably use that confidence to further fine-tune your erp detector so they'll be bi-directional fine-tuning each other and so although also besides from this adaptive learning problem i mean adding the erp detector as as actually um another big advantage is that you can correct your prediction so we didn't implement that because we are not discus discussing about that but you if you detect some arrow you can actually try to correct that like if you are you have a binary label classifier so i so i believe like adding these erp components in your in your firmware is definitely a good way aren't there cases though where like e-r-p-e-r-p and s-s-v-e-p fail at the same time you said they're independent but i would wouldn't there be situations where both of them would fail yeah definitely there definitely might be but you then in that case you kind of your host if your mask fails from out of your head yes your host yeah okay thanks it's great presentation thank you very much thank you more questions colleagues uh one generic one what is your what is your sense guangzhong if you for the cross session uh training if you had more original training data would that help or do you think it might be better to have uh let's say lots of lots of not so great quality data or maybe one two great subjects and you start to train your model and then transfer uh well i i believe i mean average probably having more subject is better because um so if you have a very good signal quality within a single subject uh you kind of you you might be overfeeding on that type i mean on that subject or all these subjects similar to that one but if you have other subject that has larger subject variability i mean i having more subject and you have kind of have a voting mechanism or in simple way will be better and having a single subject with good scene quality yeah i guess i guess what i'm thinking is [Music] until kovit it was easy to collect people's data in the lab and make sure that the same person puts on the um puts on the um the cap and everything is is great but now with kovit we might have to collect data by sending around the cap so there might be much larger variability in the quality that's one yeah i'm wondering but yeah okay i guess i guess we won't know until we try yeah uh i think but honest this is exactly the variability we would like to address in this with this project uh because in an let's assume your target device for bci for the general population you cannot guarantee that everybody will put the device exactly the same way with medical precision as experience that i think technician will do so that's actually a good thing right yeah i i guess the yeah i guess the current hardware is not really made for um how should i say it feels like with the the current hardware is sufficiently complex that you might get no signal at all if you don't know how to put it on whereas maybe if if something were to be handed to a consumer there should be much less room for error than there currently might be with the current hardware absolutely no one of us imagines that the future bci users are going to wear eeg caps yeah yeah also i think a good design for no if you're doing some like keep swapping user then probably using some disposal it actually will be better because oh i think people will feel uncomfortable when you use some electrolysis being used by many subjects already like the bran like the the smarting thing with the sponge it's disposable probably it's better for swiping users but yeah thank you more questions thoughts comments feedback well i must say that you're this you're deciding during test time your latency is just one or two seconds which is it's pretty great from from the video you showed thank you wonderful thank you dimitra more comments questions feedback if not let's thank cran junk again for the wonderful part and thank you all for joining us for this talk uh quenching don't forget before to exit the team's meeting to copy and paste the contact information nail provided thank you all for coming and in a week or so this talk will be available for wing publicly outside of microsoft corporation and i think in a couple of days it will be viewable internally from our website thank you for coming bye 