 so I'm honored here to be here today to present my PhD thesis work and this was carried out at Carnegie Mellon University and it wouldn't have come together obviously without my my great advisers Chris Harrison and Scott Hudson and my committee members Benko and Jodie floozy and of course all the collaborators I had from Carnegie Mellon University Microsoft Research and other institutions so my thesis is titled on world interaction or on world computing rather focused on enabling compute computational interaction with everyday services such as walls tables furniture and so on so you bhikkhus computing seems to be here now like we seem to actually have you because the Ricker's computing in the form of these smartphones billions of people own smartphones and these are very very sophisticated computers jammed into very tiny packages that we carry around with us constantly so of course that means that smartphones have made computing disappear into our lives right now it's not quite true if you actually want to interact with the digital world you have to do it through that handheld terminal and digital content is fully separated from the physical world this isn't really the vision of your babies computing it's trapping the digital world in these little boxes and so for my thesis I set out to try to fix this to bring computing out of the small trapped interfaces of today and out onto the world around us out onto our everyday services our walls tables and furniture and so forth this is what I call on world computing this is bringing interaction out of natural everyday services and embedding complicate computational capabilities into the real world so now one way we could do that is we could just transform our environment into computers this is the easiest way right so we could replace tables with touchscreens we could replace walls with giant TVs countertops with tablets and so on until everything is interactive the obvious downside here is the cost you know although computers might be cheap now they won't be as cheap as drywall or wood any time soon and it's difficult to upgrade imagine telling someone while you have to buy wall 2.0 next year it's the must-have upgrade and it still limits you to services that are actually suitably augmented but we already have a technology that that can actually enhance in it's at a distance and that's the humble label so what I'm going to talk about today is showing you my research in taking this basic one pixel light bulb and turning it into an interface projector something that can actually put touch interfaces on any surface in the environment so I started my journey with a system called world kit and this is uh this is a an early project that I did to try to explore the potential applications and also the pain points that we might run into when trying to build on world interactive systems so what gets a complete system and what it lets you do is it's a it's kind of a software toolkit for the world here what I'm doing is I've written an application and now I'm deploying it on to my environment this is a set a status message app for my office so when the door is closed there's a there's a touch sensor on the door when the door is closed I can let people know why my door is closed the application programmer here has basically defined so the behavior and the controls that the application needs and then the user customizes the application for their environment by specifying where each of those controls go and so how this actually works is you know there's a there's a projector and a depth sensor that are fastened together and calibrate into a single unit that you point at the surface this isn't the light bulb you'll notice and the depth camera here captures depth images 30 times a second this is what one of those depth images look like of a typical sort of office environment and unlike an RGB camera of a regular you know camera that you use this captures distance to the nearest object which is represented here as a gradient between light and dark and it's startup time what we do is whether the worldcat system will capture a couple of seconds of background and use that as a background model so you can see here this is a video of how that actually that tracking actually looks in real time how we actually track user input using the depth data so we can detect differences in the pixel depth from that background model so here the regular green color is pixels that are at background dark green is too far in the foreground and blue is those pixels that are sort of close to the background but still discernible as being different from it so that lets you know where the user is actually interacting with the system so this is how we do touch detection in WorldCat and the second component is to do output so in order for the world consistent to handle any surface at any angle we have to do some sort of cleaner rectification so the system detects essentially what orientation the surfaces are at using the depth camera and then rectifies the image so that it appears correct so on the left you can see what gets projected out of the projector and on the right what you actually see when you're interacting with the system and the other thing that's interesting about this is it makes it possible for developers to treat the whole interface as a collection of 2d planes and this makes the development process the actual programming process very very simple but the big difference is that instead of pixels developers have to think in real world units millimeters so we've deployed a couple of applications with this so this first application is is a sort of a recipe helper what this does is each of these slots has a touch kind of different kind of touch sensors here and allowing the user to visually see when the recipe they've sort of assembled has actually been fulfilled and there's actually deployed to Alzheimer's patients on a real kitchen table showing that this improved their ability to complete sequential recipe tasks and the second sort of little demo application here is my office application so this is an augmented office with a very much younger me you can see that contact tracking here is pretty rough I have to use my whole hand right so this is one of the things that's that's sort of a downside of the system but I've got a little sensor on my keyboard here so that when I put my hands down my private calendar pops up so this is kind of a like nice little way to augment my environment and it's it's pretty unobtrusive but on the other hand there are a lot of challenges with this system so first of all you know the contact tracking I have to use my whole hand for that and that's not what we're used to when we're using touch interfaces of today and the user interaction was very very simple it's just painting and hand tracking not not a lot more that I can do with that it's up to the interface author to define any other interactions that might be possible and the system doesn't really respond to its environment in any meaningful way either I mean it's a non world system but it's not really interacting with the world and finally obviously that big projector is pretty unwieldy so it'd be nice to try to shrink that down into something that's really more like a light bulb or consider an alternative to projection so the rest of my thesis work I set out to improve each of those areas and so I'll start with input sets so this is system called direct this is a called depth infrared enhanced contract tracking so what this actually does is it enables high precision tracking of the fingertips using only in the off-the-shelf depth camera and so you can see here the the filled green circles are direct tracking the users fingertips and all the previous methods in the literature that we compared against as these hollow circles so why what is direct perform much better than previous methods well previous methods have really only used depth data alone and if you only use depth data you run into some really nasty corner cases so in the left side you can see this is what happens when I put my palm flat against the surface you can see the fingertips are barely visible and they're just sort of fading into the background and on the right side this is what happens if I put my finger at a very steep angle you can see the actual fingertip is not visible at all but a lot of these depth cameras capture infrared data as part of their normal operation so if we get at that infrared data we can see something very different obviously now the fingers are very very visible but if you only use infrared of course you can't track how close things are to the surface so the key to Direct is merging both of those so in depth it's easy to segment the arm in the hand and detect the distance to the table and then your name thread we can segment the fingertip and so what Direct actually does is it locates the arms the hands and then the fingertips in the combined image and this kind of hierarchical model ensures that we can reject anything that's not really remotely human-like that's sitting on a random office table and so this is sort of picture of what the Rex set up look it looks like it's still at a depth camera and it's still like a projector but you can see that Direct actually offers a significantly better accuracy as you can see by the bottom right picture without any smoothing applied so this is significantly advancing the state of art in terms of touch interaction and we around a user evaluation to direct using crosshairs and shape tracing and you can see here directors that field green dot performs significantly better than the comparison methods we found from the literature and in point of fact actually form a crosshair test we found that the REC was able to achieve on the left there achieve an average accuracy of 4.9 millimeters in terms of mean error and that's actually getting competitive with real capacitive you know physical electrical touchscreens that's pretty good considering we're you know using a camera that's two meters above the surface and for shape tracing in fact it's even better it's around 2.9 millimeters or so this is achieved all without any temporal smoothing without any sub position correction all sort of techniques that are commonly used in in real touch screens so this is enabling the raw tracking to be significantly more accurate than previous systems so this is good we've actually managed to you know achieve kind of touch screen level touch accuracy with the system so that's the kind of a first school achieved and so the next goal is to look at interaction so building on world interfaces that live on complex desks desk services and so if you actually look at the if you look at the literature about desk services the existing literature kind of implies that we've gotten rid of everything that lives on our desk it's really clean clear no clutter but you know our desks probably look a bit more like this you know there's there's lots of stuff on the desk other you know in digital interfaces that might have to be projected on those environments are gonna have to contend with physical objects for space so I built a system to try to figure out how interfaces can actually coexist with these and to figure out what kind of interactions people would want would want to use on their desk environment so to do this the first step was to run an elicitation study to figure out what people's expectations were regarding digital interfaces living on physical desks so for example what I did is have people use paper application mock-ups so bring some different kind of applications to users to say okay where would you put these on the on the interface how would you expect them to behave what kind of interactions do you want to perform with them so here for example the users placed a map on the left side because they don't use it much a calendar be on the middle that they can sort of use and see regularly and C and D which are a music player and and a keyboard or a number pad rather on the laptop so that they can actually augment the keyboard and this is kind of an interesting fact right we can use digital interfaces to try to augment physical objects and you'll also notice some some funny thing which is that users have actually arranged it sort of radially this indicates that you know real real physical desks don't necessarily correspond to sort of nicely rectilinear grisly can expect on a you know it's not a computer desktop but rather they might be arranged in two according to a completely different coordinate system like this polar coordinate system here so in the end up with the elicitation we derived several different controls multi-touch input window manipulations launching closing etc but you just also wanted these applications to actually respond to their environment for example so that they could attach follow certain objects or or move away if objects were to obstruct them physical objects were trips or obstruct them so to actually implement all this into a real system i built a system called desktop ography named after the fact that it essentially scans the topographical map of a user's desk identifying different objects in the scene and besides the standard touch interactions it supports snapping and following behaviors by continually by basically tracking object edges in the in depth map and it supports evading collapsing by using an optimization method sort of like what anna was talking about to deal with actual changes in the environment so this is by by defining different costs and penalties by saying you know don't put interfaces up over top of physical objects that already exist it'll continually try to update the layout of the environment to deal of the of the interfaces to deal with any changes in environment and so this is what it kind of looks like in practice so notably these are basically standard touch applications they could be web apps android applications this you know much more standard than what we did with world kit which had custom sort of applications designed for world kid so what we can do the user can launch an application by summoning like this so there's about like a calculated application for example this now can snap it on to the laptop and you can see it should have shows you the different edges that you can snap on to when you do this and the user can also resize them with this little this little nub in the corner just like the desktop application this is a responsive app so it changes the layout when when they when the layout changes and the interesting thing here is you know all these behaviors are implemented just with web applications right so here's what we're doing with the optimization and this is basically the the interfaces have actually moved away automatically detecting that they've been included and moving to a more suitable location in the in the environment okay so that's a set of sort of interactions and there's sort of more more work in my thesis but I'm going through the high-level points here a set of interactions that allow the the the user interactions and both environmental responses that work on practical desk environments so for the last bit let's look at how how we can actually do output so this is split into two parts first up what I actually looked at was an alternative to project entirely so this is augmented reality head-mounted augmented reality so what happens if instead of projecting things on environment we project it to the users eyes directly so if you actually look at kind of the hololens it's it's a pretty cool device you know you can you can do you can do bloom gestures like this hunt kind of hand gesture thing you can do voice input get and head gaze but one of the things that's really missing is the kind of tactile feedback and precise position of input that you get with touchscreens so what I did was to add touch input into Hall events and call that mr touch were mixed reality touch and so this is actually with nice it it actually allows you to to co-opt any surface in your environment as a touch surface so this is exactly essentially how it works this video is showing what the user sees this is a digital overlay on top of the users first person camera view it scans the field of view it looks for basically interaction services flat planes that are sort of appropriate to put down interfaces and then this is showing sort of a debug output this is essentially showing you which planes have been detected and then what the user can do is just drag out a rectangle to pull up an app launcher and so this is an interface this is an interface that virtual interface that lives in the environment that the user can see interact with Center touch applications all work and it also supports things like in air gestures as well so in this example I can build a little blueprint for a building and just pull it out just like that so this can actually let you mix touch input really quick which is you know positionally very very precise and lets you do a lot of really stuff with hand gestures that expand the input vocabulary so this is sort of getting the best of both worlds here gesture input and touch input so how that actually technically it works it's not actually that simple in terms of the technology we actually have to you know in indirect we had the advantage that the camera on the table didn't move relative to each other but now the heads constantly in motion there's no stability anywhere so what this is actually doing is every single frame its running a ransac algorithm to figure out where they in touch interaction plane is and then to use a direct like algorithm to detect the fingertips with optimizations of course so that actually runs on the hololens we actually run a study on this as well found it achieves a similar five millimeter mean error so we can do the same sort of tracking is direct but now from a constantly moving platform and sensing environment so this is all self-contained works entirely on the hololens and actually forms part of the basis for microsoft's next version of the hololens so this is something that is being deployed for real okay that's great but we forgot one thing this augmented reality is great but we we'd haven't talked about the light bulb yet so what about that so one of the things I actually did was to try to minimize those components so that actually fit you know into a very very small environment this is miniaturized in the projection and sensing components which was for the last year at KY and I partnered with a hardware om called a su to build a super small projection module and depth sensor so in the bottom right this projector is small enough to fit inside a SmartWatch so and around it we built a quad quad core android-powered SmartWatch that projects a touchscreen onto your arm and you're probably asking how long this lasts is about an hour of continuous projection or part of day if you're intimately using it throughout the day so this is how it actually looks when you're using it and so the user swipes to unlock this is pretty typical you know the old sort of system and that swipe to unlock is not merely for security or for sort of a like play it's actually a calibration procedure so when the user drags their finger towards the towards the sensor it's telling the system what angle the arm is at because that can change as the user uses the device and then of course we support sort of standard touch interaction with this so using a little tiny depth sensor that we built to detect the distance of the users finger away from this much body and so this is fully self-contained the projector the touch sensing computation power supply we can get all of that in a very very small package so the next step is to really collaborate with them build a real info bulb something that has enough brightness enough computational power to support sophisticated unrolled interactions so that work is ongoing but the good news is you know I we've covered all the challenge I look outlined at the start so you know there's always more to be done here but certainly in my thesis I think I've pushed the state of the art in terms of input interaction and output and I hope you've convinced you today that on world interactions not only desirable and useful but that it's also practically achievable than your future so thanks for listening and a quick plug right here I just started my professorship at the University of British Columbia so I'm actively looking for students so if you're interested drop me an email and I'll take questions [Applause] [Music] and yeah it's amazing work Robert I I have maybe a naive question but I'm obviously the resolution of of why projecting a somewhat more than might would it be on our screen I like other particular applications that you think like the way the resolution doesn't really matter Jamie no or when it what the kinds of things where it's not going to be very viable if the resolution is not higher that make sense that's a really good question so things like you know there's actually so the recipe application I think is a great one where you can sort of see it at a distance right where you have it sort of lay it out on on on your environment certainly for a real for someone who's not an Alzheimer's patient we would you know have something a bit more compact but something that you can you can use it a bit of a distance right we're not thinking about things that you would use right up close certainly there'd be the resolution does become a problem so you can't do very very small text but I think things that you know you sort of imagined the sci-fi vision of like having you know the weather shown on your on your on your wall when you wake up kind of thing or when you want to do some simple interactions with with that kind of content or for things like even you know having wallpaper that changes dynamically so there's also social applications I think in that sphere that would work but I think we're not good I'm not advocating replacing these interactions the devices that we use today but simply having computation be more ambient 