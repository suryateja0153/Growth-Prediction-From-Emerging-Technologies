 We’ve said it before, and we’ll say it again – brains are amazing. And we’ve put some the best of them to work devising incredible machines that can beat us at our own games, bring virtual worlds to life, perform quantum calculations, and exponentially expand what we’re capable of as a species. But when we interact with those machines, sometimes it still looks like this. But imagine: it’s date night, and you project a silent command across the room to turn on some mood music. Or, imagine you’re a paralyzed genius, but you can communicate effortlessly through a computer. Imagine you can write volumes, without ever learning to type. How close are we to controlling machines with our minds? It’s been said that humans have something of a communication “bandwidth problem.” You're always seeing things, you're smelling things, you're feeling things, you're hearing things, and all of that's a lot of information that you're collecting in real time. We call that a high-bandwidth, and by bandwidth we just mean the rate of information that can flow over a particular time scale. The information coming into you is Niagara Falls. The information coming out of you is like a drop from a dropper. If you think about how you generate your interactions with the machine, your brain is capable of so much more. A more direct connection between brain and machine could be the key to unlocking that potential. And this idea goes by many names, because it’s a vast and complex field. So you can think of brain-computer interfaces as being used for rehabilitative purposes, for assessment purposes, and for enhancement purposes. Thought-controlled wheelchairs or prosthetic limbs, or communication solutions for patients with ALS or Locked-In Syndrome are just a few examples of how this tech can help with rehabilitation. But, the field of neural interfaces is just being born. And what we have so far can be sorted into invasive and non-invasive approaches. One example of an invasive BCI is called Deep Brain Stimulation, or DBS for short. These are tiny implanted electrodes which can both send signals and record signals from deep within the brain. There's also what's called electrocorticography, and those can read signals from the surface of the brain. This is pretty effective, but still requires cracking open the skull. And unless you suffer debilitating seizures, you might not be up for that. I think it's sort of the holy grail of this emerging field to be able to get to all of the information we could get if we drilled into your brain, without drilling into your brain. The challenge is, non-invasive brain imaging is super noisy, and only captures signals from outside the skull. That’s like trying to listen to an epic symphony through a brick wall. Dr. Thomas Reardon and his team have an idea about how to overcome this problem. It starts with re-thinking what the brain actually is –– and it’s more than just the cortex. It's also the primitive part of the brain, the reptilian part of the brain, the basal ganglia, and all the way through the brain stem. And most importantly, the spinal cord, that long, thin extension of the brain that connects the forebrain out to the rest of the body. But you've got neurons all the way down into the spinal cord, and they work collaboratively, in this unbelievable symphony of neural music, to get your body to do stuff. And when you form an intention to ‘do stuff,’ your motor neuron cells –– the largest cells in your body –– start to transmit that signal from your brain to your muscles. When it releases neurotransmitter, the muscle responds with a little electrical spark. And that causes the muscle to contract. That's the evolved natural output place. We don't have to hack into it, your brain evolved to output it. And that's the signal that we record. Turns out, recording that signal is actually pretty easy, using a technique called differential surface EMG, which these neuroscientists and engineers have funneled into a nifty wristband. The bigger challenge is decoding what the signal means to you. The way those neurons go into your muscle, what we'll call the motor map, is totally different than me. In fact, it's so different, it's different than your twin. We have a bunch of deep algorithmic work we've done to take that electrical signal and map that to the chatter of the neurons in your spine. It sits here, right above your wrist, which happens to be the most densely-innervated part of your whole body. And what it's going to start doing is streaming neural signals to a computer, maybe my phone, which is going to digest those and turn that into a control signal. So rather than the signal being, "hey, move my hand," it becomes, "hey, move that cursor." Your typical computer mouse requires you to coordinate fourteen muscles with up to 20,000 neurons. This device just learns to listen to you, down to the level of a single neuron. And from there, the options are limitless. So, obviously our producer, Anna,  wanted to try this thing out. - So we can look at your raw EMG, which is also kind of cool. - Cool. - So the band has 16 channels here. So if you just rest, if you just drop your hand here on the table, your brain is sending no signals to move your hand because you're resting. If you just slowly bring your wrist up like that, you can see the pattern is that there's some movement there, and you just slowly extend your wrist even more, and then max it out. You can see that now we're getting a lot more signal, and so we're activating thousands of motor units. - Wow. This is what’s called the recruitment curve. And this program is learning what Anna’s looks like as she tries to accomplish a specific task –– in this case, tracing a ball around a circle. You actually spent the first year of your life doing all this stuff. We call it motor babbling. Your brain was trying to figure out your body… "If i send this command, oh, I got it, that's the fingers, got it." What we're trying to do is compress that down into an hour, or minutes. To then do new things you'd never be able to do with just your body. Mapping your hand to a virtual hand or a cursor is just the beginning. With enough Jedi training, you could eventually play Fortnite with your mind. - All the control schemes that basically humanity has ever created rely on physical movement. Even voice controls rely on the movement of my vocal chords. But here, because we can recognize an individual motor unit that's not sufficient to create physical movement, but is sufficient to act as a digital button, we can control a digital environment. You can just play the game. This is just one approach to blurring the line between brain and machine. The military is very interested. Some of their ideas are improving the function of soldiers, allowing them to communicate silently across the battlefield, for example. Other applications, like hacking into a soldier’s capacity for empathy, or attempting to decode private thoughts, present major ethical questions. So you can imagine these devices in various ways will change different elements of you, and some will be more core to who you are. Careful thought and regulation will be essential in ensuring our safety as we merge mind with machine. But one thing’s for sure: that future is coming. So, how close are we to controlling machines with our minds? I can imagine that in a decade from now we have brain-computer interfaces that can help us do really simple tasks in the world, like turning the lights on and off, or changing the channel on the television. Right now, the technology is absolutely appropriate for people to start developing new things with. Whether it's how to control a surgical robot while your hands are doing other things, or you're an astronaut and you need to do high-complexity things on the space station… it's time to move past the old models, and really open up new human experiences. For more episodes of “How Close Are We?”, check out this one here. Don’t forget to subscribe, and come back to Seeker for more episodes. Thanks for watching. 