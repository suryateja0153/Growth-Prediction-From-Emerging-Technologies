 all right cool 2:05 so let's get started thanks for coming and then I would like to introduce our today's speaker and robotic similar set to Vijay Kumar as professor in university Edinburgh and then director of a center of Edinburgh robotics he got PhD and Tokyo Institute of Technology and worked on many famous humanoid including Mishima and Valkyrie and he's a expert on machine learning and but trying to apply in the real world machine so you're gonna hear a very interesting aspect today and thank you very much said - Thank You sangre for the invitation it's a great pleasure to be back at MIT and I'm very glad to be able to talk to you guys the exciting time in robotics at the moment and as a sangria briefly mentioned my interest Llyod intersection of robotics and machine learning so I trained as a machine learning and person doing statistical machine learning but the real interesting aspect of the problems in robotics today is that it's not just hardware capabilities that we need to focus on but we also need to try and look at how we can build smarts into this machines and the way we can do that is by taking some inspiration from data driven techniques in machine learning and that's really the focus that's really the USP of the work that we do in our group at Edinburgh so just a quick mild introduction to to Edinburgh itself I'm not sure how many if you visited Edinburgh it's a it's a beautiful city with some lovely old historic buildings and not unlike Boston and the university is is one of the ancient universities in the UK established in 1583 and so we live in a slightly more modern building in the School of Informatics here and our research landscape the School of Informatics which is actually one of the largest departments of informatics in Europe covers a significant range of topics which are just listed here and we are kind of clustered into different Institute's that interact very closely so that's just to give you an idea of the broader picture of what happens in Edinburgh so now just and I don't think I need to emphasize this to the crowd here but robotics is becoming ubiquitous in many different walks of life all the way from more traditional areas where you're using it for things like manufacturing underground mining to more innovative areas like agriculture smart cities autonomous transport even the surgical side so healthcare and the third image on the right does anybody recognize what that is any any guesses the blue this blue blueish image there any any ideas that's correct it sits inside the core of a nuclear reactor so this is there's a significant problem in in the UK in other parts of the of the country in nuclear decommissioning so we've spent a huge amount of effort building up nuclear capabilities but we are now left with many a nuclear plants that have murder past their life and but we don't know how to decommission them how to effectively safely decommission them economically decommission them so these are some of the areas that we've been looking at working in using robots to to assess and and help so if you think about where robotics is going so we it's not a new area we've been working on this for the past you know 50 60 years but traditionally robots used to be operated using concepts of teleoperation where you either give using either kinesthetic demonstrations or some other programming methods we tell it exactly how to do things there's no significant emphasis an increasing autonomy in these robotic platforms and there's lots of research around the world which focuses on that but really the challenge in the next sort of 10 to 15 years is going to be somewhere in between and very what I call shared autonomy here where even within a single task we need to be able to seamlessly move between significant autonomous behaviors to large amounts of human input and human teleoperation so there are scenarios where we want to reduce cognitive load and the people operating the systems by increasing autonomy but at the same time we want to make sure human input is taken into account while ensuring that you know we'll be doing the right kind of things so share autonomy is a significant bigger challenge than either of the two extremes because now you have to deal with human input in the loop during the control system so it's not a single you know independent system that's working on its own where you can you know understand the dynamics and the kinematics contact forces and and create algorithms to deal with it you need to take into account user intentions you need to take into account adversarial and cooperative behaviors as well as you need to take into account scenarios where forces from two different agents are interacting on the same same device so and that's a challenge so here's a list of things that I think are the reason why robotics is challenging now so full autonomy I'm waiting for full autonomous behaviors is going to be a big challenge because of all these issues we have robots which closely interact with multiple objects you've got noisy sensing ambiguity you've got hard to model dynamics we need guarantees for safe operations for robots to be introduced into the environment at the same we have unmodeled user intentions and constrained environments so on the side you see a collection of topics that we work on at Edinburgh all the way from prosthetic clinics to skeletons to some aspects of software stacks with self-driving cars some field robotics with our collaborators that Harriet what some aspects of inspection integrity medical robotics service robotics and nuclear decommissioning as I mentioned and another project with with the NASA humanoid which some of you here are aware of because MIT has been working on similar projects as well so what I'm going to do next in the sort of 40 minutes or so is to give you examples of some projects that we've been looking at and then talk to you a little bit about the underlying science that enables those projects to to succeed or at least create group of concepts so the the first of those is what I call shared autonomy for remote operations and the concept is is fairly straightforward you want to have remote operations or what we call assisted teleoperation with autonomous behaviors when there is no direct line-of-sight and you have computational delays sorry computational bottlenecks and you also have transmission delays so under those circumstances we need to be able to create systems that can take low bandwidth user input while making sure significant aspects happen autonomously so what you're gonna see in the in the little movie here is a project proof of concept where the screen on the right is what the user sees as console and what's happening on the left is what the robot is doing and this can be separated either by it could be in another room it could be another city it could be another planet so so the idea is a significant autonomy in terms of navigation obstacle avoidance deciding which arm to use to clear debris figuring out how to maintain balance well in this case it's not so hard because it's a wheeled robot so all of those things are done fully autonomously but the humans still have control over designing high-level goals in terms of picked this object drop it here creating some intermediate elements of of control and so so that is the kind of capability that needs to be enabled for operations in dangerous situations like a nuclear decommissioning scenario so what so to make a capability like that happen what are the scientific elements we need to work on or that we've been working on is what I'm going to address next so to begin with of course there is the aspect of real-time sensing and acting with feedback I must admit at the outset that our group are not experts at sensing so we use a fairly significant a fairly off-the-shelf methods standard off-the-shelf methods but there are a few innovations that we've been looking at so for example if you want to take view of the robots - the robots eyes like this this all of you will recognize is an RGB D image so this is what a robot sees a knively so going from such you know a naive sensing capable I detect in what is an object what RF affordances how to grasp it is a big challenge and so one of the things that we've been doing is looking at using model-based visual tracking of objects and to deal with this is Bayesian inference based techniques to deal with things like occlusion things like clutter things like you know when things become significantly out of the depth of the camera image and how can we do this tracking of objects of interest in real time while making sure that it's fast enough that we can change the control loop to be able to adjust for for movement so that's examples of work that we've been doing on model-based methods so I think I should switch this off is that right I think it's better and this way so yeah perfect and the DL bit that you see just now here is that the previous image had the camera static now the camera itself is moving its if you mount a camera on a robot obviously we can't expect the the frame to be still so to be able to do this real-time tracking and and this is model-based tracking and to do reactive control while the camera is moving is a big challenge so this is some work that we've been doing on model-based tracking systems so the other bit the video this I wanted to particularly play this video because this is out of MIT and so this was work done as part of the DARPA Robotics Challenge but this was done in collaboration with some of the people in Edinburgh including people like Morris Morris Fallon and some of our other colleagues and some people from my group on in this case this is an example of how we can use real-time sensing to do footstep planning so short horizon footstep planning with real-time feedback and control right so if you break this down into the steps required for this to happen you need to have the ability to do real-time sensing so here's in this case it's footstep planning short horizon footstep planning taking stability and real-time feedback into control into consideration this is more on the terrain segmentation and visual processing side of things so how do you take sensor data break it up into planes do plane fitting with it understand where our step herbal areas and then do planning on top of that so this shows you examples of the full control loop from sensing navigation real-time control to RL so so how do we how do we actually realize something like that so one of the things one of the platforms and again this is open source please feel free to download this try this out there's lots of documentation lots of examples so one way of dealing with that is by making sure a complex motion planning you know a problem like this is appropriately separated into tangible manageable bits so we have taken the concept the problem of integrated motion planning for share autonomy into a framework called exotica which has got three elements to it it's got a motion planner which you can swap in and out various off-the-shelf planners as well as some indigenous planners that we have also worked on including some Bayesian inference based planners so most of you would recognize it inverse kinematics planner or MPL those who work on motion planning would understand what om PL is but we can swap that in and out then we've got the the planning problem where you can define different task maps in other words we can define variables of interest so for example if you want to maintain a robot balance with while doing some other tasks you can define what exactly is the task map that you care about from it from an objective function perspective from a cost function perspective and finally we define the planning scenes which includes all the you are DFS and all the kinematics and dynamics motion plan stuff and the good news about this is that whatever we do we have got a very systematic way of comparing the where failures happen is it because of the motion planner is it because of the fact that we have not identified the right cost function do because you know a lot of people work here on cost shaping and it's not obvious in many tasks how do you specify objective functions so is it because of the cost function or the task maps or is it because there is some mismatch in the robot model so if we are able to tease that out in comparing many algorithms in a systematic way then we can make progress in trying to figure out what exactly is issue so that's that's a way to do that and so we have taken this and and deployed this in many many scenarios this framework we have deployed in many scenarios including manipulation in dynamic environments so you'll see in the video next is an omnidirectional mobile platform doing what we call the chickenhead problem and this is a company that spin-art spun out of our lab called aerobatic which is integrating the mobile base with the hand the control of the mobile base with the hand so typically people separate the control of the mobile base and the hand and then try to solve it but if we now integrate the the solution then any disturbances in the base can be compensated for the disturbance in the arm so because of that we can do things like in this case sensor placement on a realistic setting without stopping so here's an example of taking a target making a sensor placement without stopping and adjusting for various force based task goals while the robot is moving on the fly so and this this kind of technology is extremely useful in warehousing situations where you don't want the base to move separately go stop pick up things and move ahead so we've deployed this unfortunately this is in Japanese but we have deployed this technology in in Hitachi's automated warehousing solution that they are now testing out and in the transport labs so this is you know innovation there comes from the fact that you're doing full-body control without separating the base and the hand so that we can now do seamless you know tossed delivery without stopping and this this kind of hair autonomy aspects have been one of the areas we were looking at include things like the nuclear decommissioning disaster recovery collaborative manufacturing and also in underground so in this case in with the company called cos stain we've been working with the Crossrail project in London where the digging a big hole trying to create a new subway system and some of the technologies and share autonomy is being deployed on the cement sprayer where humans do high-level control and the autonomy is built into controlling the nozzle using the live sensing from various fiducials so it's a way of saying how can we make some of these machines more idiot-proof how can we reduce the amount of technical skills that's needed to operate a machine by taking real-time sensor input into account ok the second aspect I want to briefly mention is about using robots for remote inspection maintenance and repair of assets so and this is the aim the bigger aim of the project is to try and get multiple robots so quadrupeds on surfaces flying robot drones on various higher high-rise elements and underwater robots to collaborate for maintaining for asset reporting and our focus in our group has been on using finding clever ways of doing motion control on quadrupeds robots so this is an example of a corporate robot that we have in our lab it's manufactured by a company called antibiotics in eth where the idea is how can we do efficient motion planning control in terrains that are not suitable for wheeled robots so in this case climbing stairs and going in fairly slippery surfaces dealing with in a sensor transport in these kind of scenarios so to be able to do that what are the kind of underlying technologies we've been working on for planning motion planning and control in these scenarios so one of the things we've been looking at which i think is an exciting area to work on is using clever representations to bridge the gap between Robotics speak and the control speak and what humans provide us input so in general we can provide targets for for for human input for say you could say I want to take an object and place it on the table between some objects but when you want to translate that to real-world robot control you need to be able to explain that in terms of coordinate frames so the concept of topology where we can define certain in variances in the targets that we define can be used as a robust way of specifying targets and then finding gradients in that space in that alternate space efficiently so let me give you a very simple example to illustrate that so if you want to reach inside that box with a robot arm you can do it in any number of ways but if I represent the targets using topological constructs which are more relational then you can deal with the replanting of the hand without doing explicit collision avoidance by doing gradients in the space of topology so here's a simple example of a robot arm reaching into this box but in a moment and this can be done in any number of ways right this is fairly trivial robotics inverse kinematics but now if you represent the targets in topological constructs when the box moves the concept of what is inside and outside the box moves with the so if you specify targets in topological space then without explicitly planning you're able to adjust to the the concept of where does an object how should we replant the end effector so that topologically a it still remains inside the box so that's just one example of topological constructs there are many other examples like right spaces for continuum robots for snake-like robots also for dealing with ropes dealing with flexible surfaces and flexible manipulation we've used various different ways of using relational metrics so some of the papers this paper here for example describe some of the other metrics that we've used and these are particularly important in dealing with high dimensional robotic platforms like the snakes the snake robots so this is again another collaboration with the company called OC robotics for inspection and traditional motion planning methods tend to not scale when you have God continuum robots but if you start defining goals in relational coordinates in topological coordinates in terms of rights in terms of things like rotation around an axis or if you provide gradients in terms of wrap how much how many times it needs to be wrapped around an object for example that these abstract representations are clever ways of working in the right topological space to deal with the computational complexity of multi contact interactions and multi contact in in this case it's it's fairly complex structures so that helps us deal with some of the aspects of the complexity oh yeah and here's just couple more of videos just showing you where we have deployed this concept of topology so here's a person in this case we're tracking the people and then the relation between the person and the robot is defined using what we call interaction mesh so interaction mesh is another way of topological constructs that look at relational yeah so relational schemes so rather than saying here's a Euclidean distance between collision objects we say maintain this topological construct so that automatically helps us deal with you know obstacle avoidance in these kind of scenarios okay so one other thing that we've been looking at and working on is dealing you're using the concept of variable impedance actuation so even if you have the best sensing and representation even if you have the best representation for that particular task the world is still uncertain so you will still have to cat city in its execution you will still have uncertainty in sensing so one way of dealing with uncertainty in the world is by building in inherent compliance into the mechanism so we've been looking at both novel mechanisms for building compliant actuation systems in collaborations with with firms like DLR and so the the platform that you see on the second is the DLR has the arm and all the other bits the brachiation robot and the robot on the right to that you see our robots that we build at Edinburgh and these are all of these robots what's unique about it is every joint has the ability to modulate stiffness and damping on the fly so you can and so rather than just saying produced certain amount of torque you also have the ability to say change the motors in a way that you control the the stiffness of the joint and the damping of the joint so that's great because in general if you make the robots very soft and not very stiff then you know that you're able to deal with disturbances you're able to deal with with external disturbances well but can we optimally modulate the stiffness for various tasks for from a perspective of maximizing performance from a perspective of minimizing energy consumption for maximizing you know efficiency and that's what we've been looking at in some of our work so we've been looking at formalizing this problem of stiffness and damping optimization in the context of optimal control so I want to go into a bit more detail in this about this because I think it's worthwhile looking into this so the way we do that is is fairly standard so on the you need as I said we need compliant actuation you need the ability to modulate both the torques and the stiffness given the inputs so the Q and the you refer to the state and the command input and we have the ability to modulate both the output torques and the stiffness so you need a mechanism which is able to do that and you need to be able to move between these talk stiffness curves okay if you have that if you have a mechanism which can do that then you can formulate that in the classical optimal control formulation using modeling of the dynamics which includes the stiffness equations then you specify a control objective in this case here's an example of a control objective which is trying to minimize the force but maximize the distance that the ball is going to throw so you will see an example where we want to throw a ball by optimally modulating the stiffness of the joint so we can define a cost function which looks like this and then you can use your favorite optimal control solvers it can be you know IL qg d DP there are many solvers that you can use to come up with an optimal control law that gives you both the the change of stiffness and damping through the commands through the motors and also the feedback gains so so once you have that so we can solve that using classical techniques but we've also found a way of reformulating this problem as an an optimization framework in using machine learning so again those who understand the the graphical models version of of optimization you will recognize very very quickly that this way of defining States and control and transition probabilities as an MDP and then formulating a cost function that is again captured through a latent variable is it's a good way of representing the same problem that you saw using classical optimal control so the relationship between those two formulations where things that we've sort of done about yeah four five years ago you've looked at present with this at the RSS and and the advantage of doing that is that once you have this formulation then we can bring into play significant significant new kinds of techniques for solving that problem so including things like variational inference in machine learning where you can integrate over some of these variables to find the optimal solution this in this case the optimal solution is a variable stiffness actions modulation that we have and the good news is that if you look at point one inference based techniques we're working at multiple abstractions so all of those terms the x0 and y0 all of them refer to the different levels of abstraction now if you remember when I mentioned you can represent your targets as either joint angles or using forward kinematics end effectors but you can also represent them as a graph of interaction between objects and key points of interest on your robot hand so those are different levels of abstraction of the same problem but to do inference with it you have to be able to seamlessly go between those different forward and inverse relationships in in inverse kinematics is nothing but just computing that equation but now when you add an additional level of abstraction you have another forward and inverse thing a mapping and in some sense you need to be consistent across those different levels when we do the planning and that is what these variational inference methods allow you to do this was this way of formulating gives you that explicit way of inference across these spaces okay so what can be achieved out of something like that let me just pause that what are you going to see next is a video that involves a two link arm and each of the the links the two links have got the ability to change stiffness and damping and the tasks for the robots to optimize is what is the movement to be that it needs to make in order to throw the ball as far as possible so that's the target the optimization is you you're given up in this case we do at a fixed time window and the optimization is what is the modulation of each of the joints both in terms of the position and the stiffness in this case the stiffness is realized through a Mac Kappa system which has got elastic spring in it in a joint right and so that the the two each of the joints have got two inputs one controlling the what we call the equilibrium position of the joint and the second one controlling the stiffness of the joint and that's all so you've got four inputs and the output is just the distance thrown and we've we find and we give it to the optimization system and the output is this so it's basically pumping energy into the system storing energy with the springs and realizing throw that is much more energetic then what can be achieved by a single swing or so we've compared this against the other option which is keep the stiffness constant we can find an optimal constant stiffness and then do the same optimization and I don't have the time to go into the details but effectively it's about 20 to 25 percent benefit in this system by being able to modulate the stiffness during motion during loading and and that's not surprising because if you think about if you're on a swing and you kind of pump energy into the system using the right stiffness right timing you can get pump energy into the system but now on top of that if you're able to adjust the stiffness during that momentum swing you can you can do it much more efficiently so we've taken this framework and deployed it into even yeah I think I don't have the other video running but anyway and so we've done it on the Hathi arm at DLR as well this got just higher degrees of freedom same concept but also we take exactly the same principle and deploy it on quadrupeds robots so in this case we've done some work on adjusting stiffness and modulation is modulate the stiffness in this case we have the ability to modulate stiffness in various directions so in this case we can adjust just make it much more stiff or less stiff in certain directions and the idea is can be algorithmically figure out depending upon the terrain that we are operating in they can be can we figure out what is the optimal way of adjusting in this case task level stiffness so not joint stiffness but task or operational space stiffness in order to optimize the trajectory and here's another example of the same concept of using multiple arms so effectively quad repair is nothing but you invert a hand and that becomes a quadrupeds so we've essentially used that same principle of saying can we use again force based control to in this case manipulate multiple object in this case a single object with external inputs where the it's completely decentralized control and all we're doing is adjusting the stiffness between the object of interest and the manipulation so because of that we are also able to now deal with things like adjusting the weight so if suddenly somebody comes and changes the weight using learning systems we can deal with that and after a while the system is going to realize that there's been a change it gives some feedback figures out the cause our reasoning behind it and give some it improves the risk the models so this this approach we've been working in collaboration also with Honda Research Institute in Offenbach in Germany to look at dyadic collaborative manipulation and this is relatively new work because here we're looking at optimizing in we're doing hybrid optimization so what we're doing is we're optimizing three things actually four things but so we're optimizing the trajectory of the motion so a dyadic collaboration is when a human and a robot has to collaborate on manipulating an object together there's of course an element of identifying the policy of the collaborator so that's a that's a separate problem so we have assumed that we separate that problem we've assumed we have a good grasp of the policy of the collaborator so with that full knowledge if you now incorporate that policy of the collaborator into the control system of the robot that is manipulating the object then we've got three things that we need to optimize we need to optimize the force and the trajectory of the object that needs to be manipulated based on the goal that comes from the collaborator secondly we need to optimize the grasp hold locations and thirdly we need to optimize the grasp change sort of timing so when do we need to re grasp when the robot moves so it's a it can you can formulate it as a mixed integer programming and surf method or you can do it as a hierarchical task where use a star to compute grasp hold changes or the discrete elements and then you can do continuous optimization on the trajectories so so now this is work that we presented at coral this year since then we've made significant progress on trying to make this much more quicker and real time doing much more efficient computation so the a-star competition is the more efficient but more importantly the hybrid optimization using moose you know engines like K nitro for example is what we use for nonlin optimization and we're able to you to get much more responsive behaviors but the success of this really boils down to breaking the problem into two steps one is doing this sequence optimization and then doing the hybrid dynamics within each of those sequences I can talk to you more this is a really interesting problem we're very excited about this I don't have the time to go into much details but happy to talk to you guys afterwards and again where do we deploy this we've been using this on various projects including things like high value manufacturing settings we've got an you project called memo which looks at robots for multi contact interaction for human robot collaboration and that is where that's our application domain in some sense okay and so the next bit is on the NASA Valkyrie humanoid platform I don't need to say much about this here in MIT because you know you guys have had a you had one of these platforms here for a while and it was indeed a joint effort between the groups at MIT IHMC a NASA JSC and us on getting robots to fulfill full-body motion planning and control but just for fun I'll show you a nice little BBC clip that for those of you are not aware of this project of what this whole project is about this is from about two years ago NASA robot worth over one-and-a-half million pounds which has just been delivered to researchers at edinburgh university their task to program it to act like a human so it can carry out space exploration that's just too dangerous for astronauts our science reporter Victoria Gill has had exclusive access to the robot and the team 250 miles above the planet the International Space Station has been home to more than 200 astronauts and one Robonaut but the next generation of this type of robot is being developed back on planet Earth and it makes this one look quite retro you are looking through the eyes of a very human-like robots this is Valkyrie it's necess six-foot humanoid it's designed to work in disaster zones and even go to space but it's just arrived here at the Edinburgh Center for robotics so that programmers here can push the boundaries of how humans and robots work together worth in excess of one and a half million pounds with 44 movable joints scanning lasers and cameras to simulate human vision this is NASA's most advanced humanoid Valkyrie is an amazing and unique piece of hardware because there are only three of its kind in the world but the team here has to create a set of instructions that will allow this robot to understand how to use its body to carry out every new task you need to make it do the things that we take for granted for you and me walking balancing dexterous manipulation it comes to us naturally for getting a robot to do something like that takes a lot of effort with a core set of human skills Valkyrie could be put to work in unpredictable environments I'd like to be able to very benign things and everyday actions to be able to do what a human being does to be able to mitigate the situations that happens sometimes in a disaster early for example of Fukushima to be able to do maintenance on the International Space Station is what NASA are interested in it's hoped that humanoids like Valkyrie could be sent ahead of human astronauts to explore the surface of Mars but this very young robot so I'll show you so that was just kind of the introduction to what this is about two-and-a-half years ago and since then we made some decent progress with the work on the Valkyrie project M so here's some video some of the videos again this is about now year old on autonomous navigation so again full body motion control navigation and and you'll see in a moment that there are various obstacles we place it's a bit hard to see because the lighting but at different obstacles at different spaces so we're using real-time sensing to figure out where the obstacles are you're doing what we call i drm based optimal planning for figuring out for grasping that object what is the best pose for the robots to stand so that it can have maximal manipular bility a and then doing the last bit of the grasping and and manipulation is just standard motion planning using concepts like om PL or or any of the other planners so but the real challenge is is taking the live sensor input and changing behaviors so for example here's an example of what the robot sees how it sort of builds the map of the environment around it and combines that map with the the real-time sensing with the local navigation plan and in a moment you'll see also the lidar based you know planning of the footsteps and then in this case navigation and locomotion so um so there are still lots of challenges in terms of this part of lab I think a new one into sublime to show you one more thing so yeah so that's again just last year we also made some progress on locomoting on uneven terrain so that you can see on the right is the real the sensing and using that live sensing being able to plan a footstep plan over uneven terrain and then do some local manipulation and you'll see in a moment that the hands really are really terrible and bad it will fail and you know that's by the time the hardware was in really bad shape it's a to doing multiple trials this but but the good news is that we've been able to these are fairly aggressive aggressively is spaced and slanted surfaces and depending on where you place the object you the the the robot decides for itself well it's the most stable position for it to hold and then do local manipulation so so that's the challenge so and on this related note I just want to give you back so this is this is work from really a ten years ago during the time I was doing my PhD 10-15 years ago now and but that is still quite valid in the sense that we still use data-driven learning and that's one of the core us piece of our work is we we work with existing Newtonian models but we also learn residual differences between the difference in dynamics between a model to deal with model mismatch and earlier we used to do this purely in the inverse dynamic sense and do model based learning and NPC on it but now they're able to do this kind of this is an example of how you use data to collect the data and update the dynamic models but now we are able to do this in the loop with this optimal control setting so because of that this is as I said a video from historic video a long time ago including learning dynamics then learning task dynamics where we are able to collect data of various robot actions that then lead to finally learning an optimal policy of balancing a poll and and this as you can see is work with Stefan a long time ago but that this is still very valid because this this work that we've done is now it becomes a core module of the updating of the dynamics but now we're doing this not just as an independent loop but we're doing this in the texts of the stochastic optimal control loop so earlier we used to you know able you used to just instead of here we have to put an analytical model but now you're able to do this on on the fly still lots of challenges in terms of data number of data points complexity of the learned model but we at least in low dimensional problems we are able to complete this loop which is actually quite nice skip this last bit and a bit of fun I'm going to show you live demo of brought some stuff along with me and this is where we using this concept of shared autonomy also in the healthcare so we've done some work on on upper limb prosthetics so here's some work that we've done where we're combining multiple things like EMU and so EMG and I am new sensors to get better prediction of what the intentions of the humans users are and this is some trials with actual amputees where the EMG is is attached to the remainder of the stump of the amputee and then they are able to we're now using some classification techniques that combines EMG and I am you to do pre prediction of the grasp so that we can pre shape the grasp so that the cognitive load on the person operating the hand becomes much less so so this is you know classification that is a regression in the sense that here we're trying to do a unilateral bilateral coordination of fingers and decoding that using EMG but what I want to do next is to show you some stuff live so I need two volunteers to come and try this any couple of volunteers yeah that's one need one more yeah okay so first I'm gonna try and get you to wear this maybe go left hand yeah actually okay if you stand here and then wear it on your left hand and maybe tighten it up a bit but yeah good okay so what we're going to do is so first I'm going to try and do this using my hand so that you know it works then we're gonna try and generalize it to your EMG a sensor so this this is an EMG sensor which is basically a sensing the muscle activity on my hand and it's sending very coarse open and close signals and then when I turn it on you can see that it's able to to respond to the EMG sensors and now if we get something that we can grasp do we have a check of this a ball or something I think yeah okay so so what you can see if you lift it up so that people can see so so each of the fingers decides for itself when it should stop closing so I'm only sending sort of high-level open closed signals and the the fingers are depending on where I keep the ball it's it's kind of adjusting its grasp position and and this this works for different objects different shapes of objects and the the key trick is that and this is where we this concept of shared autonomy is deployed here because the high level signals are open and closed comes from the hand the smarts in terms of when each of the fingers should stop closing is decided by the load sensing on each of the fingers so because of that you can take arbitrary shapes and you can grasp very robustly a arbitrary shapes of the objects but now there's another interesting aspect I'll maybe I'll get you to try it as well so let's see if it generalizes so it's not there's no magic in the sense that it's not it's not my kinematic sorry my EMG sensors that it's tuned to so maybe so try just moving the wrist yeah yeah and yeah so you can see that it works but now also if I get my phone one more thing I want to show you so I can actually bluetooth it into bluetooth into this hand and and actually have a three-way connection between here's a human providing the input some smarts in the hand but also now I can have a third party device connecting into this - in this case for example pre-shape the grasp so so you'll see in a moment I am able to change the mode of operation vary in range things so now I have now if you'd move your hand so you can see I've changed the mode of operation using the Bluetooth and keep going left and I can change it again and this is you know a finger grasp this one the next one is called a credit card grasp that's to grasp a credit card oh hell you so so that the the interesting bit is that and this is an example of a 3-way connection to the same device through the human to a third party device and through the smarts in the hand and that's where things start getting really really complicated and so thank you very much as volunteers and so [Applause] and and that really highlights the issues that I've been talking about in terms of getting human-in-the-loop control with these kind of complex structures so that was on upper limb exoskeletons we've also got some work that we're doing with lower limb exoskeletons so this is work again in a lab gait lab where you've been developing some what we call the Apple the ankle pelvis orthoses which can apply forces on your legs while you're walking on a split treadmill and the whole idea of this is being able to find the right kind of control system that can assist a person during walking while by applying forces on their legs so we have the ability to lift the treadmill up and down and provide perturbations so the whole aim is to go towards finding ways of assisting but also being able to prevent Falls using such devices so we're not there yet we're still working on the real-time control loop in terms of the feedback but there's some work that we've been doing on exoskeleton stuff okay so that's more or less that what I want to tell you a little tell you about the science have you been working at but I thought I would end by and showing a little bit about this Enma Center that we run so I co-direct this with David Lane at heriot-watt and the whole idea is looking at how we can take some of the innovation from the research that it's happening in our labs to market by looking at these three strands so acids kills and clusters so assets we work with world-class equipment skills we have a doctoral training center and we have some clusters effectively working with governments for policy making so that's the kind of broad overview of the Center for doctoral training and this is some stuff on the equipment that we have the kinds of you know the humanoids the underwater robotic stuff and and some of the robots that you are very well familiar with but and of course in addition to all of these things I think one of the key things that we've been looking at is other questions in doing robotics so the moral and ethical aspects the security their responsibility for understanding cause all inference transferring control when how and fast how do we deal with learning systems so these are things that we're just starting to tackle and in my new role as I have a 50% affiliation with Alan Turing Institute which is based in in London in the British Library in London we've been trying to address some of these questions so you know we've got if any of you are interested in any of these questions or these things please come and find me send me email because at the Alan Turing Institute we are looking at developing this vision of looking at creating a world leading scientific program of data-driven AI research an innovation that addresses some of the unique challenges arising from and towards deployment of wrath' systems so this is really what we are trying to do and these are the aims and objectives but from a scientific perspective the three themes that we are focusing on is scalability of algorithms and the constraints methods for multi agent computations and mostly from a Turing perspective which is very unique is is looking at different ways of doing verifiable robust and explainable decision making under raciss so these are the kind of broad topics that the Turing thing that we're driving in the UK and and to be able to do that we are across various sites in the UK including Edinburgh we're setting up what we call the living labs where you've got robot for extreme environments you saw examples and projects today if you're also looking at robots for healthcare and assisted living and you know how can we create robot a scrub nurses which can do assistive mechanisms and the way we're doing that is we are based in the bate something called the base Center after reverend base which is just a new facility of our old informatics forum where we're doing all of these things exciting work so any of you there if you here in the UK please come and visit us in Edinburgh and I'll be I'll be very happy to show you around so I'm going to stop by coating one of my favorite authors and this this coat is very true in robotics we tend to overestimate technology in the short term and underestimate technology in the long term so we do need to worry about we need to worry about the hype that robotics is creating to say that you know we solve everything at the same time we need to think about policy that we need to make in order to to address some of the challenges changes in demographics that's going to rise out of deployment of robotic platforms thank you very much last thing I want to stop by showing you an interesting video which will highlight that in spite of the tremendous advancements in robotics humans are still amazing at certain tasks okay so you probably have many of you probably seen this this video and you probably definitely recognize who this person is so this is Roger Federer and in a moment you will see an incredible human motor control [Applause] [Applause] don't be nervous okay thank you very much [Applause] and the variable impedance part of your talk for the ball throw and you said the objective was maximize the distance for the one where they were bouncing the objects with the forearms what was the objective in that part sorry balance ah in the collaborative balancing stuff so in that one it's it's mostly about minimizing the force that needs that it can apply without dropping the the object so obviously it's like a Lagrangian so the the first part of the Ranjan is that you should not it's a friction cone constraints on the on the surface so you should not go the fishing tron should not go be below a certain threshold but then at the same time the obviously the other extreme of that thing is you just press it press so hard that it doesn't so so you balance that by saying minimize the amount of force the net force that the object is experiencing so that gives you the trade-off in the legged X acceleration part how do you evaluate the performance of the exoskeleton and how you know if it's everything right in collaboration with the human good so that is something that we are really um you know it's it's a new area that we're working on and I think we are making significant contributions in this area because that is exactly the question that lots of people are asking so traditionally the way people evaluate gait is by looking at kinematics and looking at various things like does the kinematic pattern follow a certain you know rhythmic symmetric pattern sometimes people look at the center of pressure whether there's something called the butterfly pattern when you project center of mass onto the surface then when you walk you get a kind of butterfly pattern and the symmetry of the left and right gives you a measure of the stability so those were tradition techniques but what and again in in rehabilitation people typically try to shoehorn a particular person's walk to some standard kinematic walk of a standard person which ignores all the very natural variability that a person has during a walk so instead of that we've been now looking at about seven or eight different metrics that include things called something called marginal stability in other words how do you what is the response to push recovery and things like there's there's another element which is which has to do with the so not the support polygon but it is a derivative of the support polygon where you look at boundary how the boundary moves and along with the with the leg movement and to make sure that the center of mass is on the average within that support polygon so this is very different from the you know dynamic versus static gait in in humanoid robotics but the principle is similar to that so there is frontiers in on AI paper that we've published on different metrics that we have compared for normal and abnormal gait for gates that are where people walk with a limp versus people walk are disturbed and from that we have derived about two metrics that are really the which captures all the elements of the invariants so that's the first step trying to find out what is the correct metric the invariance that can capture what is a stable gait and what isn't what is an unstable gait while incorporating human natural variability the second step of now how do I take that and change my activation pattern of the exoskeleton in order to provide the right assistance is the second question and for that we've been using optimization techniques and gradients in that metric so on that derived metrics place we compute gradients at the moment only way we can compute gradients is through sampling we don't have full derivative through that to tell it how to change this the forces we are working towards it but at the moment we what we do is we have certain sample points we have some active sampling techniques for finding gradients in that space sorry not really finding the metric evaluations in that space and then finding an interpolation in that space to go towards an optimal you know force profile so we have to parameterize this first profile the the way we currently parameterize this is is right rise time zero zero cross over time and maximum peak so those are the three parameters that we optimize in the exoskeleton thing so eventually we would like to go towards a non sampling based method which is more you know principles in terms of the derivatives but for that we know that it's quite hard because we can build models but the models are heavily inaccurate due to soft tissue contacts so that's the problem at the moment discussion outside with the food 