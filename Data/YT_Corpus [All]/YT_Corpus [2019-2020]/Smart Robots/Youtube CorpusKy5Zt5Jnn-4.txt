 wow! It smells great but I hope it tastes good too sure I can What kind of music would you like to listen to? voice control interfaces have changed profoundly the way we interact with technology however they are bound of fail in interactions with humans. these failures can be critical because they require costly human intervention and can cause users to lose trust. most research approaches start with the assumption of flawless interactions however as conversation is the main interface visual feedback may not always be available. users typically expect an interaction in an orderly fashion in this paper we use a set of failures informed by taxonomies of failures in previous studies in HRI. the induced failures represent typical robot malfunctions that have been reported in human robot interactions. they were either task oriented such as giving incorrect guidance or failures that violate social protocols of interaction. all failures had the consequence of delaying users in completing a task. the failures were used where the following in this failure the system simulates losing user engagement and restarts the interaction after the failure has occurred the robot becomes responsive and continues the guidance. A sample use the response to the failure follows in this failure the robot times speech improperly by producing an incomplete instruction and after a short delay continues its utterance. in this failure the robot simulates lack of user speech input by not responding for 20 seconds in this failure the robot repeats a previous statement by asking the user to perform again the previous instruction. and finally in this failure the robot produces an erroneous instruction by asking the user to pick up a non-existing object. using the failures above we asked human users in this corpus to engage in a referential communication task. users were instructed how to cook spring rolls by two robots. spring roll recipes were broken down to interaction segments and used for instructions the recipes were non trivial so users had to interact with the robots to successfully complete the task. interactions were segmented into dialogue acts and the robots would reply to clarification questions from human users. users were not instructed on dialogue options but had to interact with the agents to find out participants were led to believe that robots were autonomous. however to dismiss technical problems we used a human operator. there is a long-standing effort in bringing human likeness to robots but how robot embodiment affects users behavioural responses to failures remains largely unexplored. smart speakers and human like robot embodiments for example differ in interaction affordances. from transactional to relational interactions. while robot embodiment has shown to typically positively impact human robot interactions it remains to be explored if this effect persists when the robot fails the robots we used were the following an Amazon echo device that used voice as an only output modality and the furhat robotic head for the human-like robot embodiment. time critical situations are difficult to convincingly simulate in the lab. as such most studies have focused on robot failures where little is at stake. in this work we introduced time pressure to half of the participants using a timer on the screen. we expected that under time pressure the same failures would have a higher severity on the task. we therefore separated failures too low severity failures little to no consequence of the failure versus high severity more severe consequence of the failure respectively participants under time pressure were told that they would receive double the reward if they would finish the task on the top 20% fastest of all previous interactions. the robots failure would delay participants decreasing their chance of doubling their reward. at the end of each task the experimenter would enter the task room inform participants that the task time was not on the top 20% and that they should proceed with the post task questionnaire. at the end of the experiments participants were debriefed that this was part of the study manipulation. in a user study with 44 participants we investigated the impact of these two different factors. one is robot embodiment and the other one is failure severity we used a set of behavioural and subjective measures to investigate the effects of embodiment and severity. since we manipulated time pressure across conditions we extracted the total task time with each robot. we expected that high severity participants would be faster when an anticipated reward was at stake rather than low severity participants. we segmented frames of each interaction into failure and no failure. in these time segments we extracted temporal measures from users' multimodal data in gaze and speech. using motion capture we collected participants' gaze as informed by their visual angle and measured proportional amount of gaze towards the robot as well as the number of gaze shifts. we transcribed the corpus and extracted the number of words spoken and also extracted users' time to respond to a robot instruction. before interacting with each robot we asked participants whether they would like to use the system regularly. the question was repeated at the end of the interaction with each embodiment with the goal of comparing before and after results across conditions. to compare levels of trust across conditions we used two questionnaire items coded into seven point likert scale items. and to measure the level of intelligence our participants attribute to our robots we extracted two questionnaire items from the godspeed questionnaire. finally we measured the perceived social presence of each robot which can be described as the sense of being with one another. we used four dimensions of the networked minds social presence questionnaire. we divide each of the four dimensions into the two distinct directions of social presence perception of self and perception of the robot. we observed that the manipulation of time pressure caused a significant effect on the task time with the robot .participants spent less time in the high severity condition than in the low severity showing that in time pressure they were indeed rushing to finish faster at the task. analyses of variance showed significant main effects in gaze in the factors of embodiment and failure. proportional gaze to agent is higher with a human-like embodiment hl in the graph but also generally higher when failures occur. we also saw that participants under time pressure shifted their gaze less frequently under no failure cases. however more frequently when failure occurred. similarly but in the direction of the smart speaker 3-way analyses of variance showed significant main effects in embodiment and failure. subjects uttered mode words therefore more complex utterances with the smart speaker but also when failures occurred. furthermore while users' intention to use the human-like embodiment keeps constant across time intention to use the smart speaker decreases drastically after the interaction. here no significant effect was found on failure severity regarding trust and perceived intelligence two-way anovas showed no significant effects regarding our main trustworthy item. all of the other items were significantly higher when users interacted with the human-like robot. social presence dimensions from the direction of the perception of the robot were rated higher in the human-like robot. conversely in the direction of self only the co-presence dimension was rated higher in the human-like embodiment. in the same direction however we found an interaction effect in the message understanding dimension. that is in low severity environments users have an easier time understanding the human-like robot. where in high severity environments users have an easier time understanding smart speaker messages. users considerably changed their intention to frequently use the failing smart speaker. this effect was not found for the human-like robot suggesting that the human-like embodiment may mitigate the effects of failure. additionally higher levels of intelligence and competence were ascribed to the human-like robot even if it failed as often as a smart speaker participants did not consider that they allocated more attention to the robot according to the social presence measures. the behavioural data in contrast indicate that users do change their behaviour with the human-like robot intuitively participants are unaware of their increased attention and behavioural dependence yet they still exhibit reactive communication traits typically seen in face-to-face communication among humans. nevertheless these findings indicate that human likeness is not always favourable. on time pressure users appear to be distracted by the human-like robot. it is likely that the robot may distract the user from the task by displaying additional social behaviours as more attention needs to be given to the system robot embodiment seems to affect how users will interact with a robot and therefore also how they will respond to failures. users seem to be more sensible to failures when they are interacting with smart speakers nevertheless designers should consider the nature of the task. in high severity situations where a high error rate is expected non anthropomorphic designs may be beneficial. where time is critical designers might want to avoid systems that benefit from the interpretation of social cues as users may be distracted by the system's social behaviour. social interaction is afforded in robot design and will lead to expectations over the robot social behaviours and how users will respond to failures. future research should focus on how these findings can be best applied to the design of assistive robots that will inevitably have to deal with failures and uncertainty in interactions with humans 