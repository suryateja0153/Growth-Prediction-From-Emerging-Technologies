 hello everyone welcome to robotics today the first talk last week was a great success with over 2,000 attendees and a very lovely discussion today we're very fortunate to have a Leslie Academy as our speaker Leslie is a professor at MIT of the computer science and engineering and she has made several fundamental contributions to the field of decision making under uncertainty and the report from earlier and more broadly she has succeeded significant advance the field of intelligence intelligent and economist robotics today she's going to share with us her views about robot learning and decision making for robotics the format is going to be the same as last time we're going to have a talk of about 40 45 minutes which would be followed by a panel discussion and then we'll open up the discussion to questions from the audience that is good that are going to be moderated by the student panel formed by unsaddle James Benoit and Russell then without further ado I'd like to leave the stage - all right thank you very much for inviting me to the seminar and for the nice introduction so I'm going to talk about how to get robots to do to learn to do complicated things in complicated worlds so my overall goal here is to understand AI and to do it in the context of robotics so I really want to make some kind of a general-purpose intelligent robot and use that as a way to study intelligence in general so for me the aspects of intelligent robotics that maybe are not under as much consideration as they should be is handling really high variability so I'm interested in high variability in domains one example that I like to bore my students with is trying to make a robot that could make tea in anybody's house imagine a robot that could go to absolutely anybody's house in the whole world figure out how to make water hot figure out what kind of leaves they'd like to put in the hot water and how to do that right so that's a nice general-purpose problem that I don't think we have even the beginnings of an idea really of how to do or to do any tasks in your house so if we wanted to make really general robots or even specific robots we have to take a position about the process by which we're going to engineer the robots so I'm gonna take the viewpoint here of a robot engineer I'm gonna assume that we as their robot engineers we're gonna make a robot Factory and our robot Factory for right now I'm so I'm a software person so I'm gonna assume that the hardware is fixed and my job is to figure out what software to put in the robot so I imagine that this robot is going to be able to make observations and take actions in the world and ultimately really almost no matter what we can see the program that we put in our robots head as a mapping from sequences histories of observations and actions to the next action so that's not really making much of a commitment at all it's just a pretty generic way to see the problem of behaving in a world so what program should we put in the head of our robot so here's a way to think about that I would like to say that when you come to my robot Factory and who order some robots you tell me a distribution over the domains that those robots might find themselves in so if you were a car factory and you need a robot - well the particular model of a car then you'd give me a distribution over domains that's very very specific it would say oh you just you're gonna be in a world that's like this and you have to achieve this but if you want the robot to make tea in anybody's house then the distribution over domains is very high variance and when you come to me in order the robots I now have a problem of figuring out the program that would optimize this criterion so I wrote down a formula you we don't have to commit to the details of additive reward but fundamentally the idea that we want to find one program that works well in expectation over the distribution of worlds that this robot might find itself in that's the idea that I think is important and the reason that I'm particularly attached to this idea is that if we could understand a distribution over domains and specifications of what they were supposed to do then we wouldn't really need to fight about whether it should be neural or symbolic and whether it should learn or not or whatever that for that problem for that distribution of problems they're really using some sense an optimal strategy so that's a good thing I think we don't have to fight about that but even though that my optimal strategy might exist it's a hard hard job for us the engineers to actually find it so given a specification if you could get the specification how would you find the PI that's a hard problem so really that's what I mostly want to think and talk about which is how is it that we we as engineers can come up with good policies to put in the head of the robot ok so one way to get a handle on this problem is to think about the space of distributions over domains right so they these distributions might vary in character and depending on that distribution we might have very different kind of solutions for the problem of what what program to put in the robots head and then how to find it so one axis of variability is just like how complicated is the problem how complicated is the policy that we're gonna need so maybe solving one particular problem in one particular domain is that not so hard but maybe all of all of T is difficult another dimension is how much uncertainty we the factory have about the problem the robots going to have to solve so these are at least two axes of variability of this specification so we might say oh we have a narrow task that we know in advance or a complicated set of problems but still we know in advance what they're gonna be maybe we don't know in advance what's happening and maybe really it's complicated and we don't know okay so how can we think about making a policy so what we've learned I think over the history of engineering and it hasn't changed is that fundamentally the way we tend to approach these problems is we pick some class of models in some way of acquiring the models and we have to minimize engineering costs so what is engineering cost well there's human engineering cost there's like how hard it is to write the models down how hard it is to make a simulator how to do reward shaping all these things there's work human engineers have to do and there's work that machines have to do right so maybe they have to do optimization learning parameter tuning and fundamentally I think the way we should think about this is that we want to pick a model class where humans have useful insight where the optimization part is not too hard and where whatever kind of learning we need to do will generalize well okay so what we need to do then is come up with a policy and so let's think about first kind of forms model classes for policies for robots and then we'll think about how to find that so what are the forms that a policy might take well the simple form of the policy is policy okay so we could say all right we're just gonna think of our problems policy and and that's the only architectural structure we're going to put on it we just have to find the good policy but trying to find searching directly in policy space is actually pretty hard right so as engineers if our problem is to just produce that policy directly that turns out to be kind of hard search space so maybe we can think differently maybe it's easier to come up with some kind of a value function maybe that's easier to represent than the policy and then that value function we just have to build in a little piece of an algorithm which says our max roughly and we can make a policy out of a value function the reason we like value functions is because dynamic programming gives us a really great handle on how to there are lots of problems though where the dynamic programming principle doesn't actually help too much and so this isn't I think the best way to do all kinds of policy representation another strategy which you might call model predictive control would be to put a planner actually inside so now we have even more algorithmic commitment we might say we're gonna put a planning algorithm inside the robots head right this big blue box is still the policy right it's still the policy that's in the robots head we're going to put a planner in there and then how we're gonna represent the domain is by representing some kind of a transition model and a reward function those are the models and the planner algorithmically will figure out what's going on and then what we have to do is put those models in okay that's a way it might be hard to do this at some very raw level right if the actions have to be something like motor torques and the observations have to be something like images that might be a difficult space to do planning in so we might say ah actually the planning idea is really useful but it's more useful at a higher level of abstraction so that we might say oh well okay what we'll do is we'll build some level controllers maybe those are little policies and maybe I as an engineer can write those or maybe we can do some learning to find them and then maybe what will be better is if I can do planning at a higher level of abstraction and my actions will be to call these low-level controllers it might also be that I'll want to do some perception and estimation interpretation from the raw image stream to get a representation that's also useful for the more abstract level of planning so there's an architectural maneuver and even then we might want to add some more kind of hierarchical ability and I'll talk a little bit about hierarchy later so we as engineers can pick where we want to be in these classes of models we can vent other ones too but it's so one important decision to make when you're faced with a problem is what would be a good class of models the next question is well if you pick a paso model so how are you going to actually decide for your particular distribution of domains that you've been hired to make robots for how do you decide what models - you know what parameters with the domain-specific parts of those architectures I just showed you how do you find that so we have a bunch of different ways of approaching problems like this classical engineering is that well we just like write the stuff down right classical engineering is awesome classical engineering that's Atlas - parkour right so there's lots of things you can do by understanding your problem well understanding science and math and just kind of writing down good control structures um there's another sort of surprising thing that actually occupies the same niche in the sense that there's a reasonably clear specific thing that you're supposed to do and you know everything about the problem and and that's actually to use reinforcement learning so it seems a little funny like why would you ever use reinforcement learning in a situation where you know everything about the problem and so I just want to talk about that for a minute so why would you use reinforcement learning when you know everything about the problem well the answer is that it's um it's a way to take a representation of your problem or your domain or even a domain distribution that you can make easily right so it might be easy for engineers to make a simulator but difficult for engineers to intuit what the controller should be so in the example of the robot manipulating the Rubik's Cube it might not be so hard to make a simulator for that robot but it's quite difficult to write down the policy so what we do is we make a simulation of the domain we take a reinforcement learning algorithm which is domain independence couple them together let it run for a while and then what comes out is a policy then we put the policy in the actual robot and the policy does what it does and the way I really think is the best way to think about what's going on here is that it's a kind of a compiler right this is a software only maneuver this is a way to take an understanding of the domain in one form that is to say a simulator and turn it into an understanding of the domain and another form that is a value function or a policy and just a little editorializing here if we're going to take this if we're doing this for this reason and I would argue that almost all the reinforcement learning papers that people are publishing right now kind of have this setting in mind either implicitly or explicitly then of course while you're doing the learning right while the simulator is hooked up to a reinforcement learning algorithm it doesn't really matter how much forward you get because that reward is all just like some process that's a computational process that doesn't really matter and it doesn't matter how many interactions you do with the simulator what matters really only is as a function of how much computation time you spend how good is your pause right so not only is this a compiler it's often a kind of an anytime compiler right I could run it for longer and I get a better policy and I want to think about well how can I set things up so that if I work pretty hard computationally I get a really good policy okay so reinforcement learning one way to use reinforcement learning is to solve problems we already understand but we understand them in a different way not in a way that lets us write the policy down you could look at alpha 0 and then mu 0 which is a kind generalization as methods that can handle somewhat more complicated problems by doing some combination of planning online right they don't just learn the policy or just learn a value function they do a kind of model predictive control together with some learned control information and so they can handle I would argue problems that are more complicated in a certain kind of way what I'm going to do now is talk a little bit about a system that we have built called hierarchical planning in the now it's blue blue means that it's completely an engineer so there's no learning here but it is I think interesting from the structural perspective and it will eventually give us a point from which I think we can move upward so let me just talk about that a little bit so before I get deeply into the hierarchical planning in the now part I want to talk a little bit about the planning part so in reinforcement learning we will really just talked about this notion that you can use reinforcement learning to derive a policy and generally speaking we do it by making a simulator and so there's a little bit of you know back and forth at the moment between the reinforcement learning people and the planning people about who has to do more of what kind of work in order to make their approach go so just want to kind of talk about that a little bit so think about these two engineers so there's a one engineer who is doing the reinforcement learning kind of approach to something and then there's this other engineer who's gonna try to build a system by constructing models for a planner instead of by doing the reinforcement learning so the questions of what work do these two different engineers have to do so the first thing that they have to do or one thing they have to do is the RL person has to figure out how to specify a reward function for their problem and the planning person has to figure out how to specify a goal for their problem and so the real word function is interesting so we spent a little bit of time recently looking at the meadow world suite of reinforcement learning problems so this is actually pretty nice interesting set of problems designed in particular for meta-learning that is to say could we build a system that could train effectively on five of these problems kind of offline and make it or on 40 excuse me on 45 of these problems so most of them and then given a new problem learn very quickly because of having induced some generalities about the set of problems so that's a very interesting setup and antowain we are not treating this as a learning problem at all we just were kind of intrigued by the set of robot tasks so these tasks all involve manipulating some objects with a soya robot some are pick in place some involve pulling handles or rotating things and so on and a question we can ask is could we just do those with planning so that was the question we asked ourselves so first of all we went and we looked inside to try to figure out what the goals were and we're really kind of surprised that the reward function right so there forward function is we're in a reinforcement learning problem you specify what the goal is but this is a reward function in meta world for pick-and-place and it's kind of complicated it had at least twelve constants in it and it actually has memory right so this is the reward function that an engineer had to write down I had to say something like well if you don't have the object yet you should be rewarded for moving their hand toward the object but if you're really close to the object and you still don't have it you should be rewarded for closing your fingers and then if you have actually picked it up then you should be rewarded for being kind of up high and moving over and so on so it's it's almost like a little program uh for picking out the odd but but not quite right so I'm sorry I yeah so okay so that's the road function so uh now I'm just being slightly obnoxious I'll say the way that we describe the reward function for that same problem in our domain is something like this which is that my goal is for the pose of this object to be there so I think it might be easier to describe that goal then there or in function in both cases we need the you are DF we need a description of the kinematics as a robot we need to know where the objects are now in the RL case the RL algorithm doesn't need to know that but the simulator does in our case in the planning case the you RDF has to come in it's part of in some sense the transition model is a domain the simulator also needs physics so that's and and that's roughly what you need for reinforcement learning for the for the planning approach we need more right so this is still more mother planning engineer planning engineer had an easy time with the goal and ok time with you already of the place where the planning engineer has to do really more work is in specifying in particular this level of abstraction of control so at least for us to make our approach work we need to define some few low-level controllers and we need to describe a level of abstraction that the planner can use in order to plan how to deploy those controllers so I'm going to talk about a way of doing that next so right you could try to do planning in a raw space but that's not a good idea so instead I'm going to talk about an abstract model the this mostly what's important to think about here is why it's useful to make abstract models and how you should what it means to pick a good one and so when I want to pick some kind of an abstraction for solving a problem well presumably I pick the abstraction because it's gonna make my computational problem easier and it's guaranteed to make my solutions worse but I hope not too much worse right so that's the trade-off and it should be hopefully not too hard to implement or kind of maintain the abstraction and hopefully it will apply to most other problems you care about probably not all of them so a principle for building abstractions that I become very enthusiastic about lately is the underlying view that is taken in multimodal motion planning so I'm gonna talk about that a little bit so for now I thought I think that this view can be generalized it's kind of metaphorically at least generalized to a bunch of other kinds of problems but more concretely let's for right now assume that the robot and the objects are kinematic chains in some kind of three-dimensional workspace right so I'm going to commit to that I'm not gonna be able to solve problems that involve blobs of gas or two dimensions or twelve dimensions I'm like here I am in three space and then I have a state and I'm gonna represent the state in terms of poses and properties of objects in the robot so let's talk about what a mode is and so if we think about a robot and a bunch of objects on a table or in a house or a whole house you can see that as a problem with a very big configuration space right there's all the possible joint angles of the robot but there's all the positions and orientations and configurations of all the objects in the world this is a very high dimensional state space but generally speaking we can only change a very small subset of those dimensions at once right so I can't just like look at my coffee cup and make it move I have to actually move myself over there and then when I'm touching it I can make it move so we can think about the problem in general of describing the dynamics of this whole complicated worlds in terms of describing the dynamics of a bunch of modes and then ways of switching between modes and so a mode you can even mode is a small set of state variables that pretty much always includes the robots configuration but maybe also some configuration variables of other objects and that those inside the mode I can manipulate all those variables but the other ones stay fixed and the dynamics are reasonably smooth it's easy to plan inside a mode so let me just talk about an example here I love this picture this picture comes from a recent paper this idea is actually older and do probably to Chris Houser or maybe I don't know if that's exactly the original but probably but there's a paper that has a figure that I like that's recent so you can think of this high dimensional configuration space with the configurations and variables of the robot and all the objects as being a very high dimensional space and mode is a very low dimensional manifold embedded in that high dimensional space so the simplest mode that green mode might be the mode where I'm not touching anything I can move my arms around and I can change my state variables but nobody else's state variables so I can move around in that green manifold then this is general robot motion planning and control moves me around in the green manifold but if I grasp my cup now I've entered a new mode I've entered the pink mode now because in the pink mode not only can I control my state variables I can control the state variables of this cup now there's a kind of there's a relationship between the cup of my hand that has to stay constant and so on but that's that's a mode and there's a notion of mode family-like so if I grasp the cup in a different way then I'm kind of moving around on a pink slice but it's kind of parallel to the pig slice I was in before I can't easily move from one to the other I can't easily change my grasp on this cup but it's a it's a good way to think about the problem so this gives us a handle on planning because we can design maybe controllers or low-level policies for moving smoothly within modes and we can kind of think at the high level of planning about how to change between the modes what modes what families what modes or mode families can be moved through in order to solve our problem so we thought about the metal world problem and we modelled it as a multi mode of motion planning problem with these mode families so one mode family is this free space motion one is picking up an object right that when I pick the object I move from the mode where I'm free to the mode where I'm attached to the object I move holding the object I put it down I can push an object that's kind of another mode family and in this world there are mechanisms that you can manipulate they're just a one-dimensional kinematics mechanisms things like doors and sliding drawers and so on so we had we had these four mode families and using that we were able to solve a whole bunch of the metal world problems we did 43 out of the 50 there are some of these examples that need a different mode which where the robot kind of just rubs its hand on something and and moves it there's there's pushing things into a hole which we were just not set up to represent and there's some tight insertions that I'm not sure a real robot could do but Moo Joko is kind of forgiving so but we couldn't do those either um but the thing is that we didn't there's no machine learning required right so there was there was a lot of there was engineering time but the comet is pretty general and it just solves all the problems like once you can solve two or three or four of them then ones that are amenable to the same modes are just they're easy to do for the engineer okay so so what we did is we kind of made an instance of this architecture and we built in some basic algorithms and we built some models and one other feature that I think is kind of important is the ability to do hierarchical planning so in some sense there we had like two levels of hierarchy in some sense right there's the maybe low level moving through modes and the high level deciding what the modes should be and so on if you wanted to clean up a very difficult kitchen or travel to another country when we can do that again then you would not plan that whole activity down even at the level of mode switches so another thing that seems important to us is the idea of really doing hierarchical planning where you might start out with a high level goal like I don't know maybe I would like to go to Stanford I would like to go to Stanford I was supposed to visit there this spring but I didn't manage to get out there so maybe I plan in a very high level that involves you know going to the Boston Airport and getting an airplane and so on and then what I can do is I could take that first step which is let's say to get to the Boston Airport and I can plan that out in more detail and then I can take the first step of that which is maybe to you know get out of my building and plan in a little more detail for that and then I can be an optimist I'm gonna be an optimist so being optimistic here means that I am gonna actually begin executing my plan to get to the airport before I've worked out in detail exactly how I'm gonna walk through the San Francisco Airport or indeed how I'm gonna get from San Francisco to Stanford or something like that so we have a structure that lets us make plans and re-evaluate them if they're not working and so on and it gives a fair amount of robustness so now I'm going to show you a robot video I have to give the following disclaimer which is the pretty much any individual thing that this robot does some smart undergraduates could get the robot to do in a week or two better all right so that's okay for them so what's interesting about this is that it's basically the same color that's doing a whole bunch of problems and so that each new problem doesn't require very much work at all so just I'll narrate this you know here we told the robot to put that blue box where the can is it looked and I saw something was in the way I picked up the cans to get out of the way but blue box where it needs to go here we told it to put the green box on the corner of the table it can't pick up the green box it's too big it figured out it had to push it it realized they had to move the other one out of the way and as it's pushing is really unreliable so it also looks after each push to check to see if it worked out and it's doing replanting here we told it to go outside of the lab again it looked and it found these chairs in the way so moved one chair out of the way and then unknown to us it grabs this chair and then just brings it with it like that we did not tell it to do that but we didn't tell it not to either so that's what it did here is looking for a full boil bottle it has to gather information by picking the oil bottles up and seeing which one's heavy to know which ones full this is a I don't know some kind of a silly task and then I think just in the next movie we'll see that it actually we got it running on a different robot in a different lab without too much difficulty okay so I said I was gonna kind of talk about robots and learning and so on and now I showed you this robot doing all these things there is no learning in this robot so Tomas and I so two old professors wrote most of the code and the only learning that happened was that we learned some things but what we did learn I think was some principles and ideas about an architecture that we think would be useful for solving fairly complicated long horizon problems so that was hard complaining in the net so that's that blue no learning box so then let's talk about what happens when you're uncertain about the domain a little bit and we'll just briefly remember that reinforcement learning was designed originally to deal with the problem where you have an agent that knows almost that knows very little about the domain that it's going to operate in and it has to do learning but has to do learning in the actual world and so you know but okay so this is just a cheap shot right but you don't want there about doing re4 something in your kitchen from scratch right you don't want it to learn physics from scratch in your world so if you're doing really online reinforcement learning that's an interesting problem but you have to be very very careful and in particular now every single reward matters there's no grace period where you get to like break things for a while oh maybe little kids get to break things a little bit now too many you hope so you can break things maybe for a little while but not too long but the reward really matters so that's a really interesting regime but I think most practical engineering problems don't really live in that part of the space there are lots of good and interesting attempts to kind of think about what it means to be in between here where you know something about the class of domains but not too much and Bayesian reinforcement learning or meta reinforcement learning or good ways of addressing that what I want to do now is to just articulate a little bit a research direction that we're taking which is roughly to try to extract the architectural and algorithmic ideas from let's say the hierarchical planning in the now system but throw away most the domain dependent models and think about how we could learn those instead so that we wouldn't have to do hand coding for each new domain but in the factory we could do some learning of pieces and parts for a new category of domains that could then be put together compositionally okay so the question is if we adopt this architecture instead of hand building those things in yellow and some other stuff what could we learn so I'm gonna go in the interest of having time for conversation I'm gonna go really quickly through this part I'm happy to answer more questions later or point you to technical papers to read so one thing that we can do is learn new operator models so one thing is if you look at the current robot learning literature lots of people are working on learning policies learning controllers for cutting or stirring or catching a ball in a cup or doing reasonably low dimensional reasonably smooth reasonably short horizon problems and that stuff is working very well and we're not going to work on it because we don't like to work on problems that other people are doing because they'll probably do it better so we're doing something else so something else is to say okay you can learn to cut a vegetable or to stir something how can we figure out models of those skills so that we can put them together to solve new problems so we'll take pouring as an example so imagine that you've learned a controller someone else learned a controller for pouring our problem now is going to be to learn a model of that controller so that we can use it flexibly and reliably so maybe we want to learn we want to say okay this controller it's supposed to cause liquid to be from one beep go from one vessel into another one and I have this pouring skill maybe it has a gain parameter and there's a bunch of preconditions like oh the source cup has liquid in it and I'm holding it maybe in a certain way and the source container has a shape the destination container has a shape there's some relative pose of these things so there's a bunch of continuous parameters that describe my goal the current situation the controller I'm going to use and I want to learn a representation I want to learn a constraint a relation on all those continuous parameters such that if that relation holds if that constraint on all those parameters holds then if I execute the skill with those with that game parameter and the stuff will go where it's supposed to go if I can come up with a representation like this then I can take that pouring operation and put it in the robots repertoire of existing operations and it can just use it as needed it doesn't need to relearn its own kinematics it doesn't need to relearn grasping it doesn't really need to relearn anything else it can just add something to what it already knows so that's our goal we treat this problem the problem of learning that constraint we say okay well we can imagine trying pouring in a bunch of different circumstances we can score each of those pores as to how well it worked or not and then we try to learn a mapping from all those variables that describe the situation of the pouring into how well it works we use Gaussian processes to do this partly because it helps us do information gathering more efficiently and partly because at the end of this learning phase we would like to know the region of the that constraint space where we believe we can reliably execute a poor now you might say actually let's just go back to this thing for a minute you might say why do you want to learn this is going to try to learn all the possible arrangements of pouring that could work out right we're trying to say what what are the conditions under which pouring will work the reason that we really want to know that is that we would like we know that if we just learn one way of pouring maybe you have a favorite way a favorite grasp of your favorite cup you just like have a way of pouring that you really like and you're really good at it and that's good but I feel sure that if I made you pour with your left hand or with an obstacle in the way or from a funny picture you didn't understand very well you would be able to figure out a way to do it so we would like to understand the whole kind of locus of successful pouring operations not just the one so we want to do that and we want to know what part of that space were certain about we do some fancy stuff to keep representation of this GP and to gather information in a way that's focused on finding the boundaries of success we find that the way we do it works better than some other ways including one of those other ways as a way Tomas and I first did it the students did a better than we did that's not surprising so then we trained a real robot to do some stuff here pouring and scooping what's entertaining about this is that um data collection is like non-trivial right so here we have robots trying to figure out how well it works for scooping and pouring various ways and we had to pick up a lot of chickpeas so okay so we learn some operations we learn models of how the operations work we throw them into a general-purpose multi mode emotion planner with our other abilities to do things and now we can ask the robot to solve the winch problems here we could put objects on the table and kind of pretty much any arrangement and we ask it to put stuff in this bowl or another bowl it doesn't matter too much which here we asked it to serve the bowl on the tray with stuff in it there it moved the green box out of the way we didn't tell it to do that but it knew it had to do it so that it could pick up the stuff pick up the cup that had the stuff in it and pour here we said it had to put the cup on top of that block so on I'll show you one more example which I like so it in this next one it realizes that the bowl needs to be in the workspace for the pouring so the planner just synthesized this plan that said oh let me push the bowl into the workspace and then let me pour into right oh did you see where I put the gun yeah so this is like pretty general purpose and that makes us happy what you might have seen there is that I had to kind of hand structure the rule I picked out which properties of which objects were important in making the predictions we've done some other work on trying to automatically figure out which properties of which objects are important we have done some work beginning work on trying to cache right so one thing that you might worry about when you say oh I'm gonna run a planner inside my control loop at multiple levels of hierarchy is that that might be pretty slow and of course what we imagine is that as the robot repeats the same kind of problem over and over again it will begin to cache parts of that policy so that the routine things become more efficient and we've been working on search control right so just by analogy with alpha zero and and those kinds of models although we may be doing a search we could also do learning that helps us search more efficiently and so some work in the context again of multimodal motion planning is we have this kind of difficult search problem which involves picking discrete choices like which operation to do on which object and continuous choices like which particular parameters to use which graphs per where to place an object and so something we've been thinking about lately and it's very to us is generalizing aggressively so we might imagine that the robot learns in this apartment domain here to move one or more boxes into the kitchen area and we would like that what do you generalize to moving objects around inside a cupboard the trajectories will be very different the detailed geometry the scale is different and so on but fundamental idea is that things are in my way need to get moved out of the way that's still true and so we did some work on trying to learn Q values for the abstract action choices under the assumption that the continuous parameters were going to get picked by another mechanism we can still try to answer the question would it be good to move this object now or that object now so that we hope will generalize better than trying to learn over discrete learn over the the detail of continuous values which are very very geometry dependent in a way that's difficult to represent for learning and so again their details views a graph neural network to represent a value function and we need to do some stuff and it works better than some other things okay so what we're hoping eventually the grand design is to like be able to bootstrap a robot from basic understanding that there are objects in the world and and compositionally and continually learn skills and concepts and properties right so we might start out and say I believe that objects have positions in the in the world and I'm gonna pick this property the position of an object I'm gonna see if I can learn some policies that change that property and I can begin to make a model I can use that model to make plans if I make a plan I can execute it if it works that's great if it doesn't work I get information about why my model was wrong if I'm unable to plan to achieve something then maybe I can diagnose something that's missing in my memory and formulate a problem for myself maybe I formulate that I need a new primitive skill like this seems to be stuck to the table maybe I need a new primitive skill which is how to unstick something and maybe I could formulate that as a reinforcement learning problem and apply reinforcement learning methods that we understand so that we hope that we could eventually end up with a loop of defining new properties that seemed important defining new latent properties learning to predict them learning to cause them and build up models of the robots general competence that could be used to solve a wide variety of tasks okay so I'm gonna finish the finishing slide here is I did the cheap trick I gave a talk a long time ago and each guy and I had a conclusion slide and so I just copy that slide but I'll make a few amendments but here's my slide right there's been a lot of progress in honor they surprised me unfortunately that's true it was true then now it's like super turn now so I gave a talk and it sky recently what's this why I did this so now we've really made a ton of progress on this kind of learning back then I said it didn't really give us solutions for making autonomous agents and I think that that's still true I think it still doesn't really the the learning has helped us but it's still not really helping us solve general-purpose autonomous agents then I thought we needed human inside of some sort to complement the strengths of those algorithms back in the day people when people talked about building in knowledge or bias to learning algorithm a particular robot learning algorithm they really thought about building in something like facts now I think that what we need to build in is something more like algorithms like convolution is a great example right that's out convolution is an algorithm and a structural bias that if you build it into your visual learning system it makes your visual learning system work a lot better I think certain kinds of estimation and planning algorithms we will also want to build in and learn the models for so with this I will say thanks to lots of people Tomas my collaborator lots of students and I will let you watch the robot make mistakes and then I am happy to answer questions or move into the panel discussion you okay maybe we don't want to watch their living mistakes it's always kind of fun though this this is this is how you know this is real right like we did not hand Kurt anything or we would not have had this enormous blooper reel to show you okay I'll right now stop Lesley thank you very much it was a fantastic talk very very nice distillation of the the relationship between planning and learning thanks for that let me start with maybe a general question and I wanted to go back to this idea of multimodal abstractions which you mentioned in the context of motion planning but I guess it's general this general idea that there is there exists a set of may be discrete abstractions and I wanted to ask you whether that is where that is an assumption to were making or whether there's truth Ness to that that we live in a world where there is this sort of this small number of abstractions that is manageable or where that is a self-fulfilled prophecy because we like thinking about robotics that way what is what is your take on that yeah good I think it's totally a self-fulfilling prophecy I mean I think we as human engineers need well I know there's multiple pieces okay so one part of it is the self-fulfilling prophesy part we as human engineers can only understand things that are a certain amount complicated because we can only fit certain stuff in our head at wats so if we're gonna build a thing the methodology we use has to be understandable in in pieces and parts no it could be that we're gonna the methodology will be understandable in those terms but the artifact that gets built will not right so if we design if we design an evolutionary algorithm that evolutionary algorithm might have modularity but the it makes my not so I'm not committed to whether the thing in the end that gets made has modularity but I'm certainly committed to the fact that the process that we use has to I don't think that we can get out of that now the thing is though lucky for us humans who live in the world we live in we don't have our act our ability to affect objects in the world is actually fairly localized and and so the fact is that I can't actually just simultaneously moved all the objects in my kitchen I mean if I couldn't physically I probably couldn't cognitively and so it's okay for me if the robot can't do that either now the question about discrete so I don't believe so these abstractions are not discrete necessarily and I don't think that there's a fixed repertoire I mean you know we implemented for to do that thing I think if you learn a new like a new hobby you'll learn a few more right if you like for some reason I've been using soldering and cooking as examples right if you came over and you didn't know how to solder and I needed to teach you to solder you'd have to learn the motor skill you'd have to and so that's like another little package and you'd put that package somehow in your repertoire of packages and stuff thanks cool question on that on the point of representations and so on so in men standing is still up to the engineer to design this representation to select one of these abstractions for computational reasons is that in any hope that we can come up with algorithm saying that you know we will partition the state space in a clever way to make computation feasible oh yeah no so what I'm hoping the only thing I want to build in is that there are abstractions but not the particular ones well I'm and then I might need this I might need a few for bootstrapping it's interesting talking with Josh Tenenbaum and a little bit then with Liz spelke who's an bird on child development and so on there's good reason to think I think apparently that like even mammals are born with the idea that there are objects in the world and even other agents and that they're embedded in 3-space so like I am totally willing to build that stuff in um but I think if I even could just start with the notion that objects have locations in space I could learn motor policies that change those features and then I could learn a model of when those Marta policies work and then I could learn new motor policies that try to achieve the conditions under which those things work so this is related to to kind of the some work that we did with George Connor dyrus on you know George had this nice working on skill training and we were done also thinking about how does that induce higher level abstractions so I I think that we could make a bootstrapping thing that would need some very generic algorithmic ideas and just a few like starting kernels but I don't want to start from like nothing at least I don't I understand I can simplify some people who do but that's I don't it seems yeah I agree with you it seems another kill to you know not to use prior knowledge that we have at the same time we kind of nice that the framework in which these kind of concepts emerge in a very like you know natural way but you know I really feel what your since completing well I think so I I think that's what's important I think that that's really interesting like super cool to figure out how could these things emerge from nothing I just think that then what's critical is that when anybody writes a paper they should say what their objective is see if you say my objective is understanding how these things can come from nothing then I'm like yeah that's awesome if you say my objective is to develop a robot engineering strategy I might say I'm not sure completely here the representation should really come from the task that there ought to be the center so if you guys want again follow up with a question which is which is switching to the topic of computation and it's a little bit of a provocation for Leslie but you know at the beginning of the of the of the presentation you mentioned something that really resonated with with my thinking so you mentioned that there are a number of problems in robotics you're referring to decision-making in which the formulation is deaf right so the formulation is pretty clear is pretty elegant is pretty simple what we're missing is really the way to solve it so that is really a bad computational problem that we have to solve and computing becomes the bottleneck over there so the provocative question for you is will this problem just go away with better computers if you can design chips maybe we can design like a quantum computer in the future can we just go to the beach now and wait for this I'm slightly confused because I actually don't think we have very many of those problems well I guess I guess you you you you hinted the beginning that you know the computation of the part is tough and if you want most of the question about getting the right abstraction is a computational problem okay good so here's the computational problem the way to solve all robot engineering problems is to take this back in your mairead programs from simple to complicated until you find one that meets the spec okay so that's an algorithm that will solve every problem due to your engagement who were no it's not doing due to him he's certainly written a lot about it so that's now okay so now I've reduced all of robotics to a computational problem kind of not all I mean all of the software part of robotics but that is a problem whose complexity scaling is so so so bad that then the answer to your other question is no I don't think even quantum computers can take enough logs to make that outer them practical yeah I guess complexity you know complexity everything helping us so that was more of a hint or support form as a motivation for the young researchers I guess in the audience I think complex is just as much we can do and I think the research actually something even deeper that you know the more we go over the years the more we're going to ask to the robots so you know computation will barely catch up with what we want from our robots right that has been the pattern yeah a little question about obstructions basalt I loved you that ye rubber multi-model motion planning but and we talked and actually mentioned a few times the deal with the rights obstruction but do you have a systematic way to reason more quantitatively about what we mean by the right obstruction what are the different dimensions we should account for and dimension could be quantitative so that maybe in the long run we could come up with what Luthor was mentioning it more algorithmic automated way but computing that abstraction but first of all we should define what the right abstraction theories right now that's good um I mean some actually Michael Lippmann and George counted artists and I think Stephanie telex also they've been working on this a little bit in the context of MDPs so fundamentally I think I had a list on one of my slides okay so what makes an abstraction good it should I mean so so it should ease your computational burden and not mess up your answers too much but fundamentally that's it right because it's gonna it's gonna make your answers worse um I mean unless your domain has some miraculous properties right so you can you can write down an objective function that says you know alpha x computation time plus beta times how much my solutions are suboptimal and then now you have a search problem but it's a terrible search problem again you get enumerate abstractions and see which one is good but then the difficult now it's an algorithmic question of do we have any any leverage on that do we have any way to find a good abstraction right regard whether this is a desperately complicated problem or there are some way to crack it I don't mean what I have just the glimmer of an intuition that we might well I mean I don't think a lot of people are approaching this problem thinking saying I have this big old problem and I would like to make an abstraction of it and I think that seems very difficult to me in particular how did you acquire the big old problem without I mean if it was so big you couldn't acquire it anyway and you don't need the abstraction maybe but so I really the thing that appeals to me is is incrementality and bootstrapping so can we build little bits of policy and strategy that achieve conditions in the world pretty reliably so one thing is I think oh determinism right so if we're looking for abstractions of behavior picking things that we can achieve deterministically I think that's actually a principle that's really important it's like why do I like bags I like bags because I can put something in the bag with probability 1 I can put something in the bag right I can't put it at any particular location but I could put it in the bag and I also know if it's in the bag and I move the bag it comes with the bag that bags are a great abstraction right because it's kisi I don't have to think about it because they're reliable so thinking about things that funnel or that we have good controllers for good good good you know attractors and in dynamics I think those are the basis for making abstractions we like the digital abstraction in computing because because we can you you can kind of keep it true and if you can kind of keep it like if everything's in a bag you're good right so so the answer is I don't really know but I kind of think that you can there's no there might be some bottom-up way of doing it thank you you you're muted Alberto and I think we're gonna transition to some of the questions from the audience run salut yeah we have a couple of questions about explain ability and interpret ability so one question is in in RL in Factory versus planning is in the planning approach more interpretable than RL this could matter a lot in safety-critical applications so what are your thoughts on that I think that might be right I mean just because an engineer had to design that stuff and so if human had to design it then it's gonna be already kind of in terms that are more easy to articulate right I think post hoc explanations of what a policy that's acquired automatically does is is very difficult there are people who work on it but I don't yeah I think I think the planning approach does make it easier to explain mm-hmm we have a question from Lucas manually and that's been I guess voted by a lot of our viewers and building a little bit on their conversation with Professor mone so maybe low level policies like pouring and slicing consume different representation than task and motion planning is there a unifying representation um I think there may not be a unifying representation so so brains are like not uniform bags of goo right so different parts of brains have different connectivity and certainly the areas that are well understood like to the visual areas you know I think it's it's kind of clear that the structure is matched to the problem you can think of the structure as implementing some generic algorithmic something and so I think it's totally plausible that the algorithms for learning and applying control strategies at the low level with a closed loop are just different from the algorithms and structures for doing the higher level stuff I think that's ok I don't think that we need one thing read I think we have another question we have actually many questions but probably the next one is from James yep thanks Leslie for the great talk so this is paraphrasing a question from the chat which was posted anonymously and it's related to your example of how hierarchical planning in the now where your your robot sort of took a chair and move it along so the question is kind of saying how could we formalize the notion that we don't want a robot to disrupt the environment so you know if your robots making tea how can you say go into the kitchen make the tea don't take everything out of the shelves etc and my follow-up question would be how do you see this interacting with curiosity for a process which actively explores the environment yeah okay now that's a really good question so I've been thinking I mean for a long time I thought about that right so exactly so if you have a household robot you say robot maybe sooner to make me a sandwich you don't mean at all cost tonight you don't mean that it's okay to kill the cat or to steal things from the neighbors are like there's a whole range of things that's completely not okay and how do you specify that right it's clear that you can't you can't even transmit your whole objective function to the robot so it feels like this is something that has to happen in the factory so that it's the the metaphor I have I don't have any technical solution to this problem but what I would love is to be able to say that there's some kind of like background utility function which we build in in the factory or learn or build in some combination of some very some background utility and you'd also like to compile it to some degree I think like if you know so we have this high level background utility which is like don't die and don't break stuff but we have that already cashed into some work specific things like we all cringed when the robot put the cap down on the very edge of the table right and that's because we've compiled the like don't break stuff into some other constraints that are maybe more shorter aizen and easier to like to avoid so could you take a very generic utility function and do some work Factory offline maybe slowly to compile it into some constraints that the robot could use and then layer when you say make me a sandwich could it then like layer the make me a sandwich objective on top of that and then solve so like I don't know how to do that but I agree it is a super important question curiosity is another interesting thing that we've been thinking about a bit in our group a lot of but so okay so curiosity curiosity and simulation is totally fine do whatever you want to it's okay curiosity in the real world has to take place with a lot of scaffolding right so even so humans in and bears I don't know I mean so they're curious to some degree little bear cubs right but they also have a ton of reflexes that again got built-in by nature to keep them from doing completely ridiculous and dangerous things and then anyone with small children knows that you have to actually apply a lot of constraints so that the curiosity doesn't become dangerous for wrote and and the question is what is it for right so bigger questions like what is the curiosity for and Laura Schultz is the kind of psychologist who has a bunch of kind of interesting theories about that apparently bunch of the obvious ideas don't seem to be true in humans but I so because it's exploration that you kind of do in your free time I think of it sometimes I like to call it null space exploration it's like you're trying to do something but then you have some free time at the moment for whatever reason and then you do some random stuff not because you're trying to do a thing so it's not like information gathering that's for a purpose it's like well I seem to have some free time let me just figure some stuff out and then maybe it'll be useful but I think nature I mean the some offline process has to decide how to manage that information gathering and what is good and what what are the objectives and how to do it relative to constraints so that you don't endanger yourself so I don't have anything super clear to say about that but that's it's a it's a good question can i maybe ask a quick follow-up just about this notion of background utility so so learning in this background utility and stimulation seems plausible in terms of interacting with physical objects in terms of you want to learn now not to break a mug because then you can't learn it later at a very you know at a thirty-five thousand foot view how do you see this working with interaction with human beings so this seems very hard to simulate so how do you think you could learn a background utility for for robots a human interaction you mean like not hitting him in the head or do you mean understanding what they want in life the former seems easier to learn than the latter but maybe something in the middle of those of those you know III want to learn about a human but a human would probably be annoyed if my robot just just poke them over and over again to gauge what their response would be yeah no that's right well so there's folk psychology right so I mean I think it's at some level there's the just in simulation I can hit people and and they could be mad at me I mean I think you could do a certain amount of that stuff in simulation um but then again it's like you know if you again if you talk to little smokey who understands infant cognition her view is that eight but again mammals are born with idea that there are other agents I mean maybe not a fancy theory of other angels but their own or other actors in the world and certainly humans quite quickly can understand can put themselves in the place of that other actor and that is very powerful because if you can put yourself in the place of the other actor then you can say well would I like it if they hit me in the head or what would I want if I were in that situation and so I think this putting yourself in somebody else's shoes is that it's the way that we manage complicated interactions with other humans and I think we'll have to do that with the robot now what's difficult is of course the humans you but then you have to have some understanding the humans utility and we sometimes it doesn't work out very well but generally we assume that other people have roughly the same utility function that we do and that's okay because we know our utility the robot and a human will have different utility functions and figuring out how that goes it's complicated the robots will have to know something about general human utility I guess now we're back to the question of how can they learn that by watching I don't know that's heart sorry we have a question from Ariel Anders maybe we can move to who's asking and what she would like to hear more about the last comment on building not building in facts but rather building algorithms hi Ari she was one of my students more about that well it I think it's it's just when we try to build in facts about the world's at least for the level things right so I think if we were making higher level robots maybe this would be less true I don't know it's been what I can say is it has been more successful when people build in algorithmic structures and reasoning methods and learn models generally then when they build in the models now if you go to the very very high level like you know how do I know that the mitochondria are the power plant of the cell I know that because I read it in my seventh grade science textbook and that is just a fact that got put in my head in some way so so I think I think maybe that higher the more abstract the knowledge and the more maybe close to textual or something then the more that we could plausibly build it in but that for kind of manipulation things and motor things and even sort of a lot of that common sense a stuff that happens in the physical world that we don't know how to build in at the factual low you so we have a question about causal relationships from Hashanah would we gain from making in cooperation of human biases and insights systematic through the language of causal relationships wait I heard all the words but I didn't quite get that question can you just say it again would we gain from making the incorporation of human inside human biases and insights systematic through the language of causal relationships so he used causal relationships the language of course a relationship would would be gain anything well causality is really really important for the robot right I mean if the robot wants to cause things that's it's like goal in life and so causal reasoning is critical it's easier I mean a lot of the angst in a certain like in the when scientific people or causal discovery worry about finding causal relationships it's hard for them because they have to worry about doing interventions and interventions they're costly and I and there's that whole piece of reasoning but the robot can intervenes like all the every moment the robot is intervening on the world and so it has like all the causal inference data that it could need and that it absolutely has to do causal reasoning so and and I guess so another question is well could you could you gain could you benefit from humans telling you some causal facts or something and again the answer is maybe but I think we're less good and knowing how to benefit from that kind of stuff than we are at benefiting from outer in my stuff hey Leslie there's question about how we should deal with perception and action uncertainties ah well I enjoy uncertainty um I think yeah so I think it's important so perceptual uncertainty right so one thing that's really important is to be aware of your own uncertainty right so of course you're gonna make errors in perception but really being able to quantify your uncertainty is critical right so that one thing the reason that we can make that PR to be reasonably robust doing stuff is that we are very aware at every moment in in that stuff in fact all the planning was done in state space but in belief space it was reasoning about what it knew about the world and how I could take actions to reduce azam that certainty and so on being aware of your own uncertainty I think is really really critical and so the question is I mean so I think there's an important interface to people who work on perception and perception for robotics right which is I don't think we have a good agreement a good understanding of what is a nice interface between perception and stuff that happens later so like getting a bunch of bounding boxes not so helpful maybe like getting multiple interpretations understanding what we would like to know I think this is a coffee cup but maybe it's a something on top of another thing so getting output from perception that gives us more information already about its own uncertainties and then understanding how to incorporate that into some kind of I think we need state estimation this is another thing that lots of people are not I mean I think we have to think hard about this and and not to not enough people are working on that there's a lot of work on state estimation for like navigations but less work on state estimation or just the world around me and what's going on with this pile of stuff in front of me or in my kitchen cupboard and how do I take detection zuv objects or and depth maps and all this stuff and put it together into some kind of distributional interpretation over what's going on so that I can remember it for when I come back later so that I can update it based on new observations so that I can move something out of the way and see what happens so I think that's a whole cluster of things that we could use more focus and uncertainty in action is the same thing right so if you can model the uncertainty the ways in which your actions are unreliable then you can close the loop right you can do perception and then decide whether you so the one thing that I've learned is that you can you that closing the loop solves a ton of problems right and people know this control people know this very well and they can have terrible models and make a little controller but it's usually often it's okay if you can just kind of fix the mistake you made last time then you're okay and more a little bit more makes mistake you made it in a little more people who do high level reasoning and thinking often don't have that ethos right they want to come over the plan and the plan should be good and you should be I executed from being done and that's how that should go so we we take the control ethos up through all the planning we ever do and we know that every plan we make is not going to like live for very long we use the plan as a kind of a justification for the first step and if we're lucky the second step will be good too but probably not and that's okay great I think I'll use the opportunity to squeeze in another question you've kind of alerted these before when you were saying that we should be more proactive when we're writing our papers and having a statement saying okay this is the problem I'm solving and in part because we don't do that there's many discussions that that we have in our community where we're just referring to different problems right there they're almost kind of becoming belief systems which is what I believe in this is this is the problem which would solve and while someone else is talking about different one in there's one that I'm I'm interested and I wanted to get your opinion so there's this big part of the community that sort of thinks or it's motivated by the fact that data is expensive right so capturing data is expensive and that sort of motivates a certain kind of algorithms but then there's these other big part of the community that is motivated by the fact that data is cheap and leads to a completely different set of algorithms where do you see it well how do you how do we resolve how do we have a discussion that can bring both perspectives well that's good right I mean um you know and I think the fact is that different kinds of data come with different kinds of price tags right I mean a nice kind of work that that tries to address that is like sim to real type stuff where they say well okay sim data is cheap and real data is expensive and I want to figure out how to make them work together right so that's nice because they're taking explicitly that trade-off into account but you know I think well so if you say my assumption is the data is expensive you should name three domains where that's true and if you say my assumption is the domain is that data is cheap you should name three domains where that's true and if you have trouble finding them then you should find a different framework okay hey lastly I have a question related to to our previous discussion and this is from Andrew s and the chat and the question is I mean well it's a more of statement I'm interested in your perspective on the development paradigm and developmental approaches to learning so the importance of a childhood phase for a robot in terms of this is I guess related to the notions of curiosity to safety and some of your previous discussion about childhood right well okay so I guess I mean again with my pragmatist so in the spirit of the previous conversation I will wear my pragmatic robot engineering hat not my it's cool to understand evolution hat because I think it's that's cool let's say I just want to make robots right then kind of what I imagine is there's some stuff that engineers can build in without doing too much harm maybe you know convolution and some algorithms and some architecture there's some stuff we can learn in simulation moderately usefully and so that kind of may be some basic developmental stuff we can either learn in simulation or with like 10,000 robots in padded rooms or something right so maybe you can kind of get a if you really need some real experience so you can pay for that right and then maybe you could a grenade all that into like a really pretty competent robot that knows a lot of stuff so there's the development what happened mostly in the factory and then we could take that robe and come to your house and you'd say hey robot actually in my house this is how we do X and the robot doesn't know how to do X but then hopefully it would be like almost as good as you would be if you came to my house and I tried to teach you a new way to do something but the developmental process is something the robot should be doing and so when it's deployed I think is not accessible I think that unfortunately it's time to wrap up I would like to thank you again Leslie for a great talk and I've been engaging conversation next week we're going to have another exciting talk by Allison on camera I would also like to thank the organizer Alberto Luca and Jeanette and of course also the members of the student panel James and as I said last time the seminar series is really an experiment in online our bodies community building so we really value your feedback please use the interface on the robotics today website to send us any comments you might have about format suggested speakers and so on and so forth we really discuss each one of them and finally we're going to plaster the top of Leslie on YouTube please allow us about a week or so for processing time so the talk should be available in a week or so with that I'll close we stop today and I look forward to seeing you next time thank you for inviting me in the great questions 