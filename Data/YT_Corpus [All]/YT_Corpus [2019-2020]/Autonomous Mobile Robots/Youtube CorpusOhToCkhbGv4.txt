 I'm Tomas a poacher welcome to the first talk of the CBM series of this fall semester as you some of you probably knows CBMM mission was to push research in the interface of neuroscience cognitive science and computer science and the goal for the first five six years for the center was really to create a community of researchers in this new area at the intersection of natural science and engineering and I think we have by now achieved you know according to most measure this goal we've done that through a lot of collaborations of talks like this one's a very successful summer school and we have now a community of 25 or so faculty members mainly at MIT but including Harvard and a couple of other institutions and probably more than 200 or so researchers postdocs and students and so on and during this time which means the last six seven years it made sense for us to nurture this community to preserve its identity and now it is the time we decided to essentially broaden up to try to convince people researchers other faculty were at the boundary with computer science and neuroscience of why the synergy between machine learning and and neuroscience and cognitive science why it's important why it's relevant of course it cannot be all the research than in AI but I think it an important part of this research is really the combination of natural science of in religions and the artificial engineering of intelligence so with this in mind and a number of discussions and meetings of the mind we decided to invite a few more members in CBMM Lesley is the first one Daniela Rus is another one and Otto liver and Nick Roy and this also will help merging eventually CBMM with the quest for intelligence given that all the people I mentioned are part of the quest so with this I'm very happy to have Lesley to be our first speaker this semester and Lesley has been at MIT for a number of years she has been the face of the AI lab for many of us and the happy face I must say among other things she did a lot of things I'm not going through but after getting a PhD at Stanford she took their very good decision to move on this side this cost and among other things she's the founder of the journal of machine learning research which has become one of the top if I would say the top journal in machine learning so thank you okay thank you for inviting me and I should say that i really enjoy back talk and stuff like that and questions and so I'm gonna give a kind of an informal talk I have way too much stuff that I could talk about so I'm gonna adapt as I go and I really really welcome questions or complaints or anything and I should think you know I picked this title it's ITAR I you've used it before when I talk to engineering type people and it's fine when I walk into now the room where people actually know something about nature then I feel nervous because I don't really know very much at all about nature so you can help me out with that but so what I want to do personally is make general purpose intelligent robots I want to make robots that are indistinguishable behaviorally from you and I would like to just understand how to do that that so that that's my kind of overall goal and just to help think about that as a problem because that's like much too hard lately I have really enjoyed a proxy which I'll tell you about cuz you might want to think about it too and I actually in some sense assigned it as a homework assignment in a class last semester which is to make tea in any kitchen so I would like to make a robot that could go to your house to go to anybody's house I think approximately everyone in the world puts leaves of some sort into hot water and then drinks it right and so if you went to somebody's house you could figure out how to do this you could look around you could find the hot water you could find the leaves you could figure out how to you know so so what would it take to make a robot that could do that so let's just take that's a simpler goal than all of intelligence but it's actually really quite ambitious and I don't know how to do that either but it it scopes the problem a little bit so one thing I think I know is that it's probably not just a reinforcement learning problem imagine the robot going to your house right you take the robot out of the crate you see robot make me tea and it says okay and then okay so so probably that's not it right so the question is well how do we how do we make this work out and actually I'll just tell a little bit of the story because excellent I mean my PhD thesis was a was about reinforcement learning I'll tell you a little story about how I got there and then why I so um right so I showed up at Fri when I had my undergraduate degree and they were he had this robot and the they wanted it to be navigate through the hallways of SR I and none of us working on that project really knew anything about robots it was embarrassing we didn't know anything about control we really didn't know anything about anything and it was my job to write a program to make the robot go down the hallway using these sonar sensors these terrible sonar sensors so I would write a program in the robot would crash and I would bring it back and I would change the program in the robot would crash and I would I did this like over and over weeks and eventually I had a program that made the robot go down the hall using the sonar sensor so that was good sort of but what had happened in that process was that I learned how to navigate down a hallway using sonar sensors and I decided that I did not want to be in that loop ever again but the robot should learn and not me so that's what really motivated me to think about robot lighting so that's the story it's button so then I did this terrible reinvention of reinforcement learning the only thing that was good about it was that I had pleasure instead of reinforcement but then eventually I figured out about actual reinforcement learning literature and I had a little robot that could do reinforcement learning so that was pretty cool that was my defense and then when was this 1995 oh for those of you who are young ish these are at this is how we used to make talks by writing on pieces of plastic with colored pens so these are scans of some old slides of mine just so you know by 95 I was trying to figure out how to take what I understood from reinforcement learning and actually use it to solve big and interesting problems and I wrote the following thing the romantic ideal of a big pile of circuitry that learns to be an intelligent agent can't be achieved we have to design an agent with lots of structure and many small circumscribed learning problems okay so that I still believe that right so I think into end general purpose goo that you're just going to train up by RL is not going to get to a robot that can make tea in your kitchen so then the question is well what is and I think mail some combination of design and learning and so that's the breeding question is well what kind of a combination okay so here's how I want to think about the problem now just a little bit more formally so I think of myself as a robot Factory I give this taxon where people thought I was talking about factory robots I'm not talking about factory robots I'm talking about a factory that makes your robots and if I'm the robot factory I'm gonna make these robots and they're gonna go to everybody's house and do what they're supposed to do when they get there I have to put a program in the head of the robot some program PI I don't know what it is but I'm gonna put a program in there and this program what is it going to do its job is to map the history of observations and actions that this robot ever sees into its next action that's what it must be it's all it can be and that's just how it goes so methodology I mean there's no Dogma here this is just what has to be with a robot if I deliver it to your house okay so the question is of all the programs I could put in the head of the robot that I'm going to deliver what program should I put there and how can I think about that well the way I want to think about it anyway is the program should perform well in expectation over the houses it's going to have to go working right so I don't know exactly what job this robot is gonna have to do when it gets deployed I have maybe I I you know when you came in ordered robots from me you gave me a spec and you said well here's a distribution over places this room I might have to work so I want to put a program in the head of the robot that's going to work well in an expectation over the places that it could be and so if in fact if that distribution is really narrow right if in fact it's a factory robot then there's hardly any variability in that the distribution I'm taking the expectation over and I could write a very particular program if on the other hand gets all the houses in the world well that's a lot of variability and the question of what program works best an expectation over all those environments is a kind of an interesting and complicated one okay this is what I want to think about the problem so if there's good news or bad news one is that in some sense we don't have to fight about whether it should be Bayesian or reinforcement learning or what should be in there there is an optimal if I you give me the distribution which is hard I understand but if it were specifiable there would be an optimal program to put in the head of the robot no arguing required yes good I was confused by the expectation ah okay good okay good excellent um I'm being lazy and you would have you could put some aggregator you wouldn't want the worst case but you might want some more risk-averse measure than expectation up there good okay so I'm not allowed to burn down your house for instance but do really well in somebody else's yeah okay good so we could say I'm going to optimize the some risk sensitive measure they're good but if we pick a measure there is in some sense an optimal program to put in the head of the robot it's going to work best according to that some some metric like that but so that removes all the religious arguments from AI I believe which I think that would be kind of Awesome but except for it doesn't completely because it might remove the religious arguments about what goes in the program but it does not remove the problem that we now the engineers have which is finding that program right if you gave me a specification you said oh here's a distribution over domains that seroma is supposed to work well in now me the engineer my problem is finding that program that I could put in the robots head that would work well and we don't really know how to do that and that's what we fight about actually I would say much of the time this set up okay does anybody want to ask more question sir yep I think what you're calling the religious arguments because you're not this is not general until you just scope the problem as you said right is that the religious argument that I'm trying to remove is you you might say this robot has to be a reinforcement learning robot or it has to not have something built into it or it does have to have something built into it and the answer is that if you scope the problem then it has to have built into it what it has to have built into it what general intelligence is also a scoping right I mean there's we're not generally intelligent in a kind of very fundamental sense we're good for the distribution for our niche right so these robots just have to be good for their niche whatever that might be yeah okay so how are we gonna do this now I would say well the problem is the factory how is it that we methodological problem how is it that we were gonna find a good program to put the head of the robots and there's a bunch of strategies like you could say oh well the robots gonna learn everything from scratch so that's kind of like the hard core our l a-- version right i put the robot in your house the slides done advanced it ruins the kitchen right okay so that's not a good program to put in the head of the robot it's lazy lazy engineering says let the robot just learn it all the classical view of robotics since like forever has been well you get really smart programmers and they think hard about the problem and they just write the program but we're not that smart I don't think so we engineers are not so smart we're not so good it actually just straight up writing a program we can't write face detectors we can't write all kinds of things we use some other method to find it reverse engineering humans well that would be a strategy and a few guys would like figure out the whole story then maybe we could do that there's some reasons why that might not be the whole answer first of all it's a hard biology problem second of all my spec might be for robots that do some distribution of problems that's not like the distribution that humans do and I would be interested in having a methodology for finding good programs for a different niche and then we could like I don't have tried to just do this is a little flip but search offline it in programmer in the factory for a good program so I think when we're doing machine learning a lot of the machine learning we do and I'll come back to this in a different direction in a minute is it's kind of in this style right that offline we're gonna do some searching for a program that we think is going to work well when we field it so that's that's another strategy I didn't none of these totally I don't totally love any of these but but those I think that's kind of light in the space okay so let's think about how I would like to think a little bit about how reinforcement learning fits into this so one version of the problem of when we need to do learning is when the engineers don't know very much about the domain right so that's the case where the domain distribution is really big and that's a case where the agent might have to do its own learning in the world right and in fact if you go back to the early parts of you know reinforcement learning it was meant to be a story about how age how individual agents learned in their world right the bees and things right that learn in the world by doing trial and error and so that's a set up of of reinforcement learning and that's one where the engineers don't have to work too hard they have to invent the reinforcement learning algorithm but after that they're done argue that at this moment if you go and read papers in noir it's and I clear in all the places where reinforcement learning papers appear almost none of them are actually really operating in this setting right the setting in which they're thinking about the agent actually learning in its actual world and I would argue that they are not measuring the performance of those algorithms in the way that they should if that's what they care about right so if you're trying to design and out of them for an agent in the world it hurts every single time it runs into the wall or does a stupid action you're not interested in the asymptotic optimality of the policy that you learn you're interested in how many times how much it hurts while you're learning so that you want to integrate the reward over time and that really needs to be the measure that you use but almost nobody makes this curve right this is total penalized totally from the beginning cumulative reward but if you're say I'm making madder than for an agent that's behaving in the world I think that's the curve you should use okay so what's the other setting of reinforcement learning the other setting of reinforcement learning I like to call it learning in the factory as opposed to learning in the wild so learning in the factory is when I the engineer I actually know a lot about the domain maybe my domain is rotating at you've been my hand or something like that I know a lot about it I know enough about it to make a really awesome simulator of it in fact but I don't know enough to just write the program so in that case I do something that kind of its kind of like a pun like it doesn't have to be like this but it just turns out but a good way to compile a simulator into a policy is to run an algorithm that looks a lot like the algorithm that we would have run if the agent was actually executing in the world right and most of the reinforcement learning stuff that people are doing our dog you know Falls better in this setting then into the the previous one so different setting different goal and the ideas I think of it as a compiler you wrote the simulator that was a hard work to engineer but the engineer found it easier to make the simulator than to make the policy then this is a crank you can turn to make a simulator into a policy then you put the policy out there and the robot does what it does but if that's the game you're playing I want to argue that you should measure the performance of your algorithm in a really different way it's a compiler it's like a pilot it takes a simulator and makes a policy and all that matters is how long it takes like it's just a computational problem right it's a it's a it's a computational cranky turn what matters is how much computation you need but people don't normally make this plot so what matters how much computation you take and how good the policy is that comes out normally people plot on the x-axis like number of interactions with the world but the world is a simulator and so it doesn't really matter it doesn't hurt it you could do anything you want but that so this is the right way this is the plot everybody should make nobody makes it and this is the plot everybody should make if they're doing reinforcement learning in the factory ok all right annoyed anyone yet okay good all right so so what do I want to do well I want to try to figure out how to really aggregate a whole bunch of these approaches right so I think somehow we should take his weave engineer so I'm an engineer I have this problem I don't think I can do pure reinforcement learning I don't think I can do pure kind of software engineering so what can we do we can take constraints that we understand from the physical world that matter coheres and so on stuff we might have learned from studying humans and other natural creatures we should also be developing kind of engineering techniques that work better strategies things like metal learning and transfer learning and so on and eventually make agents that do go out into the world and then do actually learn from their own experience so the question is how do we all these sources now of insight and technique and knowledge and ideas about how to make a system and use them all as best we can that's what I really want to do okay so what am I really doing what am I really doing lately it's trying to think about a set of computational mechanisms that I feel reasonably comfortable about building into my robot now almost everybody has gotten comfortable about building in convolutional layers in division system right now we're comfortable about that because it I don't know it makes us from a signal processing perspective and we see stuff like that in brains and we say ok I'm gonna build that stuff in I trust it it seems like a good idea I'm gonna learn the filter weights but I'm gonna build in the structure so the question is what 10 other things are there like that that we could build in that we would feel like pretty confident about building in and then we could learn the parameters up so that's what I would really like to do and so what I wanted to tell you about in this talk at first is an exercise that we did for several years of just trying to build actually a robot system by hand in order to try to understand what a second set of good mechanisms would be that we could then use and that we could do learning with respect to later on so what are the kinds of things that I would want to build in while convolution in space and time the idea that I'm a kinematic agent embedded in space that I can reason forward and backward with some kind of models abstract over objects stay at aggregation there's a bunch of things I mean I am not particularly wedded to this list but I think everybody here would maybe have some list sort of like this things you'd be willing to build in and so what we've been doing is hand building building in those algorithmic mechanisms and hand building the models and then eventually what we'll do is talk about learning the models so let me say something about the strategy that we built by hand just to give you an idea of a point in space that works sort of ok actually I don't want to say it's the greatest thing in the world but it's not so bad I use this as my motivating it isn't on my kitchen but imagine that you had to clean that kitchen or make tea in it or something so what makes it hard well supposing it makes it hard is we're gonna have an actual robot which has actual perception and motor control and the motors are not so good and so on so there's just the fact of actual physical interaction there's a lot of objects right so robotics people like to talk about how many degrees of freedom their robot has but then I like to ask well how many degrees of freedom does this kitchen have how many it's not a well-formed question right a degree of freedom is like a kind of a state variable that you can vary but you could think about the positions and orientations of all these objects but there's like I don't know what's the salinity of the grapes and how many lettuce leaves do I have and just there's a queue could get in as much detail as you wanted to to model this kitchen so I don't know how many degrees of freedom it has lonna horizon so if you thought about how many primitive actions you'd have to take to clean the kitchen it would be really a lot and the uncertainty is kind of pervasive and fundamental not only I mean in in the robotics world sometimes you say oh I worry about uncertainty and the answers will just get better sensors but you can get better sensors up to a point but better sensors won't tell me like what's in the oven or when the people are coming home right so there is some uncertainty that you just have to actually act to dispel another uncertainty that really is very difficult to dispel right so maybe you can make predictions about the people but really how are you going to sense when the people are coming out so this collection of issues makes a problem like this really difficult and I can't do this problem but it inspires me okay so let me tell you about it kind of a pretty classical kind of old-school fuddy-duddy thing that we've been doing to try to solve this problem so and I'll just kind of skate through some high-level points about our solution and show you some examples and then I'll talk about learning so the first thing that we take to be pretty important is the idea of actually kind of X listen ting somehow they're robots knowledge state what does it believe about state of the world so you can think of that belief as a probability distribution over possible worlds although there might be certainly other kinds of ways of representing this but if you're ever going to reason that you should do an action in order to find something out like ask somebody a question or look in the cupboard then you have to in some sense know you don't know something so that you can maybe resolve to figure it out so we take this decomposition that's kind of a standard decomposition where you say well I have one module whose job it is to aggregate my observations over time into some representation of what I know about the world and another module whose job it is to take action based on that representation okay and our belief state is a very complicated thing which I'm not going to talk about but there's like kind of a database of hypotheses about what objects might exist in distributions over their properties there's some representation of the space around the robot and our certainty about that like whether it's traversable or not and there's some representation about what objects tend to be near by each other so this is a whole giant research kind of enterprise all by itself there's another kind of set of issues which comes up kind of uniquely in these kinds of robotics problems which is for some reason it's not so hard to solve robot motion problems anymore and it's not so hard to solve discrete AI planning problems but problems that exist it's kind of like the intersection where you have to both move the robot and make choices about which objects to move before which other ones are how to grasp so and where to put them those are tricky so a big chunk of our intellectual effort is focused on these questions of like how do you integrate discrete and continuous planning basically so another kind of high-level point here is what do we do about the fact that we have some fairly fundamental uncertainty about the world so another big piece of my former research life was studying pom DPS partially observed Markov decision process it's a very beautiful formal framework if you formalize a problem as a pom DP you find out that there are exact solution algorithms that are like understand the problem is undecidable or maybe only doubly exponential so basically although it helps you think about the problem it does not help you solve the problem and exactly perfectly solving these kinds of problems is very difficult so what we do is gross gross disgusting approximation and we learned a little thing from control theory which is your controller can be kind of not so good as long as you very quickly reevaluate the effects of the action that you just took and re and and and and redefine but try to just have a tight feedback loop so we operate on this principle of bad models not very good planners but really good feedback control and that makes up for a fair amount of badness in the models and the planners not not all of it but a fair amount so the strategy that we have is to say well we're gonna we're gonna plan in belief space and there's a whole story about that which I'm not going to tell although I could answer questions about it we're gonna make a plan we're gonna say yeah we're gonna we're gonna clean this kitchen and we execute the first step and then you know and it's gonna change the state of the world we're getting an observation and we're gonna incorporate it into our belief and then we're gonna see if we like that or not and then we might might replan and what's interesting is that if you take this view there we go then from the planners perspective so for my perspective my reasoning about what actions I'm gonna take this whole gray thing is the plant so the plant is this is a kind of a control person's description for the environment that I feel like I'm sensing and controlling right so from the robot from the planners perspective everything is in belief space the goal is in belief space I say I want you to believe with high probability that there's a cup of coffee on the table in front of me I mean the robot can't make it can't make things true in the actual world because it can never know what's true in the actual world so it's going to try to drive its own beliefs into a state that I like and so it's a kind of a funny stance to take toward planning but it makes a bunch of things work out fairly nicely so that's how we think about this problem and it's it's it's simpler than kind of exact planning under uncertainty because we're actually not taking into account we are willfully not taking into account all the possible things that could happen we're mostly planning for the most likely outcome now if the most likely outcome doesn't occur then we replan but we're not really hedging our bets in a perfectly optimal way that gets very very expensive so we're not doing it okay so that's that another thing that we do to make the problem sort of tractable is take very serious make make serious use of hierarchy so the story goes like this we start with some high level objective may be the breakfast is made or that I am in San Francisco and make a plan at a fairly high level of abstraction leaving out a lot of details I'm gonna get to the Boston Airport and then I'm gonna get to San Francisco Airport and I'm gonna do something and our planner can compute in with these the G's here you could think of them as sub balls or preimages they're like sets of states such that if I'm in one of those states then I think the rest of my plan will succeed and so what I do is I take that first that first preimage and say well that's a sub goal that's the set of states I would like to get into how can I get into that set of states so I make a plan a little more detailed plan for that like how am I gonna get to Boston Airport well I'm gonna call an uber and do some stuff so then I think archaic calling the uber how am I gonna make that happen and I plan in more detail and eventually I'm getting my phone out of my pocket and I start doing that right I started executing this low-level plan as long as the actions are primitive I execute and then if something surprising happens like it you know I call you a bird there's no uber no cars available then I can rethink but I can actually use this structure not cool this is not my normal clicker and so I don't really know how I'm doing reinforcement learning there we go I won't shine it in your eyes though I know that's not good okay so I might discover that that there's something that I can't execute the rest of this plan right so that the expected outcome of taking some action doesn't happen what's good with this hierarchical structure is that I don't have to actually rethink my career choices or the fact that I wanted to go to San Francisco I can just kind of like pop this plan off the stack maybe and re plan for this so by having a hierarchical structure like this it enables a bunch of interesting reasoning about when to do reconsideration and how much to reconsider when things go wrong so so that's kind of fun and it also obviously makes things much more efficient it does incur some risk right like I am assuming implicitly that I will be able to walk through the San Francisco Airport once I get there and I don't know why I believe that I didn't make a plan in detail and it would be ridiculous to make a plan in detail because I don't know what gate I'm gonna arrive at or which slow people are gonna be in front of me in the hallway but I believe that I can do that and I believe that maybe from previous experience and and some other kind of high level knowledge so there's a really interesting question about how do we acquire the models that let us do these kinds of hierarchical planning for the robot manipulating stuff it's not so hard okay so now I'm going to show you a movie when I show this movie I have to first give some apologies okay what are the apologies the main apology is that any given thing that this robot does five undergraduates could make it do better but it's the same program that's going to do a whole whole bunch of things and so it's pretty general purpose and it can stand a bunch of messing around so that's what I'll say so here's a robot we told it to put the blue box down where the soup can is it found the soup cans and you've got to get that out of the way move the blue box what is it no we it knows about space and objects and picking up here we told it we wanted the green box to be on the corner of the table it has to push it because it's too big to pick up so it figured that out and then it realized I had to move the orange one out of the way I never bothered putting the orange one down it also knows it's pushing is unreliable so it looked to see if it was working here we told it to go out of the lab it knows that it can't move through objects that sees these objects here we thought it was going to put the chair down back there but it just brought it with it so that's like an instance I'd be careful what you wish for here we asked to put a full oil bottle on the table it's gathering information by picking up these bottles to see what's in there and this is some other silly thing okay so um what's interesting about this what's interesting about this so this is okay what's interesting first of all this was mostly coated in Python by me and Tomas so professors can still do stuff but also there is no machine learning in there anywhere not a shred of it right so that was just programming so that was this like semi clever engineers writing a program and it's not sustainable I mean the amount of hassling we had to do in tweaking of the models and so on was substantial so what it is in my view is what it did do is it gives me kind of faith in the bones of the architectural structure of that thing so I think that worked out actually pretty well you may not think that that was an awesome robot but as robots go it's actually kind of pretty awesome if you look at all the rope if all the robot demos in the world so the question is can we keep some of the bones of that thing and learn the models right so I would like to get out of the loop again so how can I get out of the loop again another question is can we learn some of these things so one thing that I think is interesting to think about is if let's say you buy this architecture I'm not this is not I don't not gospel or anything it's just the thing we did it's interesting to think about all the different kinds of things that you could learn if you keep the bones of this but but say I'm gonna learn models so and I also think there are two really interestingly different kinds of learning that go on in a system like this so the first kind which I call green learning about the world this is actual I would say actual information gathering actual acquiring information about the world around you so you know most a lot of learning goes in object detectors and perceptual learning and stuff like that there's right now the majority of learning in robotics is learning control policies how to manipulate a staff and so on there's a ton of that but we might also learn some like transition and observation models at a higher level of abstraction those are important to be able to do long long horizon planning okay so that's kind of learning something about how the world works another kind of learning which is just as important is I would say I don't know there I called it learning to reason or you could call it analytic learning it's learning where no new bits of information enter the robots head really but where you re represent what you already know so that you can reason more efficiently and so I would argue that like the learning that you do when you learn to PI go is blue kind of learning right in some sense once I tell you the rules of go if only you were a better computer you can compute the optimal first move but you're not a very good computer and it helps to have that information represented in a different way and so by some kinds of mental simulation and practicing you can acquire a different representation that's much more computationally efficient and so we also explore a bunch of different kinds of learning that make the reasoning more effective so two really important kinds of learning and so I mean another little hobbyhorse which actually I'm right now typing a proposal about so I don't really have anything to say but it's I think the other thing that's really critical in learning for this for a robot that's you know gonna go out and work in your house or that I'm gonna train in the factory for a while and then it's gonna go work in your house is that the the the kind of learning that we do is kind of is modular and incremental right so that I don't think about training the whole thing end to end but I can train pieces and I can do I can accrete I can learn new things that don't interfere with the things that I knew before so I'm into modularity ok so good so let me just tell you let's see yeah let me tell you just about a piece of work a piece of concrete learning work that we've been doing lately and that will probably be enough and then I'll take questions okay so let's think about learning transition model so imagine that we have a robot that's already competent like it already knows how to pick things up and put it down and you want to teach it to do a new job like pour tea or cut up a cucumber right so it would be ridiculous to imagine that the robot would have to reacquire the ability to pick things up and put them down in order to pick up the teapot right that's ridiculous you notice how to pick up the teapot it just has to figure out how to pick up the teapot and why and when and what to do with it once it has it but it should be able to use what it already knows to do these things so that's what the context we're thinking about robots already pretty confident but we're gonna add a new skill and I'm just gonna okay so let's think about this so we'll just look at it in 2d so imagine we're trying imagine that we've already done some basic reinforcement learning and if we we've acquired a controller for pouring it's maybe it looks at what's coming out and it's got some gains and so on so we have some some control program PI which pours now I'm interested in understanding how do you use it when to call the pouring program what are the preconditions under which if I call the pouring program the stuff will end up where I want it to go because I want to be able to take the fact that I learned to pour is good it's useful but it's not really actually all that useful unless I can integrate it into the rest of the stuff I know how to do and actually use it to achieve some bigger table so I might say okay well I think that there's these various parameters like the relative position of the thing I'm pouring from an into and their shapes and sizes and how I'm holding the object all these things somehow are relevant to the question of whether if I call the pouring program the stuff will go where I want it to so the way we think about this and this is kind of on the engineering side it makes some people just like you know not willing to listen further but whatever we say okay well the way I'm going to think about that is I'm going to say for right now the engineer says I think these are the variables that matter these are the aspects of this situation which is I also want to say that this is a description of the pouring the effects of the pouring operator which is lifted right so the idea of lifting is it's not about this cup and that cup it's about a source and a target in general and it's articulated in terms of their properties not in terms of their identity right so it's nicely abstracted everything individual and what I want to learn is a constraint I want to learn some condition on these variables these you know properties of the situation such that if that constraint holds if those variables stand in that relation to one another and I call my pouring program then most of the stuff will end up in the target vessel so that's my job engineer decided what the features were in this case and my machine learning job is to figure out the relationship on those features such that this thing is going to work out and so I am NOT going to do the details here but we think of this as a gas student okay also experience on a robot is expensive so we really want to be careful about how we do data gathering and so on so we're going to treat this as a Gaussian process regression problem we take two bowls of these inputs which are basically attempts to pour in various situations we score them and we want to learn this mapping let me see something about why I want to learn this mapping I don't want to learn one way of pouring right you might say oh I just want to learn how should I pour but the fact is well first of all some days I'm gonna have a big bottle to pour from and a small thing to pour into and other days I'm gonna have different types of vessels so I have to learn something about this bigger space but also it might be that that away my standard most favorite way of pouring with my right hand and along this axis is occluded for some reason I can't I can't forward this way that I'm serving people and the person in the way and I have to do like the backhand in wine waiter pour you know right so you'd like to know a whole space of ways of doing this job so that when you're planning in a bigger context where there are more constraints you're not like just totally lost when your most favorite way of doing it is no longer feasible so we want only the kind of whole space of good ways of pouring okay so we do this using I'm going to play this like a movie now we do a bunch of GP stuff this the story of this is that Tomas and I did a version of this and it wasn't very good and then we enraged the students and they did a better version so that was good and I like to play these movies so then we gather data on the actual robot for pouring and scooping chickpeas now every single every single time Vee and Kailyn ran around I lost a big ugly do this part they ran around and picked up the chickpeas and the little people and stopped right so experience is expensive okay so so we get experience of these things we tried different bowls and stuff and we can acquire we can in the end acquire a representation of that constraint when is it good for poor and then and there's a this is slightly dissociated because this is the work that's going on right now so in this case the robot has learned something about pushing and something about pouring but no scooping and we've given there a bunch of different goals we can put these objects on the table pretty much any way we want and in each case we've told it to do something we give it different goals put the stuff in this bowl or that one you'll see some crazier arrangements in a minute in this case that tray is supposed to be like I don't know serving tray or something there it just reasoned by geometry that it had to move the green thing out of the way because it couldn't pick the object up the way I wanted to it found a good way of pouring there's another one we told it that now it had to serve the stuff by putting it on top of there picks the thing up and pours it this is the one where it no not too bad well that's like one or two more of these the thing is this is very general purpose we never told it steps to it's just doing means-ends reasoning about how to get the stuff in the cup but he learned constraints oh yeah it wasn't that good there that's a good one right it had to slide the bowl over with one hand so that it was in its workspace conveniently so that it could pour with the other hand it just did that again we never told it that that's the thing it just knew that the in order to get the pouring to work out it needed to have the bowl customer okay so this looks like pretty good it's pretty general kind of happy so okay so there's something that was slightly distasteful about that which is that we had to say what were the important features so another thing we've been working on is trying to learn ways of figuring out what the important features are when you want to solve a problem like this and so what objects and properties are relevant and I'll talk about this briefly because it will make some old people in the room maybe happy but so the way we were thinking about it in this work so the question is let's say I want to push an object or or poor or something like that and I want to learn a description that's generic right it's not about this particular scene in front of me or these particular objects but it's pretty generic and I want to say well if I want to pour from this object into another object in this particular world that I'm in right at this moment how do I figure out which objects in the scene are important for making the prediction of what will happen right and I have to describe those objects in some way that's kind of generic and the way we're going to describe them is actually in terms of how they relate to the objects that we already know we need to operate on so there's this notion of a deictic reference deictic thanks this is pointing in natural language we say this thing or that place those are kind of deictic references because they define things relative to the speaker or to some other objects that we already kind of understand so the way that we apply that in these geometric scenes is to talk about is to and in this work we define some specific relations like above or below or you know on top of nearby and so on and these relations when you with these these things when you apply them to an object they denote another object or set of objects so it's a way of taking one object that you know about like the thing you're trying to push or pour and using it to recruit some other objects in your scene and in this scene it might be one thing in another scene it might be ten things but there it's a way of naming objects relative to the object that you you know about already and so if you do that you couldn't write these kinds of operator descriptions that I might say well if I want to push some object oh one that was that good for some object oh one then I'm interested in the object that's on top of it or maybe the object that it's on top of and maybe the properties of those objects are actually relevant to my ability to make predictions okay so in a scene like this right we might say oh well if I'm thinking about pushing object a I could make a graph of the relations among the objects in my scene and then if I have a list of these deictic references and a graph of the relations I can use those to decide which particular objects in this particular scene I really mean to talk about okay I'm gonna go quickly now because the details here are not so important but it does give us a way of learning rules that name other objects relevant to the things that I'm trying to do and then we learn like a neural network that map's some properties of some objects in the state at time T to some properties of some objects at the next state and the reason that this is an interesting representation and it kind of related to the thing I started with before is that independent of how many objects I have I have this neural network that has the same input and output size because it's being applied to the objects I'm operating on and the ones that got captured by those deictic references so what this means is that we have a strategy for learning both the structure and the parameters of these kinds of lifted sparse prediction rules okay and now I'm gonna play this like a movie for a minute we're almost done right and so we can reapply this in some scenes and it works sort of well and it works sort of better than some other things because everything does or I wouldn't tell you so right so in the world I think there's we're making great strides on solving small problems right lots of really interesting papers about machine learning that address this piece or that piece or another little piece over there and I am really interested in this question of how do we design a whole story so that we can do learning of pieces and parts and so that we can do the learning over time so that we can build in some things that we understand about the world and learn the things that we don't understand so I'm gonna end with this which is uh so I gave a talk and achai in 1997 and this is my conclusion slide this is I had graduated from the pens and and then I recently gave a talk at each guy and I used this as a little bit of a contrast so this is my conclusion right and it's because when I saw this slide I thought man I could use that slide today so there has been major progress and algorithms for supervisors reinforcement learning check right and so now what can I say well really like a ton of progress right okay good but this doesn't directly yield solutions for building autonomous agents I think that that is absolutely true maybe it yields solutions now for building certain kinds of simple things that drive but it certainly doesn't give us you know robots that can make tea in your kitchen human insight is needed to complement the strengths of these algorithms I totally believe that I think now I have a clearer view that at least the approach that I'm taking is that the form of algorithmic and structural biases and so so good so this is work that I've done with a lot of people including even some here and what I'm going to do now is show the robot messing up and thank you for your attention and to answer questions so thanks [Applause] 