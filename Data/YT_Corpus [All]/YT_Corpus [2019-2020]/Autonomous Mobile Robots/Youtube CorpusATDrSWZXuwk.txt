 hello everyone thanks for coming it's 2:05 so we're going to start it's great to have Ken here you know ken always smiling always a cool guy in the room so I'm gonna try to find a way to embarrass him before he starts so it's actually very difficult to introduce Ken right so you go through his CV and it's like I don't know 25 pages of stuff if you go to the the list of our words there's like a counted 54 bullets so I just gave up in trying to summarize or sort of cluster how he has been recognized and I'm just gonna cherry pick on a couple of things from your bio I wanted to mention how you start your by you on how you finish your bio I know if you remember how you finish your bio anyway so the way he starts his by is by saying that so he's an artist inventor and roboticist and in that order and I don't know if that's on purpose or not but if you know Ken he's always it's very clear that he has something of an artist inside in fact he just gave me that book which is co-authored how to train your robot co-author by him and I think your daughter is the first author which is very cool and so if you sort of go through his bio there's clearly many things so he started he did his PhD at Carnegie Mellon and then started as faculty in 91 at USC and then moved to Berkeley in 95 which means that you've almost been for 30 years as faculty if you count and still working on grasping what's going on so yeah so he's faculty at Berkeley of course as all of you know distinguished chair in engineering at UC Berkeley I recently stepped down as the chair of the industry Engineering and operations research department which I'm sure that I was a great owner but I'm sure it also feels like a relief stepping down I would say that if you want to find a word that defines Ken's research it's very difficult but it's people probably choose automation and he co-founded the flagship journal and I Triple E the transactions and automation science and engineering sort of trying to helping turn all that gigantic body of work into some more cohesive understanding of as a scientist scientific perspective and more recently part of the toriel Board of science robotics which is sort of gives us a very different or I knew I would say perspective on what it means to communicate in in robotics which to many of us is sort of like a very new very different perspective but it's an exciting one and most recently as part of I guess the inertia that comes from the success of Dex net chief scientist that ambidextrous robotics company that it's working I'm guessing on grasping I don't know how much information is out there but so we have to find a way I was trying to find a way to how to end introduction and I decided that I was gonna go with how he and his bio and he says in his bio I'm reading that he's madly in love with his wife and his two daughters and I I read that a while back and the first time that I read that it just just got me I said like what a brave way and an honest way to end your bio right like how many times do you hear that and to me it's it was so important because like we often get sort of intertangled in that discussion as to whether you know we should use pixels or we should use forces or our network should be shallow or deep and that discussion is great but we forget quite often that discussion becomes fun only in the backdrop of in the backdrop of community or family if you will right so thank you for that thank you for saying that thank you you're a good man all right I I have to say also love Alberto he's my brother he's my brother and we have the same advisor and thank you for that introduction a super wonderful I am thrilled to be here and I actually I have about two hours worth of slides so I'm gonna go very fast as well Berto just said I I've been I've been really looking at the same problem for 35 years now and I'm happy I'm sorry to say we've made very little progress it's a very hard problem this is a this is a problem of manipulating objects and as you just saw from that it's clip from Iron Man and I what why I like it so much is that it's very realistic right it doesn't mean many of you are familiar with that scene it's dropping things all the time I mean as we know that's the world we live in so what I wanted to is talk about grasping from my perspective which is I want to characterize it in terms of three waves of approaches three three methods and then if there's time we can go into some new work that my students and I are working on so let's start out by going back 2,000 years to ancient Athens when we see a scene with with the great philosophers the artists the thinkers of the time and they're they're convening to discuss ideas they're trying to solve the problems of their era and understand the world mostly it was about the planets and understanding of astronomy and at the center of this of this painting and Carolyn Jones will will correct me if I'm wrong here as an art historian we have Plato and Aristotle now Plato and Aristotle that we can you can actually it's very important to understand how Raphael the painter painted them which is that their hand positions are very important Plato is pointing upward because he is arguing that to understand the world we need to start by looking up at the forms start with the pure mathematics analytics and then deduce theorems that will allow us to understand the workings of the world and his student Aristotle to his left completely disagrees by the way he's not the first time a student disagreed with his advisor now Aristotle says no no if we want to understand the world we start at the looking down word we look at the world around us and we collect data on the rocks and plants and environment and that will allow us to build up empirically an understanding of the world now I think this is this idea of hands is also very relevant to understanding hands and manipulation now humans are extremely good at manipulating some more than others but we are we've been doing this we see them do this effortlessly and it's it's so it's kind of the contrast is very stark between what humans do how we manipulate and how robots are the capabilities of robots so robots just can't they were basically robots or klutzes and they still are after all the advances now wouldn't it be nice if we could actually solve this because if we have robots being able to pick up things then you could come into your kitchen and you can make a big mess and then you go out to work and you come back and it's all cleaned up so who doesn't want that the other thing is how many of you participated in any of the Black Friday Cyber Monday anybody order anything recently okay so this is actually a photo I took in my own house with my the wife that I love and my daughter's this is this is typical okay and you know the problem is that there is a a creasing demand for all this commerce ecommerce we want to order things more and more and we want to get that faster and faster and in fact those sales the figures are increasing dramatically this is actually interesting this is a prediction from 2017 that said it was going to be about five billion dollars a year by now and it's actually more like twenty billion dollars a year so sales EECOM sales have skyrocketed and the challenge is everything is stored in these massive warehouses and I think most of you are familiar with this and the bottleneck is is the Packers the the associates as they're called who are basically tasked with putting the pulling things out of bins putting them into packages and this is incredibly this is very demanding can't find enough people to do these jobs and it's and the demand is just increasing as we speak so people have been looking into some forms of robotic solutions for this and you may have seen this video from our friends over at Boston Dynamics I don't know if any of you worked on this project but it's not the biggest success ok but that's I don't want to pick on them because this is just I'm sure I was trying to say this is the state of the art we're not good at doing this kind of thing and um rod Brooks we may all know said just about a year ago that the ability to grasp arbitrary objects if we could solve that that would have a significant impact in factories warehouses and homes and this is the problem right we want to be able to pick up new things it's not just being able to pick up a widget over and over again but it's being able to pick up anything that you put in front of the robot call that Universal picking now I want to also say I don't believe that the answer is necessarily a more complex grippers and I know that we may disagree on this and I'm happy to have a discussion about it but I I'm still a champion of the good old-fashioned parallel jaw gripper I like it because it's lightweight its inexpensive its reliable and you can do a lot of things with it this is an example now there's a human obviously driving this but but you can do very dexterous manipulation with just a very simple robotic hardware then the other thing that's interesting is the is the its advances in sensing so as many of you know the lidar has really changed and being in widely used in autonomous driving and it's it's sometimes called the secret sauce of autonomous driving but one of the things to know is that this can be used for manipulation as well and this just shows you what a being a part might look like to a depth sensor and but I want to point out that this is a reality of to give you an illustration I'm just looking at some typical objects with a depth sensor and again if you work with these you know this but basically there's there's a lot of noise and any specularity or transparency and the object causes the light beams to react in a way that's not predictable and the results in holes in the objects so this is not the magic solution it's not the Silver Bullet and the other factor is is this has to do with the mechanic the physics of friction so just to illustrate this we put an object down and we pushed it multiple times so we set the same thing up the same way and then we just had the same robot movement and the output as you can see is dramatically different so what's going on it's just that the underlying surface is is-is-is varies slightly and this causes dramatic changes in the final orientation of the object so we can predict the motion of an asteroid a million miles away better than we can predict the motion of an object just sliding across a table now all this comes together and I put this video together to show to illustrate what it's like to be a robot so if you put yourself in the position of a robot you know you're asked to clean up the dinner table put this into the dishwasher or something and this is what your life is like okay your sensors are very noisy and imprecise your actuators are actually very noisy and imprecise and then there's the uncertainty in the physics of the friction and the masses so it's not surprising that your robots are still remarkably clumsy so this is sort of summarizes we have this three sources of error all very very complex and they combine essentially at the end effector to make the task of grasping extremely challenging so I want to talk about this in three components three steps and the first wave of research really goes back to Plato and it's it's the idea that we want to understand grasping we need to start by a stating the physics and go back to the first principles so this really is dominated robotic grasping for a long time and it goes back some 200 years to the McKenna nations who were analyzing how to put together things like clocks and other mechanisms and this really has led to the huge amount of really elegant work that's been done by researchers and robotics and I actually got to revisit this when I co-taught a class with original by Qi and Shankar Sastry recently and we got to relook at all this theory that's that's in this in this text robotic manipulation and you know this is a typical figure that you'll see in there as most of you know if taken the we've taken Russ's class that or Alberto's class that this is a you know this is what you see it's a geometry where the points where the forces of torques and we'll the objects be stable as a result and so this by the way is a way of characterizing that you have the state X and the U is the configuration of the contacts that grab the grasp and then it's either going to be successful or not and you can compute that by computing the forces and torques and then you want to find grasps that it will be successful and then if you look at something like the robotics handbook which I hope all of you have read it's about what 2,000 pages now it has a chapter on grasping and it's a great chapter it covers all this material and then very interestingly on the on the last page of the chapter on grasping there's this sentence when a robot identifies an object to grasp its knowledge of the object imposed and geometry are not perfect might be the greatest understatement of all time okay this is just as amazing this is the first time this point is mentioned that there's uncertainty in these factors and that's the big problem and this is what my student Jeff Jeff Mahler and I started working on several years ago as to how we might characterize that in using an idea about a network which I'll explain you know the one thing that we did was we modified that that approach just in the sense of treating those variables as random variables the the stay and the and the gripper configuration you and we can you can think of this as a with a graphical model of all the sources of uncertainty so you have uncertainty in the pose of the object in the grasp and even the shape of the object and all those goods rise to uncertainty and where the contacts are and the surface normals we have uncertainty in the friction values we have uncertainty in the center of mass which gives rise to again uncertainty in the forces and torques and all of this is going to give you uncertainty in how reliable that grasp is and but we can deal with that by essentially treating that with with a Monte Carlo integration so we're going to sample those those random variables and and then test if each one of those samples those perturbations succeeds or fails and then we weight those by the probability of that sample and sum all those up to get an estimate of what is the the likelihood that a grasp will succeed the quality of that grasp so the intuition and I again I think most of you get this already but if you're trying to pick up a red object like this there's many different ways to grasp it and so what I'm going to do is look for pairs of facets that's every grasp is defined by a pair and then I'm going to check whether or not that succeeds or not and so I'm also going to check for small perturbations around that nominal grasp so I want to check if it's robust because I want it to be robust to the positions of the of the gripper or positions of the objects shape and center of mass etcetera and then I add all these up and this is a result for this object so this grasp right here has fairly low quality or reliability and this grasp is much better so we expect that this old graph this will succeed 90 plus percent of the time whereas this one is very brittle does that make sense intuitively that why that's the case right so because the one on the left will tend to be very sensitive to small errors okay so this is the idea behind what we called X net 1.0 so you take an image then you you estimate the state that's the X part yesterday the state of the object and then you look up using that pre-computation pre pre computed grasp analysis the grasp and then you perform that so here it is working in action and I'm a big fan of sharing your dirty laundry so yeah it doesn't work at all and it's really it really is it's a sucks now part of it is what's going on because ultimately the grass was very good but the the state estimation was very bad so and even a small errand State in this case just a few millimeters caused us to have a very good grasp but not on the object so so it fails miserably so this is the other aspect that this really really to what we're talking about earlier which is we have the perceptual uncertainty that we've ignored there there and that's because we have the the state of the object is not really what we we have access to we have access to an observation some Y of X that is in this case a point cloud that we want to then process that to understand the state and then in turn find the the best grasp but could we go directly from the point cloud to understanding given that point cloud what would be the best grasp of the quality the action that's best for that point cloud so this brings us to the second wave which is Aristotle and remember Aristotle is saying this is not the way to go with a pure analysis but collect data so this is really the the theory behind this massive trend in deep learning and you're all aware of and this has been very successful in a lot of domains and people have applied it to to robot grasping and it's actually there's some very interesting results one I want to single out is the sometimes called the arm farm that that was developed at at Google and here we have about 16 robot arms basically just collecting data grasping at objects over 24 hours a day seven days a week over a year-long period and then they published this result which is the grasp failure rate and you can see it's dropping as they go but what so interesting to look at is that this is a law scale so you're you as you collect data this is 10 million grasp so it's about a year's worth of training and we're still at about 20 percent failure here so for industry you know we want to bring this down and so just try and extrapolate here and it's not clear this is linear in any way but we're looking probably 10 years maybe a hundred years to be able to keep doing this before we have enough data to be able to to do that and by the way that would be just limited to that gripper I have to start over if you change the gripper so there's limitation to that purely data-driven approach so what I want to characterize is though new wave is somewhat based on recognizing where the strength of the of the second wave were and you're familiar with the Yolo this is the success in computer vision which is able to identify objects and scenes and it's remarkable because this is superhuman it's doing this faster than humans can actually do it now it's not perfect there's a lot of false positives and false negatives in this example from James Bond movie but it's pretty impressive and as we know this was a result of the very very large data set of examples labeled example so Fei Fei Lee and her students collected an unprecedentedly large data set of labelled images and that seemed when they trained with that it seemed to have a it was a scaling is that the critical level that seemed to be able to support generalization to new images and the question we have been considering is my group and I or could we do something analogous in grasping in other words can we collect a very large data set of of grasped examples in this case we don't want to know we don't labeling the we're not labeling images or bounding boxes but we're labeling three-dimensional models with reliable grasps and so the analogy again is sort of like with computer vision can we get up to a scale that will support us to be able to generalize to new objects that we haven't seen before so in a sense what we're doing we're saying let's let's start doing this kind of grasping but do it in simulation not do it in with physical objects and so we started out to do that we looked online to see where we could get datasets of 3d objects and there's quite a bit out there because there's a lot of computer gamers posting images of their stuff and most of it is weapons and via and and in an aircraft which are not that representative what you see in a warehouse so we went out and started looking for other other shapes so we found we've been basically mining the internet for all kinds of objects like this and then we clean them up and we make sure they're watertight and then they have they're scaled appropriately and in there in the right format so we can get surface normals from them and then we also augment that data set with with blister packs this is very typical in industry so objects are often packed packaged like this so it's pretty easy to simulate that by just putting a plane on the object and then treating that as a new object and then we also supplemented that set with what we call adversarial objects and these are objects that are this is at this point just empirically difficult to grasp and I'm gonna if if we get to the question so I'll show you I'll talk about some of the objects we've been looking at more recently which are actually systematically designed to be more and more adversarial did we put this together into a graph and and I'll explain the graphical structure but basically it has to do with the the similarity between objects and then we basically compute the robust grasps for each of them and this that that computation is non-trivial we have to actually you know on one approximation you have about a thousand facets per object that means there's a million grass points or grasp configurations if you look at about a thousand perturbations for each to talking about a billion grasp evaluations per object with about 10,000 objects about 10 trillion grasp evaluations but this can be parallelized every one of them can be done independently in something like AWS so it's very feasible we also found some we did some work on speed ups for example using multi armed bandits to take advantage of the similarity between two objects so if we precomputed the robust grasps for one object the ideas that will give us a buy a nice prior for the new object and similarly when we have and when we have a grasp here that that gives us a prior on estimating the probability of a grasp for a nearby grasp and then we put this into a basically compute a similarity measure between two grasps and then we use a a beta distribution to characterize the the probability basically to estimate the probability of success for a new grasp and that allows us to speed up dramatically the computation for new objects so this is just the result of this that if we we don't do any of this we get the best grasp we could find on this object has a quality of about 60 or 60% we have about a thousand objects we get this but when we take the advantage of 10,000 objects we get a much significantly higher quality so we are actually able to to benefit and do this search faster alright so coming back to the perceptual uncertainty problem I mentioned earlier we also can address that by essentially injecting noise into the synthetic depth map images so we have the three-dimensional models as I mentioned but then we can say what would this look like with our our depth sensor and then add appropriate noise onto it and the idea again what we're trying to do is illustrate this this idea that we're going to grasp we're gonna be performing something analogous to what Google did but we're doing it all in simulation so in that book that Alberto just mentioned the robot is dreaming it's dreaming about things that's going to grasp and then it does all that offline and by the way that doesn't take that long it's usually a matter of a few hours or overnight to do that computation now many of you seen this result left from just the summer we're opening I reported results of being able to manipulate a Rubik's Cube and one of the key factors there was the issue of the reality the reality gap right that was that they were able to simulate this very effectively but then how can they actually make this work in in reality and this is the challenge that what the idea there is essentially what they call domain random random which is exactly what we've been doing and this is the idea is that we essentially injecting noise into the sensor images and we are also as mentioned earlier injecting noise into the into the pose and the friction and center-of-mass characteristics so we factor all that in and we come up with a lot of examples of a why is both a depth map image this is cropped just around the object and then a particular candidate pose of the gripper and at a particular height which isn't shown here but that that's the U and then if we want to be able to determine what is the quality of that grasp and we can do that for for in this case we can generate about 6 million examples fairly quickly of both positive grasps successful grasps and less successful grasps negative examples and then we use that to train a deep neural network so here we're trying to train a bigger network and we're trying to Train it based on lots of these examples from of grass points in space so again what we're just looking at is a particular arrangement a geometry of points in space that's the depth map and putting the gripper here what is the probability that we'll achieve a successful grasp so then we do is we train the network and we tune the parameters to minimize the loss function and what we call this the the grasp quality CNN so again that's the input and the output in this case it's just a scalar which is what is the quality of that particular grasp choice and then dext a 2.0 basically given a new light our image a new depth map of a novel object will then generate a number of candidate grasps but very rapidly be able to evaluate them to find the one that's best so it runs a little optimization loop inside of this to basically choose the best grasp and this was also this was done with individual objects but then Jeff had the bright idea to try it with with objects in a heap in heaps and bins so he took the so this is the scenario that it's often found in industry are throwing together in all kinds of configurations and we want to be able to find grasps in this in this bin so he generates synthetic bins by dropping these these combinations of these 10,000 objects into a bin a synthetic bin doing the same thing coming up with a very large training set and then training a larger still larger network to be able to work in this context so this is what it looks like actually if you this is a camera image this is the 3d point cloud this is overlaid with a lot of the candidate grasps that were initially proposed then it goes down through a cross entropy method of reducing the number of grasps and resampling till it finds this grasp which it believes is optimal for that big configuration and here's the system running and I have to confess this really surprised me I did not think this was going to work and it's basic specifically this is around these all these are test objects that it's never seen before so it wasn't trained on these and as you can see they are many of these objects are deformable that was a little piece of fabric so it was it was surprising that it's it's basically just looking for pick points these are arrangements of points in space it doesn't know anything about these objects but it's just looking for opportunities affordances if you will for being able to reach down and grasp the object now it doesn't always work but usually after a series of attempts we were able to consistently clear bins like this so this got picked up in the press we got some nice interest from industry and industry came to us and said but you know what we don't use grippers we use suction cups in a lot of our work and so we want this to work how can you make this work for suction so it was interesting we sat down and we thought actually maybe we can do exactly the same thing but we trained it now with examples of suction grasps so we went into a literature to look at what are the models for suction cups and it's surprisingly there's not that much and I still think there's probably something out there but we just weren't able to find it but we developed our own we we developed a model of the quality of a grasp in this case based on the quality of the seal how the contact points were deformed around a particular seal placement vacuums cups placement and then we also characterized the wrench base the wrench is that a that a vacuum cup can can apply to a grasp so it's very for example that's very poor resistance to torsion twisting around its major axis right that's a suction cups are not very good at that or tipping around those those forces which tend to dislodge a corner and it caused it to air to leak out so but we once we characterized that grasp mechanics then we could we had a quality measure and we would do exactly the same thing we look at a particular choice of a nominal suction cup position we look at perturbations around that and then we can use that to compute the quality of different grasp nominal positions so this is the output showing that the green is very high quality grasps compared with the red and here it isn't a bin again this intuitively works nicely with when you have fairly planar objects but also generalizes to non planar shapes and this is now we have we had two things we had a grasp policy for suction and for grippers and it's interesting because empirically we found that that lot a lot of thing of objects that we just had in our various bins around the lab worked very nicely with suction about roughly 80% and then 20% like this we're not capable of being picked up with suction or anything with soft fabrics and use the parallel gripper so we called that ambidextrous policies that is policies that use both modalities it's going to put those together and and combine them so in this case what we do is we've actually trained two separate networks one for the parallel jaw gripper one for the suction cup and then we put them into a common framework so that we can compare apples to apples and choose which grasp has the highest probability of success given given a particular bin configuration so this just illustrates what that looks like again we're characterizing the gravity vector and just saying that these are the the grasps that of highest quality and for suction versus grasping and then we choose the one that we think there the system thinks has got the most probability of success and exit executes that so here's that system running and this had higher reliability so now we were able to actually clear bins with much more mix a deeper mix of objects and again it's not perfect so here's that failure mode so if you have a anything with with with porosities it's not going to be picked up with a suction cup and in fact the the 3d sensor is very bad at being able to sense that it can't tell if something is porous or not alright so this is we reported this in science robotics as Alberto mentioned and I'm we report by the way I think it's very interesting and this is something we can talk about later is the it's really important to think about the kinds of objects you're you're doing your experiments on and so we characterize that in terms of level one level two objects in terms of difficulty so a lot of the ycb objects for example are what I would call level one they're fairly straightforward at least for suction cups but then we started looking at level two objects and we see a decrease in performance especially when then we start mixing those together we see a decrease and then again dirty laundry with more complex objects this is level three objects we see a further decrease in performance so it pushes us to try and think about well how would we start started improving these algorithms for these more complex objects and then I also want to say is that well there's some objects that are actually even too hard for for any of these and these are what we call pathological so they're not that uncommon but sometimes objects are too heavy for the for the grippers we have or transparency really causes how wreaks havoc something like this is too wide for the for the gripper and too rough of a surface for the suction cup and then just a paper clip is a nightmare okay can't pick I can't suction that and I can't pick it up with my a parallel jaw gripper so it's just I'll never I'll just give up I can never grasp that object of course if I had a magnet okay all right so I characterize now these these three waves of approaches to robot grasping and I also want to say I'm not really going to say that we're close to beating human dexterity I still have a lot of faith in humans this came out last year this kid well you could imagine what he does anyway he's able to solve this you know like a record time of I think like a minute so humans are still good we still got some good years ahead I also want to say coming back to this image of the ancient Athens that if you if you look at this and soom out a little bit what you realize is that Raphael was also had something even more profound to say here what he was doing was characterizing all of the philosophers and thinkers of the time and their hand gestures are all different they're all using different gestures and this I think is a bigger message that to solve this problem which is still eluding us that the answer is not going to be just the Platonic approach or a pure pure empirical Aristotelian approach but it's going to require lots of different approaches and hybrids and combinations of all them all right so these are my students who deserve all the credit for this work I want to thank them and I want to say thank you too I want to say just a quick ad so I'm really interested in talking with you if you're interested in becoming a potential PhD or postdoc with my group thank you [Applause] so I have time for I could talk about some new work but I would happy to take some questions first John beautiful beautiful talk to questions humans don't need ten million examples to pick up objects so what are the prospects for learning from very few examples and the second one is what about mobile manipulation let's take away your fancy robot arm bolted to the table and yank it the robot moves through the world good ok John thank you those are both great questions yeah this is really what I'm getting out with the idea that we haven't solved this problem yet because that's absolutely true humans don't seem to seem to generalize from far fewer examples although there's some interesting and I Alison Gopnik at Berkeley is actually looking at this is actually looking at preschoolers and they seem to actually could generate a fair number of experience of just grasping and manipulating over that first six months of their life the question about mobile manipulators is a great one because we are depending on a fairly precise registration of the the system to the the camera to the to the robot manipulator in this case and so we have tried it with a with the actually with the HSR the the Toyota HSR and it does not perform as well so one of the things is that the error that we were tuning it with training with was was was not at all matched to the error that we saw with the with the HSR so we have a paper on this and we basically retrained it with much larger error margins so then it looks for more conservative grasps essentially and it also tries to take into account the fact that you're not fully on overhead and the resolution isn't quite as high and the gripper control as you said it's not not as good and we were able to get slightly better performance once we were able to retrain to those to those parameters but I don't think that's really going to fully solve that problem I think that that's a that's an interesting challenge because obviously human humans have a fair amount of and you can numb up the joints and you can put all kinds of experiments but people are still very good at grasping so that's why I think there's still this remaining gap that we still have to work on can I have to follow up so um so you you said that by adding more error you sort of condition the system to learn more robust I wanted to get your opinion on a different possible different way to condition the set of grasps which is forcing the system to do something with the object after grasping it that might also be a way to condition for the grasp policies that it learns to be more functional robust if it has to do something have you done anything along those lines or no it's interesting we have one new project I don't have slides for yet but is a it's a task based grasping where we're trying to essentially rapidly characterize that you you have some specific thing you want to do with that object or an area that you also want to stay out away from so stay out zones and then I applied forces and torques so you can indicate those and then it will basically this isn't that pre-computation phase it will find robust grasps that obey those those those conditions so it's a it's a step but here's my my biggest concern is that in the Dex net setting we don't know the identity of the object right we've lost that so we we would just see a set of points in space so we wouldn't know which task-specific grasp to apply in that case now this also relates to another hybrid which is well we sometimes call mechanical search this is where you're in a you're in a very common in a warehouse where you have a bin of objects in front of you but you want to pull out one very specific object now remember the accent can't do that because it just sees all this as points in space and it doesn't know one object from another so here we do need some RGB some color and we probably have to do some what's sometimes called rummaging which is as you well know in Amazon is to move things around to be able to find that object extract it and that once we have those in elements then we could do something like a task directed grasping and one task that's actually very important is is is being able to barcode scan that object so you want to be able to pick it up without putting your suction cup right on top of the barcode so that turns out to be an example of where you do care about your suction location can you speculate on how what percentage of the night not successful grasps are due to just performing open-loop like if you had some sort of low-level controller and visual serving or something how much do you think that would improve your performance mm-hmm good question you're right so that's thank you for pointing that out because you're right the right now this is once the the grasp is computed the robot is purely open-loop so we don't do any feedback after the fact and I should add that we don't do any further learning after the fact which i think is another area that could be exploited yeah it's interesting cuz you know Russ and I were talking about this earlier that may be a low-level feedback loop would be an advantage that is actually some I think that you could use for example the tactile sensors and your gel slim would be very interesting there for example especially to detect when you've when you've missed the object you can detect that very early what we do here in this image in this video is we're actually showing we do detect failures but only by the scale underneath so that's a little you know we would be nice to be able to detect and right as you're acquiring the grass but if it's starting to slip away and then you could correct it right away so I think there's definitely some definitely opportunity there sure so in one of your earlier videos on the ambidextrous grasping it looked like it was kind of doing a rummaging action to try to gather information or separate I'll just make the more distinct is that sort of explicit information seeking behavior encoded yes you have sharp eye so actually I think you might be able to see it in here we actually have three actions so it's a suction grasp and push and you're right I didn't talk about that so pushing being that it's not it hasn't found an opportunity so all the grasps are fairly weak so then it essentially tries to push the object toward the center of the bin and it turns out that actually turns out that strategy is more useful at the end we call the endgame when you have fewer objects in the bin then you oftentimes they get caught in corners and they're unreachable by the way a big thing I didn't talk about here is all the the the kinematic motion planning that's required to essentially move in such a way that you avoid the walls of the of the of these bins and that turns out to be fairly subtle in fact I do have some slides are related to that so we can talk about that but you're right so that pushing primitive is something else that's I think very interesting and information gathering if you will so how do you do that in the most effective way sort of some sort of you know maximal information gathering would be really nice to look at good question right here so when you're computing the quality of a suction cup placement does that computation take into consideration the wrench due to like gravity yes yeah we're using gravity and then we're looking at for example what is the the angle of that which is going to tell us something goal because you come in from an angle like this so that's going to create a shear force so we factor that in so we like to pick it up from a normal direction so it prefers that definitely if you don't mind I'll go on with a few more slides because we only have ten minutes and now I can take some more questions if you can if you have them so this is some newer work and I also wanted to mention that we did a start up I know this is a common thing these days and we're basically trying to in the world of grasping like for industrial applications the challenge is the pics per hour and sometimes they call it rate range and reliability all those combined together into what is your performance in terms of a factory setting so we were getting about 277 pigs per hour in about a year ago and that we we would like to get here right this is this would be really nice humans by the way are about 60 or sorry 600 pics per hour and in in warehouses and so this we think we could get there the key is we have to reduce the sensing time the grass planning time the motion planning time and the hour movement time by the way our arms are very slow this is the ABB Yumi which is designed to be collaborative so it's just designed to be slow but how can we improve these things now as a number of companies that are working on this and many of you are familiar with with right hand which is local and also Berkshire gray is looking at this problem and also XYZ it came partly out of out of MIT so there's a lot of interesting companies that are pursuing this we've been starting to look at faster robots like this for for being able to do it again and invest really fast arm also trying to have better models of the uncertainty when I just described we use that analytic model that Dex net 1.0 so it's not able to take into account some subtle nuances like inertial forces and small errors that arise due to the which Joe makes contact first so we are also able to do a simulation using Nvidia Zizek RN flex tools to be able to say okay let's actually do a numerical simulation for a thousand different configurations and try them all and see what fraction of them succeed so it gives us we think a better estimate of that quality that I mentioned earlier so we're doing that we also found a way to speed up the computation of the the grasp quality CNN by parallelizing it so now we can actually look across the entire bin in parallel computing for different orientations and different positions of the gripper in parallel using on what we call a fully convolutional version of the CNN and and we've been working with Siemens on this because Siemens has a an NP u a neural processing unit it's now commercially available that plugs into their controllers that's able to do this CNN inference very fast I really like this this video they put together that that was showing how this works so it's cool it's like okay this is the this is actually on a end effector although ours is mounted but then it shows you that you would find points on the object I don't know em you don't maybe don't need to go over this right now but I'll show you it in a second this part I like is that it shows you have the neural network is working so these are three could potential cast grasp candidates and then there's a neural network and it feeds each one of the grasps in and then the network computes the quality of that grass spray and then it's going to do it for the next one and basically alright so yeah anyway the the other thing I want to share with you is adversarial grasp objects so we've been looking a little bit at can we systematically design objects that are very hard to grasp so these whiskers here are colorized based on the probability of success so these are this shows you a lot of successful grasps and these are all unsuccessful but you can see that the objects are actually fairly similar so again this is very analogous to what's done in adversarial images can we come up with adversary objects so you've given an object and we tweak it so that it becomes less graspable so we can formalize that basically we want the object to be similar but we want to find the object that reduces the grasp ability under some policy in this case we'll use the the Dex net policy for grasping and so we're able to do that for these different objects and we found a way of essentially an analytic iterative way of tweaking the vertices to reduce the antipodal pairs basically to make these objects less graspable and then we also tried this in a.m. with again formulation where we have objects and then we essentially run them through again to try to train again an adversary a way to find less and less graspable objects so we also have some physical experiments we built a gripper that has point contacts because this is actually what the model is and then we ran this through for tested on physical versions of these objects and they perform pretty much as we predict which in other words that if the object is nice we predicted 200% graspable under a frictional model here and it is but then as we reduce the as we reduce this the friction then the object becomes less and less graspable so this just shows you an example of or we have an original cube it's a little hard to see here on the top and that's graspable very nicely but the we take the same adversarial version of that cube and it slips away and I actually have 3d printed versions of these up here if you want to see them afterward so and you can experiment them with them with a pair of thimbles what you wear so that you approximate the the dexterity of a robot and I have to tell you really quickly so I got a chance to show these to Jeff Bezos and it was at an event where he was he was at the backstage and I I got a few minutes to show this to him and he started you know interestingly he really gets into it right he's like trying he's very competitive so he wants to pick him up with the adversarial object he's damned if he's gonna pick that up with the two thimbles on and he really was trying this for like the longest time in that and there's his bodyguard I guess said you know we got to go and he was like all right oh and let me just yeah you're not gonna be able to do it dude is that doable and he was like okay okay and I said listen you can take them with you and he's like what really I said yeah just take him in and you know you can you keep working on it and he said what about the thimbles and you know yeah these two metal thimbles I said well I got those on Amazon for 10 for a hundred ten for a dollar I said get your own anyway so this thing is the last thing I'll leave you with this is related a little bit to art because as Berto had mentioned it we've been doing a project on irrigation and 20-some years ago I did this project called Nutella garden with my students at a USC at the time we built a robot to connect to the internet which was a brand new thing back then it was 1955 and we we let people come in and tend in water a garden online so it became actually relatively successful and I went on continued for nine years but recently we started thinking about how we can resurrect this in a new context and this is what we call the Alpha garden and so we're building this right now your hub is the new PhD student working on it and so we have this commercial system it's called the farm bot you can actually go ahead and buy this it's not expensive $3,000 but it basically is a gantry that contendere garden but our idea is to actually create a polyculture garden so we want to see if we can have a robot learn how to sustain this garden without human and intervention and the reward function here is the coverage when a coverage the available cover the available space with leaves we want to the the garden to be diverse so the idea of polyculture is you have lots of plants growing in close proximity to each other and so we can characterize that by some sort of entropy about the leaf distribution and then sustainability so we want to also make this minimize the amount of water that we that we use so this is very early stage this is a overhead shot of the of a situation just starting and to train it obviously the time constants are very very bad here so you take you eons to learn so we're we have a simulator and we're running it with this is a very simple first order simulator so how things grow over time again when they compete and then this is a slightly more sophisticated simulator mostly for visualization that shows you different plant types as they're growing so the idea is can we can we actually do a reinforcement learning to to be able to to to learn a policy that will actually be able to keep this garden under control and I think the the jury's out I mean I'm actually kind of rooting against it I don't think we'll be able to do it but it's an interesting challenge and then the ideal would be that people can come in remotely and our latest thought is that half the garden will be human intended and then half of it will be the robot and we'll see who wins so see who you're rooting for but anyway that's a that's a project in the future all right thank you [Applause] hi maybe for one or two more questions before we move to the reception reception anyone but okay great saga thank you for everything I wanted to ask you a quick question I can like if you could comment on something we talked about our grasping us just like where do you put the fingers and like think about that that's the action space can you come in like the potential value of you know thinking about more elements on that like not only about where you put the fingers but how do you reach the grass in which order do you think that they would take the approach the object in which angle do you think there's value in that or you know coming on that definitely no I think you're right I mean another aspect of this this is oh the motion planning oh did I include this one oh I'm gonna put up a plug for my daughter's book here I was saying she has an H index of zero by the way so if you could cite it she'd appreciate it Bernardo no it's a great point the the the aspect of of the actually the motion planning turns out to be very important here so Jeff it's now ski who is new postdoc in my group from North Carolina he he's been actually looking at what we call grasp grasp optimized motion planning and here's the insight that when you when you do the the grasp right now we've we pick we choose that grasp point right there grasp access if you will and then we just come down and grasp it now there's actually a nice degree of freedom that we haven't taken advantage of which is around the grasp axis is is very valuable because if you think about the rest of the arm and avoiding the bin walls that we talked about earlier that you would actually confined a grasp you have a you have this degree of freedom you can exploit to you find a better grasp and then use that to be able to hopefully get faster motion faster or less a more safe and faster motion and the same applies by the way to suction because there's an this degree of freedom right when you have the suction cup yeah you we haven't taken advantage of the fact that you have this degree of freedom that you can you can use to your advantage so he's been looking at that so we have a paper under review trying to optimize for those factors I think you're right though and there's a bunch of things like just thinking about the ordering of objects in the bin sometime we're just doing an essentially greedy way we're just grabbing the first things the things that are highest probability of grass success as we can but it's just not necessarily the optimal alright there may be ways of that you pick things off the top because there's more clearance that may have lower grasp affordance or low grasp quality but they may be easier to grasp so if you think about the bigger picture the global picture about collisions that that may change so again these are I think that's my biggest point is that there's a lot of opportunity for improving so this is just one step in a long path and I think that you know for all the questions that you've been asking you know where can we make improvements and I think it is in trying to understand deeper structure will we be able to generalize from you know I think a nice question would be if we could actually in reach a point where we can actually start picking up and testing by the way very testing this but being able to pick up much broader set of objects by bringing in more examples so continuous learning so what you want to have is when we put this out into practice and you have it an operating in warehouses and you're essentially collecting new data all the time and then using that to refine and continually update the models so that I think is is one idea but and we still might hit a plateau and then the question is what else what do we need to do even better now may be a good time to call it a yeah all right good you Ken great 