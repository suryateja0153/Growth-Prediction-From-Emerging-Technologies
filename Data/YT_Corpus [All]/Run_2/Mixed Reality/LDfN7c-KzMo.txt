 Hello. So, I'm Peter Vale, Program Manager at Microsoft on the Windows Mixed Reality Team, and here with me today is.  Dirk Van Welden, founder of I-Illusions, [inaudible] Space Pirate Trainer. I do mostly the technical side, some gameplay programming, and shader, and optimization is also my cup of tea.  Awesome. We're here to talk about Windows Mixed Reality today, so let's get started. We're going to go through this in four basic sections, starting from very high level stuff just to introduce you to some of the stuff we're talking about, and then getting deeper as we go. Basically, we'll do some introductions into what we're talking about here today, talk about porting your applications for the Windows, also in Microsoft Store, and/or SteamVR. Then, I'm going to some tips and tricks to make the best use out of our platform, and then some of the deeper details about performance optimizations. If you managed to get to the Intel talk that was earlier this afternoon with Dirk, then they went, if you would think of it as like a number five on that list, they went even deeper into some of the tools that you can use to get more information, and really work on those optimizations to get a large range of different PCs that you could use. So, we're going to go through all those things today and try to keep it going quickly. So, we'll hold questions to the end, and just a reminder, if everyone could just make sure they've silenced their cell phones, we're going to be recording this, so that we can put it up later, and make sure we don't have any weird ringtones. Okay, here we go. Getting started. Right. Like I mentioned, we're going to do a quick introduction into just what is Windows Mixed Reality. If you haven't yet had the opportunity to try at headsets and controllers that we have, I recommend that you go check that out at the Microsoft Booth that we have. We have some demos that are there right now, it's pretty cool stuff. Then, we're going to go through some of the examples there, and then we're using Space Pirate Trainer as a case study of some of the techniques and process that we went through to take the application that you already had imported to our platform, so that you can get some real examples of how to do this. First, just at a high level, Windows Mixed Reality of course, the headset is six degrees of freedom. So, full motion, full range that you can move around room scale experiences, seated, standing, whatever you need, but the real interesting key here is the inside out tracking. So, it makes it really easy to set up, you can just put it on wherever you are, go from your office, to your home, to your coffee shop, whatever you need. As well as the motion controllers are tracked from the headset, so they also just, it's just one system in a whole that you can use anywhere, really easy to do. So, I know you guys have used that effectively in your development flow.  Yeah, so at I-Illusions, we do some game jam. Our company is set up that way as everyone works remotely, and every month we come together for three, four days in a place in the Ardennes. So, it's a hassle to bring all the headsets with you, right? So, we're setting up a Vive. It's perfect trekking, but you need for base stations are close knit. One of USB ports, and the cameras. So, actually, we found out that the headset that we're bringing most to remote development, is just Mixed Reality headset because just has one USB, one HDMI, and just works everywhere. So, it's perfect for demos. Also the Flip-Up Visor pretty interesting if you're a programmer, so just flip it up and start working again. So, if you're a VR developer and you're programming game-play, there'll be lots of switching and lots of flipping of your visor.  Great. Yeah. It's pretty convenient. All right. We have eight partners with devices in market today. So, you have a great range of choices available to you as a consumer. Everybody's head is slightly different, so find the one that is most comfortable for you, and you're good to go. All the devices are based on two reference designs. So, you'll either be using a device that has LCD displays or a device that has AMOLED displays. Of course, realize that there's some differences there, so as a developer, you might need to be thinking about what the field of view differences, or the number of pixels for your performance, or even color and things like that that might be different between the two different screens. Before you get started, of course, you want to make sure that you have your development PC setup effectively with all the things you need. Make sure you have updated drivers. There is of course a difference between a consumer PC and a developer PC in terms of what we recommend because you're going to have more than just the Mixed Reality portal running. You'll also be using your Dev tools at same time. If you want to make sure that everything meets spec, then there's an app that you can download the Windows Mixed Reality PC Check app. It does basically what you see on the screen there, brings up a list of all things you need, and gives you some check marks if you pass, and otherwise it'll tell you what is maybe needing to be adjusted. A note; if you are using something like a Notebook that you are going to require some adapters for, if you're using an HDMI adapter, then you'll want to make sure that you have some adaptor that supports 2.0, if you want to get a 90 hertz experience. If you're using an adapter that only has HDMI 1.4, then you'll have a 60 hertz experience. Likewise, Bluetooth 4.0 is giving you the best experience for your motion controllers. All right, Dirk.  So, [inaudible] Space Pirate Trainer, for people who haven't heard of it, it's a game for a VR. So, a few facts about Space Pirate Trainer, so it's actually you're on the platform, you try to aim at droid Sunday attack him, sort of a bullet time effect when bullets fly around you. So, you can physically doge them, so room skill was super important to the controllers and the tracking itself was super important. So, a few facts. So, Space Pirate Trainer was a launched title, back in 2016, four days you see Vive. Lifestyle by the end of that year for touch, and now was launched title for Windows Mixed Reality as well. We've been an early access since Vive launched. We hit 1.0 last year in the end of the year, so we sold more than 100,000 units, of these platforms, and still used worldwide in a lot of arcades because it's just an easy way, just give them the controllers, and they suddenly start playing. So, they don't need to know about the depthness or the depth of the game itself. So, that's Space Pirate Trainer. So, why did we port through a Universal Windows platform? So, I'm going to use UWP. So, actually we went to the Seattle office at Microsoft. I was actually worried about the controllers, that there weren't tracking really well.  Yeah. You were concerned before you even got to try the platform [inaudible]. It has to be just right, so [inaudible] I want to make sure it's going to work.  Feeling. So, they booted up the Cliff House and watched it for 10-second drive like moving around to my back. Didn't experience any problems. So, I'd have fewer concerns, but actually Microsoft solved all of those in the meantime. We had it up and running in three days. So, one day, I prepared in advance because we already did the five two [inaudible] sport, so we had some set up already done in our project to do multi-platform compilation. So, we ended up running in three days, but we were aiming for the same minimum spec as a 970, so that's the same spec as the five binoculars is aiming for. So, there was a huge market because there's a lot of brands pushing this, HP, Asus, Lenovo. So, they're all going to push these brands, and at that time, there was almost no content available, and also the experience themselves weren't available even for mainstream users, it was worse because most of the time you need a beefy PC and a decent video card. Also, if mainstream people or people that buy a headset for the first time, they put them on and they see the first area they come into. It's a cliff house, and they see a box with the note to click on it, that's the Windows Store. So, if you want to be on there, the first initial point of sale is on there. So that's why also we put Space Pirate Trainer on the Microsoft Store.  Great, nice segue into porting is our second section. We're going to talk about just what you mentioned. So, taking your existing application that you might have already done for other platforms and then porting those to our devices in our platform, gives you a couple of options here. You have, if you go and build for the universal Windows platform, that gets you into the Microsoft Store, and you can also get your application into the Steam store. So you've got two avenues to reach your audience. And then also realize of course that you've got a variety of different PC hardware out there that works with our devices. So, what most people would experience as like 90 hertz experience if they're using a PC that is sort of considered let say VR ready, they'll be getting the full frame rate and full resolution. But our device also works with other class of PCs as well, where you get what we call a 60 hertz experience. So, it's a slightly reduced frame rate and resolution, so that we can maintain a stable experience for the user. An example of those are integrated GPU machines. So, it gives you a much wider audience out there that we can support. So the first thing we're going to talk about is updating your app to work for Steam VR for the Steam Store, because most likely you've already got an app in the Steam Store if you're starting from somewhere already in another device. So really, the only things you need to worry about are your controllers. So, we have basic APIs and assets that you can use that gives you the 3D assets as well as the 2D assets, so that you can use the right ones so that it matches the device that you're holding in your hand is going to look correct. Also, realized, of course, there's a few reserve commands that you need to think about. There's one for getting back into the Steam Dashboard, and there's one for getting back into the Windows Mixed Reality home. We recommend that you try to match teleportation model, that users are going to be familiar with from Windows Mixed Reality home, if you can, because that'll be just obvious to them. Then OneNote, I wanted to call out was Haptics. So, today if you're just using the regular Windows that's available to regular consumers who aren't Windows Insiders, Haptics doesn't work yet in Steam VR but it is in preview right now, so that it will be available soon. So if you want to test your application, you can go ahead and do that. Of course, you want to make sure that you enable that logo to show up, so that people know that this platform is supported for your application. How did you do that for Space Pirate Trainer?  So what changes did we make for the Steam VR version? Well, actually, not much, like most of it just works out of box with the emulator. So we did change the UI, to use the mixed reality controllers. We adjust some of the controller input. So, the Vive has a touch pad, the Oculus touch has thumb stick, and actually the thumb stick selection from the way you choose weapons is super easy with the thumb stick. So a lot of users were asking, why isn't this working? So we just added support for that as well. It's just an extra axis or you do direct inputs. That's one of the only things that were actually changed gameplay wise. And then just enabled Windows Mixed Reality support. So let's take a look at this. So the left side is the Vive controller. We had our own artwork, so we just recreated the same one for mixed reality. I would just changed them based on what's headset is attached to. So how do you do it? Like, how do you know if it's running mixed reality or if it's running on Vive. So, this is more technical but if you're using Steam API, you can just check the instance and there's a string called hmd_TrackingSystemName. If it has any form of a string holographic isn't there, you're using a mixed reality headsets. So once you do that, you just put a if statements, or you can set a flag or do specific functions for Windows Mixed Reality, which is for example, swapping out those images. Final thing, enable WMR supports into Steam Store backends. So just go to partner.steamgames.com. If you know, that's under you go to the store backends, there's actually a section called "Virtual Reality". You have to unhide support, like hide Windows Mixed Reality support, make sure it's unselected and there'll be an icon on the store after that. So yeah, that's all we have to do to make it compatible with Steam VR.  Great. All right. So that was how you take your existing application that you may already have in Steam VR, and update it so that the Windows Mixed Reality device and platform are highlighted there properly, and use the right controllers. Now, we're going to talk about, how do you create a UWP application for Windows Mixed Reality from scratch? Derek had a great idea, he said, "I can do this in one minute on stage." Thinking, okay, let's minimize chance for error here on stage, let's do a video of it in advance but he'll still talk through it. So, one thing to note, you're going to be using the mixed reality toolkit as a source of some components that you can pull in, and that makes it really easy. So that mixed reality toolkit has created by our team, as well as a great community that's out there working on this, has got a number of different examples, samples, and tips, tricks, and techniques that you can go up and take a look at now, and just use them in your project however you need to. All right Derek, you got a minute.  So assume you already downloaded that and also we rented the fastest PCs in the world just to make it run in one minute. Not true but, still. So you have a basic projects not XR-enabled. So there's the camera, just changeable settings to Universal Windows Platform. Once you do that, it switches all the assets through Universal Windows platform. Then you need to enable XR, so you just go to the "Player Settings", there's a button called "XR Settings", make sure that Mixed Reality is enabled. So if you're not on UWP, it will not show up. I'm going to start importing the Mixed Reality toolkit. So it's a package that's already downloaded there. Let's just go to the right folder. So, there's a bunch of stuff in that Mixed Reality toolkit. So I have a library there. External, you go to Unity packages and then just select the right one for Unity package. So once that's imported, you'll see a new folder there, here on the upper side there will be a new menu item. So in that menu item, left a bunch of packages. In the menu item itself, there's just configure and add configuration, just select all, apply, and it just adds everything automatically. Just have to change or delete your current camera, and just press "Play" and it should work instantly. So with those imports settings, even the controllers will show up, and it will track automatically. So yes. It should be around one minute.  There you go. Pretty easy. Nice. Great. So, you know how to create a new project really quickly and really easily and make it work with Windows Mixed Reality. Let's talk about actually porting your existing project to UWP and which has a few more steps. A little bit of more work that you probably have to do because you have more detail. First, of course, you want to make sure you have updated your software to everything you need to develop Windows 10 Fall Creators Update, is the right starting point and you'll need to make sure you're using the versions of Unity, Visual Studio that are listed there on the slide, and some applications that you can use to make sure that when you do have everything running, it'll be able to go into the Microsoft Store. We've got the Windows Applications Certification Kit, people call it WACK. You'll want to run that early and often just to make sure that everything is going to work and then the Portability Analyzer can help you to find other issues if they come up as part of your submission process. Then, you're going to want to update your Middleware in your project. So, we worked with a number of different developers back in the fall of 2017 as we were launching this product and help them to get their apps ported over, and plugins and third-party stuff and that was a lot of times where most of the issues would crop up. Since then, there's been a lot of updates to some of these plug-ins that are out there. So, I recommend you go and update whatever you're using in your project to make sure it's the latest, so that it's supports UWP, all you might have to do a few workarounds. AL2CPP, is another option for you and that's a really effective, really great option that Unity recommends, So it makes it easier to get your port and avoid these problems of plugins. So, we recommend it. Of course, you want to target your App to use UWP, which means that if you're using any other platform specific or device specific services like high scores and matchmaking for example, if those aren't going to be supported in the Microsoft store then you'll need alternatives for those. Target your App to run Windows Mixed Reality, like we described in the one-minute video i think that was pretty clear. This device can work in a number of different experiences where anywhere from seated to standing or room scale depending on your Application, you might actually have an experience that goes from one to another. So, you can use some of the APIs to determine where the floor is and that helps you for standing and room scale experiences, see and you don't need to worry about that as much. Likewise boundaries, those are provided by the system if the user has set them and they have those turned on that will help them to know that they're not reaching area that's unsafe, but you can also try to make better use of those boundaries in your application depending on what you have set things are within reach in a reasonable way. Then updating your input system. So, there's a few really great resources online already that you can go and take a look at. I didn't mention this but at the top of each of these slides, is usually a link to some of our documentation. Recommend that you go up there and take a look at those things that's going to have the latest updated information all the time. So, it's tiny right now but after the fact when you have this slide deck available to you in the vault, you can go to look at that later. Those three links are great references for the input system both on our documentation as well as Unity's documentation. Then finally, you're going to want to delve into play from Visual Studio, using Unity and just doing testing with play in the in the editor is great but it doesn't really expose the full pipeline that you have to go through to make sure that this is going to work is UWP. So, definitely make that part of your workflow and then you end up with your application running in the Mixed Reality portal.  Yeah. So, what changes that we make, is my mic on? Okay. So, what changes that we make. So, first of all, since we're using Oculus, we're using five, and Steam works, and steam and all those high school things. We need to make sure that we're removing all of the yellows and libraries that we're using. So, we moved time from steam across. Let's check out the differences for example, on the Steam version we have a bunch of APIs even Oculus is still in there because we're using Oculus natively and UWP version just have to one is for the game put Xbox game but, the other one is just Vertical plugin. So, there are two ways to do this, the first is not the best way but that's the way we did it like we had a dedicated machine we just removed to deal awesome make sure to get the bid ignore it. So, the way you should do it's according to you need user import settings just select i need this and this deal and it will just take out a deal but you don't want in there. So, once you do this you'll probably end up with a lot of warnings on earth. Because there are some other components which just want access to for example the leader boards. So, what do you use their reuse platform dependent completion there's a bunch of documentation out there by Unity. So, make sure to check it they're. I'm going to do a quick recap. So, you just set for example, scripting symbol. We use a more platform, it could also be used Oculus platform, I've used Steam platform. So, those were our three wants and also had an offline version for a version of Spacebar trend up shouldn't go online. So, that's how we did it in the player settings and then if you take a look at the code it's just checking which platform you're on and when we do if statement for example is four inputs, so we were using raw input data so for Steam we got one for Oculus and one for Windows Mixed Reality and then we did something platform-specific. Now, it's just unit is just an input example the most common rule use in there are Steam VR, API Steam work CPI and Oculus API. So there was a bit of work probably the most work for porting to UWP, since you wanted one a big project that was compatible with all those split firms we just need to swap out or just need to change the projects within a few minutes to another version we wanted to keep it in the same scene so we had this kind of setup so we have all Root Nodes one Steam VR, one for Oculus, one for Microsoft Mix Reality and you see they're all alike so the old user camera and the controller and underneath the controller we have a prefab for the weapons. So, based on the model or the type of system the platform that you're using, we're just unable on the right one. So, you can use auxiliary device that model from Youtube documentation to see or to forget what model you are using. So, that's super important and very easy to use and then the last thing, actually, when we're doing the ports, the Unity input system wasn't available yet and most of the time we create our own input systems because there tend to be slightly faster. So, we use native APIs for the inputs. So, first of all, on Unity documentation, there's this whole mapping. So you can use "Access Get" button to get specific buttons from the controllers, or you can use raw input data, and I have some code for that as well. So, first of all, you XOR WSA input and once you get that, you get a input measure and you just read out. It's got some source states and out of that source state, you can, for example, get the handedness of the controller. The second example is, one is a trigger press like select press or just give a thumb stick value. So, that's the fastest way. We use it because we write a room, for example trigger input system and hard triggers. So yeah, that's most of it's for WSA.  Great. So yeah. Just to note of course yeah. Like you said, the unity input model that they have is definitely recommended. It makes it really easy to do in cross-platform really easy. But yeah, if you want to do some interesting things with input, then we definitely have access at that level with the APIs provided. So you can dig in and do whatever you need. So we're gonna get some specific tips. I'll start with generic general tips from some specific tips after that with how to really optimize your application for Windows Mixed Reality. Because as you notice with this headset and these motion controllers, there's a lot of things that are similar to others that are out there, and there's also some unique differences that you'll see with ours compared to others. So, depending on where you're coming from, you might have different adjustments or different tips that you might want to take advantage of. So as an example, if you are porting from Vive or Rift, of course, those are also six DOF or six degrees of freedom. So, that means that you can pretty easily port your experience from theirs to Windows Mixed Reality. But if you're coming from something that's more of a three degrees of freedom system, say Gear VR, even though you might think that, great I've got this application that's designed to run on a mobile device and it's going to be easy to port to different platforms and then have them run on as wide of PC configuration as possible, you have to consider of course that well, if people are used to having fully movable, fully articulated experience that you don't have with yours, there's more work there. So we had some partners that came in trying to port their applications from from Gear, where they realized they had to suddenly have motion controller support to make their experience makes sense to people who are used to having that when they're using the system. Motion controllers. So like you can see, our machine controllers have touch pad and thumb stick. So depending on where you're coming from with others, you've gotten those available to you as well, and like Derek said originally, your concern was, well, what happens if it's out of the field of view? like is it going to be able to do what I need to do from my experience? And in general, yes. We have a lot of great examples and samples that you can go up and take a look at for things like throwing or grabbing something from behind your back. Those are very common experiences and those work great. As well, you might consider that with other systems that have external trackers, if you've got something that you have to, maybe like an ammo clip, that is on the ground, you have to crouch down and pick it up. Well, if the tracker is now blocked by your desk or something else like that in your room, then maybe you can't see it but because our device, generally whatever you're picking up you're actually looking at your hand when you're doing it, and we track controllers by which you can see, then you can do those things. So, it's an interesting trade off between some of these different platforms that you can consider when you're using this. Likewise, is the opposite of that, which is, you know, for other systems it might be easier for them to track the controllers when they're really close to your face, and ours if you put your controller really close to your face for, say, aiming with a bow or something like that, it's not able to track it as accurately. So you have to consider how you're going to do that in your application. Then we also have- the system provides the capabilities to track the controller if it's out of the field of view for a longer period of time and I think Derek seemed to talk about some examples of that first base pair trainer. Behaviors. Of course, like we mentioned the beginning, the slip-up visor makes it really easy for people to get in and out of being immersed versus you know part of the real world. So you want to make sure that you have good pause states or when you lose focus from your desktop to something else, just to make sure that you've got a good trade off there or a hand off there in that case. Then because you can go from seated, to standing, to somewhere else in your room, make sure your experience has good re-centering logic in it because you might need that so that you can have a good play session wherever you happen to be in the room. All right. So now we're going to get into some specifics of Space Pirate Trainer.  Yes. Same thing, motion controller, the stability splitting and recoil. I'll talk about more, the angles of the weapons that were using, I'll talk a lot more like in the next slides. Patrolling, for example, we're using electric Nestle, we never had to change anything behind the back just worked out of the box so we didn't have problems with them. Then holding blasters outside of the field of view while walking. So we were very concerned that people if they're moving really quickly and your holding a shield here out of field of view, and you're looking the opposite side. You're actually stepping in a certain direction that the shoot would just like remain floating. So what actually wasn't happening, so what Microsoft implemented is, when it's out of fields, they lock their controllers to your headsets. So if you're walking around they'll still follow you. So that's a-  You still get the full angle information so that if you're doing anything like that, but if it's out of field, if it'll be long enough, it will lock to where your body is so you can move along with it and then as soon as it gains tracking again it gets it back.  So haptics, we spend a lot of time on haptics for all the different systems and the weird thing is that, every system uses its own way to contact or to communicate with haptics. So, for example, Steam uses steam VR amplitude and frequency acoustic and audio clips and Mark Suv just has a certain amplitude or certain amplitude for a certain amount of time. So you can actually implement the audio thing as well, so you can just read out audio file, just check the volume and then just do the same thing like just set the amplitude of the controller. We ended up creating some fade-outs and it was actually one of the easiest way to implement haptics. So we just had a fade out function that works on any device which also reads frequencies. So you could have different kinds of vibration. Then testing the built, of course, Unity that we're using, super easy to just click play and starts playing, but actually for performance testing, we find out that there's a lot of performance differences between inside of Unity and Visual Studio builds. Also, other different headsets, not all of them, but they have kind of a different gamma and color. So especially to Samsung, because of the amoled, it has a much deeper blacks and the contrast is much higher. So, also anti-aliasing because of a big contrast I think is much higher than on the LCD screen. So that's something to take care of. Motion controller, stability and tricks. So, one of the first things we noticed and that's the case with any other system as well. It's for example a railgun, which you need to- it needs to super accurate, it needs to be super accurate, right? So you charge it up and when you release it, it launches a beam. But just releasing your finger at it some kind of rotation on there. So the problem was that you were always aiming a bit too high when you releasing it, so it didn't feel natural. You don't have the problem if your controller's waiting, like if it's heavy, it's probably because they're so light. So what we did, we ended up with a trick and those are the real values, so whenever you go on the droids, we lock it for 60 milliseconds and after that if it is less than five degrees off from releasing it, it's always a hit. So it felt so much better, so it's kind of a cheat, but doesn't feel [inaudible]. For more than five degrees off, you're still going to miss it. But it was a very cool trick and it works perfectly. So if you're feeling like it's not doing exactly what I wanted. It's not hitting enough like, this is a good way and is a good trick to do this.  Cool.  All these controllers you can see a photograph them all in the same angle, and just to have a look at what angle differences are, and there's not so much difference between the Microsoft one and the Oculus one, but the Vive has a totally different angle. So, how did we deal it with weapons because everyone wanted their own feeling, so actually split out the weapon in two parts like we have the handle, and now we have the barrel itself, and users can just configure it out themselves. So, you have different techniques like H3VR, and they use this kind of hologram, and so you can choose a model based on your holograms. So, there are different techniques for it. This is what we use just to make every user happy and also because of different angles. Everyone was happy with their angle. So, this is a big part, I guess.  Right, one thing to note about the the angles, I think we had a slide earlier that noted, that we have two things, one is gripped pose, and one is pointer pose. I didn't really talk about that in that slide, but I'll mention it now, so grip pose is, so that you can align digital representation of the controller to match how you'd have it in your hand, so that it'll match that grip. Pointer pose, is more for something where if you're trying to select something, the beam that would go out in front of you to do some selection or if you're using something in your hand that type of orientation. It gives you two different options that you can align your assets to what is in the user's hand comfortably, and then you can ingest it for custom things like your blasters depending on what the angle is. So, now Performance Optimizations. We're going to go through a few examples here like I mentioned earlier at the beginning of the talk. If you're looking to get some very specific details about how to use things, like some of the perf-tracking and tools, I recommend taking a look at that Intel talk that happened earlier today. They go into even more detail than we're going to cover here about those things. But, we have a tool kit and other examples that I'll give you some good examples around how to bucket different settings, so that you can adaptively or dynamically choose which set of performance optimizations you might want to turn on or off, based on what your PC spec is that you're running on. Asset optimization, we're not going to talk about that too much here because it's obvious. Obviously, everyone has different unique things in their project that they need to think about. Sometimes it's just the number of things you have. Sometimes it's the number of polygons that you have in a particular thing, whatever it is. That's obviously one of those areas that you need to be thinking about when you're optimizing for something like that, the 60 hertz experience, right? So, that's what most of these optimizations is about, trying to reach the widest possible range of PCs that are out there with a number of different cool tips and techniques that we're going to share with you today. Shader optimization is a huge one, so I'm going to go just some example on that. Then, the last one on the list there's neat, I think that's a cool trick that Dirk is going to share. But, otherwise, everything on there is great. Just general guidance and we'll go into some specifics now with Phase Fire trainer.  So, just like Peter just said, if you really want to get into detail, we did one with the Intel, it uses GPN or other tools to get into specifics of Shaders, or we're going to be pretty high-level on this. So, where do you start searching for performance optimization or what do we do? So, first of all, you can see, if you're using a mainstream or high-level PC. So, by default, it's running on 1440 resolution. I prefer using a mainstream, it's going down to 1280. So, once you know that, so you can check in Visual Studio. Is my resolution lower than 1440 that you know? In Visual Studio, that you're running on a mainstream PC, and in Visual Studio, you can then just do it a little bit lower, like you can get that extra kick out of there, but actually it's only use it as a last resort like don't start off. Any game will run great in 256 by 256. You can run crisis free on all of these little machines. So, use it as a last resort if you still want some headroom for getting to the 60 frames or an I-frames per second. So, first of all, checking out if it's GPU or CPU bounce. Obviously, number of enemy objects. Be careful with Shaders, texture resolutions, that's all varies, GPU-like. CPU, we did lot of recasting for every blood on the scene, we had hundreds of those, so pretty heavy, so we use a kind of a level of depth. So, the further the bullets are, the less they recast, and the longer they recast this. The only negative or the only downside is a book might hit targets three frames in advance, but since it's so far away, no one ever sees it. So, everything that's close to you still doing the same thing, as actually four times faster that way. Also, we search for objects like for example; the audio pitch, searching don't do it, or at least as possible make lists super important. We actually had a problem and we're experienced programmers and we also have that. Of course, the number of draw calls, super important. Be careful with how many things you draw, and make sure you can, for example, use instancing. So, the most obvious of all, low versus high quality environments. So, left one is on the 620, right one is high quality, or almost the same one, but with a higher environments, so you can just see the jam gate, for example, at stations on the platform, you see different lines. Colors are a bit different, so that's one of the most basic optimization that you can make actually, but it helps because it reduces the number of draw calls. Then, we have Half Lambert instead of standard shader, so Microsoft has a bunch of optimizations. You have shaders available under mixed reality toolkit, so they have Half Lambert and that's like four times faster than a standard shader, so use it especially for the things that are taking up all of your pixel. So that's most of time as a floor, right? So, we look at the floor, we changed that, and run so much faster. So, in this image, we used the same post effects, but just changed on the larger platform, and just changed from a Standard to Half Lambert. The only real difference or the lining of the edge, and their reflections. But it runs like so much faster, and it's still like okay, this is something I can live with. That's something that you have to deal with visual artists, so it will always be kind of a give and take. So, we thought it was all right, we run with Intel pic, and also not alone for the floor like the whole environments, everything that's small enough, we change to Half Lambert instead of standard shader. Also, boost effects, I like dropping post effects on there to make it pretty, especially in prototypes, and then they end up being there in the final game, and when we optimized it, make it run, always on a 970 and down for 620, we couldn't do it anymore, so we have to rewrite everything to just use one render call. So, if you look at the differences. So, we use two cameras, one for the high quality, one for the low quality because of all the settings. So, if we look into detail, high quality one, we're using a blend for when you get hit. There's an amplified bloom, R2B splits, fog effects by contrast Gamma. You'll be asking what's optimize about this? Well, actually in those shaders, we'll change them because there is a color lookup table there, so it uses the same goal here, it's amplify. Here's another color lookup table, use it twice just to create some sort of H3VR effect further glow. So, we tried to recolor the glow as well. If you take a look at the low-quality camera, is just one component using all these features, one draw call, the results of this is left one high-quality camera, and total 138 and then the camera image effects, 17, bloom takes like five or something, and then you have the low-quality one, and you see image effects here, one render call. So, and it's run so much faster as well. So, it's a bit technical. I can probably share some of the stuff or share the Shader itself that we used. Feel free to tweet me about it and I'll try to make it open source.  Great.  So, this is difference. Same materials for Twitter, low-quality camera and a high-quality camera, same materials. So, we'll still using statures just to show what the difference is between the post effects in both versions. So, high-quality, mainly, it's the blooming and color correction isn't that awesome as the other one but it's still something that we could live with, so we decided to implement it. Next frame. So, for example, Shaders that you use a lot on the screen, for example, in our case, lasers. They have a glow effect. Glow effects Shader actually and that as such it has so many instructions that once the lasers are like five meters away, you don't see the difference between an unlit Shader and the glow Shader anymore. So, we just swap them out. You could also say, we can just add other game objects into there but just the way we were doing, like keeping lists and stuff, it's what's much easier to just change the Shaders materials dynamically. The last thing, so you have the Skybox of the super interesting because it takes such a big part of your screen and then you have the floor. But you have to also look at dynamic objects. For example our shield, a lot of people were playing it like this and it filled the whole screen. So, if even that shield Shader has optimization process of like 10,20 percent, which is not so big. It was just a tiny object in the background. But if it's a full-screen, it makes a huge performance and bug. So, yet even people playing with two shields and you have two shields next to each other, if you do a 20 percent faster it's just a huge performance increase. So, be careful with things that could be close to your face and try to optimize those as much as possible. We even had a different Standard Shader from Unity, so we removed some of the keywords for example, just for that one extra percent because we're using a Shader so much. Very obvious, but still lighting in VR we use forward lighting to use them in high quality settings. In the 620 version, we used the zero point lights. Actually, we are using light probes, but still it also takes a hit. So, make sure if it doesn't need light probes that you can actually assert it in the Mesh Renderer to not use it. Single-pass rendering is super important for a CPU because for example, on a 620 and a micro surface it goes to terminals and it goes to controlling, either GPU or CPU. You have to make sure that both of them still have some headroom. So, single-pass fixes a lot of stuff for a CPU but it has some problems with other shares. Most of the time just updating them actually makes a big difference and if it doesn't work, there's some tricks on the Unity forms that show you how to rewrite those that exists. Most of the time, it's just one extra keyword. Texture resolution, super simple is probably one of the fastest way to test if that's the problem with your project, you just use half the resolution on your project and for a 620.  Texture resolution is very important.  Yes. Texture resolution is super important. So, make sure you do that especially for the Skybox where Skybox run from one millisecond to 0.2 milliseconds of render time. So, it's a huge difference also such an amount of pixels on your screen. Draw order, super obvious but still Unity does a good job or standard like old engines do a pretty good job doing draw order, but only for static objects. So, if you're using your weapons for example, or if there's like objects moving and your shoulder always be in front of the screen, make sure that they always draw first. So, if you first draw the background and then the environment and then the weapons, it just redraws every pixel. It's just GPU work for nonsense. So, you see that it's actually pretty easy, so you have a bunch of a rather Q. It's actually already a list, so you can select from. Just go into materials, just say, "I want to be in the environment render queue. " but sign a bit faster if you use minus 100 and you'll be sure that the weapons are always drawn first. One super cool trig that like even Eintil said, "What are you doing?" But it's worked so well. We don't use them as N620 because just it's too slow. So, we actually blur our GUIs in front like Justin Photoshop and write one as a one pixel square. Left one is just blurred slightly. If you rotate those in your game, you can see the right one is super earlier than the other one is almost it looks like we're using MSAA right? So, we did the same thing with phones exactly. So, our font texture was just a little bit blurred and so GUI Y just look much better in the end. So, that's pretty weird trick but it works and it doesn't use any performance fail, use it. So, Indians results for this 90 Hertz experience, left one right experience was a version that's now running on the Intel mac or a Microsoft book and actually the first time I settle my QA leads like test is out on your little XPS Dell Notebook with a 620 was like, "No, no way it's ever going to run." Space by Turner let it plugged it in and it run smoothly. So, the only difference is the level of quality. You can see details into resolution but because there's certainly a difference when you put them on, the resolution is slower. The 60 Hertz, 90 Hertz makes you feel that you're not looking at the screen, 60 Hertz you see that there are looking at the screen. So, that's the main difference between 90 and 60 Hertz. But still we could live with that part, that's the worst case situation and then the best case situation is, if you look at this screen shot, we're making sure that everything that's very close to you is using the same shares or almost the same shares, like we're using standard here. The head of the mother ship is also using standard share because it has such a big impact. Those are two things that we noticed but all the rest is actually Lambert in this screenshot. You can see the color correction is a big difference and the glow is difference. But something that we could live with so that people with 620s could also enjoy a Space part trainer. So, yeah I think that's it. It's pretty high level so we can always come and talk to me if you want more details or if you want technical information. So, yeah, thank you so much for being here. 