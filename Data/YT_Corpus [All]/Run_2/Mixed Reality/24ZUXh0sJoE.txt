 - Hello everyone. Welcome to our second tech briefing with Josh. Today's the 12th July, 2018, and Josh is here to present about Mixed Reality update. So as you know, Josh is a coder, author, and and all around client-side wonk, and also, he's an advocate on Mozilla evangelizing for VR now, and most of his work is focusing on Mixed Reality. We are looking for your questions. Please write them in the Etherpad. We share everything on Telegram channel. If you have any questions, you can write to me or Havi, and Josh, whenever you are ready, you can start. - Okay, thank you. Thank you for that introduction. Yeah I'm excited to be here. I'll see if my screen share is working here. Hopefully you can all see my web browser now. Hopefully you should see my presentation here. There we go, okay. I'm gonna start with kind of a summary of what I did last time and then talk about everything that's new, because Mixed Reality and especially Mixed Reality on the web is a vary rapidly changing field. So things just, it's totally different than it was even six months ago. So I'll skip, you know what Mozilla is. I'll talk a little bit about the Emerging Tech Group. So this is kind of a research group at Mozilla and all of the Mixed Reality work we've been doing is part of that Emerging Technology Group. In addition to Mixed Reality, we also have researchers working on speech recognition, deep learning, Project Things, which is a Web of Things toolkit, and of course Developer Relations is part of Emerging Tech as well. My name is Josh. I'm a senior developer advocate. And my job is really to educate outside developers and what I would call proto-developers, meaning people who might not consider themselves developers but they still are creators of things. Even if they're not creating with code, they're still creating. They might be artists. They might just be, they might be hobbyists, they might be students taking a class. People want to learn and build things, so my job is to help educate and I do that through blogs, through videos, like these slide streams. I should mention I just recently started experimenting with Twitch, doing some livestreaming there on Twitch. Twitch is interesting. It's different than doing like a pre-recorded YouTube thing but yeah it's interesting. We'll see how it goes. I of course also create a lot of demos and presentations like some of the stuff you're seeing right now. On the other hand also my job is to bring feedback from outside developers back into the company. So that we need this feedback, so that we can improve our products that we can chart new directions, which means like, are we making the right thing. Maybe there's a totally underserved market that we should be looking at, and it's both from a presentation and a software point of view and education and everything. So our goal is to learn how we can make you successful. So with that in mind, number one thing to remember my email is me@josh.earth. You can send it to my Mozilla email but it's longer and harder to spell, so we'll get that real short one. What is Mixed Reality, which is the focus of my work? We think of Mixed Reality as really a spectrum between the real environment through augmented reality to augmented virtual reality all the way to a pure virtual environment. So what does that actually mean? Augmented reality is when you are overlaying graphics or other information on top of the real world. It should be ideally anchored to the real world. And obviously if you're seeing the real world, then it's not immersive. There are lot of examples and things which are kind of partial AR. Like the recent Google Glass, it wasn't anchored to the world, but it did display information here in your glasses. The middle image here is a headsup display (HUD), which has mostly just a HUD overlaying, but it does have a few things which are based on reality, like the compass directions. And then of course on the right here is Pokemon Go, which is probably the most popular AR application, which originally did not use, it sort of overlayed on the real world because it knew about geolocations and would represent graphics around those locations. Now it is starting to use more of the camera to capture the physical environment. So that's at the AR side of Mixed Reality. The VR side is where you're completely immersive, there's a wide field of view, a very fast refresh rate, fast sensor rate. And there's a whole bunch of headsets which fit this and I'll get into those in a second. Our goal of course is that, eventually AR will be just something built into your sunglasses, which seems really futuristic, but Magic Leap is shipping something this year that will do this. Intel showed off some prototype glasses. And then on the VR side, the idea would be like the holodeck, but we are actually getting a lot closer than you think. There are professional VR studios where people have whole setups where multiple people can interact in the same physical space and the same virtual space at the same time. And this photo is from a year ago, the new HTC Vive Pro that's either coming out or has just come out. We'll make this even better. You can have people wandering in VR through multiple physical rooms all as part of one shared space. So very, very cool. Alright, why is this all happening now? Well VR and AR and MR whatever we wanna call it, we are kind of calling it XR, just to encompass everything. It actually has a very long history. People have been working on this since almost the dawn of computing. The Sword of Damocles project was created in 1968. In the 80s, VPL created the data glove and the data suit here and this thing on his head was actually called and Eye Phone with e-y-e. In the 90s, there were 3D video games you could play, but the graphics were pretty limited. This was like high-end arcade stuff and it was still pretty limited. The reason it's happening now is because of the smartphone revolution. Now you can buy a phone for 500 bucks with dual cameras. Some have depth sensing cameras. We are getting machine learning in the hardware. Mobile, very powerful GPUs and CPUs. So really we are just waiting for the technology to catch up. Now these numbers are from last fall. This has changed. The HTC Vive has gotten cheaper and the Vive Pro is announced and I think shipping. The Oculus Rift has gotten cheaper and they have been teasing their next generation one. You can get a PlayStation VR, Windows partnered with a bunch of manufacturers to create what's called Windows Mixed Reality. Despite the term mixed, Windows Mixed Reality is really VR. It's immersive. It does have cameras on it, but the cameras are for tracking. Now what's interesting is that these things required these big gigantic things called beacons which are gonna be you have to like anchor them up, super high in your room to kind of bathe the room in infrared so that the headset can figure out where it is. Well with the MR sets, they use what's called inside out tracking. So you don't need to have all the stuff in the room. You just put the headset on and through optical algorithms, it can figure out where it is. That also means that the systems are cheaper because they're offloading more to processing but you can do that with a computer. They range from like 200 to 500 dollars. Since I got one of these, I have stopped using the ones that require the beacons because they were just a pain. Now the medium end, there is Google Daydream and Gear VR, which are essentially you put a phone into a special device and it's got a wireless controller and now you have a VR headset, or all the way down to Google Cardboard which is where you stick a phone in that thing, there is no input, just gaze, but it's entry level VR essentially for free if you already have a phone. On the AR side, there is the Hololens, there is still about 4000 dollars, but the Meta 2 has come out. It's about $1500 and the price is gonna keep dropping. Again there is the kind of standalone phone version where if you already have a phone, you can do a lot of augmented reality stuff surely in software. So Apple released the AR kit APIs which are continuing to improve. Android has AR Core and the newest iPhone has a depth-sensing camera. So that's all the stuff that's shipped now. Things keep getting cheaper and cheaper. And the Vive that people made, the HTC Vive have this thing called the Vive Focus which is, runs Android and has a more limited app selection than their desktop one, but it's completely self contained. You don't need a laptop. You don't need a phone. And this thing runs on, I want to say like 400ish dollars. And it's six degrees of freedom tracking. So it can detect when you move this way, when you move forward and back as well as all the topping. For 400 bucks that's really impressive. Then there's the Oculus Go, which just shipped in May and the Oculus Go is only three degrees of tracking, meaning it can detect when you move your head and tilt and the same with the controller, but it can't detect if you move forward or back. However, it's self-contained and it's only $200. It runs essentially  Oculus's own version of Android and it has got it's own app store. You load all of the things to the app store via Wi-Fi. And these devices I think are really game changers for VR because not only are they cheaper, but they remove so many of the barriers of getting in VR. You just sit down, you put it on your head and you are in VR. You don't have to get your phone out of your pocket and set it up and this thing and then launch a special app. You don't have to install a whole bunch of software in your expensive gaming PC. You just put it on your head and you're good to go. It's really wonderful. The price continues to go down and the quality continues to go up. I expect the Oculus Go, probably to have a refresh by Christmas, even if not, this is like this Christmas. This is gonna be the thing that everyone is gonna buy. It's really exciting. Now in terms of potential users, a lot of people say VR is just for games and movies. It's true, a lot of people do use VR especially things like the Oculus Go for games and movies. You can play a lot of fun games, you can watch 2D movies, feel like you're in surround sound with a bit screen TV. You can watch 360 movies, VR movies. It is really good for that, but there is a lot more than just games and movies. There is a complete Mars simulation, where you can simulate what it would be like to land on Mars with current technology. There is an amazing 360 video, it's like a 20 minute movie created of the White House called the People's House. It feels like you are in the White House talking to the President. It feels like you get to go to the Situation Room. People are doing really amazing things. On the AR side for the Olympics, the New York Times created a special Olympics app, which had a lot of AR stuff in it. That worked really amazing, like I could basically drop one of the figure skaters into my house, and in fact you see here the top of the stairs in my house, and you can walk around and look at how they jump and where the motion is and all those amazing quads. SnapChat has lenses, both Front lens and World lenses now, so you can create these AR environments out there. And there is a lot of really cool 360 videos. It's pretty easy to build these days. Here I'm gonna show you a little clip. This is from YouTube. This is some astronauts. Come on, you can do it. Alright, we may have to skip the video. Alright here we go. (video playing) Now it's on 3D because I'm not, wait no, it does do it right. So this is cosmonauts who are about to launch. They're gonna launch satellites by hand. So you can just look around. See everything they're doing. I think this is where he launches the, there we go. He just threw out a satellite by hand. That is just crazy to me. So there's all sorts of amazing stuff. I think we are just tapping the surface of what can be done with VR and AR. You can imagine like exhibits and museums where you would scan the poster and it would anchor some animated 3D graphics around the thing you're looking at anchored by the position of poster. You can get more information about the painters in art museum, not just little bits of text but whole video. I really wanna see a walking tour of Pompeii. I went to Pompeii about 10 years ago. And if you don't know Pompeii, it's an ancient Roman city that was destroyed by volcano and they've slowly been excavating it. And they sold these little books with translucent overlays so you could see here's a photo of what it looks today and then the overlay which showed you what it looked like 2000 years ago. Well we could do that now with AR, so you could actually be walking through the rooms of Pompeii and have real architectural diagrams, animations of people walking around their daily life, really see what it was like in Pompeii. Just so many amazing things. Now the challenge is that hardware is getting better and better, but softwares are a little bit lagging. There's mostly games and 360 videos, you need lot of specialized skills to create this sort of content. We of course suggest you do everything in the web first, because the web is everywhere, and the 3D support on web has gotten pretty good. WebGL it's hardware accelerated in most places, even on mobile. Web VR 1.0 gives you access to the controllers and the headset poses. And we are part of a working group with Google and Apple and Microsoft, to develop common specs for the next generation of this which is called WebXR. And we have even created a prototype experimental browser called the XRViewer, which you can get in the iOS app store, which lets you run these demos. Here is one where I actually was building giant blocks LEGO blocks in my living room. There you go. So actually here's a solar system in my living room. I did this last Christmas obviously. Here is the block saying I just have to keep adding more blocks and create this virtual giant LEGO structure in my living room. So super fun. So you can start experimenting with the stuff today. It is called the WebXR viewer. It's in the iOS app store. Google has a similar one in the Android app store. Their version of WebXR is slightly different than ours We're in the process of reconciling the two APIs. So that's coming along well. We also want to make building content easier. So A-Frame is the goto place for building simple 3D VR scenes. And I'm pretty sure I've talked to you guys about this before. It has components for 3D objects, particles, physics, importing 3D models, really good stuff. Now what's changed since I made this slide deck is I've been working on a few workshops and I discovered this thing called Glitch. So Glitch is really awesome. With Glitch, you can create your own projects. Here's one I created for a recent workshop I did at my local library. So I'm gonna edit the project and you can see that on the side, here's my notes. Here's my code. I have an inline code editor and I can just hit show live. Then it will show my creation and I can go back and immediately do like: "let's change that green thing on my forehead to be blue. Let's see, and then, oh that was the green. Then we'll change that one also. We'll change that one to be red. And now it's immediately updated. So Glitch is really amazing combined with A-Frame, because not only can I do all of this stuff in a web-based system but I can send this link to somebody else. They can view it and then they can hit this little button here and remix it and create our own copy, which means I can now create on my workshop content here as Glitches, and the students can just start from a working system and jump in and start hacking on it and building around stuff. So to help you out I did a blog on my Medium Blog about my experience teaching WebXR to a bunch of eight year olds and they really enjoyed it. I had the kind of simple things down. The syntax probably was the hardest part. They actually understood 3D geometry pretty well. So I started with this existing Glitch project. Another nice thing about Glitch is that you can create your remix without logging in. This is really important because lot of younger kids don't have email addresses, or at least they don't have email that they have unrestricted access to. So creating an account is sometimes not always a possibility. So Glitch works wonderfully for that as well. Syntax was still tricky, so we often did things as a group, where I would be typing on the projector that they would see. I also handed then all a cheatsheet. This cheatsheet is basically the very basics of what you need to be productive in A-Frame. It doesn't include everything, but it's a single sheet. If I did it right, it should print front and back on a single piece of paper and it shows you the syntax. These are some of the common components you'd want to use. Here's how you're gonna load assets, images, and audio and video into your scene. The syntax for position and rotation. After forget the positions, meters and rotations in degrees not radians. How to change the color, how to load GLTF models. Now finding 360 images, you can create them yourself with a 360 camera and I'm working on a workshop about how to do that, but the new 360 cameras are wonderful, but here's also some links for how to find 360 images on Flickr, where to find free sounds. I've also created a collection called, I've got it right here. That is also a demo but why is that, it's funny these links are all going to, good, I just need to change the text of things. There we go. This is a set of assets I created. These are some acro rectangular images which I had scaled down, so they're reasonable for mobile devices. They have nice short names that everything in here will work perfectly and so you can just like click on this link. It's called copy link location. Go back to my workshop. Let's change, which one was that. The one called office. I'll just change that here. Again, load, come on load. Did I get to be here? What's next to each, there we go. That's all it takes to start building 360 experiences. So I created this set of assets. There's some beautiful images that can kind of convey what's possible with 360 imagery. I also prepped a few models. So it's easy to drop these and again we are just like copy the link and then you add it to your project as a GLTF model resource. You can get more resources, more 3D models on Sketchfab, but there is often a conversion process, which is why I added, I hosted all these guys here. What's new with headsets? We went over that. What's new with VR cameras? I'm working on a blog about the 360 cameras which will ship next week. I think that'll be on Hacks Blog. What has Mozilla have been working on? So if you check out the Mixed Reality Blog, blogmozvr.com, we started doing weekly updates, and we are working on some really cool stuff. We first, the two big projects is first Firefox Reality. So Firefox Reality is a version of the Firefox browser designed specifically for a virtual reality environment. So it can show you 2D web pages in 3D scene with stuff around you. It can show videos with a nice black background, but it can also show VR content. So truly 3D. It's still a work in progress. This is a little clip, browsing I guess it's a YouTube video in 3D. We are working on features. So if you're watching video clips, how to switch it into scale it, make it more immersive. It's a 360 video, up and all around you. So we are doing a lot of UI experiments. We wanted to make a browser, that isn't just for viewing the 2D web in VR, but actually takes advantage of VR and let's you do new unique things. So there's lot of great work on that. We do have daily builds I believe, but first public release that we are encouraging people to try it out should be out in a week or two. And we will have an APK available for I think the Oculus Go and a couple of other headsets. I forgot the exact headsets that we are working on. Currently, it does have to side loaded, but we will get that into some app stores when we hit the 1.0. Another thing we've been working on is called Hubs. There we go. Hubs you can think of it as a virtual reality meeting room so you can create a new channel or room, entirely hosted by Mozilla. So you just log in and choose a new room name and then you send the link to the other people and they can join you. It's true VR chat. You can hear each other. You can move around the space and because it's based on the web, there's nothing to install and it works everywhere. If you were on your desktop, if you were on a phone without a headset, then it still works. If you have a full VR getup, then in works total 3D interactive. You can move your hands around. The whole point of the web is that everybody can access it from anywhere with whatever device. You'll not have the same experience but you'll still have a usable experience. So Hubs is that for VR and we launched Hubs 1.0 in April and we are currently working on 2.0 which focused mostly on customization. So letting you customize your avatar, so you build your own rooms, letting you bring maybe like a PDF in it, so you could have a shared presentation where you all talk about your slide deck. There is a ton of really interesting things that people are gonna do with Hubs. So I'm really excited about that. So I think that is all of the actual news news that I have to talk about. Now we have am Etherpad over here. So we can start taking some questions. Let's see. See since syntax is tricky for younger kids, what about having pre-typed entity in component values? In my workshop notes, I'm planning to have a Glitch, where people can remix and type and then have basically notes on the GitHub repo, where you'll be able to copy and paste stuff in. However, I found that with kids who are like eight, even copying and pasting can be tricky because they don't always paste it into the right exact location. So I'm looking at some visual tools that are semantically the same as A-Frame. Maybe like the A-Frame Inspector. But and then host it on Glitch and see if we can make something where they can drag the objects around, move them in 3D space, set the properties through a visual tool. So semantically it would be the same, but for younger kids, I think a visual tool would be better. Is there a way to test Firefox Reality without using ADB and Android Studio? Currently no. Technically you don't have to have Android Studio to have ADB, but you do have to enter developer mode and use ADB to side load the application. Eventually, it will be in app stores just like any other app and signed by the relevant stuff, but not currently. Fabien asked how can I use custom protocol of the iOS XRViewer that you showcased in the recent business card demo? Yes, so I kind of teased this thing. Where did I post that? I need to find. I don't know where I put the original video, so it's easier to just go back through my Twitter. By the way, you should follow me on Twitter. It's all like MR and XR and educational stuff all the time. Did that in July, see that was in late June. So I did that for a conference I was at. There we go. And boom, so what you're seeing here is first I created some business cards with a QR code. I'm scanning it just with a regular camera app. The iOS camera app has QR code scanning built in. It takes you to a special mobile page. But if you click on the demo link, it does two things. First of all it is not using a regular HTTPS protocol. It is calling, I think it was WebXR or something like XRN. It's a custom protocol we created just for testing. So that you can have use your regular browser for regular browsing, but then make a link that will force it into the XR Viewer. The XR Viewer is this experimental browser that we have for iOS. That's with loading deck. Once that's done, it's loading a particular page that is looking for a specific image. Originally I was actually gonna look for the image of the duck on my business card. But it turns out the way this image detection works, it needed something like a photograph with more feature points. The deck was too simple. So what it is doing is looking for this particular image that I told it, I gave it a copy of the classical marble image, and then when it sees it, it just calls me back saying hey I found that in current 3D space. And then I drop a model in there. And this was just a model of the earth that I happened to already have. So how does this work? Well you wanna go to WebXR, I think for this I need to polyfill. It's the, here we go. If we go to, no that's the viewer app. Where is the XR Viewer? There's all our examples. WebXR examples mozilla We want this one. Yeah, okay. So if you take a look at image detection here, this is one of our standard demos. This is in the WebXR polyfill reboot. The code is a little tricky to follow, so I'm gonna be working on a new version, but my colleague Blair created this demo. So we've added some new WebXR APIs for image detection. It's called an image anchor. And it's platform neutral but it's backed by the native platform. So since this is running on iOS, if I call it create image anchor, it's underneath gonna be calling some ARkit API. So you give it the image name and the data width and height. Basically this is how big it should be in meters. That's one is 20 centimeters. So you describe the image that it should look for by loading up the image and getting the data. Then it creates this object called, I don't know where that goes. It's gonna call back with, alright I really need to document this 'cause somewhere it's creating this image detection object. So you get the image data, create the image anchor and then down here we call activate image detection. Oh I see so we're registering it under a name. And then we tell it to activate image detection for that name, and then when it finds something it's gonna call you back with a transform. And using that transform with some annoying math, you can now basically create an anchor in space. And then you can start attaching things to it. So it's still way too tricky to use. So I'm working on an abstraction which will make this a little bit easier, but essentially I took this code and hacked it up for my specific demo. Let's send it back here. Let's see. What are you most excited about in the WebXR specifications? I'm most excited about things like image detection because it's the link between the virtual world and the real world. Currently we can detect the world around you in terms of like in terms of plains like flat surfaces, but which is great, if I just want like here's the thing and I want to put it on a table. But suppose I go to the museum, how does my phone know which exhibit I'm at in the museum? It could use GPS but GPS is not accurate enough for the things I want especially indoors. It can tell you you're at the museum probably maybe which wing, but it's not going to be able to tell like I'm looking at this exhibit versus this exhibit. So with the image detection, we can now further narrow that down and say you're exactly here. And your orientation, so I can then add graphics to enhance something to roll out that actually lines up with something in the virtual world. We can really bring them together and that creates so many interesting amazing things. And it's not QR codes. It's just images. So it will look pretty. You know I imagine lot of these will have like a QR code in the bottom corner to get to the web page, but then just with this page on the museum's website I can now have this really cool experience. That's what I'm most excited about. What is the process to extend the Mozilla's Hubs scene to make your own presentation in it? That we are actively working on and you're gonna hear from us sometime in the next month with the new release how to actually do this. Who is in charge on A-Frame now? Nobody in our organization, specifically it's own governance, how do you see future of this successful framework? A-Frame, even though we supported it and put resources behind it, it's always been community driven and that is still the case. We looked at it and decided it's really it's really the best thing in its space. It certainly has flaws especially as you get to building more complicated things. But for the space they're in, it works really well. For a certain class of applications, experiences and it's wonderful for learners and there's a rich community. So our goal is to continue growing the community and to invest in the underpinnings. A-Frame is built on top of three.js. So we've hired, we've gotten some more resources to improve three.js, get it running on WebGL 2, get it running with new WebXR APIs, work on performance, improvements, fix bugs, support new headsets as they come out. So by improving three.js, we are improving A-Frame. A-Frame itself I think is really going great. And I don't think we're gonna see many new features beyond these support for new headsets and stuff. I think all of the really cool new stuff is gonna come from the community. So our goal is to continue supporting that community and encourage people to build more components. What is my favorite VR to AR vice versa experience? Right now, my favorite VR experience is probably the People's House that 20 minute video of touring the White House 'cause it shows the potential of 360 video. There are certainly lot of fun games but most of the games could still be done on a 2D screen. Whereas the 360 view of the White House is totally new. For AR, I'm really excited about these education opportunities where you can walk around a space and have it enhanced with further information. There is one actually that I do want to build myself. We have a physical model of solar system here in the town where I live. That's owned by our science museum, and it's a one to one billion scale. So like there is a sun and like a few meters away is Mercury and 90 meters away is the Earth need if you wanna go all the way up to Neptune, it's like four miles away. It is two scale, which is really cool. It gives you that sense of just how big solar system is and how far away it is. So what I wanna do is build on augmented reality experience on top of that, so as you go to each planet station, you could not only see the physical model of the planet but maybe see zoomed in version with details or see its moons actually orbiting around it or popup text that tells you more information. So that is something that I'm gonna try to build this weekend. TensorFlow. TensorFlow is rapidly becoming the defacto reference framework for image detection and more in the web and Mozilla will step in. Especially since the existing Common Voice machine learning research effort. Yes, so I don't know if we are gonna contribute to TensorFlow itself, but we are using it to build other things. And TensorFlow JS obviously is very powerful. What we're focusing on is making WebAssembly better. With WebAssembly, you can take existing C, C++, Rust code, things which are very fast and optimized and existing code like anything in OpenCV is gonna be using that. In fact I think we have some demos here, yeah. So these marker demos are actually using OpenCV like the desktop C++ version compiled into WebAssembly running in the browser. So WebAssembly gives us the ability to reuse these amazing tools and do it fast. WebAssembly can run faster than JavaScript because it has certain limitations on it that make it faster to optimize or easier to optimize. The other thing about WebAssembly that we're working on is the hooks between WebAssembly and JavaScript and the DOM. So hey it's great that I can do all those computer vision processing, but if it takes 20 milliseconds just to get the image into it, to the WebAssembly side and then maybe get the image back out, then that's not really helping. We are actively working on improving the whole WebAssembly experience and making it possible to do computer vision in the browser, purely through WebAssembly library. That way vision won't be like hardcoded into the spec. It was like we only use this one algorithm, and that's the algorithm we're gonna use for 20 years. No it will just be a part of your web application. But it will be viable to do in the browser instead of having server-based solutions. That's really cool. We're gonna lot of exciting stuff. What type of MR experience you think attracts more people? Gaming or common apps? Currently probably gaming. There's two things. There is the people who they hear about WebR and they want to try that first experience. Game actually probably is not the best thing about it. For someone who is doing their first experience, I recommend something like The Blue which is one of these underwater experiences where you don't really do anything. You're just there looking and you can see the shining light through the water and a big whale swims up, and it conveys the potential of immersion. So The Blue, that Mars landing thing and of course the People's House video are what I usually start people off with. Once I come back, I wanna do more. Games are probably first just because at least half of everything out there is a game. Some early experiments with VR movies. They tend to be short like five to 10 minutes. I feel like VR movies aren't just movies in VR. It's a new medium and we don't know how to use it yet. So people are still experimenting. But we're gonna get some really cool stuff there. In terms of things more like tools, I think that's gonna come about more from augmented reality where people are gonna use, it will start in industrial phases like construction or airplane repair. Something where someone needs information and their hands are busy. Once I'm assuming Apple will eventually ship like an AR iPhone or something like that, at least according to the rumor mill I imagine people will start using them for stuff like cooking. My hands are dirty, but I want the cooking instructions or I'm video chatting or something, things people use their phones for today in a hands free space, and we're just gonna see how this evolves. Don't think that like you're late to the party. Like it's still such early days, and then what we're gonna available in five months much less, what we're gonna available by the end of the year much less five years is gonna be crazy. So don't worry about being too late, just experiment, try new things, and share. Mostly what people are doing on Twitter with Web VR is just sharing their experiments or cool things they found. It's really exciting. Last question here was are there stats about Mixed Reality uses versus personal experience that we can show people just to show where we are. We don't really have stats yet and that is something that we're trying to fix. It's definitely a problem. I'm hoping, I assume we have some statistics about like number of times the A-Frame library is loaded from the CDN. We want to get like a central location of this kind of stuff. All I can tell you right now is it's growing, but it's really early days. If you do like a Google Trends search for things like WebXR, it tends to spike whenever a product comes out like when we announced Hubs, there was a big spike and then it drops down. It's essentially noise currently because we still really are in the experimental stage. The good news is, this is not gonna be something that just disappears. This is a long-term technology. It's gonna be a slow burn. A lot of big companies are investing in it and making good products. A lot of early experiments will fail but that's how we learn. But it's this is the next part of the web and the next part of how people interact with computers. I'm really, really excited. Okay do we have another way of taking questions or all the questions they're here? I know if there's something else, I'll just check. - I think the question are there. I don't see any others in the Telegram channel. Havi, do you see anything? - No I don't see anything else. I was going to drop a last call for questions in the Telegram channel. And so Josh if, is Twitter a good way to reach you or do you have contact info? - The best way to reach me, yeah is Twitter or just email josh@josh.earth. And my streams... There we go. - This is where I'm doing all of the classes through the streams. I'm typing this at the top. - Oh terrific thank you. - And that let's see, specific resources. Follow the Hacks blog and the MozVR blog. This one about teaching eight year olds, this online Medium Blog. The link right there. And this workshop. This is the Glitch that I use for the 3D workshop or the 360 workshop, Glitch. The really useful part here is you can remix it and A-Frame by itself doesn't really have a way to remove the scene and replace a new one, like there's a prefab component for that, so either one. So here is this component, which is standard A-frame kind of stuff, but you can use it by saying you're creating an entity with a screen component and give it a name. And then any object in here, you can add a two screen and then the name of the screen that it should go to. And that's all it takes to make these little hotspots. Then you wrap it all inside the screen switch and tell it which screen should be the initial be shown. A-Frame was really easy to extend. So I can make this nice wrapped up little component and everything works for it. Okay, let's see what other questions. Is Linux support for headsets and WebVR on their way? We want it to be. Linux graphics and Linux has always been tricky. But it is a very important platform. So honestly I would have to talk to our graphics guys to find out what the current support is. It looks like Fabien said yes we hired consultants for that a few days ago. Okay sweet, that's good. So it's coming along. - It's great, wow. - Any other questions? - That sounded like last call to me. - Okay. - Back in the channel said you covered a lot of ground, thank you so much Josh. - Thank you all for coming. You guys are in many ways our feet on the ground. I can create information, but you're the ones who share it and actually teach people day to day. I love doing these updates with you and please email me if you're gonna do an event and you want, like if you're gonna do a workshop and you want certain resources, you want like constructions, email me. I'm putting all those stuff together specifically so you guys can work on workshops so let me know. With that, thank you all. - Thank you. - Have a good week. - You as well, stay in touch, thanks again. Alright folks you know where to find us. You well, bye-bye. - Bye. 