 each year microsoft research helps hundreds of influential speakers from around the world including leading scientists renowned experts in technology book authors and leading academics and makes videos of these lectures freely available you okay well good morning everybody thanks for coming today it's my great pleasure to introduce Aditya Kayla to tell us about some of the recent work he's been doing on open NF I did there for those of you don't know is on faculty university wisconsin and has done a lot of good work since he's gotten there in redundancy elimination complexity understanding quality of experience of what not and a lot of his work has been really influential personally i think i've known as it there now for 10 15 years i think we started grad school at roughly the same time doing overlapping things to both our mysteries but you know we grew up anyway without further ado I'll hand it to him thank you thanks everyone for coming to this talk so I'm going to be describing this system called open and F that we've been working on for nearly a year now it's basically something that combines the power of stn with software virtual appliances and it enables a variety of distributed middle box style applications a lot of this work was led by my student and Gambhir but he got a lot of help from Roger who's in the audience today and from Jeanette and Robert as well so how many of you here are sort of know what you know middleboxes r and s dns and so on who doesn't know i guess is there anyone who needs something you don't ok really ok all right so I do but ok so let me just quickly works with it all right so I'll just start by telling you what these software virtual appliances are and Wes DN and why they're being used and then I'll present the motivation for the distributed processing that it works that we are doing so traditionally you know you build your network sort of routers and switches and they provide very basic functions like connectivity between different points simple access controls and so on but often network operators want more packet processing functions from their networks for security for performance reasons and so on so network functions are or middleboxes both of these are fancy terms I do not know what they exactly describe but they are terms are are devices that help fill this gap so they allow an operator to introduce custom packet processing functions into their networks and these are you know a lot of examples out there things like load balancers for balancing in network load balancers for balancing load across servers things like firewalls SSL gateways intuition detection system prevention and detection systems traffic's covers for security caching proxies and ran optimizers for performance and so on in contrast with their routing and switching counterparts these devices maintain their stateful in nature they maintain a lot of state for the flows that they process so that's the big difference between these and and routers and switches so you may be wondering why am i working on all these things these are not arcing they're actually very popular a recent study from Justine sherry and others found that these middle boxes are quite widely deployed across a bunch of enterprises out there this is a an average across 55 different enterprises and the key takeaway from this graph is that there are at least as many of these appliances deployed in enterprise networks today as there are routers and switches and this is not just true for enterprises other networks such as isp network cellular networks have a similarly you know and widespread deployment of these devices so it's not a surprising thing that the market for these deploy these devices is a rapidly growing multi-billion dollar market and more as you know newer applications arise newer devices arise newer thread so much new appliances are designed and they get deployed their popular but they're also extremely painful to manage you know in general large networks are hard to manage and once you throw in these custom packet processing things they require custom configuration custom wiring and a lot you know they're generally difficult to reason about so they are pain pain to manage so this is sort of where software appliances and sdn come come these come in and these are two trends that are actually making the management of these networks that have these network functions simpler to manager so the first thing is that you know traditionally these were hardware devices they're moving into software I know this is riverbeds steelhead appliances it's virtual a counterpart that f5s load balancer this is an in cloud counterpart of that these software alternatives are cheaper they are easier to deploy upgrade customized for a particular network and so on so they kind of bring down the management burden significantly already software of the movement is from customized hard way to commodity hardware like yeah so so so these are so it's it's it's a big support so I think there are models where you would deploy on a commodity hardware sort of an extensible middle box you know along the lines of co m be where you can deploy a bunch of different middle box applications and they would all be running in software but these are sort of machine images these are sort of in a packaged as VMs that you could deploy inside ec2 for example so both of those are fairly popular trends yeah these things become easier so it's actually it's hard to quantify exactly you know whether it things become simpler just hold on to a little bit why think once we talk about sdn some of those things will become cleaner the capacitors are associates replacing one big iron box through the blessing tons of smaller life with possible official or one-to-one your slides and she showing oh yeah it's not it's all you need to use multiple of these you need to use multiple of this like I did yes and no I mean let's wait up to the next next set and then you can we can revisit this question okay so the second trend is the use of SDN again most people hopefully here know what Sdn sdn SAR sdn is basically a framer that provides you logically central control over the forwarding state of a network and to see how this is improving management of networks with these NFS or middleboxes note that traditionally the way to use these middle these devices was to deploy them in choke points within the physical topology and then wrestle with distributed routing protocols to funnel traffic through those choke points to get some traffic subsets processed by them with sdn because you have sort of this control over state you can deploy these boxes off path and then punt traffic in and out of them to specific traffic subsets in and out of them and that immediately takes away the central points of failure and attack here and center look points of network congestion and sort of that aspect of the management story kind of becomes better the other thing you can do which is sort of an interesting line of research in middle boxes is that you don't need you what you could do is you could have multiple different kinds of these middleboxes hanging off of the network and you can chain traffic through those different sets of middleboxes such that specific traffic subsets to our specific sub chains and that allows you to use realize about a variety of interesting rich high level policies and that kind of stuff is something difficult to realize with Hardware appliances deployed it at show points okay so what this talk is about is sort of building on this notion of decoupling of middle boxes from network topology that sdn enables what does decoupling means is that you don't need to use one instance of a metal box can have multiple instances deployed at different locations within your network and you could use Sdn as a way of steering traffic subsets towards those different instances but Sdn actually gives you more teeth than that it allows you to dynamically reallocate traffic across instances in order to achieve sort of dynamic control over the distributed processing happening across those instances and you can use that to realize various high level property so one of the things you could do for example you could immediately make load balancing more effective so for example you can in this case you know whenever some little box runs out you can immediately move load off of that metal box by dynamically reallocating processing for those flows across to a different metal box and this allows you to extract maximum performance across these different versions of the middle box at a given cost so this is sort of what i mean by the you know software appliances together with stn kind of making the management story simpler right so there is some element of capex and not having to worry about doing many of these things manually I don't have a concrete argument that this actually makes management simpler but why is that example of this standard name implies rating vouch for maintained software state across multiple instances ensuring you keep on rolling updates ensuring you have a global view of correctness for the whole system right so think of what you could do is just on one box now since you have to do one property ratio from one 200 to 1 2002 month or so you're placing a signal f five more bars you actually make these order of hundreds of software greenest actually provide that kind of chemicals so it's so mean the matamata costs what increasing essentially becomes much more painful because goes out then some sort of your customers are not able to be surrounded even in this kind of a figure yeah yes it for right so essentially whenever you're making any kind of a change you have to transfer the state so what happens to connections which were actually when you're making the case for open a half so let me let me actually okay maybe this is not the place i should have stopped okay so i think we are in agreement so I mean there are some aspects of this that becomes simpler you don't have to put these in the middle of the network I mean that's also the reason why you know for example SLB is used as opposed to a big honking f5 load balancer today I mean it gives you much greater capacity you know without having to babysit custom hardware right but I mean that's kind of the trend today things are kind of moving into into hardware what I wanted to get into is it's not just sort of simple load balancing it actually the ability to do dynamic reallocation of distributed processing allows you to build novel abstractions okay and so one example is in the abstraction of a infinite capacity metal box where if a middle box instance runs hot you deploy additional copies of the distance and you distribute subsets of traffic across those copies even a Richard abstraction that you can provide is that of an always updated always available metal box where suppose you want to update this metal box with security patch you deploy a hot standby that has this patch once this is ready and that will be clear in a minute in a few slides you take down this middle box uses the end to steer traffic to this updated middle box and then you have an always updated middle box abstraction you can do something even more powerful you can dynamically enhance the functionality of a middle box by leveraging an in cloud bra newer version of this metal box so for example if this middle box is seeing something anomalous for a certain subset of traffic you can take the ongoing processing for that specific traffic subset hand it off to a bra newer version in the cloud for additional processing okay so with sort of you had the ability to do this kind of tight control over distributed processing then you can really realize all of these interesting rich abstractions so this so if so lot of cool fun things can happen if you can get middleboxes in stn to play with each other this may not be funny now but hopefully it'll become fun here in a couple of other slides and I will bring back these two characters ok so what open NF is is it's a control plane that can support key semantics in these distributed in this distributed processing application specifically for dynamic all the dynamic reallocation actions going back to the comment that you made the kind of guarantees that it can make is to ensure that the reallocation actions can be safe and that this reallocation can be done at any particular point in time so by safe I'm talking about something like a safety property which is output equivalence so such that when you reallocate traffic you know either to respond to load or whatever else you can reason about the fact that the outcome of the actions after reallocation is similar to that of one single middle box with equivalent infinite capacity and this is sort of a liveness property and what this essentially means is that an operator should be able to trigger these three allocation operations at any given point in time and you should be able to argue that it will finish sufficiently soon within some bounded amount of time okay so there is no control plane out there that does this today we are the only system that can provide these guarantees some of you know BR sitcom reviews are out for rebuttal so this is just my way of making myself feel good so it's you know you can kind of every day I look at myself in the mirror and say you're my hero no matter what the reviewers say you're doing good okay but more seriously so this is actually the first system that can provide these kinds of guarantees so having these kinds of guarantees are useful for two reasons the first thing is that it is actually a necessary basis for some of the new app sections are described on the previous slide without having these kinds of guarantees it's hard to build a dynamic remote enhancement kind of metal box and it also forms the basis for strong essays that you can actually reallocate load without taking down the middle box are impacting availability or that you can less respond to load within some small amount of time like within a few milliseconds okay just to reinforce that further you can offer strong SLS in these things like load balancing elastic scaling and the always updated middle box kind of applications are described on the previous slide in the case of Road balancing for example you can ensure that the load reallocation mechanisms are sufficiently responsive and they don't impact the quality of the decisions made by the NF same question or the net middleboxes in question and you know in terms of these kinds of guarantees being necessary for the for being the basis of new abstractions without being able to reason about safety and liveness you won't be able to build this dynamic invocation kind of application that happens this time you fundamentally need the ability to be able to move flows and be able to argue that I you know that operation has these safety and leibniz properties okay so open NF is a system that can provide these charities so yeah the real need for those guarantees like me taking such a strong stance on like in the end I think the basic extraction of the network itself is you know packets can be cut off aid package can be drawn yeah so why why go for so strong gasps so typically when packets are draw ordered by the network the middle box has some logic to deal with that okay and and you know and the outcomes are sort of basically a function of those possible inputs right what we are saying is that you know we want to be able to support reallocation actions and we want to make sure that the reallocation does not introduce other unexpected inputs that the middle box did not take care of so we want output equivalence with respect to whatever the middle box is internal logic would do for just network induced losses and corruptions does it make sense yes we like yes if you're willing to like admit let's say deviation and drops in the network then it seems like the allocation stuff a reallocation becomes trivial baby you basically you bring up another mailbox without actually worrying about any kind of consistency and this middle box has logic to deal with duplication and drops and why is that so that's so I'll come to an example where that is not possible so basically for example an off path IDs I mean under the if the network doesn't induce any drops then anything that where there is a missing event that would actually trigger some sort of an alarm for the of path ideas or if some kind of reordering happens it would trigger an alarm for off path ideas but in the case of an off by the ideas it doesn't have the ability to request those dropped packets because you know the you know the application may not transmit that so it had it would have a deal with that missing hole in some specific way what we want to make sure is that the reallocation does not change the nature of the decision that the IDS makes given what is happening with the application and given what is happening with the network does it make sense so if you can engineer a network that ensure that does not corrupt packets and that does not introduce reordering or drops then the middle boxes are output should be the same without without without reallocation our network that is my chance over the fact is we do have networks that for our package so what that means is like maybe it's going back to enjoy and like the mailbox largely should be robust enough to these things which case you don't need like this controlled name is such strong guarantees yeah so so I think so let me get to the loftiness guarantee and there with that example we can revisit this question right so I think in that specific case you know we will see you know how we are sort of shoring up the decisions that the offer ideas would take maybe hold on to that thought until may come to that example okay okay so just a quick sort of you know it just is Sdn based control just enough so this is a simple elastic situation scaling kind of a situation where you have to traffic flows a red red traffic subsidy and a blue traffic subset each of these devices create state for the individual traffic subsets because these are stateful devices suppose the volume of traffic on these individual traffic subsets crows causing the intrusion prevention system to run hot you decide to deploy another instance and essentially what you want to do is you want to shed half the load now so there are two choices with sdn here the first thing is you can wait for new flows to arrive and point all those new flows to the new instance but in this case there are no new flows right this in this extreme example and this therefore this bottleneck will persist so this impacts the responsiveness liveness property that we were after another thing you can do is you can say screw it I am just going to move the blue flows the problem is that the state that was created for the blue flows is missing from its new location so that impacts the the decisions that the intrusion prevention system makes in this case it may cause it to either raise false alarms or it may cause it to miss detecting attacks okay so essentially what this means is a simple sdn based control is not enough in this particular case if you just relied on st n then that will make some Milhouse cry somewhere and that Milhouse could be your network operator or middle box or whatever else right just going back to this picture the what we really need is that in addition to being able to control the routing of traffic we also need to move the state to the traffic's new location so essentially to satisfy those kinds of properties what we need is something that provides joint control over forwarding state and network internal state okay so open NF is basically a system that that addresses this yeah just for that you should you use the extreme example very flows yeah is that a valid not really actually so new flows will arrive but existing flows can last long enough that the load never goes down to below the point that you care about I think it will be it will be clearer if you think about this as a scaled-down situation where you want to decommission a metal box and you want to take load off of it you just have to wait for traffic to drain out and can take several minutes sometimes even hours for traffic to try it out yes I'll actually show you results where you know in the simple traits that we analyzed it took up to half an hour for a middle box traffic jams making sure that the kind your client is worth the is worth the benefit right because you seem to get the sort of strong man for why it is we could use the native technique and just waiting for some terrain yeah yeah and yeah hopefully at the end of the dog you'll be convinced that was not a terrible hike of the hair okay so yeah the state is from the old to the new bar is what do people do today so what so what people do today is the first alternate is to wait for flows to drain out right so you you have you want to decommission a metal box you wait for it to drain out and then you check it out and start using anyone so all new flows go to the new metal box all existing flows have to train out from the old metal box you know there are some research papers out there just do reallocation there will be like okay let's just reallocate but what they ignore is what happens to the false positives and false negatives in the decisions of the metal boxes there's like a simple you know time or space for what after that yeah the problem is you don't know right i mean you don't know you're monitoring some set and you don't know whether you don't know how long you would have to wait this would be an empirical yes and you know flow lengths can be arbitrarily long and you have to wait for that tail flow to finish yeah I understand the obstruction by are you trying to provide a sort of an atomic operation which essentially would say that consider this set of boxes as a single logical unit yeah so you can from n 2 in perspective you really care whether the force of traversing a single single box of multiple search boxes as long as we can because coordinate these distributors what are they in the state from one to the other right here so that have to me I presume the example goes beyond any good yeah from an end-to-end perspective or the tenant or whoever is using it he just sees that as one middle box ok and there is a control application that wants to preserve that abstraction of that one available middle box that offers a certain kind of performance and to do that the control application may need to do some reallocation of processing and we want to ensure certain semantics for that reallocation of that processing doesn't make sense what is the particularly I guesses and levels of notorious of you know falling down under high load so when your control applications try to do this reallocation it could just be that you waited presuming you would have some some motion off its signal smoothing function you don't want to turn become aggressive we can have the winning slight impression off from over you want to start doing this rebalancing on your mission because you just put it on see the same time it also lets do reverse commute sits like a ticking time bomb now you know the by the time you do anything of the allocation have you centrally have lost eighty percent of your traffic yeah so I think that sort of it's sort of so so in this picture right so this is sort of open enough controller so what you're describing is this application upset the elastic scaling application so all of that would be in the logic for the elastics at what point does the application trigger a scale out I mean and that's sort of an imperfect sign so the applications may decide once the middle box sees more than sixty percent load over a ten minute interval I will start to spin up a new instance and start sending load there so that's a policy that that's up to the application right and we are not saying one way we are not advocating one way or the other what that policy should be but that's up to the application right once the application makes those choices it would essentially translate that down to some set of reallocation operations okay and that goes into become some sort of state import and export across these metal boxes and and so the open end of controller does those things in coordination with the network to support some semantics for those operations of those operations will will complete safely and within a certain amount of time back to the slide feels like that where you're talking more we'll see if you like us yeah make sure you have an implicit trust award incentives as soon enough they saw that suit seems you know explicitly now tied to how sensitive or how aggressive you are in triggering this particular function right so then you cannot just give any opportunity to essentially now see ya yeah yeah yeah so yeah so essentially what we want to do is you know the what can the controller provided so if the controller can say look you give me some kind of reallocation operation I can give you a done signal within 500 milliseconds okay so that's the kind of soon enough kind of guarantees that we are we are after okay there's a time that it will take to be done and but that is a bonnet amount of a very good reason about how long that's gonna take and it is going to be small and I will show you how we get it isn't exactly jumping into better on the mount of schedule of transfer yes yeah what is it's all known and I will tell you why that is the case yeah do you pause the traffic during the transfer no no so again I'll get to that we are no pausing of traffic happens traffic is getting processed and that's the that's the thing that we have to deal that's a big problem it creates problems for us we don't want to pause the traffic yeah so how can you do that if you know you just say at this point you wanna transfer this stage you package are coming in they start updating the old stage yeah okay so what you're talking about is this race commission ok so I'll get there that is the biggest challenge in designing open NF ok so so these are the challenges so the first thing is we want to be able to bring in arbitrary NS that are out there into the fold we don't want to change anything about how they do their internal state allocate reallocation because that may restrict the set of NFC support what you are referring to is this big challenge that we have here you know as we are moving state packets I have an update state and so state those updates may be lost at the state's new location or they may happen out of order or state can become inconsistent in some way and we want to make sure that doesn't happen and those are the guarantee is that we well this is the safety guarantees that we want to why any other questions before I jump into the technical what yeah it's probably just purely embedded right yeah so so that's not party schnabel stage because it has a global not yeah so in theory you can have it enough that doesn't fit in this okay but what I'm going to talk about is that in in practice what the the NS that we looked at essentially either create state the day that applies to a single flow or some group of flows or also this is a bro's internal state so there is a per connection object that consists of two objects a TCP analyze an object and an HTTP analyzer object there is a bunch of state that is shared across connections this is like you know / host counters which is often used for scan detection and then there is a bunch of statuses that are maintained for all traffic traversing the the bra instance okay so we can basically think of state as being defined by these three different scopes either it is per flow state or multi floor state or all flow state and we can use the traditional notion of flow which is the connection part able to describe what state we are interested in and where what we want to do with that state okay so the the API that speaks directly to the NFS is actually fairly simple it has three calls get put and delete the calls would take a filter f that describes the state that we are interested in which is you know typically some subset of the connection 52 / don't cares are allowed it also defines the scope whether we are interested in the / flow state matching that scope and that filter or the multi flow state or or whatever but the key thing is that once that gate code or delete call is issued it is up to the NF to identify and provide all the states all the state matching that particular filter and that is for when it is exporting state and when it is importing state it is up to the NF to combine the state provided by the controller with its internal data structures okay so what this means is that the controller does not need to know what the internal state organization is your kind of punting all the work to to the NF and we also don't need to change the NF to a specific allocation strategy okay so this doesn't obviously make it easier the change in NF to bring it into our fourth but at least NS do not have to be redesigned to work with opener f okay so let me actually get into these Ray's conditions and what I mean by these you know how they may interfere with the kind of semantics we want to offer for these high level operations so first you need to tell you what is high level operations entail so suppose here we have two instances of a and off path intrusion detection system and high level operation is to reallocate port 80 flows to this instance and that's the blue flows so the first thing you would want to do is to move all the flow specific / flow state corresponding to the blue flows from NF 1 to NF to hear you just want to do it for port 80 flows the other thing and this includes both the TCP analyzer objects and the HTTP analyzer objects very scared of the previous slide the other thing you may want to do is you may want to copy the state that is shared across flows recall those connection counters that were maintained / host you could have some hosts connections go here and some hose connections go here so you need to maintain copies of that state right and essentially the the high-level semantics we want to offer for that reallocation operation over there boiled down to semantics for move copy and share right so in particular for move what we can provide is that the move is loss free and order preserving and for copy and share its various notions of consistency like you know eventual strong strick whatever okay okay so let me describe very quickly the move operation and then the race conditions move operation starts by the control application issuing the move the controller issues a gate call the NF returns a bunch of chunks of state with flow IDs each ID is basically some subset of connection five triples things that describe the what the state corresponds to at some point the controller issues a delete and then puts these chunks one at a time into the destination instance for this move and at some point it updates the forwarding controller the race conditions arise because of this import of state and export of state and the interactions with the forward let's start with the simple race condition where updates can be lost and this arises because packets arrive as state is being moved and that can lead to some bad consequences so here is an example packets arrive they establish this state you have issued a move for the blue flows the blue flow state has moved at some point another packet comes in it updates the state at the old location routing has been updated the problem here is that the updated state due to be two is missing from the state's new location in the case of an IDs instance for example if it's looking for signatures or md5 checksums to check again some kind of an attack depending on how it is implemented it may either miss that attack because it computes an md5 and it doesn't see that signature or it may throw up on alarm saying I saw something bizarre okay so both of those things are sort of things we would like to avoid okay so one solution for this is just stop all traffic stop all processing buffer all the packets let the state move then reallocate flows now this can last hundreds of seconds you may have to buffer a lot but the more important problem is that when you stop traffic they may still be some packets that were traversing and updates do to those packets to the state may be lost and you have no way of knowing whether the those updates lost or not so the higher the semantics that we want to provide something we call loss freeness is we want to make sure that all state updates due to packet processing are reflected in the transferred state and every packet that ace which receives should be processed by nnf instance okay those of you who are familiar with the consistent updates paper that does the one-shot update essentially it provides the latter property but it cannot provide the former property so we are interested in a stronger consistency consistent update property than what they the right blood paper kind of offered yeah yeah too because it might just be in your view what about rate condition but to be sure that all of that is up to the control application we are assuming at the control application is monitoring that and when we tell when it tells us to reallocate there is a reason why it shows to reallocate doesn't have sufficient capacity so justice cross that you know it cost for a TCP son on the floor but then what is that figure does during the transition during the migration recapping you flow that was supposed to be stopped if I sort of both back to the regional conditions that the very fact that you are training area location is that there is something running monitor play fact that nothing is running hot you're also not able to say coops a new connection a new layer of connection that are coming here yes so what I mean now what sort of consistency or properties in those nations are not just know so is she saying is I mean I'm trying to get my head around best effort versus guaranteed here because I mean that sounds like a customer pick essentially for things we were able to keep up with a transporter will try to pass program but there is no guarantee that all the state will still be minted in a consistent manner um John ratings yeah so I'm trying to wrap my head around that question so i guess one possible answer me is this hot mean ninety percent yeah i got here before you before you burn up for this hot me another ten percent and on your drop yeah so I mean I would think that the elastic scaling spell application would probably have conservative thresholds like you know I'm starting to see over some period of time it's hitting eighty percent and then it would your system I mean even if you start that we are getting a ten percent as one percent mmm the load is an external signal we don't control that yeah so unless you can start dropping things anyway during that transition you will still wrong things so I'm essentially trying to understand is where is it what you are not guiding in the state then you're essentially saying I had some stain of the network flows which were passing through me right some some part of that would get transferred to the new okay here so what we are saying is that the state we are transferring will not see losses due to the transfer okay it could see losses due to whatever the middle box is experiencing right that's up to the implementation of the middle box ok so our transfer doesn't impose any other losses in addition to that that's the guarantee they're not doing anything all think the common case is there are yours we can lean on corner cases where you cannot predict the load and load is so high that something's kept dropping but not but I think maybe the way you think about the common case where the load is not widely fluctuating other confidence I would say that is the convocation strong for those things feminist thinks failing is not the comment know things feeling into overload that's not the point is you are a common case like load load moves smoothly it's right light because load can be one so it's all boils down to i think of conservatism control application wants to be so the way to look at it if Utley is an opportunity in some sense to work it as yes bad things will happen and which means like nothing will provide a guarantee right so that's a portion let's take this yeah I think that this we should probably take this discussion a little bit offline because I have like half the talk to cover yeah oh yeah I'm going to come to that okay it doesn't work for scale down think about it right how would use how would you use via migration to scale down well i was thinking in terms of you can become difficult if you replicate be each incoming packet or both instances I'm talking about scale down when you when you want to collapse two instances into one instance how would you use vm your application for that okay we will come to the way okay well what do you do so you have to VM instances how do you merge their state you said she will have to apply some enough specific qualities yeah okay that's complex so we will come to that will come to that I think we are going off in a very bizarre tangent here okay so so the key idea we have is the something called event abstraction that I'm going to use to to address all the race conditions so this is basically a way of preventing state updates from happening and then sequencing them in a particular way this also helps us reason about the time it would take for these operations to finish so the simplest way we do this is packet comes in we enable this event it applies to all the blue packets and what this tells us is when a packet is received raise an event to the controller but drop the packet locally do not process it okay so then we should get and delete state is moving to the second instance packet comes in even get gets raised like it gets buffer to the controller the with along with the event eventually state gets put put returns and then the controller flushes all the buffered packets to the destination instance any packet that arrived along in the interim is handled in a similar fashion at at some later point in time the forwarding is updated and then all the packets that arrived along the new paths are updated similarly so if there are any remnant packets they raise events they get processed but all the packets get process no packet is left behind but they may get processed out of order mixing so we can prove that this actually ensures the movies loss free going back to this out of order stuff things can be processed the at the middle box in an out of order manner again this can happen because of the interactions between the steps 4 and 5 on the previous slide where we were flushing the buffer and updating the routing so a buffer gets flushed with the events in it two packets get sent to the destination instance in the meantime because the routing has still not been updated and older packet comes in goes to the older instance on the blue flow the event gets generated gets buffer to the controller then we kick in a routing update suddenly a new packet shows up gets sent to the new controller and then the controller eventually releases the buffered packet and so the packets are processed out of order this matters for something like bro runs this thing called a weird activity script that looks for things like requests happening after response was seen or a sin happening after data was seen those are indications of attack and these can trigger such alarms inside probe okay so the key order we want to make sure is that all the packets of process in the order that they were forwarded to the NF instance was a question since okay so again the basic idea we have is we want to keep track of every packet but you want to know what the last packet seen by the old instance was and then sequence all updates around that last packet okay so the way we do this is we again flush all the packets just like in the previous case but we marked them with a do not powerful flag which means the destination has to process them we enable events on this instance but instead of dropping the packets we tell the instance to buffer and hold on to any packets that it receives but instead of just blindly updating routing to send to the other instance we first create an update where packets are replicated to the controller end to the original instance and what this tells us is anytime any packet is sent to the controller and even gets sent to the controller and eventually get sent to the flush to the destination where it is processed but it helps us keep track of what packets remnant packets the first instance is sing so we will know which one is the last packet that the first instance nf1 saw so what we do is suppose B 3 was the last packet that that was seen by an f1 we have updated the routing state some other packet comes in it hits this event gets buffered locally but at some point this packet gets processed it will raise an event to the controller so now we know that the last packet b3 has been processed by NF too once that happens we can release all the buffered packets and let them be process so we kind of can sequence the processing of packets by shewing events of the two instances yeah I bother making sure I'm in a bit of another water anyone is trained not yeah so we are talking about things going out of order in different directions of a connection if they are within a connection typically the middle box will do reordering but if it happens across directions the middle box is no way to know whether that is weird or whether that is normal okay total desert address the original question that you had yeah I think the answer is if I'm that milkha are sensitive can we want to fight the devil contract or not yeah there seems is an expectation that normally that doesn't happen if it happens a lot it's okay to be two ways of falsehood yeah yeah we don't want to exacerbate that we don't want yeah exactly it's well worth it yeah here you see me controller is very beefy ok so the controller right now we have an optimized controller it can handle I don't know a few hundreds to thousands of events per second with buffering and all that we haven't optimized that further and it also has one socket that it listens to for everything it's kind of very poorly written right now but that's an active area of research how can we make this controller robust and leave you but we don't see any fundamental bottleneck to scaling that ok ok so just quickly about copy and share copy is an operation to provides eventual consistency of state across middle box instances share is something we can use to provide stronger version of consistency it is kind of its like a poor man's version of Cher right now essentially what we do is we stop packet processing from all instances that are sharing state we have them all generate events we cue the events in a 5-4 and then release them one at a time to NF instances and once we know in NF instance has changed the state and that is known through an event we then trigger copies of that state across all of them so it's sort of like very heavy weight implementation of strong consistency ok so what do applications do they decide on the granularity they decide on the scope which state they want to operate on whether multiflow per-flow state and the filter you know which whether it should be a per prefix were just port 80 flows and they also decide whether to move or copy or share that state and they also can the describe the guarantees of the desire we provide 34 move no guarantee or just lost Rinus or loss trained as an order preserving for copy it is either no or eventual consistency and for share it can be versions of stronger notions of consistency okay so let me describe this quick application and then I will go into the the evaluation stuff the point of this application is to show some of the choices of that can be made with respect to the API that we have designed so this is bro I don't know why they chose the logo it like it looks like as a 10 your eye but but that's what it is right so here typically grow runs these are very popular scripts this is a scanning script that looks for connection counters this is a vulnerable browser detection script that essentially looks for all HTTP requests and analyzes the signature this is that weird activity script that I was describing that looks for both directions for bizarre stuff to happen okay so here are two sets of flows under threat going through the third metal box and what we want to do is we want to move some prefix between two metal boxes this is the prefix we want to move this is the old instance and that is the new instance we want to move to the first thing that that would translate to is basically copying state from all the multi flow state curves from the old instance to the new instance so this is the multi flow state connection counters we want to copy that is important for the scan detection stuff and we want to make a move the / flow state from the first middle box to the second metal box and this is describes that it is per flow and these are the guarantees we want we want loss free and order person and I will get to that in a minute why we we are described disk we want those strong semantics and the way we do sort of eventual consistency is we have a loop that every 60 seconds copy state across those two instances copies the connection counters from A to B & B to a okay this is sort of what this application would look like the way it is written so the key thing to notice is that this multi flow state is needed for the scan lord bro skipped script the vulnerability detection stuff that is mainly looking for poorly designed browsers needs just lost freeness it doesn't need loss free and order preserving because it's only examining one direction what weird activity script needs both guarantees and again this recapping is necessary for the scanned or Brewster so you probably would have to look at your application and then based on that application you can decide what kind of guarantees you want and what specific chunks of flow state you want those guarantees to apply do does that make sense okay I had a more complex example of an application but I won't go into that I want to quickly go over results and is some talk about whether we can deliver on the promise that we had earlier but are there any questions at this point is this kind of clear how the application would get ridden okay okay so this is a real thing we have this controller that we wrote on top of floodlight people are familiar with floodlight it's a java-based open flow controller so it's written as a module in flood light we provide a shade NF library roughly 3,000 lines of code and we modified a bunch of different metal boxes out there bro an intrusion detection system this is a an acid detection system which looks for various signatures of different kinds of operating systems running in a network a firewall and NAT and a caching proxy roughly up to eight percent increase in lines of code for bro which was the most complex thing we change this translated or something like six to seven hundred lines of code we had to add to support open NF okay so just some quick micro benchmarks what does this mean for you know the kind of actions we need to have the NF support and what is it that mean for the high level applications so this basically shows the performance of get and put / flow across the three across three middle boxes that we modified and here what we are doing is we have some number of flows that whose state we want to get and then put and be very that number and this is the total time it took to do that operation so the big point to notice is that first get / flow takes a lot of time lot more time than Dan put / flow and we sort of if you look at the thousand flows case in in the case of bro for example get can take up to 815 seconds and the big contributor to this cost is the serialization and deserialization of state in both cases here unpacking state changing it in a wire format and so on the other thing to notice is that as a complexity of metal box grows there is the complexity of the state also grows but the key point is that once you have a metal box you can benchmark these things and you can kind of know how long it would take for a certain sized state to be read or return okay what does this mean for the operations what kind of guarantees can we provide that some operation being complete in certain amount of time so here the setup is we have 5000 flows in the some metal box and we want to move all the flows with their / flow state the middle box is seeing about thousand packets per second it is this fifty percent utilized if we have no guarantees requested the move operation finishes in about 250 milliseconds this is actually almost exactly the serialization + D serialization cost we can paralyze it in this case what is happening is first all the flow is state is being read and then it is being written but we can paralyze read and write and that will improve the overall latency further but the point is that in both of these cases there are packets that are dropped as state is being more that's up to 200 packets if you kind of see that the number of packets that get transmitted at thousand packets per second for the 250 milliseconds it takes to move the state so suppose if you do Los free with the same parallelization optimization we see a slight increase in latency due to handling of events right and that translates to hona per packet basis an average latency increase of between 100 milliseconds and a maximum latency increase of 250 milliseconds this is because buffered packets will have to wait for the state to be transferred and then buffered packets get released and this latency is exactly the total time it it took to put the state and there are about 200 30 packets in events again this you can calculate because you know again the load that the middle box is a scene and you know how long it would take for you to move the state with a different optimization we can bring down the bakit latency even lower and essentially what this optimization says is in this case we wait for the entire put for the whole state to return and then we start releasing events but we can wait for individual chunks of state to return and start releasing events corresponding to those chunks so we can reduce the amount of time packets are buffered at the controller and if you want the stronger guarantee you end up seeing higher latency and a lot of the higher latency here is because of packets buffered at the destination instance waiting to be processed but the key claim though is that these Layton sees on average latency seen by a packet or the total completion time can be calculated making some assumptions about the network it's a function of the load the thousand packets per second the amount of state that you're moving which is a function of you know how much how many chunks of flows yours that you have seen so far and the processing speed of the middle box once you know these things you can kind of predict that I will finish this safe move operation within certain amount of time okay and that we argue is something that form quick can form the basis for SLS yeah curious now realistic protocols would ever say these three zeros in your the latency jumpers yes a single software instance I don't I doubt it a single stateful software vm yes million packets per second yes oh I wouldn't seem like a puppy extra-large p.m. same standard 84 box lower end of things okay I mean yeah sure I didn't mean to poke fun at you but that's probably possible uh I don't know it's not a deficiency or whatever just curious where this was coming from but I I what I'm wondering is I why would anything change here my question is what were the latest numbers will fight ah what is that function of look so okay good questions so if you if you had a million flow so let's go back to the previous sure I mean but the latency is determined by how much state your your your your transferring and that is a function of the number of flows that you're moving and the the complexity kind of gross sorry here with the number of flows that you're moving right so you can kind of imagine that a you know when you have a lot of it depends on how many flows there are even if it is a million packets per second if it's across I don't know a thousand flows the amount of time you would take to do the move operation is purely a function of that okay what would happen is when you're moving that state the million packets per second determines how many events are raised okay and how many packets get sent to the controller that is what the million packets per second would determine and you know and we need a robust controller to handle that let me just you know that's you know it's up to the controller to be able to handle that kind of a load it doesn't change the time it takes to finish the move operation if the middle box can really handle that many packets really all we are talking about is all those packets coming to the controller and then the control sends them to the destination mailbox and they can process there if the controller can handle that kind of a load we are fine but the total latency of the move operation itself is a function of the amount of state moved it has nothing to do with the load but it depends on the number of flows and how the middle box is offering yes right the number of flows becomes 10 times or is a linear function is right ok so well that's kind of the point of this graph I think it's kind of linear maybe we are doubling the number of flows and the size of these bars is kind of doubling sure yeah but we haven't done fine benchmarking of that of how the size of the state would grow but I don't see why it would be anything but linear I mean the amount of state e power flow state should grow with the number of flows ok so let me talk about the last thing which kind of got brought up which is why not use vm replication here we did this elastic scale up scale down stuff again I'm running a little over this is probably my last slide so and again some way midway through we scale up to a new middle box and then at some point we scale back down with open enough it takes about a quarter of a second to finish the scale up and quarter of a second to finish the scale back down with VM replication when we did the scalar we actually found a bunch of bizarre log entries at the scale of thing and that I mean our best hypothesis is that because of superfluous state that the that that particular vm instance did not need and there's no clear way for it to support scale down ok and if you just did forwarding control alone ask scaled on is delayed by more than 1500 seconds this was going back to the argument that the question you had John how long would we have to wait there was a particular flow that we saw in our traces that lasted for up to half an hour and we had to wait for that flow to clear up ok so that's basically it you know what open NF is it enables fairly rich control over state and distributed processing and we can provide sort of clean semantics and reason about performance and you can go to this website to download float code and play with it I don't know if I have time but I wanted to reflect on the relationship with sdn if I have time but if people don't feel free to leave do I have a minute okay so rustling the email that is sent out said this was a talk that was sort of like sdn for metal boxes so it's it is and it is not so sdn provides control over forwarding state we provide control over NF internal state so in that sense it is similar but in many senses it is different from STM so in sdn a controller can create stated it does it sort of computes the outcome of some routing protocol and creates creates a forwarding state and pushes of forwarding state in open and f we made a conscious choice not to do that we are just handling state we are moving state or we have copying state but all the state is created by the metal boxes we haven't ripped that logic out and reimplemented that at the control panel that was a conscious choice because we couldn't figure out a common way to do that across all the metal boxes the purest view of SDN is that network elements are down control pain is ripped out and in implemented somewhere open NF is not so pure there is still a lot of stuff going on inside the middle boxes we don't really know what a ripping out a control plane would mean for a metal box so in that sense it's sort of similar to sort of the pragmatic view of st and there are there is still control pain outside in the middle boxes you have a logically central controller that allows you to lay with forwarding state the other thing with sdn research is that you know people were like there was a lot of hype and buzz about sdn and then I realized we have to deal with consistency of updates of state so it came as an afterthought you know rattle has done a phenomenal work in reasoning about various consistency semantics you can kind of think of open NF as sort of SDN with okay let's deal with the consistency problem from the ground up ok so that's the relationship with sdn research ok so I am done if you have any more questions I'm happy to talk about them thanks I want to do yeah with this one actually I think the distinction is actually less than say like in the end I think even even in the forwarding case routers if nothing else are generating an updating state if anything I want openflow counters is an example of a state and your controllers what some controllers make decisions based on income flow hunters in that sense I think okay the only difference is like yes you're not moving state or whatnot but these switches are in the end generating some state based on which decisions get made and things like that happen so yeah I mean I think you are probably quibbling more with this argument than with this yeah it's not that the the second one yeah but what I'm the point that I was trying to make here is that the the forwarding state that gets installed at a switch could be completely determined by the controller switch starts with something in the controller can say throw that out use this instead right we are not quite doing that we are letting the logic pertaining to an NF determine what that state should be all we're deciding is where to locate that state we are not actually computing that state for the NF take take proactive sdn instance enjoy they are creating state for the switch mm-hmm and so are you in the sense right like when you when you create tunnels and things like that you are proactively telling this village hey here's your stage and new things based on this date okay and in that sense you're doing the same I think the key difference is I would have put it differently which is like and traditional science like the SDN controller exactly knows what the switch is going to do base what you're saying is you don't know what this box does goodbye our space you can be eight let's say log a lot of the reasoning that comes from that but like because you don't know what that state is you don't know what processing is happening so you have to be extra careful versus like knowing hey this is a forwarding switch all it does if i give it stayed yes this is how its behavior will change what you don't know that sugar sorry i would say the arguments are kind of related you you know what the switch is doing and that kind of you can use that as the input to do future computations because we don't know what the NF is doing we cannot do anything about what the NSA is we can only deal with management of that state oh yes okay cool thanks Lee 