 our last speaker for the day is pure Baldy um Pierre did his PhD at berkeley in 1990 at Caltech and I 86 in 80s oh I'm sorry I'm okay she's info is a he did a postdoc at UCSD is that right and then was it Caltech for many years before a start-up and then eventually joining us here at UC Irvine in 1999 I think thank you so we're going to shift gears a little bit and talk about natural sciences so let me ask you when the natural sciences start and guesses pietro 10,000 years ago i think it's a little bit more recent after the renaissance and Spencer forty percent of the speakers are Italian including myself I think I will say that it started with Galileo Galilei this man who alorica Lee built one of the first telescopes which had a magnification factor of 20 which he used to look at the planets and noted that the earth was moving was not static at the center of the universe and to thank him for these discoveries the Pope put him in jail or under house arrest but to Galileo did another computer had to do everything by hand no iPhone very tough times you can see it's not very happy on this picture now he couldn't order to telescope on Amazon now what has changed absolutely nothing you still cannot order telescoping on amazon but you have now the Hubble telescope in space and of course it's producing petabytes of data and now you do need computers to store the data and to analyze the data and this is true in a throttle me but of course you have similar things across all the natural sciences so you need statistics machine learning to analyze this large amounts of data indeed machine learning start this morning I heard 70s 60s etc 50s McClurkin pits but this is nonsense I think machine learning started a little bit earlier when the gas and others for the first time derived the equations for linear regression you have a set of points you want to make a model you try lines etc and every questions about machine learning is inside this picture you know how do you feed the model to the line are there alternative models how you do prediction on new points etc etc and I would argue that what we do today is pretty much the same thing a little bit on steroids because we're not always dealing with the lines in two dimensions but we are dealing with the higher-dimensional spaces and of course nonlinear problems and so we're fitting beloften we're trying to fit highly nonlinear surfaces in spaces that I've hundred to to a million dimensions and that's where deep net deep learning has been very successful for this audience I'm not going to say anything in details about commercial networks and performance on image natural very familiar with that but the point I want to make is that these techniques have been applied to a lot of areas in engineering whether it's computer vision robotics speech recognition and LP etc and what I'm going to show you our different sets of applications in the Natural Sciences in physics in chemistry and biology let me say a few things about theory before i go to up in that direction if you want to make a theory of deep learning you first have to start with shallow learning one layer if you have one layer all the units are independent from each other and then depending on the type of units you get linear regression logistic regression etc so that's very well understood what happens in a shallow network and then deep network starts when you have to two layers in fact there is one step before is when you have two layers but one layer is fixed the bottom layer is fixed you learn the top layer there are interesting sub K TSA's of that one is so called a extreme learning machine or that's a little bit what was also in a facial thought where the bottom layer is random so you get a random projection of the data and there is a lot of Tiryns about mountain projections followed by learning the top layer or you could fix the weights in the lower layer to be your training examples and that's exactly what you have in SVM's the learning algorithm is not gradient descent but when in terms of structure that's exactly what as VMS are so deep learning begins with two layers in fact you could have the top layer fixed and learn the lower layer that's already deep learning and that case is not it's not much simpler than if you leave the two layers free so let's go directly to that case and to simplify things a little bit let's make the output equal to the input that's called an outro encoder you can show that it's about us if you can solve the autonomic coder case you have sold the general case and so we need to understand our encoders and that's one of the reasons why I think that they are very important and you have two types compressive when the hidden layers is more narrow than the input layer or expansive if you look at the compressive auto encoders you can solve the linear case we sold it in 1988 that's what what animal was also mentioning in a talk and of course it does PCA but that's not the most interesting things about that what is interesting is the landscape of the linear compressive out to encoder which looks like that because this you have to think about these these points as manifolds but the key point is that there is one global minimum corresponding to PCA and then there is lots of solder points there is no true local minima in this problem there is an exponential number of solder points where the gradient is 0 associated with projection onto different subsets of the eigenvectors of the of the covariance matrix of the data and and this picture is actually a good representation of what we see today in the in the general case in the sense that in high dimensions even in the nonlinear case it's very rare that you find local true local minima because the gradient has to be zero in all direction the derivative is to be zero in all direction in addition you have to go up in all possible direction so that's a fairly rare event in a 1 million dimensional space on the other hand soldered points are much more common and those are a problem when you're doing gradient descent to some extent and even the linear case nobody thinks of doing gradient descent here because you can go directly there analytically but it's actually interesting and you get all kinds of interesting effects where you can get stuck in plateaus and you also see that you don't need depending on your objective in the linear case you can understand all the jumps between these this solder points how they relate to the eigenvalues of the covariance matrix and so you can see that you don't have to get to the global minimum to have good performance right any solder points below a certain level will be enough if if for a given level of performance and that's also what you see in deep nonlinear networks that's the linear case you want to do a nonlinear compressive outer encoders you have the only case that has been solved that I know of is the boolean case in the bull and unrestricted case you get clustering for the Hamming distance it's NP complete etc you combine these two together you start understanding what happens if you put sigmoidal neural networks at the beginning of love learning everything is linear you do PCA as learning progresses you go towards the flat parts of the sigmoidal functions you start doing clustering in the PCA space expansive I'll encoders are different stories they're also very interesting that's where sparsity comes in but I won't say much about them here you can then start thinking about what happens when you go beyond two layers I won't say anything but I want to say two things about the optimality of back propagation and I will say something about recurrent networks which will be important for the applications so back propagation is optimal and in some partial way answers the quote answer want the second question that taught me at this morning i'm going to show you that in some sense back propagation is optimal in the following way so imagine you have a deep feed-forward network that you're trying to train and you want to find optimal weights where the gradient the derivative of the error is 0 so imagine you're training this layer if you write the critical equations you write that the gradient is equal to 0 you're going to get some overall training example of something that is the product of the the activity of these neurons times the back propagated error and so in that equation when you solve it for the optimal weights in the end in this layer the way it's have to depend not only on the input of course but also on the targets and in fact on all the weights that are above above it right so if you want to find optimal weight you have to propagate information about the targets and all the way it's evolved back to this layer there is no other way so there has to be a channel that conveys this information let's say information about the target back down to the weights if you want to get to an optimal point so you you know in a physical implementation it's very interesting to think about what the channel could be etc you can think about the biology but regardless if there is such a channel we can think about the capacity of the channel and by capacity I mean how many bits are you sending back from the targets to the deep weights and how many operation do you need to do that and there are many other algorithms besides back propagation you could think of for instance you could apply the definition of the river is right perturb oh wait by epsilon look at the effect on the error and then takes the difference divided by the perturbation that's another way of computing derivatives right and you can think about many other algorithms but to make a long story short you can see that back propagation is optimal because if you look at the number of bits it sends back / weights let's say that you are in double precision so you're sending d beats back okay above the gradient in a high dimensional space you don't worry about the s in exaggerated sense and square we cannot even go there so back propagation gives you the full information about the gradient and the number of operation it takes is about one per weight if you think about it and all the other other algorithms you that are known that I can think of they are completely down here in this in this lower part of this of this of this two-dimensional plot so back propagation sits here it's optimal both in terms of the number of bits and the number of operation it takes to send the information back and so it's unlikely that you can do much better than that up to the constancy etc the other point I want to make is that you know people complain about deep learning being black box and that's very enough and you can do neurophysiology on deep networks but if some some amount of opacity is inevitable because you are fitting these I dimensional surface let's say 1 million variable in only near the surface that process has to be opaque we're not used with our visual system to see or to understand nonlinear surfaces in 1,000,000 dimensions right furthermore you have to understand that neural networks are completely different really different style of computation from what you have in your computer in your computer you store your training set as different different addresses you have your your input and your target the neural network is training that it takes the trainees the training set and shatters it entirely across all the weights every way it's in the neural in a neural network has a little trace of every training examples and that process has to be opaque all right so let me now say well as thing about theoretical things which has to do with the design of recurrent networks for applications then I'll move to applications so in applications you have two kinds of problems one where you have inputs of fixed size for instance images or maybe you have a hundred sensors and all the inputs are vectors of is 100 your 100 measurements and then other problems where the input is highly variable in size and instructors so this could be molecules it could be sequences it could be text in natural language processing you could have trees parse trees phylogenetic trees extra etc so in these problems where the size is variable and where there is structure you have to use it's not natural to use feed-forward networks it's not it's not the right structure so those are the problems where you have to use recurrent recursive networks and I'm going to argue that you have basically two different families of approaches for designing these networks which then I will illustrate on some of the problems and these two approaches are the inner approach and the outer approach so the email approach is is an approach where you take your data you need to have directed a cyclic graph and basically you use neural networks to crawl it to troll the edges of this graph so you're inside the graph the other approach builds new metrics in a direction orthogonal to your input graphs to your data graphs in a different in a different direction and un tries in essentially to fold the initial graph in order to obtain the final answers final answer this sounds a little bit strange maybe for some of you but let me illustrate on a simple example so imagine you are dealing with sequences standard model for sequences would be an hmm as a directed acyclic graph as a Bayesian network and hmm would look like this you can rewrite this using your metrics in fact you can put a neural network here for the transition between the hidden states and a neural network here for the production of the of the output and you can do weight sharing across all these production edges as well as weight sharing along all the transition edges so this this system can be rewritten in terms of two recursive neural networks and and and that can be used to to model the sequences so that's the inner approach because I am building my neural networks inside the graph my data graph here the hmm and using that to produce whatever prediction or interested in if you want to do another approach on a sequence problem you have your sequence here and you're building networks neural networks in the other direction so you have in the first layer maybe neural networks that are looking at a certain window of the sequence etc you can do weight sharing horizonte in this layer and then repeat the operation a different layers so that the ire layers are capable of looking at you know long longer longer dependencies in your initial sequences so the application will do the same thing but say for molecules or for other objects so let me start with applications in physics there are tons of interesting up problems where we're deep learning could be applied in physics from the very small scale subatomic scale to the very large scale and what is very interesting to me is that you know some problems the two scales come together for instance the problem of dark matter we know that eighty-five percent of the universe is made of dark matter and we get no idea of what dark matter is we don't know what the particles are of dark matter etc you detected by its a gravitational effect but you can try to study that matter not only at the cosmological scale but at the subatomic scale by smashing particles in in colliders and trying to to produce dark matter in the Indus collision so that's these the CERN in Switzerland you have this Collider which is very complex instrument that Galileo could not have dreamt of which is basically a very long tunnel 17 miles I think also underground 100 meter Underground's and in this tunnel you can run protons at very high speed close to the speed of light and smash them together and around the point of the of collision you have these very complex detectors which has itself a hundred millions of elementary detectors together which are getting a signals from this dis collisions and from that you are trying to infer which particles were produced around the collision point etc etc and these collisions occur very very fast on the nanosecond scale and so this instrument literally produces one petabyte of data per second most of it has been has to be thrown away by the way because you cannot store data that comes at you at one petabyte per second this is just a is another schematic view of the collider which has different parts different you know some subsystem I won't go into the details but there is the muon detector here there is the tracker there is the calorimeter etc if you again you can see the scale of the system your favorite presidential candidate is sitting here Donald Trump and that that's the kind of objects that were interested in and i'll show you examples of the applications of deep learning to the signals that that you obtain in this industry tactus so this example the first three examples are what you get at the level of the color calorimeter which is you know not at the beginning of the processing of the collision but but further down where basically what you're getting at the momentum of the various particle of the collision and from this momentous oh this is a vector of fixed size that you get at the level of the calorimeter these first three examples and from this vector of fixed size which sizes which which has the information about the momenta you want to do a classification was the Higgs boson produced in that collision or not for example or was a supersymmetric particle produce or not etc right there is tons of classification problems like that you can go one step before the calorimeter so there is a subsystem called the tracker where the data really is in form of jets or tracks of particles along the cylinder and so there is the problems at that level where you want to do to classify those chats and their substructures etc so that's where you get data in general that has a variable size because in a collision you can have a different number of tracks need different number of vertices and so your input is a set of vectors that has a variable variable size so let me show you what we do on problems like the first three problems this is the calorimeter it's fixed size so we just run standard feed-forward neural networks in this case sometimes we have in addition to the row features the moment of the particle we have I level features that are used by physicist which are just you know mathematical formulas that you can apply and you can use the row feet the row features or the human derive features you can combine etc in the classification we have very good simulators for the corresponding collision so these are fine line diagrams for instance in the case of the production of the Higgs boson versus the background these are the simulators that physicists have built that you can use to to produce additional data and so in this particular example we we had the training set with over ten million examples this is the Instagram of some of the features all little details that are relatively and interesting this is the details of one of the best architecture we found in terms of layers and learning rates etc but bottom line at the end of the day this is what what matters is that by using these deep learning methods were able to improve the AUC by eight percent over the previous methods that the physicists were using including things like BDT boost the decision trees they draw the ROC curves the other way around but it's it's really an hour seekers so you can see in this case for instance that neural networks with the high level feature without the high level features they do about the same so we're able to get rid of the in this particular example of the high level features that the physicists like to use and then the performance of several other algorithms so we've done this a few times we have done also compression of these neural networks into shallow neural networks to speed them up using some trick called dark knowledge which is now well-known you basically trained a deep neural network for classification for example and then you use the analog values of the deep neural matrix as the target for training a shallow neural network the idea being that the analog values of the deep network contain additional knowledge so-called dark knowledge that is not present in the binary labels or of your of your training data there is really no reason to do it for the network's I i showed you either than for having a paper with the title that combines dark knowledge with dark matter but but but again there are several steps in these very complex detectors I show you some example here in the calorimeter we're not working on the tracker where you have this variable size set data but there is something called the trigger which is we're really at the beginning where most of the data is thrown away and there is room for applying machine learning methods and they have to be very fast of course because of the speed at which this data is produced but but there is potential for having compressed neural networks there also and there are other problems in the new muon detector I'm not going to go there but so lots of things that can be down there and of course there are other problems in in cosmology and astronomy you want to for instance recognize galaxies versus quasars you want to with the student working on detecting quenched galaxies the quenching of galaxies some galaxies are still producing stars some some some galaxies I've stopped producing stars you want to detect this from from their images so a lot of opportunities there let me switch rapidly to chemistry you can look at small molecules in organic chemistry's these are example the 20 natural amino acids so these are molecule derive let's say less than 50 items mostly organic so carbon nitrogen oxygen hydrogen of course you can have some some sulfur some bromine some other things there but by and large those are the components and the first question is are big is the space of small molecules well if you go into the existing databases and you look at all the molecules that have been synthesized or that we have recognized in biological systems you get something on the order of 50 million different small molecules but of course there is a virtual space of all the small molecules that could be made and that's really astronomical estimates are on in the range of ten to the 60 but this number you know I could put 10 to the sanity and nobody would complain it's just it's bigger than the total number of atoms in the universe etc so there is very large number of virtual molecules that chemists could synthesize if you chose one and you say I really need to make that one you know chemists probably can make it the problem is to trouble this space and to identify molecules that are interesting say as drugs or for other applications so as a very small step in the direction you want to develop things that can look at molecules and predict their properties physical chemical biological you know is this molecule soluble in water what happens if I eat it is it toxic will will it make me euphoric etc right so you need training sets and you need representations good news molecules we have very different many ways of representing them so you can play with whichever techniques you lie you like you're familiar with these graphs of course they have 3d structures so i could give you XYZ coordinates of all the atoms there is a language if you like texts called smiles that allows you to represent molecules it's very easy to learn and then there are fingerprints which are fixed like vectors of zero and one we're the ones tells you whether certain substructure is present or not in the molecule so you can apply feed-forward networks to this or you can use you know recursive networks and some of the other representations we heard about and Sean Lowe short long term memory units for instance you could try to apply those techniques to smile strings i'm going to show you an application of the inner approach to the graph of the molecules now remember for the inner approach you need a dog you need the graph to be directed a cyclic and these graphs sometimes of cycles but both even more so they are not there is no natural way of orienting the edges of such a molecules so what we do actually we take all possible orientation so you see a molecule here we in we take as many copies as we need and then we choose in each copy a different node and we re-enter all the edges towards that node so you can show in that way that you get all kind old possible essentially all possible directed cyclic orientations and then we have neural metrics that are crawling each one of these molecules along the directed edges so each neural networks basically takes as input vectors associated with the vertices that contain information about the autumn type etc produce an output and then it's fed to the next level etc you can collect all these results from all these networks here and then pass them in a final multi-layer Network that produced the prediction in this case of whether the molecule is soluble or not all this is because the graphs that are cyclic you can back propagate from the top all the way down in all these networks and of course the networks that are crawling all these molecules are shared so you keep the number of parameters very reasonable and that works well if you think about the outer approach you want to see how is the outer approach on molecule the molecules there was a paper of the last nips doing exactly that from from a group at Harvard here's a molecule you're just going to stack neural networks on top in the other direction perpendicular to the molecule so you have a neural network here that may look at this node plus maybe the two neighbors and looking at the fact that these are all carbons and passing the information to some output etc and you can iterate this over several layers and at the top again produce a prediction of solubility or toxicity or whatever you're interested in and back propagate second class of important problems in chemistry is the prediction of chemical reactions things like this you can try to approach this problem in different ways you can try quantum mechanics it doesn't work at all if you want to do things in a high-throughput mode it's way too slow and and brittle you can try to write a system of rules by hand I'll show you how it looks like but it's very tedious and not very scalable you could try to learn the rules in the same way you learn a grammar or you can try to do a one other approach that I'll show you using deep learning so writing rules which there is a language called smirks that allow you to write down reactions it's very much like the smiles language that I showed you before it's a variation on this mild language these numbers that you see are just a atom numbering so you want to truck if you have a carbon number one on the left you want to track it on the on the right sew them together with Jonathan Chen we built a system called reaction exported as 800 rules that works reasonably well for covering undergraduate chemistry and it has been turned into an interactive system for education which is distributed by x y so that works well but it covers only undergraduate chemistry and it's very it's not scalable because these are the set for instance of reactions that you have to write down to cover all the cases of a particular class small class of reactions it's it's not very readable for humans to begin with and then every time you add a new rule you have to check that it doesn't break any of the previous rule and so it's very it's impossible to to really scale it up without some tricks so what's the alternative that were using now it's to look at the way chemists think about reactions so this is a reaction it's going the odd row bromination of an alkene and alkyne a small molecule with a double bond that is here this is hbr which react with this molecule and produces this product right the way if you if you ask chemist you know how this reaction happens well they will tell you that first hbr gives you h plus BR minus and then the bromine attacks the double bond and you get this but at the end of the bottom line is that a reaction like this is a sequence of elementary reactions and each elementary reaction is just the movement of an electron from a source to a sink so obviously what we're going to do is to look at the reactants in a reaction find all the sources find all the sinks and then we pair them the problem is that you're going if you have 10 sources and then sinks you're going to get a hundred pairs and then you have the ranking problem but that's exactly what search engine do right so we can use machine learning to do the ranking for that we use the siamese network where we enter sourcing pairs on the right sourcing pair on the left and the network has to choose left or right which one is more favorable and we train the system and new system is called reaction predictor and it's working quite well not yet at the level of a human expert but we we keep training it and hopefully in time it's going to to get better and better so applications in biology there are tons of them this is the telescope for biology these days it's a DNA sequencer you can sequence your genome in in one afternoon or less for four thousand dollars or less and you get something that looks like that you could run deep networks to try to predict the effect of mutations and all kinds of things the problems we we have worked on for quite a long time is the problem of predicting the structure of proteins which is a complicated and messy problem it has many many subproblems you can predict secondary structure where are the Alpha Ellis's the strands the coils in the structure for instance you can predict the 3d structure of the backbone of the side chains etc the issue is that the structure of the protein is invariant under translation and rotation so you have to factor that in your pipeline so the way we do it we start from the sequence we predict the secondary structure and a few other features that are found along the sequence and that there is a key step which is the prediction of the contact map which is a representation of the 3d structure that doesn't depend on rotation and translation and basically it's the matrix of 0 and once and you put a one in position i J if and only if amino acid I and J in the sequence are close to each other in the 3d structure if you have a correct contact map it's relatively easy to derive the 3d coordinates of the protein up to mirror images so the third step is solved if we look at the first step that's prediction of secondary structure again we can use recursive neural networks you can think of it as a translation problem this is the representation in terms of an input output hmm in fact input output bi-directional hmm if you like graphical models if you reflect this with recursive your metrics you get three neural networks one for the output one for the backward the hidden variable and one for the forward hidden variable if you unfold it in space or in time this is how it looks like these are the networks for the forward propagations these are for the networks for the outputs for the backward propagations etc you train these and this is our performance has been improving over time on secondary structure prediction some of the initial work was done by true and fast man sixty percent accuracy in the in in in the 70s karingson ascii were the first to apply a neural network to this problem they got sixty four percent in the 80s etc etc and today's we are about ninety-five percent correct prediction on secondary structure which means the the doctrine and that particular problem is essentially solved because we know you cannot get to a hundred percent for all kinds of reasons that I don't have time to go into but the first so the first step of the pipeline is assault the last step of the pipeline is solved really the big problem remaining problems still open is the prediction of contract maps we are again using the same approach using recursive networks this time in two dimensions so it looks something like this you have an input plane for Eden planes and an output plane the output that position IJ should be the probability that there is a contract between position I and J and if you look at one Colin in that gigantic Bayesian network you have an output that depends on the for hidden vector in the foreign plains below it and on the input if I go into one of the hidden plane let's say the north east or north northwest plane he didn't plane the vector that position depends on the two neighbors on the nearest neighbors on the lattice and also on the input you rewrite this as recursive your metrics you get five recursive neural networks one for the output one for the hidden States and so the output is a function parameterize by a neural network of the input and the foreign States etc etc so it's a fairly heavy system but you can train it and i'll show you how it does but that that's the inner approach for the for doing contact my prediction you can also do an outer approach which means you think of your plane and you're growing your matrix perpendicular to the plane so you have a stack of neural networks trying to predict the the contact map or trying to fold the protein in different stages if you want you don't have targets of the of the structure as it falls we don't have many movies of proteins as they fall but it doesn't matter because we have the final contract map in the PDB database so we are final targets here we can back propagate through the entire stack how well this method do where the recent cast experiment which is one of the benchmark experiment for protein structure prediction in the difficult contact my prediction category deep learning was doing quite well the top two methods were deep learning methods the inner in particular the the inner approach but I think you can do the same thing with with the yes the same thing with the outer approach but so that's the good news the bad news is that the accuracy that you get is still not sufficient for solving you know the structure of arbitrary proteins the accuracy we are reaching now this is an older result but right now we have an accuracy of about thirty percent on the contacts and of course you would want the number to go significantly out other applications that I don't have time to go through the student working on circadian rhythms or we get the expression levels of of one gene taken at multiple points in time let's say every four hours during the day and you want to decide whether the gene is periodic whether the level of expression of the gene is oscillating in a periodic way and we're using deep neural networks to do that the interesting thing for this problem is that we can train it entirely on synthetic data and that's something that I've seen happening in a few other areas in machine learning where synthetic data becomes extremely useful so you just generates all kinds of different signals and then you sample them every two hours or every four hours and you get these data sets with millions of examples that you can use to train the systems of course we also have biological data and we can test them on biological data and see how well they are out up to you know understanding whether a gene is behaving in a periodic way and of course we can extract the period the phase the amplitude and all those things these are just our C curves comparing this method to two other methods I did make a contrast between engineering applications and natural sciences but of course there is no boundary and we have problems in biology in medicine where we are using you know feed for ordinary conversation your metric for computer vision this is a problem where we're looking at two x-ray images of the breast mammograms where the goal the primary goal is to detect whether there is cancer or not but in fact in these images you also get a sense of the vasculature in the breast and you get a sense of the degree of calcification of in the vasculature and of course if there is a high level of calcification it is a sign of heart disease so as a by-product if we could automate these analyses and predict how much calcification there is we could alert patient we have a high degree of calcification and have them undergo additional tests and so we're using 11 in my group is using deep networks trained on little patches of this image to detect the vessels and to you know measure the degree of calcification and this is how well it is performing against the the expert and so far it's doing quite well and the next step for us is to increase our database of images another example of the same thing is a is a high-throughput microfluidic system developed here at UCI where you can essentially grow vessels in little chambers and then these are images taken from microscopes and you want for instance to be able to detect whether the vasculature as a healthy confirmation or not and again this can be done by by neural network this is Kevin passion the group who is doing that and in addition in these little chambers you can inject cancer cells so the blue are cancer cells which are growing near the vessels typically and you can inject drugs and monitor all these happens over x ideally it's going to be high throughput and of course you're going to have deep learning detect where are the cancer cells are the growing or not etc all this will be very helpful one once the system is entirely built I am completely out of time so let me just conclude by saying that deep learning is important useful it's hardly a new idea it goes back to the 60s or the 70s in some way I've shown you my applications in of deep learning in the natural scientists scientists I think there are many interesting challenges and problems there and I hope I gave you a sense that it's also interesting not only I mean it's interesting from a machine learning point of view because some of these problems have forced us to create new architectures and new algorithms and at least for me most of all you you get to deal with the problems that are incredibly beautiful for instance this is I want to leave you with a picture of the early baby universe 2 billion years after the Big Bang it's amazing that we can get such a picture where what you see in red this is a temperature picture take it taken from the background cosmic radiation the red parts are the beginning of the galaxies before they were galaxies in the universe so thank you very much and if there are any questions always questions so with deep learning in the Biosciences I imagine especially in high-throughput regimes we're in a situation where we're have a lot of tolerance for making the wrong prediction making a false positives can you discuss how training deep Nets under such domain changes as opposed to maximally hood no we can discuss this over drinks if you want but I'm not going to discuss this now it is a great talk you connected several threads so including physics chemistry and biology I would like some insights on how hyper perimeter tuning is done across these different fields for deep learning I get the dirty laundry question it's yet the same as everybody does you know it's a combination of intuition of spending a lot of time and tuning things there are some programs like spermine that can help you in some cases but there is the it's an area that is early has not converged so hyper parameter tuning is still a messy business also so you talked about applying classification regression to data sets that have been already collected but what about experiment design and automated hypothesis generation and i truly automated scientists ideally in a closed loop system so people I've tried there's a little bit of beginning of that people have tried robotic systems for testing chemicals etc in a you know sort of automated fully automated scientist way where you choose your next molecule to tests etc etc I can't say that these systems are working well or you know widely widely use so it's definitely future direction but I don't think we're there I think it's both I think we're not there last question it's a very interesting talk I think there's one kind of difference between the AI application of deep learning and natural science is that a I like vision speech recognition they have gold standard data sets but for natural science we kind of don't have that many but a people lots of people pointed out that a good well documented data says it's very key ingredients for deeper into successful so do you think to make also standardized data sets in Natural Sciences also important note so I agree that data is absolutely essential and there is a lot of variability in the amount of data you have across sciences and across problems so there are areas where you have a lot of data and it's well labeled right in part in the high energy physics those the simulators are very good so you can produce 100 million examples if you want of the Higgs boson collisions versus non expose on collisions right in other areas you are very few data points so it's highly variable let's think Pierre again so I'd also like to thank Pierre and also pork for all of their work in organizing the symposium and thank all of you guys and our speakers for coming and giving such good talks there's a wine and cheese reception now and as well there are posters outside if you didn't see the during lunch you 