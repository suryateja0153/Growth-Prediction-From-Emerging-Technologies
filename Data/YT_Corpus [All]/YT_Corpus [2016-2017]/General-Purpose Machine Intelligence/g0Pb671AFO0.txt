 how current technological developments are alter the balance of society security and beyond thank you so much the Rockies thank you so much Georgia I'm so honored to be invited to this and I'm delighted that you're organizing this I also want to thank you an equi European Union and everybody else who's made this possible we just heard about what our organization the future life Institute is perhaps best known for so far this 7 million dollar research program that we've just launched but before delving into details about that let me take a step back and say a few words about technology in general our organization consists of a lot of thinkers who love technology but who as Cindy Smith very very eloquently put it here earlier feel that technology is something that both can empower and do fantastic good in the world and at the same time it gives us new power to screw up an even grander ways than before so we feel we want to do everything we can now to make sure the technology gets used for good if we look at not very powerful technological inventions like fire for instance we used a strategy of learning from mistakes we screwed up a bunch of times and then we vented the fire extinguisher but with more powerful technologies nuclear weapons synthetic biology artificial intelligence etcetera we don't want to learn from mistakes we want to get things right the first time because that might be the only time that we have right and the way I think about this is to trade a great future for Humanity we want to win this race this race between the growing power of technology and the growing wisdom with which we manage the technology by investing more in this wisdom we're gonna hear more in this session from Pierre and Daniel and Nick about nuclear and bio and an AI but let me start by talking just a little bit about nuclear weapons because even though I want to end up with talking about AI I feel that while we celebrate our successes here in the National Action Plans and CBR M it's a very important at the same time highlight our failure so far to learn from them when we take on new more powerful technologies so we don't repeat past mistakes and I think that nuclear weapons is a great case study of in a inadequate risk management why do I say inadequate since we still haven't had a global nuclear war well let me just ask you this question which one of these two people is more famous and let me ask you a follow-up question which one of these two people should we thank for us all being alive here today because he single-handedly stopped the Soviet nuclear attack during the Cuban Missile Crisis I'll give you just one hint he wasn't Canadian so that always says something about how little attention we as a species pay sometimes really important issues and moreover I would say the lesson that we should draw from this is that that you know relying on luck there's a really poor long-term strategy the issue with let's see the arkhipov was just one out of a hair-raising the long string of near misses with global thermonuclear war and although we've mostly focused so far about nuclear threats from terrorism and crime we must remember that there have been also a lot of close calls or we almost had an all-out nuclear war between superpowers and even if the chances as low as 2% per year that that happens by mistake you know the probability that we're gonna screw up and within centuries is virtually a hundred percent so you would love we need to do better than just hope for luck in the long term if you play Russian roulette long enough we all know how it ends the second lesson I think we can learn from from the nuclear case study here is that it's really important to understand risks in advance before you fully build out the technology and I feel that we epically failed with nuclear weapons here I feel personally guilty about this because I'm a physics professor this was our fault partly as physicists so let's look at the quickly of the fact when nuclear weapons were first built the the decision-makers in scientists generally thought that the worst the main risk was that you would literally get blown up by it and people had these risk assessments that if things weren't really really bad maybe we would kill 300 million people or something like that now we know that that's hopelessly naive and that this is not even the getting blown up by it isn't even the number one largest risk to worry about for example oops this is a photo from downtown Las Vegas in the 60s you see the mushroom shaped cloud in the background that's how close it was to downtown because people have totally underestimated the dangers of radioactive fallout and acknowledging that by the US government has paid out more than 2 billion dollars in damages to settle these down winter cases and there have been more people who were killed by fallout from these peacetime nuclear tests than who died in Hiroshima Nagasaki combined but that's also not the number one risk even though it was a big oopsie in the 60s it was realized that if you set up one single hydrogen bomb 400 kilometers up above Earth's surface you can create an electromagnetic pulse impends of thousands of volts across much the whole confident potentially this permanently this disabling electronics cars cellphones the power grid which can lead not only to catastrophic infrastructure meltdown but also if you have a long power failure together with all these thousands of nuclear devastated cities then there are additional oopsies people hadn't thought about for example if you if you actually have a long-lasting power failure in a nuclear power plant you know what happened in Fukushima well if you don't keep the pumps on that circulate the cool the the water the covers these have spent fuel rods and pools like this one it boils off within the matter of weeks then the Chaconne ium cladding on the fuel rods catches fire and then you get a super chernobyl and you you could get that basically all of these fuel pools there are 300 of them here's I only draw draw drew little wind plumes around f5 of them but you can imagine if you do that around all of them it's just further adding to the misery I'm highlighting this at the meta level just because these are things that people hadn't thought about for decades in decades while the technology while we build tens of thousands of these weapons and yet and we still haven't talked about even the worst risk that's been discovered so far right now if we if we were to actually use a large fraction of the 16,000 nuclear weapons currently exist many of which are on hair-trigger alert so we could have if you think of the largest couple of thousand cities on earth you could have them all destroyed within an hour right now if we were if to do that then a nice-looking planet here would before too long look maybe potentially like this as at the suit from the fire storms rose high up in the atmosphere in a shout at earth and and this was not realized how serious this would be until the 80s about four four decades after Hiroshima Nagasaki and although this had a very powerful influence this research and push to help persuade Gorbachev and Reagan to negotiate the largest nuclear cuts ever done it turned out that unfortunately these calculations were rather inaccurate they were made on a supercomputer which was less powerful than this phone and it turned out that they were too optimistic but more modern calculations done by some of the world's leading climate modelers on real today's supercomputers show that this might last not two years but more like 10 years and for the following summer you can see here the temperature drops this makes global climate change seem like peanuts in comparison you see in an American breadbasket Ohio for example the temperatures are dropping by 20 degrees or so that's Celsius so 40 Fahrenheit my American friends and if you look in and saw in Russia China you get drops of like 30 35 Celsius what does that mean in plain English well we don't have to be agriculture experts to realize that if this turns into this when you're gonna harvest it's not so awesome for would supply and one doesn't have to make fancy calculations to realize that rather than maybe having a few hundred million people killed and as in some of the worst-case scenario is that people had in the sixties it's very plausible that the vast majority of all people on earth would starve to death and then succumb to and I make some other things that would follow no I'm not great and the thing to take away from all of this I think is simply that this is an example of where we built the technology first and realize the bunch what the main risks were way way later and as we get more and more powerful pack we want to learn from this mistake and really understand the threats first so that we can avoid them in the first place so in that optimistic spirit let's take a closer look here at the artificial intelligence this is a technology which has wonderful potential of course to do great things and we've also seen how it's been making a lot of progress maybe earlier the early progress in AI intended to be involved like when Garry Kasparov lost that IBM's deep blue for example good old-fashioned AI were some human programmers taught the machine to do something that it could then do way faster than Kasparov and beat him similar sort of old-fashioned approaches that are very successful now our self-driving cars and to some extent went when the Jeopardy deep blue but fisheye was won by IBM's deep blue however most of the most recent breakthroughs that have happened and there's been a real real C amazing series of breakthroughs just in the last five years were things that people thought would take decades to accomplish have not happened all of a sudden most of that stuff has involved a completely different approach where the Machine actually learns like a child it's it can take vast amounts of data and using the technique using deep learning another techniques that Nick Bostrom will tell you about can actually learn to do all sorts of things that the programmer has no idea even how it did it just like your children learn to speak your language and you don't even know exactly how they did it so look at this picture for example this is something that was science fiction five years ago that was done last year at Google you send in the pixels of this image and the computer says that's a group of young people playing a game of Frisbee you sent in this picture and the computer says oh that's the herd of elephants walking it across the dry grass field and we don't really know exactly how the computer did it because it just learned you know from massive amounts of data we'll hear more again from Nick about a little bit under the hood what's involved in this stuff I just want to talk about quickly two issues that this raises so first of all there is the there's a so there are two completely separate issues we should not complete there are near-term issues with technology that almost exists right now and then there are longer term things about if machines get smaller than us one day what might happen then Nick will tell you planning about the latter but in the very near term let's talk about our weapons a little bit so our organization recently lost an open letter on autonomous weapons where which was signed by over 20,000 people and and about 3,000 of the world's leading robotics and AI researchers and this open letter it was very much inspired by the Chemical Weapons Convention that we heard about from the Dead Sea konnikova and the biological weapons convention that we heard about from David face why did these people these researchers sign this well people who go into biology generally want to make the world better they don't go into it because they want to make bio weapons people who go into chemistry they want to make the world better not to create chem weapons and it's the same of course with these AI researchers they want to use AI the cure diseases to help alleviate poverty and do great things not to figure out new ways of mass murdering people or destabilizing the world and they feel concerned that their said their technology that they're building is being bastardized for really destabilizing uses what are some of these things that these people worry about well we for example today when when drones are used to kill people it's always a human who makes the decision and is removed controlling the drone from somewhere right but within years we will have the technology that we can completely eliminate the human from this just have the drone fly around for a few hours find somebody use its own a nice offer just like that elephant recognizing thing saying oh this is the person who looks like it's our enemy and then have it have it killed with no human-in-the-loop a big risk with these things is that if once any superpower goes ahead and mass produces this thing of course all other superpowers are going to want to do so to will have it arms race on our hands but this arms race these researchers feel will be very very different from the nuclear arms race because whereas it's very expensive to build nuclear weapons and very hard to get hold of the materials these weapons will be incredibly cheap you don't need any heart to obtain the materials a quadcopter costs few hundred bucks on amazon.com today software cost nothing once it's developed and you can have the potential that someone or the axe to grind for you know under a thousand dollars yeah me backup if super powers build this if you get the arms race going before long North Korea is gonna decide the bill lid and and so on and so forth and before long some country in the need of cash is gonna sell this on the black market and then and then all sorts of non-governmental organizations with an axe to grind will have them and these are perfect weapons for for example assassination you can program in v what your nemesis looks like have a thing fly for two hours identify the person kill him and then self-destruct no one knows who did it straight for ethnic cleansing you can program these things to look for a certain ethnic group only and kill them that they're very very cheap you can imagine what swarms of little bumblebee sized things with which just the recognizable face will find the eyeball which is the source softest part of the skull fires a little bullet there which is very cheap and you don't need a lot of power kills people if you have thousands of those you know it's it would completely transform warfare in a way that's very hard for foreign nations to defend against other than by creating a police state and for this reason there's a very quick broad consensus among the researchers in this field that this is an arms race you just shouldn't start that's the best way to stop it and I want to just conclude by pointing forward a little bit towards Oh Nick Bostrom who's gonna follow me here looking at super intelligence sometime in the future maybe in 40 years maybe in hundreds of years maybe never we'll see there's certainly the possibility that we might make machines that can do everything that we humans can do and then what well we our organization organized the first ever conference of AI researchers to talk not about how to make things smarter but to talk about this issue in particular how we can win this race and have wisdom keep pace with the technology it was in Puerto Rico in January of this year and it was actually really productive there was a very strong consensus that emerged that this is something we need to think about the goal of artificial intelligence should be redefined from having the goal of just trading pure undirected intelligence was creating beneficial intelligence and there was a very look we brainstormed up a very detailed action plan a list of research projects that should be done that would tackle embarrassing unanswered questions that we need to answer and we need to it might take that case to answer them so we should start researching now not the night before a bunch of guys on red bow you know switch on their thing and what was very exciting about this was that Elon Musk was present at the conference and he said look I hear you guys you want to do this research well let me give you ten million reasons to do it and with his donation we were able to launch a worldwide competition for research ideas we were overwhelmed by getting three hundred teams from around the world putting in wonderful proposals and it's very painful for the experts who had to review this to pick out winners but 37 teams have now been selected and I've started to work on this and it is I think you'll be very very exciting to keep following how this develops we view this as just a little bit of seed funding for the wisdom and I would encourage all of you with resources of governments and big organizations to remember that if we want to win the race between the power of technology and the wisdom of which you manage it we have the mindful of the fact that almost all the investments right now just go into making the doctoral there's almost no investment on the Wiz so if you're involved with any organization and get help a little bit ramp up this sort of research you would do humanity a wonderful service thank you a MUX thank you very much for this absolutely inspiring in wonderful presentation we are now at uniquely trying to invest in the wisdom side certainly and want to have you join us in this endeavor as well intellectually as well as in any other means and your presentation certainly proves how important it is to bring these issues to the discussions that the United Nations at the General Assembly and it at the level of decision-makers to have the awareness raised there and to see what we can do together to put all the stakeholders and all the particles together now yeah max you made a comment about about the drones and how easy it is to get them recently there was an incident at the Japanese Prime Minister's house when somebody flown a drone with a radioactive material on top of it and that got there obviously this was remotely controlled drone but but at the same time what happens when machines get smarter and this is something I'm going to address it to Nick Bostrom and I will introduce him he's a professor at the Faculty of philosophy at the Oxford University he's the founder of the found founding director of the future of humanity Institute the author of The New York Times bestseller super intelligence the book and he was named one of the foreign policy Magazine's top 100 global thinkers Nick please tell us what happens when machines get smart get smarter very much for the invitation and everybody who is contributing to making this meeting happen so just while we're getting the PowerPoint slide up I I can say something in general about so I want to sort of expand on some of the things that max were saying in his talk and and this grandiosity named a research center that that around the future of humanity institute we see ours as in the business of of trying to put a little acceleration on into the wisdom side of this race between wisdom and technology capability so to start without I want to introduce a concept that that we find is useful for organizing our thinking when you're really zooming out and looking at the human condition from a high altitude and look at the really big picture this concept of what I call an existential risk there's never been an existential catastrophe in all of human history and there will only ever have been either zero or one so an existential risk is one that imperils the survival of Earth originating intelligent type but I could permanently destroy our future so all the things that have gone wrong in human history all the wars and earthquakes and plagues from from this strange perspective or sort of like me ripples on the great pond of humanity when you taught up the total amount of suffering and happiness at the end of time these might not really register where as an existential risk would be important in that context so we define it as a risk that threatens the premature extinction of Earth originating telenet life or the permanent drastic destruction of its potential for desirable future development so this focus is our attention we have this very wide mandate the future of knowledge is today that could be anything pretty much but when you put on the lenses of focusing on existential risk like almost all the concerns that preoccupy the world's population fall away because they just aren't possible existential risks in there and a very small number of concerns remain which we can divide broadly into two categories so risks arising from nature and risks arising in some way from human activity one early finding of this field of existential risk studies is that all the really big existential risks certainly if we're talking about the timescale of 100 years or 200 years are in this anthropogenic category you can see this quite easily if you just reflect on the fact that the human species has been around for a long time we have survived earthquakes and fire storms and plagues and asteroid impacts for a hundred thousand years so it's just not very likely that any of those things will do a scene within the next century whereas we will in this century introduced entirely new phenomena new factors into the world so if they're going to be existential risks in the century they're most likely to come from these new things that we will do and and most of the possible ones fear have to do with anticipated future technologies and another way to look at this is is to consider this metaphor of a giant urn full of balls and and you can sort of see human history as the process of reaching into this urn and extracting one ball after another these folds represent ideas technological discoveries the products of human creativity and throughout our tenure here on this planet we have extracted a great number of these balls and most of them have been good some of them have been mixed blessings none has been such that it has spelled our a disaster we might wonder what would it be like if there were one of these black balls in the urn is there some possible discovery some technology that could be invented such that it invariably spells the doom of the civilization that discovers it you could run a kind of counterfactual thought experiment and think back 100 years ago or eight years ago before nuclear weapons have been invented and you can ask yourself what would have happened if it had turned out that instead of requiring highly enriched uranium or plutonium like a really difficult processes to unleash the power of the atom what if it had been some simple way something like baking sand in your microwave oven or something like that all right so so now we know that you can't have a nuclear weapon by breaking sound in your microwave oven but before we did the relevant physics how could we have known how it would turn out like it could have turn out like that and in that scenario that might well have been the end of human civilization at that point because if anybody just by doing some simple thing that they can do in the kitchen could wheel the destructive power to kill millions then it might just be impossible to have cities and concentrate in population and so forth but nuclear energy turned out not to be a black ball but maybe gray ball instead so it looks like our strategy country is to continue to pull balls out of this urn and just hope that there isn't a black all in there because if there is we will eventually pull it out and then that will be the end of it we have a lot more ability to invent things than to uninvent things so so this is a general reason also for thinking that the biggest existential risks over the course of a century might be from possible future discoveries that we might make and I've put up a partial list here of some of the perhaps more likely candidates for areas where existential risks might emerge there are several things to notice about this there is also all of these technologies here have great potential for beneficial uses which and paradoxically is one of the factors that makes them go higher up on this list because it increases the likelihood that we will actually develop them if there were some technology whose only used was to cause the destruction of humanity then maybe we would have a greater likelihood of steering clear of that but if it's something that has wide beneficial impacts for health and environment and economy chances are we will eventually develop these another thing to notice about this this is that at the bottom there I have put in some unknowns so if you think again back a hundred years ago and consider what the answer would have been if we at that time would have asked what are the biggest existential risks over the next couple of centuries then none of the ones that we might now be tempted to put near the top of this list would have been mentioned I mean certainly not machine is heaviness I didn't have computers synthetic biology wasn't the concept nanotechnology was not a concept that might have worked some of our totalitarian tendencies but for the most part what now seems to be the biggest risks are ones that have only in recent decades popped up on the radar and there might yet be others that we haven't yet conceived of which is one reason why we think there is potentially a high value in doing this kind of research just in case we can find something else that we might be able to do something about so now let me transition to speak more specifically about possible concerns from the future of artificial intelligence at the very most basic level the the the point is this that intelligence is an extremely powerful thing it what makes the difference between the human species and and our in many respects very similar relatives the great apes that that share most of our biology and only in very recent evolutionary time has departed somewhat and and these small differences in our brains have resulted in all these vast differences in in our ability now to shape the future of the planet Earth it's our small increases in intelligence that have enabled us to develop this modern technology and so forth and it therefore seems plausible just just even at first sight that if if there ever were a time when machines became as much cleverer than we are as we are than other animals then that those machines could be a very powerful shaper of the future maybe they would be able to shape the future according to their preferences and then that this therefore it seems to be a topic that is worth transferring out of the domain of Hollywood movies in science fiction and kind of entertainment and into the arena where academic researchers can begin to think about it as a topic where the goal is not to have fun and be entertaining with where the goal is to develop like increasingly accurate beliefs and proposals so max was already mentioning some of the advancements that have been made some milestones that have been crossed if we look under the hood behind these applications then we see a great number of developments in algorithmic techniques that have occurred and pretty much all of these really only since you know in the living memory of a lot of people alive today I mean the computer is still quite young and so if we think about how far we have come in these past 70 years and it makes one realize that within the lifetime of us or our children that we might come perhaps all the way in addition to these advances in algorithmic design and architecture there have always been developments in in hardware and if you look at particular domains such as computing you find that roughly half of the improvements in performance have been due to computers getting faster and half due to better algorithms and and that as a rule of thumb seems to be true across the board both hardware and software are contributing roughly equally in recent years I think maybe the reason two or three four years there have been a new sense of excitement in the world of artificial intelligence a sense of having compound on stock that the field was kind of stagnating a little bit before but now particularly with developments in it's known as deep learning and some other techniques there is a sense of renewed progress a lot of exciting frontiers to explore also reflected in industry activity with some some high-profile acquisitions and a kind of war for talent among some of the large software companies of the world we find artificial intelligence already in wide application throughout the economy I'm going to read off the whole list but a lot of the inventions that were originated in artificial intelligence research laboratories we no longer tend to think of as artificial intelligence once they actually work they just become software this is sometimes frustrating i researchers that they don't kind of get credit for all the things that that have been accomplished but but but AI techniques are and why to produce already and that that list will continue to grow longer if we look for example at a game may I ask one particular area where it's easy to compare human and machine performance we find that machine Gnutella is already in many games perform as well as or better than human beings I think that the next big game were where computers will exceed us will probably be the game goal which is kind of the Asian equivalent to chess a big organ great complexity some challenges that that remain today is better methods for transfer learning this is the kind of technology we would need to be able to use insights that you learn from solving one problem and then apply them in a very different area and this is still something of a challenge that AI researchers working on concept learning more flexible reasoning with learned concepts as a just sort of symbolic tokens that don't mean anything long-range hierarchical planning reading and more complex system architectures like tea you might get a slightly different depending on which AI researcher you ask but but these are certainly some of the major outstanding challenges that stand between where we are now and replicating the full functionality of the human mind and learning ability and planning ability that makes the human mind so powerful so reflecting on these developments I think as Max said that it's very important to make a clear and emphatic distinction between the near-term and long-term both of these contexts have serious legitimate challenges and opportunities to think about but they're quite different so in the near term we have issues such as autonomous weapons next mentioned we have of course non autonomous applications of these you could have in many situations perhaps the human making the final decision by pressing a button but with a lot of AI assists image processing etc you have in a very different direction people are thinking about the impact of automation on the labor market and whether the problems with chronic unemployment that one is beginning to see in some countries have something to do with with that or whether it in fact has to do with complete the other things like offshoring of labor or an economic cycle but as machines become more capable this is likely to become a bigger issue surveillance and data mining of course cybersecurity self-driving cars have issues for regulators like exactly what will the legal frameworks be for allowing these on the road and and a bunch of other things and these issues are quite different from the issues that arise if we ask the question what happens if AI actually succeeds in its original mission which has all along B not just to create domain-specific applications little tools here and there but actually to do all the things that the human mind can do and that's obviously farther off but also the implications are much more profound so we did a survey of some of the the world's leading experts a couple of years ago on one of the questions we asked was by what year do you think that there is a 50% probability that human level machine intelligence will be achieved which we define for the purposes of this survey as the ability to perform most jobs at least as well as a normal adult so so real genuine human level machine intolerance and as you can see the median answer to that question was 2040 or 2050 depending on precisely which group of experts we asked and that estimate should be taken with a large amount of salt in that it's based purely on the subjective impressions of people expert in the field but there is really no science that enables us to predict with accuracy how long these kinds of developments will take it could have been much sooner or it could take a lot longer so I think instead of a particular year think of a probability distribution so smeared out over a wide range of possible arrival dates there is a a different question also about timing but which must be distinguished from the first so so far I asked about this kind of first era there on the horizontal axis time until take ups like how how long between now and human level machine intelligence there is a second question if we ever do reach that level how long between that point and until we have something that is radically super intelligent and you might be quite pessimistic or optimistic depending on how you look at it but you might think that it will take a long time before the field of artificial intelligence will actually reach human level I mean maybe you think that these opinions about the practitioners are biased maybe they want to believe that their field is really important and it will succeed maybe you think it will take a hundred years rather than fifty years or more you might nevertheless still think that if we ever do reach that level that the transition to super intelligence will then happen quickly and in fact that is my view that it would be harder to get from here to human level than to get from human level to 2 radical super intelligence one way to think about it is this and intuitively we'll have this notion of smart and dumb that maybe looks somewhat like this we think at one and we have like the village idiot completely hopeless mongrels everything and at the other end you have sort of your favorite scientific guru what Einstein or Ed Witten or something and these kind of define the extremes of human cognitive performance with regard to how difficult it will be for artificial intelligence to achieve a particular level of performance however I think that the picture will look more like this that we start at the left of this diagram with zero capability when we invent computers let's say zero artificial intelligence and then slowly over time the AI train moves along this track and after many many decades of really hard work by a lot of researchers perhaps eventually we reach mouse level artificial intelligence something that maybe can navigate a cluttered environment about as well as a mouse can and then after a lot more work I mean where we reach chimp level and after a lot more work beyond that we really really chariot level but I don't think that at that point the train will slow down I think it with a sushi past human will station the brain of the village idiot in the brain of Albert Einstein are almost exactly identical the same size same number of neurons more or less the same biology there is no particular reason to think that it will be much harder to match one than to much gathered so to wrap up so what I have argued and I recently wrote a book on this is that we then will confront this control problem which is the problem of assuming it could solve the intelligence problem like how can actually make machines in teller like how could it then ensure that these very intelligent machines will be safe and beneficial to humanity and I argue that this raises unique challenges technical technical challenges and foundational challenges that there are possible scenarios in which super intelligent systems become very powerful and for the reasons I alluded earlier like intelligence is a general purpose thing if you have nothing telling us you can invent all the other technologies you don't and and also as I described in the book there are these superficially plausible ways of solving the control problem ideas that immediately spring to people's mind that on closer examination turn out to fail and so there is this open currently unsolved problem of how to develop better control mechanisms that is more difficult because it will need to be solved before we actually have these fully intelligent systems by that time we already have a solution so so I'm very glad that people like Elon Musk are stepping into the breech here where there has been a complete funding vacuum until recently and and that some activity is beginning to happen and and I recommend that that we sort of accelerate this work of establishing a field of inquiry to do foundational and technical work on the control problem and recognize that as such as we stinked legitimate academic endeavor that some small number of the world's best brains should be working on them just as so many other things something started by academics that we should try to attract top mathematics and computer science talent into this new field that we should build strong research collaborations between the AI safety community and the AI development community of in industry in academia because ultimately the path to success is that whatever ideas for safety are developed also get implemented and both of these need to learn from one another rather than take up antagonize positions that in long range scenarios and planning we should consider super intelligence as a possibly important factor in shaping humanity's long-term future this does not commit one to thinking that this is just around the corner that we should hold our breath and be like super excited about every single announcement in the media but but if you're really thinking long term about humanity's future decades out then I think this is a legitimate thing to take into account and finally that it is important to integrate into this research community and into society's thinking about the long-term future of artificial intelligence that this is a unique technology that should be developed only for for the common good of all of humanity it's too big to just be thought of as something that will raise the profits of one firm a little bit or give one country a slight edge this is really a concern for all of us everybody in world if this is developed we'll share in the risks whether they like it or not and it also seems fear that everybody should stand to gain if things go well and have a slice in the upside thank you very much Nick thank you very much for this absolutely great presentation and and one of the recommendations what I would also add there is to bring these ideas to the policymakers to the international organizations and private sector and create some sort of a platform where where all of these segments all of the stakeholders could work together to ensure that yeah what we're going to do if AI succeeds and as your we see in your presentation basically we are leading or we are heading towards that ai is going to succeed and we have to be prepared about it so 