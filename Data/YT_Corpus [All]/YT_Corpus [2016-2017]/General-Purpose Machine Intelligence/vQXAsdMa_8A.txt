 I'm temazepam I'm the director of the Center for brains minds and machines which is a center between MIT and Harvard located in BCS in building 46 and I have the pleasure of hosting Demi's I don't need to say much about him if you look on Wikipedia or Financial Times as a very good caricature of damage and you can find him everywhere it was a chest child prodigy study computer science in Cambridge he started a couple or more of successful computer games companies then it became a neuroscience neuroscientist got a PhD at UCL in London and then I was lucky enough that I can put on my CV that he was a postdoc of mine for a brief period between 2009 and 2010 I think and then we saw each other a couple of times once he came to speak at one of the symposia for the MIT 150 birthday this was 2011 and we had one session of that symposium which was called brains minds and machine one session was titled the marketplace for intelligence and you spoke about a deep mind that you had just started and so deep mind is an amazing achievement damage managed to put together a company sell it to Google the company is also greater search laid out say the best one in AI these days with you know high-impact papers in nature and so on and achievements like like alphago winning against what is arguably the best player in the world lee sedol I was in Seoul for the last game at the 5th game and was exciting and historic and it's great where them is here kind of telling us about what went on and what was the background of it Demi's thanks so much thanks Tommy for that very generous introduction thank you all for coming it's great being back at MIT always love coming back here and seeing catching up with old friends so on today I'm gonna split my talk into two the first half of it is going I'm going to give you a kind of whirlwind overview of how we're approaching AI development at deep mind and the kind of philosophy behind our approaches and then the second half of the talk will be all about alphago and the sort of culmination of our work there and what we're going to go what we're going to do with it going forward so deep mind first of all it was founded in 2010 and we joined forces with Google in early part of 2014 so we've been there for just over two years now one of the ways we think about deep mind and one of ways I describe it is as a kind of Apollo program for AI Apollo program effort for AI currently we have more than 200 research scientists and engineers so it's a pretty large team now and we're growing all the time in so obviously there's a lot of work going on and I'm only going to be able to touch on a small fraction of it today so apart from experimenting on AI which is obviously the main purpose of deep mind at least half of my job and half of my time is spent on thinking about how to organize the endeavor of of science and what we try to do at the mind is try to create a optimal environment for research to flourishing and the way I mean that would be a whole talk in itself but just to sort of give you a one-line summary what we try to do is fuse the best from Silicon Valley startup culture with the best from academia and and so you know we've tried to combine the kind of blue sky thinking that you get and interdisciplinary research you get in the best academic places with the focus and energy and resources and pace of top startup I think this fusion has worked really well so our mission the some of you will have heard me state the way I kind of articulate that is in two steps so step one try and fundamentally solve intelligence and then if we were to do that I think step two kind of follows naturally try and use that technology to solve everything else certainly that's why I've always been obsessed with working on AI since I can remember because I truly believe that it's one of the most important things that any you know mankind could be working on and will end up being one of the most powerful technologies we ever invent so more prosaically what we're trying to do at the mind what we interested in doing is trying to build what we call general-purpose learning algorithms so the algorithms we create and develop a deep mind you know we're only interested in in algorithms that can learn automatically for themselves from raw inputs and war experience and they're not handcrafted or pre-programmed in any way the second important point is this idea of generality so the idea that a single set of algorithms or single system can operate out of the box across a wide range of tasks in fact this sort of connects with our operational definition of intelligence I know that's kind of a big debate and there isn't really a kind of consensus around what intelligence is but for a tional II we regard it as the ability to perform well across a wide range of tasks so we really emphasize this flexibility in generality so we call this type of AI artificial general intelligence internally a deep mind and the hallmark of this kind of AI is that it's flexible and adaptive and possibly you could argue inventive and I'm going to come back to that at the end once we covered alphago and the key thing about it is that it's it's built from the ground up to deal with the unexpected and to flexibly deal with things that it's never potentially seen before so by contrast obviously AI is a huge buzzword at the moment and is hugely popular both in academia and industry and but it's still a lot of the AI that we find around us all that's labeled AI is of this kind of narrow what I would call narrow and that's really software that's been handcrafted for a particular purpose and it's special case for that purpose and often the problem with that those kinds of systems is that they're hugely brittle as soon as the users interact with those systems in ways that the teams of programmers didn't expect then obviously they just catastrophically fail probably still the most famous example of that is of that kind of system is the blue and always see that was a hugely impressive engineering feat back in the late 90s when it be Gary Kasparov at chess but the blue you know it's arguable whether it was a really exhibited intelligence in the sense that it wasn't able to do anything else at all not even play strictly simpler games like tic-tac-toe it would have to be pre-programmed again from scratch with expert knowledge so the way we think about a AI and intelligence is actually through the prism of reinforcement learning and most of you'll be probably be familiar with reinforcement learning but I'm just going to cover it quickly here in this cartoon diagram for those of you who don't know what it is so you start off with an agent or an avatar it finds itself in some kind of environment trying to achieve a goal in that environment that environment can be obviously the real world in which case the agent will be a robot or it could be a virtual environment which is what we mostly use in which case it's a kind of avatar of some sort now the agent only interacts with the environment in two ways firstly it gets observations through its sensory apparatus and reward signals and we mostly use vision but we're looking to use other modalities pretty soon and the job of the agent system is kind of twofold firstly it's got to try and build as accurate a statistical model as it can of the environment out there based on these noisy incomplete observations that it's getting in real-time and what it's built the blessèd model it can then it has to decide what action to take from the set of actions that are available to it at that moment in time to best get it incrementally towards its goal so reinforcements only that's basically the essence of reinforcement learning and the diagram is very simple but of course this hides huge complexities and difficulties and challenges that would need to be solved to fully solve what's in this diagram but we know that if we could solve all the issues and challenges behind this framework then that would be enough for general intelligence human level general intelligence and we know that because many animal systems including humans usually enforcement learning as part of their learning apparatus in fact the dopamine neurons in the brain implement a form of TD learning so the second thing that we kind of committed to philosophically in terms of our approach at deep mind at the beginning was this idea of grounded cognition and this is the notion that a true thinking machine has to be grounded in a rich sensory motor reality but that doesn't mean it needs to be a physical robot as long as you're strict with the inputs you can use virtual worlds and and treat them these avatars and these agents in these virtual worlds like virtual robots in the sense that the only access they have to the game state is via their sensory apparatus so there's no cheating in terms of accessing the internal game code or game states or underlying the game we think if you treat games in that way then there can be the perfect platform for developing and testing AI algorithms and that's for many reasons firstly you can create unlimited training data there's no testing bias in the sense that I think one of the challenges of AI is actually creating the right benchmarks and very often this sort of turns out to be an afterthought for an AI lab to build the benchmarks and we think actually crafting the right benchmarks is just as difficult maybe even more difficult than coming up with the algorithms and games of course have been built for other purposes to entertain and challenge human players and they've built been built by games designers so they weren't built for testing AI programs so in that sense they're really independent in terms of a testing training ground for our AI ideas obviously you can run millions of agents in parallel and we do that on the Google cloud and most games have scores so this is a convenient way to incrementally measure your progress and improvement of your AI algorithms and I think that's very important when you're setting off on a very ambitious goal and mission like we have which you know may be multi decades it's important to have good incremental measures that you're going in the right direction so this kind of commitment then leads to this idea of end-to-end learning agents and this notion of going from starting with say raw pixels and going all the way to deciding on an action and we're deepmind women should in that entire stack of problems from perception to action and I think we've over the lot sort of five years that deep mines been going a pioneered this use of games for AI research and I seen many other research organizations now and industrial groups starting to use their use that use games themselves for their own AI development so I guess the first big breakthrough that we had a deep mind was really starting this field this new field of deep reinforcement learning and this is the idea of kind of combining deep learning with reinforcement learning and this allows reinforcement learning to really work at scale and tackle challenging problems until we sort of came up with this idea enforcer learning RL of course has been as a field has been going for more than thirty years but generally speaking up till then they've only been applied to sort of toy problems little grid world problems and nothing really challenging or impressive had been done with RL research so we wanted to kind of take that further and apply it to a really challenging domain so we picked initially we picked Atari 2600 platform which is really the first iconic games platform from the 80s and conveniently there's a nice emulator which we open-source emulator which we took and improved and then there are you know hundreds of different classic Atari games available on this emulator I'm just going to run you at one video in a second showing how the agent performs in in this Atari environment but before I do just to come just to sort of confirm with you what you're going to see the agents here only get the raw pixels as inputs so that the Atari screens are 200 by 150 pixels in size is about thirty thousand pixels per frame and the goal here is simply to maximize the score everything else is learned from scratch so the system is not told anything about the rules what they're controlling or even the fact that video streams you know pixels and video streams are next to each other are correlated in time it has to find all that structure for itself and then it's this notion again of generality one system be able to play all the different Atari games out of the box so we call this system dqn and we think it really is a kind of general Atari player so this is a little medley of the same system out of the box the same hyper parameters playing all these very different games very different rule sets very different objectives very different visuals out of the box with the same settings in the same architecture and you know performs better than top human players on more than half of the Atari games and since our nature paper we've now increased that to about 95 percent of the Atari games here's just any other boxing where it's the red box are here and it does a bit of sparring with the inbuilt AI and then eventually callers it and just racks up an infinite number of points so so if you want to know more about that work you can see our nature paper from last year and the actual code is freely available as well linked from the nature site so you can play around with the dqn algorithm yourselves so there's sort of two planks of our philosophy is grounded cognition and reinforcement learning a third sort of pillar if you like of our approach is the use of systems neuroscience and as a neuroscientist myself you know I think this is going to play a very important part of understanding what intelligence is and then trying to recreate that artificially but when I talk about neuroscience I really want to stress on talk about systems neuroscience and what we mean by that is really the algorithms the representations and the architectures the brain users rather than the actual low-level say synaptic details of how the brain how the neural substrate works so we're really talking about this kind of high-level this computational level if you like of how the brain functions now I haven't got time to really go into all the areas that we're sort of using neuroscience inspiration for but suffice to say it's some of the key areas that we're working on memory attention concepts planning navigation imagination all these areas that we're pushing hard on now to going beyond the work we did for Atari and actually the area of the brain that I studied for my PhD the hippocampus which is the the center part of the brain here in pink is actually implicated in many of these these sort of capabilities so it seems like perhaps the notion of creating an artificial hippocampus of some sort which has the mimics the functionality of the hippocampus might be a good plan so I haven't got time to go through all of these different areas then we'll the work we're doing here but I'll just touch on a couple of the most interesting ones so one big push that we have at the moment is adding memory to neural networks and what we want to do is add very large amounts of controllable memory so what we've done is created system which we had sort of dubbing the neural Turing machine and what effects really is is you take a classical computer you train a recurrent neural network on it from input-output examples and that recurrent neural network you can think of is like the CPU effectively and what we give this recurrent neural network is a huge memory store a kind of KNN memory store that it can add learn to access and control and this whole system is differentiable from end to end so you can learn so the recurrent neural network can learn what to do through gradient descent and really that is then all the components of a kind of von Neumann machine that you need except here it's all neural and it's all been learnt so that's why we call it the neural Turing machine because it has all the aspects you need for a true Turing machine so here's a little cartoon diagram of what the Turing machine does and you can think of this kind of input tape and then the CPU which is this recurrent your network that actually has LS TMS as part of it and then it's trying to produce the right output and then it has this huge memory store to the side that it can learn to read and write elements to vectors to now with this kind of system we can start moving towards symbolic reasoning using these kinds of neural systems which is really one of the big Holy Grails of what we want to do and of course there's a classic problem in AI many unsold kind of classic problems one of the problems we apply this your true machine too has been inspired by the shrewder loop class of problems which are these block worlds from the 70s and 80s and the idea here is to manipulate the blocks in some way and answer questions about the scene you know like put the red pyramid on the green cube or what's next to the blue square and and both manipulate this world and also answer question answer about it now we're not ready yet to Newton machines can't scale to the full complexity of this of the falls through to do problem but we had kind of cut it down to a 2d version blocks world version where we can kind of solve some quite interesting things so we call this kind of mini shrewd aloo it has aspects of Tower of Hanoi and other problems in it and the idea here is that you've got this little blocks world that you're looking sidon and all these different colored blocks and you're giving the stock you're given the start configuration here on the left-hand side and the goal configuration you want to reach and what the system can do is is lift one block from one column and put it down on the top of another column that's the only moves you're allowed to do and it gets trained through seeing many starting examples and end examples and doing trial and error with reinforcement learning and improving itself over time and then once it's done its training we then test it on new start positions and goal positions that's never seen before and it has to try and solve these problems in an optimal number of moves so I'm just going to run this little video which will show you going from that start position on the left to the to end up on the goal position I think this one's about 12 moves it's actually pretty hard tasks to do in an optimal number of movies it's really hard even for humans to do this and so now it's you know it's solving pretty interesting kind of logic puzzles also what we've been using your two machines to do recently is solve graph problems which was you all know kind of quite a general class of problems and we'll be publishing something something pretty pretty impressive I think in the later part of this year on this topic to add to our archive paper that we were really published last year now we're also experimenting a slice experiment with language as well and we've incorporated a kind of cut down version of language into these shrewdly tasks and here the neural true machine is reading a set of constraints that are given to it in code that you can see at the bottom of the screen so here each of the blocks are numbered and there are some constraints that you want to satisfy with the goal configuration so in this case block three should be down from block five four up from two one up from four and six down from three and so it reads it reads this in character by character remembers these instructions and then starts executing the actions and then it solves the puzzle and and this is the the end goal the end position that Soler satisfies all those constraints another thing we're moving to now is there's still challenges to overcome in Atari but we're also starting to move towards 3d environments so we've repurposed the quake three engine and added modifications to it we call it labyrinths and we're starting to tackle all kinds of navigation problems and interesting 3d vision problems within this kind of labyrinth like environment so I'll just roll the video of this agent finding its way through the 3d environment picking up these green apples which are rewarding and then trying to find its way to the exit point and again all of this behavior is learnt just through the only inputs or the pixel inputs and it has to learn how to control itself in this 3d environment and find its way around and build maps of the world so here for an agent like that we're starting to integrate some of these different things together deep reinforcement learning with memory and and 3d vision perception so as we take this forward we're kind of thinking is one of our goals over this next year is to kind of create a rat level AI so an AI agent that's capable of doing all the things a rat can do and you know rats are pretty smart so you could do quite a lot of things so we're looking at the rat literature actually for experimental ideas experimental tests that we can test our AI agent on so now I want to switch to alphago which is also part of these big pushers that we're doing into going beyond the Atari work so one of the reasons we we took on alphago is we wanted to see how well these neural network approaches could be meshed with planning approaches and go is really the perfect game to test that out with so this is the game of go for those of you who don't play this is what a board looks like it's 19 by 19 grid and there's two sides black and white taking turns and you can play your place your stone in your piece which is called a stone anywhere on an empty vertex on the board now the history go has got a long and storied tradition in Asia it's more than 3,000 years old confucius wrote about it we thought you know to 2,000 years ago and and he actually talked about go being one of the four arts you need to master to be a true scholar so it's really regarded in Asia up there with poetry in calligraphy and art forms there's 40 million players active players today and more than 2000 professionals who start going to go school when before they're teenagers from around the age of 8 9 or 10 they go to special go schools instead of normal schools and although there are only the rules of go are incredibly simple if I'm going to teach you how to play go in two slides in a minute it actually leads to profound complexity there one one way of just quickly illustrating that is that there are more than 10 to the power 170 possible board configurations so that's more than there are atoms in the universe by a large margin so the two rules are rule one the capture rule stones are captured when they have no free vertices around them and these free vertices are called liberties so let's take a look at a kind of position from a from a mid game of our early part of a go game and let's zoom in to the bottom right of the board to just illustrate this first rule so here you can see this white stone that's surrounded by the three black stones only has one remaining free vertex one remaining free of Liberty so if black was to play there it would totally surround that white stone and that white stone would be captured and removed from the board and actually big groups of stones can be captured in this way not just one at a time whole large groups can be captured if you surround all of their empty vertices that's the first rule the second rule is called the KO rule and that states that repeated ball position peated ball position is not allowed so let's imagine we're in this position now and it's white to play now white could capture that black stone by playing here and taking that black stone off the board so now it's black smooth and you might be wondering well can't black just capture back by putting replacing that stone and taking white so what happens if black was to play this and this is not allowed because if black was to play back there and remove the white stone now you'll see that this position we're in now is identical to the position we've started with so that's not allowed so that black move is not allowed black would have to play somewhere else first to break this symmetry and then can go back and recapture that stone and that's it those are the rules would go and the idea go is that you obviously want to take your opponent's pieces by surrounding it but actually the main thing you're trying to do is wall off parts of empty territory on the board and and then at the end of the game when both players pass they don't think they can improve their positions any further you count up the number of territory you've got and you add the prisoners that you've taken from your opponent and the person with the most points wins the game so the rules will go as simple but I'm you know it's pretty much the most profound and elegant game I think that mankind has ever devised and I say that as a chess player you know I think go is really the pinnacle of perfect information games it's nothing the most sort of complex game that so any humans have spent a significant amount of time mastering and play a very high professional level they and because of this huge complexity of go it's been an outstanding grand challenge for AI since you know for more than twenty years especially since the deep-blue match and the other interesting thing for us is that and I'm going to come back to this more in a minute that if you ask top go players they'll tell you that they use their rely on their intuition a lot to play go so go really requires both intuition and calculation to play well and we thought that mastering it therefore would involve combining kind of pattern recognition techniques with planning so wise go hard for computer to play well the the huge complexity means that brute force search is not tractable and really that breaks down into two main challenges firstly the search space is really huge there's a branching factor of more than 200 in an average position in go and the second point which is probably even bigger problem is that it was thought to be impossible to write an evaluation function to tell the computer system who is winning in a mid game position and without that evaluation function is very difficult to do efficient search so I'm just going to unpack these by comparing go to chess and you'll see the kind of difference so in chess in an average position there are about 20 move possible moves so the branching factor in chess is 20 in go by contrast as I just mentioned it's more like 200 so there's an order of magnitude larger branching factor plus go games tend to last two to three times longer than chess games the evaluation function so why is this so difficult for go well we still believe actually that it's impossible to handcraft a set of rules to tell the system who's which who's winning so you can't really create a kind of expert system for go for evaluating a go position and the reasons are there's no concept to materiality and go so in chess as a first approximation you can just count up the value of the pieces on each side and that'll tell you roughly who's winning there's no you can't do that and go because obviously all the pieces of the saying secondly go is a constructive game so you know the board starts completely empty and you build up the position move by move so if you're going to try and evaluate position in half way through or the beginning of the game it's very difficult because it involves a huge amount of prediction about what might happen in the future if you contrast that with chess which is a kind of destructive game you know all the pieces start on the board and actually the game gets simplified as you move towards the endgame the other issue would go is that it's very susceptible to local changes very small local changes so even moving one piece around out of this matter pieces can actually completely change the evaluation of the position so go is really primary game about intuition actually rather than calculation and because the possibility is so huge I think it's kind of at the limit of what humans can actually cope with and deal with and master and you know I've talked to a lot of top go players now and when you ask them about when they play a brilliant move why they played it they'll just tell you actually or quite often that it felt right and they'll use those words if you ask a chess grandmaster why they played a particular move they'll usually be able to tell you exactly the reasons behind that move you know I played this move because I was expecting this and if that happens they're not going to do this and they'll be able to kind of give you a very explicit plan of why that move was good and you can see that go definitely has a sort of history and tradition of of sort of being intuitive rather than then calculating because it has notions of things like the idea of a divine move and actually there are there are some famous games in history that get names and within those games there are famous moves and those moves are sometimes named as well and and you know if you talk to a top go player they dream about one day at one point in their career playing one of these kind of divine moves and move so profound you know it's almost as if it was divinely inspired and you can look that up online I hope there's some really interesting always about from the edo period in Japan of these kind of incredible games played in front of the Shogun and these divine moves being played ghost moves so how are we going to you know how did we decide to tackle this kind of intuitive aspect of go well we turned to deepen your networks and in fact what we did is we used actually to deepen your networks so I'm just going to take you through the training pipeline here we started off with human expert data that we downloaded about a hundred thousand games from internet go servers of strong amateurs playing each other and we first of all trained through supervised learning what we called a policy network and this new new deeply look neural network what it was trained to do was to mimic the human players so we gave it a position from one of those games and obviously we know what the human player played and we trained this network to predict and play the move the human player player played and after a whole bunch of training you know we can get pretty reasonably accurate and get to about 60% accuracy in terms of predicting the move that the human would play but obviously we don't want to just mimic how human players play especially not just amateur players we want to get better than the human players so this is where reinforcement learning comes in where we then iterate through self play this this policy network many millions of times against it playing against itself and incrementally improving the the weights in that network to slowly increase its win rate so after millions of games of self play this sort of new policy network has about an 80 percent win rate against the original supervisor learnt policy Network then we freeze that Network and we we play that Network against itself 30 million times and that generates our new kind of we call gold data set and we take a kind of position from each of those thirty million games and obviously we have the position and we also know the outcome of the game we know who finally want black or white and then with that much data we were finally able to crack the holy grail of creating an evaluation function so we created this second year Network the value Network which is a learned evaluation function so it learnt to take in board positions and try and actually predict who is winning and by how much so after all of this training which is a you know a lot of compute power and training on that we end up with finally with two neural networks the policy network which takes a board position coded as a trinary vector as an input and the output is a probability distribution over the likelihood of each of the moves in that position so the green bars here and the height of the green bars on the green board represent the kind of probability mass associated with each of the moves possible from that position and then the second Network is we get this value network here in pink and again you take the board position as an input but here the output of the network is just a single real number between 0 & 1 and that indicates where the white or black is winning and by how much so if it was 0 that means white would be completely winning and 1 black would be totally winning and 0.5 the position we've a call so we take those forwards but that's not the neural networks are not enough on their own we also need something to do the planning and for that we turn to Monte Carlo tree search and to stitch this all together and it uses the neural networks to make the search more efficient so I'm just going to show you how the search works here so imagine that we're in the middle of pondering what to do in a particular position and imagine that position is that the root node of this tree represented by the little mini go board here and perhaps we've done a few minutes or a few seconds of planning already so we've we've already looked at a few different moves represented by the other the sort of leaf nodes here and what you do is you've got two important numbers here Q is really the the current action value of the move the estimate of how good the move is and P is this sort of prior probability of the move from the policy Network in terms of how likely is a human would play that move and let's imagine we're following the most promising path at the moment that we found so far in the bold arrows here that are coming down and we end up at a node a position that we haven't looked at so far so what happens here is we expand the tree and we do that by first calling the policy network to find out which moves are most probable in this position so instead of having to look at 200 possible moves you know all the different possible moves in this position we just look at the top three or four that the policy network tells us are most likely and so that expands the tree there and then once we've expanded the tree we evaluate the desirability of that path in two ways one is that we call the value network and that gives us an instant estimate of the desirability of that position and we also do a second evaluation routine using Monte Carlo rollouts so we roll out you know maybe a few thousand games to the end of the games and then we back up the statistics of that back to this node and what we do what we found is that by combining these two evaluation strategies we can get a really accurate evaluation of what how desirable that position is and then and of course that's one of the parameters we experiment with is the mixing ratio between the rollouts or the rollers are telling us and what the value network is telling us and as we improved our forgo we trusted the value networks more and more so I think now you know with the lambda parameters about point eight in favor of trusting the value Network and we started when we started on this around last summer we got - it was about 0.5 so then once you have that you back the Q value up the tree and then once you've run out of time or you allocate a tie you basically pick the move that has the highest Q value associated with it so if we think about what these neural networks are doing then for us in terms of the search you can think of it in this way imagine that this is the search tree from the current position you know it's just it's totally intractable it's really huge what we do is we call the policy Network to really cut down the width of that search to narrow that down and the value network really cuts the depth of the search so instead of having to search all the way to the end of the game and collect you know millions of statistics like that to be even hot you know reasonably accurate we can truncate that depth search at any point we like and call the value network so once we built the alphago system it was time to evaluate how strong it wasn't tested out so the first thing we did was play it against the kind of commercially best available go programs out there that are the two best ones are crazy stone and Zen they've won all the recent computer go competitions the last few years and they've reached to about strong amateur level so in go that the ratings are you start off as this thing called kyu kyu and you go down in score as an amateur and then as you get better the strong amateur you get a down rating which goes from 1 down to about 6 or 7 Dan and then finally you can become professional and then the Dan ratings start again from 1 to 9 so really these programs were about the strength of a strong amateur strong club player in alphago did incredibly well against them so it won all in the 495 matches we tried it won all but one and it could do a 75% win rate against these other programs even when they were given a four move headstart which is huge in goes called a four stone handicap and this graph here that I'm showing you is is just the single machine version of alphago and it was even stronger on the distributed version and these rankings are quite subjective these go rankings so we actually created a numerical ranking ranking that's on the y-axis on the left-hand side which is based on like chessy low ratings and and is sort of purely to test basically statistical in terms of the wind rates of the different programs and what we found is that a gap of about two hundred ELO points or two hundred fifty low points translates to about eighty percent win rate and alphago was more than a thousand Ehlo points better than the other nearest the other best programs and so this was back in october so this is not the most recent version and we beat all of these other programs so it was time to test ourselves against some of the world's top human players so we did back in october was challenge this lovely guy called fan way who's now based in france but was born and grew up in China he's the current reigning three times European champion he's a too damn professional started playing go at seven and turned professional in China age sixteen it's very difficult to turn professional in China so he was a top top player before moving to France and now he coaches the national French team and we changed in October and this is what happened I think after for the game maybe he don't like fight in like place slowly so is why he beginning the second game I fight I said maybe and what it's why was another game I fight all but I'm joining so I'll forego one five nil and much to our kind of surprise and became the first program to ever be a professional ago and if you ask you know AI experts even the top programmers of these other programs even till sort of a year before they were predicting this moment would be at least another decade away so it's about a decade earlier than the top experts in the field expected and certainly a decade earlier than the go well the go world thought was going to happen this was the distributed version and this story ends well though he you know he looks distraught here but he and he ended up / ended up hiring him as a consultant on our team after this and he joined the side of the program office actually but one interesting point about this which is you know is interesting is that he we he then came into the office about for about a week every month to make sure you know he was part of our making sure we weren't overfitting in our self play by carrying on pitting our wits against him and he felt that his play had improved by playing as alphago and actually he went from ranked about 600 in the world at the time it October to in january/february like three four months later being ranked 300 in the world so it was you know so and he'll tell you that he it really opened his mind he said it freed my mind from the constraints of you know there's 3,000 years of tradition to think in a different way about the game it was very interesting so again if you want to read the technical details of this is another nature paper front cover that was a couple of months ago and I think it's caused a really big storm in in the sort of the AI world and the go world so then it was time to take our kind of ultimate challenge which was you know just a few weeks ago now we decided to challenge Lisa Dahl who an absolute legend of the game you know I call him like the Roger Federer of go and you know he's been undisputedly the best player of the past decade and he's won 18 world titles and he's also famed for his creative style and creative brilliance so he was the perfect player for us to you know to pit our wits against and we played him an early March for a million dollar first prize in in Korea now just before I go to the results I just want to do a side note on compute power here which I always get asked about so we use roughly the same compute power for this match as we did for the fan play match so it's around about 50 60 GPUs worth of compute and you might ask well why don't we just use more well actually there's quite a mission visit visit sort of asymptotes quite click quickly actually the strength of the program with more compute power and one of the reasons is actually quite hard to paralyze MCTS algorithms that they work much better more efficiently if you do them sequentially and if you batch them across lots and lots of GPUs you don't actually get that much more effectiveness out of it and one measure of that is that the distributed version surprisingly probably I think two maybe maybe too many of you only wins about 75% of the time against the single machining version so we played the match and many of you will seen it we actually won 4-1 and it's pretty astounding to us because even like the day before the match they interviewed Lisa Dahl and you know he was saying he was company he was going to win five nil and the whole world thought there was no chance we could win obviously they were looking at the fan way matches and trying to estimate you know maybe we didn't prove ten twenty percent since then and that would stand no chance against Lisa Dahl but actually in the five months that we had between the two matches the new version of alphago could beat the old version of alphago 99.9 percent of the time so it's pretty astounding least remark stronger and actually is you know as an amazing experience out there and I'll talk about the cultural significance in a second but one very nice thing is the president of the Korean Go Association in the middle there awarded us and alphago with an honorary nine dance so it was really beautiful we're going to have that framed up on the on the wall for its its creative play and and I just want to touch on those kind of themes actually about about the creativity and intuition and that's one of the reasons I explain to you how how to play go because I want to just try and explain to you some of the significance of what alphago did now chess is really my main game that I play but I play go well enough to be able to appreciate what's going on now one probably the best move that alphago played in the whole five-game series and maybe the go world will decide to name this move is move 37 in game 2 and this is the position alphago was black and alphago decided to play here it's called a shoulder hit move this move here in red and I just lose tronic I'm going to try to explain to you why this is so amazing this move by Tony a little bit about go so there's two key lines in go the third and the fourth line of the board that's the critical line Zingo so here's the third line now if you play a stone on the third line what you're really trying to do you're telling your opponent is I'm interested in taking territory on the side of the board that's what a third line move means a fourth line move by contrast is the fourth line what that means if you plan the fourth line is I'm trying to take influence and power into the center of the board okay so you're going to try and influence the center of the board and radiate that influence across the board so that's towards the center and the beauty of go and I think one of the reasons why I ended up evolving to a 19 by 19 board is the playing on the third and fourth lines and going for territorial influence is considered to be about perfectly balanced right so the territory that you get for playing the third line is about equal to what the opponent gets by playing the fourth line in getting power and influence into the center the idea is that that influence that you get and power you get you store it up for later and eventually that will give you territory somewhere else in the board so that's that that's the classic 3,000 years of history of go and yet alphago played on the fifth line to take influence towards the sense of the board and so this is kind of astounding to the you know goes against 3000 years of history of go and then just to show you how astounding that was to the NGO fraternity I want to show you a clip from the commentary the live commentary and so we had commentating lie there were lots of commentaries channels there's actually 14 live channels in China it's all three national TV stations in Korea but also we had an American we had we had an English Channel via YouTube and we had this fantastic commentator called Michael Redmond who is the only Westerner ever to get to 9 Dan White is the only english-speaking person to ever get to 9 that and look at his reaction to this move 37 so oh just to show you so what turned out was about 50 moves off to this move that move here influenced the fight over in the bottom left corner right about 50 moves later so you can't calculate that there's too many possibilities I think that's that was the the influence of that of the power of that move that it is Michael Redmond so yeah I think he means a misclick as opposed to click miss but so he was thinking that our operator that the person actually playing the Meuse for alphago at rank is the lead programmer had actually entered the move wrong into the machine because that surprising a move and well this is what Lisa doll fall of it he disappeared to the bathroom for 15 minutes so that's his empty seat there no one know what happened to him so he he just disappeared for 15 minutes so so you know maybe it will be called the face washing move or something because they usually named after something that happened so you know and actually later when we investigated the the statistics behind this we found that the policy network gave the prior probability of this move there's less than one in 10,000 so so alphago overcame so it's you know it's not just repeating what it's seen in these professional games because it would never afford to play this move later on some of the nine damn pros commented it's not human move no human would ever have played this move so you know it's a really kind of original move if you like and one thing that we think is going on here is that no we can't yet we haven't we need we need to build more visualization tools to actually do that we're building at the moment it's pretty hard to know why it's explained why it's done that to it for us so so you know so that so here you know tons of these kind of surprising moves you know I think this is actually sort of shows some kind of originality and what it might mean is when I talked to Michael Redmond about this is he said that you know alphago has this very light touch it doesn't commit to territory early and what we think is going on is that alphago really likes influence in the center of the board and it likes it so much and it's so good at ultimate Lee making that influence pay later on in the game that it actually thinks fifth line is influences good enough so this may cause a whole we think in the game of go as to you know what's an acceptable trade then I must say the other really spectacular move was played by Lisa Dahl in Game four so we won the first three games and then Lisa doll came back strongly because it's an incredible games plan I've met many of the best games players in the world Garry Kasparov and others but I put at least of all the top of all the games players I've met in terms of his creativity and fighting spirit and in Game four he won Game four and he did it by playing this incredible move move 78 here when I have got time to go into why this is so special but basically when we look to the data on this as well we found that alphago thought the probability of this move was also one in less than one less than one in 10,000 so it was totally unexpected for alphago and that meant all the pondering in search it had done them up to the Meuse prior to this ended up being having to be thrown away so it basically had to start again for as soon as this move happened and for some reason this caused some Mis evaluation in the value net which were still investigating what happened there so cultural impact of this match was huge we had 218 million viewers that's more than like the Super Bowl and 60 million viewers just in China for the first game you know we were being stopped in the streets in career and it was pretty crazy and 35,000 press articles literally every day was the front page of all the newspapers in Korea and the thing I liked most actually was that it popularized go in the West there was a worldwide shortage of go boards for the last few weeks after still now I think if you're trying to order a go board you might have trouble because of this game which is you know fantastic to see you know the press coverage was just insane this is the pret these are pictures of the press room just like a scrum number of live TV campers like 50 live TV cameras at the back it was on all the TV stations jambo screens in the shopping districts is pretty crazy it's amazing to see I think for Korea was the perfect matchup of they love technology they love AI and they love go so for them it was the perfect storm and this is one interesting thing as only show is the rate of progress of of alphago so we started this project only about just over 18 months ago and the progress has been relentless from the beginning we found that these techniques which can improve themselves and and you know you can kind of create more data and then train new versions and then that can create more better higher-quality data that kind of virtuous cycle has delivered about a 1-ranked improvement per month which is you know pretty astounding and the interesting thing is we haven't really seen any asymptotes yet so we're quite sure to see how far this can go and what what is the sort of you know optimal play or get near to optimal play and go like how much further is there to go and actually I think most of the go professionals are really interested in this question as well and I'm pretty sure that just like with fan way when we ultimately you know hopefully release alphago in some way to the public I think it will improve the standard of go and bringing whole new ideas so after the heat of battle I had a great dinner catch up with Lisa doll who's also an amazing and lovely guy and we talked about the match and he told me that it was one of the greatest experience of his life and the fact that it had totally just the five games he played had made him totally rejuvenated for his passion for go and the ideas and creativity about what could be done it's paying a few thousand a day yeah depends on how many machines we use potentially I mean these pros play a play several thousand games a year probably about a thousand two thousand when they're training so it's quite a lot plus they read a lot about all the ancient games I don't think so because there are three different schools of go the Japanese the Koreans and the Chinese and they're very competitive against each other and there they approach the game differently and I think that that creative tension is forced them out of local Maxima I would say so just to compare deep blue with alphago just to be clear again about the differences so deep blue you know again not to take any way from the immense achievement that it was for its time it absolutely incredible but it used handcrafted chess knowledge by contrast alphago has no hand crafted knowledge it all the knowledge it has is learnt from expert games and through self play deep blue did for which search pretty much looked at all the alternatives and that's why I needed to crunch 200 million positions per second by contrast alphago uses these two neural networks to to guide the search in a highly selective manner and that means we only need to look at a hundred thousand positions per second to deliver this kind of performance so I just want to finish by a couple of words on intuition and creativity and this may be a little bit controversial so I don't want to I'm not saying this is kind of like the the full truth of the matter or even fully encompasses on everything to do with intuition and creativity but I think these are interesting thoughts so we have to sort of define a little bit what do we mean by intuition and one way I'd like to at least the way I think about it for go is it's kind of this implicit knowledge that humans are acquired through experience obviously a plane go but it's not consciously accessible or expressible not so certainly not to communicate to someone else but not even to themselves but we know this knowledge is there and we know it's a very high quality because we can test the knowledge and verify it behaviorally obviously it's the output of the moves that the player plays secondly you know what is creativity and I'm sure everyone in this room has their own pet definition but I think again it definitely compasses sort of the ability to synthesize the knowledge you have and use that knowledge that you've accumulated to produce novel or original ideas and I think certainly at least within albeit the constrained domain ergo I think alphago has pretty clearly demonstrated these two abilities and obviously while playing games is a lot of fun and I believe the most efficient way to go about AI development we're obviously that's not the end goal for us we want to apply the technologies that we've built here as part of alphago that we believe are pretty general-purpose extend them use components of them and apply them to have impact on big challenging real-world problems and we're looking at all sorts of areas at the moment like healthcare robotics and personal assistants so I just want to thank the amazing alphago team who did all this incredible work real incredible engineering and research effort and also you know again I just want to stress all this work I've just shown you today is really less than a tenth probably of the work that we're doing at deep mind and if you're interested in seeing all our publications they're all on our website and there's a sort of about 70-80 publications there now of all of our latest work and of course the I must mentioned if you want to get involved we are hiring both research scientists and software engineers thanks for listening sorry yeah yeah good font you want to use this well for a second thank you so let's have a couple of questions anybody yeah okay let me I think the question was can groups of players together be alphago maybe so that's something that we we might play in the future actually is a group of top professionals versus alphago and the question to see because it's known that some of these top players are really good at opening or middle game or end game and you could kind of switch between them and I'm sure they'd be a lot stronger together so maybe we'll do that towards the end of the year or next year yeah it's behind you yeah yeah yeah so that's a hoop sorry yes so the question was using visualizations to understand better how alphago works so we think this I mean this is a huge issue with the whole deep learning theory actually is that how can we better understand these these black boxes that are you know doing these amazing things but quite opaquely and I think what we need is a whole new suite of analysis tools and statistical tools and visualization tools to do that and again I look to my neuroscience background for inspiration of for those of you do fMRI or that kind of analysis I think we need the equivalent of kind of SPM for virtual brains so we actually have a project called virtual brain analytics which is a round building these kinds of tools so that we can better understand what representations these networks are building so hopefully in the next year or so we'll have something much more to say about that yeah yeah yeah yeah so it's a really good question actually a question is oh can we do away with the supervised learning part and just start just go all the way from from literally random using reinforcement learning up to expert we're going we plan to to do this experiment actually so we think it will be fine but it will take a lot longer to train obviously without bootstrapping with the human expert play so until now we've been just concentrating on trying to build the strongest program we can in the fastest time so we haven't had time to experiment with that but we will there are a number of experiments like that that we want to go back to and try I will say that some very smart master student from Imperial College in London did do this for chess from scratch and they got to kind of international master standard so it seems like this this is definitely sort of possible and actually we've hired him now and so he may be the person that would end up Matthew like he's called that may end up looking at this as well so maybe someone from near back yep so sorry yeah yes yes potentially so we're thinking about adding learning into that part too but and also you know maybe there are ways of doing away with some of that Monte Carlo tree search - there are other ways of doing that search kind of more like imagination based planning so we're thinking about that as well maybe back there yeah what that so I think the question if I understand it correctly is that if agents play game well games well is that AI is that is that were you're asking or is that well I mean obviously that that's our thesis is that this will work but I think you have to be careful how you build the AI there are many ways you could build AI for games that would not be generalizable so and I think that's been the history that that is what generally for commercial games which I've also helped make lots of commercial games which have AI in them and usually the inbuilt AI is special case that you know use its finite state machines or something for the game and it utilizes all kinds of game state information that if you were just using perception you wouldn't have access to so I think you have to be careful that you use games in the right way and you treat the agent really as a virtual robot with all of that that entails in terms of what it has access to and I think as long as you're careful with that then it's fine and one way we enforce this is we have a whole separate team and evaluation team of amazing programmers at most of the x-games programmers who build the environments and the api's to the environments and so on and then entirely separate from the algorithm development teams so and the only way the AIS can interface with the games is why these very thin api's so we know there's no way even if the researcher was to be laps with this or lacks with this they could access things they're not supposed to be agents so I would just go around in questions yeah here while we're doing both actually so there's self-training in terms of the self-training producing high-quality data its tweaking itself through you know this degree enforcer learning and we're also actively doing tons of research in terms of new architectures or parameters and other things so it's all of the above so we really threw everything at it 