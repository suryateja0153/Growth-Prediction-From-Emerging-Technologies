 which one is it left or right for the controller anyhow we will see so first of all thanks for inviting however I'm actually quite nervous I mean being more on the academic side and then seeing the people that help or actually do the real work of making many of my dreams happen is amazing and but it makes me a little bit more yeah nervous and actually look I mean talking about faire ml and then coming with a more academic issue that's unfair so and so because I was somehow expecting something like that I asked one of my friends who is definitely interested in faire ml to set the stage for the talk okay and before we listen to him let first see a slightly different title that is hopefully making it more attractive to what I wanted to tell you and it's actually this question of if there's something like an o1 o2 o3 flag for machine learning for statistical machine learning so we heard a lot about we should have interpretable models we have heard about hey I want to be forest I want to deal with a lot of data but then actually looking at what is going on in machine learning there's not much work on how can we our compilers for machine learn right I mean you see here a little bit of language and there a little bit of language but not many people move much more closer to the architecture and try to get something like a high-level programming language and compiler for machine learning anyhow as promised so here's my friend I'm very happy that he managed to give us a short intro to the topic okay can someone make the volume on but as all of you now understand me let me go back because they're so cool if you haven't listened to that one if you haven't seen it that's a cool joke in the beginning hi everybody normally I'd begin these remarks with a joke about data science but about half the stuff my staff came up with was below average but as all of you know understanding and innovating with data has the potential to change the way we do almost anything for the better that's why my administration's opened up massive amounts of government data to the public for the first time with more than 135 thousand data sets available for download at data gov think about the weather and map apps we check every day on our phones many of which are powered by open government data along with countless other apps and services or our new precision medicine initiative which joins data science and healthcare to accelerate treatments for disease we want more Americans to dream up and deploy innovations like these to solve problems save lives and create new jobs and opportunities that's why I'm so pleased to welcome dr. DJ Patel as the federal government's first chief data scientist and that's why I'm asking you to help as DJ likes to say data science is a team sport that's why we want you America's data scientists to join us in this effort help us build better digital services for the American people help us unleash new innovation in areas like health care and climate change help us change this country and this world for the better Thanks so I think he's completely right it's only that it should really not only be the US right in particular Europe in particular many other regions in the world I'm Article II talk about Europe because we have the only alternative or similar thing we have there is Guttenberg and he is famous for faking his PhD thesis so this is kind of weird anyhow so I really really think we need much more about how to communicate about data science we need much more than understanding all the different disciplines and make Joe public and aware of that anyhow I think this is all showing this deed this this arms race for deeply understanding data right this is an old story in a sense already and it's typically told like yeah take your spreadsheet apply some machine learning whatever you like it's so easy right this is what you know even better than me and that's it so question is is it really that simple and and we have already heard some of the stories when it comes to being fair or maybe even on purpose unfair but they are much fundamental more fundamental issues here and I would like to go through one of these examples together with you it's a very simple example so imagine we have this deck of cards and I'm asking you ok I'm shuffling it maybe several times I turn around the first card and I'm asking you what is the probability that it's an ace it's easy right so there are four aces 52 cards so you can divide the number you get the answer I can also ask you the question what is the probability that I'm shuffling again that if I only turn around the second card what is the probability that's an ace or queen you know you just think again oh yeah it's the same answer right so you get this answer so let's see whether current AI and misleading technology can solve that and I think actually is almost independently whether it's deep learning or not so let's see so we use your graphical model so we represent that using probabilities right and so you have all these different random variables here where the first here tells you the position and the second one is telling you there the value so now a graphical model is not just random variables but you depict also in dependencies in there by placing arcs edges in between there so essentially you're coding something like the dependency but actually the independence you're interested in how does that look like here well let's have a look at the very first random variable here the value very much depends on all other random variables right if you think about it I mean if the first one is not the particular value d2 then some of the others have to be d2 and so on and so on so if you go on actually the whole graphical model looks roughly like that everything is connected so if you look then closer at that and that actually means there is no independency at all it's fully connected or you have to return something like two to the poles of what is it 2700 four states and we had in one of the talks today already this reference to how many atoms are there in the universe I think this is much more than even that number right so it's really not solvable isn't that weird I mean we are talking about whether a is taking over the world and then this very innocent problem can't be solved by the machine so what are we missing here any idea heuristics representations well we are smart enough to see that positions and cards and values are interchangeable in a sense right there's a lot of redundancy or symmetries in here but the machine is so done to not make use of it so what is currently happening is this new idea of not just using a dump language but using a high-level language in this case logic but it doesn't depend much on logic and it's a way to logic together with symmetry aware or language aware inference and if you use some of the existing systems and if you want to know more about that you can also talk to a feel a later and he gives a talk about a related problem there then you can solve the whole issue in milliseconds even exactly so main point here is if we really managed to go ahead and not just a with a simple spreadsheet a sheet and just simple languages but we go for high-level languages then we can employ these high-level languages to make inference and learning not only modeling okay so here's an example just to illustrate that it's a big Bayesian network it's a rather simple one it's about smoking and France I'm a smoker and one of these horrible European guys and so you're interested how long I will survive and then because of all these infrared symmetries in there actually you can automatically compile that to this very simple smaller model and you can imagine that running any kind of probability inference approach on the smaller model is much more efficient right so this is not an news story it's going on for at least 10-15 years already it's called statistical relational AI or status relation learning it tries to combine more expressive languages together with graphical models or probabilities and if you're interested in some more details we just published a book on that and also a fee a similar book on more probabilistic programming perspective so there is some interesting sources to understand how this all works okay but so far I was cheating in a sense like we were interested in applying machine learning in particular statistical machine learning and graphical models are cool but they are not just any statistical machine is much more actually we heard graphical models I think for the first time in this talk at least here at this venue so there is so much more how do we do it there right so let's have an example so we would like to do a very simple task we would like to classify publication scientific papers into let's say whether they are about machine learning or not right so how do we do that well one typical machine learning approaches to use the support vector machine for that right so who has used support vector machines already yeah cool but I guess most of you have never really touched the QP formulation of it the quadratic program formulation of it but you just call the standard solver and I think this is good because if you want to use the QP formulation you really have to love linear algebra and many of my students they hate linear algebra they really don't want to touch this and it's actually not easy right you have to what do you have in mind publications and blah blah blah into a row or a column of her matrix it really takes a lot of effort so instead of going directly to the matrix we were working now on something like relational language very similar actually to logic blocks that helps you to avoid these solver form but stays with the paper form in a sense right so it looks roughly roughly like that that's more the paper form you will see later on the Python paper form sort to say but you can write down something like in-line definitions well for those who knows support vector machines you need to talk about flex if you don't want to talk about linear separate separably problems only you have your objective and you have constraints on your objective please know it in contrast to all existing languages you really can have high-level concepts in here like all the features you just talked about you sum over all the features in there without even talking about the features per se okay this is quite interesting because actually this helps you to digest or extract the very core of what's going on here so if you use this SVM on a different data set you don't have to change the few like lines you have just seen they stay the same and only your data is changing right so instead of working independently as done so far all the time so I'm changing a little bit the quadratic program and then I'm the Peking and I'm not giving you that because I want to have the next paper know what we can out easily do is I change the intentional part of the program I share it and then everyone can use it and improve a lot right so instead of being somehow separate we can now work on the essence of the problem and share that and everyone is just changing their data so I was again cheating it seems like Germans like to cheat not sure so but anyhow publications are citing each other right so how can we make use of this so typically you would have to use something like a graph current and if you don't like linear algebra I can't imagine that you like spectral theory so it's not getting easier for you right this is not really the way to go so question is do we really need this fancy stuff so don't get me wrong I like the fancy stuff but I also like to keep it simple and only to use it when really needed so here's an alternative at least for the kind of problems we are currently looking at so you have high dimensional examples like the papers and in the papers all the words are encoded as a single feature and then additionally you have this information about cycling so instead of going for a kernel we are now just adding few lines of program code so you're talking about whether it's citing or not so you try to look at the labels of the citing papers and again it's just one line and you're not talking about particular papers here you just write it down as you like it and then you add few additional rules down here simply saying hey I want to be on the same side of the hyperplane as my citing papers that's all what you do so it's essentially something like three lines of code you're adding there no kernel or coming back to the previous talk this is still even understandable well maybe not to my mom but to a person that is trying to do classification okay so this is all embedded in Python because actually I don't believe that everyone that loves logic so if you want to use for loops instead of for all use it that's fine with me right so you choose what you like the most and actually you can also interface directly with any database management system you like so you get scalability in a sense and also another source of speed and even more interesting at least for me being a kakak academic you can combine that with the probabilistic language something you like you saw before and get something like store has take mathematical programs and a general purpose over for them maybe not the most efficient one but at least we can now start to explore even without being an expert in that so that I was talking about the old flex right funny part is because you have this language you can directly don't look at this formulas it's just it's a nice figure there so what you can do is now you can automatically detect symmetries so you can attack that this is one paper behaves exactly like the other paper in terms how it influences the other papers and their classification this is done automatically it runs in quasi linear time so it scales to grass with millions I think the largest one we were looking at is 60 million edges runs understand a desktop in a second so even if there's no symmetry spending a second doesn't hurt really right because to Cle solving the whole problem takes much longer so that's pretty cool it helps we can go into details but that's all rain I guess but it's really boost performance and if you're still not convinced so we are talking about here the interplay of language and optimization so what we just finished actually two hours ago because then there was the oops deadline I guess it is because we have a language we can look at the past three of the formulas now such a parse tree looks very similar to an algebraic decision diagram so instead of doing everything via the standard technology or of linear algebra we are now replacing sparse matrices by algebraic decision diagrams and this mapping is super fast but then we have a problem if we do optimization we can't do what is called Shoeless key factorization easily with decision diagrams but that's not a problem anymore because you can use so-called matrix free optimization so if you do that you get actually out of the box a system that can easily deal with 60 million modern zeros so we are not talking about the overall problem size but just the non zeros in your problem 12 minutes per iteration on the standard desktop and actually right now it's also running on the 1 billion and I think it's just a little bit of reprogramming to get it to arbitrary size using some database technology in there so with that I just want to look a little bit ahead so to say in this talk what I think data science will look like in the future we will have these high-level languages that helped us to develop even the next level of fare of course machine learning approaches and all this is in essence my hobby here is that it's all about democratization lot of data but of optimization because it should not be just few people that really love linear algebra or even higher-order mathematics it should be all of us that can do this cool stuff and hopefully much faster and because we have this high level language you really can much easier build fancy optimization approaches you can make use of sophisticated domain knowledge to build your models and you can even speed up solvers I mean we were using here industry strength sauce or like Ruby on SEAPLEX and we can all speed them up and I'm not even Boyd but yet at least so this is pretty pretty amazing if you're still not convinced actually this is all going back to an old Grand Challenge of Jim Gray one of our tooling Award winners and he was posing this challenge of automated programming and for me data science is maybe the best environment to prove that this guy was not wrong that we can really realize something like automated programming with very useful changing the world helping the world to survive in a sense applications and that can't be done just by the machine learners can't be done just by me it can't be done by a single discipline but this is really working and come all together and show that together we can really make it thanks 