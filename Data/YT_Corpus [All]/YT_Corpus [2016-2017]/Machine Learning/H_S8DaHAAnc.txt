 hi, when we build a decision tree we need to select one attribute and split the given data there are several ways to select an attribute and split the data set one of popular algorithm is the ID3 algorithm and the ID3 algorithm uses entropy and information gain concepts what is entropy so the entropy concept is like this when we have one room which is very messy we can say the room is high entropy and when we have the very organized room like the right room we call it low entropy as you may remember we had the example 8 pictures and want to find winter family photo from here so before we select first attribute from here we need to know first base entropy from the given 8 pictures so we have 8 given pictures and just have 1 winter family photo in the given photos so, total is 8, we have 1 photo is winter family photo, and the other 7 is not the winter family photo so, according to this formula we can calculate the entropy we can denote it as entropy([1+,7-]) because we just have one winter family photo and 7 which is not the winter family photo so here the denominator is the total count of pictures(8) and the 1 here is the 1 winter family photo and 7 stands for the other pictures after we calculate this one the base entropy is 0.543 and the next concept is the information gain and the information gain uses entropy concepts so, here you can see the example we already know the base entropy and we are going to take away here entropy of when we select cartoon attribute so 0.543 is the base entropy from previous page and going to minus here, total 8 pictures we have four cartoon pictures in the given pictures multiply by the entropy of the cartoon pictures so the cartoon pictures has no winter family photo and 4 cartoon picture all are not the winter family picture and plus something other than cartton the remaining is 4 pictures and this remaining is like this picture and this picture an this picture and this picture so we have one winter family picture so we say 1+ here and the others are not the winter family pictures so after we calculate this one the result is 0.138 so, same concept to the here when we select winter as a first attribute the result is 0.093 and when we select more than one person as a first attribute the result which is the information gain is 0.093 and we want to select highest information gain we are going to select cartoon as a first attribute 