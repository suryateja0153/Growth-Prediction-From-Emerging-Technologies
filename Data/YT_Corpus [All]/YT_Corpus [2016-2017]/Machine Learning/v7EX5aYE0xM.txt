 hi so can you hear me okay if you ever cannot hear me fine way for something because I don't speak very loudly normally so the microphone is good but if we lose the microphone then just let me know and just to get a feeling of the room how many people have a spark in production here just to make sure okay so a bunch of guys well yeah right so you don't need to be convinced about anything so part of the talk will be completely unnecessary for you right how many people here have looked at Scala just sort of out of curiosity okay right okay so I'm going to tell you about my experience looking at Scala and spark so I'm the director of data science retreat and we've been teaching machine learning data engineering for like three years in mostly mostly with Python and R and recently in the last year or so we started using Scala to to teach and to do delivery projects and so on so I have done the same tasks twice once in Python and once in Scala spark and then I can tell you what what my feelings were when I do the same thing twice so to start with kind of obvious introductions so python a scala language is sort of very large all-encompassing machine learning tools libraries actually spark is not only machine learning it's a whole bunch of other things and the main thing that differentiates them is that in this the left side of the Ring do you have one machine only so you have to worry about the memory of one machine the CPU of one machine and in this part of the world that the whole game is to be distributed so that's basically the the main thing that that this discussion is going to be about right so for me it's important to answer a question with this talk so the question will be like why should I bother to to go to the side right right so what is talk well how you go from single machine to multiple machines and apparently the obvious answer now is the spark machine learning and the lingering question here could be like am I missing out if I'm mistaken in the Python world right so I don't know if I got to be able to answer that question it's kind of a very personal thing but I'm going to try to give you my view of the world right so you may be thinking well yes part there is spice Park so why do I bother so I just stay with past Park right so one thing that I'm going to cover here is the dangers of using PI spark so there are some consequences of not being on the native platform sort of it's not horrible but it's not ideal either so right so take a sorry city so if you don't know what this it works as it's a retreat so it's for people who who know what they are doing but they want to be much better and then they fly from all parts of the world like literally like Brazil Korea Vietnam all kind of weird places they come for too early for three months and we hit them very hard with basically all this data there are things that we can imagine mostly a streaming machine learning deep nets are becoming a big thing all the things that that you think oh I could learn this by myself but it would take such a long time so then you get some badass to to just show you how he does things and did you get the best practices and so on I think that's the way people are attracted to to this right so the metals are pretty pretty interesting people in general we accept very few people but that's just normal I mean many applications are not that interesting some of them are interesting but they are missing one little bit which could be deveneux machine learning so they could be kick-ass bi guys who know sequel like better than you but they don't know machine learning and then you set them back and they study and they come back to you right so we really want them to be able to communicate well with stakeholders and people who'd understand machine learning that's important because I've seen many projects failed to get support just because the data scientists don't know how to explain to the rest of the company what they are doing and then who is this guy who is working around costing us a lot of money and nobody understands what he's doing so we want to avoid that so there are technical communication sessions where you just improve your your communication skills basically people who when they come they have about five years of experience on average and we we work with a bunch of companies who want to hire talent and apparently these companies are very happy because they keep coming back so I think that the product that we are resolving but I may be wrong is that there is a valley of death so there are a bunch of people who have certain amount of skills data science that they can fit random forests and they can tell you why a model is better than another but there is like a huge area here what there is not much going on and then they have you have the super good guys who who could do the next on your performance and there is nothing between this is my experience so I think this is what happens when you come to a super intensive retreat that you kind of jump over the value of death or list you try right so examples of portfolio projects let me see if I can click on this screen which I cannot apparently well it's a video of participant who built a smile recognition system so to cool the smile on this this bar will go green and it could work with multiple faces on real-time video zone this was like 1/2 years ago now this is actually kind of common but before it was not that common so one of the attendants to high-end a resist his hand he was there to hire and then he says like why do you want the job like just put this in production make a company out of this and don't get the job like he actually did so anyway so right here is actually working now on detecting fake smiles which is kind of interesting right in general people are very happy you can see smiley faces and so on they keep in touch and they they tell everybody what they are doing they are doing crazy stuff for pretty interesting companies right so why is this are talking about Scala and spark here so like plant is actually the company behind the Scala they used to be called typesafe but they changed the name kind of like Google and these guys got approached by IBM who is running this Big Data University to create training materials for data scientists who want to do things in a Scala and in this part that they don't know how they know the typical machine learning topics and so on but they don't know they how right so then they approached us to create training materials for for this that big data University their videos their I five to ten minutes and their to the point so you want to run query push these three parties how it works is how you can pair it with other models right so interestingly in the market apparently people who are doing a spark are really interested in doing it natively in Scala and you will see why soon there's plenty of Python for sure Patrice Morris column I don't know what that means so this is a totally subjective graph of programming languages in data science I think our was super hot up to a certain point it keeps growing but it's sort of God taking over I don't even know I don't have empirical data for this this is just my gut feeling basically by talking to data science badasses and so this could be not over could be far ahead I have no clue I just draw in something that's my gut feeling and what I think is happening is that Scala is about here it's growing very fast so the slope is higher than any of these two and it's mostly about spark so most people go to Escarra because they they try this park and it's such a nice much so I would like this talk to to give you a superpower and for me the superpower will be that if somebody approaches you to do a project on a spark and this is a company that is not using any Python and they cannot do any Python banks or big companies like that I want you to be able to say yes or maybe after this talk this is a very big challenge and you will see later on how we're going to get there because with 40 minutes of talking it's not going to happen but I would like that to be the superpower that you get after this talk to be able to say maybe to a project that is using Scala and spark even if assuming that you know psyche learn Python really well and you can do machine learning on your own right right so Scala has certain advantages apparently one thing that people keep telling me is that they seem to refactor code by nicely because of the type system that is super sophisticated and complex doing things for them they don't rely on tests so much fine I have experienced this but the main thing to consider is that is color and the spark are really really close to each other so if you learn one you learn the other and this is kind of a freebie so if you really wanted to get very far with spark maybe one easy thing to do is to learn Scala because the collections library is pretty much the same in Scala in this park so somebody thought oh what happens if we do the same thing that the Scala is doing but distributed and that's a spark so if you are really really interested in getting deep into a spark Escalus is the way basically because the the same philosophy the same mindset it's it's there and you probably know that in spark there are plenty of our future is not only a machine learning library and there is more than one now their sequel query engines there are streaming processing things this is changing very much no and there is graph processing tooling this was not only machine learning right so there are people who think that spark is going to take over and eventually everybody's going to be doing a spark right now it doesn't sound super possible because it's so exciting to do so much with one single machine and most public datasets are not worth putting on a cluster so it's actually not that easy that to finally the set that you really need that cluster to run your your model zone right but this may be changing very fast right so one other interesting feature of this kind of spark world is that everything is under one roof right so you have the spark core and then you have all these things and they are all the same library so in Python if you want machine learning you have secondary you have graphs you have data maybe or other things you want a streaming well I actually don't know produces Park and sequel engines well you have segurança means things like that so there are different libraries and they are not integrated and this thing been one single core it's useful because you can go from graphs to two data frames and the same thing and the people who are maintaining the projects are built in the same thing so it's not like oh let's see if the sequel alchemy guys are talking to some other group to the pandas guys maybe not so right and apparently people who are using this park in production are very happy and one of the things they report to be happy about is this integration so they use they use more than one of these blocks right so the the model first part is of course distributed computing so how do they do it they have one driver that sends code running your algorithm to different workers so the data stays in the workers and the code moves around so that was Hadoop's main innovation sort of so they built on top of that so the the obvious example work out you may be sick of seeing work on the samples by now so one thing that is important is that data is immutable in the Python world both tuples are immutable but it's not that important to operate with data structures that are immutable in the functional programming world people really invested in immutable data types and by default everything is immutable in a spark so that buys you some some performance and some characteristics that you are going to to appreciate when you are debugging it right so in in the MapReduce world this will be a map where you go from these two individual worlds and then this will be another map where you assign an integer which is account and then there is one reduce operation where data is shuffle so you have to move things around machines and this is costly you want to avoid that right so this situation is still in a spark there's nothing that has been improved over Hadoop okay now we are going to get into the interesting part a part if you are using PI spark so when you run this is that code for for this same workout example when you run lambda this is going to be pipelined into a python executor and the entire thing is only executed when you run an action like saving the zone so everything is lazy and waiting for you to set the default in this park is not to do any competition so let's hope we don't have to do anything and then when somebody actually says that well actually bring me an ADA then you run the entire pipeline but everything is lazy by default so this ideas of the in being lazy and be immutable or strong points it comes from functional programming languages nothing new Under the Sun but doing it in a distributed way and in an easy way it's the new thing that this part brings right and it also brings a fault tolerance in a totally different way compared to Hadoop Hadoop was just replicating things like factor of two or three in a spark they round the graph of operations if you lose a node and some data that should be there is not then you you go back in the graph and recompute everything to create the data so it's fault tolerant even though you don't have the replication that that Hadoop has right so PI spark on our disease so are this artists res Island distributed data sets and so on this is the one place where I've seen the the architecture explained so when you generate a spark context you get this pi 4j spawned with a socket and then this will go to the workers and a star different Python executors right so the interesting thing is that would you call a lambda function to operate on the LDV each spark worker Forks a Python worker and data has to move from the JVM to Python Compaq yes because of the lambda and this is why performance is not great and this is why if something dies the the executor really cannot tell and this is a little bit of a problem now there is a solution for that so remember that I mentioned that there are some dangers that this is one danger right so there is some overhead for cannainsider I see things there is overhead wrapping objects from one language to the other but also the class of manager doesn't know anything about Python memory requirements and that's problematic because it's very often the case that spark chokes on one single node having either structure that doesn't fit the memory of the node the typical example is a group by operation so you have imagine the Facebook Graph and you want to group by number of friends and you have girls and boys and then one node has like several million things to count and then chokes and dies another side effect of this wrapping from one language to another is that the error messages are not great sometimes you get a stack trace in Python sometimes you have to go down to the JVM and then good luck to you finding what maps to weapon to what and so on excuse me right so what is the solution to to performance problems the solution is to never do the are these filtering with lambdas directly but using data frames so everybody loves data frames I'm a very useful concept so do you have data frames in spark and if you do it like this it does everything in the JVM and there is no spamming of all our process for Python and so on if you want UDF's like functions that you create and so on and maps and soon you still need to serialize but well you can do a lot with this syntax without having to resort to to go out and create functions and you can actually write the functions in Scala and then map them if you really want to go that down low-level right so you may be thinking well but why do I want a distributed data frame which is one of the nice things of using this park well there is some kind of range where you are perfectly fine with a single load data frame and I guess it goes up to gigabytes so you know that you can buy a machine with multiple kicks and run it as a server zone but there is definitely an upper range where you can't do that now the question is whether you want to do this if you are already interested in machine learning do you want to run a model or a petabyte of data would you sample if your sample is the performance all the same it's a sample good so all these things are questions that data scientists and more soil study stations are trying to to solve right now so statisticians are sort of scrambling to to produce machinery that works with problems like this where you don't have to do they stick a reference for example to a violation and so on but you just use all the data so yeah there are criticisms about well why do I want to run a modern a petabyte of data right but but if you wanted to you can right so the interesting thing is that the way things are going been on Python it's actually pretty pretty convenient because you're accessing data frames the same way that any other language will access data frames they all translate everything to a logical plan and then there's this thing context in that produces an execution model so performance is about as good so this is a pretty old graph so this about one year old this was hardly discarded this was already in Python doing these things it was definitely slower in Python right but after theta friends were introduced look at this all languages are about the same so that's one reason to say well I'm perfectly fine with PI SPARC don't have to move right okay so machine learning there are a lot of steps that you have to do normally and the cool thing about this park is that you can do it all in one single machine so you don't have to do ETL on this monster cluster then somebody gave me and then down sample that somewhere and then run my models and then put it back in production in some other machine so everything can be one single platform as part right so everybody likes cycle learn it's a very nice library it has a lot of things under one single library which is very impressive it's very easy to use it's very easy to create new models and follow the API that they offer and so on it's pretty up-to-date but it normally sticks to one machine and that's that's problem so you can do you can paralyze things like research on all the course and Agua machine but that's as far as it gets you right although having said that you can try this Sparky learn package that apparently takes cycle encode and puts it into a spark somehow I haven't look at the guts of how this happens so I'm not going to talk about this okay what if you had to do machine learning with Hadoop so basically you are screwed because it's so fantastically inefficient so it touches the hard drive for everything and that's just not a very good idea because most machine learning algorithms are iterative like to think on gradient descent and so on every time you do an iteration you touch disk so well it totally sucks so it's just what happened it is that as Park Hill the MapReduce paradigm and everybody who was running serious jobs in my previews has moved to spark pretty much at least the banks and so on so well it's easier it's more efficient in terms of hardware as well because it's faster so things that took a week before you can run them in a day with the same cluster it uses more memory but what is cheap nowadays and it keeps you a whole bunch bunch of new things like as streams machine learning libraries graphs that isn't had before so it's the total win and in terms of popularity is going through the roof and so on but you know that right of course there's this resource to use MapReduce is still for things are like single path like ETL jobs and so on I imagine there's plenty of Hadoop : production doing that if you are not in hurry and you have a big cluster there's no reason to port that right so the interesting thing is that the people who build SPARC and they stood machine learners which is a huge huge thing because hello engineers didn't basically and this shows so they really think about the use case where you have to do things iteratively and then you keep things in memory you only speak to disk if there is no option so it feels very convenient it feels that somebody understood you right so you have plenty of algorithms that are already implemented and the ones are not implemented are sort of coming but it's the things is moving so them faster that you it's actually a problem for people who want to contribute to it because you may submit a patent and then wait for somebody to integrate it and the mean time a new version has been out and then you're screwed nobody can merge that No right so it's not just one machine learning library it started with ml Lib it moved to a spark ml which is now they make the de-facto standard so it used to be that spark ml was wrapping em and leaf solutions and now is the other way around the main difference between these two is that ml live uses heart disease which you've seen are not ideal for Python but as part of ml uses data frames and that's totally cool for Python because it's as fast as any other language system ml this time L is something that IBM came up with apparently is very interesting I have a look at it much and this Keystone ml it's another library by the mplab the same guys that that did spark it's kind of early days of very early beta they apparently have very good implementations of NLP stuff so it's something to consider so don't think for a second that you have to stick to a melody so I'm still Emily is definitely the past spark ml is where things are going and these things are probably worth a look maybe the thing that you really need to do is very well implement it here and then UPS code that indirectly right so there are all these things that if you're familiar with cycle learn are totally familiar to you as well right pipelines cross validators I mean it's called different but you have these things right the interesting thing is that they have a very elegant design so a model and a pipeline aren't the same thing it's not like in cycle there that the pipeline is one thing and a model is one thing so they have the advantage of looking at the architecture of things that have been in the market for longer and longer they just improve for that and my get my guess is that the scholar guys are super deep to computer science geeky stuff and they do everything the most efficient way and the most complicated way to so it's a damn complex language so if you think that you can pick it up on your commute which I thought I want to be mistaken product so it's a very big language it has many features it has everything worth mentioning in the history of computer science ended up there somehow right so then what are the advantages of doing everything in this park assuming that you want to do it in this park for for once you get hyper parameter tuning on a cluster so in mind that you've been doing pretty serious exploration of parameters and you had one single super powerful machine but you know it took a few days to test all the combinations of parameters what if you could just plug that same model and run it under a day that will be very pleasant so you can iterate more you can test more things you get that for free so you go to Amazon you say ok give me this number of machines it's not that expensive I think I have a plaster running right now that cost me something like 10 euro per day and it's pretty serious cluster there is no real reason to to node to not do it and the actual actually is greater cluster but ok so this you may have heard before the science is 80% dealing with the tech linen and the 10 2 percent remaining is about pitching so if you have to go and do serious ETL then why not do it in parallel if it comes for free so you may be waiting for the data to be cleaned this is pretty common in Hadoop clusters and so on well just make it faster by moving to to spark and last but not least important models that you can run can be a lot bigger so if you want a model that has a billion features why not I think one one aspect that people I don't think people have connected this the big data guys are not so weak in to machine learning and the machine learning guys are not so weak into big data when you have really complicated models you barely have enough data to forget to get along with this type of model but the guys who do super monster datasets they don't run complex models they actually run kind of shitty models that can be paralyzed easily so why so why would you not run most of deep learning model with thousands of free parameters if you have the data so this is the interesting part about doing things in parallel when it's actually not that much overhead just you can try more complex models right so hard this problem to scale up into the science add people right so what I've seen is that things in this world are normally very elegant and bringing people as to have to spit and adding contributions is kind of easy although this is a problem people don't publish components they go for kind of monolithic things or nothing and this is not ideal so one example was a pull request implementing nonlinear SPMS that never got merged and now it's been like one year and now nobody can merge it easily so that's a pity because you only have linear SVM's in this part which kind of suck and somebody actually worried and try to implement I think was her bf and they are they're sitting collecting dust right so you may have heard about datasets which are a new data type that has the same API as data frames but it has types and this is kind of a big deal for for scale people because they really like their types so I was in the room when they announced data frames that were untyped and it was like half the room Python guys - yay the other half of the room is color guys don't like not type save so now they have a an object that has type so it's type safe and it has the nice features of data frame but this this monster thing can actually do err arises at compile time which is pretty cool right so I'm going to dress you in the last stretch through three projects that I run both in Python and in Scala spark and tell you what I found so one of them was this goggle what you had to predict whether some researchers are going to get a grant this is an example of a horrible data set there is a series dirty lots of missing data all kind of data transformation so you test kind of the feature engineering part of the of the library another one this actually in progress is this visual cue a competition that is going on right now where you are given a picture and a question and you have to answer the question and surprisingly the features that do most of the work are mostly repeat so this must be a repeat ask so you have the vectors for other features in the image but most of the work is known by this so then the last project was some kind of recommender to tell you who to follow on Twitter so if you want some influencer to know that you're alive basically something what like what buffer does but better basically using a personalized page rank to find the influencers and then computing shortest path and then optimizing that path so this guy actually sees you buffer only looks at the times that people are away posting and that stills in there it optimizes so this is a graph problem mostly very iterative personalized PageRank and so on so I'm going to tell you my experience building these things in Python are and Scala spark so cattle car features we are very used to factors but there are a little bit of a pain in Scala you have to do this kind of handling manually of 1000 columns and so on and when you pass things to a model they have to be in in a format that is very and friendly this you have to call this vector assembler and it gives you features as a vector vector of features not a pandas object or anything human friendly it's just a vector of features right the pipeline's story though is super good so if you know by playing second learn you know pipelines in a spark they have the same features the way they pass parameters are different a little bit different there are a bit more object oriented so in cycle learn pipelines you do it with naming conventions like name of the model and the score is the score name of the variable here they have this abstraction of course but it works very much the same and you can write custom stages so okay the story with NLP also pretty good they have all these strengths transformers and these are the equivalent things on second learn so maybe a more proper comparison will be spacing it has more grippy stuff on this side on the spark side they have even implementations of things like beddings like were to work and so on so totally fine now graph is tough graphics to start with which is the the graph library inside the spark is not doing great with Python they don't have a python api there is data structure called graph frames and that one actually does have bindings for python but I tried and it kind of sucked so I don't recommend it well that's that's my note I have to finish so things I would then like well first things that we liked the architecture is very modular very nice types all over the place model fitting returns our transformer which is nice the story about passing parameters is very emoji news and you can do ETL I'm a model fitting on the same platform right the grid search works really well on multiple machines so you are like oh this is finished already how come so you are surprised by the how fast it is this appointments feature indexing it's the pain this assembly thing maximum death of trees is 30 for whatever reason you don't have matter or lip or anything nice to plot really people use d3 that's awesome missus like nonlinear spms and deep learning is not their native but there is this library called didn't Forge a that blacks in on top of on the Talco system right cool things are coming that you may be interested in everything is moving with back to the point zero to two data frames which is very good for Python persistence because you can save and load pipelines and this this is I love this idea so they're unifying the API is for streams and static data sets with something they call an infinite data set so we are used to something that is rectangular matrix or whatever that is limited right what if it was not limited what if it was the bottom will be always growing that's a stream for you right it's three Macias an infinite dataset you don't know where it ends so they have an API that works on both and this coming is packed up in serum which is pretty cool right so what have scholars part given as well other than distributed datasets attributed machine learning have been easier and cheaper to run than Hadoop doing streams and being easier to deploy well what have they given us so right so okay I said the superpower for this talk should be you can say yes or maybe to somebody approaching do today to do a project with Scala in a spark right tie is pretty ambitious so the way I want to help you say yes is to give you access to a bunch of videos that we made for typesafe no not type safe light band nowadays you have to pay beers if you say type safe we throw them anyway so there are 25 videos where do like this is the sign for advanced or intermediate person who knows machine learning already if you just want to know the how of how to implement things everything like feature engineering dealing with missing data and so on on a spark in Scala just go to this URL and all the videos are there so that's it thank you very much for your attention yep yeah yeah right so I'm going to repeat the question for everybody so one reason why there are no nonlinear currents in a spark is kind of the obvious thing that is just very hard to to do nonlinear cameras on that cluster basically so it may be that it never going to happen so people are doing page to this or this and writing papers and eventually somebody may come up with an approximation so I think is hopeless but okay so you say you can just do the approximation yourself right so you don't need anybody to do it for you right right right yeah it's true 