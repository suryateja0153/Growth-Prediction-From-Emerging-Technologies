 - Hi everyone. For our next session we have two very exciting papers that sort of speak to each other. They're both invited talks. They are papers that have been published elsewhere. And what we're gonna do, is each one them is gonna speak for about 25 minutes. After the first one, they'll take one or two quick clarification questions. If you have a short question, feel free to ask it at that time. And then at the end, we'll take whatever remains of the hour, and have sort of a discussion about how the papers speak to each other, and we'll take a full set of questions on both. So without further ado, I'm gonna turn it over to Adam and James, who will be speaking on bias in word embeddings, our first of two papers on this topic, both relating to natural language property. Thank you. - Great. Well, first of all, thank you so much for inviting us. James and I are gonna present this paper. It's an upcoming paper at NIPS, which is a machine learning conference, and it's joint work with Toiga Bolukbasi, Kai-Wei Chang, and Venkatesh Saligrama. And this is a paper not about prediction, and, you know, making fair predictions, but about the representation of the data, the underlying way we represent the data, which lets us kind of peer inside machine learning and algorithm, and see how it's working, could help us debug things when it's not working, but it's also understandable to humans, I hope, in some ways. So it's an interesting opportunity to make algorithms more fair, I think. So what is the word embedding? Let me start with that. So currently the way we train in word embedding, is we take a bunch of text, or this is not us, it's what other people have done. We just take their embeddings. They take 100 billion words of Google News. Let's say they put it into some algorithm, and it pops out with a word embedding, which is, you can think of it like a giant matrix. You know, there's a row for each word. They have hundreds of millions of words. And then, for each word they have, let's say, 300 numbers. And you can imagine, that for a computer to represent a word as numbers is actually much better, much easier for it to understand than as a bunch of letters. It doesn't know what the letters mean, but the numbers it can deal with. Okay, and then, so you have for each word somehow 300 numbers, and I'm not gonna talk much about how they get those numbers. And then computer scientists get excited. They know it's working, because they see things, like there are these parallelograms which help it understand analogies. So you ask the computer, "Okay, woman is to man as queen is to?", and then it looks for the nearest word in the missing spot in the parallelogram, and it pops up with "king", and you say, "Ah-ha, so this thing is sort of understanding language". "This is great!" So basically, you think of this word embedding like a dictionary. It's a dictionary that helps the computer understand language, English, let's say. And there's, you know, already thousands and thousands of papers even about just one of these embeddings, about words of that that we'll talk here. So people are using these all over the place. So just to illustrate. The concern, though, that you have here, is that these dictionaries are biased. I mean, how upset would you be if you looked in the dictionary, and it said, "A nurse is a woman", right? That would be very upsetting. The dictionaries supposed to be for our children, or people who don't speak the language, or people who understand language but don't know a word to look up and understand things, and it would be very upsetting it said, "A nurse was a woman, or a computer programmer is a man". We would all be very upset. (audience laughing) And I gave this talk a while ago, and actually made a joke, which sadly is not a joke anymore. (audience laughing) No, it's really true. You know, when I give it before, I started to feel like, well, we all know, we all agree that this would be terrible, but now I don't even feel confident to say, that everyone agrees that this is terrible. Sadly, it's not really clear that everyone would agree, but among those of us who want to design algorithms that are not biased, we would all agree, I hope, that we don't want our algorithms to be looking in dictionaries that are still biased. But just to show you, I mean, here's 16,000 views. People are asking, "Where can I find these word embeddings?", because they're so useful in applications, okay? And just as an example, how would you use a word embedding in an application? If you were building a search engine, or even, let' say, a search engine for resumes, and you wanted to find, search a pile of resumes for a programmer who lives in Cambridge, or whatever, the algorithm, you know, could just look for resumes that have the word programmer, but now the word embedding, it knows which words are close to one another. There's sort of a distance between these numbers. It can say, "Oh, programmers is close to JavaScript, and Python, and computer science and engineer". So even if the resume doesn't have the word "programmer" in it exactly, it might be able to bubble up anyway. So that's great. That's why these word embeddings can make things work better. But then you see, these is what we noticed, if you look in this embedding, you'll see that the word "programmer" is also closer to the name John than Mary, which is a problem, and it's closer to he than she, and it's closer to football than softball. That's because we know that these things are trained on, you know, natural language text as it appears in the wild, and, like, that's where the statistics are coming from, the biases that we have in our world, but that could pose a problem if two people are applying for a job, and already, there's a lot more men, male programmers than female programmers, and now, you're gonna push down the female programmer's resume, even if it's identical to the male resume in every other way, except for the person's name or sex. Okay, so that's a problem, and people I don't know, we wanna make sure, first of all, that people are aware that that's a problem. There's already multiple papers just on this topic of using word embeddings to rank resumes, or to do job matching, so. And they're used all over the place. This is just one example. You can kind of imagine any application that wants to use, to understand text, loves to use these dictionaries, these word embediments, okay? So we need to take that into account, and what can we do about it? And we're not gonna give a complete solution, but we'll talk about in this talk gender bias, specifically male/female gender bias, but, of course, there's other kinds of biases that we need to address as well, but this is just giving us a first step in thinking about this kind of question, and how it could be addressed. So what I have here, is I'm showing you some words from the word embedding plotted on two axes. The first axis, so these are kinda like columns in the matrix that I talked about, but they're not actually columns. They're directions in the embedding, so it's some combination of columns. But what we look for is, we have 300 dimensions in the embedding, but I can show you two of them, so we look for directions that capture meaningful things. So if you just take the word "he" on one side, and the word "she" on the other side, and, you know, see how how the words fall out, in terms of what's closer to he or she, you do see that there is, that the word embedding does correlate with what humans might think of as biases, sadly, but you see a lot of things here that, you know, genius is much on the more male side, and nurses are in the female side. So you see the embedding has captured the biases of people, and we did some experiments to show that they're consistent with human biases. James will talk about that. Also interesting, is that there's another direction, there's a line here I've drawn. None of this is perfect, but there's a lot of accuracy here, and we measure the accuracy. There's a line that separates in a different direction that we found in the embedding, a line that separates the words which are, you might say, "definitially male or female", that gender specific that should be. You know, a brother versus a sister, grandmother, things like that, which have a gender inherently by definition. If you looked in the dictionary, you would wanna see that. And the other words, which hopefully, you know, shouldn't have a gender, okay? So the word embedding not only knows male and female, it also sort of knows what's sexist. It's a sexist embedding, but it knows, because it's been trained somehow, that sexism got captured in the embedding as well, which means that we can maybe use the embedding to remove this sexism. By the way, I mean, this is not only an algorithms problem. I mean, it would also be okay if somebody sat down, and made their own embedding. You can move these words around, and you could have a dictionary maker get together with a linguist and a mathematician, and they could create an embedding. You know, it's like a dictionary, right? But typically, we're computer scientists so we try to create these things algorithmically, so we're happy that we can see this division in the data, and we don't have to make a huge list of words ourselves. So you see the sexist part here, and then what we can do, is we can move the words around. Oops, sorry. Wrong way. And we can move the words around, and we can create a different embedding, which, hopefully, is not as biased, okay? And you could even make the claim that it's easier to de-bias an embedding, for an algorithm to de-bias an embedding then it would be to de-bias a person. So if you went to the HR manager at some company, and you said, "Hey, you know, please ignore gender when you're hiring", they might, you know, try to do that with their best of intentions, but that's really hard for them, and then it'd be hard to go back, and look at their decisions. Whereas an algorithm, you can move these words around. You can, if it's not working, debug it, adjust it, see the decisions it's making. You can, after the fact, go back and audit them. So you do have a lot of accountability and transparency in these embeddings. Okay. So another thing, as I mentioned, these analogies give people a lot of comfort, or an insight, also, into the word embeddings, and it works really well with a lot of things, like man is to king, woman is to queen, Paris is to France, Tokyo is to Japan, and there's some kind of geometry here that's explaining all these analogies. So what we did, is we started asking the computer to generate a bunch of he/she analogies, and it came out with he is to blue as she is to? - [All] Pink. - And it came out with he is to brother as she is to? - Sister. - Which is good. And he is to doctor as she is to? - Doctor. - Yeah. (audience laughing) He is to sausage, as she is to? It came out with buns. (audience laughing) This is trained on the Google News. I don't know why that came up, (audience laughing) but he is to realist as she is to? - Whoa! (audience chattering incoherently) (audience laughing) - She is to pregnancy as he is to? (mumbling) (audience chattering incoherently) Right. (audience laughing) Kidney stone. (audience laughing) Yeah, the computer's not so bad. And then, as in the title, he is computer programmer as she is to homemaker. See, oh my God, that's what's in this dictionary, which is used in thousands and thousands of papers, and also, I talked to people who are using this in systems. So, first of all, people need to be aware that this embedding has a lot of bias in it, and they need to think twice about just using it wholesale in an application. And now, I'll pass it over to Jamie. - Okay, thanks, Adam. So I'll explain in a little bit more detail of how do we actually quantify and characterize what are the biases in the embeddings, right? It's going to be quite interesting, 'cause they show up as interesting geometries in these port vectors. And then, I'll describe how we actually leverage the geometries to automatically de-bias the embedding. So we developed three metrics, complimentary metrics, to quantify how much gender stereotypes is in (coughing drowns out speaker). So the first metric is basically, we're going to project occupational words that should be neutral onto the he/she axis, right? So to explain what this means, right? So, remember, that what the embedding does, is that it takes every word, and it assigns a vet to correspond to that word. So, in particular, I can take a word corresponds to he, and the word corresponds to she, and I can look at the line along that words, and that's sort of supposed to capture, you know, what the embedding thinks as being the difference is in the genders. And then, I'm going to project, like, about 300 occupations that are just supposed to be gender neutral onto that line in he and she, right? So each word here actually corresponds to (mumbling) embedding vectors, and the scheme of the projections. When we do this projection, something quite striking comes out, right? So the occupations that are closest to she ends up being things, like, you know, homemaker, and there's receptionist, right? So these are the words that are the occupations that the embedding thinks is closely related to the word "she", right? Remember, that this tense in the embedding corresponds to semantic similarities. And on the other extremes, we look at words that our occupations that are closest to "he", then I have these different sets of occupations. All right, so that's quite striking. So what we decided to do, is to make this really systematic, right? So for each of the occupations, in parallel, we're going to ask, go on Amazon Mechanical Turk, and we ask crowd workers to rate each of the occupations as to how much male of female stereotype is typically associated with said occupation, right? So we do that for each of the occupations, and then we can compute sort of a correlation between its values on the projection of the he/she axis, and what humans think is being the stereotypes of these occupations, and there ends up being a very strong correlation, right? So this shows that the geometry of these embeddings, at least for occupations, seems to really capture human notions of stereotypes, gender stereotypes. So there are thousands of papers, as Adam said, that looks at different embeddings, and different methods, right? So we looked at some, and we're interested in seeing something that's sort of consistent across different embedding methods, and ends up that this change of stereotype is really consistent across different embeddings trained on different documents. So, here, I'm showing you, like, sort of two of the most commonly used embeddings. So Word2vec embedding is trained on a large corpus of Goggle News documents. That's the X axis. And the Y axis corresponds to a different algorithm. GloVe, that's trained on a very different corpus, right? So each dot here corresponds to one of the occupations, and the axes corresponds to, again, the projection on to the he/she axis in each of the embeddings. So you can see that there's actually a very consistent stereotype agreement across the different embeddings, even though they're trained on different corpus using different algorithms. So the occupation projection is quite informative, and it's easy to be interpretable, but it seems restricted to a particular set of words, right? So we want to do something that's much more general that uses the entire embedding set. So the idea that we have for the second metric is to automatically generate analogies from the embedding. So here's how it works, right? So we look for analogies of the form, he to X is as she to Y, right? So what makes a good analogy? As Adam's saying, so the analogy here then corresponds to these parallelograms in the embedding space, right? So what he means, is that, you know, he to brother is as she to sister. It's a good embedding by the, you know, it's a good analogy according to the embedding, because the angle between these two vectors is very small, right? So we can formalize this as the optimization problem, and just ask the embedding to automatically generate what it thinks is our good analogies for this he to x, she to Y. Notice, here, that different analogies are generated. It's being good analogies. And, again, we go back to the crowd, to ask, "Okay, so for each of the enologies, you know, how much does it exhibit gender stereotypes according to the crowd workers?" And, again, we felt that about one and five of the generated analogies were rated as strongly gender stereotypic according by a majority of the crowd workers. So there's a lot of stereotypes systematically encompassed in these embeddings. So the third metric we develop is a more subtle kind of a bias. So the idea here, is that even if I look at words that should be gender neutral, right? Their relative distances, and their geometries could be affected by gender stereotypes. So here's an example of this, right? So instead of projecting on to the he/she axis, let's project the occupations onto the softball/football axis, right? No, softball and football really, in principle, should be gender neutral. People of both genders play these sports. What you find here, is that, you know, occupation, like receptionist, is actually very close to softball. It's not because that receptionist is, you know, the occupation itself is very closely related to softball, right? So that happens because, you know, there's a strong female component according to the embedding that's behind both receptionist and softball, right? And similarly, you know, with maestro and football, I don't think they're actually occupationary related. It's just that there's a strong male component top it, right? So there's just subtle kinds of indirect or secondary bias that shows up in the embedding. Okay, so how do we, now that we can quantify the bias, and then, finally, how do we go about try to de-bias this? The idea here is that there's actually a lot of interesting geometries of a scene that's in embedding, and we want to leverage that geometry. So one kind of geometry that is quite useful, so we've seen, you know, one of the nice properties of embedding, of these word embeddings, is that quite complex, higher order semantic relationships are actually captured by relatively simple linear operations, right? So this led us to conjecture, that if you can find a linear operations, what would be a linear sub-space that captures a lot of these gender components and genders stereotypes. So the way that we end up doing this, is to start with a set, pairs of words, right? That really captures gender opposites. So here are some of the pairs of words. So if I take a particular pair, like man and woman, then, you know, that definitely captures quite a strong gender component, but maybe there are also other aspects to man and woman that's not independent of gender, right? But the hope is, that if I take a lot of these different pairs of words, then maybe the shared directions, or a shared sub-space that's common across different pairs, tends to capture more of the true gender component, right? So operationally, that means basically by looking at principle components across these different pairs of words to see what are the sub-spaces. And when we did this exercise, it's very striking that there's, you know, one very clear direction, you know, one dimensional sub-space that seems to really capture a lot of the differences between these pairs, right? So we use that as our gender sub-space, the first principle components. Okay, so then the strategy that once we identify this gender sub-space, it's very intuitive. So I'm gonna give sort of the simple version of our algorithm. So the idea here is that, let's first identify words that we think should be gender neutral, and words that should be gender definitional, right? And then, on the words that we believe should be gender neutral, we should project away its contribution from this gender sub-space. We should remove that all together. And then, the last step is to sort of normalize these vectors, so that they're useful for the embedding, and its embeddings. Okay, so how do we find words, or, no, that are gender specific, right? So this is quite a subtle question, and there are different version of this, depends on the application. But as a first approximation where we took, where there's strict approach to doing this, right? So we say, "That we're going to identify the words "that are, by definition, associated with one of the genders", right? So there ends up being a roughly small number of those words, and we're just simply going to say, "That all the other words, the rest of the words should be gender neutral", right? So this is quite a conservative definition that we want to use here. So we can identify the gender's definitional words, either by hand, since there's only a small number of them. Actually, turns out that we actually can develop sort of a SVM, a linear SVM, that identifies those words, right? So it turns out, they actually live in a different part of the embedding space from the other words, all right? So that's definition, as Adam said, is already encompassed in the embedding itself. Okay, so now that we identify which words are gender neutral, and which words are gender specific, what we're going to do, is take all the words that are gender neutral, and it's going to remove their gender contribution, i.e., project away the gender sub-space, right? So there are gonna be claps on the gender sub-space. They have no longer any gender contributions, but they still have these 299 other dimensions to explain all the other interesting differences between these words, right? So these are 300-dimensional embeddings. And we're not going to change the words that should gender definitional. All right. Okay, so there's sort of intuition behind the algorithm, and then there's sort of a slightly more sophisticated version of the algorithm that does this in a more subtle way, right? So don't worry about the particularly questions here. I mean, the main idea against the algorithm for de-biasing is to simply balance these two objectives, right? So the one objective is that for the word to that gender neutral, we want to reduce its contribution from the gender sub-space. At the same time, we don't want to move, and change the words too much, right? We want to preserve its original geometry as much as possible, 'cause there are still interesting utilities from that geometry. So operationally, that turns out to be this particular semi-definite program, and we have techniques for solving this quickly. Okay, so here are the results from the de-biasing, right? So the first plot here corresponds to the number of stereotype analogies (mumbling) by the embedding. All right, so the X axis corresponds to, just ask the embedding to generate more and more analogies, and the Y axis, we go to the crowd, and ask, "Okay, so which of these analogies shows strong gender stereotype?" So the blue curve corresponds to what was happening in the original embedding, right? So, you know, as you generate more analogies, sort of a large fraction of these are becoming, shows up to be gender stereotypic, and the green curve corresponds to what happens after the embedding, right? So as you see, that we effectively greatly reduced a number of analogies that are judged as to be stereotypic by the crowd workers. At the same time, we also want to track that the embedding is still useful, right? It's still generating useful analogies, or appropriate analogies. And you can see here, is that basically our algorithm does not, even after doing a de-biasing, we're still generating of the same number of useful appropriate analogies as is before. And we had a lot of other metrics to show that, and based on word similarities, et cetera, to show that this de-biasing operation that we have, and by projecting with the gender sub-space, it really does not affect the overall utilities of the word embeddings. You can still use them for a lot of the tasks you want to do before, and now, key, but it is very effective in removing the gender stereotypes. And it's also effective, actually, in reducing this more subtle indirect bias, right? So we saw before, that, okay, so here, in the original embedding we see this strong indirect bias, and in the de-biased embedding, in this example, so now we no longer have this association between receptionist and softball, instead we have major leaguer and softball, which is a much more reasonable, appropriate association, right? So this de-biasing operation also is effective in reducing a secondary indirect bias. I just want to quickly mention, that while here, we focused on gender stereotypes, there are lots of other very complex stereotypes. Ethnic, or, cultural, religious stereotypes that all shows up as, you know, as different geometries in the embedding, and I think there's a lot of interesting work to be done in trying to characterize these other stereotypes. And here, I'm showing you so a few examples that we found in particular for racial stereotypes, how that shows up in these embeddings. Okay, so now, going back to Adam's slide, right? So what we're really trying to do here, is that these embeddings are really used as dictionaries for algorithms and for machine learning programs, right? And what we're trying to do here, is to come up with a technique that produces as much as possible a de-biased or unbiased dictionary. So that's, you know, the program, this sort of propagates these stereotypes down for it. So it's still quite an early stage work, and there's a lot of interesting point of discussion here. So one question is, right? As we're thinking about this, we just think about, like who is really responsible for these different biases? You know, is it the data itself, the original corpus, the training sets? Is it, you know, the particular embedding learning algorithms, or is it actually the downstream machine learning application users, right? So our feeling on this, is that, I mean, so there are a lot of subtle biases that's in the original embedding corpus, right? So if you just take even, take the "New York Times", or take Wikipedia, so there are subtle biases as a form that maybe he does tends to concur more frequently with doctor, and she tends to concur more frequently with nurse. What the embedding does, is that it takes these subtle biases that's in millions, hundreds of millions of documents, and it really crystallizes them by compressing everything to these 300-dimensional spaces, right? And by doing this, you know, compression, so it really crystallizes, like what are the biases, in its original corpuses, right? So there's a lot of responsibility to be sure to cause all of these different actors. The second point here, is that we try to argue that these embedding algorithms, because they're so effective in capturing these biases, that actually gives us a way to automatically de-bias these embeddings using its own geometry, right? So here, the geometries we see are of the form that maybe there are linear structures, linear sub-spaces, that captures gender stereotypes, and also captures what it means to be definitional versus neutral words. And this third point really goes back to the heart of this workshop, which is that, in these embeddings has this funny feature that they are, in some sense, very transparent, and quite interpretable, right? More interpretable than a lot of other machine learning algorithms, which is one reason why people like them. And, you know, for this particular setting of stereotypes, interpretation has a double-edged sword in that allows us to also quickly identify and visualize what are the gender stereotypes that's in these embeddings, and this makes it actually easier to debug these embeddings compared to a lot of other machine learning algorithms. And then this leads to the last point, so how should we actually think of this word going forward if you actually use it for applications? So I think what we try to argue here, is that whenever you're using these embeddings, you know, as well of the inputs into your machine learning applications, you should, in parallel, take the inputs are de-biased embeddings, right? And it will be a good setting we check to see if the set actually, you know, if you run these original embeddings on the de-biased embeddings, if you run your algorithm down stream application forward, does it actually make significant changes in the outcomes, in the results? Like, if the outcomes, you know, if the resume rankings, or the search rankings are very different, that should be a warning flag that maybe there's something going on, and maybe should go back to your original embeddings to try to de-bug it, right? So we're not saying that you have to use the de-biased embeddings, but we think that it is useful as a setting, you check to see, to run it in parallel to see how much different does it make in your downstream machine learning applications. Great, thanks. (audience applauding) - If we people wanna ask a short clarifying question, now would be a good time. - [Audience Member] Have you talk, very nice work, but I was wondering, have you talked to any trans-person during this research? And there is a deeper issue here, right? I mean, you are starting from some categories that you take for granted. You are starting from some definitions of what is biased, what is not biased, and what's in that, what's in these assumptions, and what consequences does it have? - Yeah, I think that's a great question. I mean, so here-- (male speaking incoherently) - [Male] So do ask questions, please, for the live stream. - [Audience Member] Yeah, okay, and see if you just, really-- (speaking incoherently) - So a question is since, so, here, we talk about for the (mumbling) male/female biases, have we looked into, so their transgender issues, and more complex situations? So I think that's a good question. So what we've tried to do here is to just give sort of a first-pass look at this issue. It's definitely a really complex problem, and as I mentioned, so there are lots of ongoing work to look into the more, you know, more fine-grained, and more subtle types of biases, including with transgender and those of racial biases. But for this particular work, we're just only looking at this fairly binary bias, but it is a good point. - [Lucia] Hi, my name is Lucia Summer, and I come from a criminal law background, and I was wondering if, like, the basic notion of your research could also be applied to de-biasing algorithms in a criminal law context? For example, the recidivism prediction that was mentioned in the opening panel, the ethnicity bias that is in the recidivism predictions, could it also be used for that, to de-bias that? - Good question. So a difference between what we talked about, and a lot of other work, is that we're talking about de-biasing at the word level. We're de-biasing this sort of dictionary representation, which could be used in one of those applications. Usually, if people talk about de-biasing, the outcomes, the predictions themselves, and looking end to end, and we're talking about this middle ground where we actually try to de-bias the dictionary itself. - Yeah, so it is a good point, that if there are algorithms that's, you know, doing maybe crime prediction, or recidivism prediction, that uses the input, these words' features, I don't know if they're algorithms, but I say, if there are these algorithms, then I think it is important to see what would happen to those predictions if you used a de-biased embedding versus the original embeddings. - All right, I know there are a ton of questions in the audience, but in the interest of keeping us on schedule, I'm gonna hand things over to Arvind Narayanam, who's also going to present about bias in word embeddings, and Arvind take it away. - Thank you, Josh. Here's why I'm super excited to be here today, often in science, there are simultaneous discoveries, and then people fight about who gets credit. But sometimes, more interestingly, two groups of people look at the same data, and make the same or similar observations, and come to very different conclusions about what this means, and how we should respond to it. So I think we have a case of that here today, and that's actually a really good situation to be in, because in the process of figuring out how to reconcile these diversion views, you end up discovering something new that neither group individually discovered. And so, that's my hope about what's gonna happen today, and in the ongoing conversations that I think this community is going to have about these findings that I'm gonna share with you in relation to the findings that you've just heard about. I wanna say quickly, that Aylin is here in the first row, if you wanna wave, and she's gonna be up here during the Q and A as well. She's the lead author. You're welcome to address any question to her as well. So we think de-biasing that they've presented is a very cool algorithm, and it's going to be very important, and it's very interesting, but we think it's not really going to solve the problem, and the difference in our perspectives comes from, what we're arguing, is that what you're seeing here is not specific biases, like gender biases or racial biases, although, that is certainly true, but as I'm gonna show you, machine learning and by being essentially the entire spectrum of human biases, and I'm gonna show you a rigorous way of establishing that. And also, I'm gonna argue that there is no clear dividing line between what we consider meaning in the data, and what we consider bias, and in particular, unacceptable bias that we wanna get rid of. Further, as Adam and James mentioned, these word embeddings are in dozens of different systems, and I wanna argue that it really depends on the application, what we wanna do about these biases. What constitutes a terrible bias? A prejudice in one application might actually end of being exactly the meaning that we want to get out of the data in another application. That sounds counter-intuitive, but I'm gonna give you an example of that. So based on all of these, I'm gonna argue that we should think of these biases not as a bug in the system, which is the metaphor that you guys used, and it's an interesting metaphor, but I'm gonna argue that we should think of these as a feature of these models. And at first, hey, that might sound horrifying given some of the examples that we saw in the previous talk, but I will explain why we have that point of view, and that will lead to our suggestion for how to deal with these. It's not that we should de-bias, at least not in all cases. However, we have to start trying to build explicit components of AI systems that understand the consequent bias or prejudice, and have ways of mitigating it, instead of trying to eliminate it from the system. Okay, that's a lot of abstract stuff, but now, let's dive into the details, and let me explain why we came to this point of view based on somewhat similar findings from the data. Is it possible for me to grab this mic, by the way? That would be somewhat easier. - Sure. You wanna hold it up? - Yeah. So here are the research questions that we were interested in. Thank you so much. Why are word embeddings biased, and how biased are they? And those two first questions really occupied most of our attention. How should we address this bias as sort of a consequence of our research? But we really wanted to more rigorously understand bias, and quantify it, and so on. And I wanna point out, that the broader concerns that we're gonna raise here are not specific to word embeddings. For us, word embeddings are a tool to understand this concept. They're a messenger. They're a symptom of the problem. They're not the problem itself. So we think that de-biasing word embeddings is misplacing the focus a little bit of where our intervention should be. I'm gonna try to convince you that there's something deeper, more fundamental going on here, and something fundamental to human language, and into our biases as well, and how that's being reflected in machine learning systems. In other words, the hypothesis that we're proposing, is that essentially all human biases are reflected in semantics, and that's not an obvious point that these biases should even be reflected in the semantics of language, much less, in purely the statistical parts of language, right? And that's not a point even about machine learning. It's a point about humans, and culture, and language, and so on, and we're gonna see how our data presents evidence for that. And if that's the case, now it becomes easy to believe that inevitably these are going to be imbibed by machine learning systems, whether it's word embeddings, or any other kind of model of language that you're working with. So how can we do this? How can we make this connection between human culture, and language, and what's going on in machine learning systems? And the fundamental way that we do it, is by using what is called the implicit-association test, which is a well-known test of human bias in psychology and cognitive science, which has been established over the last couple of decades, and it's a way to understand, and even quantifying biases and prejudices that people have. And here's how it works. The concept is really simple. Imagine a test subject here who we think, we hypothesize, has the stereotype that boys are good at math, and girls are good at reading. So here's how the test works, and the subject is going to get a sequence of words, one by one projected on the screen. On the right, you see the sequence of words. And the subject needs to press the left button if that's either a math word or a boy name, and she'll press the right button if it's either a reading word or a girl name. And the idea here, is that if this test subject does indeed have the stereotype in question, subconsciously they may not even know about it, then it will be the case that this task, which is stereotype congruent, which conforms to those stereotype is going to be easier than the opposite pairing where you pair the boy names with reading words, and the girl names with math words, right? And so, if you do this test enough number of times, enough trials, what you're gonna find is that the reaction times of pushing the left or the right button is going to vary. And these differential reaction times, it turns out, they're not gonna be dramatically different, but you're gonna get two distributions, and those distributions are gonna have means that are separated. That differential reaction time is a measure of the bias in humans, and this is the implicit-association test. There are hundreds of papers on it. It's really well established in psychology and cognitive science, and the implicit-association test was the starting point for us. That's sort of the definition of what we mean by bias in humans, and we wanna take that, and try to see how well that applies to word embeddings as well. So the main contribution in our paper, is we came up with, what we're calling, the word embedding association test. We wanna come up with a test that mimics the setup as closely as possible. We wanna apply it to word embeddings. So that's what we managed to do. The statistics are in the paper. I'm not gonna go into them here. But roughly, the intuition is that these word vectors, you know, there are vectors in high dimensions, so you can compute the angles between them. You can compute the cosine similarities, of course. And it turns out, that those cosine similarities, of course, intuitively, they measure the similarity between pairs of words, and you can use that more or less in place of these reaction times that are measure by the implicit-association test. I should mention one caveat here. This is what makes the word embedding association test a little bit tricky, and not totally straight forward, which is that, for us, there is no notion of individual test subjects. What we're working with is the corpora of text found online, and these corpora, you can think of them as being the aggregate output of, you know, millions of individuals, and so, the test that we come up with make statements about language corpora on an aggregate basis, and not at the level of individual humans. That requires redoing the statistics a little bit, but we can do that, and we've managed to do that. There's also one quick thing I should point out about the implicit-association test, and why we think that's really powerful for studying these biases in machine learning systems, is that as a term implicit association implies the test is able to elicit biases, and prejudices, and stereotypes that the subjects may not even be aware of by themselves, and I know this from personal experience from the first time I took the test online in 2004, and if you take the test, many of you might find that as well. And so, that's a really cool, powerful concept. So we have, you know, hundreds of studies in psychology that have documented these in humans at an individual level, and an aggregate level. They've quantified it, and now, we can have an analog of that test for machine learning, and we can see how word embeddings perform when they're put to the test, right? So let me show you the details of that. Here are the data and the algorithms that we use. Now, we wanted to make sure that we were basically using pre-trained embeddings. We didn't wanna train our own embeddings, because, you know, as James pointed out in his slides, people are using these existing embeddings out there that researchers have put out in a variety of different systems, and we wanted to mimic that as closely as possible. So we used the pre-trained word embedding, oh, I'm sorry, by the GloVe algorithm, which is the Stanford Project, was a state of the art research project. It's pre-trained on the Common Crawl corpus, which is a huge corpus. We also repeated our tests with this other corpus, Word2vec trained on Google News. Very similar results. But the numbers that I'm gonna be reporting here and in our paper are based on GloVe trained on Common Crawl. Okay, without further ado, here are our findings. There's just one quick sanity check that I wanna do here before going to our findings, which is a sanity check that's used in the implicit-association test itself. In addition to characterizing biases and prejudices, the IAT is also used to characterize just universal associations that everybody has in our heads. Associations between flowers, and pleasant terms, and insects, and unpleasant terms. So that's something we can look at the implicit-association test results, and try to reproduce in our algorithms, and in our models. So you have a basket of flower words, you have a basket of insect words, and then you have baskets of words for pleasantness and unpleasantness. So in this, and each of the further studies that I'm gonna be showing you, we try to reproduce both the, we try to, you know, reproduce the same set of words that were used in the original implicit-association test, and we're gonna look at the effect size of the bias that measures how well these two distributions are separated, right? How much closer are flower terms to pleasant, than unpleasant terms compared to insects? So this is a universal association. Everybody has this. So in the original finding, you have an effect size of 1.35, which is considered a really huge association in psychology, and that's, you know, fairly intuitively obvious, and everybody considers flowers more pleasant than insects. And similarly, we reproduce a really huge association with word embeddings as well. So far, so good sanity check, and this is, you know, we have statistical confidence, P value is less than 10 to the minus seven. And now we get to the interesting stuff. We started looking for IAT results on racial bias, for example. And so, we were able to find studies with baskets of European American terms and African American terms, and the same basket of words for pleasantness and unpleasantness. What we found, was in the original study, again, a pretty big effect size. This is considered huge in psychology. And with the word embedding as well, we were able to reproduce that actually in even larger effect size here. A quick caveat that I wanna point out, is that it's not meaningful to compare the P values between the two, because the subject, so to speak, in the original IAT are the participants. Whereas in our test, they're not exactly the same statistical test. The notion of subjects is replaced by the notion of names than we have here, and so, the P values just have different meanings. But I think it is meaningful to compare the effect size, and the fact that these effect sizes are so huge is really telling us something, that these biases are really captured in the statistical properties of language, and I think that is somewhat surprising quantitatively, how big the effects are, and how well they're reproduced in these word embeddings. Next, we looked at stereotype biases in terms of gender. So we looked at female names and male names, compared that to family words, and career words. Again, reproducing what was done in the original IAT. Female names are stereotypically associated with family terms, both by human subjects, as well as by these word embeddings. Again, reproducing pretty big effect sizes here. Another type of gender bias, we're looking at science words and art words, and comparing that to these two baskets of words here. Again, finding similar things. So I won't keep boring you with these numbers. Over and over again we're finding the same things. We did even more IATs for age, religion, physical versus mental illness, and so on. Essentially, every IAT that we could find that used names in the original test, as opposed to images and what not, which we can't quite reproduce, we were able to reproduce their findings, both qualitatively and in terms of the magnitude of the effect size. So what I wanna claim here, the fact that this is happening across the board with all of the biases that we have tested, and it's pointing to something deeper. It's pointing to the following account of bias in AI systems. What it's telling us, is that human bias, at least as documented by the IAT, is captured in the semantics of language, and that is further propagated to the distributional meaning of words, and that is getting picked up by these mission leading models, right? So I think our results taken together provide support for each of these three implications, and this first arrow in particular, it's not even a statement about machine learning. It's a statement about humans, and how deeply our biases are embedded in language, and we think that it even has implications for human bias and prejudice. There is a part about that in the paper. I won't go in to too much detail on that. But it suggests that we're dealing with a deeper problem here. It's not going to be something that we're gonna have really an algorithmic fix for. Yeah, I'll skip what this is about. Okay, here's the part of our paper where we start to say more controversial things, at least in a contrasting to the talk that you just heard. Now, we can ask the question, how should we address bias in AI systems? And you can come up with several potential ways we can think of addressing AI bias. Just modify the training process, whether that's the training algorithm, or just, you know, the culture of the tech industry, what not. There's various things that have been proposed. All of those are certainly useful. This middle bullet is what we just heard, the de-biasing technique. Or this is something that's often something that you hear from Silicon Valley. You know, CEO's and engineers, when they're asked about these viruses, "Oh, it's just a problem with the training data". The bias is in the dataset. Humans are biased. They're gender stereotyped, and that bias is reflected in language, and we're picking it up. When humans are in this perfect, unbiased state sometime in the future, then our models will also no longer be biased. It's not our problem. Right back to you, it's your problem, right. And I'm not making this up. There are quotes about this. This is, "What machines are picking up on", this claim goes, "Are not facts about the world, they are facts about the dataset". The real world is unbiased, but humans, in the way we speak, are racially, and in terms of gender, we're biased. So goes this claim. Let me give you evidence that this is now, this is an oversimplification, that even completely factual, real world information about our world today is going to be expressed as biases and prejudices in these machine learning models. And the way I'm gonna show this, I'm gonna compare this to a slide that you saw in the previous talk, which said, "If we look at crowd workers on Mechanical Turk, "elicit stereotypes from them, "and look at what the word embeddings are doing, there is a corelation of .5". Let me put a twist on that. What we decided to do, is we looked at not how biases are expressed in Mechanical Turk workers, but just purely factual information about the world. We looked at government statistics of 50 different occupations, and what percentage of workers in each of those occupations are women. This is released by the Bureau of Labor and Statistics. We looked at that data, and we decided to compare that to how gendered does the word embedding think each of those occupations is, and we put that on a scatter plot. You saw a corelation of .5 earlier. Look at this corelation over here. We were astounded. We were expecting a corelation, but not this much. This is .9. So let me explain again what you're seeing. On the X axis is purely fractal data about the real world, the percentage of workers in each occupation, doctors, nurses, programmers, so on, who are women. And the Y axis is how close the word embedding things is each of those occupation word vectors to feminine word vectors, as opposed to masculine word vectors, right? So forget about the ethical implications for a second. Just from a technical perspective, this is so cool that this is, you know, government data. This is from a 1990 dataset, and with a corelation of .9, the word embedding is able to infer that purely from the distributional statistics of language from uncontrolled text found on the web. I mean, there are so many reasons why you would expect a worse corelation. This is text found all over the web, from all countries. Whereas the X axis is only US data. So that's gonna decrease your correlate. In spite of all that, you're getting a correlation of .9. This is crazy just from a technical perspective. But from an ethical perspective, this becomes problematic, because now you can see very clearly that whether we consider this to be biased or meaning purely depends on how we wanna use these model in our applications. If your application, for example, were to look at historical job ads, and characterize whether more men or women worked in those occupations, this is exactly what you want. It would be a shame to throw this away. On the other hand, if your application were to look at resumes, and make decisions about the future, of course, this would be a terrible thing to have in your program doing, right, right? But at a fundamental level, this is our history, and this is our reality today, and I don't think we should edit it out of our machine learning models. It does reflect something that is there in our world, and our response should be instead to find ways, and it's not obvious how to do this yet. It requires a new research program to find ways to teach our AI systems, our machine learning systems, what this means, you know, what types of biases are acceptable and unacceptable, and to have explicit ways to mitigate this, and compensate for it in specific applications. And just one other thing super quickly. Just to emphasize that that's not a fluke, you can find these associations with all kinds of other things. Here's another one. This is names that are given to both boys and girls. So on the X axis you have, again, from statistics, percentage of people with the name currently in the US who are women, and, again, it picks that up pretty well just from the distributional properties of language with a correlation of .84. So to reiterate why we think this is a really tricky problem, and something fundamental that we're not gonna easily edit away, is there are several types of things that we're calling bias here. One, is just universal flowers versus insects. Totally unproblematic. Another middle category, clearly problematic, such as black versus white names. And a third category is things that are, you know, patterns in the real world can definitely be very problematic, but also can be really useful meaning in the data. So you have a confused robot here, which has no way of distinguishing between all of these three categories. And in particular, what we term to be prejudice or unacceptable bias, there's gonna be a cultural notion. It's not fundamentally an algorithmic notion, and it's gonna be evolving over time. Different cultures have different versions of that. And the same phenomenon on the data, depending on your application, can be acceptable or unacceptable. And finally, just factual associations of the data could be manifested as bias or prejudice, depending on how you use it in your application. So that's why we think this is a really tricky problem. So to address the de-biasing technique more specifically, again, I think it's a really cool technique that you guys find this direction in the dataset that actually captures the notion of sexism. I think that is a very valuable tool we should have in our, you know, set of tools that we use to think about this problem. However, I think there are important limitations. At a high level, what de-biasing is doing is it's altering the AI's model of language. Its perception of the world, right? Because that's ultimately what word vectors are. It is a dictionary, that's one good way to think about it, but it's also a perception of the world, that is also another way to think about it. You know, if you're de-biasing a word embedding, you're affecting something that's used in such a variety of applications in very different ways. And the technique that we heard about earlier relies on this particular geometric interpretation, right? It's not clear that it's gonna generalize to all kinds of biases that we're seeing in the data based on the implicit-association test. So what we're proposing, we don't have a concrete way to do this, I should say that up front. This requires, you know, new research programs. It's not a very comforting answer. It's not something that's gonna happen tomorrow, but I will show you on the next slide something that could happen at a more immediate timeframe. But in the long run, we're saying, "That we need to think about designing these AI systems "in a way that we alter the action of the AI, not the AI's perception of the world". That's the key distinction we're making. And the good news about this, is it's actually very similar to what, again, the psychology literature tells us humans or ourselves are very capable of doing. We know from AIT studies that a lot of the time people don't necessarily act on the implicit associations, even strong implicit biases that they have, and maybe something like that should be applicable to AI systems as well. And so, this idea of a separate component that's capable of explicit instruction, especially in a non-statistical sense, using symbolic reasoning is an idea that's totally gone out of favor in AI, right? And so, maybe this is one reason to consider reviving those lines of research. It's, you know, a controversial point, but I have something that I wanna propose to you. And I just wanna give you a quick example of how, you know, of a particular application, how these biases manifest, and also, how we can think of tailoring out interventions to those specific application. You can try this yourself. This is Google Translate. What you can do, is you can pick a language where the pronouns are not gender specific. And this is Turkish. We picked Turkish because Aylin, our first author, is a native speaker of Turkish. And so, you can put in "o bir doktor", for example, and O can be either he or she, and when you try that in Google Translate, it's gonna come out gender stereotyped, "He is a doctor", every time. You try the word "nurse", it's gonna come out, "He is a nurse". And then, "He is a professor", "She is a teacher". If we tried this with that entire list of 50 occupations, we could almost perfectly predict what Google Translate was going to do based on our word embedding association test of what the embedding model considers the gender stereotype of that word to be. So what do we do? It's not obvious how to address this problem using de-biasing. One thing that might happen if you try to do that, is it's always gonna come out "he". You know, you're replacing one kind of bias with another kind of bias. Perhaps, though, perhaps, and this is gonna be a controversial suggestion, perhaps what should be done in these applications is that you should have a separate component of the system that has the intelligence, whether through machine learning, or some type of symbolic instruction, to understand that these words here have come out gender stereotyped, and to force the human into to the loop, to ask the human user, "Hey, look, these are potentially stereotypical, "you know, gender stereotyped translations of these gender neutral sentences". You have to pay attention to this, and figure out how you wanna translate this. So we think a lot of the potential solutions to these problems of bias and prejudice are gonna introduce more friction, and the systems are gonna take away from our ability to automate all of these, and that's, obviously, a limitation, but that's something that we should confront, and maybe that's okay, and maybe that's a good thing to have in our systems. It forces us to think about what these systems are doing when we're using these systems. So to conclude, what we wanted to do, is, you know, we wanted to understand the problem deeper, and we wanted to provide an account of how and why bias gets into these AI systems, and we did that by looking at the implicit-association test, and we think even though all of our experiments are based on word embeddings, what's ultimately going on here is something more deeper than that. It's something that really affects, it's going to affect, you know, all of the sophisticated models of language that we're gonna build in the future. And perhaps we have to accept that it's going to be inevitable that these biases are gonna creep in, that we're not gonna perfectly algorithmically distinguish between what's acceptable to have, and what's not acceptable to have. And perhaps we need a new research program that seeks to compensate for these biases, sees them as features of these word embeddings, instead of bugs in the word embeddings, and learns to, you know, deal with this by compensating for it explicitly, instead of eliminating it from the system. Thank you very much. (audience applauding) - Thank you, Arvind. I'd like to invite Aylin, and also Adam, and James back up to the stage for just a couple of minutes. If you have a question, we're gonna take a few minutes just to sort of have these papers respond to each other. If I could you if you have a question to come to the center aisle, and use the microphone just so that people on the live stream can also hear. And in the interest of time, I'm going to have everyone go through, and ask their questions all at once, and then have you guys kind of respond to each other. And since Arvind has just talked, we'll start with Adam and James when we get to that. So if you have a question, please come to the center microphone. I'll start it off by asking, Arvind, you are proposing that we have a new research program for how to treat biases and word embeddings as features, rather than bugs in the embeddings, and I guess my question is, how would you define the success for such a program? Like, you gave some reasons that you thought that the approach of the de-biasing algorithm was not correct, and I'm curious of those are general criticisms of any de-biasing effort, like how you would define success for something that actually does compensate differently from the way the de-biasing algorithm works? Other questions, fresh. - [Audience Member] So one question I had was, I think the framing here is a little bit, at least from maybe Arvind in your presentation about this whole idea of competition elimination altering action not perceptions, I quite like. But I think there is, given that the goal in many of these systems is to automate as far as possible, and this is naturally in conflict with a more integrated human in the loop process, where the machine isn't. So I think the question here becomes, you kind of want both, but now you have a pipeline of systems that are now plugging into each other, and either you have humans at every step of the way, which might be a reasonable solution to sort of inspect and decide, or you have to have some way of arguing about the way these systems propagate, pushing an automated set of biases through, and that at various points in time, a human injecting into it. So I guess my question is, do you really, how do you see, I mean, I personally think that these are not necessarily in as much an opposition as you think, and so I'd like to know from all of you, how you think about how they might interact with each other profitably? - [Audience Member] So, this question is primarily for Arvind, but I'd love everybody's input put on it. So when you talk about blaming the problem, where we go blame the training data or what, my question with that always is, well, does the system, in particular, make the realities that are reflected in that training data better or worse, to the extend that we can agree on what better or worse is? And this seems like a particularly hard case for it, because we're dealing with human language, but do you have any thoughts on how the kinds of language related AI systems we build may exacerbate or possibly mitigate the biases that arise from and/or cause the bias that we see in the language itself as a way that we get this feedback loop? Because our systems are gonna have an impact on the people that make decisions and respond to them. (laughing) - You just sort of swat down too. - [Audience Member] Hi, sorry. I'm just wondering, Arvind, looking at Turkish as a language, as opposed to English, I'm wondering if the results, if it's a less gendered language? I don't know if it is. I know at least the pronouns apparently are. Mean that the associations that you find in general are less gendered? And I'm wondering about something like Hebrew, where even all the verbs are gendered, whether there's some ultimate effect in the kinds of associations you'll see? If you've done any of that research, or there are thoughts on that? - I'll just quickly say one sentence in response to that, and then I'll give over to you, Adam. Certainly, the big next step, one of the big next steps we wanna get to, is look at a variety of different languages, and models trained in different languages. We haven't gone there yet. I do know that Aylin has done similar tests that she did for Google Translate in a variety of other languages. I don't recall off the top of my head the results of that. I don't know if you wanna say something about the test that you did, or? (mic thudding) - Oh, so we tried most of the languages that have gender neutral pronouns, and based on that the results are very similar to Turkish. But for other languages that have more gendering formation, for example, Hebrew with gendered words, we haven't looked at those, but this is a very good direction to see if it is augmented in such languages. - So let me try to address some of the questions, and also, this work, which I think is great, and it's much more thorough in some ways than what we did, which was just look at male/female, and you look at racial biases, and things like that. But I still think using the dictionary analogy, think of these word embeddings as a dictionary, so what I would take away maybe from your work is that maybe we should have different dictionaries, just like there are, you know, legal dictionaries. In a legal dictionary the word grandfather might refer to grandfather of law. Whereas in a general dictionary, the first definition of grandfather might be is, a parent's father. So I still think think there's a role. Since tons of people in machine learning are using these things, and just downloading them wholesale, and we can't stop them, there's a role for producing less biased embeddings that have fewer biases, and will hopefully improve the downstream performance. At addressing one of the question was, you know, this won't solve the problem, but it will help. Also, comment that Max Larsen there is kind of doing something at the merger of these two papers where he's looking at names as a way to try to remove biases, not just male/female, but try to de-bias things, both in terms of names, age. You know, Marilynn is an old name. My mother's named Marilynn. My daughter's name is Myah. Age, race, all these things, you can try to get a whole swath of biases at once. - Great, right. Thanks Arvind, for the really interesting talk, and I really liked the paper too. So I think our work, and Arvind and Aylin's work, it's actually quite similar in many regards, right? So the things that we agree on, aside, I mean, first, it's really important to point out that these embeddings have very strong genders stereotypes and other kinds of biases, and we developed complementary ways to do that. And the implicit-associative test is a really nice way to do that as well. I guess, I will just say, is that from seaming from an engineering perspective, right? So it's really important to, you know, point out that there is this problem, and to set forward the gender for a really interesting research direction, but we also want to have sort of engineering algorithmic solutions, right? So I'm at Stanford, every week I have, like, 10 startups that comes that's downloading these word embeddings, and using it as one part of their pipeline, right? Maybe that's the input into their pipeline. They're not thinking very carefully about, like, what are the, you know, the downstream applications of this, or the downstream implications of this, and they definitely don't have the resources to go in. I have human that I redo how this pipeline process, right? So what we'd like to say to them, is that, "Okay, if you're gonna use this word embedding "as a part of your pipeline, then you should at least "try this slightly less biased version "of the word embedding, and see if your results change", right? If you're doing recommendation systems, or their favorite app for their startup, let's just see if the results change, and if the results do make a big difference, depends on which version of the embedding you used, then you should go backward and try to figure out what's causing the problem, right? Maybe in the specific application that they care about, maybe that's not what they're interested in, but there are applications where, you know, you could have really, as we seen, you could have really detrimental downstream affects, right? So I think that our two strand of work is actually quite complimentary, in that, I mean, we both agree that there is this, definitely a need for a research agenda to point out these problems. There are thousands of papers published on word embeddings, and none of them actually say anything about these problems. But we would like to say, like in addition to having just an academic discussion, one thing that's really needed, especially for the industry application, which is going on, you know, everywhere now, is to really have engineering solutions to this. - You're talkin' about engineering solutions, but, for example, with the crowd sourcing part, how are you planning to iterate over that each time? - Yes, so that's a good question. So, I mean, so that is actually, well, we just have to do ones for our project, right? So we have, you know, released our, so we had to do that to validate, that's our de-biasing algorithm is, you know, it's effective by some metrics, but once we've done that, then we have released our de-biasing algorithm, or actually, the de-biased embeddings. So that's, you know, any of these startups, if they're using this, they can use it exactly the same way that are currently using it, or to Vec or GloVe embeddings. They don't have to redo the, we run the algorithm, or we run the human experiments. - And that's only for the gender biased? - That's right. - Right, well-- - So far it's for the gender bias, and as Adam said, I think Max and others are working on other kinds of biases as well. - I'm gonna let Arvind say one sentence, and then that's going to finish up this session. If I could ask the next paper, I think it's Elisa's coming up to present, just to get ready, and Fernando has his laptop? - I do think there is something we can extract from all of this that I think all of us vehemently agree on, which is most of what we're saying can be framed as tools for software engineers to test their systems, understand what's going on, and perhaps that is something we can start building right away. Both our techniques and your techniques can be framed in that way, and that can lead to just more data on how these systems are behaving, and that can lead to further conversations, and ways to mitigate these problems. (audience applauding) - Well, thank you to both sets of authors. Those were fantastic papers. Up next, we have Elisa Celis, who is going to talk to us about how to be fair and diverse. I should mention, the next three papers were contributed papers submitted to this workshop, and chosen competitively by our Program Committee. So I'd like to take a moment, and thank our Program Committee, the first one we've had for this workshop. It's very exciting. And the papers can be found on the conference website, which is FATML.org. So with that, I'm gonna give you the microphone, and, okay. - Okay, thanks. Yeah, it's great to be here, and it's been a wonderful set of talks so far. So thanks to the organizers and everyone for putting this together. So the paper I'll be speaking about today is on how to be fair and diverse, and Amit and Nisheeth, two of my co-authors, are here, and this is also joint work with Tarun from MSR India. So to be clear, as we were talking about all of these things, there's many different ways that we can think about fairness and diversity, and they can be a bit convoluted sometimes. So just to give you an idea of how I am thinking about, or how we are thinking about these things, if when we think about diversity, what we're thinking about is data that shows differences. So you're looking at different or varied data. So if you're looking at some of these vectors, you can think of vectors pointing in very different directions, and if we have a collection of such data, we think of it as being diverse. When we're talking about fairness, what we're thinking about here is really a protected class or protected attributes, and we want data that is impartial towards these attributes, whether that be gender, or race, or any of these other things. So on a high level, this is what we're thinking about, and the specific problem that we focus on in our work is a sub-sampling problem. So you can think of some dataset, in this case, images of scientists, and what we want to do is just choose some subset of this data, in this case, a subset of size K, and this is a really basic problem that is quite fundamental to various aspects of machine learning. So very often, we want to do this just as a summarization task, right? If you search, say on Google Images, Google still needs to choose some subset to present to you. So it can be and end goal, and this is primarily what we focus on today. But the training goal is also very important. So this data, or this subset of data, you would presumably feed into some algorithm that then does something with a downward line. So for both these applications sub-sampling is very important, and both fairness and diversity end up being crucial. So in order to discuss what, more formerly, how we're going to talk about fairness and diversity, we really take this dataset, and we're going to look at it from two very different perspectives. So one perspective, on your left hand side, is an attribute space. So in this case, just as an example, we have our images, and perhaps, here, we can label them as male and female. So this is just an attribute space. On the other hand, we can think of them, as you saw in the previous two talks, we can look at them the way a computer looks at them, as high-dimensional feature vectors. So they're just vectors in some space. Now, looking at it these two ways, we can take our subset, and now, the question is, how do we measure the fairness or the diversity of this particular subset? So if we're looking at the attribute space, a very natural way to do this is to just look at your subset, and look at the fraction of that subset that comes from each of your different attributes, and you can take the entropy, which is just a standard measure of how different these are. So if you have an equal proportion of each of these attributes in your subset, that's when entropy is maximized. So we think of this as a combinatorial measure of diversity, or as fairness, and this is a very old notion. It's been used all over the place in he sciences, and in other places, ecology, and whatnot. Now, if we look at our vector notion, how do we think of diversity that way? Here, we think about a geometric concept. So you can look at your vectors, and you can think about the space that these vectors span out, okay? And you look at the volume of that space, and intuitively, the large the volume, the more diverse, the more different these vectors are. They're pointing in different directions effectively, right? So formally, if you look at the matrix, if you have a subset S, and you look at the matrix, where each column is your feature vector, and you take the determinant of that matrix times its transpose, that give you, basically, the squared volume of that space. So this is a concept that's very well studied in ML. It appears all over the place, because it does give you that sense of how different the point in your dataset are. And one of the reasons why it's been so successful in machine learning tests, is that we know that, okay, so one of the reasons why it's been so successful in machine learning tasks, is that we know that we can actually sample from this type of set, right? So you can choose some subset proportionally to its volume. So if you have some set that's very diverse, spans a lot of volume, the probability that you choose it will be much large than the probability that you choose some set where everything is very similar. Now, on the other side, it's very easy if you don't care about this vector space to just sample points that satisfy whatever constraints you want on these sub-classes, which is look at each sub-class separately, choose however many points you want from that sub-class, and stick them together to find some set. Okay, so these really are two notions of, or two metrics of fairness and diversity. Now, it might be a little bit more clear that if we have some subset that is fair, right? That is equally distributed among these classes, that doesn't necessarily say anything about the geometric diversity, right? But we could potentially hope that maybe the geometric diversity is capturing some of these attributes, and still managing to be fair. So our first question could be, basically, does this geometric diversity capture fairness for us already? And just, I mean, looking at a toy example of some of these, it's clear that they're really quite different things, right? The middle row is maybe the more obvious one. We can have something that is very fair, at least with respect to these male/female partition, right? We have two female and two male scientists, but these images are all very similar looking, right? And when you map them to that vector space they'll all be also very similar, having little volume. On the other hand, we can have very highly diverse, in terms of the vector space. The top row, there's images that look very different, but in this case, they're all male. And what we really want, ideally, is this bottom row, where we have that symmetry. We have very different looking images that are also equally distributed across attributes. So here, our question is really, can the two coexist? And clearly, they can on some level, but more fundamentally, can we find such subsets? And why do we want to do this? Again, if our end goal is the data summarization, we would to present, again, on something like search, varied items, very different items that are also unbiased. And if we look at the training application, there, we want to sub-sample data ideally in order to reduce biases down the line. If we hand our machine learning algorithm some data that has already been unbias, the hope is that the end result will also be unbiased. And the reason why, especially for this application, you also want the geometric diversity, is that this is what really helps your machine learning algorithm be robust. They need to see many different examples so that it can learn. Okay, so how do we do this? So we already said that if all we care about is the combinatorial or the fairness, that's easy to do, and if all we care about is the geometric side, we know how to do that as well. But how can we combine both, so that when we get our sample it has both properties? So there's two ways we can do this. The first, or more obvious way, is to perhaps look at each of your attributes separately, and within one attribute, you just do your geometric sampling, right? So here, I'm just going to choose two equally sized subsets according to what's called the DPP. This is your geometric sampling process. And that will give you some sub-sample that is guaranteed to be fair, and ideally, it's still quite de-biased. The other slightly different approach we can take, is do our DPP, our geometric sampling, and now, all that we're going to do is condition only select sub-sets that satisfy this equal split among classes. So we're going to choose one thing all at once, but conditioned on satisfying whatever fairness constraint we want. (audience member coughing) Okay, so these are two slightly different ways, the approaches that we can take. As we said, the first one, it's just putting the two easy things together. So we know that we can do for sure. One could ask about the second version, is it still efficient? Can we still do this effectively? Because now we're adding some additional constraints to the sampling process. So this is not very clear, but what we've shown recently, is that, actually, you can do this. You can satisfy these constraints. So this kind of leads back to a point Cynthia made earlier, what is the cost, or the computational cost of achieving this fairness? And in this case, we can handle it. Okay, so that's good. We know how to sample in these settings. The question is, is this really doing what we want? Are we really getting some outcome that is reasonably fair and reasonably diverse? So to test this out, we just tried it out on a little toy experiment, and what we did, is we collected some images from Google Image Search, and we just searched for "scientist female", "scientist male", and also "artist female", and "artist male". So this immediately gives us four attributes, right? We have these four spaces. And then, there's kind of standard ways of mapping these images to vectors, right? So this gives us the dataset that we work with, and what we're going to do is measure these two fairness and diversity metrics, and we have kind of our usual geometric, what's called a K-DDP, our usual geometric sampling process. We have the two approaches that I described to you. And in this case, the constraints that we place, is that we take equal amounts from each subset, and as a base line, we also can just take a uniformed set just to see what that gets us. Oh, so if we do this, oh, what we can look at is the, again, the fairness and the diversity metrics, and the first thing that I want to point out, is that when I set up this dataset, the dataset was inherently fair. I took 200 images from each of my classes, right? So currently, everyone is equally represented. And, of course, if we take our two approaches, those are by definition fair, because we forced our algorithms to take equal amounts from each class. So that kind of talk flat line, in terms of fairness, are two approaches. Now, one think that I found quite interesting, and very relevant here, is that if you look at uniformed sampling, okay, maybe that is expected. It's not going to be perfectly fair. There is some noise there, right? You're not going to get perfectly equal distribution. But if you look at this geometric sampling, you're actually doing any worse, even worse than uniform. So by trying to sample geometrically, which we had kind of hoped might take into account our fairness, actually, it's not. In this case, it's actually doing worse. So generally, we can see that the more samples we get, they are getting better, but still, this is not quite there. Now, if you look on the diversity side, we were nicely surprised to find that the P-DPP is comparable, statistically comparable to the usual geometric sampling process. So we're not really losing out at all here. Okay, so now, let's look at the real question, which is, what if this underlying data is biased? So here, on the X axis, we're changing, we took our dataset, and we sub-sampled it, so there was a smaller fraction of male versus females. We still kep all the scientists and artists. So it goes from roughly 10% to the equal 50% of male in the dataset. And, now, what you can see, is that if you don't take one of our approaches that is correcting for the bias, especially if your underlying dataset is biased, you're doing really badly, in terms of fairness, right? Which is expected, but really, you pay a lot, in terms of this metric. On the other hand, if you look at the diversity side, now, our algorithm starts to pay a little bit, right? When that data is very biased, you can't get as good diversity as your geometric process, but you don't do too bad. And, actually, part of why we're doing a little bit worse here, is because we had such small datasets. So we've also done some experiments, if you have a much bigger dataset, the amount you pay is even less, because you have a lot more variety. Here, the issue really is, if only 10% of our dataset is male, we have very few male images to choose from. So that's really where this cost is coming here. And one last thing I wanted to point out here, which also was an interesting observation for us, was that our second approach, when we sample once versus when we sample from each side of the partition, that approach ends up being better on both metrics. So it really is important to be able to take everything as a whole, look at the whole dataset when doing this sampling. Okay, so. Okay, so overall, what were the take home messages from this? Even if your data is fair, and this I find really important, even if your data starts out being fair, by doing your diverse sub-sampling you may actually be introducing biases into your dataset. So this really is an important problem to look at. Secondly, we want to consider the sub-samples as a whole. So looking at these type of processes that combine in one sub-sample both the different attributes and the geometric diversity is really important. Here, I just gave one simple example where we have one partition, and we want to split things evenly, but there is more and more complex constraints that one can handle, again, at higher and higher computational costs. And the last point, which really is kind of the most important point here, is that this is something that we can do. We can do this efficiently. We can guarantee it fairness, and we don't have to pay too much, in terms of the loss to diversity. Now, as I said, the sub-sampling that I showed you here, this is really just one part, and it's an important part. It's often a very useful end goal, but one thing that we're really interested in, that several people have alluded to in the past, in the past two talks, is what happens down the line, right? This is really the first step. So we have our dataset. We take our sub-sample. What if we feed it into the algorithm? So the nice thing about this approach, is that right now we don't have to dig into the algorithm, right? It's kind of a pre-processing step. We handed unbiased or a de-biased sub-sample to the algorithm, and now, what we would really want to see is what end output do we get, in terms of bias, in terms of accuracy, and whatnot? So that is really the main direction I see here for a future work, and that's it. Thanks. (audience applauding) Questions? Sorry, questions now or later? - We'll go ahead and take questions now. If you have a question, please do come up to the center aisle and use the microphone. (speaking faintly) - Sure. - [Audience Member] Okay, thank you for your presentation. So I would like to have your opinion. In fact, I would regress to fairness definition, for sometimes collective fairness can be against the individual fairness, and even the concept of fairness seems to be subjective somehow, and depending on the context, and the use case. So what's your opinion? Should we have, I mean, I don't believe that there is a unique definition of fairness. - Yes, well thanks. I completely agree. So part of the issue here is how do you decide what is fair and what is not, right? And a lot of this has to do with the end, I would say in this kind of future work part with the end application. Here, our notion of fairness was really fairness in the dataset in the sense that all classes are represented, and all classes are represented well, right? Sorry? - [Audience Member] (speaking incoherently) to say, it's fair with regards to what? - Yeah, fair, yes, yes, absolutely. So fair with regards to what? And here, what we're considering fair is equal representation in the dataset. So we're not saying anything necessarily about the outcome, but we're saying that all classes are being represented, and represented diversely in the dataset. Absolutely. Any one else? Thank you. - Well, thank you everyone. If there are no other questions, I'd like to invite Fernando Diaz up to talk to us about, it's a long title. (audience laughing) Fernando will tell us. No, it's exploring or exploiting social and ethical implications of autonomous experimentation in AI, with a large number of authors as well. (audience laughing) Yeah. (speaking incoherently) - Do you guys know there's lines on the ceiling? I've never seen that before. All right, so thank you very much. This is joint work with Solon Barocas, Sarah Bird, Hannah Wallach, Kate Crawford, and it's all conducted at Microsoft Research here in New York City. So just to be clear, we're all coming from pretty diverse backgrounds, but that seems to be representative of the FAT ML community, so that's great. So several of the early presentations have focused on measurement, the measurement of fairness, and the optimization of so called fairness aware algorithms. This work that I'm gonna be presenting today will focus on experimentation in online services, such as wen search engines or news sites. More precisely, this work deals with autonomous or algorithmic experimentation, and the ethical questions that this practice might raise. So I'd like to begin by putting this work in some context. So first, I wanna be kind of specific about the domain. As with previous presentations, the slides supposed to be black, so don't worry. As with previous presentations, we'll be considering the situation where machine learning is making decisions about individuals. So for example, hire or no hire, or add one versus add two, based upon information collected about that individual. So for example, SAT scores or safe demographics. As a running example, we're gonna be using navigation. So let's say, I want to go from NYU to Columbia. The job of the system then is to provide me with some satisfying road between the two. Similarly, if I wanna go from Broadway Junction to Broadway Theater I might be provided this route. Now, for simplicity, I'm going to assume that the system's actually recommending a fixed route, as opposed to dynamically adjusting that route in transit. Now, I'd like to provide some additional context from the perspective of supervised learning. So we might have some dataset of routes gathered by, say, GPS data from taxis or cellphones. Now, if these training routes are gathered so that some areas of the city are over or underrepresented in some systemic way, we could imagine a machine underperforming for underrepresented parts of the city. So for example, if our training comes barely from Manhattan, the machine might be less confident about effective routes within Queens. Alternatively, if the route selection reflects some unsubstantiated bias in the driver to provide the training data, the machine may inherit the bias from those drivers. So for example, if all of the training routes avoid certain neighborhoods with certain demographics, the machine might also learn to avoid these neighborhoods, perhaps degrading the effectiveness for drivers from those areas. Now, dealing with the bias in supervised learning has been address in the previous, in a lot of the previous iterations of machine, of FAT ML, as well as previous talks today. And we're gonna return to this in a few minutes. Before that, I'd like to discuss online experimentation. So for those of you unfamiliar with the topic, I'm gonna provide a pretty quick summary. So most online service today, such as web search engines or news sites, conduct what's called AB testing in order to optimize system performance. So for example, if a website is trying to decide between a two column layout and a three column layout, it might present 50% of its users with a two column layout, and 50% with a three column layout. After some period of time, the engineers will inspect the log user data, and see which layout actually improved the user or revenue metrics. Many of these experiments might be considered relatively benign, especially interface experiments. However, let's return to our routing example. So let's say engineers are interested in seeing if users respond well to more scenic ocean side routes compared to shorter or quicker direct routes. So what they're gonna do, is provide 50% of the users with a quicker, less scenic route, and 50% of the users with the longer, ocean side routes. Let's say I wanna get from Brooklyn Heights to JFK in order to make a flight. The quicker route might be taking Atlantic Avenue, and arriving at JFK in a half hour, optimistically. However, (audience laughing) if I'm lucky enough to be in the scenic experiment, I might be sent down through the Bell Parkway, arriving at the airport in an hour, potentially missing my flight. Now, this type of experiment has raised concerns from both the popular press, as well as the academic community, in response mostly to the emotional contagion experiment from a few years ago. Now, not withstanding these issues, experimentation provides the opportunity to address some of these issues with biased training data. For example, if we're able to find granularity, make experimentation decisions, we could get a more representative sample of the data, or one that avoids systemic bias. Take predictive policing as an example. Let's say we're trying to decide where to dispatch police to a neighborhood based upon features of that neighborhood, perhaps including demographic information. If the models trained on a sample over-representing certain neighborhoods as high crime areas, policing decisions may reinforce these observations, as we saw earlier. Now, if the system randomly decides to send police to areas predicted to be low crime, and observe high crime, it can incorporate this information, and adapt the model, thus recovering from the biased training data. Now, this is precisely the strategy of a family of algorithms which balance exploiting and exploring. These algorithms have been used primarily for domains, like news recommendation and web search where a machine needs to determine which article, or which ranking to present to different users. So for example, here, if I have a population of red, green, and blue users, the system, in order to determine the appropriate article for the blue user, might initially be uncertain, and therefore, present random articles until the user provides positive feedback. So here, presenting the red article, we see the negative feedback. The purple article, again, negative feedback. And finally, when it presents the blue article, it gets a positive feedback. So this behavior's referred to as exploration, and is often guided by uncertainty about what the optimal action might be. So after enough exploration, the system might determine that the optimal thing to present to the blue users is, say, the blue article, and receive positive feedback. Now, nevertheless, in a real system uncertainty can arise throughout the lifetime of the process, depending on how unusual a particular user or a particular context might be. Okay, so let's say we have this, a population of users where blue is very well represented, followed by red, and then green is a minority. In an explore/exploit system we might observe improved performance over time at a rate that might be proportional to the size of the group. It's this behavior that allows the explore/exploit algorithm to potentially recover from some of the biased training data that you would've observed in supervised learning systems. Now, compared to AB testing, autonomous experimentation is conducted on a much more granular scale, making these decisions oftentimes on a per user basis, as opposed to per cohort. As a result, the family of techniques has often been described as AB testing on steroids. Now, given this connection to AB testing, we felt that it was important to revisit some of the questions raised by the online experimentation in general, say, from the emotional contagion experiment, and whether they would be exacerbated in autonomous experimentation. The cost of the users enlisted in exploration for us is clearly compensated for by the increases of overall utility provided by these discoveries, as suggested by the superior, long-term performance of explore/exploit algorithms. Therefore, exploration seems exceedingly well justified if we're to evaluate it on utilitarian grounds. However, exploration may nonetheless raise a number of serious concerns well recognized in other ethical traditions, say, from research ethics. So we're gonna suggest analyzing these algorithms using research ethics principles, particularly published by the Belmont Report in 1979. This report established three basic ethical principles: respect for a persons, beneficence, and justice. Respect for persons protects human autonomy, allows for informed consent, and requires researchers to be truthful to those participants. Beneficence embodies the idea of do no harm with the aim of minimizing any risks to the research subjects while maximizing the research benefits. And finally, justice addresses the process of distributing risks and benefits to potential research subjects in a manner that is fair. Now, in the remainder on the presentation, we're gonna argue that autonoums experimentation systems challenge these three basic principles. So autonomous experimentation systems conduct exploration by taking potentially sub-optimal actions in order to discover the relationship between these actions and their outcomes. By subjecting users to potentially sub-optimal actions, some users will experience inconveniences or risks not experienced by other users. For example, a system may want to confirm that a previously slow route is still slow by deliberately sending some routes, some users along the slower route. Such decisions expose the users to risk, even though the goal is to improve system performance as a whole. Now, so this despairity that raises the question about, who should act as an explorer, and who should actually exploit the information that we're discovering as a result of our exploration? One obvious answer is to require the explorers to be selected uniformly at random. Unfortunately, even though the selection procedure is technically unbiased, it can still violate this ethical principle of autonomy. Now, we're gonna argue that an absent of informed consent in such experiments take advantage of user's ignorance, and potentially direct them to engage in activities that depart from their goals, preferences, and expectations. A person who will miss their flight, or be late to the airport, might decline an invitation to serve as an explorer if there's another route that might provide them ability to access the airport early. Now, respect for persons dictates that the importance of allowing users to make such decisions for themselves in an informed manner is important. Even if exploration improves a system performance as a whole, it may not justify risks to any specific user when we're considering autonomy. Now, in many cases, when we're certain about the optimal action, say, the right news article to present, we probably do not need to explore anymore. As a result, many algorithms explore at a rate that's proportional to the uncertainty about the optimal decision. Because uncertainty arises from the lack of information, users who belong to some minority group about which there's proportionally less information by definition, may be more likely to serve as an explorer. In our previous example, the smaller green population will incur a larger per user exploration rate than the blue population. In other words, autonomous experimentation systems can disproportionally target users who do not resemble the majority class, in some cases, because they belong to some historically disadvantaged group. Now, this raises questions that relate to justice. In other words, the process of distributing risks and benefits in a manner that is fair. Unfortunately, the standard notion of justice does not translate cleanly to autonomous experimentation. In a traditional human subjects experiment, upholding justice means ensuring that one group does not bear most of the risks while another accrues most of the benefits. However, in autonomous experimentation systems, disproportionally experiment on minority users, these same users are the ones that stand to benefit from precisely those experiments. As a result, a more pertinent question, is whether the extent or nature of the experiment is actually necessary? Would other experiments with fewer risks be equally effective at benefiting those minority users. This is exactly what beneficence tries to capture. Here, the goal is to try to ensure that any risks research subjects are minimized while maximizing the research benefit. Indeed, upholding beneficence sometimes requires researchers to consider using alternative research methods, which we may hae to do here for minority populations. Now, in traditional human subjects, in a traditional human subject experiment informed consent means that the potential research subjects must be provided with information about the experiment, and given the opportunity to agree. Unfortunately, it is much harder to obtain informed consent from users of an autonomous experimentation system. First, the reasons for actually performing the experiment depend on the interims of the system, its current representation of the environment, its set of possible actions, its strategy for selecting explorers and actions, and its assessment of potential risks and benefits. The transparencies about the reasons behind experimentation in a manner that's unambiguous, and that accords with the user's notions of risks and befits is not easy. Second, the number and range of experiments, and the speed at which they're typically performed may make enacting informed consent extremely difficult. Imagine trying to get informed consent for every web page you browse. Now, these obstacles to obtaining informed consent also make it difficult to develop procedures for ensuring that compatibility. For government funded human subjects research the common rule requires that institutional review boards review all experiments, and evaluate research protocols and methods, and asses the potential risks and befits. There is no clear analog for autonomous experimentation systems here. One possibility is an automated checks and balances mechanism, perhaps as a part of the systems reward function for monitoring a system's actions, and to penalize experiments that an IRB would find problematic, for example. In many scenarios, although, it is likely that such a mechanism would be, it's likely that such a mechanism would be accomplished by some form of human review. This hybrid approach would make it possible to share accountability between the system designers, the people designing the agent or the system, and external reviewers who are evaluating, or judging the experiment decisions conducted. But then, again, this raises a challenge of actually communicating to reasons for performing an autonomous experiment to the human decision makers. In addition, it would slow down, it would slow the system down, and obfuscate who users should hold accountable if something should go wrong. Finally, regardless of the procedure for ensuring accountability, the task of assessing potential risks and benefits to users is non-trivial. In autonomous experimentation systems, risks and benefits are typically derived from some behavioral data, such as clicks or wearable sensors, often by some other machine learning system, which may be crude or even vulnerable to errors, or biased training in itself. If a system's assessment of risks and benefits are inaccurate, then any decisions based upon those assessments will necessarily also be inaccurate, and so we're stuck. So in closing, we're at the beginning of a new phase of experimentation with autonomous experimentation systems increasingly found in critical infrastructure, such as transportation, and healthcare, and every day infrastructures, such as search engines. However, despite the ongoing debate about existing research ethics regulations, and their applicability to data science, researchers have largely ignored the unique risks posed by autonomous experimentation. As we have shown in presentation, autonomous experimentation challenges the ethical principles codified in the common rule, respect for persons, beneficence, and justice. Therefore, we believe that more research is urgently needed in order to fully understand the social and ethical consequences of autonomous experimentation systems. Thanks. (audience applauding) - Thank you, Fernando. We have about three minutes for questions. If people want to come up to the center, and ask questions, and, yeah. - [Audience Member] Hi, it was a great paper. You picked the right ethical items to think about. But I wonder if the distinction in the traditional biomedical area between the operation or practice of medicine in an experiment is relevant here? How do you distinguish the normal operation of a navigation system versus, you know, an experiment that's being conducted in that context? - Well, I mean, experimentation's sort of built into these systems these days. I mean, systems that do explore/exploit, for example, it's all part of a practice. So, essentially, depending on the algorithm, everything might be an experiment, yeah. - [Audience Member] So it's interesting to compare this talk to the talk earlier today by J.B. Morgenstern on explore/exploit for - Yeah. - [Audience Member] solving fairness where she showed theoretically that we need to have explore/exploit in order to achieve fairness in learning. And here, we're seeing that if we do explore/exploit, then we are inherently not fair. - Well, I mean, I think I've said-- - That basically, yeah-- - I'm seeing sort of - Yeah. - something slightly different. I think that paper, and the paper that's gonna be presented immediately after me, is going to address some aspects of what I'm raising here. But it doesn't address things, such as respect for persons, or even beneficence. - [Audience Member] Okay, so real quickly, is there a way that you can think of to maintain the theoretical guarantees that we see in, say, the Morgenstern paper while achieving some of these additional goals here? (speaking incoherently) - Well, yeah, I mean, this is a reoccurring question I'd say of FAT ML, where we ask, like can we guarantee fairness, and at the same time, provide some of the guarantees about accuracy, whatnot? Sure, I mean, the point of this paper was mainly to raise a new set of ethical principles that we may want incorporate into these algorithms. Okay. - Thank you, Fernando. Our next paper is Rawlsian Fairness for Machine Learning, which I believe is being presented by Michael Kearns? - Yeah. - Yeah, oh. - Yeah, I am. - Your-- - Seth Neel. - Seth Neel, okay. Thank you, Seth. - Yeah. - Okay, so I'm very honored to be speaking on behalf of our research group, which does include Michael Kearns, as well as Matt Joseph, Jamie, who you've already seen, and Aron Roth as well. So we've seen a lot of discussion today about how given a black box machine learning algorithm, like, say, Word2vec, how you can sort of analyze the discriminatory behavior, and how you can perhaps post-process it to try and achieve more fairness. And then we've seen a presentation following it, which suggests maybe instead of altering perception, or how the algorithm is receiving the data, we should alter action. And this talk is sort of in that direction, which says, "Let us propose sort of a general fairness definition", which we term Rawlsian fairness, "And then, from the ground up "at every stage of the learning process, let us try and be fair with respect to that definition". So I'm going to briefly situate our definition of fairness, Rawlsian fairness, which is the same definition Jamie used earlier, in the context of other fairness definitions that have been proposed. Then, I'm going to walk everyone through an example of how you can apply Rawlsian fairness in the sequential decision making framework, and sort of analyze some interesting theoretical and empirical guarantees that we can show for our algorithm. Okay. Yeah, so again, this is our research group. All at Penn. I think we've already covered that. Okay, so the first definition I wanna highlight is from the Fairness Through Awareness paper from 2011, which says that, "Any two individuals who are similar "should be treated or classified similarly by an algorithm, and that's what it means to be fair", that a fair algorithm treats similar individuals similarly. And that does pre-suppose that we already have a notion of what it means for two individuals to be similar, or a distance metric. So our definition will sort of also say that it's fair to treat similar individuals similarly, but we'll also propose a specific kind of general notion of what it means for individuals to be similar with respect to a particular task. And another definition, which we've also seen in one the five-minute talks today, is that given some sensitive information, like race or gender, a fair algorithm should not ignore them completely in that, you know, the predictions shouldn't be independent of the protected attributes, but conditional on the outcome. They shouldn't depend on the sensitive information. So it's this, you know, among people who are classified a certain way, or who should be classified a certain way, their true quality is the same. Their prediction should not depend heavily on these sensitive attributes. And our definition also captures part of that. Although, it's a little, it's not immediately obvious. And the last thing I'll highlight, again, is that we're not taking some batch learning model, or some model that's already been made, and trying to give fairness guarantees for it, or prove properties of its fairness. We're starting from no information, and at every step of this sequential process, we're going to try and enforce this fairness definition. And so, we're simultaneously gonna be trying to be fair at every step, but also, to learn and improve our model. Okay, so a brief philosophical motivation for our definition. We call it Rawlsian fairness after the philosopher John Rawl. I'm not well versed in Rawlsian philosophy, but essentially, we're trying to encapsulate this notion of fair equality of opportunity, which says that, "Two individuals who are the same talent, "natural talent, or ability should have the same prospects of success in the system". And so, what that means concretely in our setting, is that two individuals who have the same value with respect to the classification,. so the same probability of paying back the loan, for example, should have the same probability of receiving that loan. And so, that's what throughout this talk, that's gonna be our working definition of fairness. So before I briefly introduce the learning model that we're studying here, called the contextual banded framework, I want to mention, that this definition, and sort of the ethos behind the Rawlsian fairness definition, applies to a wide variety of machine learning frameworks, which is why two of the other papers today are also using this exact kind of Rawlsian fairness, except in reinforcement learning, and one studies a similar banding problem. (mumbling) All right, so the contextual bandit, in a simple way, is as follows: say we have an unnamed bank, and at every time individuals from three different racial groups are arriving. (audience laughing) They can't sue me, 'cause they don't exist anymore. (audience laughing) So at every step, we view an individual's credit history, represented by the shaded papers at the right, and, you know, other relevant characteristics about them, and we make a decision whether or not to give them a loan. And suppose for the purposes of this talk, we're only giving one loan at every time. So at the beginning, we have no idea how this credit information and these characteristics actually maps to the quality of the individual, and how it actually maps to the likelihood they'll pay back the loan, or how much money they're gonna pay back on the loan. So simultaneously, while assigning these loans, we're gonna observe who pays it back, and then, at each step, after we observe how much they pay back, we'll update our sort of idea about the model, and we'll hopefully make better predictions over time. Actually, we'll prove that we will. So now, we're gonna define what it means to be Rawlsian fair in this sequential framework. So we require that at every time T over all T greater and equal to zero, if an individual I, so one of the, an individual from one group. One racial group, for example, gets a loan with a higher probability. So the probability that it's chosen as the guy at that time who gets the loan is higher than an individual J's probability of getting the loan at that time, then that individual must have a higher quality, or it must have a higher repayment than the loan it was favored over. So that's sort of exactly modeling this idea that if an individual has, you know, more natural ability or more true quality, it should have a higher prospect of success in our system, and that's the only way that that can occur. So for the purposes of analyzing, you know, proving things theoretically about the algorithm, we assume that the mapping from the information about every person to the true quality is a linear function of the observed features. But, again, that's not an artifact of the fairness definition. That's for the purposes of the analysis, and it's obviously a very commonly used model. So just to quickly summarize the model before we get into any analysis, at every time T we're gonna have individuals from these, say three, but K groups arrive, each with a vector of features, some relevant information, then one of those people is gonna be chosen to get a loan, and then we're gonna observe the amount of money they pay back as a linear function, an unknown linear function of that person's features, and then, we're gonna update our information about these linear functions, and then, we're gonna repeat the process. And there's noise here as well. Okay, so, yeah, this is the formal definition, which just, again, is exactly what I said, "The probability of choosing an arm I "is greater than the probability of choosing an arm J "only if the true quality of I is higher than the true quality of J". And we specify that this happens simultaneously over all T greater than zero with high probability, and there's something a little subtle there. You could imagine a weaker definition of fairness, which says, "You know, at every round with 99% probability we're gonna be fair with respect to this definition". So at every round there's a 1% chance that we're being unfair. We're saying that, with over all of the runs, that all T, the entire span of the algorithm, we're gonna be fair at every single round with probably, you know, one minus delta. And the reason we do this, is because if you just require it on a round-by-round basis, that would allow for discrimination against, arbitrary discrimination against, you know, individuals who occur very rarely in the data. So if our, you know, delta was .01, if, you know, less than delta a fraction of the time an extremely qualified individual from a certain ethnic group came along, if we had this weaker definition, then we would simply be able to never give them a loan, and that would be fair, but this definition prevents that. Okay, I'm not gonna dwell here too long, but regret is essentially the standard way of measuring the performance of an algorithm in this online setting, and it says, "What's the difference between "how our algorithm actually did, or the expected difference, "and what is the best you could possibly do if you truly knew the linear function?" So the best possible algorithm would always play the individual with the highest quality, which by the way, would be fair with respect to our definition. But, obviously, you don't know those true linear functions, and so, you can't always do that. And so, (coughing drowns out speaker) how this regret performs of our fair algorithm is a way of saying, "What is the cost of fairness?" How does our regret of our fair algorithm, which I'm about to define, compare to the regret of the unfair algorithm, the best algorithm that we can do? Okay, so the main (coughing drowns out speaker) the paper, and, again, there's not too much time to get into it, is that we prove this regret bound for a fair algorithm by our definition, and combining that with, like, previously known lower bounds, and from the literature, we know that we can precisely kind of quantify the cost of fairness in this case. So, like, without requiring a fairness constraint, if the best you can do is something, like, O till the root TD, and we're showing an upper bounded D root, TKQ, then we can sort of say, that the cost of fairness, is at most, root DKQ. So that's a nice way of sort of formally quantifying the trade off that's been discussed all day about accuracy versus fairness. Okay, so I'm gonna briefly talk you guys through the high level of idea of how we formed this fair algorithm. So at every time T we're gonna form a competence interval around the payoff of the mean from each arm, and it's just a linear function, so that's something we know how to do in many different ways from statistics. So we'll have those competence intervals, and you'll see on the next slide, we'll chain them together, and we'll form a chain of competence intervals that are all connected to the best arm at that given time. So some of the arms will be in that chain, and others won't. And then, finally, we'll just play pick an arm, pick a person to receive a loan uniformly from the set of the arms that are chained to the best guy. So how it looks is as follows: so we have a competence interval with the top arm, the second best, the third, the fourth. And as you can see, the second competence interval intersects with the top confidence interval, which means that there is some chance that the second, that arm that we think is the second best actually has a higher mean, because their competence intervals intersect. So to comply with our definition, we have to play them with the same probability, but then that reasoning means we have to play the third arm with the same probability as the second arm since those competence intervals overlap. And the fourth arm with the same probability as the third arm. So by the transitive property, our variance definition requires we have to play all of those arms with the same probability, but those bottom arms don't intersect any of those, that higher group, and so we don't play them at all at this step. And one thing that's worth noting, is that the optimal algorithm that doesn't worry about fairness just plays this, the top arm, the arm with the highest upper competence band, UCV. So this is sort of a nice conceptional kind of transition from that optimal algorithm to a fair algorithm. Okay, so quickly, this shows how the chaining takes place. I guess one key thing to note here, is that in the bottom slide, you can see the gray band measures the width of that big chained competence interval. So all the widths of those competence intervals that are played from uniformly random at each step, and over time, because we're learning more and more about the linear parameters that govern those arms, the width is shrinking, and that's sort of the visual proof of the fact that our algorithm actually is playing with no regret asymptotically. So we're eventually converging to playing the best arm, and that's a key property. So there is this cost of fairness, but it's not so high that you can never actually perform the task. Okay, so before I finish up, I'm gonna highlight two sort of empirical findings from the paper. The first one is, there's a couple different in these plots, but I'm just gonna focus on the middle one, shows how the trade off of the difference between our algorithm and the best algorithm, so the difference in average regret between those algorithms varies as a function of the number of arms that we have, the number of people arriving to get loans at every time. And as you can see, let's focus, I might have like a pointer here, but let's focus on the green line in the middle of the middle plot. So as you can see, as K starts to vary with very low values of K, the difference in the regret is very low. So our chaining algorithm is doing almost as best as the best algorithm that we can do, but there's almost like a phase transition around K equals eight, and then, all of a sudden, as K is increasing up till about 35, there's a lot more, the difference in regret is increasing, and that's because K has hit the point where there's a lot of chaining occurring, and so we're not actually playing that top arm. We're being forced by our fairness definition to play a lot of sub-optimal arms that are in the chain. And then, that starts to level off around 40, because now there's so much chaining going on that we're basically playing randomly among all the arms, and so the regret is just, the difference is gonna be constant there. And then, we have similar plots varying T and D, but that information's in the paper. So I think more interestingly, we also define a notion of structural unfairness, and without getting too much into it, we say that the structural unfairness with respect to an individual. So this is an element of, I guess, the action space. So what that really means, is an individual as represented by that vector of features. We define the structural unfairness to be the probability that that individual is discriminated against by an algorithm given that it's involved and around where an unfair decision is made. So if it's involved and around where an unfair decision is made, then it could either be benefited by the algorithm or discriminated against, and discriminated against, being like it has the top quality, but isn't chosen, and benefited being, it's actually sub-optimal, but the algorithm gives it a loan anyways. So we construct a basic instance where we can, we sort of plot the structural unfairness for every point that could possibly arrive, and this is an example of, so we're in D equals two, so there's two features, and then replying the structural unfairness for all these points, and this is what our fair algorithm produces. So the key thing to note here, is that at every value of X, every cross-section, the color doesn't change, and the color is indicative of the height, the amount of discrimination, and that's because in this example that we set up, the true quality is just a function of X. So that's what you'd wanna see, individuals who have the same true quality have the same chance of being discriminated against. So that's why the curve has this nice property that every sort of X cross-section has the same height. Now, we took the same exact, you know, data generating process, and we ran the standard, you know, best practice UCB algorithm on it. And as you can see, along a specific cross-section of the data, which is a subset that we had designed carefully using sort of like a heuristic about why these algorithms could discriminate, there's actually, like, a massive spike in the probability that it's victimized by the standard algorithm, even though the true quality of those point on the diagonal are exactly the same. And so, this is just kind of a compelling, oh, whoops, visual presentation (audience laughing) of the exact problem that can happen if you just sort of blindly don't incorporate a fair algorithm. So I guess to summarize, the contributions of this paper are, we propose a flexible definition of fairness that extends to a lot of other frameworks. We derive a sensible algorithm that complies with our definition, and the more mathy parts of the paper, we prove a sort of technical regret bound on the fair algorithm to precisely quantify the cost of fairness. And then, through these empirical experiments we demonstrate why in this online learning setting it's very important to try and incorporate mentions of fairness. Yeah, that's it. Thanks. (audience applauding) - So since we're a little behind, I think we have time just for only one question. Seth, I'll let you pick someone. And in the interest of saving time, if you could just shout out your question, then I'll repeat it to the live stream. - Hey, I really like this definition. I just don't understand in this setting what the good regret bones are. So it's be Rawlsian fair to do, to just, like, reduce to offline learning. So just, you know, set theta equal to zero, randomly sample, pick the best theta based on the data that they got, and then, just play that. So quantitatively, that seems like really naive and dumb. So quantitatively, do you know the improvement over that? - Well, that-- So what you just said is that, you're saying, randomly sample for a given prefixes-- - Yeah. Which if you have X-- - Right, but over those rounds you're incurring a lot of regret. So that would be one way that you could do this. And I think the dependence on T you'd get from that is like T to the 2/3rds or something. It's not as good as the algorithm that we proposed. - Well, the idea being-- - The algorithm from outside is quite as definitional. - Oh, if you set that equal to zero per some fixed period of time initially it's not Rawlsian fair? - [Male] So the important thing that you're not doing if you're doing half the competence, so your empirical estimate, what theta is, (speaking incoherently) it's not certain that you'd always come up with uncertainty, and if you don't take that into account it's not gonna (speaking incoherently). - Okay, so that you actually play (speaking incoherently)? - [Male] If the algorithm you described, it would not let us (speaking incoherently) - Okay. All right. - Okay, yeah. I guess, I thought what you were suggesting is randomly sample for a prefix of rounds, which would be Rawlsian fair, and then sort of apply this similar strategy. - [Audience Member] That is what I was suggesting. - Okay. But maybe we can talk, maybe we can talk offline about that. (audience members chattering incoherently) - Well, thank you, Seth, and let's thank everyone who spoke in this session. (audience applauding) And-- And then, for one more very brief bit before lunch, I'm sure everyone is very hungry. I'm gonna hand it over to Solon. - Hi, everyone. Thanks for bearing with us, and I know I'm running a few minutes late. I just wanted to take this important opportunity, first, to recognize a number of folks in the room who actually come from federal regulatory agencies. We have representatives here from the Federal Trade Commission. Is that person here? Wanna stand up and identify yourself just quickly? (audience laughing) I do this actually with a purpose, which is that I would hope that the people in this room who are interested in seeing their work translated into practice actually approach these people, and talk to them about how some of the work they're doing can be relevant to government practice. So if you're interested anyway. - [Jim] I'm Jim Paligreeno. I'm the recent director in the FTC's Office of Technology, Research, and Investigation. - We also have people here from the Consumer Financial Protection Bureau. Anyone in the room wanna identify themself? - [Bryce] I'm Bryce Stephens. I'm an economist and section chief in the Office of Research, and I work with a team of folks doing analysis, and (speaking faintly). - [Marion] I'm Marion Schroeder, and I'm everything but the section's chief part of this unit. (audience laughing) Helmet's here too, fellas. (male speaking incoherently) - Great, and we also have some folks from the Equal Employment Opportunity Commission. - [Kelly] Which is me. (laughing) Hi, I'm Kelly Trindel. I'm a chief analyst at EEOC. I have a psychology background. - And finally, I think we have someone from Treasury, is that right? - [Male] Yeah. - My name's Austin Hinkle. I'm the senior policy advisor from Treasury. (speaking incoherently) - So we're gonna break for lunch in a moment, and I hope people will come and introduce themselves to the folks who just introduced themselves to you, but before we do, I have the great privilege of actually being able to introduce you all to the Chair and Commissioner of the New York City Commission on Human Rights. I am going to terrible mispronounce your name, which is going to be terribly embarrassing, 'cause my name's always mispronounced. (laughing) Carmelyn? - Very good. - Carmelyn Malalis, who will speak to us today about the agency, and some of the work that they're thinking of doing on these topics. - I will be quick. Far be it for me to keep anyone from lunch. And first of all, I wanted to say, one, how kind of surprising it is for me, and how kind of gratifying it was for me to walk into a room and see so many people here interested in this topic. For the perspective of government, as some of these other government folks could probably tell you, and from the perspective of somebody who is looking at a lot of these kind of big data issues from a discrimination and, you know, kind of fearness framework, it's really exciting to see that so many of the people who are really kind of the first line defenders of fearness, and transparency, and accountability are so interested in this subject matter, and on taking on this work. I also just wanna take a moment and say, you know, I almost wanted to take a picture of myself here, 'cause, you know, while my mother was chasing after me when I was a kid to, you know, practice piano, my dad was, like, chasing after me while I was in elementary school to learn Fortran, and if he knew that I was in this room, and not, like, surrounded by books with lawyers, he would be ecstatic. So thank you for giving me that opportunity to share with my parents. (audience laughing) So I am the Chair and Commissioner of the New York City Commission on Human Rights. We are the city agency that is charged with enforcing the city's very broad anti-discrimination and anti-harassment protections. So there's part of our agency that does kind of civil law enforcement work. Kind of the three main areas we do that in are in housing, employment, public accommodations, and we have some other areas, but those are really kind of the big areas that we work in. We take complaints in from the public. We also, on behalf of the city, when we know that we're, when, you know, folks from universities, or from elected officials, or other kind of community-based organizations, when they identify issues in those areas where they think that there is, you know, a pattern and practice of discrimination, or there are actions that we should be addressing, they'll also contact us, and without having somebody come forward and saying, "Hey, I've been harmed by this", we, as the government, have the ability to affirmatively investigate those issues, and sometimes, also, affirmatively litigate those issues. So it's a very important part of the agency. The other part of our agency deals with kind of going in to different communities, or working with the different constituent groups, and educating them on the human rights law. So educating them on, you know, what their rights are, what they have a right to be free from, in terms of discrimination and harassment, and frankly, also what their obligations are. How can they prevent discrimination or harassment from occurring in their housing units, in their businesses as employers? And, to me, certainly, you know, as technology is increasingly becoming the area where I think people are getting their news, they're getting their information, that is, you know, technology is kind of the big influencer in the room. You know, we are seeing a lot more ways that people can use technology, of course, to intentionally sometimes, and sometimes unintentionally treat people differently, you know, in all those different categories. So, again, it's really wonderful to see so many folks out here. As we've been working with different groups on, you know, educating them on rights, as well as obligations, you know, what's been really important for us is really figuring out how we can have impact in certain areas. Sometimes our impact is on enforcing the law through, you know, initiating these investigations, and then litigating these cases. But quite often, and in mostly, I mean, in the last, you know, almost two years that I've been at the Commission, a lot of the way that we actually impact change is impacting behavior. Making people aware of how their actions, their decision making influences outcome. Making folks aware of how they're focusing on something that may be, you know, because of what has been put out into the general public, maybe because of the information, or the misinformation that has been put out there, maybe that's influencing a way that people think about a certain issue, but correcting the record. So people actually understand, and actually realize that sometimes that information is wrong. You know, one of the ways I think that really kind of connects to some of the work that you all do, is, you know, one of the first big city campaigns that we had launched when I came in as the Chair of the agency was a city-wide campaign on credit history discrimination in employment. You know, and it had been this area where for however many years long, employers were very use to, either on their own, or, you know, contracting third parties, to kind of cull together folks' credit history information through all the various things that are available in public records, through other kind of credit history related information. They'd put together these reports. They'd get this, like, one score, and based on that one score alone, kind of regardless of what a person's qualifications were for a job, or regardless of how maybe, you know, past employers had rated that person's performance, that person would be kind of put on the reject pile for certain things. All of this, while knowing that, or while not knowing, actually, for a lot of folks, while not knowing that there was actually no research connecting any sort of credit history to trustworthiness for applicant for employment. And so, this administration worked with the City Council. We worked with other organizations that saw how credit history was being used in this discriminatory way in order to disadvantage well qualified applicants, particularly applicants who were in communities of color, particularly applicants who were women, and that is because still it's the case that disproportionalty women, you know, tend to be the kind of care takers or care givers in family units, and so are less likely to be able to build, you know, a solid credit history, you know, to disproportionally influenced folks who had gone through divorces, who had been in school, and had, you know, past payments with some of their student loan debt, things of that nature. So kind of very similarly here, we find ourselves now also kind of tackling this question of how we can work with people who are, you know, in the field, and study, and figuring out, and really kind of designing the different technics for machine learning to make sure that as you are designing these different algorithms, you're thoughtful of the different ways that the information could be used. We, as an agency, from our enforcement perspective, we're already seeing how some of this is already coming up in employment. And, you know, our kind of sister agency on the federal level is the EEOC. It's great to see somebody here from the EEOC. I actually just came from a conference when I was talking with the chair of the EEOC, and we were talking about, that already in the area of employment, the focus that has been put on this issue. But I also wanna make sure that we're also focusing on some of the other areas, like housing and public accommodations, because that, increasingly, in our agency is where we are seeing these things kind of creep up. And in housing, for instance, we're seeing the ways in which, you know, again, you know, whether you're looking at real estate agents, or you're looking at big housing providers, and big companies are contracting out to third parties, and the third parties are, you know, providing them or proporting to provide them information that helps them kind of ween out people who speak second languages, people who they think would be coming from immigrant families, people who would be, have different names associated with them. So, of course, that would have a lot of different hits in our anti-discrimination laws, right? There's different ways that that could be used to discriminate against folks because of their immigration status, their national origin, their race, their gender identity and gender expression. You know, if they're transgender, they're likely to have some changes in their names. So there's different ways that this could be used. And, of course, if they're kind of selling themselves in this way, there is the intention of discriminating in that area, and where's there's the intention of discriminating in that area, then, certainly, under the City Human Rights Law, you know, even if they are a third party company who is raising these issues, even if they're not the actual housing provider, there is the possibility of also prosecuting actions under the City Human Rights Law against those folks. Again, what we're finding, you know, I think more than the intentional discriminators, though, are the folks who aren't really thinking of it from that way. They're not thinking of the ways that information that's being called, or kind of the data that's being fed into the algorithmic data plans, that they're are being kind of colored, right? By kind of the human touch of those things. And so, again, it's really important that the folks who are, you know, on the front lines of doing that, on the front lines of putting together conferences, like this, are having these discussions, and putting that out there. So you see how your information is being used. You see how your work product, frankly, is being used, and you have ways of, like at Rawlsian fairness, of trying to preclude that from occurring. I think, also, you know, outside in the general public, people tend to think, you know, it use to be the case that people use to think, "Well, if it's in a book, if it's in writing, well, it must be true", right? And I think that very much in the public people think too, you know, "Well, if it is produced through, you know, through machine learning, then it must completely unbiased". "It must be completely objective." And I think, certainly, that is oftentimes the objective. But also don't realize how much information is actually influenced by the people kind of feeding in the information, and I think that's always something that we have to be critical of. It's great that we're able to work. You know, I've been told that 70% of the room right now is technologists. There's about 30% of the folks who are here who are kind of policy and law related. I think that, increasingly, we're gonna see a lot more, you know, of these types of gatherings, and you're gonna see a lot more of the lawyers and policy makers really interested in working with all of you, and getting your ideas of the ways that we can be thinking through things, not just from the enforcement realm, but also through the impact realm. The City Commission is very interested in this issue, and so, if there are folks wondering, you know, especially right now in this day and age, where we've been getting a lot of inquiries in the last week, if you could imagine, of people saying, "Hey, what can we do to be working on human rights?" "What can we be doing to be making sure that our communities are not discriminated against?" There's a lot you can do, and, certainly in this area, this area is extremely fertile ground, so if people are interested, please do come and see me afterwards. I have two other folks here from the Commission on Human Rights. And, also, you can contact us through our website, which is NYC.gov/human rights. So thanks again, and thank you for doing all this important work. (audience applauding) 