 hi my name is Brian CAFO and this is the second lecture of the executive data science specialization course on data analysis in real life so in this lecture I'd like to talk about machine learning versus traditional statistics just to orient you to the the way I think about the difference between the two and why most of what I'm going to talk about in the subsequent lectures really applies to more traditional statistics but it also somewhat applies to machine learning and in this lecture I'll tell you the difference or at least how I perceive the difference in this lecture we'd like to contrast machine learning versus more traditional statistics because some of what we some of what we're going to cover in subsequent lectures is universally applicable to both however some of it is really more oriented toward traditional statistical analysis and in this lecture I'm going to tell you kind of what I perceive to be the difference between the two so it's very kind of oriented toward my impressions so here's my sort of boiler plate bifurcation of these two styles of analyses so to me machine learning really emphasizes things like predictions and so it tends to evaluate performance via prediction performance and we'll go through some examples later on so there is a lot of concern in machine learning for overfitting that's fitting way too complex of a model but but not model complexity per se if you do good prediction and you're not overfitting then it's okay to have a rather complex model in machine learning so the emphasis is on performance and automated learning and generalizable is generalizability is usually obtained through performance of the algorithm on novel data set so machine learning tends to be a very data oriented discipline so if you want to discuss that your your machine learning algorithm is generalizable usually there's a sort of ShowMe attitude in machine learning get a new data set and apply it to that and there's usually no super super population model specified so there's no this my data is sample from a larger population and I'm thinking of it this way through statistical assumptions and there's a lot of concern over performance and robustness okay so khat let's contrast this with traditional statistical analysis so traditional statistical analysis focuses on so-called super population inference we have a data set we're going to assume that that data set is a sample from a larger population that we're interested in and it tends to focus on a priori hypotheses you that your hypothesis specify diary and in general there's this principle of parsimony in traditional statistical analyses and that's basically saying all us being equal the simpler model is going to be preferred over a more complex model and I would go even further that most traditional statisticians would say even if you take a ding on on prediction performance even if your model isn't performing that well you can you can you'd rather have a simpler model than a complex model unless the difference in performance is extreme and you in traditional statistical analysis you emphasize parameter interpretability a lot in contrast to machine learning where you may have lots and lots of parameters in your in your model your algorithm that you don't really care about those individual parameters but in a regression analysis for example you care about the slow at every one of the slopes so the other super population inference so typically we have a statistical model that connects our data to the super population that's that's the crux of statistical modeling so some sort of statistical model or even in nonparametric settings or robust settings we often have sampling assumptions that connect our data to the population and so just like machine learning there's concern over robustness but there's also this large concern over the set of statistical assumptions that connect our data to the population so let's go through some examples and I picked mostly prediction examples and I'm that I'm going to talk if I were to have approached this problem as a traditional statistical analysis what I would have done so one of the most famous prediction problems in recent years was the Netflix prize and in the Netflix prize teams competed to produce the best recommender system for the company Netflix so they wanted to recommend to users new movies that they might like through the their star rating system the Netflix prize was interesting because it was a million dollar prize and some some good friends of the team here actually won a close friend Chris Valencia great data scientist who works at 18 T so the goal of this prize in his team won the goal of this prize was to build a machine learning algorithm that produced these recommendations so you wanted it to be automated you wanted it to be something that could be adaptive that maybe could relearn the algorithm as needed and you would define success as anything that produced reliable recommendations and you can check that by when people watch a movie and actually then subsequently rate it if they rate it high than it was a good recommendation if they rate it low than it was a poor recommendation so that's the goal of the machine learning analysis but what if you were to approach this same problem with a traditional statistical analysis well rather than building an algorithm what you would want to do is build a parsimonious an interpretable model to better understand why people choose the movies that they do so you'd be much more interested in what is were the sample of people that you were interested in were they representative of the totality of Netflix users or if you wanted to generalize not just to Netflix new users in general but to movie watchers in general you'd want to know to what extent are my Netflix users in my sample representative of the larger population of movie watchers in general okay so in this case you would be building probably smaller models you'd be trying to get very interprete balal --tz-- interpretable parameters and you would define success as anything true that was learned about movies choices so if you learned that certain demographics like certain kinds of movies or you learn that people who you know psychological reasons why people like certain movies or anything that was true and parsimonious learned about movie choices would be the hopeful result the hopeful success of such a statistical analysis even if you didn't gain any better prediction right you can learn something true but that may not really help you with prediction another example that I was involved in was the heritage health prize so this was actually a slightly bigger in terms of the monetary payout prediction competition in this case the goal was to take previous years the previous couple of years insurance claims and try and predict how long a person would spend in the hospital in subsequent years so if you for example well just because we spent a lot of time on this prediction an example one that we found was for example if a woman was in the hospital for a hospital stay for a baby then two years later they had a higher they were more likely to be in the hospital again just because they a lot of people tend to have babies about two years apart right and so that was an example of a predictor so for among women with having had a hospital stay for having a child two years previous they had a slightly increased prediction of being in the hospital two years subsequently and then there was some other obvious ones you know for example people who were in for cardiac problems and things like that tended to have a elevated risk of being in a hospital the next year so at any rate the machine learning algorithm for this problem which we spent a lot of time building was basically to build an automated system for predicting hospital stays so you'd want if you were a hospital or a provider or something like this you would want to know from of the collection of previous claims that you have what exactly how long people would spend in the hospital in subsequent years and you could evaluate that you again like most things you would want the system to be able to relearn itself over time as things change and you'd want it to be robust and but success would be anything that produces reliable predictions and anything that produces more reliable predictions is better than than something that produces less reliable predictions and you could come up with a pretty simple formula to define what you meant by good predictions by the way in this case I think we got 12 we were listed as 14s but two other teams got disqualified so we came up with twelfth and it was you know this was a three-year prediction competition or 2-year prediction competition and by the end we things got pretty grim we have to come up with a prediction every single day and boy were we tired of building prediction algorithms by the end so I would just give you as a caveat if you ever intend to enter in one of these competitions think a year later down the road if you want to be building predictions every single day so let's now contrast what I would have done if instead of engaging in this prediction competition if I had been interested in the science of insurance claims and hospital stays well then I would want to do perform a more traditional statistical analysis I'd want to build a parsimonious and interpret Illuma model and I would want to figure out things like what I mentioned earlier about the issue with the pregnant women now that probably wouldn't be terribly of interest to scientists or medical practitioners anyone because it's just a kind of quirky little it's just a demographic fact really but you you might want to learn if there's certain combinations of the claims that might lead to a greater propensity for hospital stays which might then lead to the ability to treat people earlier so success in this case would be anything that was learned about hospital stays from insurance claims data another very famous prediction example was the Google Flu Trends this this came out several years ago and it was a very generative excitement and here the very clever people at Google came up with the idea that where they could detect flu outbreaks before the Centers for Disease Control by looking at search terms in geographic information associated with the search terms so if lots of people were searching for Tamiflu or you know searching for headaches and fevers etc that might lead to a greater propensity or a greater risk that the flu outbreak in that particular area where most of the searches were coming from so in this case their goal was to build an automated system for predicting flu outbreaks and their success was anything that produces reliable predictions as judged to by the more the more thorough information that came out from the CDC but much slower so their goal was to beat the CDC in terms of of time but then they they had the the high quality data coming in a little bit later to actually validate their algorithm algorithm so contrast this with what flu researchers like I'm in the School of Public Health at Johns Hopkins that flu researchers here do all the time they want to better understand flu outbreaks so they want to build models that helps us understand specifically why flu outbreaks you know occur in certain areas so for them search terms probably wouldn't be enough they need more of the kind of information that the CDC for example was collecting which includes prescriptions for to actual prescription counts for Tamiflu and that sort of thing so again so all these examples really emphasize in my in my mind kind of this big dividing line between what is typically going on in a machine learning experiment versus what's typically going on in a traditional statistical experiment so I hope what I've emphasized in this lecture is that both approaches are extremely valuable and they have their place however typically the amount of model complexity and assumptions in etc differs quite a bit between the two approaches so their goal and because their goals are very different and I would just end this this part of the discussion with a little caveat that says well that there's the machine learning community I've noticed has started to build super population models to evaluate their algorithms and so they're kind of connecting it to traditional statistics in addition there's been a lot of work on making machine algorithm output in addition to producing these ultra-high quality predictions making them much more interpretable I've also seen a lot of people taking traditional statistical approaches and building them up slightly in terms of complexity to try and produce better predictions so it's interesting that in a lot of ways the two areas are meeting in the middle and these distinctions that I've outlined a lot in this lecture that I am outlining a lot in this lecture are getting grade quite a bit 