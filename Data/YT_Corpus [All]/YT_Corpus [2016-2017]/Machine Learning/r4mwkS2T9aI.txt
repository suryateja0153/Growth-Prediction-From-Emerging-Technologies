 What is going on everybody welcome to the [fourth] machine learning and third regression tutorial where we left off we had defined Our or at least figured out what [our] features in our labels were we haven't quite [yet] defined them, but in this one We're going to define them We're going to actually pass them through to a classifier train and test that classifier to see how we do So before we get started let's go [ahead] make some imports [also] I'm just going to put kwan Deland math on the same line now We're going to go ahead and import numpy as NP numpy is just a nice Computing library it's going to allow [us] to use arrays python doesn't actually have arrays, but numpy will let us do that Also from Sk Learn we're going to import Pre-processing this gives us quite a few things, but we're actually going to be using the scaling scaling your data aspect is usually done [on] the on the features and The goal is often to get your features to be between somewhere between negative 1 and a positive 1 this can just help with Accuracy as well as just processing speed or how long it might take to actually do the calculations When we get there? I'll explain why you may actually just either Choose not to do pre-processing or maybe it's just [too] you know too tedious to incorporate it in reality, but anyway We'll show it because it exists and it can be useful Next is cross-validation We're going to use cross-validation to create our training and testing samples. It's just a really nice way to split up your data into it'll shuffle your data for you, and which helps with that with Statistics basically so you don't have a biased sample [and] then also it helps separate your data It's just nice time-saver basically and we're also going to bring an SVM [that] we're not to the support vector machine part, and we're not I'm not going to explain support vector machines just yet We will get there, but you can use an Svm to do regression and this is probably we are probably not going to come back to regression, so We'll just show it as an example using it.also It's useful because I'll show you how [frigging] simple it is [to] change the algorithm that you're using so anyways Svm Next we're actually gonna bring in regression so from Sk learn dot Underscore Model Import Linear Whoops linear Regression okay now. We are [ready] [to] Rhumble so the first thing that we're going to go ahead and do is define our x and y [so] generally features and labels are Defined features will be a capital x labels will be a lowercase y x is going to be equal to numpy array of Df Drop and we're going to drop the Label Column right, so your features are basically everything Except for the label Column and we can do this because df drop Returns a new data frame so it's returning a new data frame. It's can being converted to [a] numpy array it's being saved to the value of x Now the value of y is our labels, so you might be able to surmise that we're going to say Mp. Dot array df label easy enough Okay, so now we're going to scale x so x equals pre-processing dot scale X okay now think about it. So here We're scaling x before we feed it [through] the classifier, but let's say we feed it through a classifier We have a classifier, and then we're using it in real time on real data well, let's say you're reading in that data and You feed it through your classifier, but before you do that. You really have to have scaled it and to scale it It's all scaled together, so it's like normalized with all the other data points So in order to properly scale it you would have to include it with your training data So keep that in mind if you ever go into the future and you're actually using this you need to scale the new values But not just scale alone, but scale them alongside all your other values, so this can add action while it can help with the training and testing it can actually add to processing time especially if You're doing like any sort of like for example, but with using stock prices You're doing like high-frequency trading you would almost certainly skip this step, but anyway There's that now we're going to redefine x as being equal to x to the point of where [we] got where we were able to forecast underscore out [+1] So this is is it includes all the points because remember we shifted to you know this point zero [one] You know so [that's] like What [1%] basically so we made [that] shift so we just want to make sure that we only have x's where we have values for? y and so we do that, and then we're going to say d f dot drop in a in place equals true, and then we're going to do we're going to Define [y] is equal to np [r] [a] df label and Let's go ahead and let's print Lem of X and then len of y, so, just make sure we have the correct lengths here right so we don't have the correct [lengths] so let's close this and Really probably at this [point]. So we actually may not need to have this let me rerun that real quick So I'm thinking yeah okay, so so because we won't have [a] label for The reason why I was doing this shift initially was because we wouldn't actually have labels But we dropped those labels here are those those rows here? So we didn't need to do what we were doing there, okay? so we've got our x's on our y's and We don't need this hopefully Okay, so now what we're ready to do is create our training and testing sets, so we're going to say x underscore Train x underscore Test why Underscore train why underscore test? equals cross-validation dot train underscore test underscore split and What you're going to pass through here is the x's the y's and then how big of a test size Do you want and we're going to do point two so [20%] of the data? We want to actually use as testing data so again what this is going to do is is going to take all our features on our labels remember kind of the order it's going to shuffle them up right keeping x's and y's Connected right so it's not going to shuffle them up to the point where you lose Accuracy, so it shoves them up, and then it outputs for x training Data wide training Data x testing data and y testing data so x training y train we use to Fit our classifiers, so let's do that next so first we're going to need to find a classifier so we're going to say classifier equals, and we'll use linear regression to start and Then to fit or train a classifier. Just see a left fit and you fit features and labels, so which ones should we use well we would use the x Train and y train Now we've got our classifier. We can [actually] use this classifier to predict into the future do all kinds of crazy stuff But first we probably should test it, right 200C is so now what we would say is Cl. F dot score so fit is synonymous with train score is synonymous with test, so we'll use x Test y test so real quick why might you want to train and test on separate data? Well if you train a classifier to predict based on the same data that you test against when you go to test it It's going to be like I've already seen this information, so I know exactly what the answer [is] right So that's not good. You don't want to do that. It's no different than if [you] know if you were in school, or whatever and you were The same questions that you were asked in Class were the exact identical questions on a test right if you missed those then you just weren't paying attention or something So here we have our score and what we're going to say is Thence equals Cl f. Dot score you could also maybe replace confidence with accuracy accuracy that's probably a better choice because confident you not only you can kind of get to values not necessarily from this one, but As we go into the future [you] can actually compute both accuracy and confidence As two different values so we'll go ahead and keep accuracy there and let's print Accuracy, so let's save and run that wait for root and the accuracy that we got out of this is 0.96 so [ninety-six] percent accuracy on predicting what the price would be Shifted one percent of the days, so this would be let's go ahead and print Forecasts out so we can see what that value actually is So with linear regression just four just so you know The accuracy is going to be the squared error, and we'll talk about that Coming up next as we break down. How linear regression actually works, so this is actually still 30 days in advanced So it's pretty interesting that you'd still be that accurate [but] okay [so] it's okay. Yeah so now what if though [A] Couple things first of all its squared error, so this this actual percentage of accuracy is not Necessarily like [you] would get rich off this algorithm. It's almost like Maybe directionally accurate, but that's actually that's pretty darn pretty darn accurate So let's say though remember. I what we also brought in Svm. Let's say we wanted to use the support vector regression, which is not Simple linear regression, so we're not going to actually break down, but what if we wanted to use [a] [different] algorithm? So we're using linear regression. Here's how easy it is to switch our algorithm SvMs B are done so now we're testing a new algorithm, and This one does actually [a] lot worse wow let's run that more time. Let's see if it still is inaccurate Yeah, wow That's that's interesting. That's a huge difference [anyway], you will actually kind of find that that happens now for example [in] With machine learning like with support vector regression for example you have these things called Kernels So you might say Kernel kernel equals. I want to say the default is linear so let's try a polynomial Kernel so you can change these Kernels in there? And you can kind of fiddle with [it] and see if you could get better values. [oh] my gosh, so 51% accuracy is like just barely better than average or better barely better than like clique coin toss Let's see if we can actually get under 50 No, wow, that's that's a significant Variance. Oh goodness Anyway, as we can see [it's] [e] support vector regression is not what we're going to be using in this case anyways But so here's another decent example about why you'd want to follow this tutorial series though like what's a Kernel [alright] So we'll be explaining. What a kernel is when we get to support vector machines, but anyway what this was meant to show is how easy it is to switch between clatter Algorithms and this is the case whether you're doing regression, or whether you're doing classification or clustering you can switch out rhythms really quick So you you definitely want to like test the algorithms oops do we do oh Svm linear regression? Okay, so anyways. There's that one more thing to talk about Before I let you guys go is with the various algorithms you'll want to check the documentation so let me pull up the documentation for linear regression for example and What you're looking for is which algorithms can [be] threaded So in this case we [are] looking for and Underscore jobs this the question is how many jobs can we perform at any given time and so? Right now it may not be totally obvious to you. Why? with Let's say linear regression We can thread the heck out of linear regression as opposed to a support vector machine where? There are ways that you could do it You could like do it in batches or something there are ways that you [could] do it But it's just not inherently as easy as something like regression is to thread it Massively and run it in huge parallel operations, but with regression. It totally is and you'll see why Later on but anyway, so yeah So this is just so if if you were following along, and let's say you're skipping the true breakdowns shame on you but but to find out if it really quickly if the algorithm Could be threaded you would just go to the page right you can just search linear regression support vector machine on Google You'd find yourself on this page, and you're looking for N Jobs, so this just means how many jobs how many threads are we willing to run at any given time? So the default let me think here so the default for regression is actually 1 so this means it's running in I Hate to use the word linear here, but it is running linearly right whereas we could run it in parallel by doing N Jobs equals, and you could say okay? [I] want to run at least ten jobs at a time so I will run ten jobs at a time and in theory the training and testing part actually I'm sorry just the Training part would be significantly faster [or] you can use negative one and this will just run as many jobs as As possible by your your processor so Anyway speaking of which processor wise as you follow along this series if you get to a point where like I'm getting it really fast And you're going really slow like if you're following on like an older computer or a laptop or something like that [it] may take you a little longer to run some of these things this one should [be] pretty quick But it's conceivable especially when we get into like deep learning you might want to think about Spinning up a server or something like that, but that's obviously way down the line Well, we'll talk about it when we get there. I suppose, but just keep that in mind So anyways just remember this, but then again just one more Plug about why you'd want to dig in deep which we're about to be doing is To understand which algorithms you can do this with so exam for example with deep learning Like linear regression, not just in deep learning like all the algorithms really. There's a lot of regret a lot of like linear Algebra that goes in because a lot of these things can be Super threaded and you can run many operations at once and like so as we grow [in] processing power that becomes very useful to have Calculations and methodologies that can that can scale like that Anyway, that's it here in the next tutorial. We're going to be talking about predictions into the future Using Scikit-learn and then after that we'll actually going to be breaking down linear regression and doing it ourselves so stay tuned for that if you have questions comments concerns or whatever up to this point feel free to leave them below otherwise as Always thanks for watching thanks for all the support and subscriptions, and it's he'll next time 