 okay good morning it's a little bit early but I just want to start before we get our first speaker on stage by introducing a background to this lecture series so the first lecture of this year's conference sees a Posner lecture now the Posner lectures are named for IDI Posner who organized the first nips conference in 1989 and also formed the NIT Foundation he was tragically killed in 1993 in a bicycle accident so historically nips aim to invite speakers who were peripheral to the field to inform and dare I say it entertain attendees about related areas this worked effectively two main the diversity of interest of the community a diversity that we are rightly proud of this is a tradition that continues with our invited speakers at this year's addiction Edition now the personal lectures were begun in 2009 and have included such luminaries as geoff hinton toasted mouse key michael jordan and bernard shaw cough they were formed in recognition of the community's desire to also hear from its leading thinkers it is a great honor to be chosen although we are lucky that in the field we have many people who are worthy of such an honor this year the first personal lecture will be given by Zubin Gorham Arne now Subin completed his PhD in 1995 under the supervision of Michael Jordan he completely postdoctoral position with Geoff Hinton at Toronto before moving with Geoff to UCL to found the Gaspee computational neuroscience unit in 1998 in 2006 he was appointed to a professorship at the University of Cambridge where he founded the machine learning group in the engineering department subin's work has been recognized in a myriad of ways perhaps most importantly when earlier this year Zubin was elected as a fellow of the Royal Society a highly significant honor particularly for an American rather than attempting to summarize all his achievements in machine learning I'll give you a selection by reading from his certificate of election Zubin Guerra Marnie is a world leader in the field machine learning significantly advancing the state of art in algorithms that can learn from data he is known in particular for fundamental contributions to probabilistic modeling and Bayesian nonparametric approaches to machine learning systems and to the development of approximate variational inference algorithms for scaleable learning he's one of the pioneers of semi-supervised learning methods active learning algorithms and sparse Gaussian processes his development of novel infinite dimensional nonparametric models such as the infinite latent feature model has been highly inferential ladies and gentlemen I give you Zhu and Armani thanks Neil I'd like to dedicate this talk to my friends Samara Weiss my mentors over the years Arvind Joshi Geoff Hinton Mike Jordan Daniel walpert and David Makai and to my many brilliant students and postdocs I'm honoured to be giving this sniffs Posner lecture nips is my home conference it's my community your 3,000 of my friends out there and I've attended nips every year since 1992 except for one year that I didn't get a visa and maybe my greatest contribution to nips was back before we had corporate parties I used to organize the infamous Toronto Gatsby condo party which some of you may remember for about 10 years but anyway nips has obviously grown a lot since 1992 when I first attended so what was nips like in 1992 what did we know back then and what have we learned since well in 1992 neural networks were all the rage I just done my undergraduate thesis on recurrent neural Nets for parsing natural language there were NIF's papers this is a table contents from 1990 to the first page of table contents you'll see there were nips papers on things like you know regularization methods adaptive learning rates combinations of neural networks and reinforcement learning that year actually at nips Radford Neal published this remarkably modern paper showing how to do Bayesian inference in neural networks using Hamiltonian Monte Carlo this is something you could imagine this paper you could imagine it at this nips now that same year David Mack I was developing Laplace approximations for neural networks but already in nine ninety-two the tide was starting to turn against neural networks people were dissatisfied at getting stuck in local optima difficulties determining architectures numerous rules of thumb needed to train them effectively around 1992-93 support vector machines were being developed and often outperforming neural Nets they're mathematically very elegant and they ushered in a great deal of interest into complex optimization about a year later Radford Neel again showed that neural nets with one hidden layer converged to a Gaussian process that is a Bayesian kind of kernel machine in the limit of infinitely many hidden units so I work with you know messy finite neural nets when we can do exactly Bayesian inference in infinite ones was the thinking at the time also in the early 1990s we were introducing ideas such as the e/m algorithm graphical models variational approximations and so on to the nips community drawing links between neural nets and probabilistic models and this brings me to the main theme of my talk which is machine learning as probabilistic modeling okay so well what do I mean by probabilistic modeling what do I mean by model so here is sort of what I mean a model describes data that one could observe from a system that's what a model is in my view if it can't describe possible data then it's hard to know whether it's a good model or a bad model if we use the mathematics of probability theory to represent all the forms of uncertainty that we have in all aspects of our model and all the noise then the good news is that we can still use probability theory in the form of inverse probability what's known as Bayes rule and that allows us to do inference about unknown quantities adapt our models make predictions and learn from data so it's actually rather straightforward you're all familiar with Bayes rule you may be familiar with this image of Thomas Bayes it's a long story but it's actually not Thomas Bayes but this is the best image we have so we use it anyway some poor chap who's not Thomas Bayes anyway oh you're all familiar with Bayes rule written as a way of inverting distributions over X's and Y's I like to think of it more in words like this so what Bayes rule allows us to do is it allows us to do reasoning about hypotheses given data so before observing the data we have a distribution over possible hypotheses that represents our uncertainty about what might be a good hypothesis for the data the hypothesis could be any kind of model model parameters etc now each hypothesis is as I said a model so you can make predictions about data so we can evaluate the likelihood the probability of the data under the hypothesis that's the second term here and then Bayes rule tells us how to combine these to do learning to figure out a posterior distribution over hypotheses given the data that's all learning is it helps us do inference about hypotheses from data and learning and prediction can be seen as forms of inference and the beauty of this whole framework in a slide that might be familiar if you've heard me speak before because I always use this slide it's a very powerful idea in my mind the beauty of the framework is that everything follows from two simple rules of probability theory the sum rule and the product rule the sum rule says that the probability of X can be written as a sum or integral over some other variable Y of the joint probability of x and y the product rule says that the joint probability can be factored into the marginal probability of x times the probability of Y given X now this isn't just a boring rule about probabilities if we now replace X's and Y's for the kinds of things we care about in machine learning d for data theta for parameters of a model and M for say the structure of a model then we get a simple rule that tells us how to do learning from data we start with some prior over the parameters of the model that represents our uncertainty about sensible parameter values before observing the data we have our familiar likelihood term which many of you are used to maximizing but we're not going to maximize anything here we're just going to follow the summon product rule we combined this prior and the likelihood and we get the posterior of our parameters given data and that's learning now if we want to do prediction let's say there's some unknown quantity maybe it's something having to do with test data or missing data let's call it X then again Bayes rule well it's just a corollary of the sum rule and product rule prediction is also just a corollary of the sum rule and product rule what the prediction rule says is what you should do is average the predictions of all of the different possible parameter settings weighted by the posterior probabilities that you computed when you did learning that just follows from the sum rule and the product rule and now if we want to do model comparison we just apply the same thing at the level of different models and so so it's a very simple framework and it's very widely applicable now let me focus a little bit on model comparison here's a familiar form of model comparison imagine we have you know these eight data points and we're trying to model the relationship between some output Y and some input X and we could fit different polynomials to it so the blue lines show you 0 if that is the constant linear quadratic cubic etc up to seventh order polynomials with maximum likelihood fits to the data and clearly here you can see one of the great you know problems in machine learning which is we don't want to under fit which is we don't want to miss out on structure that may be in the data and we don't want to over fit we don't want to fit a model that makes ridiculous predictions so we want to avoid under fitting and overfitting and model comparison is very central concept in machine learning I've shown it to you for polynomials but let me show you lots of other examples of learning model structure and model comparison questions like how many clusters should there be in the data what should be the intrinsic dimensionality of a dimensionality reduction method when apply to the data if you're doing feature or variable selection is this input relevant to predicting that output if you're fitting a dynamical system what should be the order of the dynamical system how many states should you have in your hidden Markov model how many layers or units should we have in your neural net what should be the structure of a graphical model that you learn from data these are all examples of questions about learning model structure they're all analogous to that question that I asked with polynomials and the beauty of the Bayesian framework is the questions may all be different but the basic answer is just apply the sum rule in the product rule so to do model comparison we get something from those rules which we've called people have called Bayesian Occam's razor that is if we want to compare model classes m and m prime may be different polynomials or different numbers of clusters whatever it is then you simply compute the posterior probability of the model given the data that's just Bayes rule and this term in red is a really interesting term this is the marginal likelihood sometimes called the integrated likelihood or the model evidence it's the familiar likelihood that we are all used to maximizing but we're not going to maximize it we're gonna integrate the likelihood with respect to the prior because that's what the sum and the product rule tell us to do they don't tell us to do maximization so what is this marginal likelihood and why does it capture Occam's razor this preference for simplicity well here are several ways we can interpret the marginal likelihood we can just read out what this equation says in words it's the probability of the data under the model averaging over all possible parameter values not maximizing it's the probability that randomly selected parameters from the prior would have generated data D so again you know a model that's overly complicated will be penalized if you like information theory log base 2 of 1 over the marginal likely there's a number of bits of surprised at observing data D under model m so we should prefer a model for which the data is less surprising and so if we consider this abstract diagram where on this axis what we have is say all data sets of some size n then different models of different complexity placed different distributions over possible data sets a simple model may be characterized by concentrating its probability mass on what we could call simple data sets something like this green curve a more complex model still has unit probability masses these are probability distributions that have to integrate to one right so it can spread its bets over many more possible data sets but by doing so it can't model simple data sets as well as a simple model under this marginal likelihood criterion so then if we observe a particular data set D what happens is that model classes that are too simple like the green ones are unlikely to generate that particular data set so they're penalized that way model classes that are too complex can generate many possible data sets so again they're unlikely to generate that particular data set at random and so that's how Bayesian Occam's razor works and if we just look at that essentially apply to this polynomials example that we had a minute ago again here we have the same examples before the blue curves are the maximum likely curves the green curves are samples from the posterior distribution of polynomials given the data where I've just chosen a fairly harmless Gaussian prior over the parameters and inverse gamma prior over the noise variance and the interesting curve is this thing this is the a plot of the marginal likelihood as a function of the model complexity the order of the polynomial what you can see is that the marginal likelihood very sensibly says well given this data it could be a constant function it could be quadratic maybe could be linear or cubic but you know the marginal likelihood for fourth order to higher is very small on this plot it's there but it decreases exponentially and that is because basically those models are too complicated for this particular data so you see Bayesian Occam's razor at work you know rejecting models that are either too simple or too complex and this concept can of course be applied to all of those problems that I talked about before throughout machine learning and we've done a lot of that over many many years now the although in the polynomials case is straight forward to you know compute the evidence analytically for a lot of those interesting complicated models we reach the point where we need to worry about computation so in general for Bayesian inference we have to compute things like the posterior distribution over parameters and the marginal likelihoods that i've been talking about and these involve computing integrals over parameter space and computing integrals is a bit more expensive than doing optimization but the good news is that we have a very very well-developed series of approximations for computing integrals going all the way back to laplace and probably earlier we have Laplace approximations the basing information criterion which only requires you to do optimization we have variational approximations which turn integration problems into optimization problems again but optimization over the space of distributions algorithms like P MC MC sequential Monte Carlo exact sampling annealed important sampling many many many methods so all that's required to apply this framework is to familiarize oneself with tools from this set of approximation methods okay so that concludes the sort of the tutorial part of my talk and now I want to come to a question that I think is very high on my mind and that is of course we can do a lot of machine learning without the probabilistic framework there are many many things we can do in machine learning that seem not to require us to use probabilities or to think in this Bayesian setting so when do we actually really need probabilities and I would argue that there are actually a lot of interesting problems for which the probabilistic approach is essential so many aspects of learning and intelligence intelligence depend crucially on the careful probabilistic representation of uncertainty I'm not saying everything in machine learning has to be done this way clearly we can make great advances without having to do this but for certain problems like forecasting it just doesn't make sense not to have a probabilistic representation of your uncertainty for decision making when you have to consider the consequences of your actions into the future it really seems to make sense to have to represent uncertainty in probabilities when you're learning from limited noisy and missing data probabilities come in very handy when you're learning complex personalised models models in which you know you may have lots of customers or patients and for you might have a lot of data but for each one you might have a very small amount then it it does seem to help to think about your uncertainty data compression I'll talk about this is fundamentally based on the representation of uncertainty and finally if we're trying to automate scientific modeling discovery and experiment design then uncertainty plays we'll roll so these are the things I'm gonna focus on I'm gonna talk about some of the current and future directions in probabilistic machine learning the things that I'm really excited about myself and things that we've been working on so this is a fairly long list but I'm gonna go through them all quite quickly just to give you a flavor for lots of different problems that are interesting I think the first one I'm going to talk about is Bayesian nonparametric s-- and Bayesian nonparametric s-- addresses a particular issue with probabilistic modeling which is that probabilistic modeling isn't isn't an answer to everything you know it depends on the quality of your model and we need flexible and realistic probabilistic models to be able to deal with complicated realistic problems so the Bayesian nonparametric approach to building flexible and realistic promising models is to define infinite dimensional probabilistic models using tools from stochastic processes and I'll try to explain that a little bit more detail and the picture is sort of a picture of a Gaussian process which I'm going to talk about in more detail there are lots of examples of this Gaussian processes dear Ashley proceeds infinite hidden Markov models Chinese restaurant processes Indian buffet processes etc and the way we can think of these examples is by organizing them into a sort of table where we can think of different applications of Bayesian on parametric's so let me just summarize the sort of key idea and basing nonparametric it's a simple framework for modeling complex data I'm saying it's a simple framework because it just follows from the sum rule and the product rule so the framework is simple but the models we're gonna use are gonna be rich complex models these infinite dimensional models these models with infinitely many parameters and clearly we can't optimize infinitely many parameters and that's why it's useful to do Bayesian inference over those infinitely many parameters now the nice property the Bayesian nonparametric models have is that their predictive complexity grows with the of data so as I get more data the models themselves are going to become more rich and complex do better predictions and the examples are you know if you take simple parametric models like polynomial regression then you can consider instead using something like a Gaussian process as an example of a way of doing nonparametric function approximation or instead of using logistic regression you could use Gaussian process classifiers instead of mixture models you could use the early process mixtures instead of hmm so you could use infinite HMMs etc and let me focus a little bit on Gaussian processes in particular but before I do that I'm just gonna give throughout my talk I'm gonna give a few pointers the things that are going on at nips that that are relevant to this and there are actually two workshops one on Friday and one on Saturday which will be talking about Bayesian nonparametric s-- so let me focus on Gaussian processes consider the problem of nonlinear regression you want to learn some function f with some uncertainty or error bars from some data here shown as these points in magenta these are pairs of X's and Y's very much like this example of polynomials that you know I showed you before so instead of thinking about models in terms of a finite set of parameters what Gaussian processes allow you to do is to define a distribution over functions let's call it P of F which can be used for Bayesian regression loosely speaking you're just applying Bayes rule over the space of functions although these functions are these uncountably infinite ly dimensional objects now what makes any of this at all feasible is that Gaussian process is defined by the fact that any finite subset of points call them x1 through xn subsets of the domain input domain X at those finite set of points the marginal distribution of the function values which is now an N dimensional vector has it has a multivariate Gaussian distribution so Gaussian process is an infinite-dimensional version of a multivariate Gaussian distribution but the good news is if we're going to do inference with Gaussian processes all we have to do is operations on these n dimensional Gaussian vectors so we can do all of this in sensible finite amounts of computation time and memory and Gaussian processes can be used anywhere where you have a need for an unknown function for a regression obviously classification ranking dimensionality reduction etc now it's interesting to relate Gaussian processes to other models and I love creating these cube figures which give you relationships between models and this particular cube figure is starting with this corner being linear regression and what we're gonna do to linear regression is three different operations the magenta operation is turning a regression model into a classification model so linear regression becomes logistic regression when you do that which should really be called logistic classification the blue arrows turn a classical model where you usually do point estimation into a Bayesian model where you integrate over the parameters so you get Bayesian linear regression here and then the orange arrows are kernel izing things so you can take linear regression map it map your inputs into some feature space which may be infinite dimensional and then you get kernel regression those are the orange arrows now if we apply orange followed by magenta or maybe magenta followed by orange what we get is kernel classification which is where support vector machines live they are kernel classification linear models if we apply orange followed by blue or the other way around we get Gaussian process regression which I have just defined and then if we do each of these three operations in any order we get Gaussian process classification from linear regression so all these models are very nicely related to is very useful to understand how they're related here's another interesting relation which i've alluded to before if we consider a neural network so here is a neural network mapping from X is some inputs to some outputs Y and through some layers of hidden units and usually the parameters of the neural network we call them weights but we could just say use the symbol theta for the weights of the neural network then instead of doing optimization of the parameters of neural net you can just do Bayesian inference it's very straightforward you define a prior which may correspond roughly to what you would have used for regularization and then you try to approximate the posterior over the parameters given the observed data and prediction becomes averaging over these different parameter values now what Radford Neal showed which I mentioned before was that a neural network with one hidden layer and infinitely many hidden units in the limit of this thing becoming infinitely wide if you put Gaussian priors on the hidden to output weights then that converges to a Gaussian process now in his 1994 paper actually I went and looked at it again he has a whole section where he he even tries to analyze what happens for infinitely deep networks which was quite interesting it's less relationship between Gaussian processes and neural networks of course we're talking about one hidden layer neural networks with infinitely wide layers and the properties of multiple hidden layer networks with finite way width are somewhat different than these particular ones that converge to gaussian proceeds but it's very useful to see what the actual distribution over functions captured by the neural network is now there's a lot of recent work that that brings these two fields together the Gaussian process field and the neural network field looking at models like the deep Gaussian process by Damiano and Neil Lawrence who introduced me just a few minutes ago there's some lovely work by Tang bui and colleagues looking at inference and deep GPS at the workshop on approximate inference there's some very nice work on deep kernel learning by Andrew Wilson and colleagues and we've done some work interpreting drop out in neural networks as a form of Bayesian inference with Yaron Gao so these fields are coming back together and we re analyzing and understanding the relationships very nicely the second topic I want to talk about is probabilistic programming and this is an area that I'm you know incredibly excited about and let me try to convey why I'm excited about this area so the problem that probabilistic programming is trying to address is that probabilistic model development and derivation of inference algorithms is difficult it's time-consuming its error-prone okay so this stuff is usually done by hand but just like nowadays you don't convert from high level programming languages to machine level code by hand we have compilers for doing that probabilistic programming offers a beautiful and elegant way of automating the process of deriving inference algorithms so here's how it works the solution is that you have a probabilistic programming language which expresses probabilistic models as computer programs that generate data these are simulators in the kind of same sense that I talked about models being things that generate data you just write them in some high-level general programming language maybe a turing-complete programming language that is able to express you know universally expressed probabilistic models that can express any computable probability distribution it's possible to do that so you write down your model in that very general format this is much more general than say a graphical model then the amazing thing is that we can actually develop universal inference engines for these languages that do inference over the hidden states of your computer program those hidden variables in your computer program those calls to the random number generator given observe data so you write your simulator and then you say well I don't want to think about simulated data I want to model this actual observed data can you please reason about what the hidden variables in my simulator should have been to match this observe data and the universal inference engine will do that and these things already exist which is probably surprising they're not incredibly efficient but they're getting much much more efficient every year to the point that we will be able to hopefully replace hand derivation of inference methods now there are many languages and there's a long history to promise like programming things like bugs stand blog church venture Anglican probabilistic C you know whatever your flavor is there is a language developed for doing problems like programming and we've developed a few of them recently ourselves and there are so many inference algorithms in this engine in this Universal inference engine the backend that have been developed many of them based on MCMC ideas particle filtering particle cascade MCMC methods for Big Data etc now here's just like a quick picture of what a promising program might look like for something like a hidden Markov model this is a graphical model for a hidden Markov model this is a probabilistic program for a hidden Markov model it's fairly short I'm not going to go into it any detail it's written in our in our version of julia problems like programming language and the basic thing is that you just write down your model and you say okay predict the states you told it what the data is and predict the states and then it goes and does inference and now if you want to modify your model let's say you want to go from a hidden Markov model to a bayesian hidden Markov model or to a switching hidden Markov model or something like that you just have to modify a few lines of your code and the inference will automatically then work on that now I really think this could revolutionize scientific modeling machine learning and AI once we are able to abstract ourselves from this and I hope that many of you were able to attend Frank woods tutorial yesterday on like programming and if not then there is also a workshop on Saturday on black box learning an inference which is about promising programming the third topic I wanted to talk about is Bayesian optimization another thing I'm incredibly excited about the basic idea of Bayesian optimization is the following is trying to solve an incredibly general problem which is global optimization of functions that are expensive to evaluate so imagine this is the problem they're trying to solve you have some function f of X you want to find the the optimum X star and I'm calling these blackbox functions because all I'm going to assume is that the only way you can access F is you give it an X and then outcomes f of X you might be able to also access derivatives and so on but we're not going to necessarily assume that you can do that this can all be extended to handle derivatives and now the key idea is that these functions are expensive to evaluate these are maybe experiments in the real world or something like that okay and so we want to be able to optimize these functions as quickly in as few function evaluations as possible so instead of trying lots of random things or whatever what we need to do is act rationally so the solution the basin optimization gives you is to treat the problem of optimization as sequential decision making under uncertainty and the uncertainty is about what is the actual function value at points I haven't evaluated yet so here in this picture let's say I've evaluated the function at these three points I know the function value is at those three points maybe it's a bit noisy but I don't know where it is in other places so I model it I model my uncertainty about the function before I've actually evaluated it and then I have an accusation function that tells me what is the optimal greedy next decision to make to learn where the optimum of this function is so that might be here then I evaluate here and then my acquisition function changes and I need to evaluate somewhere else now this has a tremendous number of applications from you know robotics getting the gait of a robot to drug design to learning neural network hyper parameter is many many things where you have to do expensive experiments and here's just an example of some of the work we've been doing with many colleagues listed here on base in optimization criterion called predictive entropy search and the basic thing that you want is curves that go down quickly in the sense of like you know here's some error metric lower is better and you want to come down as quickly as possible with as few function evaluations as possible and you can do this with constraints as well optimization with constraints which is something that we've recently been working on and if you're interested in more then there is a workshop on Bayesian optimization on Saturday okay so the fourth topic I want to briefly talk about is data compression and you're all familiar with data compression you know if you ever do gzip or zip or you know if you ever use the CD or the Internet you know you're dealing with data compression all the time now the problem data compression tries to solve obviously is that we often produce more data that we can store transmit and we need good methods for compressing data now the solution is quite interesting it turns out and I'm sure many of you already know this but it turns out that by Shannon source coding theorem all compression algorithms are implicitly based on probabilistic models ok so every compression method out there is basically a promising model now developing better sequential adaptive nonparametric models of data then allows us to predict the data better making it on average cheaper to store or transmit so by improving promising modeling we can implicitly improve compression and we can do that actually very explicitly in certain cases and we've been doing some work this is with Kristian Steiner Kuhn and David Makai developing a new compression method called ppm DP which is based on a probabilistic model that learns and predicts symbol occurrences in a sequence and in fact the underlying concepts are actually hierarchical Bayesian nonparametric models and some of you may be familiar with for example the sequence memorizer which is also in this same line of thinking it works on arbitrary files wood delivers basically cutting edge performance at compression for human generated text human generated text is you know things like text or source code or music and things like that okay these models are also useful for smart text entry anomaly detection sequence synthesis so they're not just compression it's not just for compression that we need models of sequences and here's some you know example of how you evaluate different compression methods here's some results these are a whole bunch of different data sets these are all different kinds of data sets and then these are different compression algorithms on the columns gzip is the first one here purple is bad these are bits per symbol okay so gzip is relatively speaking a pretty bad compression algorithm compared to what's really actually at the state-of-the-art our methods ppmd PR these last three columns and we're basically for a single compression algorithm not one that's based on a humungous ensemble of different methods but for a single compression algorithm we're basically better than all the other methods except for interestingly a few of these datasets where L zip is better and the reason we think is that these datasets are actually computer generated they're not human generators so their statistical structure is actually very different and you would imagine that maybe some combination of ppmd P and L zip would produce incredibly good compression rates okay the next topic I want to talk about and I've only got two more is the automatic statistician by the way these things are all in my mind at least related there's there's many common threads to all this and I don't really have time to describe all the common tools but I mentioned that for example compression is based on basing nonparametric s-- and Bayesian optimization is often based on Gaussian processes and so okay here's the automatic statistician this is a project I'm I'm really excited about it's been you know on my mind for about ten years to do and only in the last two or three years we've really managed to make some serious progress and the basic problem this is trying to solve is that data are ubiquitous as we all know there's great value from understanding data but they're not enough people out there like you guys to analyze all the data they're not enough machine learning researcher statisticians and data scientists out there so if we think about that well what's the natural solution to that well let's automate some aspects of data analysis we're not gonna be able to automate everything but we can automate certain things so we're gonna develop a system and we have been developing a system that automates model discovery from data processing the data searching over models discovering a good model explaining what's been discovered to the user and the the last one is actually really important we don't just want a black box that's doing prediction we want something that will also be able to do interpretation okay so here's some ingredients of the automatic statistician there's an open-ended language of models from which lots of models can be generated there's a search procedure for finding good models there's a principled method for evaluating models and that sort of the basis of that is the marginal likelihood that I've been describing although we can also use cross-validation error for example for that there's a procedure to automatically explain models from data and the goal is to go from sort of raw data to these reports which I'll talk about and for the example that I'm going to talk about the language of models that we have is going to be a language composed of words these atoms are going to be kernels of Gaussian proceeds so these are just five based kernels that produce different kinds of functions and we can combine them into you know by doing addition or multiplication of kernels and much more complicated kernels and with all that then we can do search over good models for data so this is model search for time series here's a famous data set called amount Aloha healing curve using climate science here's the observed data and this is the model fit and the extrapolations it starts it evaluates the base kernels and then it picks good ones and expands on that and keeps going using the marginal likelihood as a guide and knowing when to stop when the marginal likelihood starts going down and this is the sort of final model that it comes up with and then it generates an entirely automatic analysis with some textual description of the form of the model so this is the executive summary of what it's found and actually what it ends up producing are these 10 to 15 page reports which we cheekily formatted like nips papers okay so these are automatically generated reports from the automatic statistician and you can look you can look at them if you want by going to the web page lots of examples of these reports now the automatic statistician doesn't just produce interpretable reports by doing this systematic model search we're actually ending up with models that have very good predictive performance really state-of-the-art predictive performance at extrapolation this is sort of an example average over 13 data sets and root mean squared error standardized here so you know three times better than linear regression is sort of down here okay and another thing that we've been thinking about is that we want our automatic statistician to be self critical okay we wanted to do good statistical modeling which includes model criticism you know asking the question does the data match the assumptions of the model and we've developed methods based on posterior predictive checks dependent tests and residual tests to implement that self criticality in our automatic statistician and we're also doing some work on you know more systematic nonparametric approaches to model criticism this was in a paper presented by James Lloyd last night here so the final thing I want to talk about is a key ingredient actually of the automatic statistician but it's a separate concept actually in some ways this is the rational allocation of computational resources again a place where uncertainty plays an important role the problem here is that many problems in machine learning and AI require evaluating a large number of models on potentially large data sets so that's potentially very very expensive to do a rational agent needs to consider the trade-off between statistical and computational efficiency and the solution to that that we have is to treat the allocation of computational resources is a problem in sequential decision-making under uncertainty so again you you test out different models here's how it actually works we we extended a beautiful paper called the freeze-thaw bayesian optimization method by surcease Newton Adams to build predictive models of how things will perform if they run longer so based on their performance running them for a short amount of time we try to get them to run longer and form ensembles in the model combination framework and then we allocate our computational resources based on what's actually more or less promising and I have a video I could show but I'm gonna I'm gonna skip it in the interest of time ok now a very nice thing is that when you allocate computational resources reasonably you can do great things and this I don't want to take any credit because this was a bunch of people in my group I wasn't really involved put together a system that actually won first place at the most recent round of the Otto amel classification challenge to quote design machine learning methods capable of performing all model selection and parameter tuning without any human intervention and so they're doing machine learning under strict time memory and disk space constraints and they're doing that by running a bunch of machine learning methods and then having this sort of Bayesian reasoning system on top of it figuring out how to allocate the computational resources to individual things and I'd love to show you the video but I'm gonna skip that so we have time for questions so if you're interested in this there's actually a nips workshop on challenges and competitions and machine learning well that were there talk about the auto mail competition on Saturday so to conclude probablistic modeling offers a very powerful framework for building systems that reason about uncertainty and learn from data and that goes beyond the sort of traditional pattern recognition problems to think about all sorts of other things and I'd be briefly reviewed a bunch of things that I'm personally really excited about at the frontiers of research in this field a lot of what I talked about actually was covered in a review paper I wrote earlier this year in nature called probabilistic machine learning and artificial intelligence so if you like this stuff have a look at that paper which talks about in a little bit more detail I want to thank my collaborators and I want to just do two more things one of them is to announce this very exciting new venture which is the Alan Turing Institute we have a booth outside if you're interested is the UK's new National Institute for data science it's a joint venture between a bunch of universities Cambridge Edinburgh Oxford UCL and Warwick and it's located in this fantastic location in central London and the alan turing institute is looking for on the order of ten or fifteen Alan Turing fellows and these are really great jobs three to five years independent research in any aspect of data science including machine learning the deadline to apply is very soon so I just just look Alan Turing fellowships if you're interested in and one more thing I'm this is the first public announcement and I'm delighted to talk about my involvement in a New York based startup called geometric intelligence we've got some great technology and that can learn from much less data and we're hiring so alright thank you questions start with this gentleman at the front so assuming that was a fabulous lecture as always so there's a revolution that is occurring in the business world data are being digitized at a prodigious rate and in a recent report by the the that there was a McKinsey which is a consulting company predicted that the next ten years forty percent of all middle-class jobs will be eliminated or at least forty percent of the tasks of people who are shuffling papers will be all digitized and then susceptible to machine learning techniques now what I've learned from you is that not only will these middle-class jobs be taken over but your automated a statistician will take over a lot of machine learning jobs it's a great question on the one hand I am actually worried about where this is all going and I think there's some serious issues you know that are coming with the process of science but you know let's be clear the automatic statistician is not and I've had statisticians complain to me it's not going to take over the job of statisticians think of them as very powerful tools that will make the people out there more efficient and they'll also make a lot of those people who can't afford to hire a machine learning researcher statistician have some nice online you know server where they can upload their data and then it'll do some analysis for them for free basically so you know it should make everybody more productive so there are the questions they want to quickly come to the mic now you must have some eyes okay question no question it just a quick question about the Julia Code you flashed on the screen is is that does that work or is that just a scheme um okay it works and as of last week it works even better okay so it worked to the extent that it it does in behind the hood we can implement any model we want and then it will do a form of particle MCMC inference on our problems like programs but the problem was that for some technical reasons namely co-routine cloning it wasn't actually very efficient to do this in julia so my student you know heroically went and and rewrote bits of julia to be able to get that to be efficient as of a week ago so I think it's actually reasonably efficient now but there are many choices for problems like programming languages out there and I don't I'm not trying to push any one of them I'm just trying to promote this whole area which i think is really exciting sure thanks Zubin you've made some really interesting comments about the importance of having representation of uncertainty from any task can you comment on how important it is to have a proper probabilistic representation of uncertainty compared to having more predictive representations of uncertainty yeah that's a that's a very good question there many ways of representing uncertainty you can look at for example you know intervals is representations of uncertainty confidence bounds which are not sort of you know Bayesian predictive probabilities and so on I'm generally in favor of representing uncertainty now I I think that I particularly favor the Bayesian representations because they're actually quite modular in the sense that we can combine we can pipe together many components that have representations of uncertainty and get a coherent representation over overall uncertainty something that is more difficult to do with confidence intervals or with or with other kinds of intervals so I think there are some nice advantages but this is not the only representation of uncertainty is true okay one last quick question so thank you very much for a career orientation of the fundamental to cutting it so I'm very much in that impressed with the automatic death data scientist idea so for example I've been working on automatic building of a neural model based on some underlying knowledge about neuroscience so for that auto repair citing scientists to make a model how much do you think you need we need to implement the knowledge about physics or biology for the mechanism underlying data that's that's a great question and essentially what we're targeting with the automatic statistician is essentially agnostic tools so basically you upload a table of data and it goes and turns away and tries to come up with a reasonable model the data and then explain it in words to you clearly for a lot of scientific applications you want to put in as much prior knowledge as you have and for those you can go either two ways you can build it yourself or you can if they're repeated forms of data analysis that you need to do of a particular kind you could build a special form of automatic statistician that has sensible knowledge about that particular domain that is good at producing analyses of a particular kind of data set like examples that I've looked at are not in neuroscience but for example for gene expression microarray data you could build a special system for that okay I'm gonna stop there I think yeah great yeah so if we could just thank Zubin again for a really great start for the conference you each year Microsoft Research hosts hundreds of influential speakers from around the world including leading scientists renowned experts in technology book authors and leading academics and makes videos of these lectures freely available 