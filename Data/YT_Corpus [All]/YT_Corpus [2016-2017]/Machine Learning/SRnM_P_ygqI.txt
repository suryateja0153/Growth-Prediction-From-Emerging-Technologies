 Hello, everybody, I'm very happy to be here, this is my fifth time in Barcelona, and my most favorite city in Europe. So today I'm going to be talking about the process of building. "Building a Recommendation Engine with Machine Learning Techniques." By: Brian Sam-Bodden. And it's the early part of the process. I don't know what I'm doing. I need to figure out what the hell I need to do to actually make this happen. And to add more complexity to that, I decided to turn it into a product, so I could actually make money and retire early. So I am Brian Sam-Bodden. Again, FullStackFest, what a wonderful show. I'm @bsbodden on Twitter. I've been writing code since about 1986 on whatever I could find and get my hands on. I am from Panama, as you can tell from the British accent. And I lived in Arizona, the state of acceptance and tolerance. I'm the CEO of a small company, and we do Ruby, Rails, Java, and whatever can keep the lights on. So this talk, it's about the glimpse into the ML world, into the machine learning world. And what I've gone through to get anywhere. One of the funny things that happened at the beginning is that I took an AI class in 1994. And I thought that after graduation, that's what I was going to be doing. AI. Everything was going to be AI. Fast-forward a couple of years, I was doing tax software. And then after that, accounting software. And then I thought this is not what I signed up for. So I've been trying for the last few years to bring that back and actually enjoy programming and have my brain really engaged in what we do. So this talk, it's also a backgrounder on recommendation engines. And a bit of a primer on classification. And specifically text classification. Classification is really the concept that it's behind most of the really cool AI that's happening right now. Whether it's supervised learning or unsupervised learning, or deep learning versus shallow learning. It is at the core. So this subject is really broad and deep too. Just for this presentation, I probably read close to 40 Ph.D. dissertations. And it made me realize that I had forgotten 90% all the math that I need to survive as a computer scientist. So that's on my agenda to go back and relearn a lot of the math. This talk, it's not about everything that you need to possibly know about recommendation engine. And it's also not a bloodbath into the math because otherwise you guys would be asleep. And also not a lot of code, and there's a reason behind that. So let's start talking about what a recommendation engine is. Most of you interact with one every day. Most of you have probably built a simple one. Maybe a few of you have built a really complex one. But they are a fact of life. If you have a system that sells or promotes or displays any kind of information, the amount of information nowadays is growing at a pace that we cannot keep up with. And searching doesn't cut it anymore. Half of the time, I spend, you know, 20 minutes trying to craft the search query on Google to find what I need. So now we need to actually invert the equation and have the systems smart enough and know UL enough to push stuff in your direction. So in 2004, I read this book on the recommendation of a friend of mine. And this book is called the paradox of choice. And the premise of the book is that there's so many choices that we are completely overwhelmed. And the side effect of that feeling of being overwhelmed is that we do nothing. How many times have you been looking at the TV Guide for a full hour where you could have watched a whole movie, at least a short action American movie. With very little dialogue. But you spend an hour searching for a movie and at the end of that searching, you go, no, I don't have an hour to waste on a movie. But you spend an hour looking for it. So that's to me all the time, and it's an exercise on really now knowing what's on the TV Guide but never watching anything. So a few things happen. First of all, it's the -- and Americans are really good at this. The illusion of choice and freedom. By having all of these choices, we feel really free. But what happens is that we end up doing nothing, or we end up being overwhelmed and then our happiness levels go down. Now, when did that what happens at a website, and you're the owner of that website that means sales are going in the direction of that happiness. They're going down too. As I mentioned freedom in commitment. We feel that we're fighting those two forces. When we have to spend so much time crafting the idea of what we want, it's really hard to then get to the inline and be, like, okay. Let's do it now. How many of you have put a book that you spend hours searching on a shopping cart and abandon that shopping cart? Show of hands? Exactly. That happens to me all the time. And sometimes it's because I'm a cheapskate. But sometimes it's because I feel that the effort that I put into looking for something, that's -- it went into the search and not the actual reward of getting. So it was the destination versus the travel. I spend the travel, the energy in the travel and did not get the reward at the end. Also, our brains -- and according to this book, our brains create little -- you know, you can think of many rule engines of how you go about behavior when you purchase something. So, for example, one of the things that I typically do, and this is what they call second order decisions. It's any time I go to a technical bookstore, I put a book on the shopping cart, and then I -- let's say for a new language, Elixir. Everybody seems to be getting that one now. And what I do right away, I spend time searching for all the Web frameworks built with that new language so that I can pick the two books and buy them together. Because typically Amazon or some website will bundle them and give you a discount. So I'm taking the whole ride together, I'm getting the language and the framework. And what happens is I end. Putting those two books in there, and then I start questioning my decision. And what happens, it stays in the shopping cart until the book's out of print. I've gotten those noises. It's, like, you have a shopping cart that has a book from, you know, 2005. You probably want to, you know, get rid of it. And then the miss opportunities problem is that every time we spend so much energy in that journey, then we question what other avenues we could have taken. So if somebody -- if you go to a bakery and there's hundreds of cakes, you're likely to basically walk away because of the number of choices. I won't. I will eat the cake. I'm telling you that. But sometimes you say, well, you know, I haven't had it in 20 years, but I really like key lime pie, so then you basically end up in this paradox of choices. And if you take that into the shopping cart world, that means an abandoned shopping cart. Is to recommendation engines are built to come back this paradox of choice. They also help us with the exponential explosion of information. And they're now coupled with growing user expectations. Users want more. They demand that the systems know them. One, for example, I want Netflix to really figure out what my taste in movies is. Which it seems not to have it right, and they are probably at the leading edge of AI research in recommendations. So a recommendation engine, let's define it so we can actually now move into his how one of these things is put together. It's the level of interest that a user might have on a item. And an item could be pretty much anything. Let's think of it as a product on a E-commerce site, which is typically what we're talking about. But it could be an article on a new site. It could be another human being in some kind of dating site. There's a lot of different things that as long as we can classify, we can compare them. And if we can compare them, we can do certain things to be able to recommend them to you. So, again, an item, it's anything. A book, a movie, a travel destination, articles, recipes, clothing. The -- one of the great examples of how the paradox of choice is being combat in the U.S. right now and probably a lot of different places are the places that will tailor and outfit a fashion persona to yourself and send you an outfit every month. My wife loves this place. I started trying it. They just don't seem to get this part of my body to kind of work correctly with their outfits. But other than that, it's amazing. They send you something every month. And whatever you return, those become basically unlikes in your profile. So now they can mine that information. And, first of all, figure out what other users are like you in terms of body type, style, demographics. And they can use that now to recommend different symbols for people to wear. So it's a pretty interesting world that we're living in. One of the most common and famous recommendation systems, one of the first ones is Amazon. I love that I have a laser. It's -- I haven't had one of these in a long time, and I'm really excited about using it, so watch your eyes. So Amazon is one of the first retailers that started recommendations, and they did it well. But at some point that system did not scale. And I'll talk about some of the things that they did to actually make it scale. And there's two different types of E-commerce environments. You have places where you have hundreds of thousands of items and maybe thousands of customers. And then you have the ones where you have millions of customers and hundreds of items. And then you have the worst-case scenario -- well, not for your bank account. But where you have millions of items and millions of customers. And to be able to compute those recommendations becomes a pretty expensive pass, computationally speaking. So at the forefront of recommendations for Netflix. Netflix uses information about the users in their behavior. And other users behavior, and also about the items themselves. So combining those two concepts, it's something that has become a trend. Early on, there were recommendation systems that did one or the other. Either used the collective intelligence of the masses, or tried to figure out who you are, and what you want. And try to figure out what this item is about. What this book is about, and can we match those two? ITunes have basically kind of dropped the ball in the last probably five years. And it is weird because Siri and all the other systems have advanced so much where the iTunes recommendations have been left behind. Maybe that department isn't really well staffed nowadays. Zappos, another place that uses very simple recommendations because as I play with a lot of E-commerce sites to learn about this, what I notice about them is they just show me different color variations of the item that I was looking at. Even though I have purchased things before. So I could be wrong. It could have been a fluke. But that's what happens. And there is no way I can actually wear those shoes. I'm not hip enough or young enough anymore. So Netflix in 2009, they did the Netflix prize. And they offered a million dollars to anybody that could beat them at the recommendation game by 10%. And they make machine learning sexy again. Of course not everybody Sadat scientist. I've seen people that, you know, have two years of programming, and they're a data scientist. And I'm, like -- so I'm a want to be data scientist, but I can tell you I'm not anywhere near to basically have enough knowledge to call myself that yet. So getting back into recommendation engines. Now, there's a couple of ways to do this. And I kind of went around, beat around the bush to kind of mention the two ways. But I'm going to formally define them now. So the recommendation approach is that there are one that it's the wisdom of the masses. It's called collaborative filtering. And it relies on a lot of data on the actions of your users. So, for example, if I'm on a website, and I like something, that's an implicit way to collect the data. But let's say that I did a search, and I clicked on a item, and I browsed it for ten seconds. They might gauge my interest maybe on how much I scroll through the page, the page I was still active, and say, well, maybe we should give a weight, a value to Brian's likelihood of enjoying this item. Now, on the other side, you have content-based approaches. And the content-based approaches is where the modeling and the learning comes in a place. For example, some items are very rich information about the items. So, for example, a book. There's the possibility that you buy a book because of the title and the cover. Usually saying do not judge the book by its cover. Titles are very misleading. You open, and you read the first chapter, and you're, like, this is not what I thought I was going to be reading. But they have a lot of content. So if we can mine that content or just part of that content, let's say the summary of the book. The table of contents, we probably have enough information to match it to things that you browse purchase before without you actually having to open the book and read a page or two. And that's the idea with some of the content-based recommendations. To really analyze what makes the item the item and match that to your interest and see if you can actually create a list that you can push to the user that way. Obviously somewhere in the middle, it's the sweet spot nowadays. It's where you have a hibernate environment where you can concentrate in mixing all of the data that you get from your users and the content of the actual items. There's also another way to classify recommendation engines, which is based on the algorithms that they use. Memory base are the ones that really happen with statistics and computations that work on pretty much the whole matrix of users and items. And you have model base, which is where the machine learning comes into place where data mining comes into place, where data enrichment comes into place. And, again, mixing those two, it's the hybrid approach that seems to work the best for websites and systems. So let's talk about machine learning. And machine learning, it's a very broad subject. Some parts of it are easy to digest. Some parts of it are really complex. So having a roadmap of basically how to go about learning many learning, can you say that three times fast? It's actually hard. So what I did first is basically find an environment that fostered the understanding of what I was doing. I realized that writing code, I wrote a bunch of code in Java, Ruby, I learned some Python to do some of this stuff, and I was overwhelmed. Completely overwhelmed. So first, I needed to find where to start. And I started with classification. Which is basically supervised learning in predicting what bucket certain item belongs into. Probably regression is fairly easy to understand, at least at the surface level. And then it gets really complex. Feature selection, to me, now that's the hardest topic in recommendation engines. And we'll get into that. And then you have things like anomaly detection. Finding what data point doesn't belong in the set. And grouping. How can you without knowing the makeup of the items have cluster themselves in a way that you can say, well, I know I have three piles in here. Now I can maybe investigate what makes those three piles different from each other. All right. So learning machine learning. My recommendations first are that you get the big picture first. Whatever you're doing, it's kind of like writing software application. You basically write stubs for everything, and you return simple things like strings. And make sure that you have a front-to-back and return trip and that everything works. And then you can concentrate on tweaking the algorithms, and tweaking the parameters of the algorithms, and maybe having multiple algorithms compete in both to see who gets the best result. All of that stuff, it's going to be necessary to actually fine-tune what you do with machine learning. But if you try to do it from the beginning, it is really frustrating. When you write code, write really small code examples around a specific algorithm with a known payload in a way for you to test the output. And focus on data first. This is one of the biggest mistakes that I made trying to basically get into this world. If you don't understand the data, if you don't manage the data, if you don't know where to get the data and clean the data, you're going to fail miserably. It doesn't matter how amazing your system is, knowing the right data and getting it into the right form, it's the first step to success. So in terms of frameworks as a programmer obviously, again, it sounds like we need recommendation engine to recommend machine learning frameworks. This is only a small sample of what's out there. There's hundreds of machine learning Pratt forms, frameworks, stand alone algorithms that you can actually use. But like I said, first focus on learning and understanding the whole process. One of the ones that's becoming the hot commodity is tensor flow. And also if you're learning and Pythonista, I would say start with kit and move. Down here I put the things that are Java related. Obviously Scala, Ruby, Python 2, there's a few things there that can help you get started. So my approach was to pick something that was easy for me to grasp the whole process. And for that, I picked this product called rapid miner. Rapidminer is going to look a lot like an IED that a lot of you ran away from in the past. This Java developers probably remember eclipse. So when I first opened rapidminer, I first had acid reflux, but it is an amazing product, and it taught me a lot about machine learning than writing code. It seems counterintuitive, but it's the way that it worked again. So, again, it's an eclipse-based application that has an amazing set of operators to basically do everything from classification even right before this presentation, I found a recommendation extinction where I could use to test some of the things I'm building. If you can live with the virus, then this is coming along for the ride. Just kidding the Java VM, it's an amazing platform, and I stand behind it 100%. The language? Ehh. Okay. So let's talk about one of the first types of collaborative engines, which are the ones that rely on collaborative filtering. Sometimes known as the click base recommendation engines. And the idea is to collect large amounts of user feedback both explicitly and implicitly and infer preferences for specific users that way. So you can recommend items. So feedback, this is the topic that is actually dicey. You can get into a lot of privacy issues, creepiness, I, for example, a system recommends an item to me, I want to know why. Some people are okay with the mystery. It's, like, how did they know that I like panda bear outfits and swords? But -- that's a really strange combination there, but that happens in my brain. But I like to be presented an explanation of why the item was recommended to me. So explicitly you can like something, you can rate it, you can write a review. Sentiment analysis, it's also another hot topic nowadays. So by reading -- and somebody was talking about NLP. Natural language processing. Now when you have sentiment analysis, natural language processing comes into play to figure out if it's a good review, bad review, and how do you tell sarcasm? How do you tell basically colloquialisms to the area where the user that it's from wrote the review? So those things are hard problems. Sometimes I can't even tell other human beings are being sarcastic. And we have this supercomputer in our skull, imagine trying to do this with a collection of tags and things like that. Or just words and tokens. So implicitly also explicitly users tend to present feedback that represents the ideal way that they see themselves. While implicitly users just do. And when you get implicit feedback, you're learning the true nature of that. It could be the highest level of purchasing something. I really like it, I really need it. Maybe I really need it, I don't like it, that's a whole different topic. Searching for something, you can based on a query figure out the likelihood that some items that match that query are to be liked by the user. Browsing, how long do you spend on a page and scroll through the page and read the text on the page. All of us have been trained by inures agreements to scroll really fast and click okay. So can you detect that? Do you know if the person actually read it? Those are the things, the problems that at the UI front we're actually facing. And then this of course taking into account positive and negative feedback. So here's an example of reviews on Amazon that I forgot that I've done. In the U.S., we have Halloween, which is a light version. In Spain, I don't know what you have, but I know you guys like to throw tomatoes at high speed at each other. That sounds a little bit more painful than Halloween. But here's a couple of reviews. This is an example, it detects whether it would be good or bad by a machine. But Amazon is smart enough to basically bundle that with a start generating and also a direct sentiment expression. So here you can see that when I bought my Halloween Fidel Castro outfit, it did not go well. The neighborhood in Arizona did not like that I was wearing this. But that's a whole different story over beers tonight. So you can see I have a title, a generating, a sentiment. Here's a review, a friend wrote a book, and I said I love the book. I read the first chapter. So that's another problem. We lie. We lie a lot. We lie to ourselves. We lie to computers. We lie around the world. If you could really have a counter, like, maybe a bell that goes ding every time you say a lie or mild lie, a day you'll be impressed. And as somebody that has a couple of kids, they lie to me all the time. And I said, well, you know, I kind of get on my high horse. And it's, like -- I just lie to you about something else, like, five minutes ago. So it's a human nature to paint a picture of ourselves that it's the ideal picture and to basically provide information that it's not very accurate for machines to deal with. So to deal with this type of filtering, you start with a matrix. And this matrix, it's called a utility matrix. And it's a matrix of your users, and your items. And on the intersection of those, you'll basically will have either the star generating or the comments, or whatever the user has provided for that specific item. So you basically want to use the ratings of other users to find similar users. And the users or items. So there's two approaches. And they scale differently based on your environment. If you have a lot of users or items or very few items and a lot of users, that determines which approach you will use. But, again, you want to figure out items that are very similar to the items that you like before and recommend those items. Or items that other users like you have used before. So you have user item ratings. Let's say these are star ratings and the question marks are the users that did not provide a generating for a specific item. Now, this matrix I'm putting here as an example is very dense with populated ratings. What happens is you have a lot of question marks and a few numbers. So it is a problematic environment to basically find this ratings. So of those two styles, let's go with the first one. User base recommender. Find users similar to you, find what they like and recommend this things or things that are like those things too. So, again, it relies on finding similar users. The similarity, it's between the users, it's the items in common. So, for example, if you liked a specific book. Let's say a Ruby on Rails book. You provided a three star generating. I provided a four star generating. That is a similarity between us. Now, the more things that are graded in common, the more similar we are. So if you can find, basically do the pairwise comparison of all of your users. So, again, if you have a lot of users, this becomes computationally expensive, you can now have a similarity between measures in users. And now you can look at the items that, for example, you liked or purchased, but I never rated or seen before. And you can use those to and recommendations for me. So, again, it relies on the items that we have in common to find similarities between the users. So the first example, we're going to take that utility matrix, we're going to turn it on its head so we can have a list of user IDs, products, and ratings. So we pivot that table. And we have user ratings for the different users. For the different items given by different users. And I get question marks for the missing items. And the goal is to predict those question marks. In some systems will basically do this in batch mode overnight. Let's find for probably you're going to classify your users by the high end purchasers, the ones that browse but never buy. The ones in between that buy something from time to time, and then start with the high value customers and figure out customer similarities between those and other customers. So you can start your recommendation list. So in this utility matrix, you can see that we actually use the rows to find similar users. So our row-wise comparison is what we use to find the similar users. Now, once we find the users, we need to basically find the users. And to find the users, we need to have a measure of similarity. So how do we calculate similarity between two users? And in this case -- or items. It could be users or items. We need a mathematical way to do that. One of the simplest grims, it's called the K nearest neighbors or KNN. And KNN is kind of a centerroid approach to figure out what are similar to this user. The KNN factor, that's where the N part comes from. The nearest neighbor. So we're going to use a factor of three nearest neighbors. So here you can see as I draw a circle around those three nearest neighbors, I'm going to basically find these three users, and then ache take the average of what they like and use that to compare them to me or to make recommendations. Now, you can see that the classification or the similarity will change based on what you picked. So this is one of the places where as I was building systems with some of these technologies, I had to grap that circle and go wide, narrow, sometimes there might be clusters. Okay? And a small number would give you a pretty good answer. But as you open that circle, now you're grabbing two users and the comparisons become harder. So there's a lot of aspects to do this. You can do actually a weighted average of everything. You can have the users then do the nearest neighbors recursively to a certainty depth to basically average the clusters. So it can get pretty hairy pretty fast. And that's why I like a platform like rapidminer to explore this. So let's look at this in rapidminer, and what I did was load data from the database in data that I showed you before. I find that the uses the nearest neighbors. And I predict the ratings of the -- to predict the ratings that the user in question would give the items that he has never rated before. So here's a prediction part of the equation. In RapidMiner, it's a graphic environment, so you start again with a big picture, and you want to figure out how to have the pipeline of machine learning processes chain together correctly. And then you can go into the detail of each node and figure out the details of how to make it better. It takes hours. I thought it was going to be easy, but you have to go to each node and change the values and really keep annotations of what you did. So the whole scientific method comes back. You have to -- I have notebooks now where I basically write every observation and then figure out what my control group is and all of that stuff. I feel like I'm doing clinical trials for something. So data science, it's not as sexy as they paint it to be. It's a lot of work. So as you can see, I'm reading data from the database. One way that you use in machine learning to basically test whether something is working correctly, is to get a big chunk of training data where, for example, you have things already figured out and to split those examples into training data and test data. So you train your algorithms with some training data. In this case I'm going to use an algorithm that it's based around the KNN algorithm and compares users in that utility matrix. As long as you provide the matrix and the right shape, it will do the right thing. And then I'm going to evaluate the predictions. I'm also going to test the performance or the recall quality and the accuracy of my classification system. And I'm also using the test data to predict ratings once I know that the system has been trained. So let me quickly go to RapidMiner, and in RapidMiner I have a repository, and in here, I'm going to open my user base recommender. And this is what you saw on the slide. The thing that gets interesting, for example, if I click on the user KNN, you can see that there are some parameters in here. So for my -- that it's not what I was expecting. Thank you. I was just about to freak out. Thank you, sir. When the demo gods don't smile on you, and you're, like, oh, no. Okay. Good. I actually have higher resolution. All right. So in here, if I click on that user KNN node, you can see that I have a K value of five, and I try to basically play with all those values a few times to figure out how to get the best results. Also notice that there's all kinds of choices of how to calculate the correlation. So when you have these two vectors in place, you can use Pearson correlation or cosine correlation to figure out how similar two things are. Obviously the cosine similarity one of the most used ones and relies on basically calculating the cosine between two multidimensional vectors. And we'll talk about that in a few minutes. Another thing to look at is this performance node. And in the performance node, you can see that I have some performance matrix. So, for example, here's the generating range that I'm going to use. And I also have a way to apply the model so I can evaluate the predictions. And also apply those same -- that same model to the test data. Notice in here that the model that was created basically its past from the user KNN to the predictions and every node in RapidMiner has another output to chain that model somewhere else to use it. You can also present the models to disk. So now you have already trained model that you can retrieve and actually played with. And this was invaluable to be able to understand some of the systems. So let me go ahead and run this so you can see what's going on. And I have a small set of data. Obviously I couldn't pick large sets of data for the demos because otherwise we would be here all night. As you can see in here, we have our example set loaded, which has the user IDs, the product IDs, and the ratings. We also have the test data. And we have the performance of our system. So that's the root mean square error, which is a measure basically how close you are to basically success. So with RapidMiner you can do some of this stuff in a very quick passion to -- when you drop the components, some of the default values are the default values that the overall data science community would pick. So you get a baseline for behavior, and then you can start tweaking things around. All right. So the -- one of the issues with the -- so notice that there was no empty values there anymore. All the question marks were replaced with ratings. But those ratings in the test data were predictions. But now we can use those ratings to find the highest rated items and recommend those to you. So the problem with this is that it performs poorly if you don't have enough ratings obviously. And like I mentioned before, that matrix is pretty sparse. Customers don't like to be told to do something. So you will have had very few ratings or sometimes they go to the extremes. Either I hate it, or I bought it already, so I might as well give it a five. But sometimes people don't take the time to think in the middle range. So that's one of the problems with user-generated ratings. System analysis is better because the only problem is that typically super happy or super angry customers leave reviews that are text based. And the other problem is that these are computationally expensive to calculate the pairwise comparison between all users. And if the user changes, you have to recalculate. So, for example, in my Amazon recommendations, there's still Java books that are out of print. I'm getting a Bruce echoes book, which I remember I used to carry around in the '90s. I probably use it as a doorstop now or to beat attackers with it. But it's still coming up in recommendations. It's, like, you might want to buy this. Not really. So that's one. The other type of collaborative filtering, it's the item base recommender. And the item base recommenders are actually very similar. The big difference is that we're actually now finding similarities between the items rather than the users. And that matrix when we do things, we do them now column-wise. So we use the columns as a template to find all the columns that are similar. And now we can know the similarity between items. So they're very similar approaches in RapidMiner, it's pretty much the same. You just have a component that gives you this implementation. Now, building a recommender component like that item KNN, or that user, it's pretty extensive internally. So this is what I use at the high level to basically get an idea of what other things I need to do correctly and how to clean my data. How to get the data from the right places. And now I can maybe replace one of these components with my own implementation. So we are at 41, so let me move faster at what I have through here. We don't have much left, but I'm already at the borderline. So advantage of the user base filtering is that items do not tend to change. Users change. A book that is written is a book and maybe opinions about the book will change in time. Some books that we thought were great are not great anymore. Remember the J2E patterns book? Yeah, I probably burned some millions of dollars of some employers with that book. But let's talk about that over beers. Smaller dataset. So dealing with a smaller dataset, it's actually easier and faster to compute, and you don't have to compute as often because items do not tend to change. So the content-based recommendations are the ones that really excite me. And these are the ones that you do the machine learning to basically go turn the problem into a machine learning problem and classify tells me according to their content. You also classify users according to their content. How do you get the content for the users? You have to profile. There's a lot of different ways to do it. Demographics, for example, you can do H, geographical information, things of that sort. And you use collaborative filtering to find out what the tastes of the other users are and try to test those in the profile of the unknown user. So there's a lot of different ways to actually mix the two. A good sample of a content-based recommender simple is last FM. It really analyzes the music deeply. They figure out basically beats per second, the level of the volume of drums versus the guitars and things like that, and they use that information, very fine grain to create multidimensional vectors to deal with this recommendation process. Pandora is another big one in there. IMDB. But, again, the idea @Santa Ana create a predictive model out of the information that you have, fill in the blanks that I don't have maybe other users information with collaborative filtering. And the profile of the user becomes a classifier. So I'm going to skip forward a little bit. We're going to skip the vector space model. I'm going to give you the last slide in here. So I was going to basically really go into vector space model so that you get a little bit of the math behind this stuff. But, again, think of vectors in multidimensional space, and you want to figure out how similar they are. So two that are here similar, these two are dissimilar, these two I don't know, and I am doing the YMCA dance now. So understand thy data. That is the first thing about building recommenders. Data cleansing. It's an arduous job. Sometimes I spend more time in cleaning the data and enrich the data than using the data. Peter, Norvig which is the head of AI at Google right now will tell you that simple models will trump complex models if you have more data. So whoever has the most data wins. That's why Google is constitutional winning. Again, at the bottom of all of this, there's classification algorithms. And I'm going to also now move again. In the machine learning world, there's a lot of different algorithms. So, for example, when we do deep learning, we're talking about neural networks. Shallow learning used to be two or three. You guys remember percent forms in school? Two or three layer neural networks. Deep learning now has 2- 3,000 neurons in between. One of the hottest topics right now is random forest. So, for example, you create a forest of the decision trees and then you randomly -- you create those randomly and then have them vote to see what class has specific piece of text or description fits into. Obviously my timing was not ideal. But  I actually decided to use world knowledge to enhance my classification. And what I did, is use Wikipedia to basically create a corpus of knowledge about a concept and then be able to grab a specific article and classify with that world knowledge. So the idea is by having something like Wikipedia. By compote unquote experts, although I'm no expert. The idea is that now you have really good training data. And having that fine grain semantic reputation allows to you build better classifiers. Also Wikipedia provides a taxonomy that's built in. When you look at a Wikipedia page, you have the parent topic, the children topics, and you can really use that information to create features that are more relevant. One of the classification examples that I did is I had thousands of features and most were Earth. You process text, you use stop words, dictionary to clean it up, but you still end up with a lot of words that don't add to the value and add to computational burden of the algorithms. So one of the things that I did is basically create a crawler to go to Wikipedia for specific topics, get that information, and process those documents to create vectors in this hyper space. Now, once I got the text out of those Wikipedia articles, I did 150 examples just for this and 18 test pages. So it's a small dataset. When I did it with more data, it worked better. I trained the simplest classifier, which is a KNN classifier, and I classify Web pages. The process, which is fairly simple in RapidMiner, and it really gave me an insight into how things work and should work. I stored the model so that I could reuse it later. And then what I did was store multiple versions of the model and then I did a -- gigantic loop to basically run the different models that were fine-tuned differently against the same data problemmatically. Now, the did a the that I used in the reputation came with amazing recall. 100% accuracy for this one class. So if an article was a Java-related article. In 99.4% if it was machine learning. So once I did this, I thought, man, you are a data scientist. You are good at this. You drag and drop things, you connect them, and it worked amazingly. Wow I'm a genius. I'm going to need to send my résumé to Google. Well, well, well. I ran this whole damn thing on a small sample of set data. And notice my Java articles were classified correctly, so this is the category that they actually are, and this is the prediction. And then they start failing around hear. In all of my machine learning topics fail. The problem is that when you have a taxonomy, building the taxonomy is a lot of creation. How far apart are those concepts? There's a lot of machine learning libraries in Java. And some of those articles in my training dataset made those two groups overlap. And I found articles that had more text in the machine learning than in the Java world, which makes some of the terms in the Java world more important. So it screwed up my whole dataset. And this is where I said data understand the data, understand your taxonomies, and that's how you will basically get by in machine learning. Otherwise you're going to pull at your hair and scream at your computer. I did not do that. That's a Web search. I typed bullet-riddled laptop. So, again, overfitting classifiers need to be tweaked. I tried a bunch of them this small dataset with the data I provided, they're still not great. So there's some data engineering that needs to happen ahead of time. So, again, why would you build one of these things? It's not really a product, it's a feature. So your application really don't build recommend engine after I told you to build one. I want to build one as a product. But if you're a Web developer, you had E-commerce, let's say Shopify might be able to build their own and integrate with the product. But if you are putting all of your own E-commerce store, don't spend three years building the recommendation system and six months building the store. Okay? Find somebody to actually do this for you or help you. So, again, more data beats clever algorithms. But better data beats more data. And that's what I run into. I need better data. And I need a better classification of my environment. So once I went through all of this, I came up with a architecture that I think makes sense for what I want to build. And I built some pieces of this. So I started basically building this content analyzer first because this was the fun stuff to do. But then I realized that remember that circle in the middle of the machine learning squares that said feature if engineering or featured abstraction or selection? That is the hardest part. So what I'm focused on right now is building this taxonomy API that allow you to build that hierarchy or that list and also assist you in building that list. And I think this is where I lost the battle in this example. But this is what would make a product like this to people. This stuff, it's easy. Most of it is can. You can -- there's three that I found for recommendation algorithms. Use the simplest algorithms. And they take -- most of them just take data from the system as user actions. User browse this, user rated this, and and don't provide any recommendations. So my idea is to mix the two but also provide the hardest part or assistance to build the hardest part, which is the feature engineering. All right. So go and learn machine learning. This is me wearing the infamous Fidel Castro outfit. My wife begged me not to put this picture on the slides. But, hey, in case something went wrong, I needed humor to win your hearts, or I'll buy you a beer later have the pragmatic programmers, they agreed to let me use some of the data for their example but also provided a coupon for books. So if you use that, just don't tweet the coupon first because otherwise people will use it. But use it. It's until September 30th. Thank you. [Applause] 