 hello everyone my name is Fred Ryce I'm the chief architect at the IBM sparks Technology Center down the road a bit at 425 Market Street here in San Francisco and I'm here today to talk to you about building custom machine learning algorithms with Apache system ml now I'm going to start my talk by answering what is probably the first question in many of your minds then looking at this title which is what is system ml after that I'll segue into a demo and then I'll talk about how you can repeat much of what I did in the demo yourself by downloading and installing system I'm allen trying it out so let's get into that first part what is Apache system ml so I've put up on the screen here a time line and you can see we're in the middle of 2016 here to tell the story of Apache system ml we need to go back in time a bit and in fact quite a bit far back in time to around 2007 2008 when at the IBM Almaden Research Center where I used to work primary prior to working with the spark Technology Center there were a number of different research groups all working on scalable machine learning problems on top of Hadoop well we called it and Duke back then now we call it MapReduce and we noticed that we had all these different instances of similar problems but we didn't have a machine learning group so around 2009 our lab formed a dedicated group for dealing with problems and scalable machine learning and once we had that group in place we started attracting IBM clients who had interesting big data problems that they needed some help with and so we went through about a dozen different engagements with various IBM clients and through the process of working on those engagements we got to learn about how real world data scientists out in the fortune 500 build custom machine learning solutions to their business problems let me give you one quick example a one customer that we approached us was a large automobile manufacturer they had a lot of proprietary data from various data sources they wanted to use that data to predict when they would really require a car because it was a couple standard deviations beyond the mean in terms of reliant in the auto industry it's very important to have a good relationship with your customer so if their car is bad you want to buy it back so you maintain that relationship now when we started the engagement this customer had a kind of standard machine learning pipeline going they started with their data did some feature extraction to get some features and labels they were feeding that into a machine learning frob algorithm and out of that algorithm they would get predictions and the problem was the predictions were not very useful because there were lots and lots of false positives and you really don't want to buy back a lot of cars that you don't have to buy back so we went in and we looked and first we focused on the features we saw that there were a lot more additional features that we could get out of this base data so we expanded the feature set but then with this new expanded feature set we saw that the data was no longer well well conditioned so he had to add additional data items and now with this much expanded data input we turned back to the original algorithm and we saw it's not quite fitting the kind of data we have anymore so we tried out about half a dozen other algorithms until we found one that really works well and at the end of this long process of iterating over the solution we ended up with a 25 times better precision in doing those predictions now we had a lot of other engagements that had the same flavor of repeatedly going towards a solution to a business problem now if we were to step back though and look at this from a high level over and over again we saw the same kind of iterative machine learning development processes going on it's basically a two-step process you start by building a pipeline an end-to-end pipeline that will cover your use case going from data to the results for the business and then you ask are those results that come out of the pipeline is that prediction good enough to base your business on it if the answer is yes then you're done you can move on to the next problem but more often than not the initial answer is no so that starts the process of iterating you find what part of the pipeline is not working is responsible for those in and not good enough results and fix it and then ask the question again and go through this maybe tens or dozens of times eventually you find a solution that works now to manage this the other thing that we found across these engagements is that for problems where the data and the computation fit well onto one machine like a laptop there is actually a very effective set of tools and best practices that work really well basically the way that people do this out in industry is a data scientist will write a high-level program that implements their their algorithm their pipeline in a high-level language like R or Python that's very easy very concise to write will run that program directly over the data which fits on the laptop and get back an answer very quickly so that the data scientist can ask is this answer good enough and if necessary repeat the process until the answer is good enough this works really well because we have this high-level language that makes it very easy to express algorithms very easy to run them but as soon as the problem doesn't fit on one machine we have to fall back on a different set of tools and the different set of practices that it turns out are much less efficient in this case when you have a big data problem you may start out with the data scientist writing a high-level program but that program needs to get translated into something that will run in parallel and for that task you bring in a systems programmer who can translate that program in our Python down to something like Scala so it can run on an environment like SPARC then you get some answers back the results come back to your data scientist and you ask that question again is this result good enough for my business problem if the answer is no then you have to go all the way through this iteration process now the trouble with this approach is that it's it gets the job done but it's very expensive because of two factors you're writing the program twice and in between those two instances of the program that you read and you have human communication so that process of going from the initial program to your results that used to take minutes or maybe hours is now taking days or weeks not only that but again because you're writing the program twice you have two different diplom ins and if you get results back that aren't good enough you don't know whether it's because the original program was not good enough or whether the translation to a parallel version was not you did not produce the same results so not only does each iteration take longer you're going to have more iterations because you have more complexity and more errors so we looked at this our research group looked at this situation and we came up with a vision for conducting some long-term high-value research and the vision looks like this we're going to make that big data use case look like the best practice for small data for problems that fit on the laptop by building a system which we'll call system ml and what system ml will do is it will take a program in that high-level language a subset of a language like R or Python and it will automatically translate it into an efficient parallel program that runs on an environment like spark or hadoop mapreduce so that we can have the data scientists back into the business of running the entire iterative development process and get results much quicker we can get the advantages of running on one machine but the be able to do tackle big data problems we can iterate quickly we can always guarantee that the result we get back the same result you get if you were able to fit that problem onto your laptop so that's the vision now remember when we had this vision we were way back in 2010 so since then in the intervening years at the almond in lab we had a lot of very innovative very interesting research going on and at the end of that process we had built up a system which was still called system ml and one year ago at SPARC summit in this very city our our director general manager rather Beth Smith announced that IBM was going to give away this technology to the open source community by open sourcing the system ml system and since then things have moved very quickly we made the code available on github we started an Apache incubating project we did our first Inc patchy release earlier this year we have a second binary release that is in flight as we speak should be out very soon and today you can go and you can download Apache system ml and you can try it out for yourself and I'll tell you at the end of the talk exactly how you can go about doing that some people have in the past couple months actually done that process of downloading it and trying it out here are some examples IBM Watson health got a copy of system couple months ago and started using it to build new algorithms for predicting patient outcomes and they found that the new generation of algorithms by leveraging the high-level language and the parallelism they had in their underlying Hadoop cluster could I get a substantial improvement in accuracy not only that but they were able to use system ml to do a migration from their old Hadoop based infrastructure to a new spark based infrastructure system ml can target both spark and Tudou so what the engineers at IBM Watson health did was they wrote their script once they were running it on their Hadoop cluster and then just by pointing system ml at a different cluster with the exact same code by running on the job against spark they were able to get their iterative algorithm running three hundred times faster but no code changes really impressive benefits that they're seeing there another example is a company called cadent technology which is in the business of helping large cable operators and operators of streaming video to do ad placement and to help market to customers and there's a lot of D very deep interesting machine learning going on there their chief scientists Michael's argument the system ml research group a couple actually the system ml group at the IBM spark technology center a couple months back and since then we've been having helping them to deploy system ml inside their infrastructure to use it on a proof-of-concept engagement hello he's right there in the front of the room by the way and he just well you can talk about it yourself offline but the quote that we got from Michael is that system ml allows cadent to implement new advanced numerical programming methods in Apache spark and empowers the group to leverage specialized algorithms in their own predictive analysis software so they're having some very good results doing the custom development they were doing on Big B fees it's single servers running are but on SPARC using system ml but really instead of these stories it's really best to see the system for yourself so I've dedicated most of my time slot here to a demo so some quick background before we get started in this demo scenario we're going to imagine I work for a company that does ad placement on websites and I in particular my company is targeting ads based on demographic information which is stored in the individual users cookies on their web browsers now because people don't like giving away that kind of information the information I have is complete so what I want to do is I want to interpolate it I want to complete that set of information so I have an estimate of the full demographics about each individual person so that can trigger my business rules and put up my ads the data that I'm going to use for this demo is some publicly available data from the US Census Bureau this is called the public use microdata sample from the 2010 census this the datasets is a 10% sample of the US population it's been anonymized but it gives the full set of survey responses on the short survey for all of those people now because time is very short here I'll be using the California data which is 10% of the population of California so it's still quite big and we're going to use this data set which is of course the full demographic information to generate some synthetic data that would look like what we had if we unhealthy partial information and then we're going to fill in those holes again using some machine learning some custom algorithms so I'm going to switch now to my web browser window now if you were to deploy system ml to to run some machine learning algorithm to build an algorithm the first thing you need to do is get a copy of system ml to do that you go to system ml Apache org that's system ml dot Apache org and you click on the big blue button here and that takes you to a download page where you get a tarball and if you unpacked that turbo and look inside hopefully you can read the text here there's a jar file in there called system ml jar you can use that jar file as the argument for spark submit if you're running from the command line or you can attach that jar file to your zeppelin interpreter and you can use our api which we call ml context that functions a lot like a sequel context you write down a script in terms of our language which looks like a subset of our or alternate language which is a subset of python and you can register some inputs come in this spark data frames and you can execute a script that you pass in as a string like if you were executing a sequel statement and this is a very useful way to do integration not only for notebooks like Zeppelin and Jupiter but also to run applications but today I've actually got something a bit cooler because this Zeppelin server that I've got running on the IBM cloud is actually an experimental custom build of Zeppelin that one of the committers on system ml has been experimenting with that has native system ml integration so if we switch over here we will see what I mean so remember I said my date our data set is the public use microdata sample from the US Census Bureau and here we've got some the data wrangling part of my demo which I have I will make my text bigger yes absolutely and what I'm going to do it's like that and then like that and that and that there we are there okay we'll be zoomed in for the rest of this presentation so hopefully we can read things on the screen so we're looking first at the data wrangling part of the demo we're going to use spark to get that input data I massage it into a format that's useful for machine learning the data comes in in two files one file with records about persons one file with records about their households we need to join those two things together with which with spark data frames that's a very simple thing to do we just call the join operator and we filter out some additional columns that are just metadata about the statistical properties of the other columns and then we end up with this data frame containing records about each individual person's personal demographic information as well as the information about their household since this is Zeppelin we can do cool things like showing histograms this is the number of persons in household field the number of persons over 60 we'll come back to these histograms in just a sec the other thing that we can do because we're running on a special build of Zeppelin here this experimental builders we can put this data frame into the special variable Z which Zeppelin uses for exchanging across different programming languages and we can switch over into system MLS domain-specific language or our like language called DML that stands for domains sorry declarative machine learning and we can read back that data frame as a matrix so when we run execute this command it'll go dig into SPARC do whatever computations are necessary to compute the tuples in that data frame converted into matrix turns wrench laid out that into a binary blocks distributed across the cluster takes about five minutes so I've run that ahead of time but you can see the plan and again this needs to be zoomed in a bit sorry that was executed back when it was running we have at the left here of the screen you can see we have some sparks equal jobs running to scan the two files to join them together and then right in the middle of the stage of the spark job the second third stage of the spark job we are getting translating into spark code that was generated by the system in all optimizer to take this data frame and translate it in parallel into a matrix so we we go through the lines of the data frame we generate blocks we distribute those blocks across the cluster and now we have a binary matrix distributed across our cluster ready to do parallel computation on it so coming back to our web browser so since I ran this ahead of time I just saved the data in binary format because once you've done that translation it's much quicker to cast it on disk and load it back again so I'm gonna go into a second browser window where I'm going to start up a fresh session using our internal language DML our subset of our to do some actual machine learning on this data so I start out started out by reading in the data you can see that took just a few seconds as opposed to the five minutes it took to generated in the first place because it's already nicely spread across the cluster now what I'd like to do is let's do some let's finish up the data wrangling so this data as it came in was coded such that the value 0 actually indicates one of the values in each of the census data every field is the answer on one of the form questions so we need to add one to everything because I want to use the value 0 to indicate no no Democrat is present but because this is all as a matrix that's really really simple to do I just take the I define a new matrix D which is R take our D or raw data and I just add one to every cell and that's all and just to prove that I have this matrix in memory and I've got all the data there let's up do some basic descriptive summary statistics so some column means let's say and then to string on that matrix after the computation we run that and and we're done so we've taken that entire matrix and we've done we've added one to every cell and we've done this aggregate and as you see it returns almost instantly so now we've got our raw data which is real data but we need to make some synthetic data so that we can run our demo scenario so what we need to do is we need we have a full set of demographic information we need to remove some of it so we can generate that back with machine learning so when you're dealing with matrices and a high-level language that talks in terms of linear algebra these kinds of operations manipulations can be really really easy and let me show you what I mean if I want to strip out some of this data at random I'll just create an indicator matrix that says what cells am I going to retain and I can generate that randomly I'll just do a uniformly around him to save time here and number of rows is the same as our data number of columns is the same as the columns in our data all the values that are nonzero are 1 and the number of non-zeros is about 30 percent of the data and then once I have that indicator matrix using it to sample from the original data set is just a matter of doing a multiplication a cell wise multiplication so I'll make a training set by multiplying and then what did I do I must have mistyped something gotta love demos thank you I think it's actually in row and and I've end calls instead of n call there we go it's good to have X programmers in the audience thank you very much so we run that it's got to generate a pretty big matrix at random so it takes a few seconds and then once that has completed we've now generated our training set now if we're going to train how the training set we should also have a test set so let's take all the values that we just stripped out and put them in another place and that's really easy to do we just generate another indicator matrix we'll call it t I and we just apply we can apply a boolean predicate to our first indicators to do that so I equal to zero everything that zero turns into one everything this one turns into zero we do another multiplication and then we have a test set D times D I ran that okay now we've got as soon as this completes a training set and a test set so let's use this training set to do some to fill in that missing data a technique that we're going to use to fill in that missing data as matrix completion we have this data as a matrix it's got zeros in it we want to replace those zeros with values that are consistent with the non-zeros and we'll use matrix factorization to do that I'll explain the details of that in just a moment but one thing to keep in mind is all these algorithms for completing matrices tend to bake in assumptions about the distribution of the data that you're generating and in this particular case if we look back at the at the data wrangling notebook we were at a while back you can see our data our data distributions look like this they're all discrete they're all greater than 0 they have a it and if the mode is close to 0 as in here then you have modes the most common value if it's close to 0 that the distribution is very asymmetrical if it's further to 0 it gets more symmetrical this should look familiar to you this data is roughly distributed as oops as plus on you can see that right here so the columns are roughly + on so we want to have an algorithm that will fill in that matrix with new values - drawn from + on distributions and as it happens there is such an algorithm that's hmm somewhat obscure but fairly well known and if you go to the assistant memo home page source code for that algorithm you know highlighter language is right there so we can copy and paste that actually I've got a version here with a little bit of boilerplate before and after for input and output I'll stick that into the notebook and we can start that running did that run now it's running okay so I'll go back to so while that's running let me just explain what is going on here in the background so we are factorized we're filling in blanks in the matrix through factorization and the way that you work is we this works as we have this matrix every row represents a user every column represents a particular field in the demographic survey a particular cell if it's present if it's nonzero indicates the answer that user gave on the survey if we take the data that we have in our training set we end up with a nice sparse matrix with some nonzero values and some zero values if we're going to approximately fill in the remaining values to complete this matrix using factorization what that means is we will build two smaller matrices called factors so that if you multiply these two matrices together you end up with a new matrix that has some new nonzero values in addition to something that closely approximates the original johnsy rows and these new nonzero values become the new interpolated demographic information so that's the algorithm that's running in the background a variant of this factorization algorithms family called twice on non-negative matrix factorization that generates nonzero values that are distributed roughly plus on and if we are lucky it's going to complete in about a minute this takes around 2 minutes to run and while it's completing to run let me point out some details of this that will be relevant in just a moment on each iteration of this main loop I'm computing the loss function again so that we can graph that later I'm putting that into a variable called losses which is going to be a vector of loss values and at the after running that script I'm going to write that into that magic Zeppelin variable Z which contains the which gives used for passing data from one language to another and we we're done we've finished our 2 minutes so we now have our two factors W and H computed we've got our losses inside this variable losses which we've put into that into the Z map so we can use it in other languages let me just show you one out of there aspect of discipline integration which is we can take these matrix data from system ml and we can reintegrate so if I open up a sparked paragraph in this notebook I can say Val laufes which is a data frame equals Z that get losses the key that we stored it under in the system ml in the previous paragraph and then I can use Zeppelin magic to show this the contents of this very small data frame if we run that then going fullscreen again we get a call we get some various visualizations the most interesting one here is we're showing the loss the loss function that this algorithm is optimizing is just dropping and then converging as we go through different iterations so the algorithm has converged that's great but of course anytime that you're on a machine learning algorithm the metric the loss function that it's optimizing for it isn't always perfectly aligned with your business needs so now let's try and compute a different loss metric to see get a second opinion on how well we're doing so far I will just compute a mean squared error broken down by column again because we're doing in our Kel coding in terms of system ml high level our lek language with matrices and linear algebra this is very very concise so varta DML to sell tell Zeppelin we're using zip system ml and we'll just write down this expression which I have to type here equals all means we're going to take a mean across every column of this expression our two factors multiplied together using matrix multiplication then multiplied cell wise by that remember we've generated a matrix that tell us what values were retained so we're going to filter it down to the values that are in our test set and we're going to subtract that out just subtract all those values from our test set which is another matrix so it's just a matrix subtraction and we'll square everything in that matrix and then we've got mean squared error broken down by column and we can just print that and run that takes a few seconds because again we have to multiply two matrices that produce a fairly large output and we're done and you can see overall we're doing pretty well in terms of squared errors many of the columns have very low error some of them are not we're not doing so well so turns of prediction because what the algorithm is optimizing for is not the metric that we're looking at here so if this was a real data science scenario and I was a real data scientist what I would do is I would go back to this pipeline that I've built and I go and tweak the algorithm tweak whatever step is responsible for those columns not working right introduced some new details to the algorithm to fix that particular problem so hopefully this gives you a flavor of what it's like to do data science and system ml this new Zeppelin integration you will be able to find hopefully in a couple months if you want to get a preview of it just go to the Apache Zeppelin JIRA site and search for system ml let's get back to the main thread of the talk and I just want to summarize some whoops i think i just added a slide to my deck that's not good i'm going to press the right button this time wow the right button is not working okay summarize some key points from the demo so first of all we showed you that system Amell spark and Zeppelin can work together they can work together with our stable API is very well there's the ml context api's and they working together even better with this new prototype integration that I'm showing off here I also showed that the linear algebra based languages are really really good for data science they get you a very concise easy-to-understand representation of a lot of critical operations so it's really important to a lot of data scientists to be able to work in that domain even if they're working on top of a parallel environment like spark the third thing I showed you is that customization being able to copy and paste a textbook algorithm that meets the particular distribution requirements you need being able to go back and fix your algorithm when you find it's not quite working right it's very very important in these kinds of end-to-end machine learning use cases so these are the slides I just added by accident so I've shown you how I've shown you myself using system ml now I'd like to talk about how you can get all this software yourself so it's all available for free and you can download it off the web and run it yourself and the way place to go to start for any search to get system ml for yourself is the system in my website which is system ml Apache org I'll say that again one more time at the end of the talk but it's system imelda Apache org if you go there you'll see a picture that looks like this there's a big blue button at the bottom that you can click to download our latest binary release current release is 0.9 0.9 will come out very soon if you look at the upper right of the page there's a place where you can look at a lot of our documentation we have tutorial we have a reference manuals we have installs installation instructions all the material you need to get started if you still have trouble with getting started after you've read all this documentation you can either browse through the source code directly or you can get in contact with the members of the project or even contribute to the project you can go to our mailing list you can go to our JIRA server this is an Apache project so we follow all the Apache governance models we're very open to contributions from outsiders for example there's a group at tu dresden working on a flink backend for there's other contributors at other companies and we're very interested in having people use the software having people contribute the software so I encourage you to go again to system amel Apache org and try this stuff out so thank you very much for coming to my talk and for watching my demo I encourage you to download system ml I'd like to thank the people who helped out in producing this demo I'm Mike Duesenberg and Nicole Jindal Mike is actually here sitting in the third row so again thank you very much all right I think we have time for about one or two questions over there hi thanks for the nice talk so I wonder whether there is any has to get head to head performance comparison i between a and algorithm implemented in system and male versus say and then ml late the question is how we had a comparison between an algorithm and system of Allen and algorithm and in ml lib and the answer is yes we have such a comparison in my talk it sparks somewhat east I talk drilled down on the compiler optimizer and performance aspect of system ml we have some numbers there now the the thing to keep in mind is anytime you're comparing two machine learning algorithms it's not just about wall clock running time you also have to take into account how well it converges what is the error you also have to take into account does the data fit the particular algorithm that you're doing so I talked about all these aspects in my talk which you can find on the SPARC Summit East website it's a great question thank you very much all right that looks like it's it let's have one more round of applause for our speaker you 