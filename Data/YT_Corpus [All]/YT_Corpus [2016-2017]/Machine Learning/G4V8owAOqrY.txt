 alright alright you guys want to gather your things and come have a seat well go ahead and get started on over don't be shy take a seat in the front row you know you want to all right got some seats over here in the nosebleed section lots of open seats okay we actually have four speakers tonight so I'm going to keep it pretty short welcome to Pinterest my name is Oona I manage the engineering ads and commerce teams here at Pinterest and I'm super excited to be introducing this particular discover Pinterest event although my machine learning knowledge is admittedly low largely because I think machine learning is one of the most interesting and prevalent topics that we that we have in engineering today at Pinterest we have machine learning engineers working in sort of almost every one of our core teams they're over in color and home feed they're in growth they're obviously in ads ranking they work on email we have them doing a little bit of stuff in almost all of our teams um we also think that machine learning at Pinterest is probably one of the most interesting places to do machine learning right now we think that for two reasons the most obvious one to most people would probably be our size we're only a few hundred engineers and what that means is that individual engineers still have massive impact every day we're still at the point where most of our teams are only four or five people and every engineer rolls out sometimes double-digit percentage gains on some of our key and most core metrics coming from one of the larger companies in the vet Bay that's not the kind of incremental improvements you normally see you normally see people working for months and months and weeks to eke out kind of one percentage one-percent gains while we're pretty small we're also big enough that a lot of the infrastructure and sort of the key stuff to get going on the good stuff is already built we already have a fair amount of our data pre-processed and digestible we have tools to debug and tune and roll into production real models and we have enough traffic to train those models on just really interesting data very quickly more interesting speaking of data then our size is our data set so I think we generally believe we probably have one of the both best data sets in the ballet today we have billions of pins and each of those pins sorry this microphone is really are talking to with billions of pins and each of those pins is labeled by hundreds if not thousands of users with boards those boards in turn have names they have descriptions they have more pins that have even more links to boards and they have users that follow them all that put together puts together one of the largest human curated webs of data that's available in the tin really anywhere today it also has some unique aspect of combining images and text and that from what is really one of the things that allows us to take techniques from two pretty different areas and meld them together in ways that other people working on other data says really don't have the ability to do there's some people here who are going to tell you way more about the details of all of this we have speakers coming from both inside and outside of Pinterest we have Melanie Warrick who is our keynote speaker coming from stage sky mine she's a deep learning engineer there and then we have three engineers internal to Pinterest all of whom have last names that I am NOT going to even try and pronounce we have Ouray who runs a team here that does a Stanford X Stanford professor who runs kind of a consulting team on date on machine learning projects throughout the company Pushkar himself comes from one of my favorite teams ads ranking and McCune comes from one of our oldest engineers on the home feed ranking team I hope you enjoy hearing a lot more about machine learning from the four of them and have a good welcome to Pinterest hi so thank you very much for having me here as she mentioned I my name is melody work I work at sky mine we're an open source deep learning platform for deep learning for Jay and I wanted to talk to you guys today about and I'll see if we get my in well anyways well they're bringing up the slides I came here today to talk to you guys or kick this off about machine learning but I'm actually going to take you down a path in terms of talking a little bit about machine learning and then talking about AI and to do this I figured I'd explain a little bit about myself and how I got to where I am right now I was working in business before and about three years ago I had a bit of a life-changing event in my life that made me decide you know what I want to do tech I always had a love of tech that I been telling myself this story that this is not something that is possible for me that I don't have the skill since I don't have the capabilities for it and not only that I was like you know I think I'm going to go after something that just seems like shooting the moon and I want to go after AI I didn't know how to get to that I didn't know what that meant and I know that sounds all fun and fluffy and maybe mythological but when I say AI you know of course I'm thinking many different images I grew up with some of the images that we've seen more recently these are the things that meant AI to me some kind of intelligent technology so I figured you know to get there I had to start somewhere and probably start learning how to talk to computers learn the actual language of those computers so I went to a boot camp hack right actually to learn how to code and then I learned through hack write about data science and I went to another boot camp to learn about data science and if you work in data science you know that what you're trying to do is take a lot of information and put it into some type of into some type of format that will help you make better decisions and so what I'm going to do is take you through my path to where right now and give you some understanding of these concepts that are very popular in our space so data science how many of you guys are data scientists all right great how many of you guys use machine learning okay yeah so machine learning I as I went into data science I learned about machine learning I was like this seems to be the right direction to be going in if I want to get into intelligent technology machine learning right well it's about using algorithms for pattern recognition and prediction and it's you know really leveraging and a set of algorithms to help you abstract the data and make you know whether you're trying to give some type of results summarize results or more I think more adequately make your technology make your services and your products more interactive intuitive and in the ways that it actually engages your users or your or whoever you're trying to engage and with in machine learning there's an algorithm that's neural nets and I realize as I continued to research the space that neural nets is really kind of a step much closer to AI because you know neural nets well neural nets at the heart of it was supposed to be loosely based off of the brain now I cannot stress that enough that it is not the brain but the algorithm itself has got this kind of connection to the brain in terms of if you think about your brain you've got you know you have neurons you have synapses they're connected they're transforming data until you have some understanding so that concept very high level is what sort of Bohr was where they went with developing neural nets and what you see here is a very simplified version of a neural net and I've given a few talks on this actually so you know I I can highly recommend there's a lot of resources actually I've listed it the later in the slides that you guys can check out if you want to learn more about them but ultimately you see this net it's got a number of equations that are taking place on each node or each node has its own set of a its own equation it's doing a linear and linear transformation of data that you're feeding through it and they're great for very complex high dimension data they're fantastic when you're trying to work with images and text and they're also really great when you're doing feature engineering now how many of you guys actually use neural nets okay got a couple people in the audience and how many of you have done feature engineering okay so feature engineering you've got data you're going to break it down into some kind of components and what it is it's about being able to find components of your data that will make your models more predictive so you're trying to make sure that whatever question you're asking or whatever problem you're trying to solve you want to be able to make the you want to be able to identify components of the data that will be more of a predictor for whatever that problem is so with this car and in terms of the car if I was going to break it apart and say let me let me determine how fast this car goes or predict how fast it can go the feature of the seat won't necessarily tell me that much but the tires might tell me a little something and the engine definitely will and so we do a lot of work when we're doing any type of modeling but especially machine learning to identify features that are salient for whatever you're predicting what's great about neural nets is that you can take all this data and just shove it into the neural net and it will help you automatically identify those features and so that's where some of that popularity for actually starting to apply this algorithm this algorithm that honestly has been kind of the it's been seen as this you know not realistic approach necessarily because it's hard to fully quantify or explain what kind of results you're getting out of it but now people are starting to see that they can actually use it for something like future engineering automatic feature engineering where you save time and effort but it's not just used for that they're starting to apply it to some really interesting problems as I mentioned from an image and text analysis side and on Drako path a a researcher out of Stanford and with opening eye he has done some fantastic blog posts and classes around neural nets and this is an example of one where he fed in the works of Shakespeare and this is what his neural net generated kind of interesting stuff it's also being used by companies like deep dream or actually a groups like deep dream I think they're based out of Google and they're feeding lots of images and then having the neural net actually generate the images and come up with its own interpretation it's kind of trippy little disturbing depending on the images you've looked at there's a number of research that came out in 2016 that actually shows how they've been able to identify objects in images and classify those objects and then put that neural net in terms of object identification with another neural net that does a text prediction and or text generation and be able to come up with descriptions for the net for the image and so that's actually pretty exciting all of this is actually pretty exciting what's interesting to me is that movies media all that it's sort of downplay haze the advancements we're getting because you know users expect us to already be like living on the moon with the way our technology is and this kind of stuff we didn't have this a few years ago or I guess before 2014 but we have it now and it's it's pretty amazing what these types of algorithms are allowing us to achieve and of course the ubiquitous example I think everybody talks about the self-driving cars neural nets are playing a role that's not the only thing that goes into this but it is playing a role in terms of making this work so you know where it matters most why companies are willing to apply them why they're willing to use them is for things like what you guys and I saw Andrews I who's in the visual team here he presented it rework a couple weeks ago about how they're leveraging a neural net for you guys to do object recognition with your image with an image search and Pinterest which I was pretty impressed at I mean that is taking some amazing likes a great way to take some research that's been done in the space and apply it to a product that's very valuable to your user base or to the Pinterest user base and you can only imagine that this will continue to be applied in many different companies going forward and you can see how it will be applied that kind of image search analysis that's very valuable and that's why companies will start to pay more attention to this and look at how they want to apply it so okay I've told you about how you know there's machine learning use these set of algorithms you're doing in this is all machine learning this is ways that machine and machine learning is being applied it's specifically about the neural net algorithm and machine learning but iNSYS AI is this AI what is AI what is artificial intelligence because you know I've been trying to move down this path towards it and that's a question that you know as I mentioned when you say those words you think robots you think humanoid robots typically maybe not everybody but these are images that will start to come to mind a lot of people have gotten together and argued extensively about what that term means or what those terms mean and there were some actually some researchers who put together a paper on all the different definitions that exist and it seems to be there were some common themes about you know it's about learning it's about it and you know this idea of reasoning and addict adapting what your knowledge is to the situation it's having an optimal learning agent or having some level of intelligence in your technology that potentially you know is that is at a level of human maybe exceeds human these are things that come up when you start to talk about intelligence in the technology and then there's arguments about what exactly is intelligence how do you know it's intelligent what is consciousness how do you know that there's some level of consciousness it's interesting stuff and I'm going to continue to go down this path and tell you a little more I'm going to go a little off the rails a little more in the Wednesday because when you say a I it's it's a little bit mythological in some sense it feels almost like it's not possible and then also feels like it should have been already done we've talked about it for so long now and one of the the people who spend a heavy researcher in the space said that you know if you label something a I when you finally actually achieve it it no longer has that label because it's it's real now so I read this great blog that it's you know wait wait but why that did a pretty extensive research and explanation about the space and that's posted last year and he helped break it down into kind of three three sections to in terms of how you want to think about artificial intelligence there's weak they're strong and they're super and so where we're at right now we have artificial intelligence that's in a narrow focused space you know we have some expert machines that are helping us do things like beat us at chess beat us at various games that we know how to play as well as you know we have computer viruses that can adapt and avoid being able to be detected we have it helping us to do facial recognition the self-driving cars that's AI that's narrow focused machines that are performing at a level that's as good maybe better than what humans are able to accomplish where we aren't at and what we're all looking well and actually to go down that path a little bit forth further is that there was recently a research that came out from deep mind which was acquired by Google and they beat a world champion and go and if you don't know what go is it's a much more complex game than chess and when you again when you talk about AI you say it sounds mythological and you hear about the games being being played to beat a are to show AI well game is actually playing games is actually a great space to be researching for this area and the reason why is because it's hard to search that space and that is a very complex space and you have to get very strategic in terms of how and and what's the word I want to look for you just have to be very creative in terms of how you're going to design the software to think about the space and quickly adapt especially when you're working with an adversary in it so when we talk about AI and as I mentioned neural nets are not the only thing in terms of how you're going to get to AI these are there's a number of different areas of research that go into artificial intelligence game playing is definitely one as I mentioned but search which is what game playing would be is where you're seeing a significant amount of effort placed into research for AI being able to access the information that Google search can do in the amount of time that it can far exceeds anything you and I could do and that's a big deal but when we want AI we want human-level AI we want to see human level of intelligence and we are not there yet and we may never get there or we may get there tomorrow who knows the research is going on there right now it's seriously being pursued and human-level AI is really the idea that you know maybe we will get technology that has the skill level the IQ level of a human at 100 or more some people wonder will we get there if we take all these expert machines that we have and just collect them together or will we not to be determined in terms of how that goes but when we talk about getting to that point Turing of course gets mentioned it's the chest to tell whether we've achieved this point and I'd argue is turn really the right test for this drink is great for creating chatbots and we beat it apparently we beat the Turing test of the Turing test was beaten in 2014 by Eugene boots men which is this Ukrainian 13 year old Berkut Ukrainian boy whose second language was English did you guys hear about the Ukrainian 13 year old eugene boots Minh yeah so the idea with the Turing test if you don't know is you're supposed to have a computer and a human and you've got human judges who are trying to assess whether that computer is human or not and if they trick 33% of those judges or 30% of those judges then it's proven that they've beat the test but does that mean we've actually achieved human intelligence arguably no because you know that's an imitation game and that's the whole point of it but are you really seeing some type of level of consciousness and of course what is consciousness right so some of the approaches to get us to human-level AI is looking at how we can leverage Hardware how we can get our hardware to be as as solid and as performant as our brains in addition to how to get the software and the hardware supposedly were there we're there in terms of our micro processors are faster than our actual neurons and our brains you know the we there's a computer that they say has been well that has been built in China it's a supercomputer it takes up a ton more space like 720 square meters so it's fairly large but it's the kind of look at this it's the 10 he - and they came up with this they came up with this equation basically or they came up with a way to measure how fast our brains and how performance our brains are incurs well in particular who's a futurist who does a lot of research and writes prolifically around singularity he actually helped define that there was the number of calculations per second and our brains is around 10 quadrillion while they're saying that this 10 he too is actually able to achieve you know 3 times that or more granted as you can see it's using a significant amount more wattage in terms of power and it's definitely as I mentioned taking up a lot more space so are we there yet in terms of the size and the structure of our brains being able to fit the technology into that space no but we are getting there in terms of the hardware performing but that's not what's going to get us to human level legends it's a software there's a couple of approaches that are going on to try to get us to the software one of them being the whole brain emulation which right now where they're at they've got us a to a level of a worm and as you can see as I've listed up there they're around 302 neurons while they estimate about a hundred billion although I've read a couple different accounts on this but a hundred billion neurons in our brains so that's also a little far off then there's also some other ideas in terms of how to get us to that intelligence but I think one idea in particular is being able to make the technology recursive and self-improving and so that's where we start to talk through you know if we can get us to human level if it can self improve and it can be recursive then maybe we will start to go past that into the super intelligence and some people think that if you can get to human level within a certain amount of time that it will be a fairly quick turnaround it could be you know months days hours before you start to hit the super intelligence mark that's where they call it the intelligence explosion and they say that this would be the last human invention because once the intelligence is able to exceed hours we won't understand it we won't be able to keep up with it and what's to stop it from just doing whatever the hell it wants there's a number of different predictions and yes I'm going off into this is the law I'll and a little bit but it's also some of this is you know it's kind of fun to talk about some of this potential where tech could go so the predictions there's a number of predictions from various sources and researchers brought this all together and averaged them out to be 2040 so maybe it's 2040 maybe it's 20 you know 21 40 maybe it's never what's interesting about this is that you know this stuff like I said there are researchers who are pursuing this pretty avidly whether they achieve super intelligence or not they are trying to get us down this path where we have technology that is indicating you know and general intelligence it's not just this narrowband I beat you at chess but that I beat your chess and I'm aware of what I am and I'm aware and I'm not only able to beat your chest I'm also able to make coffee and I can then you know write us on it and understand that sonnet we don't have the tech able to do that right now but they are trying to build it and when you talk about it there's fear and there's excitement and especially when you talk about superintelligence there's fear and excitement to the extreme so let's go ahead and touch on the fear in the excitement at the extremes the biggest concerns the biggest excitement is definitely we could live forever or we could all die so everybody run for cover go out and spend all your money right now or before 2040 okay so that's a possibility this might happen this might not have kind of fun to sort of explore where that could lead us but I think it's more realistic to talk about how AI is impacting us especially narrow AI how it's impacting us in the near future as well as from a positive and a negative perspective so I showed you all this cute and maybe not so cute images of robots in the beginning one of the challenges we have is that we anthropomorphize the technology we like to ascribe these human emotions on it and it's important to realize that you know this tech doesn't necessarily just because you've coded it and you're human does it mean that you are making the technology itself human it's not taking on your characteristics unless you're finding a way to fully encode into it that understanding so it's important to understand it is its own alien it's very different in terms of how it is its intelligence works and that its goals are being set by you right now so that's good to be aware of I think from a more you know rounded perspective when you're working on machine learning and you have technology that can improve itself and can adapt and especially when you're using technology kind of you know influence on others that there's an interesting kind of line we're going to start to see being blurred more and more from a persuasion and a privacy perspective because we already have sales and marketers who are amazing at selling stuff are amazing at figuring out just the right way to present things to you to want stuff you didn't know you wanted and we're now building this technology that can adapt and understand you understand your behaviors understand your nuances and quickly show you things you didn't think you knew or didn't know you wanted but now you do I think I remember reading the thing about how they started putting beer near was it diapers because I realized fathers were coming in to grocery stores looking for diapers and picked up some beer on their way out so that's an interesting space and I think you know brings to light or I'm trying to emphasize this from the standpoint of it's good to be aware of it when you're working on it you know it's not to say we won't see this continue to grow and change but good to call it out from a legal standpoint as our technology becomes more intelligent and adapts more quickly to us we have to be aware of the fact that our legal protections won't necessarily keep up and that could be a challenge I mean it takes a long time to get certain laws into place but if we have self-driving cars that changes the rules in terms of you know liability and what that means what that means for us but what's more salient what's more pertinent to us right now is the potential challenges are from an economic standpoint there is definitely a risk that we will start to see wages stagnate and save jobs job growth kind of reduce it may it may be happening and maybe already happening right now I actually was at nips last year and this MIT professor presented this is a graph you shared where he was trying to show wage growth over different decades and that bottom blue line is his way of showing how in the 2000s from what their research has shown is that you know we're starting to see kind of level out there's always been an argument that as we switch industries that we actually see you know just shifts skills into different industries but there's not a disappearing of our actual jobs we just have you know new job new jobs or new skills needed in other places like I said the concern is that as we're making our technology automated or automating things as we're making more intelligence in our technology that the technology can do things better than we can at certain skill levels we may be actually reducing the jobs that are out there and what kind of jobs are available so ok it's not all dire it's not all the end of the world those who are researching AI are researching neural nets in general and and and I'm not trying to say neural nets are only AI I know said that before and I want to make sure I'm very clear on that but those who are looking to AI are typically working in neural nets and what they like to talk about a lot is how the excitement around how this is going to impact us from a health perspective I spoke to somebody who worked on a project where you can take a picture of a child's ear and send that into computer system they built that will be able to help you diagnose if it's infected and what kind of treatment you can get from that that's pretty awesome actually and that type of solution those types of solutions are being explored more and more I was you know reading about how they're hoping for faster diagnosis they're hoping for maybe faster cures I mean if we start to see different diseases pop up like the Zika virus maybe they will be able to come back with more intelligent technology to quickly turn around some type of you know some type of of medicine that we can take and offset these different viruses or different problems that are arising from a safety and security standpoint this can go either way in terms of the positive the negative but you know to be able to identify different anomalies that are out there identify terrorists identify local crime issues somebody Google had told me that they're expecting from a safety side of things that when they have more automated cars on the road that they're estimating around 1.2 million lives saved a year from an environment standpoint you know we talk a lot about global warming and the impacts that we're having humans can have on the environment I mean we already see you know utilities are looking at ways to use artificial intelligence to better plan their resource usage and where they can actually save you know the impacts on their infrastructure and you know potentially reduce and optimize how their resources are being shared and you know going back to a little bit more fun we have was it prosthetics that people were able to have implants into their brain and they can potentially use brain waves to manage and to move prosthetics you know there's other potential opportunities for intelligent tech to be integrated into our bodies for helping us overcome different challenges we have as well as maybe increasing our ability to work in the world around us so those are things to think about when you talk about a guy and you think about where this might be taking us and then a few other things I wanted to touch on just kind of for fun you know or not for fun yeah yeah is going on I mean artificial intelligence is happening you may not want to call it that you may in it does get it that term does get thrown around a lot right now but we do see through machine learning through neural nets to other types of algorithms that are being applied ways in which to make this technology more adaptive and integrated into the world around us there is serious pursuit of AGI and there's a common agreement that if we were to achieve a si and I'd love to chat with anybody afterwards they'd want to discuss where we're going and what that's going to look like but the discussion is that it's going to change everything and it's hard for us to really know what that would look like if it does change everything I mean how can we imagine what that intelligence would actually bring to the table fully and then you know if we were to merge with tech I can't help but wonder would we still be human so on a more related to this whole talk side of things you know from what is machine learning as I mentioned it's an algorithms algorithms that help you do pattern recognition help you do prediction what is neural Nets it's great for at the end of the day it's an algorithm that it's great for doing feature engineering and it's very valuable for companies the reason why you will see them spend money is for customization and what is AI I mean AI is a level of intelligence you can spend however long you want discussing what kind of intelligence but I think when you start to get to the heart of it it is definitely around looking at achieving some type of self improvement recursive learning and those who are researching that space where they think the next step is in artificial intelligence is being able to explore unsupervised learning and seeing what opportunities what information you can get out of that that's where they're focusing and I told you this explaining this story to you from telling you what I was interested in how I got to where I am a little bit because as I mentioned AI sounds a little fanciful the pursuit that I've been on seemed a little bit unbelievable in some ways but I am working on neural nets right now and it was one of those things where you know I took a risk I went after it and and I'm still going down this path it's not like I've completed it but I'm trying to emphasize because machine learning to me is something that requires some level of creativity some level of going after things that you wouldn't have imagined dreaming about or asking the hard questions asking the silly questions asking some crazy questions kind of being a little out there in terms of what you're going to work on because that's where you're going to start to find and solve problems that you wouldn't have imagined solving so I highly recommend pushing boundaries you know if you find that you're going to fail at it fine fail move on but if you push boundaries you are really going to actually be more successful in this space if you really seek out some of the more interesting and fun and creative and silly and strange ideas you'll find you're solving some problems you ever would have imagined so take risks I highly recommend it all right and I have a lot of references especially images because they use a lot of images and some people help me I was really help very thankful for that I am going to post this out there if you do want to see these slides I'll be tweeting them out shortly but I work for sky mine as I mentioned we're an open-source neural net platform if you guys have any interest in contributing to it we would love it you know we're always happy to have anybody join in the at nidal is my twitter handle and we have a chat channel on its Gitter is actually a github chat channel so if you want to join that at any point and geek out about neural nets or anything else we welcome it and that's all I got thank you so much for your time ok thank you good hello thanks for coming I saw quite a few took my book so I hope you enjoyed if you find any mistakes definitely let me know so that the next edition is even better ok so what I want to talk to you about is I've been here working for Pinterest now almost a year and I'll tell you what I learned and I'll show you one of the projects I worked on ok and really if we think of pins as this objects that people are collecting all over the web the question is how do these pins fit together so that's what I want to teach you ok so here is one way how to think about Pinterest right you shouldn't be thinking of Pinterest as a social network but you should really think of it as a discovery engine right where people go and collect interesting objects on the web and save and curate them on Pinterest so that they can come and retrieve and do something with them later okay so what are the subjects that people curate of the web right so people take these objects we call them pins here's a pin it has a description and everything else right it's a particular letter bag and then they organize these pins into collections called boards right so somebody can come and say here are my favorite leather bags right so what Pinterest is it's really like a catalog of different types of objects and how this object in some sense fit together we have around 30 billion of these objects and they are categorized into 750 million collect collections or boards as we call them right and really this is all done by humans and the question is can this kind of data about how these objects fit into collections can it reveal anything about how would the spins fit together into a giant Network right so how do these spins these objects how do they relate to each other and this is really what I want to tell tell you about and the reason why we may want to know how things relate to each other is because if you are serious about building any kind of recommender system then you should know that if I recommend you one thing how does that thing relate to all other things I can recommend you write so that's kind of the reason why we want to understand relationships between these objects okay so what's the question the question is can I go discover relationships between these pins and how to kind of connect them into a giant network that's the that's the task okay so now I'll tell you how to how to structure this and how to think about it right so the first thing is that I have to defy want a network I need nodes nodes I have right those are my pins right those are the objects I care about now I need to define the relationships and for now let's work with just two types of relationships let's talk about what is called a substitute and let's talk about what is called a compliment right a substitute two objects are just substitutable if one can be used instead of the other right so if you think about a leather bag here here are other leather bags they are substitutes but then projects are also come or objects can complement each other so basically one one object can kind of be added on to another object right so for example together with the bag you can have a scarf or a set of sunglasses and things like that right so those are the complimentary projects so for now I just work with these two types of relationships substitutes and complements now of course if I'm able to identify the pairs of objects that are in the relationship I can combine them integer into this giant network of all kinds of objects and I understand how they fit together okay so that's kind of what I want to do so why is this useful for why how could I how could I use this right the first thing is this is very useful because I know the relationships and I know what substitutes what and I know what can be complemented with something else right so if I'm making recommendations I can make much more context relevant recommendations because I understand now I'll show you a few of the things once you choose of one of them I'll start recommending compliments for example right so that's the first thing the second thing is that I actually also know these relationships they are not undirected they actually have a direction so I know that one object can substitute the other one but the other one may not substitute the first one so kind of I know that that people may navigate from one object to another but not the other way around the second thing that this network allows me to do it allows me to reason about groups of objects so in some sense it allows me to discover this tightly knit groups or clusters of objects that are all substitutable with each other right so I can find this kind of micro categories of products that are very related to each other okay and another thing that this thing allows me to do it allows me to reason about again about kind of groups of objects right it allows me to kind of start for example if I want to recommend it I can start recommending outfits I can start recommending you know if you want to make a cocktail here the recipe here the here's the alcohol here's everything else you need to know right because again I understand the relationships okay so now how do you go and build a system that allows you to do this and there are really kind of two questions the first question is how can I model this pin-to-pin relationships and the other one is how can I go and explain the relationships to the user so those are the those are the two questions so here is the the problem setting the problem kind of the way if you write it down turns out to be very simple right I'm given two pins x and y and what do I want to do I want to predict whether there is a relationship between them meaning they are whether they are substitute or complimentary right so my goal in some sense is just to build a probabilistic model that will take her two pins and tell me whether they related or not and you know let's say that this P will be a probability 0 means not related 1 means related for sure ok so how would I go build that right so let's kind of keep working on building this intuition right so the first thing I need I require is I required a bit of training data right I need a few pairs of products for which I know our substitutable and I need a few pairs of products say no no these are not substitutable or these pairs are not complementary and then what I want to do is I want to learn this predictor P now I have some set of parameters theta and one I want to go over all my training data data pairs of products for which I know are related and I want those probabilities to be high and I want to go over kind of all pairs that are not related and I want those probabilities of relationship to below so one minus the probability is high right so now the game is figure out the status that make this probabilities work the way we want them to work okay so now I still haven't told you what this piece P thing gives right so the first thing I need to do is I need to decide if I have these objects what will be their features how will I describe them and at Pinterest we have really lots of data I know a lot about these objects I know the title I know where on the web are coming they are coming from I I know what board they belong to I know I have data about the image I know what users interacted with them and so on and so forth and the way the way I can now represent this object is as this super long super sparse feature vectors right there basically everything the zeros and there are a few non zeros okay and now I can start saying okay the probability that x and y are related somehow depends on the distance on the differences between their features right and because I have these parameters here I can think that some some features are more important for the relate for a given relationship and some features are less important okay so that's again just the basic idea this still doesn't work right the reason why it doesn't work is because the question is what are the right features to use and the second one is how can I learn over this super sparse feature vectors and not over fit so here is now how how how you would go about it right one way you would go about it you'd say I'll do some dimensionality reduction right whenever we have too many features not enough kind of things are super sparse we want to smooth the data wanna do some dimensionality reduction the way to think about that is in some sense that you want to take your data and project it into a lower dimensional space right you will do some SVD or you to do PCA or you would apply later deletion allocation something like that right that would take your PIN and kind of project it into thus this much smaller dimensional space where you could also have some intuitive labeling what these dimensions correspond to and this is a fine approach but it's not good right and the reason why it's not good is because these topics these embeddings you will learn will be good for you to explain how how these features relate to the private tutor to the object but they won't allow you to make good predictions about the relationships ok so the point what we wanna do and how we solve the problem is to really the low dimension to learn the low dimensional embedding that is good for predicting relationships and not necessarily good for explaining the features of the product right so in some sense I want to learn the embedding of how to take the features of a product and embed the product such that I can estimate or predict the relationships well and what turns out is that this decent weddings are kind of very comparable to the kind of standard ones but just things are shifted in certain ways subject prediction power is really maintained in the in the embedding and this way I now enjoy kind of the the good features in a sense that I was able to lower the dimensionality of my problem and I can do good predictions ok so that's basically the idea so there are still a few problems you have so the first problem is that if you just do this embedding then you still don't know how to think about the direction of the relationship so we really need two models first model says are x and y related and then the second model says if x and y are related what's the direction of the relationship and what is interesting is that what you find is that the properties of the product tell you whether two products are related but then this very kind of subjective type of language what people think about that that object tells you which direction the relationship is going so that's kind of if you look at what the model is learning so that's one thing and then the other thing is we don't only have one type of relationship but we have multiple types of relationships so the way you solve the problem there is that you learn one embedding but then the predictor that takes the the low dimensional data and predict the relationship though those predictors depend on the relationship you're trying to add to estimate so one in Bannack multiple predictors or multiple regressors for each of the relationship types so that's kind of the sketch of how we do it so the way we now do this is we implement this in C++ we have these sweet machines with the terabyte of memory so you just kind of suck it in to memory and it and it works really well you use OpenMP to paralyze the computations the way you do the learning you use kind of a second-order quasi Newton method l-bfgs is the method of choice deep sampling for the low dimensional topic modeling and things works really well so what I want to do next is show you a few examples um the first way we we tested this was actually we tested using human human data and some human ground truth and in particular these things work really well on on objects where kind of that where these relationships are not entirely clear like for example for clothing and things like that in electronics and so on these relationships are kind of in some sense easier to determine but they are less surprising wiring clothes it's interesting to see what matches what what substitutes what but overall we can do quite well finding substitutes is easier than finding complements and if you think about it intuitively that makes sense a few examples this is the query right this is what you are looking at here are the substitute here are the complement if you are looking at this type of belt these are the substitute belts here are some complement ideas something for men again another belt matching belts and some complement ideas things are quite good other here as well I was telling you before is that you can find clusters of tightly related products here are some examples of that black t-shirts checkered shirts shoes and you see how these things kind of really visually match each other and just to conclude kind of what are the next steps the next steps are really to kind of expand the number of relationships we are able to learn so we can really start reasoning about how are things related and how one type of object can be recommended in the context of an the type of object um if you guys have any question happy to take them later we also have papers on these things thank you very much thank you thank you Ray hi I'm Pushkar and them okay some references today I'm going to talk about unsupervised topic modeling using Bayesian inference so if one were to take a ten thousand foot view of Pinterest as you have pointed out it's a network which connects users to the best worst possible content so that's the problem we are trying to solve every day the issue is that standard approaches of matching users and content don't work well at our scale we have hundreds of millions of users several billion pins so you cannot really match every user against every pin to find the best possible matches so the first step towards solving this problem is to come up with a good representation for our users and add pins and that's exactly what I'm going to talk about so good starting point for developing this relationship this type of a representation would be to use a bag of words so for users you can derive a bag of words based on their interactions with the site so this can be the queries that the user has done or the boards that the user has created or the titles for the pins that he has interacted with similarly for content you can derive these bag of words based on the boots that they are pinned to the searches where they show up and a whole bunch of other metadata so that's a good start but there is a problem with using bag of words in any sort of model and the issue is that bag of words are often very noisy and they're often overwhelming for for a given model so for example if you look at the bag of words shown on the left hand side in this picture it looks kind of noisy and doesn't convey as much meaning as if I were to say that the user is interested in trucks and so how do we derive this kind of relationships automatically and that's exactly the topic for this talk I'm going to describe a system which we call pin fear ins which given a user pin or a query and a handcrafted hierarchy of concepts it finds out the subset of notes from the hierarchy that best describe this user pin or query ok so for the next few minutes let's punt on the question of handling queries which are freeform text and just focus in on users and pins okay so our journey begins once again at the humble bag of words so we can derive this bag of words using the techniques I discussed earlier and to really understand how these bag of words fit into our given hierarchy we need to build a generative model that is we need to understand how these bag of words came into being and if we can do that we can also put them into our hierarchy of the right places so a generative model will be a Bayesian net and simply a Bayesian net is a directed acyclic graph where each node turns on depending on the states of its parents so an example net is shown over here so we can take our hierarchy and naturally push it into a Bayesian net it just becomes the layers of a region net and then we add another layer at the bottom that is the set of all words from our dictionary ok so this is the structure for our net one more thing that we have in our net is that we associate probabilities with every edge and these probabilities are used to define conditional distributions for every node given its parents the way we define conditional distributions is we use a noisy odd for every node so you take any node and look at the active parents and a noisy odd of them gives you the state of the new node so let's go through an example that will make things clearer so suppose we have a user who's interested in health and in in sports and it's not and he is not interested in yoga and what we want to determine is whether this a user would be interested in dieting so what we can do is using this noisy or concept we can say that the probability that is interested in dieting is given by the equation shown over here so that's our factor for every node that's our description description for every node okay so that's how a generative model now the next question X this brings up two questions first how do we train these kind of models that is how do we figure out the probabilities on the edges and the second question is suppose I give you a train model how do you do inference on it so given a model and a set of words how do you figure out which nodes in the model best represent those words so the good thing is these two problems are actually tied to each other if you can solve one you can solve the other so let's see how we go about doing the training part so our solution to training is to use the e/m algorithm implemented over hadoop abstractly in the e/m algorithm you take a bunch of examples so you take a lot of data and throw it at a really simple model so our starting model had all probabilities set to the same small random positive numbers and then you see what sort of predictions that the model come up with what sort of internal nodes light up for these these words and then you use these predictions to refine the model to come up with better probabilities which make the data more likely and then you rinse and repeat so that's a very abstract view of how we did training we can discuss more details offline more importantly if you sprint a little bit you will realize that the first part of this whole training pipeline the e stage is exactly the inference phase right so the plot the part where you throw they tie the model and see what it does is exactly the inference problem so if somehow we can figure out how to do inference we can also train the model so if you can use the model you can improve the model so how do we do inference so the bad news is that inference is a really hard problem for general Bayesian Nets it's been studied for a long time by folks in machine learning and they've shown that it's np-hard for general general Nets you can't really have an optimal algorithm for it and it's also a px heart so you can't even get close enough to a solution and say and declare victory so in the machine learning community in the late 1990s and early 2000s they came up with several techniques to do inference over reasonably large networks but unfortunately these techniques do not scale to our size so we had to innovate a little bit and get around these problems so here is what we did did so before I get into the details one way to think of the state for a one way to think of any given internal configuration is to think of it as proof for a given input any state describes a hypothesis on how we believe this input was generated and the probability of getting to an internal state is the measure of how strong our belief is so if we could intuitively if you could enumerate all possible internal states for the network and then find the probabilities associated with each of these states then you can take expectation over these probabilities and get the activations for every node but this would obviously be very very slow you can't enumerate over thousands of nodes so what we did was we came up with a technique to find just the best possible internal configurations that explain a given set of words and then average over just these best possible internal configurations so how do we do that we use a technique called local search so what we do is we begin with a random assignment of nodes say all nodes turned off and then they slowly perturb this assignment to come up with better explanations for our given set of words so in each step we flip the state of one of the nodes and see if it gives us a better explanation and we keep doing this until we run out of compute resources or after say hundreds of steps and what we found was that towards the end of this local search we had really good explanations for our given input set and if we average those we get a really good activation set for these set of words we have so recall that I pointed on the question of handling freeform text the way we do that is we simply use our search back-end to bootstrap a bag the bag of words so suppose your give me a give me any query like rooftop gardens I'll hit the search back in and look at the results that are returned for this query then I look at the metadata associated with each of these results and that gives me the bag of words and this bag of words can again Bishop Throop inference and you get topics or verticals for this query so kind of a g-pack once we have stuff working for bags of words so let's look at some results to get intuition on how this thing is doing so on the Left our queries and on the right are topics that we input from them for them from our given hierarchy so I'd like to draw notice to couple of results here the last two results notice that inference managed to distinguish between egg and Easter egg so Easter egg is classified into more holiday themed stuff like holidays and events parties whereas egg is classified as a pure food and desert and quick meals kind of query so I would consider this a win as opposed to other token based approaches which would get confused with the common token between these two searches so in an audience which is primarily engineers it's imperative to point out that Lilly Pulitzer is a fashion designer this is a Wikipedia page soap imprints got this right as well and this was used to me as well so seems to be working well okay where else can we go with this so apart from keeping us abreast with the latest and greatest fashion influences also found some uses in our ad stack so in particular as would be most obvious to you guys we use it in our click-through rate models to predict how good a piece of content is for our users we also use use it as an implicit signal in ad retrieval to find the best content that can be shown to a given user we also use pin prints as an explicit targeting signal so advert can explicitly mark out notes from the hierarchy and target their content based on our pensions labelings and finally we also use it for broad match and for keyword filtering so broad matches the problem where you want to take a given query and expand it to a list of related queries that the advertiser may have missed and keyword filtering is the problem of removing poorly targeted words so in both of these cases we found two infants to be a really good match for the problem and that's all that I wanted to say we have questions let's catch up offline thanks bushka so my name is Moe I'm an engineer here at Pinterest and today I'm going to be talking about some lessons that we've learned while building out machine learning models for home feed so as you probably know home feeds it's a personalized stream of pins it's generated from a variety of sources and the home feed team uses machine learning very very heavily in this talk I'm actually going to be focusing a little bit more on the operational aspects of machine learning I'm not going to really talk about some of the algorithms specifically that we're using I'm going to talk a little bit more about the operational side of machine learning here are some problems that we're actually applying machine learning to in-home feed you know we have to generate candidates from billions of pins because obviously we can't show you all of those pins so we have to generate a subset of candidates we need to rank and score those candidates that we actually generate we need to blend these different pins from different sources we need to blend them together so that we generate a phone feed that's both relevant and diverse and finally of course we need to scale this out you know we need to do this millions of times every single day so we use machine learning for all of these tasks and I'm going to be talking about what we learnt while we actually built out models for doing these various problems so the first problem is that you know the value of debugging is one of the most important things that we actually learned machine learning systems are you know they they are very hard to debug they essentially robust systems they are designed to be they're designed to work in the presence of noisy features they're designed to work when e features are incorrect or not predictive enough you know so for example it means that you know if you're incorrect if you implement a feature incorrectly you know the machine learning system will still do something intelligent it still works if your system if if your feature suddenly stopped working you know your system does not behave it does not it does not fail catastrophically you know it degrades gracefully so what this means that it makes debugging these failures very very difficult so as an example you know what we had done was we had actually made several of these similar kinds of mistakes one of the first models that we actually built out the feature was actually transformed differently in production than it was actually trained in you know so you would think that this would actually behave very very badly but nonetheless when we actually put it out there it showed lift strange when it actually worked you know and the reason is that you know there are lots of other features that actually worked in this case and so it kind of you know the robustness was hit the problem that there was a problem with this feature you know we've had problems in our training tools in which case you know some of the features actually get dropped nonetheless these systems you know they somehow compensate for that because that's what they designed to do so if you only look at the output of these systems if you only treat these systems as black boxes then it becomes very very hard to actually find out when something goes wrong right and so what you need to do is you need you need to not just look at the output but you also need ways in which you can actually investigate and then figure out if everything is behaving internally as well as you possibly can behave right the other big one is that you know so besides the fact that you know debugging is hard it's also interesting to know that there is really no objective data set for many of the problems that you're actually trying to solve in practice there's a lot of subjectivity that goes into your choice of data set you know so for example if you are actually trying to build engagement prediction models you know very often what you would do is the training data that goes into these models actually get sampled from your logs and depending on for example when you actually sample those logs the training data is actually going to look very very differently you know so for example in our case if we actually sample the data and the December January time frame we expect to see lots of new users if you sample it in the you know October number time frame you know you won't see as many new users so the data set actually looks quite different depending on the time of the year that you actually sample the dataset we know for example that you know if you're actually we know that you know for example we make certain choices about the data set and that choices for example depending on computational constraints again if you're actually trying to disengage Maury's then you typically will not actually sample a positive samples any negative samples on a one-to-one basis you know typically you will sample the negative data at a much lower rate in the positive data so what this means is that you know there is again arbitrariness in your choice of training data and so all of these arbitrary choices that you make they contribute much more significantly to your final model or they have bigger impact on your final model then for example your choice of models right so you know whether you pick a logistic regression model or an SVM model matters a lot less than you know how we actually sample your data so it becomes a lot more important for you to actually systematically test and bury this much more so for example in training then varying the regularization parameter you know so it's not just the training data that that's that's an issue you know the evaluation itself tends to be very very hard so it's pretty common knowledge for example that you need to have a different training data set than an evaluation data set right so you kind of know that you don't want to do your evaluation on the same data set that you use for training but it's not just a difference between the evaluation data on the training data but it's also the kind of metrics that you use when you are actually training a model you know you typically optimize for the loss function when you're actually evaluating on data or offline data when you're actually evaluating the quality of a model it's not necessarily a hinge loss or a loss function that they concerned about but something like for example the classifier accuracy or the area under the curve however when you finally ship the models what you actually care about as you know are the business metrics that you're interested in do they actually change or not now all of these three things are actually quite different you know so for example if you know that you all fit on your training data the classifier accuracy might be different on your training data and then your test data and similarly if you over trained on your offline metrics the you might actually have a different set of models that perform optimally on your offline data then perform optimally when you actually ship it and this is particularly important because you know every experiment that you run it actually takes a significantly long period of time to actually get the results of the experiment whereas in the other hand offline metrics you can actually read them quite quickly so the feedback loop for offline metrics is Dulli small the feedback loop for online metrics takes a long period of time so it's very very important to actually pick a set of offline metrics which actually correlate well to your online metrics so rather than picking you know an arbitrary offline metric you actually wanted to your offline metric any tune you want to tune the way in which you actually select your offline data so that you can actually do a good job of predicting how the model is going to perform when you ship it and this is even more important because you know as I just mentioned there's a lot of subjectivity in your choice of training data right so because there's a lot of subjectivity in the choice of training data the evaluation data typically is different in distribution in the way that is actually generate from the training data so even though it's conventional wisdom to actually make sure that the evaluation data on your training data have the same distribution in fact you know are experiences that you actually want them to be different so this is you know also maybe obvious but you know you actually want to invest significantly in your automation processes there are two aspects of automation one of them is that you want essentially a push-button process to take training data and then generate a model out of it now you want to make it very very easy for you to actually build these models so that you can constantly be retraining new models make it easy for new people to death generate new models there's another aspect of automation which is you want a stable predictable process the idea being that if you train a model today you train a model next week you don't want the models to be widely different you want your automation process to be robust and stable so that you can actually you know with high confidence trainer model and regular basis and know that you're making progress so automation essentially in my mind automation means both of these things and having a well a good automated system means that you know it essentially lowers the barrier to expect to experimentation you can try out lots of variations easily it reduces the temptation to actually bundle lots of different different changes into a single experiment it means that when you want to try out something new you have an easy way to actually generate a baseline based on the data set that actually trying it out on we have found that you know regular training yields predictable results and simply because we actually have a good automated process you know it took us a bit of struggle to actually get to this point because earlier on sometimes we will see again sometimes you wouldn't and we had to invest a significant amount of work in actually getting these things so that it's predictable in fact one of the one of our goals is that we want to make sure that we have a process that anybody who is not in very familiar with our processes can go through a documented process and then build a model which is roughly on par with our production models so of course if you actually have a good highly automated system you know you can build lots of models and you know that's kind of you know one of our goals we want to make sure that you can actually build more models you can use more data more features run lots of experiments and this is especially important because you know it's hard to tell upfront what's going to work well and what's not going to work well so you want to make sure that you can try out lots of things you want to make sure that you can try out lots of things cheaply when things fail you want to make sure that you can debug them easily so yeah now if you're building out lots of models one important things to keep in mind is that you know you end up with lots of models so because at this point we literally have hundreds of models in production literally hundreds of models in production and they trained they have different kinds of models we have linear models we have decision tree models we'll have neural network models they're trained by different engineers they optimize for different things they used they use different sets of features they use for different purposes but they all run on the exact same infrastructure right and because we have this infrastructure which can actually do lots of different things that lots of people are using it for different different reasons it's kind of important to actually have a process in which you can actually manage all these many models that you'll end up with so we treat our models like code we treat our experiments in the same way that you know other people who treat code as well you know so our experiments go through a rigorous rigorous review a models that actually go into production actually go through a rigorous review as well every model that we actually build out we know I mean we documented fairly we documented well we have good processes for tracking experiments and then communicating successes and failures so if you have lots of models used for lots of different purposes stands to reason that you know some models are actually going to last for a really long period of time and that's essentially what we have so we we have models in fact we have models that we built a year ago that we could actually launch right now right so we have these models which can actually you know it's are in I mean we have the capability run old models and we do this every now and then we actually run old models we do this for a number of different reasons we definitely want to be able to do long term holdups we want to make sure that you know every once in a while we can actually go back and then test out an old model and then see there's really a significant difference between the old models and new models so what this means is the lifetime of the model and the lifetime of your infrastructure have to be decoupled right at this point we literally are at the point where our infrastructure is changing but our models are still the old ones so it means a new infrastructure has to be able to run our old models you know so had the other aspect is that you know we want to be able to use the same models and very very different kinds of environments we want to be able to use this an online production serving environment we also want to be able to use this offline in Hadoop we want to be able to run this in batch mode so we have these involve the requirements that you know even though these similar environments look very very different you know we want to be able to run the exact same models we want to be able to get the exact same answers regardless of where we run them now you know when you run these models over a relatively long period of times lots of things change so what what we try and do is we try and make sure that you know we're not just representing our models in a minimally possible way but in fact we're actually representing models in a very rich way which is which is a complete representation of everything that the modules requires so for example if you have a linear model a linear model is not just a array of coefficients but rather a linear model actually aren't models in this case it's the coefficients along with all the raw features that consumes along with all the transforms that get applied to the raw features along with any transform that you might actually apply to the output of the linear model along with documentation about how this model is produced what date ranges is what date what date ranges the training data so pretty much everything about the model is actually encoded in our model description so we actually have our own model specification language our models are actually not written directly in code but we specify them as configs which means that again you know it's it outlives our infrastructure and infrastructure going from C++ or from Java to C++ to whatever but in our models we have a specific domain-specific like a specification language and that specification language is independent of the language that we actually implement our infrastructure this config the other advantage in this configures that you know it's it's manipulatable by tools and so we can refactor these things we can actually find out for example for particular features actually being used in any model anywhere you know so when it comes time to deprecated certain features we can actually find out hey is this feature actually active in production or not again that's that becomes important because you know what model lifetime is long right when you're long the other thing is that you know related thing is that you actually need to minimize the dependencies in the environment so you can't have features which are implicitly encoding factors of the environment so everything that the model requires is completely specified in this language so you know we talked about machine learning but you know when you build machine learning systems there is much more to machine learning systems than just the machine learning the infrastructure is a critical piece and obviously because of the scaling but also infrastructure enables lots of things and you know because building these systems these high quality systems requires the people with different expertise to actually work together so for example you want to make sure that people who specialize in machine learning or specialize in building outranking in models they don't need a deep understanding of the infrastructure in order to make progress at the same time we want to make sure that our infrastructure experts can build and evolve in scaley systems and not necessarily be restricted by by our current models right so decoupling the infrastructure and the modelling pieces is hard but we found it to be extremely useful you know because now we can actually have people with very very different backgrounds working together kewal in the system it's it's hard because you know these kinds of systems you know you have to clearly define what the interfaces look like and it's it's something that you need to decide up front you know so organic evolution doesn't quite cut it so we have to go through this painful phase when we actually reorganize re architected our infrastructure in order to support this but you know it's it's turned out to be extremely useful now right so the last piece is that you know since the models actually run in very very different environments you know there is a tendency to believe that all these environments are equivalent but they are not then we actually launch this in production production has you know for example production and in our online serving infrastructure there are much more restriction or there are much more constraints on production serving infrastructure it has to deal with missing data you know an upstream service might actually you know data that it consumes from upstream services that might go missing so it has to deal with failures and upstream systems it's important to actually specify what the fallback behavior is in these various different failure modes which you don't necessarily have to do for example if you're just running it in R or on your desktop or even in Hadoop right so making sure that the behavior of this online model and this offline model is consistent also takes work and a lot of that has you know just investment in monitoring and measurement and debugging you know and so as many of the previous speakers are mentioned in a Pinterest we operate at a fairly high scale you know we already have hundred million users who visit the site no regular basis we work then on our way to our next hundred million users the set of users that we have is you know its vast its diverse and make and modeling these users you know because we are actually building out personalization systems modeling these is a big challenge simply because the diversity of our user base the other aspect of it is because the users are so diverse you know stuff that actually works for the majority does not necessarily work for a current minority you know as we started going beyond just a us-centric product as our product is more and more international you know it's more and more important that it actually works for different kinds of use cases right so the product has to work just as well for these nice populations as it does for your mainstream population you know you want to make sure that when you actually do your optimization when you run your experiments when you actually do your measurements you not only just measured aggregate effects across the entire population but you also have measured the effect on small subgroups right and so you know the experimentation the monitoring all of that stuff actually has to reflect these needs and it's not just about the user the same kind of same holds for your content you know so we have an amazing amount of content and obviously because the content is the corpus of content is so large you know the content tends to be very very diverse modeling this content well is critical and and making these trade-offs you know when we when we decide what we actually show the users you know we actually have to make sure that we actually show the users diverse and relevant content as well and so these are all challenges that we're currently working on and you know it's as a note mentioned you know this is this is an interesting problem you're working you're having impact on hundreds of millions of users and relatively small set of engineers working on these problems so if you're interested come talk to us come work with us 