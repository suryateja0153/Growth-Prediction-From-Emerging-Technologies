  JULIA FERRAIOLI: Hello, everyone. How's it going? Welcome to the machine learning session at GCP Next. Thanks for coming. This session is all about how you as developers can add elements of intelligence to your own applications. I like to think of it as a superpower, because the capabilities of machine learning far outstripped what any one of us can do on our own. My name is Julia Ferraioli. I'm a Developer Advocate for Google Cloud Platform. Back in the day, I did some machine learning research, but it's been awhile. I know just enough to be dangerous. Later, I'll be joined by David from Wix.com, who will talk about their experiences with Google Cloud platform's machine learning capabilities. Now I suspect that the machine learning plus Google story makes a lot of sense to many of you. It's right there in the mission statement, to organize the world's information. And information is just another way of saying data, and data is oxygen for machine learning. Anything that you do with your system can be considered data, whether it be logs, behavior, uploads, et cetera. And that data is something that you can get greater insight from. How you use that data and how you extract knowledge and understanding can determine the degree of your success. So what is machine learning? Who here is an expert in machine learning? Yeah, OK. There are a few of you. Excellent. I am not one of them. It's been the domain of the few. You needed a Ph.D. to dig into it, and you needed to dig into it to make use of it at all. But understanding the machine learning concepts doesn't require a Ph.D. You can think of it as taking data, running it through an algorithm, to yield some sort of insight. That's it. The more data you have, the better. And of course, you need some place to store the data and run the algorithm. So infrastructure is an integral part of a machine learning system. Over the years, we've had to solve new problems. And oftentimes, we've made use of machine learning techniques to do this. So let's take a brief look at how we use machine learning Google. I'm going to take two examples. Who here uses Google Photos? So not terribly long ago, we added the capability to search through your photos, not using when it was taken or where it was taken or anything that you can find in the EXIF data. But we added the capability to search within the photos themselves. So if you look for photos of flowers, you'll be able to get returned photos that contain flowers. And this comes from an application of machine learning that actually attempts to comprehend what is going on in the pixels themselves. What story does it tell? Google Translate is a great example of an application that's built almost entirely on machine learning. It's powered by an area of research called machine translation. And we build up a statistical language model so that lets us translate from one language to another. So you can get all of your translation questions answered, like how to say there are too many pictures of cats on the internet in Italian. I'm going to use that when I go to Italy, I'm sure. So what can we as developers do with machine learning? You're hearing a lot about it, especially yesterday and again since you're here today. Well I like to think of it as a spectrum. Now on the left hand side here, we have TensorFlow. Who here has played around with TensorFlow? A few of you. So TensorFlow is meant primarily for researchers, or folks in academia. It needs more background, more understanding of machine learning and machine learning techniques, and it means you're going to be writing more code. Now on the right hand side, we have the machine learning APIs. Now, these let you access fully trained models that we've developed as a service. It's meant for people building applications. Cloud machine learning, which is what you heard about in the keynote yesterday, is kind of the middle and it extends to either side. It lets you get hands-on building and training your own models using your own data. And since it's an integral part of Google Cloud Platform, it also gives you easy access to other cloud services, like storage and BigQuery and data flow. So let's take a brief look at TensorFlow to give you a little bit of a basis of understanding there. So it was released last year by Google, the folks at Google Research. And it's an open-source machine learning library. So right now, it's one of the most popular machine learning libraries on GitHub by forks and stars et cetera. And it's meant for numerical computation. What we're seeing people do is use it in-- the most part, for deep learning. And deep learning is an area of research that attempts to answer problems that we always thought you needed human intelligence to do. And oftentimes we do that using neural networks. So TensorFlow lets you do things like run recurrent neural networks or convolutional neural networks. So let's take a look at cloud machine learning. And we've got a pretty fun demo for you to today. So as I said before, it really does span the spectrum of our platform. And what does it do? Well first of all, think about your own data. Think about what answer's you know that your data holds, you're not quite sure how to get access to them. Well, using cloud machine learning, you can really extract that knowledge and understanding that I was talking about. You use it when you have a customized problem that you want to solve. It gives you tools for the entire process, from ingesting data to creating and training a model, and then predicting it's scale. Now, if you're familiar with Datalab, which is built on top of Jupiter, you can use it to get the rich integrated environment in training and managing your models. But what can it do? Right now, we have the capability to perform two primary tasks, regression and classification. But what does that mean? So let's say that you are running social media analytics. And for each piece of social media, you have a few things. You have the text, when it was published, a number of interactions, the responses, et cetera. Now, if you used all of those features to predict-- like an efficacy score, how effective was this piece of social media. That would be an example of a regression task. You use those features to get a value. But if you took those same features, and used it to tell you-- well the subject of that piece of social media, that would be an example of a classification task. So let's take a concrete example. So let's say, you were a biologist. And I say this with the caveat that I am not a biologist. And you have a bunch of images of viruses. And you want to automatically sort them into different classes. Now, you might be able to manually look at all of these images. And say, oh, yeah that's strain A and that's strain B. But it's kind of hard to develop the criteria for what makes strain a different from strain B. We're actually hampered by our own ability to pattern match. We can't analyze enough. This is where machine learning can really help us out. So let's use this scenario to dig into a demo that answers the question that I know everybody here asks themselves, almost every single day. "Can I Hug That?" This is the difficult, but squishy problem that we're going to be tackling today. "Can I Hug That?" is a system that takes an image, it processes it, and then tells you if you should hug it. So for instance, let's say we have this image of an adorable fluffy bunny. You say, "Absolutely, I want to hug that. Maybe not too hard, it looks rather fragile." So "Can I Hug That?" would say, "Go for it." But if instead, I gave it this image of a shattered light bulb. It would say, "Back away slowly. I'm preemptively calling 911. Just don't hug that one." So this is easy for us to understand, but it's actually quite difficult to do. If you thought about what makes something huggable, we would get into this conversation of the platonic ideal of a huggable object, there's no defining characteristic. So what are we going to do. Well, we're going to take images that people have found huggable or not huggable, and those are the labels. And we're going to use them to answer a question. Can I hug that? And get an answer. Hug or not. So what do you think? Is this going to be an example of a regression or a classification problem? Yeah. It's going to be a classification problem. We're going to train a classifier to answer this question for us. So let's go to a demo. I'm going to head back. We're going to use cloud machine learning to train our classifier. And then to use it to predict new images. Thank you for switching to the demo. So we have our example images. And I have them in my cloud storage bucket here. You can see, hug and not hug. And if I go into them, they are just good old jpegs. So I'll show you a couple of examples. So in my NOT_HUG category, I have an image of a cactus. I have hugged a cactus, I do not recommend it. But we also have things like clouds, in our category. If I could hug a cloud, I definitely would. So what we've done is we've taken all of these images, and we've transformed this data set into something that CloudML can process. And that turns it into a bunch of JSON files. So if I go into my data, you can see these are our JSON-test, and JSON-train. If we look at these JSON files, then we get something like this. It gives us the byte list of our images. And if we scroll down a bit, I'm just going to search because this is a very large text, for each image we also have a label that's coming from the HUG or NOT_HUG label. And here we have-- it's telling us, well, this particular one is value one. So of course, we go back and we say here's our dictionary, and at value one, we have NOT_HUG. So that particular byte list was for an image that was of category NOT_HUG. So once we have all of this data, we then can use it to actually train our classifier. So I'm going to switch to the command line, and I'm going to clear it again, so as not to give anything away. And we've got a simple Python script that lets us train. But before I go, I'm just going to show you a couple of environment variables that are necessary. So first, I've got a few directories to find-- where we're going to put the output, and a temporary holding directory on my local machine, and where our data actually exists, our input directory. And then we also have our model information. So we're using a model that we've created with CloudML, called gpc_next, and we're on version 4 of that model. So if we do python train. I'll just walk through what we have here. So first we have which model we're using, the model name. You can define a number of different models. The version that we're working with. Where our training files are. And where our test files are. Now when you're training a classifier or any system, if you do not test and validate the model, then it's not really going to be all that useful for you. So we're taking 160 images of each category to train the model. And then using some to test the model as well. We're also using this hyperparams file, which goes into what is actually happening with the system, and I'll show you that in just a minute. Then when we output, we'll output back to Google Cloud Storage in our output directory. And we'll tell it the project, and that we want to run it in the cloud. So while I kick that off, I'll show you the hyperparams file. So this tells us some information about the network that we're using to train. So we know what the learning rate is, how many labels we have, and how many lay hidden layers we have in our neural network-- that's 512. So our training job has been kicked off, and we can check on it using gcloud. And we've got a lot of jobs. You can see I was busy testing the system earlier. This is the one we just created, here. And it's currently running. We can check on it. And say, describe this one. So this one isn't quite done yet, training takes a tiny bit of time. So I'm going to use one that I've already trained on this exact same data set. I did this earlier today. Now what I've done is I've created a simple flask server that takes an image and then uses CloudML to give us a prediction. So let's take an image. I'm going to take an image of an octopus. We did not train on any octopus. And I'll show you what that looks like. This is that image. Now, if I click back that was actually a pretty fast prediction. So it says, "Don't hug this. This is not something you want to hug." But let's try something a little bit different. We're also we're going to use another octopus. But we're going to use one that is made out of the yarn, it's been crocheted. And it looks like this one. We did train the model on pillows. So what's the prediction? Do you think we're going to hug this or not? And yes. This is actually something you would hug. I find this really interesting, because it's been trained to neither an octopus or on stuffed animals. But it's been able to accurately predict whether you would want to hug this or not. So let's head back to the slides if I may. [APPLAUSE] OK. So that was CloudML. But maybe you're like me, and that actually kind of pushed the boundaries of your comfort zone. Let's take a look at easy API's that lets you add jolts of intelligence where you need them. So if you do know machine learning then you can use cloud machine learning. But if you don't, you can tap into models that Google has trained over extensive corporate data. You don't have to provide the training data, you don't have to provide-- to tune any parameters. It's fully trained and validated by Google. One of them has been in production for a good while now, and that's the Translate API. It lets you tap into those statistical language models that I was talking about earlier. And the new one, that you heard about yesterday is the Speech API. This lets you pass in just audio data and a language. And it will tell you what words it extracts from that audio data. You can get it all at once or as it comes in. The streaming mode is very-- will be very valuable for real-time systems. And the Vision API launched into general availability yesterday. But we actually launched it into beta just in February. You can use it to label images, identify landmarks, logos, do OCR. And what I find really cool is the sentiment analysis over images. We've been able to do that with text for a long time but images are a little bit harder, right. Sometimes I can't predict whether somebody's feeling angry or not. So all you have to do is throw it an encoded image or connect it to Google Cloud Storage, which is great if you want to do mass analysis over a bucket of images, say maybe our hug data. And it returns a label and a score. That label is what it sees. And that score is a number between 0 and 1, that indicates how certain it is that that label is correct. So now I'd like to introduce David Zuckerman, he's the Head of Developer Experience at Wix.com and he's going to talk about their experience with machine learning on Google Cloud Platform. [APPLAUSE] DAVID ZUCKERMAN: Thank you, Julia. Really excited to be here today to tell you a little bit about our story. How Wix went from not using machine learning at all, not knowing anything about machine learning, to doing something very, very powerful. So like most of you in the audience, when Julia asked, "Raise your hand if you're a machine learning expert." I did not raise my hand. Nor would most of the people that we have working for us today with Wix. When I think machine learning, I think AlphaGo, I think autonomous vehicle's, I think "Can I Hug That?" I don't think site design, which is what Wix does. For those of you who don't know Wix, we are one of the largest providers of web presence on the internet. We let people who don't know anything about technology build really compelling websites. I don't think machine learning here, right. This is not something that screams, "Wow, we're really advanced machine learning." Not that we don't do cool things, but it's not in the machine learning category. Unless when you saw our ad in the Super Bowl of "Kung Fu Panda", you thought, "Yeah, so that company-- that company really knows what they're doing around machine learning." So why am I my up here? Why am I on stage talking about something that obviously is not our forte? To really understand why I'm here, and why we're talking to you, let's talk about Wix's business model. This might come as a shock. I know we're a startup, but we actually have a business model. The idea is that we come in, you let anybody build a site. And we've let 80 million people build sites so far. And we want them eventually to pay us. They don't have to, we're a freemium model. But we'd like them to hopefully give us some money at some point for either e-commerce or hosting or one of our really cool applications. And something really interesting happens here, which is that as we're trying to drive them to convert-- the more we know about our user, the more information we have, the more we can figure out, the better the experience. But the more data we collect about our users, the lower our conversion rate is going to be. And this makes a lot of sense, right. If I'm designing a site. I've come in, I've added an image gallery, and I picked out every single image, and I really spend a lot of time with this. And then we popup dialogue that says, "Hi, what are you building? Tell me about your demographic. Are you building a music site?" It really turns off our users, and it drops in a lot of cases our conversion rates. So we're stuck in this interesting position where we want to know more about our users, but we don't want to ask. But we want to convert. But we can't ask. And so inevitably when we sit around, we always ask basically, wouldn't it be great to know more about our users without having to ask them. And there's a lot we can do in terms of historical data. We can infer and we have inferred that a lot of our users build e-commerce websites. Or a lot of our users are musicians, and would love a music experience. Or hotels-- but this is all historical. This is not in real-time. And it's things we have to infer. And in a lot of cases, it's a human or team looking at sites people have built to figure out what's going on. And so inevitably, whenever we're having this conversation, and I'll admit, even though I know almost nothing about machine learning, inevitably someone says, machine learning to the rescue. This is obviously going to solve all of our problems. If we can drive a car autonomously, we can definitely learn more about our users through machine learning. And then you get to this little asterisk that-- basically, does anyone at Wix know anything about machine learning? We don't. I'm very comfortable getting on stage telling you this, we don't know a single thing about machine learning. It's not our core competency. We have other core competencies. We have other things that we are exceptionally good at. We know a lot about scalability. We serve 35 terabytes of data every single day. We hit-- we handle over 2 billion HTTP requests. During the Super Bowl, when our ops team was basically really concerned, "Oh my god. The commercials going to air, what's going to happen." They ended up having to watch a football game instead of doing what they love, which is putting out fires. So scalability we get. We do a really great job here. User behavior, we have over 1,000 A/B tests running in production at any time. We analyze this data, we have giant Hadoop clusters that manage all this data to try to gauge what people are doing historically again. This is something we're very good at. Also HTML editing products, this is our core competency. We built four of them. The current one is probably the largest React install anywhere outside of Facebook or Instagram. And we've put a massive UX and usability investment into this. These are our core competencies, these are the things that we hire for, and these are the things that we're very good at. Machine learning is not in this list. We've got some really great people also. We have over 250 engineers. We've gotten to be relatively good size company. Over 40 product managers, over 30 analysts and data scientists, but not a single machine learning expert. We have people that dabble, we have people that know what it is, and know enough that we shouldn't get involved in it. But it's not just that we don't have the right people. It's also that machine learning has been, and you all know this, has been a very, very expensive proposition. You have to hire the right people, you have to hire these experts, the few people in the audience that raised their hands. You have to find them, you have to pay them. You have to put in the investment in the architecture in the infrastructure. And you have to have enough data, or hope that you have enough data, that you can train these models so that-- eventually get what you're looking for. And we're a start up. We can't necessarily pay the cost of spending x amount of money for all of these PhDs, plus the money and the investment for something that might not necessarily payoff. I mean machine learning is still in that category of science fiction. It's still something that seems out of the grasp of us mere mortals. So when we think about this, when we think about machine learning and how we could use it. We can go back and-- what is our oxygen, what fuels-- what could possibly fuel machine learning for us, even if we were to think about going down that path. And for us, it's images. We have over two petabytes of images that our users have uploaded to us over the course of our existence, that's a lot of data. And imagine if we could mine this data for something interesting, the possibilities here are basically endless. We could recommend similar photos from Bigstock, one of our stock photo providers. We could pre-build sites for target verticals. If we ask one question, "What are you building?" We could dynamically produce a website with the right images, and the web is very visual, and build the user a really compelling experience. We could make it easier for users to actually find images in the first place, which is definitely a pain point that we've seen. We could know what our existing users have built. And not know in the sense that we're inferring, but actually know what's in these images, and provide a better experience either in real-time or for future users. So all of this has been impossible. There's been no way that we would think about doing this. It had been. So enter the Vision API. We were lucky enough to get early access to these APIs. And we immediately sat around and thought, OK, we could do all of these really compelling things. But let's not get excited, let's see what we can actually do. And so we handed it off to one of our junior engineers. And said, OK here's the API documentation, you know the system, see what you can do. She came back like 15 minutes later, and we were expecting two or three days, and she came back 15 minutes later and showed off some pretty amazing things. To the point that even before the API was out of beta, even before the Vision API was released to the public, we started rolling it into production. So what do we do? All of the images that are uploaded to us, and these are over two million daily images, we extract labels for. Every single thing that our user uploads to Wix at this point, we extract. We've also taken all over existing user photos-- everything, all these two petabytes of data, including all of the stock photos that we have. And we've analyzed all of them. This is actually really convenient. We actually store our images on Google Cloud Storage, anyway. So we're on the Google backbone, we basically pointed Vision API at our bucket, and it went ahead and did all the other work. So that means that all of these images are now searchable by our users. A user can come in when they add a site-- and I'll show you this in a second-- actually find something. It sounds so obvious. Of course, image search. But image search, when you really think technically, what it entails, is near-- has always been impossible. So we've also added face detection as well. So we take-- for every image that's uploaded, we take out the faces. We extract that into EXIF data. And we are able to add things like face cropping all across our platform. Both in the Wix editor itself, and also in the media hosting platform we have called WixMP. So let's switch to the demo machine please. Can I get the switch please? Thank you. All right, so I wanted to build a quick little site to remember my experience GCP Next. And I'm not a designer. I know what looks good, but I cannot design it to save my life. So I did what most of our users do, which is to take a template. And you can see this image of someone doing yoga on a cliff, while very beautiful, doesn't really scream GCP Next, doesn't really scream San Francisco. So I'm going to go ahead and do what our users would do, which is change the image. And we get this media manager that comes up. You can see I've uploaded one image. It's not even remotely of San Francisco. And so let's put yourself in the mindset of a user, or any of you who are coming to do this, you want an image of San Francisco. You'll probably open another browser tab, you'll search for San Francisco, you'll find an image, you'll save it, you'll download it, you'll then reupload it to us. For a lot of our users, they're not technical. This might actually be something that trips them up to the point that they will not continue building a site. So now let's take advantage of image search. So I'm going to search for San Francisco. And I get some really interesting results here. I have a map that actually has a pin that says San Francisco. I have a cable car, I have the bridge, I have the Golden Gate. I have a hotel that's inside-- that's somewhere in the city. These are not just obvious things that say San Francisco somewhere. These are actually really well analyzed images. So now from the user's perspective, I can decide I really like this one. And I can go ahead and I can add it to my site. And immediately something that used to be incredibly hard for users just becomes easy, without them knowing anything about what we've done. Can we go back to the slides, please? So the next thing that I'm going to talk about and show you is WixMP. WixMP is Wix's media distribution platform. All of the imagery resizing, all the image distribution-- audio, video, everything that we do, and these are part of that 35 terabytes of data that we serve every day, is a service that we resell. It's a platform as a service, and it's actually running on GCP. Wix is the largest tenant on this platform. And this platform also gains labels and face detection. Why? Because we do. Because Wix does. Because we're just calling an API. And we're pushing this down to users of our platform as well. It's one of the powerful things about platforms and APIs, right. You can mix and match and you can easily give functionality to anybody down the chain without really having to do a lot of work. So I've shown you what it looks like, and you've heard us talk about it. But let me show you the code. Can we switch to the demo machine again, please? So let me ask a different question. Julie asked before, how many machine learning experts do we have in the audience. How many of you can call a REST API? OK, kind of thought that would be most of the audience. So here is our from production code that does all of the dirty work. And you'll notice that, as I walk through this, we have about five lines of code that are actually calling the Vision API, and the rest of it is basically Python scaffolding. So we set the API. It's very simple. It's a-- if you want images, give it our key, which sorry, I'm not going to show our key here. We go ahead and we pass in an image URL. And from that URL or the image data, we call a built-in Base64 library. And we encode the image to Base64. Then we get to the really challenging part, which is constructing a JSON request. That tells us which information we actually want to extract from this image. So we decided we want face detection, label detection, landmark detection, and logo detection. So we're going all out here, basically. And in this JSON, we also put the Base64 encoded image data. So this is preparing the request. And you can see that when we actually call execute, all we're doing is using an HTTP library, and executing and waiting for the response, that's it. There's no rocket science here, there's no advanced computer science here, it's calling a REST API. And it was that simple to gain huge benefits, things that we could not do otherwise. So I want to show you one other thing here. So this is a node library for WixMP. WixMP again, is our immediate distribution platform. And it's a simple node API, and all it does is it uploads a couple images. So I wanted to try something interesting. So I took this image of San Francisco in 2005, it's been here for a long time. There's no EXIF data, there's no GPS data, there's nothing encoded in here. I wanted to see if the Image API was going to recognize San Francisco, even 2005-- 11 years ago. And Julia and I, before this, we took a selfie. Because, of course, you're giving a presentation, you have to take a selfie. So what we're going to do is we're going to upload both of these images to the API. And as you can see, it's about as easy as possible. Upload, pass in the name of the file. So let's kick off both of these. Upload SF and upload the selfie. And in honor of Julia's should I hug this or not, I have a little custom loader that's going between bunnies and cacti. I'm sure the classifier would have a field day with this. But so what is this doing right now? So this is uploading two images to Wix, to WixMP. It's taking them and then WixMP, while it's persisting things to cloud storage and while it's extracting and copying, it's also going ahead and it's sending it off to Google to tell us what's actually in this image. And let's take a look at our from 2005 image. Water, sea, sure. Landform, bridge, waterway, Oakland Bay Bridge, San Francisco Oakland Bay Bridge, and San Francisco. It's really fascinating to me that, OK, sure. You know, a human might look at this and say, oh yeah, it's a bridge. It's just the Bay Bridge. But because you have this giant trained model in the cloud, it also realizes water, sea, landform, and it picks out a little bit of San Francisco. If you didn't know that that was the Bay Bridge, you'd have no way of knowing that that was actually San Francisco. And then obviously, for labels, it's good to see that the machine thinks Julia and I are people. And you can see that we've got a couple of faces from the API. And if we go to the Wix Media Platform console, you can see that we can go back and forth between turning on the face detection or not. Face detection is pretty good. Also takes our tag here. If I could go back to the slides, please. So we basically, we did the impossible. We did-- or what would have been impossible even two or three years ago. We harnessed machine learning, without really knowing anything about machine learning. We now know more about our users, without asking them anything. This is the holy grail for this kind of work, for consumer facing products. And now that we've gotten our feet wet. Now that we realize that the Vision API is within our grasp, and other APIs are within our grasp, and now that we know that we have enough data, and now that we know that machine learning actually pays off, we're looking at CloudML as well. I'd love to put the "Can I Hug This or Not" as a hidden feature in Wix. But we're looking at different ways that we can take advantage of this. We could see which layouts people like, and start auto laying out sites. There's so much potential when you harness machine learning. And the great thing about this is it's the democratization of machine learning. It's bringing it down to everybody. Either from the API side, if you can call a REST API, you can use this. Or the CloudML side, that you can know a little bit more without having to be a researcher. It's really amazing. And with that, I'd love to hand it back off to Julia. And she will walk you through the rest of the cool stuff. [APPLAUSE] JULIA FERRAIOLI: Thank you, David. You're stealing my talking points, though. We're going to talk about that later. So you really heard so much about machine learning yesterday and today. And we have these easy APIs that you do everything from understanding language to understanding vision and understanding audio. We're kind of covering the range here. And like David said, all you need to do is be able to make an API call. It's as simple as it can get here. And this is really the year of machine learning. We're seeing an uptick in news stories. We're seeing an uptick in how people are making intelligent systems. I'm always impressed with what's going on in the industry right now. And our machine learning platform here, is really just the first step that we're taking. So what you saw today, you saw the easy APIs that you don't really need to invest a lot of time or education to be able to use. To cloud machine learning that lets you solve custom problems. Sophisticated or as simple as making informed decisions about whether to hug your dog. Hopefully you hug your dog. And to TensorFlow, for people doing innovative deep learning research. Now I said before that this is a spectrum. But you can decide where you want to fall along the spectrum. You can self select. But even then, it's not a final decision. If you're like David and Wix.com and you say OK, I'm starting with the machine learning APIs. I know they can add value for me and my use case. Now it's time to invest a little bit more time and effort, hire the data scientist. You can upgrade or move to cloud machine learning to get more and deeper insight out of your data. Or if you're working in research today, and you've been working with TensorFlow, you can use the graphs-- the custom graphs that you've built with TensorFlow as models on cloud machine learning. You can productionize your research. So it's fluid, you can move between where you want to follow here. So with that, I'm going to leave you with some resources to get you started. All of the cloud machine learning and the APIs are hosted off of cloud.google.com. This is where the cameras come out. And TensorFlow is on tensorflow.org as well as GitHub. You can see an example of image classification on TensorFlow, with this TensorFlow for poets link. And the images that we've used today are all Creative Commons and they're linked off of the gist we showed you earlier. So David and I will be in the playground. The Emoto booth is actually using the Google Cloud Vision API in production right now. So if you want to chat with us, please feel free to meet us there or online at any of our social channels. So that's what we've got for you today. And we hope to hear from you and what you're doing with machine learning. Thank you so much. [APPLAUSE] 