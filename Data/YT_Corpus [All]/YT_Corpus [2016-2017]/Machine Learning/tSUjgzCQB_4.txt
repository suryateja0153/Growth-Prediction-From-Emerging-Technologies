 so my job here is to tell you about simple machine learning quick caveat I'm not an expert and that's why this should hopefully be a bit more interesting so who am i I'm a senior software engineer at DataSift I look after the web application for controlling everything the public API and a bunch of internal services prior to that I was at Time Out magazine and working on crm systems for secured loans companies before that industry fell through I'm also the founder and co-organizer of PHP Berkshire okay another co-organizer there and yeah if you want to head out to reading then we're more than happy to to welcome you all the way all those 20 long minutes away from Paddington I also act for the record it's not group therapy it's not individual therapy it's me being a dog I do have the hair for it so so yes so the challenge I call this Hawk simple machine learning because machine learning itself at its basic is actually quite straightforward there's a bit of math in there when you get to some of the classification stuff there's a bit of probability theory and stuff that we can just say well here's an equation let's chuck stuff in and forget about it but in essence it's a very straightforward process but in order to kind of apply this challenge I've got to have a case study and the one that kind of kicked me off in thinking about this now and in PHP and so on was a blog post by Larry Gura field in August when he looked at the the number of new speakers at PHP conferences throughout well since 2011 I believe he basically grabbed a bunch of data which I'll show on a sec the joined in API he used he grabbed all the events all the top and he used that given that it's vastly PHP biased because it's written in PHP it's promoted by people from PHP and we all love our own he did an analysis of first time PHP speakers across all these all these conferences and he came up with a number of about fifty fifty one percent new speakers I've run that code again in the past well past few hours and that's actually running now about forty-eight forty-nine percent so still doing pretty well but one of the things that joined in is it's not just PHP focused it's built in PHP as I said it's promoted at PHP events but there's some rather large conferences in there there's that don't say anything about PHP there are there's javascript stuff in there c plus plus there are away days for people like in victor in vika and so on so i'm a lot of that data is actually not quite focused on PHP in the way that that core assumptions made so there's a tagging system in enjoying it and you can say this talk is about or this event is about PHP this one's about JavaScript and so on but we have a bit of a problem we have only out of a thousand 84 event we only have four hundred and nine of those are tagged the other the majority are untagged so we can't just go and run this analysis by looking at the tags and seeing everything that's tagged with PHP and improving our results in that way so so we got to look at how do we make up for the 675 on tags and then going forward unless we start enforcing tags on joined in how do new events start getting tags and how do we kind of keep classifying things as we go through so there are a few solutions number one the most boring one is to go through 1,100 talks by hand and tag them individually if you estimate one a minute you can do it a bit faster but if you estimate a one a minute that's 11 hours of work you might find typos or have typos and your tags you might not apply your criteria consistently and as such you're into a bit of error there then once you finish classifying everything that's great but 20 new conferences or 15 new user groups have sprung up in the time it's taken you to tag those events so that solution isn't exactly one that goes forward and it's not necessarily one that really scales no one really wants to be there going right so I've got to spend an hour tonight tagging over linkedin or the joined in event it's not fun it's not cool it's not an appropriate use of our time perhaps we have mechanical turk I hands up if you've heard of Mechanical Turk that's about half of you so mechanical turk is a system kind of a system that's put forward by Amazon where you put in kind of micro jobs you say here's some background here's the output that I want from that it distributes it to real humans and then they do your work for you in little bit so it's quite scalable it's done by humans and it's applying that kind of the human eye that we'd like but again you have to pay for such a service it can get expensive if you need to get a lot of a lot of people contributing and you need to get a lot of stuff checked as you go through it is fun it is cool but again expensive and you're relying on others and their interpretation it could just land in the lap of someone who has no idea what PHP is and couldn't tell you the symphony talk for example is about PHP of its core so our third solution and the one that you're here for is machine learning yay it's fast well it's fast when you apply it as we'll see in a bit it's consistent so once you've got your model it can be applied time and time again and you'll get the same results automated and it is fun and cool there we go that's our selection criteria for our solution so before we pick up on machine learning and what that is and how that works let's have a think about how we as humans learn because ultimately what we're trying to do is come up with a model that models the way we as humans look at the the input data and and how we can understand and interpret it interpret that and apply our learnings as such so how do we as humans learn pattern recognition is a huge part of what goes on up here and not just this one ball yours we spot patterns we see cause and effect we associate well this happened then this happened so if we see that enough times we make that link that that cause cause that effect and correlation in that in our brain anyway is causation and the mere fact that we know that that's not the case and that's being propagated correlation is not causation is quite you know it's the phrase that's because we have to subvert that innate way that we think and the way that we make those connections for example pigeons you can make them superstitious if you feed them every time they turn 360 when they're hungry they'll turn 360 just to get the food if you do that enough times and that at the basis is the way that we learn we also look at past experience we build a mental model of what we're how we experience the world and the people and the places and the things we encounter and we create stereotypes and again we were talking about our hearing about biases in the in the keynotes and some of that is learned from our experience and we might have some bias from our own experience and some stereotypes from our own experience but that's the way that we deal with the world in a general sense in our kind of type one immediate sense and one of the examples of this pattern recognition is something called baader-meinhof not the terrorist gang but another kind of another way of looking at things it's basically where you learn a new word or you encounter a new brands or you shop at a new shop you've never heard of it before it's never been important to you before suddenly it's become important to you and you're you're very aware of it and you step out of the shop with your bag that's got the name of the shop on it and you go I'm really cool it's the first time I've seen this new shop and you look around everyone's got bags with the name of the shop on you learn a new word and suddenly you see that word everywhere and you wonder why haven't I fit encountered that before and that's because we attach importance to the new things that we've encountered the other way or another way that we learn is through shared experience and teaching so being instructed about things being told that this is the way that the world works being told this causes that and getting that information from an authoritative source it gives you that gateway into the ideas and because it comes from a teacher you know that well you hope assuming you're in a good educational system that there's Authority there and that what they're telling you is correct and from all that stuff you've learned and picked up on you can apply that model that you've been taught to the rest of the world because you've got the authority you then have accuracy we hope and also because you're doing it in conjunction with a teacher you have those opportunities to fail and to be corrected and that kind of feedback loop helps us to learn and to refine our model as we go through so machine learning it's not as scary as it sounds some of you might not think it's scary but it's straightforward it's a chunk of break down a problem figure out how to build a model of it and how to apply that model to get the output that you're after the things we're going to talk about aren't going to be as advanced as neural nets mainly because I don't know that stuff I'm going to pick through a one example of how we build classifiers and how we apply stuff but one of the things that I realized when I started tackling this is actually I've been doing this on and off in various guises for years I've been looking at how often things pop up I look at the correlation between some some items and then I can make a prediction of the back of it so I was doing this it's a big buzzword now but I was doing this without calling it as such 12 years ago trying to do image recognition at the University in that project that I don't understand anymore but that kind of the fact that it's been around well pretty much as an idea since the 60s and 70s it's only just becoming scalable and practical as an approach that we can pick up there are a couple of different types of machine learning there is unsupervised learning and supervised learning so unsupervised is where you just say here's all the input here all your documents all your events from joined in go off and figure out something just leave it to its own devices and the way that kind of works is it you might think take all the other words that come in look at the similar words that come up and group all the events under similar words then pass it off to a human to label what those clusters are find similarities between documents and grunt lump them together and then present those or find inherent connections between those clusters and present that is kind of a network diagram of all the events there it does help uncover hidden structure one of the cool ones that has been described in the past few years and is now being done at scale is called word tyvek and effectively what that ends up doing is you give it a load of words a load of English words if you're like loads of text it starts figuring out the relationship between individual words and it's above me and over my head but what you get at the end of it is this kind of spatial model of words in our language and it inherently without actually knowing that male and female exists it can start to cluster the words around there and start to I say gender words so it can connect female to Queen and male to King and royal to king and queen and then you can do some fun stuff like give me all the words that are royal minus mail and it would give you queen of the back of that like I said over my head so that's my simplistic understanding of it but that's a good example when you come across a of unsupervised learning supervised learning is where you actually do that teaching you say here all the documents but here's a set of documents I've done for you here's a set that I've actually pre labeled in our joint in example we've already got some events that are tagged as I said before so we can think of that as being our source of authority and good set of labeling we build our model from those known accurate knowns to borrow a phrase from Donald Rumsfeld and we take that model that we build from it we refer to that and we take all of our assumptions from that and we apply that to new documents that come in new things that we come in and take that stereotype that we built and apply it to the new stuff and it's important to know that it's the fact that we've attached metadata in those labels in those tags to our documents that's a supervised part you can have interactive and non-interactive supervised learning and the important thing is that you've told about some of the relationships already and the kind of relationships you want to bring out there are a few types of output that we can look at from our machine learning so we have regression which is basically line of best fit so you you plug a load of data in you say here are my independent variables or are they are they dependent and figure out exactly what the relationship is without understanding that upfront so if you know that the temperature of a room goes up when a lot of people are in and you have a number of people and you have the data that you could correlate the number of people in the rumors that the ambient temperature in the room then you could build a model via regression linear regression to actually predict the the temperature of the room given the number of people come in without actually doing that working out for you it's a very simple example but if you imagine extending that to a lot of different variables where you'd be there all day by hand plugging all this stuff in and looking for those different pieces machine learning that does regression is one way of figuring all that stuff out and it's used for those continuous values as i talked about temperature but about anything on a spectrum basically where it's not discreet this is a this this is of that and so on it here's a predicted output value then this classification which is what we were looking at tagging so you say these sets of documents belong to this class this set of documents belong to this class and as a result when I give a new document which classes are belong to you can think of it as which bit in the library do I put this book into what's the Dewey Decimal Classification number of that or what mammal is this based on the attributes of that and a mammal so it requires those known labels it requires that teaching upfront that supervised style to actually come up with this classification that that's certainly would that we're after so our approach given that we have a bunch of documents given that we have existing tags that we have the output that we want to get new tags and improve the ones that don't have tags at all we're going to go for a supervised classification we're going to use the joint in tags that are already there and then I'll do a demo we're actually using the interactive tagging where I'll classify stuff on stage and hit go and we'll wait and we'll see what the results are i'm going to show you what's called in very long words multivariate naive bayesian classification which is less scary than it sounds especially when you just take it as read that one of the calculations is good what that means is you've got lots of variables coming in and then naive part is the fact that we make some assumptions to reduce the problem someone said to me yesterday that classify classifier is a machine learning they're all solving the same problem some of them just do it more efficiently than others and it's all about reducing the problem space will also have a very quick look or an example of a Bernoulli multivariate new nave I can't even say at this time in the morning Bernoulli multivariate naive bayesian classifier kind of worked which is a similar kind of approach but it takes information about when something isn't there alongside when it is sort of steps in machine learning I vaguely touched on this before but they're really straightforward you've got a couple of phases you have the training side which is feature extraction and then you're modeling and then the prediction which is taking the same features outs which I'll talk about in a sec and applying that model so once you've got the model built which is the tough bit the actual application of it it's a pretty straightforward affair so training the supervised learning part we're we're actually plugging in those documents and plugging in those labels as we go through we start off with feature extraction now one of the things about machines about computers is there's no inherent understanding of any of the stuff we took at it they're just ASCII characters is the bytes underneath and so on we've got to find a way of describing a document in a in an objective sense rather than saying it rather than just kind of saying others about PHP we need to actually say well it's got the word PHP in it if you like so we boil a document an image or any set of data down into numbers and we do that in a consistent fashion so that we have this multi-dimensional concept of what a document is take it to the numbers and then the computer can work with those add those together multiply whatever we want to do with them but the important thing is that it's consistent and that once we've applied to that one of our documents we're going to take those same descriptions those same features that we're extracting and apply them across all our documents so that we put them into the same kind of space and we normalize them if you like there are a few well there are many approaches basically anything that you can apply to a set number of documents that turns into a number you can make it a feature extractor so term frequency how often does this word appear how often does that would appear and so on in my line of text we've got a fun one term frequency by inverse document frequency which is basically how important is a words to the document you're looking at verses all the documents so this kind of house search engine starts it off I don't know if Google every used it but i'm pretty sure altavista started off using it basically you know that the word the is really common so you need to the fact that it appears in all your documents while the fact that it appears really often in the document might make you think naively that it's important the fact that it years in all the documents a lot needs to be ruled out so in that respect we can actually bubble up important keywords as we come out n grams which basically one two three words in a row so it might be that instead of saying if a specific word appears we want to see if two words appear next to each other or a three so by grams and trigrams we can infer well well we can construct something so if we know if we're classifying people and we know their date of birth and we know that the date is the 18th of februari 2016 then we can in calculate their age and that can be a number that comes out basically as I say anything that creates a number and what we're trying to do is look at the data that's come in and figure out where those numbers and those dimensions correlate with the with the tag that we've we've come up with so I mentioned about thinking about things in multi-dimensional space 3d 4d 5d ND the the kind of cool thing about doing this is you have a lot of side effects you can think about it in a really interesting sense anyway a document might be over here in 3d space another document might be over here and the very fact that they're quite distant means they're not similar so we can use what's called cosine similarity to take two documents that we described with these feature vectors these collections of feature features that we've extracted and we can look at the two of them and say how similar they are because we've modeled them an n-dimensional space done a bit of Pythagoras and figured out that they're far apart they're not similar or they're very very similar they use very similar words so we can start thinking about things in a spatial sense I bring that up only because it's really cool and it's way that we can take the inputs to machine learning and figure out relevancy as we go through the other thing is that we can use the scary matrix multiplication stuff again I've forgotten since my GCSEs and a-levels but i will remember one day so I talked about feature extraction the the text on the the right might be a bit small for some of you at the back but effectively we step through each part of our input document and this example this talk we can take out some of the common words that's called stop words and removing those and from and have and 2 and so on we can take the words themselves and reduce some of the problems that even further by doing something called stemming which is where you take you do your conjugation if anyone ever did Latin you do your conjugation on the words and you come up with a common root of each word so apply and applying boil down to the same kind of token if you like and then we kind of count them as we go through so we build up a list of features event title at PHP for example we've got one up there and so on and we build that as we go through so so we come up with a simple feature vector have I lost the mic nope so so that's kind of an example of what we've done there what we can actually do is again we can take two words we can go with reading everything as the by Graham example and that becomes another measure that we're going through I seem to have copy to slide there that's wonderful so what we do after that point is how it done that no there we go so with the supervised learning we have what we call a training set so we have that information where we've got all these documents that are already tagged we have these known knowns and that model that we're going to build from and we start to make the assumption and this is where some of the naive element of that that bayesian classifier comes in we start to make the assumption that the tags that are in our training set appear just as often in there as they would in the whole of our set so if the word PHP appears in three quarters of all the documents on joined in then in three quarters of the new ones will find that word appearing three-quarters of the times again so we we take that assumption and we start counting them as I've just shown there and eventually we'll take the number of documents they'd appear they've appeared in and divide that through by the number of documents we have as a whole and figure out there there the number of times that it appears across the whole document so in that learning stage we extract those features we go through those step for all of our things there we take those tags that we know already and we build that correlation so we're counting this tag appears when this word appears and doesn't appear when this word appears and so on so again many methods of probability but if we're assuming that our training set is representative of all documents that can never be put into joined in then we can say that the probability that a tag predicts a feature so the probability that the word PHP as a tag predicts that the word PHP appears in the document that's a number of items with PHP in the document and PHP is a tag divided by the number of items with PHP in the whole thing it might be a little early in the morning it's almost afternoon but you know it's pretty straightforward a better example because that's pretty obvious might be the probability that the tag PHP indicates that symphony is in that document is the number of items with PHP as a tag and symphony in the document divided by the number of items with symphony in the document we can also write that as the probability that feature given the tag occurs is that so I'm going to use some of that note some of that notation as we go through some of the equations that I'm going to say just trust me on this one and encode again and might be a bit small for you at the back but effectively what we're doing is we're looping through the number of things that we've that we've identified in there we we have think we've for each of the tags that we found we built up the number of the number of features that occur against that tag so that's in our feature list as we go through there and we're just there doing this division with a bit of cheating so see how we've got our list we're looping through each one of those we're filling it out so that we have zeros at the point of which feature hasn't occurred so that even though that word in the current document it occurred somewhere in all our documents so that we're filling it in with zero we do what we call what's called Laplace smoothing which is basically make sure that we never get a zero by adding one to the the item on the top then we do a bit of cheating to make up for the fact that we've just massively biased our numbers and in reality i could find probably a lot better ways of doing that mathematically but again it's beyond me so so just adding that kind of tweak there is what we're doing is cheating and effectively what we're doing is we're setting ourselves up for the application of Bayes theorem which is the probability that a tag is predicted by a feature so that the tag PHP is predicted by the occurrence of the word symphony in our document is equal to the probability of the tag appearing across all our documents times that thing we worked out a second ago the probability that a feature is predicted by a tag divided by the probability of that feature existing in all our documents you don't have to remember that I've said it I've won the points that's good now this is Bayes theorem I've missed out the multivariable I wrote a really nice blog post that explains it and that's what you get so the probability that a tag is predicted by features one two and three is the probability of that tag appearing times the probability that the feature is predicted by tag featured predicted and so on / that was a genuine yawn yes and you can find that basically all the steps as I was working out I wrote it down and it might make absolutely no sense to anyone but you can see I've at least on the working what we can do is we can identify bits this is the prior so this is the number of documents tagged with that tag / all the documents we've got there remember I was talking about we're going to apply the the assumption that the tags appearing in our training sets are equally distributed against our tags everywhere the number at the bottom which is the probability that all of these three features exist in the same document that's going to be fixed that's just a number that's not going to change whatever tag that we're applying this to so we can kind of cheat that again there and like I said classifiers are about reducing complexity to try and make things a bit faster in a bit more applicable so we just replace that with a Z we can just mess about with the numbers as we took that in we can assume it's one that they're always going to be there or we can find a nice balance that bring stuff out at the end of the day it's just like removing a constant and frequently that's combined with the prior because again that one's fixed every we were going through or encode again I said simple machine learning I think the first slide was of code was about 10 12 lines this one's again about eight it doesn't do anything very much special but again we're calculating our prior at this point so the number of times the label occurs versus the number of documents in the whole training set we then going through do we have that feature in in the model as we're doing this prediction stage and the if the answer is yes then what we do is we actually just grab that probability that we've calculated previously how often does the feature occur for the tag and then we just multiply it together so again probability the tag is predicted by feature and like I said we then multiply the problem with multiplication and and all these things is we're calculating for big numbers of documents we're calculating really small numbers which is great for a probability when you're doing it once but as soon as you start multiplying it gets worse and worse and worse and smaller and smaller and smaller and dashi 27 and 80 and blah so if we actually switch using log we can turn that into addition not lose out on any of the numbers it's just a different bit of transformation that we can do there and we'll see a bit later that the numbers are very different they also end up very negative again beyond me oh dear so I'd love to build this up and say it's spectacular it's wonderful it's the best thing you'll see all day it might be the best thing you've seen since the keynote I'll go with that because it certainly doesn't beat that uh-huh so here's a little little web app that basically takes everything I've shown you model to all in the background for all those thousands and forty eight events and all those huge numbers of tags 350 something like that builds a model then applies the classifier and then just gives you the probability that a given tag is right for for the event so what we've done is taken the title of the event and the body we've broken it down this is a bit of a better one dumb code we broken it down into the features that we have there I lied namespace them just so that I could use more than the engrams that are saying in the past we then do all that multiplication together we apply all those probabilities we've worked out in our model and we work out that that's something that this is PHP is a really small number the important thing is because that we're multiplying for each word in the document effectively or each unique word in the document that number is going to be relative to the whole thing so we're looking for big differences so we can see there's a huge level of difference between DOM code being about PHP which it it mentions PHP we have a result versus it being about the web is quite high but you can see how we're actually picking up some of the ideas we've seen some correlation about symphony and Drupal as we go through I was going to go through and figure out what a really good page to show you on was Madison PHP is thirteen percent likely to be about PHP which is a result and it's a consistent result look so effectively that's what we're doing we're taking those tags that we already have existing we're doing all the multiplication together I mentioned it was about 350 tags this is only done over 20 because as I'll show you a little bit later there are some slight memory issues in doing this kind of thing and again that's I've reduced the problem by chopping out 330 of the tags for this model I mentioned about using logarithms so instead we can have actual numbers just shifts the problem into something that we can deal with a bit better and so on as we go through now this model when I built it talk about 23 seconds I'm now going to switch it to using Engram by grams I'm now going to hit build the model and then we're going to wait it's going to fail I'm going to have waffle for 23 seconds for no good reason I'm going to cry and then switch to another demo just to show you some of the interactive kind of ideas as we go through I think we still got about 15 seconds left on this so I'll keep going one of the things I found in through doing this is I'm not necessarily the best of it yet I'm getting better yay memory issue let's go back to where we were we can't lovely like I said it failed and this is an important lesson don't go off-piste when you're doing a talk skip to the next demo one of the things I didn't talk about there actually was the fact that we've got the multivariate naive Bayes which is doing all that multiplication together but that's basically saying if a word exists there what we do is actually multiply when we encounter it Bernoulli that I mentioned earlier actually models for all the words that you've got and if it works out the coral not only of when the tag exists and a word exists but when it tags doesn't exist and the word exists and tries to figure out where you have the probability that something doesn't predict and the probability that something does predict as we go through and merge that together it's a fairly straightforward process again I'm going to give you a link to the code later so you can pick through that as we go through this one is more interactive and this shows kind of the interactive supervised learning side of things so this is halfway house to that machine that human tagging I was talking about being boring as we go through this is how I know it's not quite a minute to do each one because that's about PHP that's about PHP that's about PHP are you giving a scene that one isn't excellence and so on as we go through we can then submit that through we can build our model as we go through this one's just working down on the actual titles as we go through just to speed up for the demo and so on we can see that using the Bernoulli side of things we're saying it's either PHP or not PHP and we can see that because we have an interesting mix of languages in here and place names we can see that Friday apparently is a hundred percent correlated with PHP whereas a jas days yay is positively correlated with not PHP um there are a lot more tags as we go through here but this just kind of gives a flavor of what we're building up in the background I've constructed a confidence score which is relatively random because we're taking eighty percent of our knowns we're building our model from that and as a result of that were then correlated checking that against our twenty percent left over of knowns to actually figure out how well we're doing if I had a bit more time in preparing this I might have actually created a feedback loop that you could use that confidence score how many times did I get it right against what I know versus how much how many times I didn't as a kind of a fitness function to help say that this is a good model and this is a bad model and so on and start tweaking the inputs some of those numbers the cheating numbers some of the weights if you like so that we can start building up a more complex picture but for the purposes of this we can see where our successes of failures are including in CSS and where we're getting stuff right and totally wrong as we go through so so that was the scary and didn't quite work demo as we go through all that codes is gone now on the floor all of that codes is going to be up I'll give a link to a later especially after i finished the hacking that'll make it look good so you can follow that through so given that I've now demonstrated a bit failure having been so positive going on the way through we had a bit of human learning to do as I went through this process and i talked about how i learned or how we learn as humans but this experience is fed into that their cause and effect side of things so lessons I've learned bayes theorem is annoying just trust that it works because at the end of the day it's all about multiplication division it's just those kind of figure out the numbers plug it all in let it fly do it stuff joined in it turns out that Larry Garfield was very correct in considering it to be a fairly accurate proxy for PHP conferences because it's very heavily PHP biased we're I've built something that pretty much labels everything is PHP it's not that's uncharitable it's because at the end of the day most of the data coming in is about PHP we've made that assumption that it's representative of what's actually out there in the wild and so if we've got a lot of prevalence of PHP in our training set that's going to be reflected in our output as we go through a lot of the tags are noisy I could have stepped through and shown you that the members of so flow PHP really love to tag themselves with the so flow PHP tag as do PHP Hampshire PHP hants and as a results because it appears quite a lot in our training set we start thinking that things in Paris are in southern Florida again that's not something that we can create correct overnight with the data coming in but it gives us an opportunity to actually figure out what features might better to describe stuff and kind of quilts that noise as we go through all this approach were building up lots of grids lots of numbers lots of multi-dimensional arrays that takes a lot of memory 350 tags which is what I was trying to calculate this with when I started by about 1300 13,000 sorry features 76 bytes to put one item in a PHP array that's lots of memory that's 300 and something megabytes worth of memory and that's only going to get worse as you add more names so one of the things that we're learning by the hard way is that we need to actually be a bit more discerning are we building classifiers for one tag are we only going to pick out the actual signal features so the ones that appear lots versus some of the ones that don't appear much at all or the ones that that have high or very low correlation and ignore the ones that are middling and so on to reduce that problem like I said classification is actually the difference between classifiers ends up being how you can reduce that problem set so you can fit it in memory or you can fit it into the constraints that you already have so my next steps as I said I need to improve on my feature selection I need to actually start picking out bits from that data set that that are actually indicative of signal and stuff that I can combine in the better way and improve that confidence score that I've shown you there to implement different types of classifiers so I can't just stand up here and say to you bernoulli multivariate naive bayesian classifiers and go on a bit more and talk a bit better on that you build the model into joined in model even though I now lost the numbers was applying itself very quickly it was applying itself in about 30 milliseconds once we've actually loaded up passing a new document it would take 30 milliseconds and you've got a tag or a set of tags out of the back of it so all the pain is in that creating the model maybe in some of the loading of the model but when it's applied it's really quick so if we found some way of applying that as you submit your documents that would be a lot quite useful hopefully in the future classification of joined in talks and again improve the performance actually figure out how matrices work actually add caching and do stuff that's a bit more rigorous than I happen in spite of this one of the things that did come out having applied having applied all these classifiers and given it a shot is this is the original output well I said the original id's it was calculated this morning it's the original form and the original methods that Larry Garfield came up this is where I was picking out the fact that we have j/s day popping up with 24 talks and fifty-seven percent new speakers you've got I'm not entirely sure what that conference is there but it's 98 talks and a hundred percent new speakers that's quite a lot of noise and that's the problem i was starting to look at and you can see down the bottom here the overall number currently as of this morning is about forty nine percent of new speakers in our proxy PHP side of things if we actually look at something I've classified which has taken out some of those some of that noise it's left in a few of them but it's taken out some of it and we actually see that our results starts to shrink down to forty four percent and when I actually get the classifier is right I'll publish the numbers there and feed that back and we can actually look at how we've we've gone through and improved that diversity of our new time speakers our first time speakers so you can find I promised the code I'm creating a library called the enamel which has currently multivariate and Bernoulli multivariate stuff in there and some of the the ways in which you can actually feed your own documents in and you're in tags and so on they joined in audit stuff created as I said bye Larry Garfield I've made some alterations to provide that web hadn't to bring in enamel and so on into it but effectively you can go to those places and check my working and check that the figures actually match what I've actually shown you not doing today so with that thank you very much and have I got any questions Billy similar things in elastic search and generate the same slot of statistics using aggregates I was wondering how your code compares to something like elastic search which has been like around for a little while how does it perform in comparison you get the same sort results um I assume he'd get the same sort of results he just would have something that's a hell of a lot less performance so this is more an exercise in not in making a fast clip fast thing but actually implementing myself I'm putting it forward yes there are a hell of a lot better stuff out there there's scikit-learn in in Python which is the industry standard for the coding side of things you can get better storage and this is backed with my sequel you can get all that kind of thing there in a way that PHP in the way that over it now anyway is a love a lot less performant about but this is an example of showing that hopefully anyone can do it because it's so simple just questioned by the interpretation of that thirteen percent that came up earlier that given something is tagged as PHP there's a thirteen percent chance that it actually is about PHP and so it's more that was the cumulative probability and because we're multiplying in that example without correct thing for it we're multiplying the fact that it's not just got PHP in there but it's got all these other words together that are existing in other documents that reduces that number down even further so the way the numbers represented is more about the comparison between the different tags themselves for that document rather than as a whole so it's reduced by the fact that I know you had a common word that showed up in JavaScript say talks in different documents so it's not necessarily there's only thirteen percent correlation in interpreted how should that be interpreted it's it's the fact that its most likely of all your tags to be applied so PHP was at the top it was the highest number even though it was a relatively low number it's the highest the highest correlation for that given document a better approach might be to say normalized for that we've got very different sizes of document there so a better approach might lead to say have some kind of if you're heavily weighted there versus all the other tags its most likely to be about that and as a result the actual number itself doesn't matter but the comparison with all the other numbers that you've generated for that model would be a better way of doing that hi I mean III saw the time it takes to actually execute the result do you have any thoughts about the memory management on this case like for for example this using pc may not be enough for the hardware and the approach I've done here you can probably do it in a more scalable side of things you could do all sorts of things to actually spread the load out the better approach in my example in would actually just to be reduced the amount of information being calculated all the way through so I'm not actually trying to use the memory in the first place so as opposed to using it more efficiently I use them more efficiently by not using as much of it if that kind of makes sense so instead of saying calculating all that data for the rehabil reduce the number of features on calculating so that I've only got I've got less information that I'm trying to calculate for each of those tags thank you I wonder when it might be useful if it's useful to manually edit the model after its generated for example you've said that fridays and represent correlated with BP is that something you might want to manually delete after you've generated a muddle or not and so there's no reason why you couldn't do that I'm so kind of prune what's going in I we can the advantages we know that's a ridiculous her thing but one of the interesting things of machine learning is the fact that it might be it might be that PHP only ever gets talked about and Fridays and you might assume that we talk about it every day but an actual fact on joined in it only ever gets talked about on Fridays and machine learning doing this and showing the actual numbers and calculating that might actually show you a correlation and a results that's entirely counter intuitive but might be entirely accurate so yes you could manipulate the model that you've created and that would be valid certainly in that example but that's making the assumption that we know right and not the thing that's actually crunch the numbers so I think there's only a couple of minutes left to actually going at a coffee so when you take a break now thank you very much for your questions if you'd like to get in touch 