 what light in yonder window breaks and Julia is the son who's the father no no no she's a bright ball of gas though someone's son is Julia is the star in the sky hello world welcome to Serie geology in this episode we're going to build an AI reader that is an AI that can analyze human language this type of task is considered natural language processing or NLP NOP is the art of solving engineering problems that need to analyze or generate natural language text we see it all around us Google needs to do it to understand exactly what your search query means so they can give you back relevant results Twitter uses it to extract the top trending topics Microsoft uses for in-car speech recognition NOP is basically extremely dope because it deals with language Kurzweil once said that language is the key to AI a computer able to communicate indistinguishably from a human would be true AI there are 6500 known languages in the world and each of them have their own rules for syntax and grammar some rules are easy like I before E except after C and some one based on intuition since there is no consistent use case so how do we write code to analyze language before the 80s NLP was mostly just a bunch of hand coded rules like if you see the word dance translate it to kuduro or if a word ends an ING label it as present tense well this worked it was really tedious and there are a million corner cases it's just not a sustainable path to language understanding a way forward was and is machine learning so instead of hand coding the rules and a aya learns the rules by analyzing a large corpus or piece of text this is proven to be very useful and applying deep learning to NLP is currently the bleeding edge so when thinking about what tool to demo in this video I was really torn between an NLP API called API to AI and Google's newly released English parser parse emic parse fix file API that AI takes in an input query and returns on an analysis of the text Parsi is Google's newly released English parser both have similar functionality but I'm gonna go with Parsi because a it's currently the most accurate parser in the world be if you built it into your app that's one less networking call you have to make which means you can parse tax offline and see building this parsing logic from the source allows you to have more granular control over the details of how you want text to be analyzed Parsi was built using syntax net an NLP neural net framework for Google's tensorflow machine learning library so we could use syntax net to build our own parser or we could use a pre trained parser Parsi let's do that once you parsed your text there are a whole host of things you can do with it let's try it out with our own example we're going to build a simple Python app that uses parsing mc-- parse phase to analyze a command by a user and then repeat it back to them both where it is different we'll begin by importing our dependencies then we'll set up our program to receive and store the user input the input text is the corpus will be analyzing we can get an array of all the part of speech tags in the input tab using the tagger function so what is part of speech tagging it's assigning grammatical labels to each word in a given corpus you know all those words we learned back in elementary school so take the phrase I saw her face and now I'm a believer if we tagged each word in that phrase individually without looking at the sentence as a whole we might tag saw as a certain bird which means this would be a quote from Leatherface but if we look at this word in the context of the sentence we realize that it's a different verb Google trained Parsi by interpreting sentences from left to right for each word and the sentence and the words around it extracted a set of features like the prefix and suffix put them into data blocks concatenated them all together and fed them to a feed-forward neural net with lots of hiddenly which would then predict the probability distribution over set a possible POS tags and going in order from left to right was useful since they could use a previous words tag as a feature in the next word so what is a parse tree well part of speech tagging isn't enough there's another part the meaning behind some piece of text isn't just the type of word that's being used but also how that word relates to the rest of the sentence take the example phrase he fed her cat foo there are three possibilities of what this phrase could mean number one he fed a woman's cat some food that's the obvious one to us intuitive humans but there's also number two he found a woman some food that was intended for a cat or number three he somehow encouraged some cat food to eat something the meaning of the sentence depends on the context of each work the team used something called the head modifier construction to sort word dependencies this generated directed arcs between words like that and Cat Cat being a direct object the word debt the sentence starts out unprocessed with an initial stack of words the unprocessed segment is called the buffer at the parser it encounters words as it moves from left to right it pushes words onto the stack then it can do one of two things it can either pop two words off the stack attach the second to the first which will create a dependency arc pointing to the left and push the first word back on the stack or create an arc pointing to the right and push the second word back on the stack this will keep repeating until the entire sentence is processed the system decides which way to point the are depending on the context ie previous POS tagging once that's done it uses the sequence of decision to learn a classifier that will predict appendices in the novel corpus it applies to softmax function to each of the decisions which normalizes or adjust them to a common scale and does is globally by summing up all the softmax scores in log space so the neural net is able to get probabilities for each possible decision and a heuristic called beam search helps decide on the best one when predicting them once we have our parse tree and parts of speech variables let's store the root word and the dependent object into their own variable we'll call a synonym API to retrieve a synonym for the dependent object then construct a novel sentence that repeats the command that users entered back to them in different wording looks like it works pretty well the scope of what you can do with this is so that you can use this text analysis to create a text summarizer work recognize the intent of a query or understand if a review is positive or negative or my personal favorite create a political debate back checker links with more info below please subscribe for more ml videos for now I've got to go fix a buffer overflow so thanks for watching 