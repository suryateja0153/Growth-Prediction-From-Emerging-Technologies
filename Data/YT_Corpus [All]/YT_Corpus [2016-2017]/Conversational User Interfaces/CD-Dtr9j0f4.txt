 oh it's the microphone good can everybody hear me perfect so hello everybody and welcome to my talk on conversational computing or how Okazaki made McCarthy right once again now let's start off with some puzzle time since everybody's probably a little bit sleepy by now does anybody know this guy yes who is it John McCarthy of course especially they close your conference everybody knows him John McCarthy is the guy who coined the term artificial intelligence in 1955 he discovered the theoretical foundation for the half a century of computing so far Lisp and on top of that he also invented garbage collection time sharing and non-monotonic reasoning to boot now does anybody know what John McCarthy published in the year in 1959 no it's not the Lisp paper thank you for that incorrect answer the correct answer is that John McCarthy published this paper programs with common sense and this is a paper in which he introduces a concept called the advice taker now the advice taker is basically some sort of thought experiment for a system that lets you speak formal statements to its and that then will automatically be able to improve itself in its understanding of the world using those statements and in the paper you see that he didn't really mean that theoretically either it really assumes that this system has common sense in the form of a rich repository of knowledge and that you be able to take any inputs and make the right decisions given all the other information it knows about the world sort of just like a person would and he says several well maybe some of the ambitious goals for this like all behaviors must be represented in the system changes in behavior must be expressible concisely all aspects of behavior must be improvable the Machine must have concepts of partial success and be able to create subroutines now if you read this paper you would sort of get to what like call McCarthy's conundrum because to be honest it's a pretty odd paper it's pretty vague it's somewhat incoherent at times in turn internally contradictory in places and in some way also just sort of very obvious like yes of course you would like a computer that can do everything duh but as we all know that's not everything that McCarthy wrote because McCarthy also wrote papers like the lists paper which are incredibly clear beautifully concise amazingly elegant and groundbreaking like never before now what's going on there well my theory is that this is sort of classic left right brain intuition thing going on what we're sort of McCarthy's modus operandi for his big breakthroughs was to sort of first set his filters wide open and you sort of let in many false positives but make sure that whatever was the right ID was that it's in there to just collect all the data everything that could possibly be true and then just churn and churn and churn and work on it until you get to a sort of a flash of insight and once you do that evaluate everything in the context of context of that insight eliminate all the distractions and simplify your idea to its core now my theory the key theory of this talk is that that's how McCarthy came up with lists so he took programs with common sense worked on it for 18 months together with Marvin Minsky to sort of get the flash of insight to use Alonzo Church is lambda calculus and reduce the bully thing of the programs with common sense into the original answer the list paper that was published in 1960 and I like this thing because the list paper actually says part one but there isn't a part two yet which is pretty meaningful in the context of this talk now so in my theory is that this is how this was created but after that what we haven't really heard much but forty years did go by and in those forty years McCarthy two things happened the first was that McCarthy went to a conference and he attended a talk called computer languages for the year 2000 now McCarthy really didn't like what he heard there because he thought the talk wasn't nearly ambitious enough and luck for us another thing that happened is that he started reading some books but this guy John Searle now John Searle is a very interesting philosopher for programmers because he basically describes how humans use language to build up the part of the world that's only in our heads how we use words to create customs laws and institutions and how we generally make the social world now I really like to imagine that John McCarthy was sort of reading this book and just like happened to me when reading it had his eyes drawn to these parts where Searle uses parentheses nested and nested concepts which reference each other to describe how we create our social world and well after having been challenged for what the right programming language for the future would be he decides to sort of get himself worked up one last time and deliver this paper elephant 2000 a programming language based on speech acts I meant what I said and I said what I meant an elephant's faithful 100% moreover an elephant never forgets now what is this thing it should be fascinating right to figure out what McCarthy's vision for the future programming would be well if we go through it here he sort of says that inputs and outputs in this language are analogous like our sentences analogous to the concept of speech axis used by philosophers like John Searle it's also a language whose correctness is described by correctly performing those speech acts sure and it should be able to generate sentences expressing forms of correctness about itself okay it's a language that might do without data structures because it has a full history of the past that can be referred to okay furthermore programs in this language are also themselves also sentences of logic which don't need to model commands in terms of changing program state okay and these programs can provide output statements not just about their formulas result but also the practical effects on their on the world which is sort of saying that they cannot just tell your kids to clean their room but actually make them clean the room you just I don't know pretty ambitious he says that the most obvious application for elephant are applications that do more than just crud okay and finally in conclusion while elephant gets close to card AI work it doesn't really need to be now if you read this like actually a lot of people who read this sort of assume by now McCarthy has lost that lost his edge because well he was getting old and the paper was once again the sort of fluffy thing it's hard to make sense of this like yeah like you know even I had trouble reading it at first but what if instead of dismissing it we give him the benefit of the doubt and assume this paper might be another advice taker what would the flash of insight be this time that could take elephant 2000 and give us possibly recursive functions part two well luckily for me I read the sural book before I read the elephant 2000 paper and I found a little hint there and here is a page from Searles book where Searle is basically arguing that the key aspect that differentiates our linguistic Society from pre-linguistic societies is our ability to express novel statements out loud in a social context because of the simple fact that we can't unspeak what we spoke what we've spoken and by saying something out loud we sort of create a commitment to having made it sincerely and like this is a thing for my Kindle and there's a little note there and it said there's a very interesting parallel here with immutability because basically is saying that immutability is the key to making our social world tick now it took me a while to understand it like how does that relate to you know less but we are working on though and at some point I realized that as Koreans we are very intimately familiar with a certain kind of immutable speech ax X but with Y this is an immutable speech act that we work with every day although we probably don't recognize it as this we do recognize it when we put it like this though this is an immutable speech act and I think that that actually is the key that gives they would get us from elephant mm to the list paper part to purely functional data structures persistent data structures an immutable data structures whatever you want to call them so if we actually go back yes so if we actually go back over the elephant 2000 paper here we see that his money says that inputs and outputs are meaningful speech acts is basically sort of describing Awlaki she loves work on extensible effects which is state-of-the-art research in making side effects first-class citizens in functional programming languages so that context this actually certainly makes quite a bit of sense and well using a history of events instead of a data structure basically describes event sourcing or the lambda architecture or a unified log which all by themselves are state-of-the-art technologies of this very moment and well this one about the doing where the crud is pretty vague by itself if you look at the examples in the paper this sun's looks remarkably a lot like what the atomic already offers us so so you were probably onto something but Oh as I said like these things well we can already do all that so that leads us through the questions so what you can do all this well there's one more thing that McCarthy also mentioned in his paper and that's this elephant programs themselves can be represented as sentences of logic now what does he mean by that I think I can best demonstrate that with a bit of code and that's this piece of code so this is just very simple closure code we have some Commission calculation function and we calculate the Commission over 10 euros sure and then at some point okay the Commission changes it gets a base fee so oh yeah add that we change the Commission function we calculate the Commission again now it's 250 very fine but what happens if we now want to calculate the old value again other thing is we can't because the old Commission function is gone it's mutated closure is actually half mutable to be fair pretty much every other language right now also is half mutable but still it's a very key thing and that actually I think is the key realization that McCarthy wanted to convey with elephant 2000 so the thing that I think is what really makes McCarthy's puzzle tick is whole system immutability or if you prefer a fancier term fully pure lambda calculus if we change this little part if we let our system systems become completely immutable then we enter into a wholly different world then we move from using the power of purely functional data structures and so all sorts of disjointed applications towards a unified formalism which incorporates this all into a cohesive whole worthy of being called recursive functions part two now because McCarthy didn't get to name this I thought I'd give it a go here and I think we should call it conversational computing because it's inspired by speech acts and my definition of it is computing using sequences of immutable statements which reference the pass now McCarthy's prediction was that this would be the language for the Year 2015 so let's see let's see how his prediction fared actually it turned out to be pretty right because in September 19 2013 both shosanna the author of functional programming in Scala launched unison now what is unison well unison is a typed fully pure lambda calculus in exactly the way we've just been describing it does some more really cool stuff so it's it's ASD is actually a content-addressable Merkel tree which means that every individual sub node in the ASD has a hash and can be retrieved and identified by that hash on top of that he builds a really cool distributed computation model by which you can take arguments and instead of downloading the function that's behind a hash just sending those arguments to that function on another server and basically get back a memorized some function application and because this all wasn't ambitious enough yet he also decided to create a semantic editor with reddit style type correctness meaning that is impossible to create incorrectly typed code in unison and that's really cool but that's not everything yet there's more half year later Reed Mackenzie released aux line now aux Lang is based on his work building the oxcart compiler which was a statically optimizing closure compiler and he sort of kept optimizing closure code until he came to the realization that a lot of optimizations were blocked by the fact that namespaces were immutable and in that sense aux Lang is his effort of taking closure of beyond the limitations of mutable namespaces by building a fully pure lambda calculus interestingly enough I recently found out that there's another conversational computing language already being built and that's Eve Eve by none other than Chris Granger and Jamie Brandon of light table Fame their new big project it's not technically yet a fully formal lambda calculus but it is built on a fully immutable log which stores both the code and the data so in that sense it's definitely very conversational it's also the only one that's already sort of using a usable it has a useable IDE which is also remarkably conversational because you sort of type speech acts and then create new pieces of data just by writing them which is a nice so actually it seems like McCarthy was right both about the ID and the prediction but okay so given that we have these languages what can we do now that we have them well I've given this idea is something like two years of hammock time by now so let me give you a whirlwind tour of everything that I've already envisioned that these languages will make possible now the first things to revolve around historical code awareness because in languages like this you have access to all historical code now the first thing that changes is related to sort of the oxcart compiler does is that compiling actually just becomes creating an append-only machine optimized reflection of your code which can be very optimized because whenever you call a function you know that it's never gonna change so if you're using a specialized version you can just specialize the function and this way you can create amazingly fast code another thing that you can use this old code for is for example if as part of the fan block processing like we see with the lambda architecture and we're starting to really store all the old events we have but currently whenever you want to reprocess that you cannot just use all the old code you have to create all sorts of complex intermediate stages to keep it still concise then language like this all the old code is just still available so you can just use it to reprocess the old events that's pretty cool but you don't need to stop there and what you can also do is to take the old code and the old code that visualize the data and using that you can sort of create long-running interactions that's actually how I originally came up with this idea because I was once asked to create a forms application in which you could sort of change the forms and change the validation and changed the visualizations of forms but the system had to span like it was for doctors who tracked kids from birth to 18 years so it had to span all the 18 years and all the data all the way back needed to be visible with current programming languages that's not really feasible but if you have a language like this you can take the old forms and just visualize them render them using the old code which you still have now that's not all there's more that will change another thing that will change dramatically is experimentation in a language like this it becomes really easy to create an alternate version for example for an a/b test you just create a new function you put it next to the current one and sends a certain amount of traffic over to it but now when you think of it deployments what are they actually other than sort of really big experiments that are intended to sort of get all the traffic so deployments can become experiments like this too and the coolest part is that you can actually and decide that you take a deployment but not automatically deploy it but only deploy it if it performs better on some business metric so you can use for example multi-armed bandit say this deployment will be automatically deployed if it increases the click throughs but if for whatever reasons they drop it will be taken back out of production that's really cool another thing that these languages make possible is that you can actually drill down to the full playback of user experience so whenever something happens you can sort of go in see exactly what that user was doing and figure out why something is happening which is very powerful but well that's not all you you cannot just see a single user going through your system you actually have access to information of all users going through your system you can do really cool stuff for that like for example analyze all your users and query for users that are exhibiting certain suboptimal patterns like they're using the interface inefficiently they're stuck yeah they're not using it anymore stuck on a plateau and then a design an experiment that targets those specific users and provides them an intervention and you can deploy that intervention tested and integrated if it works well if you do this a lot then you actually start getting through an entirely different kind of system it's what I call conversational interfaces so these interfaces that react to you using them and that can be for example in the form of an explanation when you first reach some part of the system like many onboarding systems already do but you can take it a step further also and detects satisficing so when somebody takes five clicks to do something that can be done in one click and detect that and immediately propose a better approach if that's not good enough for you you can take that even one step further and say whenever you detect that someone's using your interface in a suboptimal way rewind the interface make them do it the correct way so that you can actually have an interface that forces you to use it in the most efficient way possible and so if you keep doing this more and more and more you actually get at some point into what I really call the conversational interfaces where it sort of becomes like a conversation you are having with the system now that's also what I'm taking our current company in the direction of although we really cheat and we don't do this with a fully conversational system we just store state machines in the atomic and it sort of gets us 50 percent of the way with one percent of the effort now taking this into account I would like to extend our previous definition of conversational computing to computing using sequences of immutable statements with reference the best and using these systems historical statements to adjust system behavior or make systems which acts like people now this is sort of a quick overview of everything it makes possible functionally like what it can give the user so what about the non-functional what you know how does it work under the hood and what does it get us there well the first thing to start is let's let's start off with a classic ast that's the one thing that's sort of a constant in pretty much every language ever although in conversational languages it's not really in abstract syntax trees more of an abstract syntax graph because first old versions of the app can still use same part parts of your current code and actually other apps entirely could be using something like some parts of your code as a library now we were also talking about speech acts so it's important to realize that every single piece of code is actually also someone's speech act so here you see that John is referencing code from listen Mike and Alice sort of ties it all together in the final app so if you're coding in a conversational style you do that by basically creating an abstract syntax graph with note level code ownership and a distributed version control system built in which is awesome although if you talk to someone from small talk apparently they have this in the 80s already but I guess that goes for everything now the next topic is how do we get this code to the end-user well what happens is we always sort of squish all this code together and then the end user will get a reference to the root node but well the root node Cerf is the entire app so he doesn't need to download it all at once he can sort of download only the parts he currently needs for example display the first screen and he would do the same for the database where like if you access the database he only gets the data he currently needs but not the rest which brings us to sort of the second cool feature of conversational computing which is its caching structure because cache in the conversational language is never invalidated because it's a mutable data it never changes the only thing you can do is forget it if you run out of hard disk space or whatever but as long as code is not forgotten it can always be remembered because if you once had all the pieces of data necessary to view an item you can always view it again which is really good for offline apps and though in many cases but let's continue so then the user has the parts of the app it needs and it uses the data to generates a login screen now suppose the user enters the login details correctly it then would go off and download the code for the rest of the UI and afterwards the user keeps clicking around a bit it comes across a form that gets some more data and like this it slowly gets the rest of the data but that's sort of how you interact with the system like this and sort of from years like okay so where is the conversational aspect in this well it's actually if you look there at the login submit' UI click you can sort of view that as a conversation between you and an actor and I call them transparent actors which are basically actors you interact with sequentially but whose actions you can predict barring certain synchronization or branching points and basically I found that this like transparent actors is a much better way to think about isomorphism because it's not just about coach it's not about coach sharing but it's about if the other party you're interacting with the app is transparent enough for you to be able to anticipate its actions which is actually also what allows people to speed up turn-taking in regular conversations now so far it's been a one-way street of receiving data but in a conversational you can always a tional database sending data is really simple because you just send it across and it's just a speed check the atomic datum for example you send it across becomes part of the database which already exists of everybody else's piece of data and I think that's really powerful because databases then basically become repositories of speech acts from users and developers with built-in provenance tracking now the provenance tracking part for me is very important or very big because I think it's the ultimate knobbly Buster whenever someone asks me but how Europe should be Silicon Valley I always answer with this because I think the Europe is just never going to be set up for a big monopoly breakaway successes but we are very good at breaking down monopolies and I think if you have created a law that sort of mandates that every single user can get back all the data he put in you get this sort of situation where if every Facebook user at one day decided I've had enough and pulls their data out Facebook could be gone in a day and I think that would really enable a lot of innovation now another feature you have is conversational authenticity because if we see here basically but you have like three actors another thing you can do is you can give each actor their own private key and then let those actors sign all their data if you do that then you get what I call a conversational order authenticity which is where each speech act has a verified identity now if you add on top of that verification and verification infrastructure so that you can handle lost keys but also have a specific key that's tied to your password for example you suddenly have the perfect platform for digital self-governance because you can organize a referendum in a matter of minutes now finally the final feature is conversational privacy because so far all this data still had to go across the internet otherwise known as NSA friendly territory now is there anything we can do about that well yes there is because we can take the data and instead of just signing it we can encrypt it with our key and then we can take that encryption key and encrypt it against the public key of our recipient and that way only they can descript this data and that's basically how for example the OTR off-the-record protocol works and if you use that you can get perfect forward secrecy now using that it gives you an NSA proof computing paradigm in which our everyday concept of privacy is actually embedded into cryptographic reality with one minute despair I would like to round up my talk with this conversational computer computing or Al McCarthy discovered the foundations for the first and second half of computing first century so if there are any questions 