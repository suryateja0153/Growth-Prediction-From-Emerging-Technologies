 Well, good morning everybody, or good afternoon, depending on what time zone you're in. Welcome to the BD2K guide to the fundamentals of data science. I'm Michelle Dunn, I'm in the office of the associate director for data science. Please go to the next slide. I just wanted to say a few things about where we are in this series. We started with a wonderful talk by Mark Musen in early September that gave us an overview of really all of data science, and then immediately jumped into section one, which was about data management. So data management, we heard about indexing and binding data within the data management section. But mainly about how to make data that is usable. So how to make data that's well curated, following standards for metadata, and including prominent information. Now we're into section two. Section two is titled Data Representation, but really it's about getting data into usable format. This will take us through the end of the calendar year. And then starting next year in 2017, we'll start up again with a section on computing followed by a section on analysis, and then other topics like ethics. Can you go to the next slide? So I'm sorry, I have a mistake here. I should be highlighting the section two data representation, which is where we're at, not computing. But data representation includes topics such as databases and data warehouses, wrangling, normalization of pre-processing. Those topics we have two guest speakers speak on that for two weeks. And then natural language processing and exploratory data analysis. But today we are incredibly lucky-- can you go to the next slide? We're incredibly lucky to have with us Dr. Anita Bandrowski, who is currently at UCSD and has a background in both neurophysiology and bioinformatics. She trained in neurophysiology at UC Riverside and at Stanford. She is the founder and CEO of SciCrunch, in addition to being a part of the NIF, the Neuroscience Information Framework. And she works at the Center of Research and Biological System at UCSD on these two things, NIF and SciCrunch. And NIF and SciCrunch work together in order to aggregate vast amounts of data. Though she's really an expert in this area of knowing what to do with the data. So I really look forward to her talk, and I know we all really appreciate that you are taking the time, Dr. Bandrowski, to come talk with us today. And so with that, I think please take away. The floor is all yours. Thank you so much. And what I will do in just a second to switch over to my slide deck, and I will then mute my speakers. So if you need to get a hold of me, please do so in the message box. So I think now we will have better, clearer voice. So in this guide to the fundamentals of data science, as Michele had pointed out very poignantly, this is an overview of data representation. And I have worked on the Neuroscience Information Framework for about 10 years. And the goal of that particular project at the University of California San Diego is to make as much data available and useful to people, mostly to neuroscientists. And so many of the examples that I will use will actually be from that particular project. But I do also recognize that there are some core fundamental things that happen. So in terms of the overview of data representation. The lecture series will cover things like types of data, databases, and data warehouses. I think it's wonderful we have a wonderful list of speakers, including a Dr Chaitan Baru, who also used to be up the road from us at the supercomputing center. Doctors Picone, and I am going to butcher Doctor Harabagiu's name, and some of the others. But I wanted to point out when these lectures are and what are the sorts of things they're covering. Today I will cover in a small amount of detail the basics of databases, the basics of data wrangling, and then you should be able to know a whole lot more from Doctor Baru and others. So one of the very basic questions is, what is data? And I just wanted to make sure that all of the vocabulary terms have definitions, as this is supposed to be kind of a primer lecture. So data is the facts and statistics collected together for reference or analysis. We'll also refer to metadata, which is data about data. And actually, when you think about it, one person's metadata is another person's data, so then you could have metadata of metadata. And can go on from there. But another term that I think you should know is what is a schema? And it's basically the structure of the data. What does it look like? And this has a very set of specific meanings when you start to talk about databases. What do we look across biology? Data is incredibly diverse. And again, this is one of the examples from neuroscience, but you can substitute the brain into a kidney, and to a liver, and to whatever else you might want to look at, and essentially the problem is extraordinarily similar. Biologists think about interactions of very, very tiny things. They look at NMR, they look at other spectroscopies. They look at data types of that sort on Brownian motion, and they look all the way to the social interactions of animal herds. In fact there is a fun study published by a group of ecologists-- also biologists-- trying to figure out how to use Google Maps to determine whether cows point mostly north-south or east-west when they were going to use themselves in herds. And they look at everything in between. So they were looking at intracellular currents, they're looking at MRI images. They were looking at latency to button presses, and all kinds of other things. So the data that comes from all of these different techniques can be incredibly variable. So really, biological data is characterized by diversity. Because it can be text, there can be time series, it can be categorical, there can be images, there can be video, there can be locations-- geolocations and locations within the body. And all of these things actually are data, and they're data for different biologists. But when we look at data type, it's a little bit different than that. So when I was trying to prepare for this lecture, there was this piece of data on my desktop, and so here it is. And there is application, specificity, clone, quantity, and quantity type. And when a biologist looks at this particular piece of data-- this just happened to be on my desk last week-- they might start thinking about what this data actually means. So one of the things that I've learned by being part of the Neuroscience Information Framework is that biologists think about data very differently from computer scientists. And biologists tend to imbue the data with meaning. So what does it mean to me? Well, application, WB. OK, that's western blot. So I need an antibody that works for western blot. Clearly this is a piece of data about antibodies. And these are some of the things here in the pink boxes that a biologist might think about when they see this piece of data. But if you're a computer scientist, or a data scientist, you might start thinking about this same piece of data, the same exact spreadsheet, slightly differently. They might need to think about what the structure of this data is. That there is an IF comma WB. This is a comma delimited list. They might think about the specificity not as something that I might or might not use, but they're actually thinking about, well, what is the external lookup that will be required in order to explain this specificity better? This is actually a lookup to NCBI gene. Clone number they're a little bit confused about, because this is free text. But quantities and quantity types, they're thinking about the types of operations that they might do on data that is numerical. So data type is a particular kind of data item defined by the values it can take-- whether it's text or number-- the programming language that's used, or the operations that can be performed on it. So again, we can do things to numbers that we cannot do to a list. And so if we look across disciplines, it's not the same thing. So when computer scientists look at data, they imbue it with structure. And data types, I've listed some here, there are many more. These come from SQL. Data types are obviously text controlled vocabularies, which you heard about from Doctor Musin and actually Doctor [INAUDIBLE] and others. There are gene names, which these are the sorts of things there are. They're free text, there are all kinds of different types of numbers, and there are, of course, dates and other things. But why do we care about data types? Turns out the only time you really deeply care about data types is when you want to put data into a database. And a database is essentially a structured set of data that's held in a computer, especially one that's accessible in various ways. So really to understand what a database is I wanted to go back and just ask, what are some of the things that a database can do? It's essentially a container for different types of data. It's a container for data files. You can think of it as a file system on your computer. So our little example here for this spreadsheet of antibodies could be part of the database as long as it's accessible on a computer. So what are the kinds of operations that I could do if this was part of a database? Well, I could do things like count the number of rows that have western blot, that are like western blot. Which antibodies work for western blot? You can get that. And the answer would be all four of these. You can select rows where specificity actually is one of these zinc figure proteins, or the other one. And then it would bring you back the data about that particular row or that particular data item, depending on what you asked. You could find all rows where quantity is larger than or smaller than a particular value. So essentially the database is a container that's very useful in many ways in order to ask questions about a particular dataset. But you can do this as a biologist, and I bring this piece in to this overview lecture because most biologists use Excel. And Excel is an incredibly powerful tool. And most people know all kinds of things about Excel. It is one of these programs that's incredibly powerful, incredibly ubiquitous. But also every tool has its limitations. So I wanted to make sure that whoever doesn't know about these limitations knows about them after this lecture. So one of the limitations of Excel is size. So there is a certain number of data points, a certain number of data rows and data columns that Excel will take. And it is important to understand if we have data of a certain size, a certain number of rows, that simply would be too large for Excel to take. And so it's one of the limitations that everyone needs to be concerned about and aware of of Excel. The most recent-- and actually for the last probably five, 10 years-- versions of Excel have been getting more and more aggressive in terms of autocorrecting. So autocorrecting here in this particular article-- this is now quite old, but it hasn't changed so we should continue to be aware of it-- it is that gene names actually become dates. So this is something that my Excel did, and I have the latest version of Excel running on a Mac. And so what this is doing is, I put in the APR 4 gene. As soon as I hit enter, it became the 4th of April. And the way that that's coded in Excel is the number 42 464. 64. So if I put in 1,000 genes into my particular spreadsheet, and some of them were APR 4 genes, then I would have a very large mistake in my spreadsheet. And this has certainly been called out in this paper and many others. So it's something that one must be aware of. The other problem with this is that when you change from an Excel sheet that's actually in a Mac to one that's in a PC, and so you're passing a file over to your graduate student, your PI, your fellow at whatever institute, and they happen to have a different operating system and a different version of Excel. What ends up happening is that mistakes like this, which now become coded-- so April 4th becomes coded as this particular number-- that number is only April 4th for a PC, or it's only April 4th for Mac. It will not be the same number if you switch operating systems. So in a sense it is a really bad idea to handle a lot of data, especially large data where there are genes involved on Excel it is an extremely bad idea because there are these known differences between the dates the date columns and how Excel actually handles them. This is another type of mistake that is very common. So here I've taken the ZNF 622 gene, and instead of copying and pasting that gene I just drag down the little icon. And the newer versions of Excel, again, will not only iterate by one number when you only have a number, but they will actually iterate to add one number to almost any column that has a number in it. So this is another thing that one must be aware of, and I see a lot of data, and this is the kind of mistake that I see unfortunately quite often. So how can I really understand a database? So again, in order to get biologists to understand what computer scientists are thinking, I want to bring in some of the things that computer scientists and data scientists would be actually concerned with when they see our file. So computer scientists very, very, very, very concerned with efficiency of data retrieval, with the size of the total file, with the data types, which we just discussed, and with the types of operations that need to be done on that data. So this particular Excel file, actually I have saved it on my desktop in different formats. And this is something that we've all done, and I'm sure that all biologists basically largely ignore this unless there's some reason to pay attention to it. So this particular file is a 46 kilobyte Excel file save as latest version of Excel. If I save the same information as a PDF it actually becomes a 12 kilobyte file, so you can see that there's a very large difference. But as CSV or text it actually becomes much, much smaller. So that really reflects something that, again, for most biologists, we don't care. My computer is large enough to handle any of these kind of files. But the larger the file size you have, actually the more you have to care about this sort of question, these sort of ideas. So again, understanding a database is really taking this particular example of this file. What we can see is that computer scientists, because they are very concerned with the process of efficiency of data storage, efficiency of data retrieval, and the size of data, they have driven these spreadsheets down from 153 bytes to even smaller and smaller sizes so that you can actually store much more data in the same space. And one of the ways that this happens is through a process called normalization of data. So here I've normalized this particular table, the one in yellow, into the two tables in green. And the column I've normalized is the specificity column. So here you will notice that there's a zinc finger protein here, there's a different one here, and there are two rows for for zip kinase. And now the way that we can make this spreadsheet even smaller is actually to code the fact that zinc finger protein 622 is actually now corresponding to the number one, the other zinc finger is two, and these two zip kinases are both 3. And now there is something called a key value pair. So the number one corresponds to this, number two this, number three, this. And what that does over millions of rows is it further decreases the size of the data, which is great. It speeds up retrieval of data, which is great. But the more rows and the more columns that actually look like this, the more data that looks like this, the less readable it is by a human. This spreadsheet, I would argue, is relatively well readable by a human. But once you start changing out a lot of the actual labels with numbers, what starts to happen is that you can no longer read it. You have to do a lookup for each thing, and those kind of operations to make the spreadsheet back into something that's human readable really starts to take a data wrangler. And I'll talk a little bit more about this. So what I'm talking about here are a lot of relational databases. Now relational databases, the reason that I'm talking about them is because they handle very large files very, very well. So you don't have to stick to just a million rows of data. These are some of the oldest and the most common databases throughout the internet. They run banks, they run stores, they run all kinds of transactions, they run many, many websites. Everywhere you have a box you probably are filling out a form that goes into a database. So we have a lot of interaction with databases, even if we don't necessarily know that we do. They do allow multiple users, including multiple machines, to actually add data simultaneously to the same data store. They allow data to be extracted at the same time as it's put in. There are lots and lots of things that people can do if with these. There's a tremendous amount of diversity in terms of what can they be programmed to actually do, and there is, of course, a programming interface which makes everything very flexible. But as every tool, it is very, very understandable that there are some limitations. So one of the first limitations for biologists specifically is that these kinds of databases are actually more difficult to set up than an Excel file, or a Google Doc. It is more difficult to do some of the simple operations like copy this data into that particular space. There is no undo button, there is no autocorrect. Which is either a limitation or a high benefit, depending on where you sit. And they are very dependent on data structures. So you have to predefine what you want to actually have in the database before you can actually fill in the data. in the database before you can actually fill in the data. So this is something that is a limitation that many computer scientists have actually considered as a very significant limitation. And they have created a whole bunch of new types of databases, which essentially are much newer, less common. And most of these database types have been created to solve one or more of the shortcomings of the standard traditional relational databases. So Neo4J, for example, does a lot of very efficient computation on graph structures, and I think that will be covered by Doctor Baru and others. MongoDB does really wonderful things with text storage and retrieval being very, very much more efficient than the conventional SQL databases. But new tools, as all new tools, have smaller numbers of people that can support them, and not all the kinks are usually worked out from a lot of these newer databases. So when you're choosing how to store data it's important to consider some of these things. Not just always go for the newest best thing, because there may be some lacks of support. So really I wanted to ask the question, what makes a database useful to biologists? And this is something I've run into, and my colleagues have run into. Because I am trained as a biologist, I see data from this perspective quite a bit. So what computer scientists think that biologists want to do with databases is actually quite different than what biologists actually want to do with databases. And so one of the things that computer scientists often think is that biologists are just dying to create hypotheses based on-- and here I have pointed out, this is a gratuitous graphic. I figured it wouldn't be a big data talk without one of these lovely hairballs. This is lovingly referred to as a data hairball. Now, computer scientists really think that biologists would love to create hypotheses based on data. Now, I haven't run into many biologists that actually would like to do this. Mostly they have a hypothesis in their head already that's based on reading literature and understanding biology. But it would be really nice if we could do that in the future. Also, comparing their data to other people's data, biologists have a really hard time doing that. And actually it's one of the things that biologists are really reluctant to be able to do. And exploring data space is one of those things where it's nice, but it often doesn't go right into the middle of the biologist's pipeline in terms of what they're doing in the lab actively. What biologists actually use databases for is they look up papers. Now, I haven't been able to find exact numbers in terms of the internet traffic to biological databases, but I do know from various sources that the vast majority-and this is a very, very high majority-- of all web traffic to any database in biomedicine actually goes just to one. That one is PubMed. That is a database, and it is where everyone goes. Biologists do share data, and they store data in databases, but they often don't look for data there. Bio informaticians do, and so that's a whole different story. But biologists tend to just take their data and put it somewhere. And they need to make sure that that's relatively easy. Biologists also are starting to use databases as a lookup. So for example, NCBI gene, I need to store my data, I need to figure out what other gene is related to my gene. Is my snip in this place or in that place? What are the places, and who else has published on that particular snip? So there's a lookup function. But for the lookup function, you need to understand the database has to be relatively complete. So for biologists to actually use a particular database as a lookup function, it has to have that completeness. Unless it's useful as a storage device, in which case there's no question there, and completeness is not necessary. So how many databases are there currently for biologists? I've tried to look at some of these numbers, and I try to do some lookups in both WorldCat, so this is the library catalog of resources, including a whole section of internet resources. And what they have is about 5,200 databases, but this covers all fields. So it's geology and everything else. In the Neuroscience Information Framework, we have cataloged 2,600 of them essentially. So these are all biologically relevant databases, or largely biologically relevant databases. So that's the kind of thing that we're looking at. And NCBI, which is the National Library of Medicine, there are 150 databases that are housed actually inside of NCBI. Of course, everyone knows PubMed. Most people know PubMed Central, but there is actually 150 others, including some that you probably know, and many that you've never heard of. But I would encourage all people to explore where some of these other databases are At EBI, 53 databases are actually housed, including Europe PubMed Central. So a way to get articles for Europeans. ArrayExpress, which is very similar to Geo, ChEMBL, and others. But the biologists might ask, with all of these different databases-- and here I've actually pulled out some of the keywords from the neuroscience information framework system for all databases, and the organisms that have been tacked to those particular database entries. So essentially, if you're a biologist and you're looking for a human genetics database, you will have hundreds to choose from. So if you have the choice of trying to find your data in hundreds of databases, or thousands of databases, you might ask some questions as a biologist. How many of these are still alive? How many of these are still being added to? Which ones are being cited, which ones are being used? Which ones house actually the same data? So we know that a ArrayExpress and Geo actually house many of the same data sets. Which ones are the gold standard? Where do I put my data so that it gets saved and kept? And this particular question is not just being asked by biologists, but it's also being asked by the journals. Because the journals need to ask the question, OK, I have an author who has a particular type of data. Where do I send that author to deposit that data? We know that depositing the data is important. We want to go along with the standards that are being put forward by the granting agencies, but where do I go? There are 2,600 databases that might be relevant to them. Which ones are the ones I should actually point them to? Computer scientists actually look at this landscape and think other thoughts. They think, well, which ones are relational, which ones are MongoDB based? Which ones have this or that kind of schema? How can I get data out of this or that one? They're looking also at standards, and which ones are structured in a particular way. And again, Susanne [INAUDIBLE] I would encourage you, if you hadn't seen the last lecture to see the last lecture in this series. But she did a very nice job discussing some of this in terms of the standards of databases that are being used through bio sharing. So if I'm a biologist, there are 30 databases relevant to me. How do I navigate, inquiry this number? Because we have to remember, databases are not completely standard. In fact, they're very likely to not be very standard. Each one has its own interface. Each one has its own organizational system. Each database has its own caveats. Each database may reference a completely different standard in terms of how it stores its data. So how do I navigate this mess, essentially? And this is where we actually have to learn about the data warehouse. And the data warehouse is essentially a whole bunch of databases put into one system so that somebody can run a query across those 30 databases. But of course, in a data warehouse, what we can actually start to do is we can ask questions of all these multiple databases. We can ask a question such as, which Parkinson's disease causing snip, this [INAUDIBLE] house, that may have a homologous mutation in a worm? This is something that can be asked of three, or four, or five databases together, but not of any one. Because one database may not have all of the right links, all of the right connections, to ask that sort of question. So a data warehouse starts to broaden the scope of a particular database and starts to connect the different databases based on different rules in terms of how a particular set of data can interact with another set of data. How can a particular database talk to another database, and what are the sorts of things. So here I'm showing you an example of one data warehouse system. This data warehouse system actually, the thing I have highlighted here is the RID strain data for worms. This is a worm base table, which talks about the genotype of these particular c elegans. And within this particular database, which is a data warehouse, there's a lot of other data, including neuron DB. There's data from worm mine. There is data from homology, and there is data from all of these other databases. But then as soon as you start to hold this amount of data and this kind of data, the thing that happens is you're very, very concerned about when that data actually came in, because it might be old. What were the data transformations? So there is something on worm base. Now, we made some assumptions in order to create these particular tables. Those assumptions need to be absolutely explicit. Those are data transformations that happen between what worm base says and what we say worm base says. So there are these assumptions that are made. There is a provenance in terms of when the data entered our system when the data was released from the worm base site is another set of provenance, right? So there are a lot of things to know when these are metadata about the particular database table that you're looking at. What am I looking at? Very important to know. So these are all very meaningful things that need to be answered. So when you do this though, let's say we want to put together 30 databases. Each database basis makes a lot of different assumptions about the data. If I want to do this myself in my lab, will my programmer computer science colleagues actually deal with the kinds of complexity that a lot of these biological databases house and contain? Will I be able to interact with them well enough to get them to understand what it is that the data actually is supposed to mean so that the structure can reflect that properly? So data wrangling is a term that's been thrown around quite a bit. But we're starting to have an understanding of what a data wrangler is. But in terms of what is it, it's a process of extracting data from both tools, databases, and basically putting it into your tool or database or warehouse. So it's a data stewardship function. But if we want to look at the job description, what must this person really know? If you have a data wrangler, what must they know? Well, this is a fun job description. But this person must have a deep, deep knowledge of the meaning of the data. So a Ph.D. In biological science really helps. Because that person would be expected to understand the caveats of that data. Deep knowledge of clinical and genetic databases. Each of these databases, again, has its own caveats. They all take a certain kind of data, they don't take other data. They can be queried in ways that others cannot be queried. Data structures are also very important to understand for this person. So computer languages, statistics, Excel and Arc all really, really important, as well as a knowledge of ontologies, the deeper the better. So the kind of person that needs to do this wrangling is the person that sits between computer science and biology. That person must have a deep understanding of all of the ways that biology has of dealing with the data, and all of the little things that can go wrong with that particular data. And of course, the computer science, which is the structure. How do you now pull this so that it's properly structured into a warehouse? So the kind of things that biologists do, which end up having to deal with a whole lot of wrangling of data-- and there's a lot of data wranglers over at Craig Venter's company, Human Longevity, which just produced this very nice paper in proceedings of National Academy, where they've just released the deep sequencing of 10,000 human genomes. You can imagine, this is 10,000 data points for each particular genome, and each genome will have many, many, many, many, many, many, many millions of data points in and of itself. So making sure that getting that right, putting everything into one single warehouse, is actually the work of a large number of data wranglers. So let's take this down into a more understandable level. So if we think of data wrangling as doing laundry, there are these terms called manual curation. So the way that we can think about data wrangling, if we're thinking laundry terms, is manual curation is sorting everything into appropriate piles. There's something called semantic mapping. And semantic mapping is assigning the proper word to each of the piles. So here we have shirts in one pile and socks in another pile. So these are two terms. Now these piles are semantically mapped. There's automatic indexing, and I think you heard a lot about that previously. Now we know that there are 10 shirts and five socks. Wonderful. And there's a resource identifier. So the research identifier is something I also want to spend a little bit of time on because that's how we can trace back to what we actually have in the data warehouse. So if we look at semantic mapping first, semantic mapping is essentially adding a term to a particular data item. And there are different ways to do that. Again, Mark Musin and Suzanne [INAUDIBLE] had actually done a lot of the work in terms of explaining how some of these terminologies get put onto different data systems, so I'm not going to talk about it a whole lot here. But an ontology is essentially a very, very structured glossary or thesaurus. It's essentially a way of saying this is the thing that I'm talking about. And the semantic clarity of an ontology is very, very high compared to even a taxonomy. A taxonomy is something that we understand. We know that mammals are related to rats. And we know exactly how they're related because there are different steps in between those two terms. But an ontology is even better than that. So for example, here when we look at an intelligent of socks, we know that a sock is a type of clothing and clothing to it typically has part thread. So these are all terms that can be related to one another. Now if you say I am going to label my particular set of data with the word sock, now if I do it with just a controlled vocabulary, that's great because then you can find all socks. But if your database has thousands of different items or millions of different items, maybe you want to look for all things that are clothing. And if you want to look for all things that are clothing, if you mark your data with a controlled vocabulary, that controlled vocabulary doesn't know that sock and shirt is related. That controlled vocabulary doesn't know that sock and shirt is a type of clothing. But if you mark with an ontology, those relationships are inside of the system of tagging. Which means that what you can actually do is you can ask a database or a database system for all clothing. And I'll show you some of that in a little bit. Well, not with clothing, more of the genes. But OK. So when we also look and think about the stable unique identifiers, this is a really big deal. There's actually a big conference coming up in Iceland next week. But here is a set of numbers that you may or may not recognize. There is a telephone number. And in my telephone I see that number, and it has a little translation. I used to be able to remember these numbers, but I don't anymore because now my phone says, oh, it's your mom calling. Great. So that number has been translated by my phone to a label, Mom, or Jane Doe, or whoever. We don't know this number, but we should because we go to it every day. This is the NCBI server at the National Library of Medicine, which houses PubMed. So we're on this particular number a whole lot. But when we type in Google, when we Google the word PubMed, this is actually the number that we're hitting. So each database server has a number, it's an IP number. And we no longer understand that that number exists, but believe me, it does. And Phil Borne here, in this particular perspective, has told us that essentially each of the items needs this kind of number. Data software narratives, all sorts of things, must be uniquely identified. They must be sharable. Because this is how computers talk to one another. So identifiers-- all the paper identifiers of us, identifiers of items within the paper-- really can tell you a whole lot that pretext or beautiful prose may not be able to tell you if you're a computer. So we have some nice examples here. The re15869 is a snip inside of the BRCA2 gene. The NCBI tax on 9606 corresponds to human. So if these two identifiers are actually inside of a particular paper, a machine can tell you that this paper reference is a cancer causing snip in humans. Now if you're reading one paper, not a big deal. You can probably read that. But it turns out that about a million papers are published every year in PubMed. More than a million now, actually. Which ones do you really want to read? Is there some way that we can know better about which ones we want to read? Maybe there are some colleagues that you didn't know you had from another part of the world. We actually might need to get some assistance from Siri-like robots. Siri helps me navigate my way through different parts of different cities, tells me which restaurants are reasonable or within a particular price range. Why can't Siri help me find good papers on PubMed? So stable, unique identifiers are way that we can actually start to understand publications free text without doing a whole lot of data wrangling. Usually curators and data wranglers turn text and data into linked data objects by adding these kind of identifiers, but anyone can do this. So for example, ORCID is a researcher ID. If you do not have your ORCID, I would highly recommend that you type ORCID into Google and you try to find that. Because ORCIDs are a way to disambiguate the John Smiths, the Wangs of the world, and even people like myself. I certainly have an ORCID, and I put all my papers into my ORCID record so that I understand what is my intellectual output. And I would encourage again everyone to do the same. Grant numbers are another type of number, and I know that NIH has been asking for a very long time about making sure that we get the grant numbers, and we put those in properly, and they go into PubMed, because then they can ask very straightforward questions. What has been produced out of this particular funding mechanism? What is the paper, what is the data that has been produced? So these are things to tag a particular paper. DOI is a way, again, to tag a particular paper, just like PubMed ID, those are both nice identifiers. GeneIDs will tag things inside of papers. SNIPIDs will tag things inside of papers. And those kind of IDs actually disambiguate what it is that the author is saying. We have another type of ID called the RRID, which tags the research resources, the antibodies, cell lines, organism stocks, of a particular paper. Again, so that you can ask fairly simple questions. Which gene were you really talking about? Which snip is that? Is it this particular codon, or is this codon? We don't know that unless we actually put in a snip ID. For RRIDs, antibodies, we all know that antibodies are really hard defined inside of papers. But an identifier, RRID, when you look that up, add it to your paper, it starts to become like open data. That paper starts to essentially add to the total amount of knowledge about that particular reagent, about that particular cell line. And we know that this is one of the big pushes to disambiguate some of the things that are inside of papers and to make them more understandable. So now we can talk a little bit about the closed world versus open world. These are assumptions that are being made by databases. A database assumes-- especially a relational database assumes-- that the answer to your question is inside of the database. This is what's called the closed world assumption. How much money do I have in the bank? You have x amount of money in the bank. That is the kind of question that a database is really, really good at answering. Now if I want to know all genes related to Parkinson's disease, that's a little bit of a different question. That's a biological question. And it assumes that the answer would be most likely not in a database. Certainly not in one database. Maybe not even in the totality of all of the databases out there. So the literature actually covers a small part of all biological knowledge. And traditional databases still, even with the multitude of these databases, only cover a small part of what's known in literature. So when we ask databases big open world questions which really are biological questions, we might be running into some assumption problems. Because we want to know as biologists the open world things, but databases are very, very good at telling us about the closed world, about the world that's inside of the database. So exploration and visualization. It's not actually completely useful. But let's see how close we can get to an open world with some of the bigger databases with some of the warehouses. So when we start to look at data analysis, and we start to explore it, what are the sorts of things that you can do when we have a lot of data together? And this is one of the gap analyses that we did with the Neuroscience Information Framework data. So each of these columns actually represents a single database. And some of them obviously have more data, some of them have less data. So that the darker the green, the more the data. And then here in this particular column, there's a whole slew of labels. And now each of these labels is related to the brain. And in fact, some of them are actually linked to forebrain, some of them are midbrain, and some of them are hindbrain structures. So we understand that. And there's the magnitude of the data. So when we look across all data sources-- so this is around 100 and some odd different data sources. And we say, well, what do we know about any structure in the forebrain, versus any structure in the midbrain, and any structure in the hindbrain? Do we know more about forebrain or midbrain? It turns out that if you then count up the total number of data points that are essentially known about the forebrain, it turns out to be much, much larger than the data points known about the midbrain. And here's the thing, though. When we look at connectivity matrices in the brain, it turns out that the midbrain is actually really highly connected. So we should know more about it. But when we look at the data, we don't know as much about it as we do about forebrain structures. So because we have an ontology where we know what we should know, and because we have a lot of different databases all in one system, we can actually know what is labeled, what is a part of the data structure in each of those particular databases. Then we can actually start to do a gap analysis, which is, where should we look? Perhaps the midbrain would be a great place to look, because there isn't as much data covering midbrain structures, and they're likely very important. So we can do a lot of other fun things. There is a concept of a homunculus in neuroscience, and a homunculus is the representation of the body on the brain. So each animal has a representation of the skin of the body, and we know that when we actually redraw this representation of the body, it's not exactly proportional to the skin that actually covers it. So we know that the lips, the tongue, the hands, the fingers are very, very well represented in the brain. So they're drawn in a homunculus as very large. Whereas you know the back and the stomach are actually very small compared to the amount of brain that actually covers understanding and processing of the information from, say, the lips. So there is a homunculus in data. So each of these points-- if we look at each of these points on this particular graph-- is a disease term like Alzheimer's. And there's lots and lots of disease terms, because there is an ontology, so it's a very long list of all sorts of disease terms. And so what we've asked is let's look at all of the terms across all of the databases. And let's plot that versus Reporter. Now, Reporter gives us an idea of what grants have been funded, and they also cover disease terms. So it turns out, now I haven't done a full correlation on this, but it looks pretty darn good that if you fund it, data will come. So it turns out that if there are grants that are actually funded to look at a particular disease, you tend to get more data points. So the data points are here and the number of grants funded are here. So that kind of tells us that what we fund is actually very, very important in terms of what we see in the data space. And I want to hit this one more time. Comprehensive indices are usually the most critical to biologists. So one such comprehensive index is the antibody registry. This has about 2.5 million antibodies, which is about as many antibodies as we can probably get to. There are certainly some things missing, but it's something that's being worked on. But you can ask certain questions, such as, well, let's see what all the different genes are, and this is running the 30,000 human genes, this is down sampled, across any results from the antibody registry. So how many antibodies are there for most genes? Turns out for about half of the genes, we actually have between 11 and 100 antibodies. So there's pretty good coverage of this type of reagent for anything you might want to hit. Some of the pseudogenes, and the predictive genes have not been covered. But most of the genome is very well covered by lots and lots of reagents. So fun things that we can know. So databases really provide structured data, and therefore they can tell us what data is available. Warehouses start to actually show places where there is no data. But what if we don't have a database to cover the thing that want to know about? What if there is only text? Well, this is where we would get into natural language processing and a talk in about five weeks. So natural language processing is a whole field of computer science. Deals with artificial intelligence, computational linguistics. It's very, very concerned with interactions between computers and human languages. So remember your second grade teacher, your third grade teacher made you do these diagrams of sentences. Well, it turns out that computer scientists have been able to train computers to do this. So had I known that I could have cheated in second grade, maybe I would have done it. But now there are lots and lots of different methods. The Stanford parser and others are available to researchers that would like to figure out how to diagram sentences. There are also a lot of statistical algorithms, like LDA and others, that actually show which part of a particular passage, which part of a paragraph is more important than the other parts. So they pull out key words. And so what I wanted to show you is just a couple of examples on how this is being used today inside of biological databases. So one of the issues with functional magnetic resonance imaging studies is that everyone tries to tell us which parts of the brain lit up, which parts use up more glucose, when a particular task is being done. So now there's a lot of people who do this kind of work, and they all try to tag these different parts in the brain. But getting that information into a database is actually relatively difficult and really time consuming. So Neurosynth, Talia Coney's database, has actually utilized a lot of text mining in order to make this process much faster and simpler and create a complete dataset. Not maybe as accurate as some of the human curated, but certainly it's complete because it houses all of the papers, and it basically gets papers into the database very, very quickly. So what it does is it removes a lot of the talairach coordinates for the particular places in the brain where the activation is actually being reported. Then it does a bunch of text mining and statistical analysis to figure out which are the most important terms in that particular text that are being discussed. So here is an example for working memory. Now, working memory has about 901 studies that have been reported on it. And Tal's work essentially shows that this particular term is most relevant to the particular fMRI images, and then he replots this onto a human brain, which is great. So here are the activation foci for these 901 studies, and they actually do cluster. So even though the data is coming from all kinds of papers, and even though this is all computer generated data, the computer is actually pulling all of these 901 papers. He didn't have to do this manually. But what happens is, we can actually see a pretty good clustering of what are the sorts of things that are being reported about working memory on the brain. What are some of the other applications? Well, there is a program called Textpresso, and Textpresso's very cool. There are lots of other tools that can actually do this, there some that are commercial. But this gives you an idea of what are some of the other things that text mining can do? So when we have text, we can diagram sentences we can figure out what's a noun we can figure out what's a verb, et cetera. But what we can also do is we can figure out what are the terms inside of that particular paper that are relevant to me? And I can ask questions such as, give me all sentences, Textpresso, that you know about which list Parkinson's and a brain area. Now, I'm asking for any brain area. I'm not asking for the word, brain area. If you typed this into Google, you will get brain area Parkinson's. But what you're really asking is, I want Parkinson's disease, and tell me which brain areas are most relevant to it. So here's a sentence, one of the many hits that Textpresso gave, and here is a match for a sentence, Parkinson's, and talks about the substantia nigra pars compacta. Now, how do we know that the substantia nigra pars compacta is a brain area? Well, remember back I showed you that ontology? Well the ontology says, substantia nigra pars compacta is part of the brain. So it is a brain area. So what we can do is we can have large lists of terms that are related to a particular thing we want to know, like brain area, gene, expression type, et cetera. So there are large lookup lists which we can actually feed into a text analysis system to light up the kinds of things that we might want to know about particular sentences. And again, we're producing papers at a million papers per year. Something needs to help us sort through a lot of this data. And systems such as text mining systems are actually starting to get pretty good, and are able to show us some of the things that we might not otherwise know. So with that, what I wanted to do is I wanted to thank you very much for listening, for tuning in. Thank you for the opportunity to talk to you all, and I hope you tune in for the rest of this series. Thank you so much, Dr. Bandrowski. We are out of time now, so we won't be able to take questions. But I think that's OK, because you were so incredibly clear on this, and we all really appreciate this talk. So thank you very much. And to everyone out there listening, thanks for tuning in, and we will see you next week. Bye bye. 