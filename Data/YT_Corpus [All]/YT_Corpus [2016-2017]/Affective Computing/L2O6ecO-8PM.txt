 all right hello everybody my name is Sam and this panel will be about tracking emotions as you've seen in the in the booklet I actually found out about state festival when I was in an Air Berlin flight and I opened up the magazine and I was like wow that's cool I got to be there so I contacted the guys from states festival and they invited me over to moderate this panel and also be giving a workshop tomorrow I think the banner will be very interesting it's very it's a topic that's living at the moment you see a lot of changes in the landscape I am part of this change so I'm part of a startup called 12 graves and we are doing recruiting intelligence and what we do is we measure micro expressions in the face and tonality your voice and we try to fit people in an existing team based on values and the United States is a large problem people are leaving their jobs within a year very quickly the reason not because of skills but because of a lack of values so this is what we try to tackle by measuring these emotional metrics obviously the tracking emotions is becoming more and more prevalent in society i was at the microsoft building in prague about a month ago and they have cameras all over their building tracking micro-expressions to see how people feel in the different rooms they're doing this in their stores as well so for marketing it's used a lot and today we have three speakers lined up that are experts in the field will be giving their talk and explaining what to do first off we have Maya pontic my ass right here and she has a PhD in computer science which he got in Delft and currently she's working at Imperial College in London where she his head of the I bug group and I bug stands for intelligent behavioural understanding group and she is focusing on the analysis of human behavior including facial expressions body gestures laughter social signals and affective states so without further ado another plus for my Atlantic good afternoon everyone so today we will be speaking about this technology that goes actually under really various names and sometimes we talk about it as emotional robots because the technology is integrated within the robots affective computing is a very classical term but they really like this emotional artificial intelligence because in principle it is about artificially or by machine understanding emotions and attitudes of people so behind this technology is in principle automatic analysis of phase and facial behavior the face the human face is really fascinating if you think about it so we are using it to recognize up other members of our species but we also use it to judge various things such as for example age and gender beauty and even personality but what is really most important is that we are using it to analyze the inner state of humans so in fact facial expressions and facial behavior is the only fully observable window to that inner world to our emotions our intentions attitudes and moods so it is therefore not surprising when you think that if we could analyze automatically who the person is and how that person feels we would be able to apply this to a very wide variety of applications some of which I illustrate here when we are talking about automatic recognition of people so person identification in principle the state of the art is such that we can do this except tionally well even if the faces are reputed or under very difficult illumination for as long as those faces are upright so in frontal view very similar is the case for facial expressions so we are capable of tracking facial expressions and understanding different gestures like smiles and frowns and even as Sam mention some micro expressions for as long as the face is in frontal view we can even analyze a higher level behavior such as for example interest or boredom but again this is all possible for as long as we have more or less frontal view but this is not the case when you go to the videos which people usually upload in YouTube and Facebook in these completely unconstrained environments sorry about this we still have great difficulty of understanding all those different facial expressions and so in order to address this problem we will definitely need a lot more data data which is collected in such unconstrained environments or from the videos uploaded in the wild like in on YouTube and on Facebook and we would then be able to train models facial models to be able to deal with these drastic changes in head pose I mean when people say to you we can automatically recognize facial expressions and we can apply for commercial purposes be wary of that this is not really true they can do it for as long as you watch the camera but in principle if you start moving around they cannot do it current technology cannot really do it so we also need the much better inference models those are the models books take the context into account so not only who the person is that's very important contextual question but also where that person is and what this person is doing so for example if I raise my eyebrows right now it means that this is important for me I'm emphasizing this point but if you do it it might be just the question is it really important so the meaning of the very same gesture will be completely different depending on the context currently we do not have context sensitive models this is something we should be working on however even though the current technology is not really fully mature and we cannot apply it in all possible applications we can still apply it in many applications a very successful application about which Elmer will be talking in a little while he is coming from this company called realize is automatic analysis of successfulness of an advert or a product and how likely it is to be bought or to be set sold based on behavioral reactions to different products and adverts another application on which we worked quite a lot is automatic analysis of pain this is very important application in medical purposes because if you would be able to recognize the pain automatically you would be able to build a different kind of therapies automatic therapies where people would be guided through these therapies by virtual agents or robots or simply machines and these the reason for this is that we simply do not have enough personnel that could do that because our population is getting older is aging it's also that we are just having way too many people and for example in UK we have a huge problem with lower back pain because most of the jobs are services so people actually sit along hours and they suffer a huge leap from lower back pain in fact 60 Plus percent of people suffer from it at least once in lifetime so there is simply not enough personnel to help those people so that's why this automatic machine machine based therapies could help this got the attention of IBM recently and we are currently working on building the center which is called artificial intelligence empowered health care where we want to have a remote consultations between the doctors and patients this is especially the case for primary care because there is huge waiting times in UK where we have the waiting times from two days to three weeks for anything what is wrong with you which means that some people never actually come to to the primer care and suffer quite a lot so in order to speed it up and speed these processes and there are some current services that allow webcam based consultations however the problem there is that doctors cannot really very well observed the people and they are not so often familiar with people so we want to change this and to actually have automatic analysis of things like pain for example or fatigue depression is another one which is really important but also to provide to the doctors the medical records of the patient with whom their consulting because currently they in fact do not have automatically these records available which is really horrible so we are currently working on that another I mean this was quite a big topic and he has been reported by the CBS 60 minutes just last month and another topic on which we are working which is also from the medical field and it's quite a good application of robots you know you probably heard about that usually when people talk about artificially the agency's always this classical trashing of the technology and also fearing the technology so you will hear most often the robots will kill us all and you have the war against us and all the movies are about that like there are very few things that say that this kind of technology can truly help us and this is a really good example so we are working with autistic kids the little ones who have autism cannot recognize facial expressions as we do because they miss Gestalt consultees actually understanding of the face as a whole they understand or see the face as a set of parts so if you would change a very very small gesture on your face they would see there's a completely different expression so they have a great difficulty in understanding our expressions and also hence to learn how to express those emotions that we expect them to be able to express in a way we would understand so in order to teach them that we built this system within the robots one robot at this point that can show the expression and encourage the child to show similar expression as the robot does and if the child does not succeed try to actually motivate the child to do it again so they prefer robots because of this consistency so the robots never change this expression because they programmed to show it in an exactly same way and we are also working with IBM on another problem and that's the mythic analysis they're very interested in having remote meetings between their own members are a very large company they have 400,000 people employed in their company but also with having different meetings with different clients and so on and often people cannot come to their premises so they can do that remotely but they're interested in things like and what are the points of the meetings where people agreed when people disagreed where they had a good discussion where people were enthusiastic and interested and so on and this is very new we are working with a company that just started it's called love lens it's a company that provides the online video dating this is the first company that provides dating based on videos as opposed to all dating sites which are text-based and the video based dating seems to be preferred because people can see each other in a very short period of time and if they like each other they can see each other again via that video or exchange phone numbers or whatever and if they don't they don't do it and this actually circumvent all these problems of putting the pictures which are 10 years younger than people are currently 20 kilo kilos less and probably much higher and so on and so forth so we are trying to help him with with that and the last application but which i will talk is in autonomous vehicles we are working with Ford to try to understand fatigue stress attention of the drivers this is I don't know if you know about it but Tesla had a car crash last I think April or something and the main reason for this was actually non intention of the driver and although all the instruments were telling that actually the camera is blurred and cannot see what's happening in hence that the cars start going under this trailer of a track passing by and the driver just didn't pay attention and got killed this got really big in the news and so on so people need to find alternative ways of checking whether the driver is actually attentive to the equipment and road and so although we cannot deal with all and every application that is could be actually or could benefit from this technology we can deal with quite some applications this kind of technology is really interesting and important if you think about the future the future is this fourth Industrial Revolution which is all about artificial intelligence and about robotics and about actually cognitive empowerment empowerment of people meaning making people people's life easier and giving them more information than they would be able to read or get in an easy and fast way on their own so just giving them the the information and the way of dealing also with other people and just purely data so with that I leave you thank you very much for your attention Thank You Maya the next speaker we have coming up is Ellen R yet let me know if i pronounced it correctly and my already touched upon it she talked about realize and Ellen R is the co-founder of realized and before this he also created the first online beauty pageant in Azerbaijan which attracted millions of viewers got nationwide coverage and Ellen are will be touched touching more upon the business side of things with regards to emotional metrics and I think the presentation should be up at any moment be coming up there it is okay okay a round of applause for Ellen R thank you Sam thanks everybody so as i mentioned i will be talking about one particular application of emotion measurement today so during this morning with spoken about the history of emotions and different ways of measuring emotions and what i would like to to present today is a particular real-life application that is happening has been happening already for quite a few years and the value that emotions are already giving to two people and companies that use this technology that we develop so i won't be talking about a company called realize that i co-founded several years ago and what we do is in one sentence we measure emotions to guide better marketing decisions what does that mean well i don't know if you know but over ninety percent of human behavior is driven by emotions we are emotional creatures and for businesses it is important to understand how people feel when they're providing their services and there's there's more and more need to have an efficient measurement of emotional impact on people and that's the service that we're providing so let's take as an example advertising industry people don't hate don't just hate the advertised advertisements they hate really bad advertisements the ones that interrupt during watching something really interesting advertisements that are annoying and and popping up in them you know not not the best moment on the other hand if an advertisement is touching is joyful is engaging that's something that people want to see more of and that's some something that people would be happily sharing as well so so we're we're helping both companies to build better video and creative content and also hopefully that people get less of the kind of advertisement that they don't want to see or videos that they don't want to see so how much how much of it is already available and what what is needed to to measure how people feel when they're watching a video content well all you need really is a web camera advice with a web camera and a device that's connected to the internet and then we can you can already participate in an emotion measurement study and we've been doing that for years we've accumulated one of the largest databases of recordings of people reacting to various kinds of video content online in their in their natural environments we have won over 1.4 million recordings done so far people watching more than 8,000 videos across the globe in 1 187 countries last time counted and who are the companies that using this service well there's an increasing increasing demand in emotion measurement technology and many of well-known companies and brands are using this to oh gee companies like added us Procter & Gamble Mars HSBC Phillips and many others are using a motion measurement to make their videos better that come out of their production to choose the better videos to be used for advertising to choose the audience that is more engaged and more interested in the video content that they're producing and to evaluate overall emotional impact that they're making on the society when they're releasing their video content so how exactly does that work how exactly can somebody use our technology to measure the emotional impact on an audience well it all starts with the video that you want to test or several videos you come to our platform you upload the videos specify the demographics that you want to test it with it can be as simple as just choosing the country in which you want to have it tested or as sophisticated as providing age gender and various other demographics parameters of the kind of audience that you want to target and then using special online panels these are online databases of people who've subscribed to participate in such tests using those panels were able to invite them to take part and in the studies and all they need to do is once they receive an invitation if they want to take part in the test they open it up in their browser they don't need to install any special software for that we explicitly ask for their permission to have access to their webcam and after that they watch the video and we'll record what their reactions are all of that gets transmitted to the cloud analyzed and in near real-time appears on the delivery dashboard that our clients see and when there is enough data collected usually between 24 to 48 hours to collect something around 300 different recordings then clients can start already making some some analysis of what the audience is feeling about the video that they have to give you a little bit better understanding i'll show a short video clip that explains how it works so when people watching a video our software registers their facial expressions something that we've talked about quite a bit today and those expressions then get aggregated and shown on our dashboard and there you can choose between different expressions you can see what the results are second by second and generally our our dashboard provides various tools for people to analyze them and see the emotional response from different perspectives so this is how typically how the dashboard would look like you have here on the left all the different emotions that you can see measured in this particular study you can slice the data by let's say gender for example you can see how different genders react to the same video content in this in this screenshot there are two videos being compared so you can compare the the emotional reactions for two different videos you can also refer to a normative database so it's a database of all videos that we have so you can check how does your video compared to some of the competitor videos for example to understand how well you are you're performing so if we so let's let's dive in and see how how would companies typically use this information one it's once it's bill collected it's been collected so one example is that of course you would pull up the emotions and then start zooming in at various peaks of emotions and see what's going on there what kind of scenes in the video does it correspond to and for example if it's a peak of a negative emotion then maybe you want to see that was this a desired effect that you wanted to have or is this something that you were unaware of maybe this scene you want to cut out so that's one example another example would be to see what's the overall trend of for example an overall emotional engagement most of the time when when you want to have an impact and emotional impact with the video then you would be looking for an upwards growing trend if if your trend is upwards growing then you're doing the impact that that you want to have and you can see what's the what's the trend of particular emotional overall emotional engagement we also have tools that based on the emotional measurements that we have and an emotion particular emotional patterns which we call which we call attraction engagement impact so so all these combined together in a simple scoring mechanism and using the scoring mechanism you can see and compare how different videos compare to each other in terms of the emotional impact so this is what we call an emotional score and here's an example of an emotional score of five different videos that 11 customer was was comparing and you can see that clearly that CT rivalry is is the top performer overall because it has the highest emotional score but also in the different countries where it was tested in this case it is UK Germany and Spain you can also see that while the other videos were less performing perhaps you have invested quite a lot of effort into producing these videos and you don't want just to disregard them you want to try to see what's the optimal way of showing airing the videos and here you can see for example the mission statement video number 3 from from the top although it has overall low scores in UK the score is almost as high as the top performer so you could still go ahead with this video in UK and get quite a good emotional impact that you might be after so these are some of the examples of how the dashboard that that our product provides can be used there's several other tools and mechanisms of analyzing the data of course and people who usually work with it they are they are the experts they know exactly what what's the goal of their campaign what's the goal of the video content that they want to air and so they know how to best analyze the results but really one question that almost everybody is asking is okay so we have we have these emotions measured and if we take all the complexity of the human behavior and we try to analyze that is there a way that we could show a link between the various kinds of emotional behavior to the performance of the video is there a link that would with confidence give us how a video is likely to perform and by performance here I mean for example performance of a video on a social media so how many YouTube likes or shares would a video generate is there a way to try to predict that information from an emotional reaction of people or another example would be if it is a social awareness campaign so is there a way to predict how many donations with will this campaign or create by the way that people are emoting and reacting to the video that's something that we are in a good position to try to answer although the question is very difficult and the reason why we are currently in a good position to to start looking into this question well because I believe there are several critical components that are coming together to help us answer this question well first of all over the years of working in this business we have accumulated a really big database of people's reactions to video content that's that's one of the few unique databases one of the largest databases in the world and it's a database that holds a lot of secrets about us humans and about emotions and about our behavior that are waiting there to be unlocked and we have that database and we should be really looking and trying to understand it the other component is of course the the classifiers the algorithms that that can extract that information from from the pure recordings that can understand the different micro expressions and different movements of the face that's something that we're continuously working on and continuously improving our algorithms and one of the projects that Maya mentioned is called sever project which is funded by the European Union where we're trying to advance the technology of measuring human emotions not only just from the facial expressions but also analyzing the audio and trying to extract emotions from the audio together with several partners that we're working in this project so that's one one building block the other one is the videos themselves so we have a over 8,000 videos in our database and we we have a lot of valuable performance information about this video so some of the videos we have information of how much they're impacting the sale so once the video went on air how much did the an advertisement video how much did the sales grow as a result of it or as I mentioned earlier the social media performance results now getting all these big databases together and leveraging the power of cloud computing that can scale to to the levels that were unimaginable before and advancements or in in data science as well so putting putting these things together we can start mining this database for new behaviors and try to find find you find new behaviors as well as make performance predictions and this is something that we've already started doing and we have built a model that can predict with 75 percent accuracy whether an advertisement will have high or low sales impact seventy-eight percent accuracy whether a video is going to be successful on social media or not and sixty-seven percent accuracy whether a charity ad will will be successful and here 67% may seem like a low score but if you think about it these are all negative emotions and they are much harder to to track automatically so I'm really hoping that the future of advertising and the 2020s looking that like that with better understanding of our emotions and reactions to to video content and what really engages us there will be less advertising certainly less advertisements that are unengaging and not not interesting and my final slide I'd like to I'd like to finish with with a very relevant quote by American poet and social rights activist Maya Angelou who said that people will forget what you said people will forget what you did but people will never forget how you made them feel thank it's colder in here than I expected it to be really I'm sitting there like freezing a little bit so we had Maya with the scientific background of course an Ellen are more taking a businesslike approach the next speaker we have is Ruben from the Fen and he brings the creativity to the table he is a background in film making any programming and he recently graduated from the pits of Art Institute in Rotterdam where he investigated computational quantification and categorization of emotions so here's Ruben a round of applause thank you first of all I'm honored to be here at the State festival with so many people looking into emotions and doing research in that I already learned a lot today from the talks this morning and also the exhibition upstairs if you haven't seen it this really it's really cool to be among actually some works that I last year still had references when I had as a reference when I was developing the work I'm showing now so that's really cool um so that's that's this work emotion hero if you haven't seen it see it upstairs or download it if you have an Android phone so well over the one lost about one and a half years I've been looking into this technology and well it's already introduced a bit by by the previous speakers and so I think I don't need to really get into what it what it does and rather am I want to use my sort of background as being an artist researcher to sort of broj do not stick into one field but rather sort of use my knowledge from boro my knowledge actually from other fields and sort of combine and it's one story also I will take a bit more criticals perspective while using that and so so um the first thing i would like to compare it to i will make two comparisons to start with the first one is and now the presentation is working so i need to get the video here it's not showing should I just okay that was so the first comparison is with the Kuleshov effect and the Kuleshov effect for those who know it in there who don't know it I should in the beginning of the 20th century left Kuleshov a Russian filmmaker did an experiment into emotions in film and what did it is that he added the same face into three sequences namely that of that of a dad dead woman a plate of soup and an attractive woman and what he noticed is that if he showed that this clip to the audience's that in each instance the audience thought that actor looked really convincing and that he was looking very happy very very full of desire very sad very hungry and what is remarkable about that is that it shows us that that emotions are are recognized by humans in a specific context now if we take and I go back to my presentation now if we go look how the software emotion analysis software looks at these images this is from Microsoft and they break it down pretty much all software companies use the same emotion skills namely dose by Paul Ekman the seven emotions and it mainly recognize it as a neutral face it hardly sees any emotion there you could say that interpreting the face is neutral is more precise than describing any motion to it because yeah maybe if you look solely at the face it looks neutral however if we look at the context in with the in which the person is supposed to be then he probably is sad or he probably is hungry so that context really depends how we should read that face and apparently the software doesn't do that yet um now another example is this musician here and he's picked up by the software as being very what is it very sad and very angry or somewhat angry and whereas if you're in the audience you probably have completely different feelings I mean this one is not picking up any any sense of joy are pretty much whereas if you're in the audience you do feel a sense of joy when listening to it and you described as an audience that feeling on the person um now I haven't double-checked it with him but I'm quite sure that there was a sense of joy when he was on stage um now what for me is important here I mean it seems a bit like I can just show another 20 examples of mislabeling of the software but that's not my point here my point here is is that these miss labelings are not just accidental mistakes but more fundamental to the procedure but I will get back to that later first I want to go to the to the second comparison and the second part I will look into the definition of what is emotion already this morning it was pointed out if you were at the talks that emotion the definition of what is an emotion is really troubling that it's you know is what is an emotion and it's a feeling and the feeling is effect and the fact is an emotion so it's sort of it's hard to grasp what it is and it's used in that way bye-bye psychologists etc and their daily practice however what happens if you try to sort of delineate and quantify this this very vague and fluid term once you get into that I want to showcase the example of hysteria which was in the end of the 19th century in France chaco was yeah had a hospital for females because hysteria back then was seen as a feminine mental illness and and there he tried to sort of research and cure hysteria in his patients but to do that in a scientific way he needs to have a definition of what hysteria was in hysteria like I said was seen as something that was feminine mental illness however that was a sort of yeah common use in in everyday language and even though they did endless observations really try to sort of do a lot of data gathering yeah writing everything down this sort of the definition became really vague and all-encompassing because the yeah it was as we know now a non-existent thing that was trying to be captured also photography in those days which was a new new invention and it was really seen as an objective technology it was thought that if we put people in front of a photograph what it captures is highly objective however the camera might might be objective in a way but the doctors it's at the doctors and the photographs photogra first they they sort of thought sought for specific cases that they want to have in front of the camera and by preferring the deviant they they sort of encourage the behavior in other patients as well and through that they sort of substantiated the yeah the concept they were looking for now if we compare that with with emotion these days you sort of see might see something similar happening with this technology that something which is is highly highly fluid and and hard to delineate when it's try to be quantified you need a concrete and a fixed definition what's happening though is that now I get back to that later so if we look at these these practices of data practices can we compare it and that's really a sincere question I have can we compare it to what happened in the historical perspective with hysteria that these immense gathering of data can be seen as a way to sort of substantiate the act of labeling emotions and labeling using the seven emotions as defined by Paul Ekman which I think are troubled in the problematic in the first place as was also pointed out this morning that they are often very context specific and much more nuanced than those rigid basic 7 as they are called now why is this relevant and over the last few months I've seen already quite some anella analyses of Trump and Clinton pop up in their debates and sort of try to pinpoint who feels what and what does that say about them now this is this is only your first instance I would say from a more general use of the technology as also a few years ago a research has been done using similar technology on CEOs noting a correlation between facial expressions and revenue of a company now if CEOs are aware of that and they have access to the technology which they have because this is just you know some cases just available online then it's not not an amount in my imaginable that they will start to use the technology to train themselves in their interactions this might seem far fetched but it isn't because already a company called hirevue they use this technologies among other attributes to to find people for a job to find job candidates and simultaneously the same company uses the tool to train people to behave while in inner conversation so that they use a tool to find an objective truth while at the same time using the same tool to train people to well be look better and that is I think the in vient the the part of my story is that if you look at at these images which in this case are from Microsoft but I think most most companies use similar images to show that their tool works you know these people look really happy those that man looks really surprised however if I see those images I see stock photographs and when I see stock photographs as he acted people who act and although it's often claimed that these technologies reveal what we really feel I would say that is in this case it reveals what those people on the photographs set out to convey um so um i would say rather than sort of giving us new insights into what emotions are and how we use them into our daily daily interactions it sort of reinforces an existing stereotype so in the end yeah like I said my question is how do these practices of photography being a new technology seemingly objective relate to this new seemingly objective technology or presented at least as being objective the cloud so to say so then again as a final bit I will pluck this again the thing you can find upstairs and what's for me essential there is that if these tools are indeed used to train people you sort of the the facial expressions become much more mechanical and actually the tool that in the first instance sets out to well have autists because I think it's pretty much started with autism research have autos better understand emotions have a general public better understand each other encouraging a more sincere interaction actually leads to the opposite namely train sincerity so that's it thank okay we have about half an hour left so 15 minutes will be a discussion between the four of us and then 15 minutes to get some questions from the audience as well what I find interesting while I was preparing my questions as well is that this the measuring of these emotional metrics I can think of a lot of examples where it can benefit society as a whole for example the way we use it in our company we try to match people on their value and in the end you have people working together better same with the example you gave with regards to the advertising industry in the end the goal is to get advertisements that positively influence these people but right before we started the panel we also talked about potential dangerous aspects of its and ruin you touched upon it as well in your presentation where emotions can be context-dependent in this case so this is the direction we can go in for example Eggman gives his his seven basic emotions but emotions can be much more nuanced in certain cases and you see these large companies like Microsoft for example that use put cameras everywhere where products are being sold or even in their office buildings to look at the emotional reaction of a person there in order to influence that positively but can you maybe what are you guys have starts with the the sort of dangerous side of it where do you think this can go wrong on like a large scale okay first I think I would rephrase a little bit what was sad I think icon has six basic emotion the seventh one was never really confirmed and these basic emotions are just the expressions of these emotions basic expressions which are actually not basic at all and people in real world life don't react that way so you can just watch youtube on Facebook or whatever else you you have videos that you will find very rarely these expressions all the expressions you have shown are totally exaggerated and so I think we should not talk about it but about the danger and I cannot see really if we say we will train people like this with you what you said about the motion here you will train people to express in a certain way and I think that is totally on not naturalistic so in principle if you would train people like that I think that I think there is a psychological research on that as well we would see that as something which is exaggerated and non-natural will actually not be trustworthy so one of the things for example in negotiation like 40 for interviews or so and what is really important is how much we mimic each other so and there are people who know about that and start making quite a lot and this is like spotted immediately because you you simply see that something is wrong is not natural and hence we don't trust so I don't think this really works so that would be my comment so far okay okay well dangers is of course an important question and like with any technology there are advantages and disadvantages that come with it and there are dangers as well as the exciting stuff that is associated there's many things you can think about in terms of Y emotions can be dangerous well if you if you can measure emotions and you can then understand how you can influence them then and you know you want to do it with some bad intent in mind then of course this is a very very dangerous Avenue to go down so that's that's just one example there are probably many more examples of how motion measurement technology can be dangerous but it doesn't defy the fact that there are also all these advantages that have been mentioned you know ranging gold away from making videos better to helping autistic children so I think the important thing is that we're aware of this that we're aware of where the technology is moving that we're aware of that okay now we have these capability that it's a long way to make it as good as humans are and bring in the context and bring in all other ways that we humans measure other people's emotions when we when we speak we don't just look at facial expressions there are all sorts of other things that are happening but it is already happening it's already out there technology is out there and the most important thing is that we start talking about it and if we talk about it if we talk about the data privacy issues associated with it the dangers you know and we start maybe working on the respective legislation that becomes necessary for that as well because now this technology is available then then we're on the right path then we will be able to contain the dangers yeah when when talking about the dangers of the software yeah I think so when we talk about the dangers I wouldn't say some sort of orwellian a story is really the case I mean even though there have been proposals to use it in CCTV cameras and apparently I didn't know about the Microsoft building but even if it's used for surveillance well there's a lot to be said about it but for me the discussion would be more and at a fundamental level namely the the images that were shown for example they were from the way that the software is brought forward I mean there there were all promotional images so they were used to show that to technology works which is what promotional images are for of course put you on there and they do that through these quite stereotypical expressions now in doing so they sort of hide the fact that this technology actually in its basis has a lot of human labor in it it's not some sort of magical work that certainly happens and suddenly you have this software that gives you an objective outsider's view there is a lot of theoretical assumptions underlying it one of them is Paul Ekman theory which is sort of i think open for discussion already if we from what i heard this morning on the other hand it's also the procedure that goes into making the software measure faces I mean they these these the software recognizes the faces and facial expressions through a machine learning procedures so there is a lot of data that goes in for a computer to recognize the expression and give it a certain label it first need to be fat with a lot of images showing that expression that means that there is at the first instance already humans determining which expression is what so there is there's even though as it as if it's shown that it's a fully automated procedure there is already the human labor at the input so I think that sort of is left out of the story I have it I read an article recently because we are in the recording sites and this article was sort of sketching out the the dangers of these emotional metrics taking over the hiring process completely and what was suggested was that this should never be trusted on its own solely these emotional metrics again because it's it's obviously context dependent and underlying assumptions can be you know discussed in this place so it should always be in conjunction with you know something of an interview or a face-to-face talk to the person and this sort of leads me to my next question I'm interested to see I mean it could be for all for all we know that this you know a robot or or some sort of algorithm will take over the hiring process completely we'll have to wait and see sort of but where do you guys see see this what paths do you see here what do you see in the future what can we expect in the next let's say decades or even even beyond do you have any any thoughts I mean you're obviously in in the field where the developments what directions is it going in at the moment well first of all what one comment about these hiring process that you said and that we might want to wait and see you don't actually have to wait and see it's already up and running on the fourth floor I think there's a group of creative people who set up an experiment where some kind of interview process but done by a machine so you can you can experience that today it's not gay it's not a full machine but it kind of gives you the flavor and and and as I previously said there's pros and cons right so you said that well you know there's always have to be a human involved in in this you know on one side yes I agree on the other side there's there's this thing that when there's a human involved in interviewing then that human well we all have biases and so you know if a machine makes some kind of objective decision then you can perhaps make you know less biased of course a machine is built by humans so whoever built that machine puts in their biases in it but that's a that's a separate topic in terms of the future of this technology I think I think it's undergoing very active development writer right now it depends on where where you really want to use it I think many of the technologies well you know what one thing that that comes to mind that everybody immediately wants to think when talking about this technology is that all right let's let's create a robot that that just comes and interacts with you like like a human being but for a robot to interact with you like a human being this technology has to be like another human and it is not at that level you know you we don't have the context we don't have all these other modalities so there's a long way and a lot more of research that needs to be done to get it to that level so that's one of the directions where I think this technology would be moving but in terms of the the wide spectrum of applications you know this we Maya has presented a lot of different applications these are medical applications these are automotive applications robotics you know we've we've been presenting measuring the the impact of creative content so so I think I think there's a lot of gaming is a huge huge industry that everybody is really excited to try to do that and there's several attempts but as soon as somebody tries to use it in gaming it's like oh it's not quite perfect yet so there's a bit of that as well let me just say a couple of words about universality you don't you do not mean emotions are not Universal construct as such they're context-sensitive the culture depot immediately when is a context-sensitive that immediately means culture dependent in fact person dependent right but there are something else this is the facial muscles so we all here exactly same facial muscles we may activate them me with the different intensities and to the different amplitudes but we all have exactly same measure facial muscles so if we will be able that's like part of the research we are currently doing to recognize those actions in terms of facial muscle actions then the interpretation which is higher level interpretation in terms of emotions or attitudes or mental state or whatever you're interested in is actually easier and you can have a very objective system of tracking and recognizing the gestures up to that point and then the higher level interpretation could be left actually to the next stage and regarding the robots that would interact with us as human beings there is a very few applications for which this would be really relevant so while I know like millions of people really would like to say that for no other sake then to have these artificial intelligence really come to life I do not see it at all as anything purposeful so and so I mean robotic nurse sure but even with the robotic nurse there is a certain level until you will actually need empathy or whatever kind of a true emotional engagement so truly interacting with the robot in a fully human way like this really according to me I'm sorry if I upset anybody this totally ridiculous movie ex machina it's something which I cannot subscribe to so it's it's really outrageously stupid so ok do you agree ruin yeah do I agree with it yes I think so i but i think the relevance is also why would one desire that that certain why would one desire indeed such a robot i think that the applications of driver attentions that are much more interesting to look at but then i don't know then its deadly where we can talk could talk about it as if it's emotions I think it's then something else but that's again the term emotion being may be problematic okay I think we have around 15 minutes left to have some questions from the audience as well is there anybody yeah we have fun back there how does this work with the microphone hi there hi Pablo thank you very much the tree leaves were wonderful with tree inspiring talks from the technology to the actual business and obviously a very nice approach from the arts to the current topics my question goes basically to Mayan ellner what are the current the constraints in regard to legal of permissions or like legislation what is what's happening at the moment how how much information can you obtain can you archive it is it is it a field that is it is it growing are the countries that allow more of this information to be obtained an hour time or what is what is in general didi what does it look like in nowadays okay um when you work with this whenever you have data collection you need to acquire ethical permission and the permission of people from whom you are collecting the data so which is great and however as we know there are many companies including Facebook Google and Amazon who do not follow this procedure and obtain your data just by you using their applications so that's dangerous there are no legislation to prevent those companies to use that so this is something we mentioned this before the talks and this is something which is really important and people should start working on that and that is in which way we can truly protect our data the data that for example you put on on your facebook currently these data do belong to Facebook and they can use it for whatever you want what's up is bought by Facebook all your conversations are also mind and use for the commercial purposes so you lost your privacy you don't have the privacy so the issue is how we can get back our privacy and the only way I see is actually using a kind of person identification technique which would be exceptionally unique so definitely not the face because the phase can be tempered it must be something else by which we will be able to tag this data and proclaim it our own hence be only owners of this data so this for example is one of the legislation so which should be worked however of course companies like Facebook Amazon and Google are very much against it because this is how they make their profits so we are currently in a kind of you know local minima what we will call I really hope it will not become a global minima but whether this would be changed or not I do not know however I think this is a worrying issue and not what elder is doing or for this doing with the watching people for their own for example safety in case of cars or when they sign that their reactions will be used for these marketing purposes and they are aware for which that for what the data is used ok so I'll give a quick and concise answer to that question for us it is very very important topic because we know we're working with sensitive personal information so there's several steps that we've taken to make sure that this is treated accordingly as well so one is that of course we're working with professional legal entities that have advised us on what we can and cannot do and helped us to define the correct privacy policy and make sure that we're following it and you know there are different legislations for example in the European Union were not allowed to let this data to be stored on any foreign servers or machines anywhere outside of European Union it was easier previously because we had the safe harbor agreement with United States but it was made invalid a year ago or so and you know so this there's few complications there but that's something that we're following on the other hand we also have very strict policies as in how the data is stored so it's stored in in an encrypted way and unless so there's no no the access to that data is limited to only for the purposes of training the algorithms and so on so this data is really really treated very very carefully and every time somebody is taking part in our tests even though they have signed up to take part in such tests and so on every time there's a request to from them to give access to their webcam and there's a notice that that their facial expressions will be recorded so we're trying to make it as people at the word as possible of what we're doing what data were collecting how are we storing it and and then on the other end we're trying to make sure we store it as safely and securely as possible it's a question over here as well and who's I am I am thank you very much for your talks this questions for Reuben and I really liked your link with hysteria and to your finishing statement of that we could have a future with just train sincerity I was wondering if you had any proposals of like how to avoid this and do you have any altered alternative paradigms of how you could see a way as individuals or collectively that we can avoid the that this happening or it increasingly happening as I I would say it's happening all the time and especially in the universities in this creative industries there's all this there's constant courses where I've studied as well like I to promote yourself how to mimic these like you were also saying these responses and I think it's happening all the time and I think there's such a nuance now and down in itself that it is really hard to tell if someone's just being charming or is their integrity and i think i think it's it's going to be more and more harder to ya differentiate what's sincere in not and i was just wondering if you had any ideas of how to avoid that then in the future thanks for the question it's a very good one and it even relates to then what would be sincere I mean there's other things that influence your behavior in public anyway I mean their social norms it's the whole discussion that was already pointed out that when we interact there is actually a culture that influences our behavior so I'm not sure what the response to that could be I mean there there have been these projects to avoid facial detection in general to sort of undermine that I'm not sure whether that's really a sort of that's more a statement than that it is really something you do in everyday use and I think for me now the first step would be a sort of awareness of of the influence for example a few years ago there was this finished researcher liechtenstein who did who looked at quantified self applications in this case heart rate monitors and how that sort of influences your behavior and that indeed the information that you get back that the measurements of your own heart rate which is really minimal compared to facial expressions already influences your behavior and you actually get an interactive relationship with it so it's not that you take the information for granted per se but if you see your heart rate you sort of re reflect on your day like oh yeah my heart rates were is very high there oh I must have been quite stressed or that you say well this maybe doesn't fit with how I felt so that for me that the point is that the relationship you have with those numbers is not in its not a concrete fixed one I mean like I also write in my statement about my project what does it mean to feel thirty percent surprised I think that's a really an open question and first asking that would be a good I think that's a good step to go so another question yeah right right next to her yeah thank you all from an emotion researchers perspective um I think you guys should let go of basic emotions ah you've won you're giving Paul Ekman way too much credit um but there's like a lot of research you guys need to be thinking about and in fact I don't need to admonish you I think you should be the innovators and the leaders because you have big data right you should be telling us what facial expressions actually look like in terms of how they predict behavior so I don't understand why we're even talking what is it because you can when you pitch the vc's you can talk you could say happiness and you think that that sells ads as an emotion researcher I don't we don't know yet right or slow that what emotions map onto behavior you're you're using your own like your introspecting as a human it's like yeah happy means good right that's gonna make more sales we don't know what emotion how that there's no one to one relationship in fact it's big data approaches that could be telling us these things or generative models that will tell us like this is the weird face that comes up that predicts when people are willing to accept an unfair economic offer it looks nothing like any expression you've ever seen before so it isn't so this is a more of a comment than a question I am partly admonishing you for talking about basic emotions but I really think you can innovate in this way because you can mine the data in ways that can tell us what there's so much you have there that will be important for the real world and for you know commerce but also emotion researchers that we don't have access to this kind of these kind of data so okay here's the question in this comment how important is it for your models to actually have discussed fear whatever why don't I mean I've done some modeling i understands like do you really need those as as feature detectors i mean if you find a little bit of a head tilt is what really is the thing i mean as as i understand it from computer science perspective you shouldn't care what the features are that predicted behavior because you're not trying to explain emotions you're like hey i can predict some economic behavior from a head tilt but that doesn't fit into any of the basic emotion models so how do you guys you must have found these things they just don't align with disgust or anger why aren't you to showing us pictures like heck ekman darwin the wrong these are the features that actually predict behavior so that's my comment question thanks thanks for the command you know very very topical I think I think I think Eggman's basic expressions is is is a framework and it's a framework that is easy to understand and easy to begin with and this is something that we started off with and there are all these reasons that you mentioned as well why it is good you know it's it might be a good place to start but you're absolutely right that there are all these different expressions and you know faces and head tilts and so on which may have nothing to do with the basic emotions while they can be predictive and that's something that we started to do is I was mentioning and we're already finding such weird expressions or behaviors and then it becomes a question did we really find a weird behavior and array you know let's let's try to see what it is or is this a chance you know it's just we correlated something with something and it isn't really any useful behavior it is a bit of a tricky territory and you have to be really careful there but we are exactly on to things like that so you're you're absolutely right it's a thank you for this comment and so they are not wrong Eggman and Arvind were not wrong there are as you know Orton internal for example have shown that it is not the full expressions but the parts of the expressions which are actually universal so the issue is that when it comes to Elmer for example I don't know for your open but I know forever what's the issue is that they need labels so they can label it with these parts of expression which we talked about for a long time that they should do or they should leave or they could label it with those prototypic labels right and the second one is exactly what elder said it's easier for 24 people to understand right and somehow this is how it always end up then they label with those labels disgust sadness happiness whatever right and regarding the strange behaviors we did find some strange behaviors this is in collaboration with realize we now recorded six different cultures the data will be available so in case you are just a researcher we make the data available for whole research community so we have something like 400 different people and six different cultures and one of the cultures is Chinese culture and its really wonderful they smile all the time when they are happy when they don't like it when they hate it when they're surprised when they don't know how they should feel they laugh all the time it is just the way their life and what are the position of the head and the eyebrows at the same time it's really wonderful data so this is kind of the things we we are finding out but this you should understand for the big data that they have to deal with this is very difficult we have just peanuts and everybody is constantly talking ah you guys have a lot of data why don't you do it currently deep learning everybody should go into deep learning and just deep learning is just one of the machine learning techniques when you have masses of data you can train deep learning methods and they will do wonders for you but the issue is we don't have these data so we have very little data with those specific strange expressions that are truly representative of for example culture so this is why they one way or the other end up in this like you know basic emotion labels however I hope we'll move out of it so we'll see okay i think that that is a nice conclusion to wrap up the the panel it's run four o'clock so i wanted to ask for another round of applause for Ruben ellner and Maya err thang you 