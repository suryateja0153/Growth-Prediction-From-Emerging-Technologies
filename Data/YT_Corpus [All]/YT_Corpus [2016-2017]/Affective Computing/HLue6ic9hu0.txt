 hi Bert and when I'm not making artisanal pickles in Brooklyn I report about technology and and venture capital investment in in Manhattan and points elsewhere around the globe we have a wonderful panel to talk about can you develop a relationship with a machine I think all of us would agree that not only can you but we have the question I think to interrogate is what kind of relationship you have our machine with the machines and how desirable that relationship is to help us get into that we have a incredible panel we've got Fred munch who is with North well health and mobile health intervention interventions thank you very much sorry about that we've got John Gretch who is a professor at USC who does research into all of this stuff and we've got Katie Aquino in the middle that's not John gratch that's Katie Aquino who works with the robotics company bought AI and talks about transhumanism and the future of technology and our relationship with robots so there's a lot of stuff to get into but that was a pretty cursory introduction of what these fellows and fine ladies are lady do and I'd like them to do a little bit of a brief introduction to explain a little bit more about how they work with technology and and mental health and the relationship between humans and machines so Fred if you wouldn't mind giving a brief intro sure it's a pleasure to be here my main entry I'm a clinical psychologist and I consider myself a technologist as well and my main interest is how do we facilitate progress in the therapeutic relationship and behavior change over the long term by by understanding the interaction between technology and and the human and how do we maximize behavior change using technology and the human relationship so what what kind of digital dose do you need at the right time and when is that most effective and when do you need human empathetic contact and that's a lot of the research we do is to to understand that phenomenon ok ok so i founded a company called bought AI and what we're doing is we're creating personal use robots and we're using a myriad of technologies including VR artificial intelligence and also we're creating advanced humanoid robots called bods and at body I we're looking to provide these personal use robots to all peoples of all budgets all passions and that simply does not exist today and I think that these emerging technologies such as a rvr will be opening those experiences up to the public and we're very excited about it John and you I believe you have a multimedia thing for us to yeah we'll see if it works I tried to bring some technology to help introduce myself what it's not the G on a tech panel that's insane crazy never gonna work this is the way these things go this is why I read ok oh this is what I do so I try to actually build machines that can in some sense have empathetic contact with people this is Ellie and she's designed to do an interactive screening with people let me just play a little bit of that you can't hear the audio but she's basically explaining that she's here to have she's not a therapist but she'll talk with you huh there we go oh there goes are you okay with this yes so how are you doing today I'm doing well that's good where are you from originally I'm from Los Angeles oh I'm from only myself one of the things we do here is we're doing facial expression recognition so we can automatically infer the pattern of facial expressions people do or analyzing the audio signal and the voice we look at posture with like a connect and then this character is also doing similar things that she generates facial expressions and postures and can do the kind of moment to moment kind of feedback that conveys a connection to people in natural conversations and ultimately the goal is to try to elicit indicators of depression and power of our research shows that people actually disclose more to this kind of technology than they might want it on spoilers come on we'll get into that in the conversation all right I'm on I'm done um one bit of housekeeping uh just to to set things off I like to make these discussions as interactive as possible i'm also really lazy so if you wanted to tweet questions at me while we're chatting up here if you have anything any burning question you want to post any of our panelists over the course of the our discussion feel free to tweet at me at my twitter handle which is j my first initial my last name schieber shi eb ER and if there's good enough reception in this hall hopefully i will get them and it will work and i will incorporate your questions into our discussion if they are good if they're bad i'll just make fun of you um because that's the kind of I am um so I guess the first question that I'd like to pose to all of you is not you know sort of do can people have a relationship with their technology which which clearly people do but what kinds of relationships people have with your technologies um and Fred we'll start with you John you and then Katie will will will will wrap up there and and and then move on so Fred what's your take on that what sorts of relationships do people have with the sort of offering that you provide so based on the work we do the research and the work we do we see two or three three primary points of relationship one is that the technology opens it up to engage into the into this system very non-judgmentally is that it's not only are you expanding reach you're allowing someone to have the opportunity to disclose and engage in a system at their own pace and technology allows people to do that so we've seen engagement rates go up dramatically we built a automated SMS program for problem drinkers and what we found is that that people were signing up in droves for this program because they didn't have to go into care and it wasn't connected to their health record and and so they were engaging the other relationship that we see is that you can provide an ongoing salient touch of therapeutic goals in a way that the traditional therapeutic relationship can't do which is if you're introducing some automation that's completely tailored and customized to someone's goal is that you're allowing that relationship to continue when the relationship with the human can continue whether that's at three in the morning whether someone's going out for a drink and they type and drink and they get a drinking plan for that evening whatever it might be your allowing that and you're supplementing the human contact and then what we've also found is that there are crisis points and there are points when people want human contact and as long as you can integrate that into automated systems like when someone is has a risk for relapse one of the things we found is that people type in regret very often into these machines so after they have had heavy night of drinking they're typing and regret so they want to talk to someone about that and and so what I'm very interested in what we're really seeing is is that people want to interact with machines in certain settings and they want to interact with people and others and how do we find that now John with your research is there going to be a point where where there won't need to be a human on I mean I mean ultimately what you've got is this responsive sort of AI system that's learning how to be empathic while also being a completely non judgmental program that has no conception or need to conceive of what exactly is going on with that person it's just this reflective conduit right or not conduit but this this sort of reflective party that can can maybe guide a discussion if it's programmed the right way or a sway jet is that the next step from say where Fred's a work is going does that become the natural progression to ease sort of the human part of the equation out of the equation yeah more so well our ultimate goal in our lab is to make machines able to understand people to able to empathize to be emotionally intelligent I would say the state of this system and is it gives the illusion of emotional intelligence and so it doesn't really understand deeply what you're talking about and so I think it's a useful first is an icebreaker to open up the conversation but you wouldn't want us to plant and I don't see you able with technology in the near term to be able to really find my understand a person's goals motives needs and that's necessary for for treatment and so I see this as a door opener people in the original reason for the project was that we're trying to figure out what how can we use technology to reach people who are reluctant to talk to people who and so maybe they'll be more interested in talking to virtual people and write data they are but that's I think the first step right need a person and you do find out what how the technology complements the strengths and weaknesses of human clinicians ok i mean when i think about where y'all are it seems like everyone's on a spectrum right so so Fred you're you're you're at a very sort of a text based version of solving the problem John you you have like this visual element to it as well Katie you're full-on like that that like these are these are robots that are going to be interacting with people what is what does that do to the relationship what is the physical element add how does it change things what sorts of things are you looking for your body to do are they hot bods are they attractive buds are they well it depends on you know what is hot and what is the relationship I think the relationships in general are just changing and we see that all around and I think that our perspectives our humanity I mean we are changing we are evolving we are becoming in a sense cyborgs you know we live through our phones they are an extension of us so can we have a relationship with her machine absolutely and at body I what we're doing is we are working closely with all backgrounds of people to find out what they want what makes them tick now when we think about our favorite sci-fi movies right even talking about the film interstellar all of us fell in love with the character tars tars was an AI and Tars was in the cool AI because he was a smart artificial intelligence robot no that's not it tars captured our hearts because he had a little Moxie he was a you know interesting cool robot he could challenge you we want challenges as humans we want challenges so we're going to bring those challenges into the AI is that we are continuously to develop wants them to be fine-tuned want them to challenge us and want them to excite us now as far as yet aesthetics go well we have hot robots sure we'll have robots they are anatomically correct but also at the same time we have to keep in you no consideration that not everybody in the future is going to want a perfect looking woman of today it's not going to be like that it's going to be changing do we even want to be with someone that looks like a human may be revolving so I think there's a lot of things to take take into consideration I didn't mean to go down that rabbit hole quite yet although you know we'll get there well go get we'll get there we'll get there look okay let's look at the adult toy situation today now we're definitely gonna put a pin in Attica back to it geez am i blushing good hot here y'all whoo all right so do you want um do you want your emotional your your a is to have moxie is that something that no I'm John do you want your a is that Moxie is moxley something that's important well it can be important I mean in general what part of my research is trying to understand that people use emotion strategically and they don't always just show what they feel they use emotions to achieve certain social goals in humor self-deprecating humor but also you know Moxie can be effective to convey a certain attitude which could promote certain goals in certain situations in therapeutic situations I'm not quite sure but absolutely in part of our research we look at business context negotiations where people clearly use emotion strategically to achieve ends would it would it at some point what an application for this be to have the the AI do the negotiating absolutely yeah this work on that right now is that being done in your lab or it's being done in our lab that's that's all over yeah I'm so again i'm lazy so this just sounds phenomenal like get a glass art for me where's my basic minimum income um kidding so when you when you think about what it is that your machines are feeling or what it is that they're they're reflecting or providing to to the humans that they're in a relationship with what does that look like and how does that happen Fred and John y'all are doing sort of the research on that stuff what what are you seeing and and what is it exactly that's being collected to give these these programs a soul now I'm not going to go there but to give them that that intelligence that emotional awareness let's say or is that even the right turn hey it's some level mean machines need goals to achieve their ends and emotion is an important part of goal achievement motivation and so in some sense very rudimentary sense these machines have needs and goals I wouldn't necessarily equate them with human needs and goals but I think eventually as they have to become fully socially intelligent they will bring you know important needs to a therapeutic situation I think there's also an interesting ethical dilemma because it's also totally possible to design a machine that creates complete illusion of needs and ends but has different ones and of course that makes a lot of sense of negotiation and so I think there's a some interesting ethical dilemmas that will be coming down the pipes and Fred are your robots are your sort of programs learning and what are they learning how are they learning so they are learning and what we what we we start out though we getting a comprehensive picture of the individual and just like all machines they have to be trained as Jonathan was saying so as we build the algorithms to then trigger an ongoing intervention it's what someone puts in is guides it so we have to allow for the end user to to give us information and we know that people are much more honest in self disclosure to these machines so people are going to give us information and then we take that information and then modify it and when one of the pieces we found is people hate people who dislike getting the wrong type of feedback so we're very careful and we're very conservative in what we put out there in terms of making assumptions about what they want so we're conservative and then we look at certain things and allow individuals to make judgments about their change for example do you feel like over the last week you've changed to achieve X goal and then based on that we have enough information to understand gold revision understand where someone is provide just-in-time empathetic feedback and then move on so so what I would say is worse at such early stages in building these adaptive algorithms and we are very careful as not to throw certain things out there because of the potential downside to someone getting the wrong information and i think that's that's incredibly important Katie how are um how are your robot what can you talk a little bit about the technologies that y'all are looking at in terms of how you're going to integrate these kinds of intelligence is into the robots that y'all are building yes so it's kind of a mixture between it's all right the backtrack a lot of things that we see today especially those viral videos of things that look like AI a lot of the times or more like a chat bot right well we're developing is something that's not a chat bot we're going to bring in a little bit of deep learning we are going to have some elements of the ability to chat back and forth it will be able to remember but what's important is having that beast personality and the experience of what we're feeding each bod or personality is going to be gamified so through gamification we're going to be creating a seamless user experience that's controlled by your smartphone app through your smartphone app you're going to be interacting with your phone collecting data this data from who you are what your interests are is going to be fed to the algorithms that is part of the spot this personality so I think that when we're online dating right and put a profile up we have what we like our interests are we're kind of looking for people they have similar interests it's kind of the same thing we kind of see it as like you're creating your own online dating profile but for your bod for your synthetic partner mmm so what I mean how does that how how has that vision of the future manifested itself in in the reception that you've gotten for this idea of putting sort of the BOD AI product out there as something that that people will eventually have or there's another one that does these sort of exoskeletons that that are also enhanced with all of this like enhancement or relationship making the relationship with technology much more physical and bringing it out of this this sort of realm how is that how has that been perceived sort of broadly what's what's your sense of that people are actually very excited about it and it we have a different different kinds of feedback from different kinds of people and that's just how it's going to be there's people who are saying oh I can't wait for the day then there's people who are saying that's interesting I would definitely try that that sounds awesome I haven't heard anybody actually see to me that's scary I don't want to do that but of course there are plenty of people who get that feedback as well yeah the most important thing is we are working with an entire subculture community out there known as I don't laterz hardly anybody's heard of them but they've been on television Wanda's named Dave cat he's been on TLC my strange addiction and taboo and he's married to a synthetic person unfortunately he is a robosexual but because the technology simply doesn't exist and what does exist on the market currently is slightly horrifying people are awaiting the day when they can either experiment or actually live with a synthetic partner huh really no one has questions about that like there's no it either that or y'all really are bad at Twitter we're gonna get to like an actual person walking around with mikes at the at the very end so in about 10 minutes um so so that is that is something that's going to happen but I'm waiting for the the comments to start pouring in reception is also really bad so y'all my I'm gonna follow up with that a little bit are you know um well but i wanna i want to telescope it out a little bit John and Fred do you look at something that is this material and physical that that people can have an actual sort of tactile relationship with as as sort of the the end game for the types of therapies or or or systems that y'all are looking at where you have these these either robotic proxies or these avatars that are exist in physical space yeah a little bit because so part of my research I try to understand how people respond differently to machines than people and one of the reasons I was fascinated with this therapy project is because actually when you interact with people you engage in impression management you try to build yourself up you don't want to disclose because you feel your fear being judged and one thing I was curious about this project was the idea was to add a human-like element to a machine and we know that people disclose more information to computer forms than two people in some cases because they feel more anonymous they feel less judged and question was the other pathway is that you can build rapport or alliance but by being a person and giving that kind empathetic feedback but if you combine those two I wasn't sure if it would be the best of both worlds or the worst of both worlds because you might actually undermine the advantage of a computer by adding these the present like elements and so far we haven't showed that to be the case but my guess would be that in part that's because the character here emphasizes her computer pneus in many ways throughout the interaction and I think if you add full and we're actually going to experiment with this creepy life like Android that's the system from creepy and horrific or words that have been used read this thing and my guess would be the more you add the more you make it really like a human the more you'll undermine the benefit right that you get from listening well there's a theory about this right it's the uncanny valley right there's that point at which the the this sort of anthropomorphize thing becomes near enough to human but not quite human that's why all those baby dolls are really creepy right because it they look they're sort of real but they're not real I've read I jumped in on you you know I just want one of the things i'd like to do with that to it because we work with disclosure as well and and and you know women report more sexual partners to a computer men report less you know people report domestic violence because they're there they're there's no impression that and and it's not necessarily just that it's anonymous it can be you know someone's at the other end but Jonathan brought up the non-judgmental right and and take to go back to that is that there's a tremendous opportunity to to gather information and with diagnostics and I think that's where we're going to see this massive change in what therapy and psychiatric and mental health is that were it's going to be completely taken over digitally right because it just does it better it does a better job I don't think that the human relationship and the goal of the therapeutic will be a machine right as that I think machines can augment to contact but I teach a class called crafting mindful experiences at NYU and and what I find more than anything is the majority of projects they're all over the place but a lot of them are how do we get used technology to create a deeper connection with other humans and technology allows us to connect in ways we haven't done before but we also have to disconnect and for people with extreme trauma I do think that you know that we know that dolphin therapy pet therapy any type of connections good whether it's a machine or not but at the same time I think it's about helping people engage in their world we know people with social anxiety would rather talk to a machine the goal then is to get them to talk to humans read so how do we find that balance and right right well I mean I think that that just lends credence to a theory that's that's sort of circulating more broadly in me and the tech community in general which is that you know there will be specific applications for specific types of a is right and some will be more effect less and some will be more effective or have them a more emotional response based on the kinds of things that you want to get out of that ai ai is the wrong word it's more like more like a sort of learning program right um someone someone brought up Tay and I feel like we should talk about Tay just a little bit um I have some things to say about Tay and and and and basically so the question from Ariana tobin yay Ariana was can we talk about Tay therapy is inherently vulnerable and technology is inherently experimental and the tension between that I think that Tay is not a good sort of corollary to the work that y'all are doing because Tay was just goddamn dumb like why would you unleash an innocent program on Twitter it's like expecting the worst of people um that that was like Microsoft being completely myopic in terms of the the ways in which it was thinking about how the AI would actually learn because I the assumption somehow that Twitter represents real speech or like some sort of actual forum for fright I mean in some senses it is a forum for ideas and I mean motive exchange but um there are just too many trolls and the world I mean like trying to learn from the unfiltered internet is a terrible terrible idea though you wouldn't throw a child onto Twitter and be like okay this is how you're going to learn about humanity nor would you do that with a bot that's just my take on things I don't know do y'all of anything the only thing is that just dumb just as you know we disclose more to computers in the reverse also is that we see like the just downside of humanity is that it's yes when we have a problem we're going to open up and also you know we're just going to open up about anything and and you'll see it with bullying on certain social media apps we see it all over and so and and the other is that people are impulsive and what someone says in the moment on twitter account is is it is all about just pure drive you know it just the sum of what people i mean if you also have the the foundation of the Egyptian revolution oh oh I get it but I'm saying using that to build in a system to learn off of I completely agree with that you're getting the best in the worst of both worlds anybody I haven't I guess I'll just say that I think it's it was an illustrative example of the limits of the current speech processing technology most of what you see with priests speech processing it's it's fairly shell it doesn't have a deep model of what's going on in the interaction and I think that on the one hand highlights the need for those kind of deep models but also I think kind of also counteract some of the great hype around a I these days it's about to take over the world I think it illustrates that these techniques have severe limits when it comes to and deep intelligence right and frightening if you think that you want to cede so much control to them does that also does that sigh aspect of things give you pause in terms of how much how much the limits of what the technology can do or are we just saying the same thing I think I mean I think technology people in a I recognize the limits much more than the media and Elon Musk seem to at the moment that you know it's there will be important slow advances and people incorporate these things when they're effective and a lot of the hype I think we can realize it'll take a while before these things are really able to deeply understand and communicate with people and so I I'm going to give in to my people more apparent and salacious nature and let's talk about sex baby um so so look with the artificial intelligence is the way they are currently now is a relationship like I it is like a sexual relationship with a robot or an AI something that that is like I just wonder about the power dynamics of that I guess and whether that's actually something that's desirable or like something that people should should be should should should be looking for or whether that's just a replacement for people who have some sort of associative disorder and can't relate to humans um I don't know if I'm being too judgmental but I just I'm throwing that out there anybody want to take a whiff of that you and take a crack at it I I think that we need to kind of take a step back and think that the far future implications of this technology that we're making is going to be advanced and there will be like people and that it's not so cut and dry you know even going back to taste tweets that was like I said before there's a clear distinct distinguish meant but between the chappa-ai s and then also more of deep learning there's a lot of differences between them so of course when you put something like Tay on the web it's going to have that effect because it's too tempting but on the other hand I think that as far as the sexual implications go for these robots I think that it's going to be an exploration I think that where we people are exploring ourselves at technology is becoming a part of us I think that if anything sex is going to become better in the future by far we are biological machines and there's a lot of problems that happen unfortunately to our biological bodies but there are real companies organization real people scientists researchers working right now to create a next human of an evolved human in the far future of course not right now but I do believe that machines will be the first step towards us exploring our own sexual identities and then in the future we will also perhaps evolve theoretically into maybe something else Amanda machine with a machine Fred Johnny y'all want to wait into these waters well I guess I'll say I think we should distinguish sex from relationships and by having sex with machines right now right but I teach I mean I think a relationship like if you see the movie her that would say is an image of what a relationship with a machine could be like and I actually use I teach a course on affective computing on emotions and machines and one of the assignments I give my students is to think about should we build her and I give them some of the research you know on you know how does Facebook changes or things of that nature and I think it's a good thought experiment I think it's a thought experiment all technologists should address is to you know should we and what are the implications of building machines that could form real relationships with people and is that a good or bad thing for society and just to add is that I mean you if you look at the sex industry they're always advanced in terms of technology they're always the first to do they're doing augmented reality virtual reality adding in machine learning toys so you do see where things are going to go when you look at that industry but the depersonalization and I think with Jonathan mentioned in terms of the you know a relationship for sex and you're seeing the people who grew up with internet porn having trouble with sexual relationships you're seeing an increase people trying to understand what it's like to be in a true loving relationship and as someone who has a 13 year old boy who and that he knows what the Deep Web is who knows how to get there when we have every restriction up is how are we going to maintain a loving relationship in the face of all these tools and so but i also think at the same time we can engage people and build interventions and avenues toward intervention if we have a window into getting that information out and he gets it he gets the potential harm of diving into this in a way that's uncontrolled and that's my fear of it that's not and at the same time I think there's a lot of hope all right i am it i've eaten into about five minutes or four minutes of the questions from the floor are there questions from the floor I can keep asking these people all sorts of stuff but there were hands up before anybody really nothing Wow oh there you good sir right over there earlier you mentioned some of the ethical dilemma that we're going to be coming up in the future the gentleman on the right yeah I wish you could elaborate on what some of those might be well I think you kind of touched on it so one of the issues I mean there's actually a lot of question around Facebook is facebook something that helps us grow as individuals or is it something that's nihilistic and has us reflect only upon ourselves and there's actually a fair amount of kind of interesting research around that it seems to play some role in self-affirmation so so Jeff Hancock who was at Cornell now at Stanford has looked at how does it change your decision-making how does it change your mood when you read your own wall and see how other people's connections about you influence you know do you actually have sort of feel better but do you actually change your notion of self there's also work that people who use a lot of Facebook or some of the loneliest people it's work by casio put University of Chicago and so this this technology enables different ways that people use it and it does seem to have long consequences for behavior and it's not very well understood what those consequences are and it's certainly not being considered by the designers I I think that's why I there are also i think at some point you get to sort of question from the opposite side the ethics of the ai's themselves like what rights do they have much can you program them when do they start to become legal entities and you're seeing that across a range of things not just with psychological ethics but but real moral dilemmas as we get into issues of like autonomous vehicles and self driving cars who you kill the old man in on the road or like the car full of kids or you know the the puppy right and at some point the the AI is going to have to make that decision right and what that looks like no one knows yet and I think people are still grappling with in a real sort of profound way and that's just one example there are hundreds like if you have a eyes that are doing your contracts who's legally liable when those contracts go bad or which the negotiation instance did anybody else want to sort of weigh in on well we already have a eyes controlling stock markets and things I mean we already have it in our lives the only differences it's going to be more abundant and and more ubiquitous everywhere it's gonna be just from our cars to everything our transportation everything will be controlled by a eyes and that's just where we're headed I don't think we should be afraid of it and I think that just you know going back to the theme of what this discussion is really about we're talking about relationships with man and machine right what is happening is it this is about filling a void I think a lot of us we are overwhelmed by technology yes and also on the other hand yes we do have more exposure to other people that's how maybe perhaps Facebook is considered a negative is that people are able to connect with one another then you have tender then you have these apps that are about instant connections instagram if occasion it is harming human-to-human relationships but also on the other hand we have to take it to account it's just a lot of really lonely people and what can really what's what's the hurt if you have a synthetic companion who is programmed and is there for you it is there to help you and you could trust this synthetic being I think that that's going to have a positive effect on the world and I think there's a lot of single mothers it's going to become more advanced more like a person so if that could fulfill that boy and people I think that would be a positive and well Freud it gets by I mean the work that you're doing is incredibly positive when it comes to the relationship that folks are having with these these these SMS systems right where where there are responses that are positive that are keeping people from doing things like either regressing in their behavior or or you know if they have regrets getting them in front of people who can help them out and keeping them on on a path that that is positive for them are there other questions we've got about a minute and a half yes ma'am in the front thanks you all so much for being here this is so interesting and i love the evolution of the field and i'm mostly really curious about because i started my own online practice and people were so concerned about like classified information and mandated reporting and things like that and now that it's gotten to this stage with the RNA are clearly y'all are looking at the data when I think about the phases what's the ultimate goal for this data like now that y'all know that people are more likely to report like are we going to design different elements to improve the human experience for example yes right is that yeah I think it's too as you can see from the panel I would say I'm the Luddite of the past the it would the amazing work that these guys are doing in terms of Ni and I do think it's going to constantly evolve and what we will find is what's working and what's not and the the mental health field the medical field were primarily a risk averse and this has allowed us to that the new digital medium has allowed us to really test and iterate in a much more agile way so it's so exciting to see what's going to happen whether it's from a disclosure whether it's from safety planning whatever it might be we know that technology is the greatest thing in the world to help people who are suicidal there's nothing better yet practitioners are often fearful well what if someone suicidal no this technology is good for this we want technology for this so I guess the short of it is I do think what's going to happen is going to constantly be evolving and I don't think there's any right answer and with that we are officially over time so thank you all so much for listening thanks to the panelists for participating Jana I think you just 