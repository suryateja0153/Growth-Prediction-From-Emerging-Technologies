 [Music] unlike other technologies AR and VR has a potential to enable people to experience what someone else is seeing hearing and feeling our next speaker is professor of the University of South Australia mark billinghurst who will explore the coming age of empathic computing and how AR and VR technology can be combined with wearable physiological sensors to create shared empathic experiences please join me in welcoming mark billing hers to the stage click Rolleston right well it's great to be here thank you for coming my name is Tom introduced me as Mark billinghurst from University of South Australia and I know about all of you but sometimes i feel i wish i could have some sort of magic ball that would let me look into the future and see where all this technology is going over the last couple of days and the exhibition hall next all we've seen some fantastic examples of a wide variety of wearable and AR and VR technologies and of course these are being applied in many application areas like gaming and health education and so forth but what I'd like to talk about today is one direction I think there's a promising future and that direction is in the area of empathy so empathy is famously defined by Alfred Adler as being being able to see with the eyes of another person listen with the ears of another person feel with the heart of another person and in many ways empathy is what makes us create strong connections with each other as humans and over the last decade or so there's been a lot of research in this space and in particular in the area which now is becoming called empathic computing in three different topics so one is building systems that can understand our feelings and emotions secondly as building systems that help us better experience the world of others and the experiences they're living and thirdly building systems that help us share the experiences of others so in order to do this we need to first of all to understand emotions have some sort of sensory systems for experiencing other people's experiences virtual rails he's an idea technology for that and for sharing and and connect with other people live augmented reality is a good technology for that over the next few minutes I'll give you some examples of each of these so first of all the area of understanding this was famously popularized by Professor oz Picard at the MIT Media Lab and about 20 years ago she started doing work on affective computing and basically building systems that could recognize your emotions and respond to them so one of them is a company called efectiva and at factiva have built technology that recognizes or emotions from face expression so you can see here an example of using the the BB 8 ball and the ball Rhett responds to his emotion so when he's angry the ball runs away from him and when he's happy the ball comes back towards some kind of simple little demo but its characteristic of technology that responds to a single person's emotions [Music] so secondly in terms of experiencing no needed a piña is a very famous new media artists and journalists and she says a virtual reality offers a whole different medium to tell stories that really connect with people and create an empathic connection and so she's been over the last four or five years been developing immersive journalistic experiences that help you experience situations we not may not necessarily be in one of them she's at job a couple of very famous ones one was called a project Syria and she took a video footage of a terrorist bomb going off in a marketplace in Syria and then she recreated that as an immersive virtual reality experience see who walked through this market it looks very beautiful lots of people there and then after a minute or so you hear the bomb going off you see smoke and you see body parts and it's very traumatic experience and that helped people who aren't in Syria understand what was like to be living in that environment I won't show you that because it's a bit of a traumatic experience but I'll show you a second video clip from a project she did called Project Homeless where she tried to give people the experience what it was like being homeless and Los Angeles so this is the Project Homeless video and this isn't a VR experience based around actual audio clip of an experience of people home so in this case these people are waiting in a line outside of food bank and you'll see in a second the 3d environment and they're waiting together here were lined up and something happens and while they're waiting so you can start hearing a bit of a discussion going on and if you watch this man in the white shirt he collapses and he actually starts having an epileptic fit and so and then people start responding to that so in the VR environment you're able to experience what it's like being in that in that situation another example I really liked was a project called childhood that was shown at siggraph last year this is a very interesting project by Kenji Suzuki from the university of tsukuba and he wanted to create the experience of what it was like to be a child of course we've all been shot children but most of us can't remember anymore it was like to be four or five years old so he used a VR system to create that experience where you could basically where a oculus rift hemant display with some cameras but you took the cameras from your from your I point and you put them down at your belt and you move the cameras closer together to simulate the I separation of young child and also it was a special hand a little mini plastic hand you put inside your real hand that simulated the grip of a young a child as well as a child so you can see this when you're here transform your embodiment into a child by using wearable devices here we have the honda exoskeleton to simulate choice tiny hand and we also have a viewpoint translator to stream a child's prosper and perspective so this is the hand exoskeleton it's a different based on the five years all the children's dimensions so designers can feel the usability or products and toys for the children through the oh man this is the adult normal stager and normal ipd the popular distance and children has a little smaller closer of a fair distance so I make it a little closer and I mix my stager lower by removing my eyes like this and [Music] it's so the motion of the cameras are slaves who are here feature that giaghran who is so i tried that experience of cigarettes quite remarkable your edges feel like you're being transformed back to be a five-year-old and you couldn't grab big objects anymore you're looking at everybody in belt level you couldn't see up the table so it's quite amazing experience so so far I've talked about how using sensors you can capture and respond to individuals emotions I've talked about how using virtual reality you can have the experience of another person's experience and those are both areas have been very well researched right now and become very popular the third area that we're doing our work and there's an area of sharing at current experiences and what we want to look at is can we develop systems that allow us to share what we are seeing feeling and hearing with other people and it turns out this is an area that hasn't been very well researched right now so it's a great area to do I working and so last year all this year we developed a set of glasses called the empathy glasses and the idea is when you're wearing these glasses you can transmit what you're seeing to a remote person and also information about your face expression and your feelings as well and so we combined three pieces of technologies together first of all an eye tracker secondly the epson AR display and thirdly a special pair of glasses called the effective wear glasses and so one person wears this i'm using the epson displaying and camera we can send a video view to a remote person and we can use the eye trackers know exactly where they're looking and the effectively glasses tell you about your face expression so this allows us to send implicit cues about what we're doing to a remote person so effectively glasses are special glasses that we developed we had chinese or japanese partners where we took photo sensors and map them around the frame of a pair of glasses and when you perform different face expressions your muscles move the skin closer to the camera or to the photo sensors by measuring the distance we can know what face expression you performing so this allowed us to move away from having a fixed camera facing your face so by the system allows us to recognize eight face expressions I'm quite reliably so in practice we would have one person wearing this period glasses and performing a collaborative action like trying to do a physical task in the real world and then he would send video of that task to a remote person a rope per second a user pointer and to annotate the video and seen the pointer back so you can say odd no move this block or this object but we're also tracking the eye gaze position and that helps us give us information whether or not the person's actually following those instructions and so I'll show you a video of this working so here see here's the empathy glasses right here and here's the remote collaborator and they're trying to collaborate together and putting together a picture may other blocks so they're trying to work together about how they ought to put the blocks together the remote person can talk to him and move this green dot pointer saying move this block here the the red dot is the his eye gaze so this is really important because the the remote person can say move the block over here and if the person's eye gaze isn't following where he's pointing he knows the person isn't playing attention and also at the bottom here we have a heart rate sensor and a face expression sensor as well so as he starts changing face expressions we can know what expression he's performing so the overall message from this is that we can use the technology to provide implicit cues so normally with a system like this without the eye gaze tracker you would have to watch the person reaching out and grabbing objects with the real hands or talking about things but now because we're wandering eye gaze people always look at objects before they interact with them and the remote user now has a much richer remote collaboration with them and gets a bit of sense of what the person's feeling what they're doing so in a variety of user studies this is wouldn't from this first of all pointing from both sides really helps and remote collaboration the gaze is really exciting because it shows the context of what the person is talking about and it helps establish shared understanding and awareness and also a focus of attention the face expression actually didn't work out as well as we thought because we forgot when people are speaking they also exhibit lots of face expressions and so our face expression monitor was saying Pistons angry surprise but actually they weren't so but it was useful in certain cases where a person would give some options and the person listening might be feeling confused and that would show on the face exhibition monitor but of course it's easy to fix if we just have a microphone monitor the microphone and one of the microphones catching their speaking getting all those face expressions it'll be fine so there's some limitations with this technique so far we have some limited expense accuse the task we chose wasn't apically scary tire score didn't really elicit a lot of emotion and as I said before the glasses needs some improvement and the most recently work we're doing is looking how we can take that lesson and apply it into a VR environment so we're developing a verra environment now that involves a one person in playing it or being experiencing a VR environment and a second person also wearing a him out display but seeing exactly what they're seeing but then so their viewers slave to the the player and we have a variety of scary and exciting environments you can put them in but not only share the view but we also share some emotional signals coming from a hairy monitor and a GSR sensor as well and so if the theory is that by providing these remote effects of signals that should enable the remote person to feel some of the same emotion as a person who's playing the experience so here's a video of what this might look like this is a kind of a house experience look around the house looks quite normal but then after a while you discover this is actually quite a scary haunted house and you can see in the top right corner this is the live raw emotional data signal come today coming through the top one is the GSR the bottom one is a heart rate is a little bit of a lag of about a half a second because we're sending it through bluetooth but in a minute you'll see the person looking up into the ceiling and there are some dead is a dead body hanging from the ceiling which is quite surprising so you'll see the the GSR spike up and the heart rate also spiked up as here's a dead body here as well and the spike is coming in a second here it comes right now so there's a big spike in the output there so just to rip up in this talk what I've taught showed as how that you can use AR and VR systems for developing a panther computing interfaces V our systems are really ideal for for enabling people to try the experiences of others so they're very strong storytelling medium they're strong way to provide a total immersive experience and it's very easy to change body scale and representation on the other hand AR is a really great technology for an enabling live sharing and trying to help people share a live emotional and based experience that allows overlay on the real world supporting remote annotation and collaboration and enhancing reward tasks so I think there's a really exciting trend now towards empathic computing towards technologies that enable us to understand emotions to experience different people's emotions and to share them and certainly AR and VR enables these types of experiences by changing our prospectus by sharing spaces and experiences and by supporting communication but there are lots and lots of air directions for future research for example how to be really measure emotion how do we convey that to a remote person how can we establish ground truths in these areas so this is a research here we're just getting started off of my university if you'd like to collaborate with us on this would love to have some partners here's my website is my email address and here's my twitter feed so thank you very much for your time [Music] 