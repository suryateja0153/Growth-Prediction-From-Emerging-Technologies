 Coming up we look at the latest updates to software-defined storage with Storage Spaces Direct. New with Windows Server 2016. And, the advantages in terms of  cost, performance, and reliability. We also look at the deployment options from Hyper-converge or branch offices to converge for enterprise scale workloads. Keep watching, because we'll also look at  how this applies to disaster recovery. Microsoft Mechanics I'm joined by Elden Christensen from our software-defined storage group. Welcome. Thanks Matt, great to be here. Now, whenever we think about storage one of the first things we think about is cost. So what are you and the team  doing to help address that? So storage arrays have traditionally  been very expensive so we want to try to give lower cost alternatives  through software-defined storage We really started our software-defined  storage journey in 2012 and 2012 r2 with storage spaces. So if you kind of look at a  traditional storage array its kind of this black box, right? Its kind of this mysterious black box. But if you really peel that open and take a look inside it's really some x86 PCs, running software with an operating system. And, you've got some connectivity. You've got some disks on the back end. And then you've got some sort of a  front end on that you connect to it. That might be fiber channel, iSCSI,  or that might be FCoE so that ties it all together. So with with short space is what we've done is that we've really commoditize this and we've moved this software which was in the controllers up into the Windows Server operating system. This really reduced the cost of storage and then you can connect on the  back end SAS cables to an external JBOD enclosure. And then we use SMB3 as  our protocol to connect to on the top and then we can deliver local  like performance with SMB3. But there are still a few challenges here. You are limited to specific more  Dual Port, SAS hard drives, cabling, and the enclosure. What we did is with Windows Server 2016 we've introduced Storage Spaces Direct. And with Storage Spaces Direct,  what we've been able to do is completely remove all that external SAS enclosure, those external drives. You had to have multi initiary capable drives that you can have multiple machines connect to. And the whole SAS fabric. So we wanted to really reduce all of those costs and move that up into the operating system. So can you take a look at how we provision Storage Spaces Direct? Yeah sure, so let me give you  a look at how this works here. So the first thing I'm going to do is I'm going to create a cluster. So I'm going to run the new  cluster Cmdlet and we're going to create ourselves  a four node cluster here. You could do this through the GUI? You could also do this though the GUI as well. Right. So now that I've got the cluster created What I'm going to do now is I'm going to enable Storage Spaces Direct. So I've run the enable Storage Spaces Direct Cmdlet and what that's going to do is that's going to identify all the disks I have. I've got some NVMe devices. I've got some SAS devices  it's going to identify all those. It's going to put them together into enclosures and it's going to identify them and  set it all up for me under the covers. So now that I've got a virtual disk, what I'm going to do now is that I need to create a volume. So i'm going to create an ReFS volume on top of this. I've got a one terabyte disk and so I'm going to actually allocate  900 gigs to my capacity tier. And, I'm going to allocate  100 gigs to my performances tier. And so what that actually creates under the covers now if I take a look at that. You'll see that I have a parity tier of 900 gigs and I've got a mirror tier of a 100 gigs. So can we have a look at this in failover cluster manager? Yeah sure let me show you now that we got this up and running, let me show you what we got here. So here's my four nodes. And you can see I've got my four nodes here. So if I go and take a look at the pool you're going to see I've got my NVMe devices here. And then I've also got a set of SAAS SSD's. So this is an all-flash system so it's  really a very high performance system here. But it could be SATA hard drives as well? It could be low-cost SATA drives as well. And then when i go and take a look at this so here's my disk you can see  I've got a one terabyte disk. And it just shows up under the  C cluster storage namespace. So this looks like the exact same experience if you're using a traditional SAN today. We've been using the last couple releases. So it's a very consistent experience  with everything you're used to. There's no big learning curve,  just those simple three Cmdlets to set it up. Once up and running, it's everything that you know  and been doing for the last couple releases. Nice, so it's a very simple user experience. But is this converged architecture  and is this the only deployment option? No so there's actually two ways that you can deploy it. So we have the converged architecture  which we were talking about. And we can also do Hyper-converged. So that's part of the kind of power  and flexibility with spaces direct Is that we can actually can collapse that that  compute and storage layer into one layer and we can have now compute and  storage running on the same set of nodes. So to clarify each of these physical servers has Hyper-v and Spaces Direct configured on each server? Yep that's correct. Ok, so now we understand a little bit more about the deployment options. How does all this actually work? So let's take a look under the cover. So if you were to go look. what we're actually creating under the covers is what I showed you is that we've actually got a set of NVMe devices, we've got some SAS hard drives. We're creating a capacity tier and a performance tier out those. Then we're using up for caching. So what's happening is that when hot blocks are actually written down to the storage array those are actually written to the SSD's or the NVMe to that caching tier. And then as those blocks cool then we actually move them  down to the capacity devices. So that might be your SSD's, your spinning hard drives, or low cost SATA devices. So it sounds like we're really using  the hardware in an optimal way and to the best of its ability. But, how is it all redundant or resilient? Sure, let's talk about it. If you remember earlier in my demo,  I made that one terabyte volume. So what's actually happening under the covers  is we create that virtual disk. And then we actually carve that  virtual disk up and the 250 MB slabs. So then what we want to do is we want to take these 250 MB slabs and we want to spread them  across different fault domains. So in this case in this scenario a fault domain, would be like a node. So we want to put each individual  slab across the different node. So, here's just an example. I've got one slab of 256 MB  and spread it across three nodes because I'm doing a three way mirror. And then I have the next slab of 256 MB that I might spread across different nodes. This could be two-way mirrorings, it doesn't have to be three? It also could be two-way mirroring as well. Right, nice. So you mentioned mirroring and erasure coding, but how how is that used? Yes, let me actually talk about erasure coding first. You can have a three-way mirror. And then we also do the same algorithm  when we're going to do a parity. A parity can be actually spread in  those 256 MB slabs across the node. So in this example, I've got a four-way  node doing erasure coding. So I'm actually getting a fifty percent efficiency. On that last slide I was doing a three-way  mirror and getting one third efficiency. Here I'm doing a erasure coding and  I get fifty percent efficiency. And then that actually increases its scale. So if I were to add a fifth node to this cluster now I'm getting sixty percent efficiency because I've got four data slabs  and I've got two parity slabs. So let me talk a little bit about ReFS and kind of how that works because I talked about a mirror tier and I talked about erasure coding and how we're doing this. So what we're doing is that we are using mirrors to really optimize writes. So we're going to do a write we're actually going to perform those writes to the performance tier and we do that so we don't have to calculate parity during the write. And that really allows us to get  really high performance writes. And then later as data cools we will actually move it from the performance tier down to the capacity tier. Right, so this is all within the ReFS volume itself? Yeah this is all within ReFS volume. So you get that one terabyte ReFS volume and then we're actually always  doing the writes directly to a mirror and that way we don't have to calculate parity. And then as the data cools  we will then move it to the capacity tier. Now one of the interesting things is  that this really helps get us those IOPs by not having to pay the overhead of calculating parity. Now reads remember can happened from both the capacity tier or the performance tier equally. So, there's no overhead on a read it's always just on the writes, with the writes were writing to the performance tier. We do it to the mirror to get that off load and then we can do reads from either side. So what is performance really like, can you give us an example? Yeah, so let's take a look at a system here. So here I've got a four node Hyper-converge system with 4 NVMe devices. I've got 20 virtual machines running on each node in the cluster. I've got a load simulation tool that runs inside of them. I've also got a hundred gigabyte Ethernet connecting these nodes so it's a pretty beefy system and so it's pretty fast. It's four nodes and it's Hyper-converged. So what I'm going to do now is  I'm just going to resume them. So I've got the low generator paused. So as you can see on the top it's not generating a load. I'm going to go ahead and fire this up. That's going to kick off the load generator in each of those VM's? Yep. So I'm kicking that off. So now if we look at the top in green you'll see that we're pushing  about 60 gigs per second And that's gigabytes? Yeah, gigabytes per second. Right. So again this is 20 nodes with 20 VMs per node with four nodes, so this is 80 VMs  pushing against an NMVe device. That's pretty incredible performance. That is incredible performance. But, one thing that's important to a  lot of our viewers is disaster recovery So, how does this fit with disaster recovery? Yes so with a Hyper-converge solution you normally get your servers in the same rack and the same system. But, if you were to have a loss of a data center it's a great HA solution that is resilient to a disk failure, its resilient to a node failure. But, you really wanna be resilient a site failure. So you can actually take those nodes and you can stretch them across physical locations. And you can actually get a disaster  recovery and HA solution all in one. So we use another new feature which is coming to Windows Server 2016 called Storage Replica. And we use fault domains also here. So in this scenario,  we're slabbing that data across a site. And then we're using storage  replica to replicate that volume from one set of nodes to another set,  synchronously or asynchronously and we can have automatic failover across them. So a good example using synchronous rep might be Site A New York, Site B New Jersey synchronous rep between them. Yep exactly. Cool, so this sounds awesome  but how does somebody get started? What's a good way to get started? Yeah, so let me kind of show you some of the tools we have to help people get started with storage replica. So we have this nice Cmdlet called Test SR Topology. And what it does is it actually  goes and looks at your system and it will analyze it for compatibility. It will look and go and conduct some performance test. So it will actually go in and do some  data replication between the nodes and actually see what it looks like. And it will tell you how long  your initial synchronization takes. So depending on how big your volume is, how much data you've got, and then establishes how long it's going to take to erupt to the initials sync and get all that data pushed over. While it's doing that it will give you some kind of a graph you'll see here of some performance metrics on what's going on in the background. It will also give you information such as  how big your log should be. So if you're conducting the data replication and you need to know how much data can I sustain before have to do a full sync it will tell you that as well. So it will give you some nice kind of way to  prescriptively set up your deployment. Awesome, so it looks like we got some pretty cool stuff in Windows Server 2016 around software-defined storage,  but where can people learn more? Go download the evaluation. There are several other Hyper-converged  solutions on the market and I think you'll be very surprised how we stack up. Great stuff. And of course keep watching Microsoft  Mechanics for the latest tech updates. Bye for now. Microsoft Mechanics www.microsoft.com/mechanics 