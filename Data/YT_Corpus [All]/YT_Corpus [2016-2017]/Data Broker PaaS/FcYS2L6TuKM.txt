 hello everybody my name is Julian Fisher I'm CEO of any nines we are German based cloud foundry consultancy and specialized on Cloud Foundry data services the stock is going to be about delivering a production Cloud Foundry environment with Bosh so what's on today's agenda we're going to have a few words about Bosh and excellent automation technology we look at how to deploy a production Cloud Foundry runtime as well as solving the data service problem the missing part in Cloud Foundry so a few words about Bosh everybody with Cloud Foundry experience knows Bosh and most of the cloud foundries are deployed using it so that one is pretty obvious so what Bosh can do many more things among others you can deploy data service along with it so in any nines at any nines we started with running Cloud Foundry of course being deployed with Bosh and coming from a chef background so automating a lot of system configuration with Chef we had several years of experience with it but still Bosh convinced us that it is the right automation technology for a large amount of scenarios around cloud foundry so our team fell in love with Bosh a while ago so what makes Bosh so special first of all and the most important is it's very comprehensive when it comes to life cycle management of distributed systems and where other tools have a similar feature set rarely those tools are well integrated often you have to use several tools they do not interact very well and or seamlessly so with boss you actually take care of the entire lifecycle of distributed systems which makes bosch so special so in the cloud foundry context well we use cloud foundry to deploy stateless applications with ease and cloud file itself is a system that half state so how is life only deployed it uses Bosh so you know challenges such as deploying data services where state is essential a also is ideal ground for using Bosh so Bosh comes with a variety of functionality and features that is very interesting for example its infrastructure independence so the you know we started any nights with offering a cloud foundry based on on vmware in 2013 and migrated to x two different infrastructures one time we created another the new deployment and migrated to OpenStack for cost reasons and recently we migrated to amazon for stability reasons so this wouldn't have been possible without Bosh as Bosh allows us to take the entire automation with us and translated to new infrastructure along with infrastructure as or or customers you know some customers have certain requirements in their corporate policy on using certain operating system operating systems the operating system independence is also critical you know a lot automation technologies for example chef they do they do allow the usage of different operating systems but in the example of chef you have the problem that chef cookbooks will be lets say floated by supporting various operating systems so usually you have if-else constructs dealing with various package names and you know you also have to deal with resulting heterogeneity as various packaging systems of different operating system set up different directory structures place configuration files some wells and with boss you can actually take control of that so there's a unified directory structure independently from the operating system you using another nice feature that Bosh comes with is the separation of Bosh releases and Bosh deployments so where you have a blueprint on how the distributed system you are going to deploy may look like it is it does not actually tell in detail how it's done it's it's more in abstract a description of the components you want to you want to deploy in the end but it is the deployment manifest that really describes how the distributed system is going to look like in detail so that's very interesting because especially when dealing with data services this comes in very handy where you can have one boss release and use different deployment manifests to distinguish various data service plans for example a Postgres database smaller Postgres database baked postes database cluster small and a Postgres database cluster big and it's all described and one Bosh release but it they are separate separated by offering different deployment manifests Bosh has more tricks to offer one of which is rolling update so whenever you have a certain amount of different machines for example in a database cluster you can actually perform upgrades in rolling fashion so that only one instance per time will be upgraded so it won't take down your application system at once it also comes with significant monitoring and self healing capabilities so you have with inside of a Boche manager virtual machine you have a process monitor called monett that will take care that your processes are running you have a bosch agent that will take care that your monitor's running and this porsche agent will also send heartbeats to a boss director and once the boss agent goes down or the entire virtual machine goes down there's a bosch resurrector that will recreate this virtual machine but will also take care of your network configuration and remember it has all of that being operating system and infrastructure independent this of course includes the creation of virtual machines and management of storage persistent storage disks packing packaging templating and software configuration so everything from a single tool and everything being infrastructure in operating system independent that's a create technology I'd say alright so I mean nobody would be on a Bosch day without having you know some interest in bosch so let's dive into the topic of providing a cloud foundry environment with production readiness so what does it mean what does it actually mean to have a cloud family being production ready I'd say production readiness besides our factors such as scalability also means robustness and that's very important because basically every component in your system will failure at some time and most likely you infrastructure fails first because I mean infrastructure is where everything comes together for example you have physical service underneath whenever there's something on the physical server let's say you have a broken broken mainboard or whatever you know there's a physical holes going down and we'll take several virtual machines along with it so these infrastructure effect of this will this physical hardware failure will actually take down parts of you infrastructure and with that part of your platform so what do we actually want to achieve well first of all what we consider to be production ready for the rest of this talk is that a system is production ready if nobody has to get up when ordinary failures occur so what is an ordinary failure it's a failure that happens within a single availability zone so that's very important to to figure out that you actually will design your your environment to cope with certain failures and the means to do that is availability zones on the level of your infrastructure so it's very important that you get your infrastructure setup right in the first place because other otherwise you won't be able to really deal with failures in elegant way so the philosophy of infrastructure availability zones is that maybe you can prevent failures but you at least can contain them so setting up your your infrastructure should be done in a way that you have at least three availability zones so this means that you have at least three wrecks that you'll have three networks which that you have ideally three different you know power supplies that you have three different storage device storage servers so that these availability zones or it ideally also placed and you know physically separated so that you don't that they won't be affected by fire let's say so you want to entirely have three separate zones that in case of our failures won't you know be taken down at the same time simultaneously well that being said it's also very important to ensure that the network latency between those three availability zones is kept to a minimum so y 3 and y having a low latency that's easy it's because whenever use a quorum based service you have to provide a majority to separate networks partitioning from real failures so let's say one availability zone goes down how do i do how do you actually recognize that it's a availability so an availability zone that went down instead of just a you know temporary network outage so in order to do that you'll require a cluster manager and cluster manager require most of the time a odd number so the smallest odd number with redundancy is 3 so 3 availability zone is a good start all right so let's say you managed to get through the establishing 22 through establishing 3 availability zones what do you actually how do you actually tell bosh to use it so before recent bosch release we with cloud config what we did was to create a resort pool resource pool for every availability zone and then name the appropriate resource poor in in the posh job description so let's say you create a Postgres database as in this example you will map it to the availability zone to the resource pool small zone 1 in zone one will then have the cloud property de sky one so with blood config you can actually extract that from your deployment manifest and put it into your cloud config llaman where you specify acs in a separate paragraph and then you can use the availability zones directly in your job description so that makes the deployment manifest a lot easier as the declawed configuration is kept separately alright so we now know that one thing we have to do is provide redundancy we we have to ensure that on the level of availability of infrastructure there are three availability zones but what does it actually mean to create a production Cloud Foundry is that we have to go through the runtime configuration and deployment manifest and eliminate all single point of failures so we basically do clustering cluster everything the biggest basic strategy to do that is one that's independent from whether you're using das or diego or whether using cloud foundry or you know any other distributed system is you create a list of your system components and you check every component whether it's a single point of failure and when it's a single point of failure you'll check whether it can be clustered how do you do that you look into the service and follow it's very dependencies and check whether you can actually you know add redundancy to the to these dependencies so you cluster it if it's possible and and if you can't cluster it you know you won't be able to have it to make this system highly available so prepare for night shifts all right so let's look at a runtime manifest a runtime like Cloud Foundry runtime deployment with single point of failures as it is has been in the past so you know checking out postgres checking out Cloud Foundry came with a scenario like that so every Korean component was closed up by default or can be clustered easily and the orange the orange components they they haven't been clustered so you can actually very easily replace the pop store with an Amazon s3 object store you can easily well or and you have to create your post crest cluster or you have to create a relational database that's being redundant so let's have a look at this a little closer so they are basically two ways how you can do that one of which is as we've done it using a postgres cluster for you a and ccdb and an alternative would be the my sequel error cluster so let's have a look at the postgres cluster so that's how it's going to look like you have one cluster on three virtual machine machines are spread across three availability zones and yeah you can either share one cluster with CC and you a to be or deploy two of them that's up to you so what kind of challenges do you actually face when when deploying a postgres cluster with Bosh so first of all of course it's deployed and monitored by Bosh so the director deploys you're a master database slave and two slaves they have different IP addresses and any each virtual machine runs a Bosch agent you know sets up a replication manager demon as well as the console agent is that it is because in case a database one of the database service fails you actually want to be able to failover automatically and so how do you actually do that by default postgres does not come with a failover capabilities so we we've added the rep manager to the replication manager separate open source product that will do do failure detection and also perform automatic failover alright so how does it actually work you know when when the problem with failing over automatically even with rep miniature in place is that you'll have to provide a set of credentials to you a and Cloud Controller that points to your master part to your master database the credential must always point to master and at the same time IP addresses may change doing failover when promoting new master database server so how do you do that because you can't use IP addresses obviously in inservice bindings or your configuration well what you have to do and you can't use Bosh DNS names as if you can't really update those dns entries dynamically also bosch bosch internal power dns is a single point of failure and snot highly available so what we did is that we edit a console high-end console comes with a highly available DNS system so the system are the the architecture will then look like that you will have the Boche the Boche will create three virtual machines and whenever the virtual machine comes up it will register again against the console so the console dns then create provides a dns name this dns name you can use as a host name for the database configuration so in case the master server dies you actually you know the replication manager will recognize that and will automatically perform a failover by electing a new master triggering a promote script that will then talk to the console agent and update the dns name to point to the new master who will then be promoted you know by by the rebbe manager all right so now your database is is operable again but still you're on the creative mode and this is where using bosch the self healing comes into the play because the Bosch director health manager will recognize that there's a missing virtual machine and instructed director recreate the virtual machine so this virtual machine will be fully deployed by Bosh there it will when when started recognized by invoking the rep manager that there is already a master database master so it will turn itself into a database life so this way you actually not only have highly available postgres but you'll also have a automatic recovery from a degraded mode all right at this point weekly reached the checkpoint where we have a spotless Cloud Foundry runtime but a cloud foundry is not production ready without data services without production where you say with data services so let's have a look at data services well first of all the you know one of the main things is that applications they often have a very strong relation to data services so it's not enough that you have this wonderful applications of acrid called the cloud foundry runtime where you can deploy your apps and apps will be restarted whenever they fail because using a single database cluster and for example will be very dangerous because when whenever this database cluster goes down you know your you'll have a lot of applications being affected and most likely this is going to turn your your day into a nightmare so for that reason share data services are not an option from production perspectives so using on-demand provisioned dedicated instances is recommended so that scenario when a database goes down it will only affect a certain amount of failures so following the design fail approach you may not be able to prevent that but at least you have your problems contained so a question now is how do you actually provision those virtual machines automatically because we want to achieve is that whenever somebody creates CF create service Bosh should do the dirty work and an exemplary exemplary architecture is shown in this diagram that's what we've done in the past two years is you can see that whenever somebody creates service the color controller will talk to a service broker who will then trigger the creation of an issue of service specific credentials which will be used to trigger a deployment against a deployment component who will then generate a boss deployment manifest according to the service plan that's been picked and run this deployment against a a Bosh director Bosh will then do its job and create service instances so with this architecture which you can do is a great NAR betray amount of service instances of dedicated service instance is represented by single virtual machines or even clusters alright so also interesting is that with a solution like that you only have these two component being postgres specific in this example so in in order to add more way to be for example you also only thing I have to do is to implement a small class called the SPI and provide a Bosch release for it so having this solution on hand the creation of additional data services will take maybe two to four weeks so you can keep on adding new data service types to your platform at a high pace so this is how the creation of service instance will look like as I said earlier whenever they create service command is issue again the clock against the clock controller the service broker will be notified who would then prepare the deployment retrieving post gross specific deployment attributes from a so-called SPI that's one of the that's the only post rest specific component beside of the bas-reliefs so basically it contains configuration and deployment attributes that are necessary to and will be filled in later in the deployment manifest in the next step where the program triggers the deployment of a Bosch deployment using the any nines deployer who will then subsequently talk to the Bosch director so during the creation of of such a service instance which will take awhile the of course here the as in kronos service broker api from platform is used the claw controller will Paul against the against the Bosch director through the service broker and the deployer and when when the Bosch direct the Boche has finished deploying the this will return to the deployer and will trigger the service are broker to update its credentials that's very important because the service broker handles the available you know knowledge around which service instances are there so whenever you create a service binding the service program knows to connect to which to which service instance it has to connect so as the service instance is going to be a dedicated postgres cluster or post a server the service broker has to know about the credentials on how to login and then create a new database user this behavior is actually not within the service broker but within the SPI as deserves programs a generic component so let's have a look how this looks like if you put it all together and this an hour in this diagram you only see the post grow service so but of course you can add multiple services in the same way what would this diagram tries to illustrate is that you'll have three availability zones on the infrastructure level and that you will configure your Bosh to con to distribute all critical components well first of all all critical components are deployed at least three times and each of which is placed in a unique availability zone of course and we do this for the runtime components as well as for for the postgres service and in this case we're not talking about the post reserves of the UAE and ccdb which would be in the upper part where the runtime is but down there as you can see the post the any nines post cast data service it will issue on create service individual postgres databases so here you can see we have you know cluster 1 plus a 2 and cluster end so they can be an arbitrary number and whenever you deploy a postgres cluster our three virtual machines will also be deployed across three availability zones and that uses the same logic as a placement logic from Bosh s also you will be used by while deploying probably so in the end I can say that what is the Create companion for cloud foundry related automation challenges and that this explicitly includes the data services so thank you very much for your intentions for your attention and in case you have any questions feel free to reach out 