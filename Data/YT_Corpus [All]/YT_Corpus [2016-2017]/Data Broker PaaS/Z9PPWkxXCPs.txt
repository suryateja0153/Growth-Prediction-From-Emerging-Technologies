 well fondly come points by availability zone and OpenStack on this slide we provide planning sheet for Cloud Foundry jobs and we play I place only some of the jobs to give an idea how the plan is is done the values the valence in rule total represent the total number of instances in all availability zones also we have totals for each of three availability zones and also we have now a total number of memory and CPU the cells that are highlighted in yellow are the Cloud Foundry jobs that we recommend to place in three availability zones there are service registry called etcd which which should have three instances to fit ACA requirements and what we Gator traffic controller that is that we recommend to have at least one instance in every zone and as I mentioned application runners that maybe a DA or da gazelles and a major resource consumers in Cloud Foundry deployment our application runners some of the components of qualifying the deployment they do not support by default a she configuration and on this slide I highlighted some of these components one of component is the cloud control and you you a database databases to have it in kind of high availability mode we can configure a Bush resurrection process for database instance or we can use external my ETA Galera cluster other components in the deployment are Bush director for Cloud Foundry and cloud service services of automation Bush is not is not directly a part of cloud foundry but is used to manage deployment of quad foundry so we have to provide the plants out through color a single instance virtual machine another Knology component is blobstore by default anaphase mob store in our family deployment is single instance and we can use object storage to place blobs for example OpenStack Swift in our case let's take a look at the some of the details for this component so that what does it whoops what does a plan for a current board director look like the process quite straightforward you need to locate Bush state file and deployment merriest and also the persistent disk for most virtual machine it should be available first we have to edit this worst state file leaving only several properties and that were the ploy bush and the tush persistent disk in our tests according to this scenario board director virtual machine was recreated in around 25 minutes as an alternative we can use OpenStack VM migration case of persistent and generic drives are stored in OpenStack safe storage and during the migration process or recovery process ephemeral persistent disk disk they can be attached to a new virtual machine also instructs F it makes it makes possible to support life migration of virtual machine between in one availability zone and as the last example configuring a she support for non HD component in Cloud Foundry I provided the sample of deployment manifest to set up instead switched as provider for blobstore we have to define 3d actions and URL to connect to OpenStack and also so the temporary key and capability it's one of the value which has to be unique for every Cloud Foundry deployment if let's say on one OpenStack we have two two installations of Cloud Foundry and it should work in the next next life I would like to highlight information about how we can configure process to restore database virtual machine in our case we have a single instance of database that contains a Cloud Controller and a UA database the boss resurrection it is a feature that allows to have a recovery of virtual machine a using board and when we resurrection process it takes around two minutes to mark virtual machine as unresponsive and around three of four means to recover to recreate virtual machine is both resurrection process but we have to take into account the side-effect when we stop a physical machine essentially and drop wash a resurrection property configured for virtual machines running on this physical node both directed tries to recreate all virtual machines in the same availability zone and there should be enough resources we seen one availability zone of OpenStack to recreate virtual machines if they are configured with both resurrection as an alternative to such type of recovery for database instance we can use external my EB cluster for all CF databases now let's take a look at the cassandra storage in our case we use OpenStack self with the replication and the data blocks they are distributed among all storage nodes this means that one that single data read request videos triggers several network operations first the application on this slide it's it's an API it's called the central coordinator node and the second and the second step center coordinator not consequence cassandra data node that should have requested data role the Cassandra node data node from some specific compute node in OpenStack and the third step the compute node talks to OpenStack safe controller and the finally OpenStack safe controller rich data blocks from OpenStack storage nodes so there are four steps and there are four work operations to retrieve a single row of data and it takes us to pros and cons of using OpenStack surface the storage for the storage option for Cassandra we serve with OpenStack safe we can deploy all cloud services in OpenStack and it simplifies deployment and management because services service deployment can be automated for example Bibles and approach to deployment managed cloud services is unified also OpenStack safe it is distributed scalable and replicated storage so the failure of one physical driver or one physical node does not affect the availability of data blocks and as the last but not least point we can mention that the price of storage is such type of storage is quite cheap comparing to hardware storage area network systems what about the cons self storage we have an additional replication factor and totally with Cassandra data if we use replica factor of three which is recommended to replica factor and Cassandra we have six times dated one datablock replicated in from Cassandra and the performance of cluster it here oh it depends on the network performance so it is recommended in Opa State Department to use ten gigabit networks for four storage services in our case we decided to benchmark isandro in OpenStack to understand whether it can satisfy the project requirements we use cassandra stress this tool and the cluster sample cluster of six months there was a simple replication strategy with a factor of three and the network in OpenStack for all compute and data nodes for all compute and storage nodes it was one gigabit network every Cassandra note was configured with eight visual CPU and say it will gigabyte of memory and the ratio between memory and CPU is four and it is one of the recommended ratio for Cassandra knows the test was conducted with just one object in Cassandra and the proximate test duration was around five minutes so the next slide I provided some of the figures from from this test you can see there are three type of tests for write read and read and write operation stress test - it measures throughput as number of iterations per second and several latencies that shows the distribution of response time during the test on the slide we put number of iterations per second average latency 99% latency maximum and minimum latencies all the latencies they are measured in milliseconds in terms of latency deviation we may be interested in 99% latency and maximum latency these numbers they can give you an idea what to examine in details in terms of storage and Cassandra nodes configuration these types this type of tests can be executives very quickly after we install the cluster and it can give an insight what kind of performance is to expect from from Cassandra cluster let's see if our requirements used to serve 10,000 operations per second in with average latency less than 10 seconds then Cassandra deployment in OpenStack can easily satisfy this requirement but we have also to consider [Music] Cassandra data model and the application access pattern because they are also heavily influenced the performance of cluster and applications other recommendations for Cassandra plane and include effective data size for one Cassandra node is from a three to five terabytes and total number of tables should be less than she should vary from let's say 500 to 1000 tables to make compaction process and Cassandra effective and this free space on average salary node should be around from 30 to 50 percent to allow compaction process to complete yes for the recommended storage the data stacks recommends to run Cassandra on bare metal you Genesis D drives in jb OD mode so we just provide the drives to Cassandra Nova configured rs4 for Cassandra nodes and all the data distribution is is handled by Cassandra process so these are some of the technical aspects from the project that I decided to share and both part of my presentation I would like to say a few words about ultra sting contribution from this project so even when we work inside restricted area s healthcare we can find a way to spread ideas and experience during the project we created the Cassandra service broker that suppose authentication and k-space provision actually at the beginning of the project we searched all over the internet there are the where at the time several projects but they didn't provide all the service broker functionality and right now we're updating it the regular basis to accommodate all the changes for latest Cassandra version also we continuously improve ALK stack specifically we added number of input and output and we also provided extensions to Lex to log stash as an example there is a log stash a log stash extension to merge multiple lines of exceptions and stack traces in one message in elasticsearch this helps to easily to find the full context of any application error in in Cabana also we developed a web tool that allows developers to interact with and to work with Cassandra and to run any Cassandra statements and also store these statements in the history this type of web tool is useful in private cloud when there is no direct access from developer machine - Cassandra data node to develop this project we were inspired by data stacks Dev Center which is a desktop tool and to work with deft center you need you have to connect directly to the Cassandra cluster but in case of the private cloud which is allocated behind the firewall there is no way to connect directly to Cassandra datanodes and we made this project Cloud Foundry so it can be deployed to quarries regular web application these are some of the information that would like to share thank you very much and we'll get to answer your questions [Applause] 