 yes I wanted to talk about neural design principles for understood for understanding biology and advancing engineering so I think in this near morphic community I think one of the major bottlenecks towards getting useful neuromorphic devices may not be the hardware actually but algorithmic understanding design principles of neural computation essentially so that's kind of the stuff that we we think about so what's the stuff I'm not going to talk about so I spent a lot of time in close collaboration with experimental neuro scientists trying to understand how specific neural circuits instantiate specific behaviors there's a famous phrase in biology that nothing makes sense except in the light of evolution there's a riff on that for neuroscience that build Newsome likes to say nothing in neuroscience make sense except in the light of behavior and so we want to identify interesting behaviors and try to understand them so we've made lots of strides in terms of understanding how retina see how flies detect motion how grid cells how rats correct their internal estimated position based on grid cells how we can even understand aspects of neural dynamics correctly while only recording from one hundred neurons out of circuits containing 10 to the 15 10 10 to the fifth neurons or so and also how enhancing neural plasticity can either answer or learning I'll say a little bit about this but I wanted to spend more time you know the other thing we think about is theories of neural network dynamics a searching for general principles and so today I wanted to talk about that i'll give you a sort of four vignettes sort of for advertisements about our work the first will be on the role of synaptic complexity and learning and memory and here we're going to elucidate fundamental limits on the memory capacity of synaptic circuits given constraints on the on the complexity of synapses the next thing related sort of depend ease talk I think one of the secrets in sort of extracting design principles for neural computation is trying to understand the secrets of properties of high dimensional objects high dimensional spaces mappings from high dimensional spaces to other high dimensional spaces it's likely that the brain exploits many of these mathematical secrets in order to instantiate the algorithms that and behaviors that it does and so we're going to actually exploit the geometry of high-dimensional error surface it's a speedup neural network raining and deep learning is also you know of course a very you know very exciting field but there's very little theory in this field so we've recently developed a theory of deep learning dynamics and we can use this theory to develop random initializations that are related sort of to this edge of chaos idea to very rapidly train deep neural circuits and also in this community fundamental trade-offs between experimental resources and performance are of extreme interest right a speed energy and accuracy are some of those quantities that we'd like to understand trade-offs what governs trade-offs between these quantities and wonder what are the fundamental limits of achievability in this sort of three-dimensional space of trade-offs and we've recently in do elucidated fundamental limits and the accuracy of communication given constraints on both energy and time you would think that information theory has sort of solved this but information theory doesn't deal very well with time and we have a we have a contribution here so what will go through this stuff now if you want to relax almost everything that I'm going to talk about is published and I'll flash the slide at the end as well if you want you can sort of take a picture of the site but you can find all of our our work on this website just the last part on entry speed and accuracy is not yet published and sometimes I say things on Twitter if you want to follow that anyways so okay so let's talk about memory uh this is related to the co local memory topics so a memory frontier for complex synapses so if you ask a neuromorphic engineer or a theoretical neuro scientist or a neural network theorist what is the synapse from neuron Jaden you're on I and they'll they'll say that's easy it's just w IJ or they might battle fiercely between w IJ or j IJ we like to we like to battle about our notation but regardless it's just an analog scalar value denoting for example the size of a postsynaptic potential induced by a presynaptic spike if you ask an experimental neurobiologist what is the synapse from neurons age and your own eye they will give you a much more nuanced picture about a plethora of molecular signaling cascades there's there's a large number of different molecular players there's a large number of networks for example this is a phosphorylation network of kindnesses and phosphatases hiding within the postsynaptic density of every single ca3 neuron measured in south grand slab at Cambridge okay so when you see this your eyes kind of glaze over you know we don't know what this molecular complexity is good for functionally okay it must be there for a reason because it's conserved across many many species across many many millions of years of evolution so what's its functional purpose okay and do we need to exploit something like this on our neuromorphic engineering devices to achieve performance so we can't be driven by experiments alone we have to have a good theoretical idea as well as to why this is there and actually that idea also exists so you know we're all used to this idea of the hopfield model having exponential having a linear capacity for random patterns and maybe exponential capacity for very very special patterns that Eli talked about but that that is one of the seminal results in theoretical neuroscience right this linear capacity of the hopfield net work for random patterns but it turns out that that seminal result is actually on shaky foundations if you put in very simple biological biologically realistic assumptions about the synapses so imagine for example just a very simple consider the associations that is a very simple perceptron can store you know one of the things we have to think about is online learning so streaming experience you know we keep seeing a stream of of items and we want to remember them as we go along so we need an online learning rule as opposed to the hot fields original hot fuel storage rule which was not online so let's say we want to we're given a set of patterns we want to just come up with a very simple recognition memory device that says have we seen it or not a very simple online learning rule is to allow your weights to decay a little bit and then add in the new pattern or you know add in the new input output association that you want to store so this allows analog weights to decay slightly so you forget the past inputs and you add in the new Association to learn a new input you must have decay otherwise the weights will grow without bound so forgetting and learning our two sides of the same coin and a natural question is how far back in the past and synapses reliably recall previously stored associations and the answer is if the time constant of the of these synapses is order the number of synapses is order n then the past order Anna so see Asians can be recalled so you're in a happy regime where the number of memories you can restore grows with the number of synapses okay however when you look closely at this this solution relies on individual synapses to reliably maintain order and distinguishable analogue states okay so the typical degree of connectivity of a single neuron is about 10,000 neurons in the system sorry sorry 10,000 neurons connect to one neuron typically and it's unclear that the synaptic voltage you know induced by a presynaptic spike has 10,000 reliable analogue states right that that number is really order 1 and in some experiments it's it's it's actually one because you see these binary synapses that jump but regardless if that if the if you if you put in the biologically relevant scaling where the number of synaptic strengths of synapse can can can reliably maintain is order 1 not order the size of the network or the size of the degree of the network degree of connectivity in the network then you get a dramatically different answer so let's look at the very simple case where the synapses are binary they can either be weak or strong and if the conditions are such that the synapse should potentiate then maybe if its weak then it will potentiate with some probability Q but if its already strong it can't do anything else and furthermore if it's if it's weak then it D potentiates but if it's already weak it can't do anything else if you look at this very simple real model stefano and daniel amigo a while ago showed that there's this stringent trade-off between learning and forgetting if you make the synapses very jumpy so that you can rapidly learn anything that comes in right then the memory capacity of the system catastrophic lead grows only logarithmic with a number of synapses the basic intuition is if you learn quickly each time you see something you overwrite your synaptic configuration with what's required to store that thing there by rapidly erasing memory of your previous stream of experience you could try to slow down the synapses and make them sluggish and then the memory capacity can grow with the square root of the number of synapses but bet you you it sort of violates this this intuition that we viscerally store very very well what we just saw right so so so so you can either quick you can quickly learn but you'll quick forget or you can be sluggish to learn and that then and only then will be will you be slow to forget so this is kind of a stringent trade-off that we don't want to be bound by and we don't think the brain is bound by it so basically now we have both theory and experiment essentially demanding that we should expand our theoretical conception of a synapse from that of a simple scalar value to an entire stochastic dynamical system in its own right however this yields a very very large universe of synaptic models to explore and understand and we need theory to impose some kind of order on this model both to develop engineering design principles for neuromorphic engineering and to interpret the wealth of molecular biology data that we have on synapses that so far haven't been interpreted from a very theoretical perspective ok so the framework that we're going to exploit is we're going to really think of synapses as arbitrary stochastic dynamical systems we're going to assume that they have em internal states these em internal states are stable configurations of molecules in the synapse but we're not going to assume any particular mapping to molecules yet so the basic idea is so it's an arbitrary dynamical system with M internal states some internal states correspond to a strong synapse so this is the you know the this is a single synapse some of the states are strong some the red ones some of the straights are weak the blue ones and a candidate potentiation or depression event ie whatever electrical conditions are suitable for causing potentiation or depression or strengthening or weakening induces a particular transition matrix so for example for potentiation there will be an arbitrary M by M stochastic transition matrix a Markov transition matrix for potentiation and a different one for depression and actually in some prescient work by Montgomery and Madison they tried to summarize electrophysiology data on on synapses through this kind of finite stochastic finite state machine type diagram it's you know it's unclear whether this the the stochastic transition diagram is the same across synapses within a brain region across regions and so on likely they're probably quite different so the basic idea is we like to understand the landscape of all possible models ok so the measure what's the measure of memory capacity we imagine a simple scenario where we have a continuous online stream of experience we have a population of n synapses each with their it em internal States we're going to track the memory stored at a special time which will just call time T equals zero that that memory demands that some synapses potentiate while others depress yielding an ideal synaptic weight vector W ideal across the population we're going to imagine that the storage of future memories are just random random other ideal patterns for those memories and they caused the change in the weight vector over time and an upper bound in the memory quality of retrieval of any memory using neural activity is something you know is given by the signal-to-noise ratio curve which is just the dot product of the synaptic weight configuration across the population of synapses with the ideal pattern corresponding to the memory time zero appropriately Z scored so this is a memory curve that starts high and ends low and it ends you know it's the Chi goes infinity this memory curve will go to zero ok and this is a formalism that's been used by stefano foo see Larry Abbott Barrett and when Russell and so on we're just following that formalism and you know in previous all previous work that has looked at this problem have looked at one model or two models at a time right so this is one model you know so these are the strong weights sorry these are the weak weights these the strong weights potentiation moves you this way depression moves you this way and this is the memory curve for that model this is a different model this is the memory curve for that model and you know we can't we can't understand this field by looking at one model at a time to really elucidate the functional contribution of molecular complexity to memory we want to not simply understand individual models but understand the space of all possible models within this family and under sign the design principles involved so the types of questions we're going to be asking is this is a classical problem in structure-function in biology but suitably formalized right so the question is how does the structure of a synaptic dynamical system is specified by this pair of markov transition matrices determine its function or its memory curve what are the fundamental limits achievable over all possible choices of such a synaptic dynamical systems what is the structural organization of synaptic will dynamically to chivas limits and most importantly what theoretical principles can control the combinatorial explosion in the number of possible models as the number of internal states increases so these are the questions we'd like to ask and these are the questions we have answer so you know in the short talk I won't go through the details of how we derive it but i'll tell you the result the basic idea is we formulate the problem of memory is a problem in first passage time theory you know and and in this leads to a very nice way to for example order the states of a system so using results from first passage time theory we can impose an order on the set of synapse on the set of internal synaptic states do you have to take my word for it that this chain is identical to this chain and this chain is identical to this chain and then let me just tell you the final result so what we find is a finite time upper limit on the memory curve of any model and recall this a memory frontier right so this is the signal-to-noise ratio by time plane any individual model has it has a memory curve sitting somewhere in this plane what we find is a memory frontier beyond who is born the memory curve of no model can ever really cross okay and and and this is this is the memory frontier okay individual models can touch the frontier at one point but they do horribly at other points okay so to optimize memory at any one time scale you necessarily take a hit at other time scales the models that touch the memory frontier at one time scale turn out to be simple chains so essentially this is the model that is optimal for any one time scale and you can tune the time scale at which its optimal by tuning these transition rates and we have formulas for the optimal transition rates so that that's the nature of the that's the nature of the optimal structure and it turns out that various measures of memory the area under the memory curve the prefactor of this of this frontier the lifetime of member of a memory they actually grow linearly with the number of internal States and the synapse but only grows the square root of the number of synapses so so essentially what it's saying is it's much more powerful to have a small number of complex synapses than a large number of simple synapses and and you can design things you know trade-off m and n to get linear capacity in the number of synapses which you for dense patterns you can never really do better than that okay so the details can be found in this paper we're happy to say that actually won an outstanding paper award at nips you know a couple of years ago okay so that's sort of vignette number one right so this suggests that we really should think about synapses if we're going to have them be binary in our neuromorphic engineering devices we should have internal structure and history dependent transitions in these synapses so so basically the dividends of understanding synaptic synaptic complexity are threefold on the mathematical side we actually derive new theorems about perturbations to stochastic processes in terms of first passage time theory which I completely glossed over but are in the in the paper in terms of neurobiology it actually gives us a framework for interpreting molecular neurobiology data which I won't talk about but I'll briefly mention and in terms of technology potentially the next generation of artificial neural networks will will place lots of sophisticated signal processing capabilities in the synapses and there are other clues for this for example various solutions to spatial credit assignment problems rely on having internal synaptic states and recent results from Ricardo sakina and colleagues on learning as message passing also show that for example in little in learning the binary perceptron it's useful to have hidden states inside the synapses of the binary perceptron I'm going to skip what I was going to say about the connection to neurobiology here but we have a nice connection that tells us when when you enhance synaptic plasticity through various molecular and pharmacological perturbations when will you get enhanced learning versus impaired learning and we find that it depends on the history of the previous learning experiences of the wrath so there's a collaboration between three labs at Stanford that involves genetic knockouts optogenetics theory it's a very fun collaboration okay so let's move to the next next sort of idea okay so that this is now exploiting an understanding of the mathematics of high dimensional spaces okay just speed up your own network training so what's the idea so it's often thought that you know convex optimization is easy because there are no local minima every local minimum is the global minimum and it's often thought that for non convex optimization local minima high error Stan ender's a major impediment to non convex optimization and you know the idea behind that is if you just draw a typical random landscape say in two dimensions this is a cartoon of a protein folding landscape over two dimensions a typical random landscape that you draw on low dimensions does have many many local minima which might impede the the protein folding the protein to achieve its native state right but we know that our geometric intuition derived from living and moving within a very low dimensional world is woefully inadequate for thinking about the geometry of high dimensional spaces and it turns out that in random non convex error surfaces over high dimensional spaces local minima high error are exponentially rare in the dimensionality ok instead saddle points proliferate and we developed an algorithm that wrap of the escape saddle points and this was a collaboration with yoshua bengio slab so you know Yahshua invited me to come to Montreal I told him about these ideas and they said we can test these just test these ideas so what were the ideas so it turns out statistical physicists you in back in 2007 and in subsequent work looked at the geometry of high dimensional random landscape so what they did was they considered a random Gaussian error landscape over n variables and they asked what's the geometry of its critical points now let me just tell you the intuition first of all if you have an error landscape over a thousand over a million variables what are the chances with it when the slope vanishes it's a local minimum well for it to be a local minimum the function has to curve up in all 1,000 are all 1 million dimensions if it's a if it's a random typical generic landscape the chances that it curves up in all n dimensions is exponentially small in n unless you're already near the bottom so there's nowhere nowhere really to go down right or unless you're already near the top in which case most directions curve a curve down actually so essentially at intermediate levels of error you're going to get saddle points not local minima and they so what you can do to make this idea or tuition more concrete and then experimentally testable is you can plot each critical point a point where the gradient vanishes in a two dimensional feature space the horizontal axis is its error level and the vertical acts is the fraction of negative eigenvalues of the Hessian the fraction of negative curvature directions okay and you might think that all the critical points might live anywhere in this two dimensional feature space but it turns out they actually don't because the various concentration of measure properties they concentrate on a monotonically increasing curve here is where the global minimum stands it's the lowest error and and it has zero negative eigenvalues so it's down here here's the global maximum it has the highest error and it has all its eigenvalues of the hessian are negative because it's a global maximum and then basically what happens is critical points at higher error develop more and more negative curvature directions okay the chances of a critical point being an order one distance away from this is exponentially small in the in the dimensionality of the system okay so then you can mumble some things about universality we're used to in physics thinking that there or knowing that there are certain questions whose answers don't depend on the details they only depend on the dimensionality and symmetry properties of the system so we might say well this is we expect this to be true for neural networks because neural networks are also high dimensional arrow landscapes a computer scientist will have a visceral gut reaction to this and say no random landscapes are random they're not special at all the things that we're doing our very special where we're doing complicated neural network training their synaptic weights or structure it's far from random okay so who wins the physicist or the computer scientist let me tell you that the physicist one now been Joe is a variant yoshua bengio is a very enlightened computer scientist so we had a lot of fun you know he said we can test this idea so he put an army of his army of graduate student excellence a distance and postdocs and we essentially tested this idea now intriguing me this idea was never tested in physics because it's extremely hard to hold a random landscape in memory you need a memory that's exponentially large and the dimensionality but for neural networks you can do that and what they did was they plotted exactly so it turns out you can use Newton's method to find the critical points and they plotted the critical points in the same two dimensional feature landscape and they feature the feature space and they found exactly this monotonically increasing curve both for feed-forward networks on em NIST and on see far ten another category image categorization task and so this idea is qualitatively consistent with a statistical physics theory of random landscape so then the question is can we do anything about this can we exploit this knowledge now that it might really be saddle points that are the problem not local minimum at high error and so the basic idea is and I won't have a chance to really go into detail on this but but again it's all in the paper the basic idea is Newton's method gets attracted to the saddle points because it goes down the gradient and then you're dividing by the Hessian but if they have seen as negative curvature you're turning yourself around and then going up the gradient so a very simple idea is to divide by the absolute value of the Hessian which is just the Hessian with all its eigenvalues replaced by their absolute value its eigenvectors unchanged and so this basically repels the learning dynamics from saddle points and there's other ways to derive this that makes sense even if you're far away from saddle points in terms of trust region methods and so does it work so basically what we did was for a deep auto encoder problem and for a recurrent neural network problem we just trained using stochastic gradient descent and we found a plateau in the learning dynamic so this is a training time an error and there's a plateau so once you hit a plateau you think you're in a local minimum but it actually turned out that this was an elusive local minimum was actually a plateau surrounding a saddle and when you switch to this new learning algorithm you see a sudden drop in the error okay so basically what this did was it found a negative curvature direction around a saddle and got repelled down that direction to increase to decrease the error okay so okay so that's sort of the second vignette the next two are not as long so we'll be fine so so the theory or they could be a lot longer but but but any case i'll try to keep it short so that so now what about the theory of deep learning dynamics okay so can we say anything about that what we so this is joint work with Andrew Sachs and Jay McClelland at Stanford Andrews a really great graduate student that I advised along with Jay and Andrew and now he's a sports fellow at Harvard so what would we like from a theory of deep learning dynamics well we'd like to understand how does training time scale with depth how should we set our learning rates to scale with depth how do different weight initializations impact learning speed and and you know we did a lot of theory it but I'll just tell you the upshot what we find is that weight initializations with critical dynamics which I'll explain it a bit can age deep learning a generalization so what is thought to be another impediment for making deep learning hard it's the credit assignment problem or the vanishing exploding or vanishing or exploding gradient problem so basically in order to figure out how to change synapses of the activity of neurons down here to correct the activity in the output you know you need to know how what happens if you wiggle this neuron how will this neuron respond and then how do you use that to change an error well you can back propagate the susceptibility down the chain that's back propagation so what happens is back propagation is fundamentally a linear approximation it's a product of jacobians or susceptibilities between neural activity at one layer in the next and if the weights are large this product could explode and if the weights are small this product can decay okay so the question is can we understand this a bit more quantitatively and it's very difficult to actually understand the dynamics of learning these nonlinear networks so what we did was we tried to look at the tried to look theoretically at the dynamics of learning in deep linear networks now lest you think that we're throwing the baby out with the bathwater because of course the composition of linear functions is still linear so that deep linear networks do not have any more expressive capacity the deep shallow networks it turns out that learning dynamics depends critically in depth and the learning dynamics is highly nonlinear because the error function still involves a product over weights okay and we know I'm going to make a long story short and skip these slides but we analyze these learning dynamics you get these non linear ordinary differential equations that are typically difficult to solve we found exact solutions to these equations it turns out for example a simple network like this with one hidden layer is successively building up the singular value decomposition of the input-output covariance matrix of our data so it gives us insight into how the statistical structure of a data set in a time-dependent manner is embedded into the weights of the neural network but that's not and we use that to model infant cognition but I you know here but I don't want to talk about that one I wanted to focus on was a striking prediction of this theory okay the prediction of this theory is that if you can randomly initialize the weights in a certain way where you pick these weights to be random orthogonal matrices so you pick these w's to be random orthogonal matrices or if it's rectangular random matrices all of whose singular values are exactly one then you can circumvent the vanishing exploding and great exploding or vanity you can circumvent the vanishing or exploding gradient problem and we can actually prove that you will get learning times that are independent of depth at least learning times measured in the number of gradient evaluations right of course in a deep network the evaluation of anyone gradient must grow with depth but you might think that there's an even more adverse scaling to learning time where the number of gradient evaluations have to grow with depth and we show that that need not be the case with our random initializations okay other people tried other random initializations where they chose these to be random Gaussian matrices so that the back propagation vector vectors norm would typically be preserved but we show that that actually doesn't enjoy depth independent training times so you know this was a striking prediction that we actually didn't believe but we believed our math so we we became computer scientists who don't believe equations and have to simulate it to believe it we sank that low I'm kidding of it anyways but uh so we did the simulation okay and this is what we found so this this initialization using random Gaussian weights properly scaled so that you don't get vanishing or exploding gradients does according to intuition have depth dependent training times so this is the number of training f Fox to reach a certain error threshold and this grows with depth thought on the omnium pneus problem if you switch to a random orthogonal initialization you get our predicted depth independent training times so for depths that are five six seven or eight there isn't much of a difference which is why they could get away with these random Gaussian scalings but as the depth gets much much larger there will be a huge dramatic difference between this and essentially also for recurrent works that are essentially infinite depth it's also very useful okay we verified that and we actually understand theoretically sort of why this is happening based on random matrix theory and products of random matrices you can see the details on the paper but basically what happens is these random orthogonal matrices allow information about errors to isometrically propagate through the network it doesn't only preserve the norm of any one error vector it preserves angles between error vectors that's the critical idea okay and it also works in non linear networks as well so even if you have the non-linearity what happens is this dynamical I saw isometry property holds and and we can get much better performance and speed and training nonlinear deep networks even though the theory was derived for linear deep networks and we have some hand waving arguments why the theory of nonlinear learning dynamics in linear networks is robust to the incorporation of a non-linearity in the neural dynamics right so so basically this dynamic isometry kind of you know out of all possible other you know the interrelations done the does the best in 30 layer nonlinear networks I could mumble some things about criticality in the brain and the connection to this but I'm not sure how seriously to take the field of criticality in the brain but anyways that's there okay so just to summarize this part a major impediment another major impediment to deep learning is the faithful propagation of error signals across many layers previous attempts to combat this problem by choosing good weight in US relations had learning time and number of corrective steps grow linearly with depth our new random initialization enables learning time to remain order one as the depth goes to infinity even in nonlinear deep nets and these weight initializations correspond to dynamical dynamical critical networks they're sort of just beyond the edge of chaos all right or at the edge of chaos okay so then I just have any two more minutes for the last part so I think I'm like almost done so so so simultaneously considering energy speed and accuracy and computational design so a particular communication so as this is joint work again with with shabana Lahiri who also did the work on the memory from for complex and Absolut's he's an excellent post doc we'll probably on the job market next year i would i would really suggest you hire him in any case okay so why has an information theory solve this problems right so information three places limits on the accuracy of communication given for example energy constraints for example the cost capacity curve but not time constraints right so what it does is gives you bounds on bit weights in terms of energy constrained channel capacity but achieving such bounds requires long block potentially requires long block lengths you take your messages you code them into long blocks and you send the blocks through the channel biology doesn't have the luxury of waiting right it has to send information right now it can't block information otherwise you could get eaten before your brain knows what's going on there's also the thermodynamics of computation that shows that accurate computations at zero energy costs are possible actually but it requires finite time right and the basic idea behind the old field of reversible computing is that they isolated the energy cost of computation in racing bits basically if you erase the information in a computer you compress the physical states of the computer you reduce the entropy of the computer but the second law of thermodynamics you dump entropy in the outside world which means you dump heat into the outside world which means you dissipate energy so you can try to avoid this by making the computational reversible but then without a thermodynamic driving force to move the competition forward it requires infinite time so time is very very hard to incorporate along with speed with with accuracy and energy okay so what we've done is we've proved a theorem a very simple theorem that's easy to state and relative I mean a little bit more difficult to prove but easy to state so I'll just ate it so basically what we assume is let's assume that we have an arbitrary signaling substrate that is some markov chain okay we have an external signal that's coupled to this Markov chain by modulating the rates this is typically what happens in any chemical kinetics scheme that systems biologist for example right down then we have an arbitrary unbiased a signal estimator that looks at the state of the Markov chain and tries to estimate the signal okay the speed that we're referring to okay so so there's going to be some time scale the fastest time scale in stochastic that chemical systems ow that sets the units of time the speed which we're referring to is the amount by which the SEC sternal signal changes over the time scale of the intrinsic dynamical system so this is speed energy is the amount of power so we're assuming that this Markov chain is sort of coupled to a heat bath it's a physically realizable Markov chain so we can define power dissipation so this is the amount of power dissipated in the fastest time scale of the system / the units of thermal energy so this is enter dimensionless measure of energy and accuracy is just the inverse variance of this estimator we can show that the product of speed and accuracy is less than energy it's a very simple theorem so if you want greater accuracy for a fixed speed you better spend more energy if you want want to maintain a given accuracy for a faster changing world you better spend more energy this bound is tight for systems that have a single time scale so it's not a loose bound at all and the basic idea is that we make a connection between energy dissipation as quantified by thermal dynamic friction tensor on the manifold of Boltzmann distribution associated with stable states of these this family of Markov chains we make a connection between that and Fisher information so we essentially show that Fisher information lower bounds friction and then everything falls out from that and we get this you know through the Cramer outbound we get this so this opens up a framework for thinking about what are the fundamental limits of communication under these three quantities simultaneously and how close does biology come to these limits so we're very excited about this ok so again you can read about everything on our web page here except for the speed accuracy energy we have a manuscript that will probably done in a week or so a week or two it's almost written up and yeah thanks we have questions after a question our next speaker yeah Chris hi surya so with regards to the UM the f / e levain and how i'm different datasets make a rise to different curves in that plane for example the the curve or the amnesty dataset seem to have a very different shape than that for the Seaport innocent the question that I already have it you have a good deal understanding I mean I'm actually having a really hard time hearing you for some reason so I don't know what your question is at the moment actually do you want to state it just like in a sentence or two I'll try to so you have a sense about what in a given data set will give rise to different curves in the f / e plane so this is for the dynamics of deep learning part nobody's southpoint point for the put this will put the instructor of the other services in high dimensional spaces oh oh yeah yeah yeah yeah so I think that monotonically increasing curve is fairly Universal because the essential intuition is just in a high-dimensional space it's extremely hard to get all directions to curve up unless you're already near the bottom so if i get that point my question is move is a little bit more detail so if you look at the amnesty sort of the Steve our curves although the arm monotonic they still dakota Meyer I qualitatively different so if you have any intuition finer resolution a version of that we it's very difficult to get that we don't have a finer resolution statement than that look hopefully there's time but uh I very much enjoyed that so I just had a question about also about the second part so it seems to me that that that addresses optimization in general with high dimensional space is not just anything about neural networks is that true is that the way you yeah yeah that's the way I see it is a very general statement about high dimensional non convex optimization and and do you think that have you analyzed sort of the dimensionality you need to for that approach to be optimal yes so in the in the numerical network simulations we had order of a thousand synapses so a thousand dimensional space and by then it was already working pretty well we actually observe the predicted reduction in performance relative performance of the method if you lower the dimensionality so this isn't one of those intriguing algorithms that works better for larger networks than it does for smaller networks as predicted by theory thank you 