 hi I'm Katie Schumann I'm a postdoc fellow at Oak Ridge National Lab i presented last year and nice on with a colleague on some of our neuromorphic architectures we've done a lot of work in the last year but I'm going to focus this presentation on what we use to train our neuromorphic architectures which is evolutionary optimization so I'm going to do a quick review of our architectures from last year then I'll tell you about the framework that we apply and how it's different for the two different architectures I'll give you some preliminary results very preliminary toy problems from the UCI machine learning repository that have given us some encouragement and moving forward and then I'll tell you how I think this can be applied to neuromorphic architectures in general so the first architecture I want to talk about is neuroscience inspired dynamic architectures or need end networks nita networks are spiking neural networks that are embedded in a three-dimensional space they have extremely simple neuron and synapse implementations we have two parameters and our neurons a threshold in a refractory period and two parameters and our synapses wait and a delay this is an abstract architecture that can be implemented in a variety of ways the results that are presented here are based on our software simulations and we do our software simulations with discrete event simulation zand you can see some example behavior of a need a network on the bottom right of the slide you could also see an example a very small example at need a network on the bottom left here what you'll notice is that there's no particular layered structure the neurons can be located throughout the three dimensional space and any two neurons can be connected by a synapse so when we're training these networks we're training not only over the parameters of the network but we're also training over the structure of the network the second architecture that we've looked at is dynamic adaptive neural network arrays or Dana's Dana's are an implementation of needed networks and hardware but there are some major differences as we move into the hardware from nita the primary feature that we share with nita networks with Dana is this program ability so this was a key feature in developing this hardware our Dana networks are made up of a raise of programmable neuromorphic elements so you can see an example Dana Network four-by-four here each of the boxes is a programmable neuromorphic element what we mean by programmable neuromorphic element is that it can be programmed to be either a neuron or a synapse so the simplistic implementation of the neuron and the synapse and the nita architecture lended itself to a simple implementation of this neuromorphic element that still encompasses the behavior of both a neuron and a synapse in fact you can see the gate level logic of a single per single element across the bottom of the slide sort of it might be kind of hard to see one of the major differences in moving from nita to Dana is we have this restriction and connectivity so any given element in our array can connect to up to 16 of its neighbors we can form more long-range connections by chaining together sin taxes if we need to but we've we've had success with just these 16 connections so far the current implementation of Dana is on fpga but we're also working on a custom chip implementation and one of my collaborators Garrett Rose will present tomorrow on an implementation of Dana that also includes memristors the work that's presented here we got all of these results using our Hardware accurate software simulation the software simulation matches our FPGA results exactly so the framework that we use for both Nita and Dana is an evolutionary optimization framework it's a basic genetic algorithm where we have a parent population of networks we have a fitness function that we have for each individual task we evaluate every network using the fitness function we rank the networks from best to worst we do selection and reproduction to produce child networks the primary difference between what we do with nita and what we do with Dana is how we represent the networks in our population for Nita the representation is a graph representation where the neurons are the nodes and the edges are the synapses for Dana the representation is the array otherwise the exact same framework applies we have custom crossover and mutation operations for each of the representations that operate directly on either the graph structure for nieto or the array structure for Dana so we've successfully applied this framework to a couple of different tasks there's a lot of results on these tasks on these slides so I'm going to try to cover what all it's showing here the first data set that we looked at was the iris data set this is a very simple data set from the UCI machine learning repository but there are lots of reported results on this task so we've got a bunch of comparison results on the slide here different neural network and neuromorphic implementations the different color codings on the bars represent the different types of techniques so traditional neural networks train with back propagation evolutionary algorithms and hybrid methods are shown in pink blue and purple the hardware methods are shown in orange the harbor methods and include memristors are shown in yellow and the different spiking neural networks are shown in the different colors of green the heights of the bars are the classification error on the testing set and the different gray boxes are the different numbers of training examples that were used in the reported results so the Nita and Dana results are indicated with the arrows here as you can see we did achieve the best performance on this task nita had the best result across the board on the software Dana also outperformed many of the software implementations which we were very encouraged by and outperform the other hardware implementations as well we looked at the Wisconsin breast cancer data set also from the UCI machine learning repository the color scheme is the same here for the different methods you'll see a different type of method is included here the ensemble neural network methods as well nita and Dana did not achieve the best results here but they were comparable with the other methods the best hardware implementations for this task were the memory stirs train with a back-propagation but they achieved comparable results with Dana and with Nita the third task that we looked at is the payment Indian diabetes data set once again Nita and Dana did very well on this task we got comparable results with the other hardware implementations and the other software implementations but the best results overall were achieved with ensemble nip ensembles of neural networks which we could also do with Nita and Dana moving forward to try to achieve better results so we're encouraged by these results moving forward these are kind of toy problems but they give us a direction to move in building our architectures so one of the things that i'm working on a Tokra is building a library for arbitrary graph optimization the idea of this library is that it can be quickly applied to new neuromorphic architectures that can be represented as graph structures so if you had a neuromorphic architecture that you wanted to learn more about there are a couple things that you would have to specify and then you would be able to use this type of software the first is a graph template file which would define the node types and the edge types that you have the parameters of those nodes and edges and any connectivity restrictions you may have we would also ask you to specify an evolutionary optimization template file which tells you which parameters and structures can be manipulated as part of the optimization we have an internal data structure a graph data structure and we would ask you to define to user-defined functions for that architecture that convert from your existing data structure to our graph data structure and then for any given application you define a niche and initialize graph function and a fitness function that will be used as part of the evolutionary optimization so all of the results you saw previously were obtained using custom implementations for Nita and Dana of the evolutionary optimization but I have also used this arbitrary graph optimizer to optimize both Nita and Dana and feed forward neural networks so it can be extended to two other types of architectures moving forward so what I want to emphasize is that evolutionary optimization is a convenient way to start to explore the characteristics and capabilities of new neuromorphic architectures I don't think it's going to be the end-all be-all answer to how we're going to train these devices but if you don't know anything about how your architecture is going to work what does capabilities are what types of problems can it solve this is a good first step to start to explore it we've had success on these basic benchmark tasks using our evolutionary optimization framework for our two different architectures and I also want to emphasize a couple of the advantages of using evolutionary optimization it can interact either with your software simulation or directly with whatever hardware is available which can be a major advantage it can learn hyper parameters so if you already have a learning method in place but you still have some characteristics that are user defined evolutionary optimization can work as kind of an outer loop in that learning it is scalable for high performance computing implementation so as much computing resources you want to throw at this problem evolutionary optimization can scale with it and what I'm most excited about is it generates lots of networks and their performance characteristics for study moving forward so we can learn more and more about what these architectures can do how we can develop new algorithms and how we can refine the architectures to improve them so I've got all of the references for the reported results here I would like to thank my collaborators both at oakridge and at the University of Tennessee especially Jim plank Adam Disney and John Reynolds who helped compile these results and one brief promotion slide we are having a neuromorphic computing workshop at Oak Ridge National Lab jun 29th through July first if you're interested in learning more about that workshop please visit our website send me an email or come talk to me over the next couple of days thank you very much 