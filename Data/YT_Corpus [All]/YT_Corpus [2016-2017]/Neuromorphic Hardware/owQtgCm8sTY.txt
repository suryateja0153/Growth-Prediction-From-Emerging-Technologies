 thanks to the organizers for this interesting meeting it's really a pleasure to be here so I should have had the word memory in here so it should be strong error correcting codes and exponential memory capacity in neural representations you know the question came up yesterday what is it that brains do that might offer a promise for improving upon conventional computing right and you know it can be build exascale computers can we can we do some of these things so I don't know I don't know the answer to these questions and I obviously none of us do really in this room but I hope that today I can provide you a couple of interesting examples one directly from the brain when inspired by the brain of computing that really is qualitatively different from the kinds of computing that we normally think about in incremental computers okay so just like penty made the case for you yesterday I want to make a few observations about neural representation it's high dimensional it's very distributed it's parallel and it's implemented by dynamical systems involving elements that are each relatively short-lived or memoryless in their responses and their individual dynamics and what I want to show you is that these properties can lead to really qualitatively new interesting capabilities okay so let me start with directly with an example from the brain ok and some of you may have heard of grid cells so I'm going to talk about the grid cell code and how it enables a qualitatively different kind of representation then we have previously known about from other measured codes for variables in the sensory and motor peripheries all right so let me try to unpack that so here's a picture of the response of a single cell a single grid cell so what you see over here what you see over here are a black line which is the trajectory of an animal as it's running around in a box and a flat 2d plane and the box is just about a meter per side and the black is the trajectory of the animal and each red dot here represents a spy committed by a single neuron right so this is all of these spikes are from just a single neuron and what you can see is that these spikes are sort of happening all over the place unlike place cells which most of you probably have heard about which tend to fire at just sort of one neighbourhood of a location in space this grid cell this particular cell is firing in multiple locations but also you should be able to see that the density of firing isn't uniform over this whole enclosure in fact the firing has these fields these discrete blobs and the blobs themselves are arranged in this nice periodic pattern this nice equilateral triangular lattice pattern okay so this is a cell in their 2/3 of the n-terminal cortex it was discovered in rats but it's there's evidence since then that it exists in bats in mice and there's also some evidence that it probably these cells exist in humans so probably this is a general mammalian system for for and the firing of the cell is clearly related to some encoding of space okay so this is one cell all right and so if you want to sort of um formalize what it is that the cell is doing let's just consider one dimensional mathematical statement about what what these cells are doing so I showed you one cell so one cell is coding has this nice triangular lattice if you look at a cell next to it a neighboring cell right so we just go back here the neighboring cell to this cell has also the same triangular lattice pattern the same spatial period the distance between the blobs it also has the same lattice orientation which is the orientation of these two primary lattice vectors the only difference between neighboring cells really that our grid cells appears to be like a shift in the 2d phase so just a 2d translation so in other words these different cells are together encoding location in the same way in this periodic way but there are different responses are just phase shifts of one another okay so in 1d in blue you can think those yours I've just plotted one cells periodic response purple is another cell and pink is a third cell right and so if you know that this cell is firing a lot then you know something about position in fact you know that the animal is at some position X such that X mod lambda which is the period of this periodic response divided by lambda is say one quarter which is the phase of this cell and if you knew that the purple cell was firing you'd know location is a phase again but the there's no information in this response about where the animal is across periods because the response repeats itself the whole population of cells were the same period the response repeats so all you have information about is location modulo the period okay so that's the coding of position in this whole population of grid cells so it turns out that that's actually not just one population of grid cells there are actually a few different populations of grid cells so as you go along this cortical region along one axis this torso ventral boundary there are groups of cells that have a given period and as you systematically move up to the ventral end of the boundary the there's a systemic change or these there's different groups of cells have different periods in fact these changes in period are discrete so there are just a few discrete different periods in the representations and the periods all fall in some you know one decade very narrow range so about a third of a meter is the smallest period and the biggest period is actually less than three meters so you know this is only one decade in scale all right and for reference the kinds of distances and animal travels over the course of the day these rats travel 100 meters to one kilometer per linear dimension per day in search of food and so that's kind of the Barrell distance scale over which they would want to encode and decode their estimates of position okay so so clearly even the biggest period is much much smaller than the skills that we're talking about here okay so what is the code then for positions so because there are distinct periods the position variable X is represented in one group of neurons with a with a given period lambda one as this phase X mod lambda 1 over lambda 1 but then there's a separate phase with respect to the second period lambda 2 and so on up to the the end period lambda N and their order 10 different periods in a given animal okay so so n is order 10 so this is what I like to call a population of population codes each population codes one phase and then there's a set of populations that encode these different phase variables okay so this seems really bizarre right I mean you've got this position variable which is you know two scalar quantities local scalar numbers and they're being encoded by these periodic functions and very non-local and complete periodic so why would you want to encode position that way so it turns out that so the this kind of representation representing a number X by its moduli okay modular phases has a name if X is an integer and lambda is an integer and we get rid of the dividing by lambdas this is actually a number system it's called a residue number system in computer science and residue number systems are very interesting systems so they have some interesting properties so we're used to fixed base number systems so if I want to represent numbers using our conventional decimal system you know you have these registers and if you have five registers I'm six registers you can represent a number as big as 10 to the six and you know so so you can do that but if you use a modulo system you can also represent a numbers because a million but you could just use these three different periods or moduli or you could alternatively use six different moduli but now there's a big difference which is that here the moduli span many many orders of magnitude and scale okay but here in the moduli are all about the same size okay which is from a biological point of view this is very interesting because as we all know it's very tough to construct things that span many orders of magnitude in dynamic range or in size it's there's a parameter that would need to be to and to cover six orders of magnitude to get these different periods here you can get away with all periods that are roughly the same okay and you get something similar so so first of all the network parameters don't need to span several orders of magnitude and you require only a small dynamic range okay it's also very interesting for another reason so we heard from penty about holographic or distributed representations where each sort of piece of the representation carries the same weight as the other pieces so now in if I want to represent the number 45 using a decimal representation this is it 800,000 this is an eight hundred eight hundred thousand and one and this is it and you know as I move by one unit all the other all the other registers stay unchanged but only the one unit register changes right and so in fact as I move through by you know a unit at a time this is the only residue or the register that changes only when I changed my tens or hundreds or thousands or ten thousands do the other registers changes a very non whitened non equal non holographic representation in that sense my contrasts and the modular representation if you have roughly equal size moduli you know a shift from 800,000 to 800,000 one involves changes in all of these modular they're all kind of doing the same thing it's a very distributed and whitened representation okay and also um you know damaging if you if you if you destroy the unit here the information and in one of these registers here you destroy information another specific scale here if you destroy one of the units you destroy information on all skills equally okay and so so so it's it's whiten in that sense okay another interesting property about this modulo system and by the way it's not only a system for representing integers so in the residue number systems are for integers this is a generalization to representing even reals right modulo operations well-defined for real numbers as well okay so the other thing that's very interesting about modular arithmetic we we talked yesterday about actually the the time complexity of addition in this system the time complexity of addition so in decimal you know if you want to do addition or in any fixed base number system you have to first do the smallest register add up look for any carryover go on and so on so the time complexity in a naive system is n in a more sophisticated system it's like log n it turns out that addition and modulo systems is completely parallel so two plus four modulo five is one and so you just write that down and even though there was a wraparound you don't actually have to carry information over it so you can verify that the sum of 97 plus four written in modulo system is just three five one over here without any carries okay so you need it's completely parallel non serial updating across moduli so there's a speed-up here the the the the time complexity of computing that sum does not depend on the size of the sums that are being computed okay and from a biological perspective as an advantage because you don't have to build wiring that pushes information from one of the registers to the other okay so from a neuromorphic or biological perspective that's also an advantage so so so that's that's an interesting property as well and the reason why arithmetic is an important thing to do in this grid cell representation is the system is representing position okay and to represent an updated position as the animal moves some display it has a displacement moves by some Delta X you need to now update the position estimate by adding the displacement to the representation and then arrive at the new representation right so the system is constantly doing these addition operations okay there's another very interesting property of the system so this this system is this coding this coding into these modulo residues is it's not structure preserving or metric preserving in the following sense so if you have two numbers X and y that you are representing right if they're two numbers X and Y and they have a certain distance between them so two positions or two locations of the animal it's a certain distance between them it's not the case that the distance in the code words so see of X is the modular representation this vector code for X the distance between these is not equal to the distance between these in fact the fact that that's not equal is going to be very important for what I'll tell you about the error correction properties okay so it turns out that it's a very interleaving representation it doesn't preserve metrics in this sense but it is very crucially structure preserving in the following sense so the more important property is that if you have a representation the the representation for X plus the representation for y equals the representation for X plus y okay so it's metric preserving that sense so it's not like it's not like code words like I'm gonna call you know I'm gonna assign a code word Alice or you know and 2x and code word Bob - y those are not sort of representations that you can add there's no sort of metric structure inherent to them okay so there's some interesting algebraic properties of the system has okay so um what what what are the properties of this modular representation well it is a number system you use log as many moduli to represent you know large ranges and so just like fixed base number systems you can encode exponentially large ranges using log of that number of registers if you will so here you know so the phases are all 0-1 quantities they go between zero and one and there n of them so the space the coding space is n dimensional torus and if you start the coding lines say you identify this zero phase as for the position zero and then you start moving your position variable X along some range you start wrapping that the vector is a point here the phase vector is a point here in this coding space and you start wrapping around the coding space okay and as X starts to increase by the time X has moved by say lambda one the smallest period then the coding line has wrapped around the torus and one of the dimensions completely but not quite in the other because these are not on the same period so it's not quite wrapped around so it's just going to keep going it doesn't close on itself and it keeps going and going and wrapping around okay so at the point when you know this line has a finite thickness you can ask how long is this line before the space is filled by this finite with coding line and this is the range that can be uniquely represented of positions and all these calculations can be done in the reals and so it turns out that the range that can be represented scales like lambda on the front which sets the scale which is the period of the individual grid responses grid cell responses and that sort of sets a scale and since all the periods are about the same I'm just going to give them a name lambda but notice importantly that n which is the number of distinct periods is up in the exponent right so it's possible to represent exponentially many positions uniquely using only linearly many different periods okay so now it's it's a funny thing because okay so what's what's also interesting is that now if we reduce so this is this is the this is how things behave when you scale the whole range by if you if you wait until the whole space is filled with um this coding line but it turns out that if you if you stop short of that right if you stop say here where there's a lot of gray space still so the coding line hasn't filled all of space then you can see that there's really some gaps here between the coding lines and it turns out that if you encode and decode information over some reduced version of the range so instead of going all the way up to e to the and beta some constant close to one you instead encoder is some range some limited range which is e to the Rho beta and where Rho is an additional constant which is smaller than one okay now you have this empty space between the different segments of the coding line and it's possible to also do error correction on the system right so any small perturbations of the of the of the word just map into the space here which is this gray gray space and as long as your perturbations are smaller than half the separation between these lines it's possible to decode the the position and and and back at at a good estimate denoise estimate of position so this is a code where if you're restricting the coding range to some subspace of the full coding space okay where Rho is this constant that tells you how much smaller than the full capacity you're using then the information rate of this code scales like about log of this restricted range or log of the full range and that's Rho okay so you can dial in your information rate and for that information rate it's possible to achieve this exponential capacity in n alright and it turns out that if you look at the in the presence of noise if we use the same number of neurons using more conventional neural codes then this is the decoded posterior estimate of positions looks like this green curve here but with the grid cell code using the same number of neurons in the same noise per neuron you can now achieve posterior distributions that are much narrower okay so and and so so this is a interesting property of this code right so now the thing that makes error correction possible is is the fact that as this coding line right so this is this n dimensional torus which I've wrongly is a two-dimensional torus but as X has increased we talked about how this coding line continues to wrap around until at some point it covers all space and I said what you can do is instead of going through that whole range of X's these positions here and stopping here when all space is covered we're gonna stop short in just short and stop here okay so Oh what a property of this code that makes error correction possible is that these additional coding lines that would have been added if we'd kept going would have interleaved with all of these existing lines in fill space okay but by decimating the range that I'm interested in coding very conveniently the portions of the coding space are decimated interleaving with the existing coding lines okay it could have been that I had wrapped around the torus in a way where all the lines were closely packed locally here and then when I decimated the coding line there was all this empty space over there right that would not have led to noes tolerance and error correction right so it's this very nice interleaving property of the coding line as you continue increasing the range X that allows this error correction to happen so in a sense this is a generalization of sort of well-packed sphere codes to four discrete variables this is a generalization or well-well packing of a line for analog codes okay so just a contrast with them with known codes in the brain you know here is a conventional code that a lot of people are familiar but this is like orientation tuning in v1 these are different neurons here's a stimulus angle here's the firing rates of these codes each neuron is firing some spikes as independent Poisson given those those tuning curves and so you get some number of spikes firstime stimulus angle and then you can then go ahead and decode the variable that is being encoded and it's all worked out you know through classic neuroscience literature that the Fisher information of these codes skills like n the mean squared error then scales like 1 over N and the information rate of these codes which is the log of log of the amount of number of bits used for for representing the variable vs I'm sorry the log of the information bits versus the the number of bits used to encode the variable it goes like log n over N and that itself goes to zero zero okay so the conventional neural science codes have information rates that go to zero all right so they have a linear decrease in the squared error but at the cost of a linear decrease in information rate okay and how much better is the grid cell code well the grid cell code if we look at the the width of this posterior distribution of the decoded position divided by the width of the posterior distribution for these conventional codes it's exponentially small in n ok so the width is much smaller all right and so and we can and and so that's that's actually analytical calculation and then we can also numerically verify that this is true so in other words so just to summarize this the grid cell code really enables exponentially better coding accuracy over range this ratio of coding accuracy over range than the classical population codes that we know from neuroscience from the sensorimotor peripheries and allows that to happen at a finite information rate row okay so so in other words the grid cell code really achieves a performance that's similar to strong error correcting codes in the Shannon sense all right so we already know that Shannon had proved that you know before 1948 it was believed that it's not possible to have decreases in error while having a finite information rate and what Shannon proved was that it is possible and his his theorem showed that it's possible to achieve asymptotically zero error at a finite information rate for discreet codes and the corresponding analog statement for that is that you should be able to achieve exponentially vanishing error at asymptotically finite information rate and that is what the grid cell code seems to achieve okay so the interim summary here is that the grid cell code really can generate these unity representations for exponentially large ranges of location using the nearly many neurons it's capable of this very metric updating so the code word for a sum of inputs is the sum of the code words of the two inputs so that's a that's a very unique property grid cell code is a strong error correcting code and and I would just like to point out as a sociological note that the discovery of these properties of the grid cell code was very much bottom-up from my side right this is not I didn't come in as a coding theorist saying I'm gonna look for you know residue number system codes and in neuroscience I'm not gonna look for strong lirikrekt in codes and neuroscience it really resulted from this discovery of grid cells and trying to understand why this puzzling representation for space what are the mathematical properties of it and a chain of deductive reasoning so you know bottom-up approaches really I think have a lot of promise still for Neuroscience I don't believe in the Marr hierarchy completely you know software and hardware completely independent I think there really is a sense in which you know neuroscience discoveries are going to tell us a lot more about interesting representations and codes and computational abilities in the future ok and yeah alright and so now just to the second part then ok so this was these were all properties that we have discovered about what it is that a code in the brain is capable of right how much information can it encode it's an encoding analysis okay and so you know in coding theory when people study codes right you've got some satellite has to encode represent some transmit some information so what it does is it's got some information Phi that it wants to send this it wants to transmit the source it encodes it as X of Phi it goes through some noisy Channel and then the information corrupted information reaches some decoder and now the decoder is free to take its own sweet time and decode this this the the the transmitted variable right so and so what what we just talked about is Shannon's result the theoretical capacity bounds on strong error correcting codes those all deal with information that how much information can be encoded and pushed through a noisy channel assuming the existence of some optimal decoder right but those capacity results don't take into account the decoding complexity and the decoding cost okay but um oh this this is a duplicate so the I guess the the question is that in in the brain though we have to understand that the neural representations and in memory in all of these systems the encoder the channel and the decoder are all built by neurons right it's all using the same resources we have to count take into account if the decoding involves exponentially many neurons now you've sort of nullified the gains made by the encoding step okay so we have to really try to understand whether it's possible to also do decoding of exponentially many states or denoising using linearly many neurons and that's kind of a question the next question ok so that brings me to talk about hopfield networks okay and so hatsune networks I think don't need too much introduction in this audience you know they're defined by these units that are 1 or 0 depending on the sum of the inputs that the units receive they flip their state to be 1 or 0 here's the inputs that each unit receives to make that decision and the dynamics on a hopfield and oh and these weights are symmetric and conventional hopfield network and with these dynamics and symmetric weights one can write down an energy function and which looks like this and the dynamics of the hopfield network then are equivalent to descent on this energy surface so for example if you have a hopfield network that has some some fixed points or some of these low energy states over here and say this low energy state corresponds to some pattern here at neural activation then if you initialize the system in some version of that state that's a corrupted version of the state the dynamics of the system do descend on this energy surface and bring the system back to that minimum into that state okay so so because of this sort of dynamics the hostile network can equivalently be thought of as a model of associative memory but you can also just think about it as a model for correction or completion dynamics or denoising in neural systems okay and so I'm going to work within this paradigm over here okay so I want to define a notion of robustness so you know so robustness means the ability of the system to correct or recover from some errors but it means something specific in my case so what I want to say is that robustness involves correcting errors in a finite fraction of nodes so suppose that every node has some probability fixed probability of error then the number of errors in a system will scale in proportion to the size of the system okay so robustness means the ability of the system to recover from errors and a finite fraction of all the nodes all right and that's my definition of robustness but for a system to to satisfy that definition of robustness it means that the basins of attraction in this dynamical system in this energy landscape that the size of these basins must scale up as a network size scales up okay so the nations should get bigger as the network gets bigger all right so let me then bring you to a brief history of the Haab Field capacity results or results on how what it is that hopeful networks are capable of in terms of capacity in how many states they can store and decode and what the robustness properties of each of those states is okay so in typically many constructions of hopfield networks the system storing very few states but robustly okay so if you have pairwise weights and random patterns and you can store order end patterns with a small finite error if you allow the system to have peeth order connections rather than just pairwise weights you can have Peter order weights then the system can store order and to the p- wine patterns but it's extremely densely connected and it only becomes exponential when P equals n which is that their nth order connections in the whole network so then it's sort of not a good situation ok on the other hand it's possible generically in these awful networks to storm a very many states but then not do it robustly writes if you have complete a random network then also their constraint satisfaction networks that were constructed by hopfield and tank and others these allow for the construction of e to the square root of n states and these states are not fine and not robust at all to errors ok so so they can't recover that very narrow Basin spin glass the same thing you can have order e to the N States but again they're not robust to final fractions of errors the basins are very narrow and much more recently there's some interesting constructions of havel networks that can store e to the square root of n errors they're partially robust they can recover from some number of errors it's not clear if the errors are proportional to the size of the network and these are networks that are clique networks with very specific configurations of connections within them and these were actually done by Chris Hill R who's here at the Redwood Institute in Berkeley and his collaborator Tran and by myself and Tran the same co-author and some other colleagues so in general either Hopsin networks have low information rate or are not robust ok so so that's the that's the step that's the state of affairs right now and the question is is it possible to have exponentially many fixed points so therefore exponentially decoding of exponentially many states and hopfield networks in a way that is robust to finite fractions for errors in the system ok so how many minutes do I have like 10 all right thank I think we started after the introductions at 10:00 after seven minutes okay that's perfect great thank you okay so um so the question is is this is this possible okay is it possible to have these hopfield networks with the x-mansion many states and robust basins and so you know we so since the motivation here is sort of the motivation from error correcting codes right the motivation for the first time for my talk we could just say well let's take a good error correcting code that we know is good and try to implement it in a hopfield network directly okay so lets us take this famous seven for hamming code right it's a good binary error correcting code so the Hamming code the seven for Hamming code has you know four bits that carry information so you can have a binary vector of length four and that's your sort of your source it's your data and each of these components is one or zero okay and now what you do is you take those four bits that are information carrying and embedded in a code word that's of length seven okay and so you've appended on to this four but code were this forbidden source you've appended three additional bits right three redundant bits and these these three additional bits are not independent of their coding bits they actually satisfy the following properties so you there their parity checks right they check their their check sums if you will and so the the the additional bits five six and seven satisfy these three algebraic relationships where the Sam mod 2 is zero for each of these cases ok and so here's here's the code the optimal decoder you know we exist some you just recompute the three sums above you know given a noisy version of this code word you just compute all of these three sums and call the first sum alpha the second Sam beta that there's some gamma and in fact that number that you get alpha beta gamma is actually the index in binary of the incorrect bit okay and so then you just go ahead and flip that bit address by alpha beta gamma and that will correct the error okay so the information rate of this code is 4 over 7 because there for information bits out of seven total used bits and and and the capacity of this code actually you can compute from Shannon's theory and itself 4/7 as well so this code is really saturating the Shannon capacity and this code actually corrects all one bit errors so and one bit error in this code word can be corrected according to this algorithm and it also signals when there's a two-bit aerial though it doesn't correct two-bit errors okay so the rate in this case equals capacity and so now to embed this code into a hostel network what we want to do is we want to make these code words right the allowed set of states here for these seven bits we want to make each of these code words become stable states of the hopfield dynamics okay that's what we would want to do to embed this in a hopfield network okay so what we're gonna do is we're going to take our zeros and ones and map them into ones and minus ones okay and and remember these are the three constraints the three extra sort of parity checks over here and so what we we can do is we can construct a if you write down an energy function that looks like this where the energy function is given by the product of these each of these terms as a product of the terms in these sums okay and we make all these J is equal to one this is you can easily verify that this is an energy function that's minimized at all valid code words okay so the system has an energy minimum if this is the energy function for all valid code words okay so that's nice right and so and because I've written out an energy function it means I can write down it's just a hopfield network and but it involves these like four-point couplings right these fourth order coupling terms between all the between all the different nodes so you've got seven nodes here these are the seven nodes or this in the network here are the edges the yellow is like the I don't know it's the two for yellow is two J 2 3 4 6 so it's that one so the yellow is this fourth order connection here the orange is 1 3 4 7 so it's that connection over there that fourth order connection and so on right so now I've written down a hopfield network that has as its minima the code words okay so that means that if you have a perturbed version of the code word hopefully the system can cut it correct correct the errors in in the system okay so so with these weights or energy function the code words of the minimum of the dynamics and we have we can obtain to the four states with seven neurons okay so because all possible for coding states are allowed as long as those checks are are validated in fact you know these four to ordered edges seem really exotic you know can we even write down fourth order edges well it turns out we don't have to write down fourth order edges you can unpack these fourth order edges into just pairwise weights if you allow yourself if you allow the construction of a bipartite graph right so hidden nodes in a bipartite graph can induce higher order in dependencies okay so are we done are we done can we go home we have got now the construction of a you know it stronger correcting code that can then decode noisy states so actually the no we're not done so hopeful dynamics cannot properly decode on the Hamming code so it turns out that you know so we've written on this energy surface so this energy function over here here's a here's a valid code word it's the all ones code word and the energy for this is minus three you can verify from here now let's just flip one of the bits let's flip that first bit and now the energy goes from minus 3 to 2 plus 1 okay and so you know so one way to decode is to flip back that one bit and you know this is in the direction of reduced reducing energy because you go back to minus 3 and this is the correct decoding step but it turns out that the same energy dynamics also permit the system to wander in a different directions so if you flip now the third bit it actually reduce it reduces the energy because flipping the third bit actually reduces the energy to minus one so this is a valid direction for the dynamics to flow in to they'll flow they're allowed to flow in this direction or they can flow in this direction but then if you flow in this direction and the next downhill step will involve a third flip over here and on this results in again a very low energy state another correct code word but it just so happens that it's an error because it's the wrong code word relative to the corrupted one okay so the picture here the geometric picture here is that you started at this perturbed version this is the true code word this was the perturbed version we started with one one iteration of the dynamics can you bring you in this direction in the correct way but the other iteration downhill also in the energy landscape can bring you to the other minimum okay so in other words you get suboptimal decoding by hopfield dynamics and small perturbations can map to the wrong or further away codeword okay so should we be surprised you shouldn't be surprised okay because in general strong error correcting codes require very complicated decoding algorithms they require things like belief propagation which have a dynamics that's much more complicated than the dynamics of simple summation and non-linearity the nodes that that typical neural networks can do okay so so they require non-biological decoding algorithms okay so so clearly it's not possible then to implement this - having code in Hopsin networks so the question is can you do this at all for any kind of strong error correcting code okay so I guess I'll just tell you the bottom line and then go very quickly just through some features that make this possible and I'll end over there okay so the bottom line of of what I'm going to tell you is that it's actually it is possible to construct a robust and exponential capacity hopfield network so you have exponentially many states with linearly many neurons and the basins of attraction scale with network size okay so on the system is robust and the dynamics allow for correct decoding or noise reduction or error correction on them okay so this is just the summary of the results of we can construct these networks where the number of states versus the size of the network note that this is a semi-log plot it's growing exponentially with the with the size of the network and moreover if we look at the fraction of correct decoding events as a function of the percentage of input that's corrupted okay you can see that we get basically close to perfect recovery even when just like a finite fraction in this case about 4% or 6% of the nodes are corrupted okay so this is the bottom line right this is what we were going for and it's possible to do this construction and so so so the summer here is it's possible for happen that works to have exponentially many states and correct errors and a finer fraction of all nodes and so it's possible to have robust exponential memory and decoding okay so then the question is how does this network work okay and I'm not gonna be able to spend a lot of time here but the architecture of this network is it's bipartite okay so you've got we've got input nodes which are the nodes that are gonna be learned and decoded and it's in this case this construction that I showed you is regular so each input node has a fixed number of outgoing edges it's very sparse in weights meaning that each node sends a fixed number of edges okay that does not scale with the size of the network so and goes to infinity the number of edges / size of network goes to zero okay so it's very sparse the weights themselves have very low dynamic range they're just binary weights ones or zeros this network has an expansion property so small subsets of input nodes project to maximally large sets of of these constraint nodes and this hidden layer up here and and each constraint know it is a small hopfield sub Network okay and so the architecture of the system okay is is is this is this case where I talked about this expansion property okay so what is an expansion property here here is a picture of good expansion where it says that expansion okay so let's consider what's bad expansion suppose that each input node can send out only three edges okay and let's consider the set of these three input nodes if each node can send out three edges right the maximum size of the set that they project out to can be if they project a completely disjoint sets in the hidden layer then the the maximum size of the the neighbors outside and the hidden layer is nine right because the three times three so it's nine but the minimum size is gonna be just three because all three of these can project to the suppose they project to the same three okay so then the the minimum size of the neighborhood is three so it's good expanders are expanders are our graphs where the size of the neighborhood for a given number of outgoing edges is close to a maximum it's large okay and so a good expansion property is one where epsilon is close to zero all right so that's what the formal statement and so I'm gonna skip over some of these statements but they sort of provide the the guarantees on size etc the make the system work okay so dynamically how does we how do we implement it this is all hopfield dynamics helpful updating so the inputs are initialized and they're noisy corrupted States and a finite fraction of the states in have been flipped okay and then now these constraint nodes appear which are each mini hopfield net works quickly equilibria to the to the inputs of their receiving some of these constraint nodes are you know not satisfied because some of the input nodes that should have a certain sign are actually flipped and that causes the constraint nodes to drive a flipping of the input nodes and so the rule is that the inputs are connected to more unsatisfied than satisfied input nodes actually switch state constraint nodes switch States and when you iterate this over time but just hopfield dynamics we're guaranteed we can prove that you can guarantee that the system will go to a minimum energy state and will go to the correct state okay and so you know conceptually what this expansion property of this graph allows is it allows solving of the credit assignment problems so in the hamming network the reason why the hot sauce dynamics didn't work is that when there were two errors in the input nodes the system couldn't really determine which was the the node and error okay and now because of this expansion property of the graph it's possible for this credit assignment problem to be solved and the dynamics allow for the correct flipping of the of the corrupted nodes all right and so I'll just mention that we can actually do learning so we can learn input patterns as long as the input patterns of the inputs consist of mixtures of constrained as far as subsets and we can use contrastive divergence learning and a hetero synaptic competition term which is like a l1 regularizer and actually learn the structure of the input so in other words each constraint node will learn and find and connect to all elements of a jointly constrained subset of the input so if they're small subsets of the input that are in a cluster and should be constrained together they have relationships to each other learn the real learning rule will learn that okay and so I'll just summarize then also the features are that sparse random and high dimensional structures that I've described here they have low weight complexity and in high dimensions you can deterministically it's hard to deterministically construct expander graphs but in high dimensions it's very easy to do that so random graphs can be expanded graphs that holds tantalizing implications for biology each state is stabilized by a large number of constraints but each of the constraints is very a weak very weak constraint on the set of input nodes that it connects to it also provides some insight into why in biology we see these very interesting long tail distributions of activations so the Bou sake laughs has these results and many other labs have represent their replicas the results where a few neurons are active all the time and and and many neurons are almost active never okay so it's really these Forrest's distributions of of activity of these long tail as far as activity distributions they're ubiquitous in neuroscience but from sort of coding perspective it's very tough to understand why right you would want to equalize your your burden that each neuron has why is it that you have these long tails and in this case the constraint nodes are very sparsely active and the input nodes are densely active and so this is very natural sense in which most of the nodes are constraint nodes sparsely active a few input nodes are densely active all the time and they're being corrected okay so there are a lot of questions that we have about learning rules and but I would say that you know really inspired by modern low-density parity-check codes we can construct these objects and high dimensional spaces and actually even generalize them so the low-density parity-check codes usually have a lot of structure but some of these constructions here we can relax a lot of the structure that were used in proving theorems but we still retain the properties so lots of this work was done in the second half of the work was done by Rishi Chaudhary from my group postdoc and the first half was done by Simon Srinivasan and NOC is a collaborator and some of these projects thank you very much for your attention so I have a quick one well so you've mentioned learning at the very end I think there was more in the hopfield net context so the hippocampus does a lot of one-shot learning where you know counter some space you come up with new play cells would you predict that the grid cell code by virtue of being very dense would not be a place that you'd see a lot of plasticity the same sort of way so far we haven't seen a lot of evidence for change so of plasticity in grid cells in the sense that when we look across environments and across experimental conditions the relationships between pairs of grid cells like the the the activity relationships whether they're Co active or not tend to be quite preserved across environments so it's true that in grid cells we wouldn't maybe expect to see too much plasticity and we don't and the question is whether hippocampal representations may be like these expander graphs and these optical networks that can do error correction and that that's a great question that's what we're thinking about lately 