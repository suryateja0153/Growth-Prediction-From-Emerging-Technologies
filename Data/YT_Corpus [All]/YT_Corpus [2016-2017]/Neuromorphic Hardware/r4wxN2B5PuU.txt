 hello everyone in this presentation I will give a brief introduction to a neural network called a liquid SATA machine also I will talk about the neuromorphic system we developed for it in order to understand this concept I guess we should first talk about water as it's why unknown life on earth could not live without water so water must have a lot of amazing properties also we don't pay much attention to it water can be really powerful sometimes and the main focus here in this work the liquid city machine has a lot to do with water before we talk about the liquidity machine let us first make a salt experiment that is what will happen if we toss rocks a lot of rocks or small stones into a glass of water of course we can get a lot of ripples and waves however we can toss different atoms to the different locations of the glass of water at different times which makes many different patterns and this glass of water will generate unique response for each particular input pattern and considering the nature of the liquid some researchers believe that the difference between these patterns can be amplified that water and a famous philosopher whose name is Bruce Lee also have some famous comment on the power of water it becomes the cup you put water into a bottle it becomes the bottle you put in the teapot it becomes the teapot the water can flow or it can crash be water my friend before we talk about neural networks let us first take a look at the belliston neuron as shown in this figure a bellows could neuron consists of many dendrites a cell body and also an exon the connection between two neurons is called the synapse obviously a bellows good neuron is a multiple input single output system the neuron caste receives stimulations from the tendrils so the membrane potential or the membrane voltage of the neuron can be increased when the membrane potential is higher than the threshold it is neuron fires and send out a spike to the axon this process can be illustrated by this animation which demonstrates how the membrane potential changes over time this word utilizes the famous leaky integrated fire neuron model and the neuron dynamics of this leaky in decreed and fair model are described by all these equations but in this video I am NOT going to talk about the mathematics of neural models too much and for more details please take a look at the following referenced paper now it's time to talk about liquid state machine liquid state machine was first to propose the by welcome and Harry in 2004 which is a really powerful recurrent neural network which closely resembles the bellows code structure of the human brain the liquid state machine consists of an input layer a reservoir and also an outer layer the reservoir contains a group of neurons randomly connected by the Fichte snap sees and it can map the input signals to higher dimensional liquid response which will be later protected to the output layers to plastic synapses the liquid machine has a biologically plausible structure which makes it very competent for processing temporal patterns such as speech signals and ural a supervised the learning is performed on the output layer which updates the synaptic width of the plastic synapses which have been highlighted by the red - the circle in this big and now it's time to talk about the learning or the training of the liquid machine the training of the liquid machine is based on a biologically plausible learning rule which applies a Haven and local learning what does that mean that means that the synaptic the synaptic plasticity involving maybe two neurons are multiple neurons only depends on the neighbors only on its neighbors and also the calcium concentration of the neuron is used to characterize the long-term affair and activity of this neuron as the calcium contrition is updated by the equation in the following figure as we can see every time the membrane potential reaches a threshold this neuron fires and it's constant concentration C is increased if there is no ferrant activity for a long time the calcium concentration C will job over time so it will help the network to capture the long-term affair activity and because we are trying to realize a supervisor learning rule a teacher signal is injected into each output neuron to modulate its membrane potential since the Ferren activity is influenced the update of synaptic weight is also modulated by this titre signal for example the upper neuron corresponding to a particular input pattern will be potentiated by the teacher signal while the other output neurons will be depressed by this teacher signal so according to the following equations the synaptic weight associated with each output neurons are only updated when the calcium concentration of the neuron is in the correct reason and again I'm not going to talk about the details of these equations and you know in order to learn more about this mathematics you can go to the original reference paper of this work or the original paper by Ibaka and Harry in 2004 from now on and we will talk about average neuromorphic system to show the advantage of the hardware machine learning in this area of big data and machine learning there are growing concerns in device reliability and energy consumption for the traditional Veneman act actors mean well the Neumann machines also consume tremendous power energy and space resources however the brain is spared neuromorphic computing provides an appealing attack resolution which shows great efficiency in terms of both hardware cost and also the runtime in addition the inherent error resilience offered by the brain spare the neuromorphic system is very suitable for large-scale integration in the wee hours I technology in this work we developed a parallel hardware architecture for liquid state machine let us first start from the implementation of the reservoir first we implement multiple parallel liquid admins to calculate the liquid response then a crossbar switch interface is used to send the external input spikes to their target liquid adament at the same time the output spikes of the liquid element are recorded by a register called the liquid response and another crossbar interface feed the liquid response back to the liquid element and meanwhile the liquid response is sent to the outer layer as a simulation the output layer of the liquid state machine is implemented at the training unit and the output neurons are implemented with the output element the plastic synaptic weights are stored in the prm's which are the unchipped memory restores of the sitings fpga and each output element receives the liquid response from the reservoir or the reservoir you need to update the corresponding synaptic weight in parallel to realize a supervised to learning teacher signal is used to hear to modulate the variant activity of each output Edmund and this instrument a particular form of hey B&E now it's time to talk about the design details of the digital equipped neuron namely the liquid animate according to the algorithm each liquid element updates the membrane potential based on the four state variables and these four state variables are calculated by a functional block called the synaptic response unit this unit is realized by basic logics such as adders 50 bit registers and multiplexers so this is really efficient thanks to hardware friendly algorithm we are utilizing in this work as mentioned earlier the output neuron is realized by the output element which update both the membrane potential and the calcium concentration and the four state variables are still calculated by the similar synaptic response unit however this time each stitch two neuron is to communicate with its private B Ram in order to trim the neural network namely to update the synaptic weight and also a teacher signal is needed here to modulate the membrane potential of each output neuron and forth popular public domains benchmarks of real-world applications are used in this work two of them for the speech recognition and the other tool for the for the image recognition and as we can see this and recognition rates have been achieved for all these benchmarks in this work we use the silence FPGA as our platform the hardware liquid steam machine is attached to the X I bus of an SOC at a customer IP for each benchmark our FPGA accelerator can achieve over 80 times speed-up over a software counterpart running on a general purpose CPU of 2 gigahertz this is really fast at you our highly parallelized implementation and the following figures showed experimental setup and the signals of our liquid immersion Epicor for more information please take a look at our recent papers listed here and that is all for this video hope you can find something useful thanks for watching 