 So we'll go ahead and begin here, and see people roll in. Before getting started, I'd like to thank several individuals that have made this discussion today possible. The first is the Chair of the Department of Landscape Architecture, Anita Berrizbeitia, who has supported this effort with critical feedback and departmental funding. The second is the school itself and our Dean, Mohsen Mostafavi, who has provided the platform and funding for the discussion. This would not be possible, also, without the support and dedication of Shantel Blakely and her event staff, who have handled the planning, logistics, and helped to shape all of the details. I also hold a of gratitude to Dan Borelli for planning and orchestrating the exhibit you see on the experiments wall, and Jeremy Hartley for his work on the layout and video production. I'd also like to acknowledge and thank Justine Holzman, my colleague and collaborator, who has helped to shape and articulate the body of work for the responsive landscape's manuscript and was central to bringing the panels together. I'll be brief in my introduction as I'm looking forward to having time for an open discussion led by our moderators, presenters, and with feedback from the audience. Computation in landscape architecture has primarily been applied to problems of analysis and representation. The first, analysis, has become more efficient and precise, but also highly mediated. Representation has found new paradigms in parametricism generative models, and fabrication, but has primarily simulated previous modes of drawing, drafting, and illustration. Responsive technologies, machine learning, and material intelligences propose methods of interaction and manipulation in the environment that rely on the virtualization of complex systems and continuous feedback from environmental phenomena. This posits a computational approach that collapses the linear design process of observation, analysis, and proposition, and moves from designing human device interactions to forms of ecological management, form making, and process creation that hinge on the feedback and subsequent adaptations. So today we'll look specifically at methods and frameworks that address the concurrent physical and virtual spaces where these technocentric operations interact. The two sessions have been organized as realities, the place in which we take action, and realms, a lens we perceive the environment through. The presentations we have organized touch on a broad range of topics that move between the applied and the theoretical, framing an overarching set of issues that are inherent in our conceptualization of computation's role in the physical world. These issues are pressing, as we feel the weight of control systems implicit in the parameterization and generalization of complex ecological, biological, and cultural relationships. It is inherent in this conversation that we not only embrace an embedded computational ecology, but that we do this while finding forms of resistance and productive countermeasures to state corporate and other forms of external control. It also calls for a decentering of humanity, prioritizing multiple species and processes, stating that optimization is not always the end and that the cultural project of design find relevancy in servicing more than the security, comfort, and pleasure of humans. I've asked that our moderators, Justine Holzman and Earl Ellis, provide a framing for each session through the lens of their research and scholarship. In an attempt to keep the conversation fluid, I'll provide quick introductions for the moderators and descriptions of each session and the presenters. Our first session is a session called Realities. I'll start with a quote by John Barrow. "Perhaps all boundaries are illusory, whether erected by ourselves through our lack of information about the nature of things or by the choice of an oversimplified or even overcomplicated model of reality." So advances in sensing technologies have fundamentally altered the methods in which the landscape is known and modified, from oil exploration to precision agriculture to interplanetary autonomous rovers. This ability to sense, virtualise, and simulate environmental phenomena has produced a range of composite physical and digital realities which often exists simultaneously across multiple temporal moments and scales. While responsive technologies furthers the blurring between physical and digital space, this model of understanding the landscape through measurement and modeling must be connected to modifying rather than solely analyzing the environment, which is the principle practice of landscape architecture. Justine Holzman will be moderating the first session. Justine Holzman is an Assistant Professor of Landscape Architecture at the University of Toronto Daniel School of Architecture. Her background in research and responsive technologies and modeling have helped to form the current framework for understanding the relationship between forms of abstraction and technological mediation in the profession. In this session we're lucky to have four presenters. Natalie Gattegno is the Chair of Graduate Architecture at the California Center of the Arts and the co-founder of Future Cities Lab. Her work at CCA has helped define making and prototyping as central tenants of architectural curriculum, particularly in respect to nascent forms of computation and technology. Future Cities Labs is a design practice located in San Francisco that engages in a broad range of architectural design work from installations, exhibitions, and provocations to help re-imagine the role of computation and robotics in urban environments. Alexander Robinson is an Assistant Professor of Landscape Architecture at the University of Southern California and the director of the Landscape Morphologies Lab. He's the co-author of the seminal book Living Systems and is currently working on a manuscript that focuses on the sociocultural potentials of infrastructural spaces. His work is deeply tied to forms of physical and digital modeling, looking at issues surrounding the Los Angeles River, Salton Sea, and the larger southern California watersheds. Next presenter, Sara Dean, is an Assistant Professor at the California College of Arts and works in research and development-- or excuse me, previously worked in research and development for Blue Homes. Her work examines the implications of digital methods on social and environmental justice. She has taught at a number of institutions and previously worked with Stamen Design, where she was the Director of Research, developing cartographic projects focusing on visualization and analytics. And finally in the session, David Klein is a multiple startup veteran and adviser. He was the Algorithm Architect and Machine Learning Manager at Audience, where he developed neuro-inspired chips for the iPhone and Samsung Galaxy. He was also the CEO and Co-founder of Black Swan Technologies, developing a neural network-inspired video codec, and the CTO of Ersatz Labs, developing the first cloud GPU deep learning algorithm. He is currently the lead artificial intelligence developer and advisor for Conservation Metrics, where he is using sensing and AI to monitor biodiversity at scale. So our first presenter will be Natalie Gattegno. [APPLAUSE] Thanks, Brad, and thank you so much for bringing this great group of people together. It's rare when you're kind of brought together to these events and have time to actually talk. And it's nice to see that there's room for that here. So thank you. And thank you to the school for inviting us and making this possible. So I wanted to maybe begin by contextualizing a little bit the work, and then sharing a little bit of what we've been up to in response to the prompt that Brad set up. So in their description of the micro event and microenvironment for the exhibition, Italy: The New Domestic Landscape at the Museum of Modern Art in New York in 1972, Superstudio envisioned a landscape of energy supporting all habitation. We can imagine a network of energy and information extending to every properly inhabited area, quote, they claimed, adding that the whole city as a network of energy and communications was essential to the production of this landscape. In this world, nomadism, impermanence, mobility, and transience were the norm. No inside or outside could exist, and the control of our environments happened solely through the regulation of energy. A new kind of urban condition emerged in their vision, one where spontaneous events and crowd gatherings could be triggered at any time in any location on the grid. Nothing and nobody was bound by built structures. All you have to do, they said, is stop and connect a plug. The desired microclimate is immediately created, temperature, humidity, etc. You plug into the network of information, you switch on the food and the water blenders. One could argue that for Superstudio, the whole world became neither an interior or an exterior condition, but rather a variable, nomadic, ephemeral sequence of events and spaces, essentially the space around Reyner Banham's campfire. This landscape could nurture a completely nomadic lifestyle, one of gradients, of space and energy-- and in some cases, I would say, data-- that can be tapped into at any time to create Superstudio's microenvironments. Of course, this energy landscape does not seem to be as far-fetched today. The paradoxical relationship that we have between big data-- and by big, I mean both in scale and amount of information-- and the need for impact at the personal, visceral scale of our communities and our daily lives is one that a lot of us are actually kind of grappling with. So to maybe trigger some conversation, about this in light of some of the conversations that are probably going to come up with the rest of the panelists, I'd like to share three projects that try to operate at very different scales of information and very different scales of input, and by result, output as well, both at the physical and the data scale. So Lightswarm is a project that we recently completed at the Yerba Buena Center for the Arts in San Francisco that actually tried to kind of capture the city, the surrounding sonic environment of the city, and kind of play it back on the facade of the Yerba Buena Center for the Arts. So it was essentially pulling vibration, essentially vibration information, from the facade of the building, and retransmitting this into this kind of playful representation, in this case, on the building. So instead of fortifying ourselves and using our buildings to essentially kind of limit that relationship between inside and outside, how could the facade actually become a kind of translation of those two kind of environments? So the installation basically encourages interaction through a series of sensors embedded on the glass, what we called our sensing spiders, that would be triggered-- here you can see them on each pane of glass-- that would be triggered through vibration, and then trigger and response, a kind of ball of swarming light on the facade of the building. So you could walk up to it and clap to it, and it would actually respond to you. Or a bus could go by, or the museum could have a huge kind of event. So the idea was that you were kind of translating information on the one hand, but that there was this constant feedback loop between inside and outside. People would actually converse on either side of the glass and kind of try to trigger it and create different effects. A lot of the work that I'll show is work that we've both designed and fabricated. We have an office and a shop space in San Francisco. And for us, that's kind of a big part of a lot of these installations, is to kind of also try and figure out how to actually make these in such a way that they also become attractors, people actually gather around them and kind of approach them in a more intimate way. So here's just some images of our office. And we've kind of been exploring this relationship between the things that we build, the things that we make, and their relationship with the things that surround them. So how do buildings respond to their surrounding context, whether that's temperature, humidity. In this case, this is a fog-catching structure proposed for the edge of San Francisco. But the interesting thing with Lightswarm was that we realized that we had essentially created a large urban canvas, an urban canvas where, on the one hand, you could kind of playfully respond and kind of interact with it, but that it essentially became a surface that we could potentially hack, and that we could potentially trigger a completely different set of effects. So one night, due to our longstanding relationship with the night guard at the museum, we were able to turn out the lights at the museum and hack our own installation, and essentially kind of test out multiple modes of interaction or response and back and forth. So here you're seeing motion tracking, camera tracking, where we're looking at ways of actually triggering the insulation just through a series of IR sensors, or direct user interaction through the development, essentially, of an app, to be able to actually kind of control an urban surface at will, in this case. And for us, that kind of relationship of the big information, all the way down to me being able to-- or in this case, Jason-- being able to control a surface that is actually essentially kind of operating at a large scale was really interesting and compelling to us, and something that we've been trying to work through the work that we've been doing recently. At a completely different scale, I wanted to present to you guys a project that is currently in the long-term making. It seems like it's been years that we've been working on this, but we're close, and it should be constructed next year. It's an interactive installation north of Union Station in Washington, DC, in one of the large underpasses north of Union Station, so where Amtrak, Metro North, essentially this kind of large confluence of rail lines come in. And essentially it creates this kind of no man's land. It's cut through, obviously, a neighborhood. And the installation was originally asked to be lighting, a safety lighting sculpture, so we were asked to basically provide adequate safety lighting, but also to beautify the underpass. And through an RFQ and an RFP process, we've kind of arrived at a point where we're essentially proposing a way to kind of weave the two neighborhoods together. On the right-hand side is a kind of residential neighborhood. On the left is where NPR and Google and a number of other companies are actually starting to set up their headquarters. And for us, the installation was seen as a way to, on the one hand, provide light, provide variation, but also find a way to interconnect the two sides and provide for something that maybe could respond to either people moving through it, or in some cases, vibration from the trains above. So the same way that you're kind of running to catch your train because you can feel the kind of rush of air coming up through a tunnel, here may be the color of the lights or the tempo of the lights would actually kind of give you a cue, an urban cue, one that we're used to knowing how to read in our urban settings to run for the train. Or maybe you've missed it. So on the one hand, being something that can codify information at a very personal level, but also have a larger presence in terms of the data set that's inputted. And here's some renderings. We've gone through the Federal Commission of Fine Arts. We're essentially being treated like a monument in Washington, DC, and having to go through a very kind of different set of hoops to set this in place. But hopefully it should be close. And finally, Murmur Wall. I wanted to kind of end on a project that kind of tries to kind of bridge the big and the small in a very, very direct way. Murmur Wall as a project that's located in Moscone Gardens and kind of straddles this wall that connects, essentially, what's the ramp that takes you to the Convention Center, where all of the major San Francisco conventions take place. So for us, it was an installation whose audience was very transient. And it was kind of people coming through the city, usually not people who live there, trying to understand where they are. They're in downtown, but what does that actually mean? So Murmur Wall picks up trending search terms in a half-mile radius. So the idea for us was that this was almost an anticipatory structure. So if you're trying to understand the place that you're in, nowadays if we could actually know what everyone around us was searching for, maybe that would give us a better sense of the place rather than Moscone Gardens with the Metreon Theater and the view of Telegraph Hill, that there's a very different kind of understanding of place and location when it comes to social media and the Internet of Things. So Murmur Wall captures this kind of half-mile radius data set and plays it on these screens. And there's a video to kind of go through that once it loads. Otherwise I'll just keep talking. There you go. So one of the things that we were trying to do with Murmur Wall--and it's, on the one hand, kind of a representation of that information on these screens. But we were also trying to find a way to allow a person and individual to actually leave an imprint on the installation. So when you approach it, you can actually see these kind of trending local searches moving through, but then you can also send your own whisper, or you can send your own, let's say, piece of information. And you can actually see that kind of make its way through the system. So there's almost like an override mode, that if you need to, essentially, urban graffiti protest-- and it's been interesting, I haven't quite gotten those images from the last couple of weeks of what has been on the wall-- it allows people to, essentially, obviously, voice an opinion. But it has that kind of override mode of a big data set, a big kind of piece of information that is neutral or not neutral, but it's there, and then a kind of personal layer that is essentially placed on top of it. And in some ways, Murmur Wall, for us, is that kind of interesting overlap between those two very different scales of information. What's been compelling to us about all of these installations is that these projects may appear really singular, and really kind of individual one-offs. And they probably are because they've all been funded as one-offs. They're all individual kind of installations or projects. But for us, I think these environments, I would call them, are not necessarily singular. We see them as kind of spatial, visceral, and interconnected. So for us, although these are kind of singular installations, they actually describe a future city where our buildings and cities can actually create experiences. They can talk to one another. They can share information. They can actually respond to one another and create atmospheres and environments onto themselves. They can be rich in information, on the one hand, in terms of the information that they actually transmit. They can be open source, responsive, and ultimately participatory. Thank you very much. [APPLAUSE] Thank you, Natalie. I think, actually, some of the things I'm going to talk about relate to this idea of a perceptual architecture to the world that has an effect on us. But we're going to go in a very different direction out into the landscape. And this is kind of a sidebar to a manuscript I'm working on on the Owens Lake. And this is an ongoing project that I've been working on that's 300 miles out of Los Angeles. It's very intricately connected to the city and the resources in the city. And I've been developing an interface for design out there. And so working on the manuscript, I finally decided to get back into the project and understand what was going on on the site in this infrastructure and try to understand what were the forces at play in making this project, and making this sort of bizarre and peculiar infrastructure landscape. And one of the things that's come up is this idea of measurement and its role in design, and sort of the kind of relationship it has to design in this landscape, and more specifically, how does the kind of perceptual architecture of the landscape create different kinds of measures and influence measures, and how the aesthetic landscape kind of contributes to these objective measures and guides the project. And I think there's sort of interesting ecology, and we sort of exist within this cozy ecology of perception and senses that is sometimes obscured by all the objective measures. So this is the Owens Lake. Just gonna give you a little background. This is the view of the Owens Lake before it was drained. It's in this beautiful valley 300 mile miles north of Los Angeles. And when we built the aqueduct, we took all the water that fed this lake. We took all the surface waters from this valley. I'm actually in this picture. I was very excited to actually find that I was part of that historical record, waving a flag. So we took this kind of desolate, beautiful, but not very populated valley and lake and drained it. And there's a little bit left. There is this hyper saline pool that remains. But we've removed 95% of the water. And it's never been a good reservoir. It's an alkaline lake. It wasn't considered a great loss because it was considered possibly a waste of water to put all this sweet water that's running off of the Sierras and put in this alkaline lake. The valley-- I mean, it's still a very beautiful place though. Mary Austin is the John Muir of the desert. It's a gorgeous place to go. And in the midst of this beautiful landscape that still remains, there is this wound-- there was. So the lake was drained by 1930, and until 2000 it was this incredible emitter of dust. And it took a long time for environmental legislation to catch up with what a problem it was. So when they actually started measuring the dust on the lake, it was so big that it just was inconceivable. They had to reprogram the EPA database to accept the values that they were getting because they didn't have enough numbers to put them in. And it's a very damaging kind of particulate pollution. It's PM 10. It's very small. What happens is the salt crusts freeze and thaw and just pulverize themselves into this very fine particulate matter. And it would just fill up this entire valley, like 7,000 tons of dust would come off the lake in a single day, four million tons of dust in a year. So the EPA finally regulated PM 10 particulate pollution, and incredibly, Congress decided that anthropogenic sources of pollution could be a lake. So they said that basically the air pollution control district was regulated to control the smokestack, which was the lake, for the aqueduct. So the aqueduct had this sort of unaccounted smokestack, this giant, hundred-square-mile lake. And it made Los Angeles kind of-- not that concerned at first, but it started to become-- just the scope of the problem is so giant that they really started to be concerned that they were going to have to control all this and that they were on the line. And it was this incredible collision of this old regime of water extraction with the new environmental regime. And both were sort of blind to each other. The measure of dust, air pollution, didn't care that the cause was related and tied up within this major source of water for Los Angeles that basically had supported the growth of the city beyond its meager Los Angeles River. And there was going to be this kind of fantastic reckoning. How do we sort of solve this problem? How do we stop the dust but still keep the water? Or will the DWP have to give back the water? Because there's a tricky sort of part of this whole project in that the lake is protected by a public trust doctrine, which is a Justinian law, and it's been updated to basically protect public value. It doesn't protect the landscape in its integrity. It protects it protects its uses, its values, independently of actually the integrity of the lake itself. So there was some amount of flexibility, possibly. But when they first started looking at it, the landlord who owned the lake, who had to protect the public trust, told the Air Pollution Control District to look at refilling the lake. And so they had to do an analysis. What would it take, what would happen, if they refilled the lake? And a lot of people were hoping that that's what would happen. But they determined that technically, the lake was not a very good way to control dust because it would take 10 years to fill it up again. So everyone would suffer for 10 years, plus the lake's never been very reliable as a dust control method. So basically the APCD was sort of saying, we can do it better. We can build a better lake. And we can probably provide all the same public trust values that the lake did previously. So we're going to rebuild this lake and do something else. But it's still going to take a lot of water, because they decided they needed to involve water to be lake-like. But just in terms of what was happening here, you had this incredible project, this kind of middle ground that they were searching for. And there was no clear gradient between having this dry lake and having the full lake that's an alkaline lake. Alkaline lakes are just strange lakes altogether. And so how do you find that middle point? How do you satisfy all of these things, especially at this scale? And basically the only way to get there is just to follow the measurements of all the different values that you want. Because you're going to marshall a massive amount of human resources and money to get there, you need to have some ability to predict what you're going to have, and understand and be able to prove that you've created the value that you wanted. So I sort of see this as kind of this dark-- this terra incognita in between. It's in the middle. You're trying to measure your way through to this new kind of vast landscape. And you don't really know where you're going, but you know the measures are leading you there. You're not sure-- there's no precedent for the landscape's going to look like, but it's going to somehow meet all these measures. And it really puts this landscape in a position where it's very sensitive to measuring and to observation and to all the objective measures that determine it, because it can't cozy up to an old precedent or archetype. And so all the measures and values that they wanted the lake to have started to snowball. It was not just dust control, it was bird habitat, eventually it was aesthetics and other things. So became a very complicated project very quickly. But it had to be very objective and measured. And so when I looked at this project originally, I was like, well, this is really interesting. It just seems like the most supremely objective project, like everything has to be on a planometric scale and measured very carefully. There's not really a place for the body or the human to be involved in it or to understand it with this sort of raw sensory apparatus. It's either too big-- it's a giant lake-- or it's too small, it's dust, and there's no place for a body in between. And that was sort of why-- it was interesting. It was like, what does it mean to be in this kind of landscape, and what is the experience of being in that landscape? And the thing about this kind of landscape-- and there's a Reyner Banham quote-- but it's saying that it's a supremely visual place and a pleasurable place, like there's extreme visual pleasures. The desert landscape is just a panoptic landscape. Nothing can hide. Everything's visible. It's a very sensible landscape. Nothing is obscured either from your eyes or from the satellite, generally. So this is kind of this hyper visual landscape. And because-- we'll get into it-- the infrastructure they created, it's very easy to watch the entire thing all the time. I mean, in terms of an urban condition, it's dystopic, but in terms of creating the ultimate anthropocene landscape that achieves all these different goals, it's the ultimate landscape. You could potentially watch all of it all the time. And I would say it's one of the most watched landscapes out there. It has drones scanning the clouds of dirt to see if they're the right shape. There's satellites passing over all the time doing all sorts of analysis. They're doing constant fieldwork on this lake. It's just watched and watched and watched. And so it's interesting to sort of understand, that in that condition, what is the hierarchy, the scopic hierarchy? Because it really enables visual regimes to kind of flourish, and observation to flourish. So what happens in that condition? And I think the surprising thing for me when I looked into it is that the body has a much bigger role in this, and the human body in the field of observation plays a much bigger role. And there's sort of this bending of the objectivity into this subjective sphere that mingles and creates a different kind of data space and involvement than I expected. And it sort of bends this kind of assumption I think a lot of us have that everything is kind of bending towards remote sensing and this kind of model of the world. So in the manuscript I'm working on, there's a chapter where I look at these three different kinds of visual measures of the lake and kind of break them down. And they all sort of connect to each other, but I'm just going to look at dust, which is the sort of linchpin of the project. And in terms of sensing dust just in terms of the human sense of it, there's not really a clear-- a good relationship between what we can see and how it hurts us. The most invisible dust is the most damaging. But in this situation, you have this kind of unusual condition where you have a very damaging dust, PM 10, that is just this visual disaster, this sublime event that's extremely visual. So it's very tangible dust. And so that's kind of changed everything, and you think, because it's such a visible problem, that there would have been a clear link between the visuality of it and then the actual health issues that they were having in the valley. But perversely, it was just the visual quality, the obstruction of the dust to our vision that really started the dust control efforts, just a vision in and of itself. And that was because there was a naval weapons base about 50 miles away, and they were annoyed that they couldn't run their missions all the time. And you get a sense of how big this problem was. It's 50 miles away and it was obstructing their missions. So they started doing studies to look into where this dust was coming from. And already you start to realize, oh, it's actually harder to watch dust from the sky than you think. You can't use remote sensing as well as you think you can. So it's sort of already breaks down the hierarchy of remote sensing. Not that that's my goal today, but to some extent I'm doing that. So these are satellite images at different times of the day-- I mean, different weather events, and some dust. So when they wanted to measure it, they used their weapon, their advanced capability to fly, and did these daring missions where they flew around these dust storms even though there was dust on the ground where they would land. And they measured it visually just to sort of quantify it, and that quantification is what sort of started the whole dust control effort. When you get to the actual regulation and control of the dust on the lake, we get back to this kind of remote sensing network where they were trying to figure out how do we isolate where the dust is coming from on the lake so they can control the worst places. And so they built this network of sensors on the lake. And these are [? SenseITs. ?] These are these holdovers. They were never used, but they were in Desert Storm. They were designed to guide tanks through dust storms so they wouldn't go into where the sand was. But they only measure sand. They don't measure dust. So to sort of fill in this huge gap of not actually knowing if the dust is coming from these places-- it probably is, because the sand creates the dust-- they had to do visual observation. And what they did is, if there was a dust storm during business hours, they would jump on their ATVs and ride up into the mountains all around the lake, and sketch the dust storms, and triangulate them. And so the incredible thing is, unbeknownst to them, they were essentially sketching out the design of the lake in this sort of calibrated, instrumentalized plein air. And so you can see that this is a composite drawing of their sketches. And it's correlated to the [? SenseITs, ?] which is the square grid, which is a one-kilometer grid. It's way too big. And eventually they set up cameras and fixed them so they could resist the high winds. And so they watch over all these parts of the lake. This is the most recent setup they have. And I think it's interesting, when you look at these videos of the dust storms, you realize that it's not just about the subjective data point at the end, that there is this kind of powerful byproduct that's created by making these videos. And these cameras are now calibrated perfectly to the planometric view, so they know where each pixel is. But the dust storm videos have this other kind of potency that mingles with the objective data, and they trade power and legitimacy to each other. So apparently at this point, they've been able to model the lake so well that these videos don't really have a function anymore other than kind of being this kind of startling emotional image. But I think that a lot of these kind of subjective-based fieldwork methods leak this other aesthetic and other emotional content that surrounds and upsets them and pushes them around in an interesting way. On the lake itself, it's a totally different scopic regime. You have no idea where you are if you walk out there. It's completely disorienting. Now it's more orienting, but when they first went out there, they could be miles from the shore, and they just didn't know where they were. And they needed to measure what was going on the ground. And so they were trying to understand where they were, and it was just around the time that GPS's came out. So they had these bread boxes on their ATVs, and they could figure it out. And I think what was interesting for me was realizing that you sort of think of the GPS as being something that positions you, that you talk to the satellite and it helps you figure out where you are. And what was happening with the fieldwork was they were kind of turning that relationship around, because they were making the satellites have the kind of intimacy that they were having on the ground and measuring. So they created all these tools that allowed-- they calibrated all these remote sensing networks all, these pixels, so that they could detect wetness. They would go out there and they would tap the ground with their foot to test the wetness. But it also sort of stimulated, I think, an interesting kind of empowerment of the field worker, and an interest in sort of multi-instrumentality that used the field worker as the basis of their project. And one thing they started doing is, they realized after the dust storms there were these scars on the ground. So they started a protocol where they would drive their ATVs with the GPS's around the dust scars. And so this created all these tracks that, to their surprise, the DWP just built. That's what they used to create their dust control cells. So created this bizarre kind of land art project, which is almost like a-- it's no longer a natural shoreline, it's a dust scar shoreline, like a measure of those dust scars. And it's totally unintentional aesthetic, but it sort of rings of an art project. And I think that there's this kind of sensibility that we are always kind of caught within a loop of in terms of how we see the world and what we're attracted to. There's sort of inherent datascape that pulls us in. And when we partner with technologies, we can kind of break in and out of that, but there's still an orbit that we exist within that does have this kind of underlying influence on these projects. And having the body, though, involved is a problem, too. You know, it's having that field worker in there. These tracks were from a lawsuit against the APCD. The DWP sued them saying that their measuring was causing the dust. Going out there and measuring with ATVs was the cause of the dust. So they were in this dispute where they're like, no, it's not actually causing that much dust, but they weren't allow to measure it. So they had to stop the ATV stuff because it was too contentious to some extent, even though it still exists as a protocol. So I think that the body in the process-- it's not necessarily insidious, it's not necessarily breaking down objectivity, but it's in the way and it's having an effect. So what does it mean to have the vital organ and to start to acknowledge it, and have all these kinds of ways in which aesthetic kind of leaks out of the objective, having these kind of cinematic representations of the landscape that are objective representations, to be creating kind of an aesthetic act of drawing that's also measuring? And I was saying, I'm not trying to question objectivity, but I think it actually does take us back to a more basic practice of landscape architecture, of just understanding the architecture and the perceptual architecture of our space, but beginning to kind of expand that a little beyond just the idea of experience, but to think about measuring, and the way in which it is sort of a datascape that we're operating within, and kind of understand the proclivities that we have, the ways in which we drift. And I was interested to see someone like Lynch looking at different kinds of perceptual architectures that were coming up at the time, the highway, and considering, what does it mean to be on a highway? How does that create a different experience? And what can we learn from that proclivity of experience to adjust how we design? But maybe there's an opportunity for more of an understanding of how we measure and observe our landscapes objectively, and how we kind of carry forth with that. And I think there's maybe two different ways that we can kind of enhance that practice with that understanding. And one is maybe practicing calibrating our vision more to these kinds of landscapes and the way in which we measure them, so we have a greater awareness of that and the gravity of it. It also puts a little more weight on the perspectival view as not just an aesthetic view, but something that has a lot of power in many different ways. And then the work that I've been doing that sort of led to this work, thinking about the interfaces that we use to design, and how they sort of straddle subjective and objective spaces or planometric spaces. And that's all I got for you. Thank you. [APPLAUSE] Hi. So I'm going to be looking at the ways that we view the city, and especially the ways that we talk about it as smart. To start, I will offer this image as the most benign or banal representation of the smart city. This is a city of real time, responsive infrastructure. This, in particular, is the Bay Bridge in San Francisco. And in this case, data is playing a role on top of physical infrastructure, where the fees are adapted by the time of day, the lanes change what's accepted in them, there's discounts for high occupancy vehicles, and automatic ticketing and traffic signaling. So the smart city creates infrastructure fluidity and economic fluidity in this conception. So I'm going to offer three futures of the smart city. And with the self-driving car as the most anticipated change in our urban environment, I'm going to set up these futures through three stories of cars. So the first one is the smart city as the data-enriched skin of this city. And so here's that same bridge through its data layer. So this is anticipating the Internet of Things, objects, infrastructure, and devices that become an urban interface. The things themselves, in this case, the roadway, are neutral nodes, as are the data things, in this case the Fast Track system. And they remained largely unchanged and unchallenged by their data. So in this, we can see the roadways are designed as one system and the Fast Track designed as another. So this Internet of Things future creates the data it needs, adding a layer of information to everything it touches. The data, in this way, plays a secondary role to the city itself. And it's discussed in many ways, as a skin or a commons or an interface. When Tesla released its latest update of its autopilot software last fall, it included a feature that mapped the streets as they drove, as the cars drove upon them, acknowledging that the self-driving cars would need much more detailed maps of the roads than our current navigation software needs. The cars themselves start producing this layer of data for its future use. And this is an image that they released with that. So the data-enriched skin as a smart city is a distributed computer embedded within physical host objects. And this computer, though it's distributed and can infiltrate all parts of the city, is a system separate, then, from the city. The second smart city I'm calling the smart city as the reductive city. So as part of the same software update that Tesla had last fall, they released this diagram that described the cars' communications system. So it shows the communication system, the sight of the car, seeing the environment through various means, a cone of visibility, proximity sensing, and satellite communication. The view of the driver is replaced with the sight of the car. But this diagram really fascinates me in the information it excludes. And it does such a good job of feeling full and complete that it's hard to see what's not there. But of course it's not showing the city. It doesn't show pedestrians, bicyclists, buildings, even curbs. It shows a complete closed system that removes any controversy or any worry about the rest of the system or city. This change of sight creates new failure states in the smart city. So a remarkable example of this can be found in Google's recent patent registration for adhesive technologies for the front of self-driving vehicles. So according to reports, Google would solve the problem of difficult-to-detect pedestrians by simply gluing them to the hood of the car to avoid running over them. So in this future of closed systems and thing-oriented protocols for vehicle-to-vehicle communication, the objects outside of the system, such as people and bicycles, will have to start pinging through the same protocols to be visible in the city. So this has led to governments creating protocols already to try to anticipate and avoid these types of conflicts. So before we even have a need for these protocols in our daily life, it's already become a cluster of communication bottlenecks and potential miscommunications. So as Alexander Galloway and Eugene Thakkar explain it in Protocol Control and Networks, "the ability to move between discrete parts seamlessly by design is both the objective and result of protocol." and so these new types of protocol will become designed and vital elements of our environment to create safe, fluid ways of being in the city in the conception of the reductive city. So my third future of the smart city is the smart city as a field of play. So to stay on the theme of cars, there's many ways we could talk about this one. But if we go back to the toll bridge, one of the elements that's included in all of our roadways and tolls are ways to gather license plates for automatic ticketing. And so this information is collected with time, date, location. And it exists in a machine-to-machine communication, right? The idea is that there doesn't need to be human intervention within it. But we found out recently that the NSA is tracking every license plate that moves through these cities instead of just the ones that are traffic violations. And so this creates a situation which [INAUDIBLE] Michael calls Uberveillance, or the NSA calls bulk collection. And this human intervention into these closed systems are outside of its original intention, but they're also available within the structure of the data. In the same data set, license plate data, here's a second intervention. So this is a hack from Poland on the license plate database. They've created a simple SQL statement which intends to cause the database to erase itself when the plate is added. So the plate, whether functional or not-- and it's questionable whether this would function-- it provides another question about the potential for design to interact across these media, physical and digital machines and human systems, and acknowledges the field of play between the things and the people in the smart city. So the question, then, that I have is, how do we engage these systems intelligently with an eye to other ends or other functional possibilities within them, and through other allowable results within the systems at play in our city or cities? So the field of play is one, as we've seen, that can both top down and bottom up be slippery, leaky, and hackable. And so to understand the smart city as a field of play, it's important to acknowledge not just the Internet of Things, but also the internet of people. And so in that way, I propose a defense of noisy systems. So as a way of talking about this I'm going to use the city of Jakarta. Jakarta has the highest urbanization rate of any city in the world. It's moved in the last 30 years from a city of nine million to one of 28 million, and doubled in size in the last 10 years. It's a delta city, as many worldwide are, with 13 rivers running through it. And it's seen increased flooding every year as a result of many factors, including expanded development into more of the watershed basin, ground subsidence, the increased impervious surfaces added both to the ground condition as well as to the riverbed. And this type of expansion and, say, type of emergency is not a single one. This flooding is a common event in the city. People are living in and around this periodic condition throughout the monsoon. So there have been many large-scale projects globally to try to model this city. And much money is spent trying to understand its city of things, its infrastructure in order to produce a smart city solution to the vulnerability of flooding. So these models try to understand its ground condition, flow rates of the rivers, the footprint of the city, and they use that as a static backdrop through which to redesign the infrastructure. So here is a collaboration between Jakarta and the Netherlands to produce a plan, along with the World Bank, to build a seawall and create freshwater lagoon inside as a barrier to the sea. There's a lot of elements unacknowledged in this drawing. But you can see through the change of color of the ground that there's a band of city against the sea that's a much higher elevation than the city itself, which is below sea level, which creates a real problem-- that again, this clear diagram is good at hiding-- about how the rivers actually flow into this lagoon. So there's this fundamental bias in the smart city that my three examples, as well as the last diagram, have, which is that humans are inefficient, emotional, and chaotic actors within a city. Urban residents in a smart city are data providers, sensors, and contributors. They're participants in what is otherwise a static and organized and knowable environment. So within the city of people, Jakarta actually has huge resources. They have a lot of people, for one, and they also are fast becoming the social media capital of the world. Jakarta is the most active Twitter user base in the world, and the country of Indonesia is the fourth most active on Facebook. Half the people in Jakarta are under 30 and social media is really ingrained in their daily life. So if we look at social media as a communication infrastructure within the city, there's a whole nother way that we can see the city. So this is every incident of Tweeting banjir, which means flood, within Jakarta during the 2014-15 monsoon season, which looks very noisy. This is about as noisy as it gets as far as data goes. But if we look at that same data through time, we can see more of a logic at play in the use of aligning the terms with the incidental or periodic emergency of flood. So I worked with a team in Jakarta to develop Peta Jakarta, which is a real-time flood map of Jakarta using Twitter. And using Twitter as a platform where the data already was, we developed a way to engage the Twitter community on their own terms, making an open access community tool. This is what that looks like in practice. It's a real-time map. And when you zoom in when you're on the street in Jakarta, and you open it and you're geolocated, it will pull up where you are and any reports around you in last six hours, if you're looking at it on a laptop or away, it'll be a heat map of probable floods throughout the city. So we used Twitter in this way as a vehicle to clean the data that it generated. We used the structures already within the system to enhance the data. So when anybody tweeted flood or banjir, it was counted as unverified data. When they sent it to us, to the Peta Jakarta account, we called that clean data. And this is what that looked like. Anyone who tweeted about flooding not to us would get a very casual note in, say, Twitter speak, not in emergency or NGO-speak, but to say, hey, it looks like you're talking about flood. Do you see one? Can you tell us about it? And anyone who was telling us about it would get a Twitter card to promote the project to other people. So this kind of flow chart accommodated geolocation, topic, and direct communication, sorted the data, and responded to people accordingly. And this is what that looks like in practice. So this work emphasized to me the role that social media could play in emergency response, and does already play, as well as the importance of emergency communication to happen in the same place that we use for daily communication. So I saw an underutilized potential within this system of emoji as a non-language-specific emergency communication device. So emoji are part of Unicode. And you can see the Unicode column there of the strings that the symbols stand in for. Unicode is an internet protocol for language communication that covers all language. It was originally developed for non-Roman characters when the internet became a global structure. So it's the reason that when you send a piece of text in any language on your phone or on the internet in any way, it arrives at the end location in the correct format regardless of device, hardware, software, anything. So emoji is part of this system. So by being included in this system-- and, I think, because they're frivolous and because we use them in daily communication-- they're important. So in the same way that we use Twitter to just say, hey, to a friend, and then we're already using it when emergency happened, I think emoji have the same potential. So if we look at another type of emergency-- so looking at #earthquake, earthquake in Japanese is one of the most used hashtags last year. There's a lot of potential to organize emergency communication through social media, but especially this emoji, say, possibility comes out of the increased use of mobile phones on social media at large to communicate after emergencies, and also the current bottleneck, after a disaster, in communicating between the global response network, which generally operates in English, and the language, which the community, wherever they are, is communicating in to each other. So these are tweets from this fall, from recent earthquake events. So we can see in Italian and in Japanese and in English how different these are as hashtags. So what this project, which we're calling Emerji, has developed, is finding the holes within the emoji that currently exist and that are being used in hashtags for local conversation, and developing from that a set of icons to be used to connect these conversations beyond local communities. And I think since I'm talking about the Internet of Things and the internet of people, it's worth just adding one last twist to this, which is, in the news right now, the discussion about the role of social media companies as content providers, and fake news using the aesthetics and tropes of news, are becoming less and less distinguishable from journalism. And I say this because, really, the internet of people, which has really been operating very separately in some ways from the Internet of Things, is now getting trolled by the Internet of Things. And so in all of the potential that also exists within both the Internet of Things and people, we're really getting a lot of complexities added to the landscape at this moment. So this is the most shared fake news story from a fake news agency leading up to the election. And the sharing of the fake story-- there was three times as much sharing of the fake story as there was the debunking of the fake story. So it's something I think we will be contending with as designers as well. So thanks. [APPLAUSE] Super fascinating. I love the concept of the Internet of Things trolling the internet of people. And as we know, the internet of people have been trolling the internet of animals for awhile, and we hope to use the Internet of Things to help the internet of people stop trolling the internet of animals. So that's kind of what my talk's about. And thank you for joining me into this discussion. I'm slightly nervous about having these slides, this jumble of slides in front of the design eyes in this audience. So just to give you a little context of the worlds that I've been operating in, my academic career has been at this intersection area between neuroscience and artificial intelligence since the mid-'90s. My academic career at Georgia Tech in Maryland and ETH Zurich was focused on neuromorphic chips and auditory neuroscience and robotics. Something I want to highlight here, on the bottom left, was an installation I designed the auditory system for at the Swiss Expo in '02. It's a synthetic organism we called ADA that you could walk inside, interact with, and experience. And ADA had its own point of view about how it wanted to interact with people coming inside and had many ways of sensing and interacting with people. I've done a lot over the last 13 years in Silicon Valley, and most of it has had the stamp of having a neural inspiration. The thing I'm highlighting here is a company called Audience. I was the algorithm architect, and we did have neuroscience-based technology that we ended up using to improve speech enhancement and also speech recognition in iPhones and Samsung Galaxies. And probably one of the first cases that I know of of a successful neuroscience-based technology company going all the way from a model of the human auditory system to an IPO, although it took much more effort than you think-- than you see on TV. OK. Much more pertinent to our discussion today is the work I've been doing for the last five years with Conservation Metrics. It's a small company based in Santa Cruz, and we use remote sensing data-- well, principally microphones and land-based trail cameras-- to progressively automate the monitoring of endangered species. And so as most of us here appreciate, our planet's been blessed with a richness of diversity of plants and animals that have co-evolved over hundreds of millions of years. And we're but one of millions of species on this planet. Estimates vary widely-- 8.7 million is one that I've seen-- but I don't think we really know. But us, a single species, is reaching across an ever-increasing number of these ecosystems to extract the goods and services we feel we need to sustain our growth and well-being. These services are as diverse as storm protection, climate regulation, pesticides, medicines, obviously food. Economists have made bonafide efforts to put values on the total sum value of these ecosystem services. And as you can imagine, they've come up with some pretty large numbers. Again, we're not sure how much we believe these numbers, but we do feel that it's multiples of world domestic product worth of value. And not to mention the aesthetic and recreational, even spiritual, qualities of nature we all enjoy. I don't know if we can put a value on that, and we don't know if we can survive as a species without it. But we may soon have to find out, because as we know, we're finding that rapidly, over the last few decades, we're losing. We're losing these systems. They're becoming heavily degraded. So species extinction rates are maybe 1,000 times the natural rate, and wildlife populations have been halved in the last few decades. And there was another report that came out about that, that there is a healthy debate about, but you know, for example, that freshwater fish populations are down around 85%. In response, people and organizations all over the world are trying a whole host of different ways of addressing the problem. And in doing so, they're spending an estimated $20 billion. But what many of these things have is a dirty little secret in that they really don't know, on a scientific basis, how well these things work. We all too often lack the data to say with any statistical significance what the impact of a single conservation intervention will be, let alone to compare multiple alternatives. So if you think of an example like restoring populations of an endangered bird-- this is a Whiskered Auklet-- what would be more effective? Restoring nests using artificial nests, or eradicating invasive rats? What about the return on investment? How expensive are these two different things? That's not the kind of language you typically see in conservations circles. And so as a result, conservation funding has tended to be on the left-hand side of this continuum, relying on emotion and logic to determine which projects get funding, as opposed to outcomes-based, data-driven decisions that are so much more common in other sectors. So why is conservation lagging behind the use of data-driven decisions? Well, in part, it may be cultural, but we believe the main reason is that the necessary data are difficult to obtain, sometimes dangerous and often remote, and almost always labor intensive, invasive, and just prohibitively expensive. Now a conservation metrics mission is to move the conservation world to the right side of this funding continuum by harnessing recent technological advances in sensor networks data and machine intelligence to provide affordable and effective measures of conservation outcomes. And we're doing this leveraging an increasing variety of sensor types. So we have a song meter. It's a dual microphone data logger. On the left-hand side here we have a typical trail camera that we call them camera traps because they were really developed for hunting, originally, I think, and an increasing variety of sensor platforms. So now all of these sensor platforms are gaining traction in conservation research and practice. We have hydrophones under the water. We have infrared sensors. We have ultrasound. We have accelerometers detecting collisions on wind turbines, UAVs and micro sets, and all the way down to the use of DNA sampling. So from the very micro to the very macro. Now these are really wonderful because they're cheap, they work 24/7, they never have hangovers, and they collect lots and lots of data. The challenge that we're addressing is how to manage and analyze all these data, especially for governments and conservation organizations with very limited budget. And this year we're monitoring around 50 conservation projects in 14 different countries and processing around a petabyte of sensor data, just with a small team of conservation scientists and me working to provide them with software machine learning. Now we're leveraging recent advances in deep learning for this. And you can think of the vision-- so if you think about these conservation scientists in the lab processing more and more data, having to think less and less about the data formats and detecting individual animals, but thinking more about how population trends are changing and their ecosystems models moving to this global kind of nervous system that we can tap into, so it's this global biodiversity monitoring AI. It's a planetary intelligence layer in the service of understanding and safeguarding the natural world. Now in execution, we've experienced large improvements in analysis throughput due to our expert in-the-loop integration of deep learning models for signal detection and classification. What is deep learning? It's the most vibrant research area, and it's really remaking our tech industry. So it's the technology underlying most of the new products coming out of Google, Microsoft, Baidu, et cetera. It mostly refers to modern, large-scale neural networks. It's a sequence of operations that are learned carte blanche. They're able to learn complex tasks with high accuracy if you have enough data to train them, and we think of them as sort of mapping raw data through a hierarchy of features that are completely learned. And so this is a face recognition example. So if you go back and introspect, what is the system learning, it's often exciting when we see that it's learning information that we already know about, although it's not constrained to do that. In early layers we learn to extract little edges and little blobs of light. Those get constructed into higher order features, and finally distinct sort of face detectors at the outermost layers that are signaling which face it is. The exciting thing about these techniques is that they're data agnostic, so you apply them to any kind of data source, and they've been used as speech recognition. So all the speech recognition you see out there, large parts of it are using these technologies, image processing, text analysis, and the like. Now I have a lot of experience in this, and a lot of interest in neural-inspired computation. But it actually is a good fit to this particular biodiversity monitoring application because it is particularly effective when you have a lot of training data. And that's something we do have a lot of. So we get in tens of thousands of hours of audio data from monitoring projects. We get in millions of images. And due to the combination of nice, advanced filtering methods and a little workforce of conservation scientists, we produce these label data sets. The other thing I wanted to highlight, again, is that it's an approach to that's extensible. So you can use it for many disparate types of sensor data. And I talked about all the different sensor platforms that some of them were using. Some of them we can imagine using. So we can reuse the same components, and in fact combine multiple sensor sources into a single model without having much domain expertise into, OK, how do we construct an image processing system? We just rely on the domain expertise of the conservation scientists, although I guess some of you might think it's sad that conservation scientist goes from this, out happy in the field, to maybe toiling over a computer in the lab. But I guess two observations on that is that , they're not replacing field work. They're certainly not replacing field work. In fact we rely on the field work to make sure that our models are our corresponding to reality. So secondly, they actually enjoy this. So you get to go through so much data and use advanced methods to listen to-- this is audio data that's being audited here, but you're not constrained to being in a single field location for a day. You have access to a whole year's worth of animal data. So the different methods to explore the data, to compare multiple models. In the end these models are being used mostly to estimate population counts for different endangered species that are being monitored. And so we are detecting, for example, with birds, the sound of a particular species' call, and then those get plugged into a back end Bayesian model that's estimating actual population trends. So call rates are not one to one with population, and it's a species-dependent thing. I have an example here of-- these are audio spectrograms and might not make sense to most of you. But this axis is time, this axis is frequency, and then the more red is the more energy. And so when we get data in, we can look at these windows of audio data and quickly make an assessment of what's in those pictures. And this is from a monitoring project that's monitoring an endangered and elusive seabird called the Marbled Murrelet. Unsorted, we have a page of results of robins and jays and man-made sounds like saws. Once we apply our Marbled Murrelet model that was trained by our conservation scientists, the first page is this. And although I didn't bring audio, you can see there's a lot of commonality here. These signals and this frequency band, these are all Marbled Murrelet. We're bringing online similar capabilities on the image processing front. I have experience on both sides. So we're getting millions of images for trail cameras, many of them infrared. And one of the additional things I'm highlighting here is the utility of localizing individuals within images. It's useful for counting multiple individuals and also modeling the behavior between individuals. In some cases it's extremely challenging, and it's something that a human would be hard pressed to be able to do. So here's one of millions of images we've received from the jungles of Guam in order to monitor an invasive snake called the Brown Tree Snake that's decimated native bird populations there. And so maybe some of you are conservationists and are used to seeing images like this, but I'm not sure if many of us can see a snake in this image. Let me just highlight where the snake is. This is what our model is showing where the snake is. So I don't think many people would argue against automating a way of finding enormous snakes in the jungle at night. That's one aspect of fieldwork that's probably not that fun. OK. So I've been given the five minutes. I'm going to skip ahead a little bit. Another one. This is 3,000 by 4,000 pixel images from drones flying over beaches in Costa Rica, and we're able to pick out sea turtles so Olive Ridley sea turtles. And they're using that to model distribution of turtles as a function of distance from the shore. Let's see. What do I want to highlight in the remainder of the talk? I think these things are important. So these are intelligent systems, and we can use them in many of the same ways that we think that we can use human intelligence. We can take a system and leverage information about similarity. So for example, if we train a model on a particular species, and we have a lot of data for that species so it does really well, we don't necessarily have to have the same amount of data from a related species. So we can apply that the property transfer learning to reuse components of a model. So we reuse the early features learned from the first species in order to improve the results on a related species. We could do the same thing with multi-task learning. If we have multiple data sets, we can combine those data sets into learning shared representation that ends up being more effective for the individual data sets. And this is an example of classifying a species, but also detecting the behavior of that species. So since we're getting into the realities portion of the talk, I can just skim through it really quickly. What are we using it for? Detecting rare and elusive species. So we've detected species that weren't thought to exist anymore off the coast of Japan by the use of these monitoring methods. We're estimating population trajectories, and also we're using it to detect the sound of endangered birds hitting power lines in Kauai. And that's an interesting one because, skipping ahead, we can put these sensing boxes out there 24/7 and find out where most of the problem is occurring. And then that information is used to close the loop, to determine, OK, let's experiment with shining lasers down a particular stretch of power lines where we know the problem is much more severe. And it's a problem that was not known without using these automated monitoring techniques, because these are out in the jungles. You know, carcasses fly far away from the power lines and are carried away. So these are the kind of benefits these techniques can have. And so I just want to close on one topic that I think is really interesting to discuss over a drink later is that we're using these methods now to kind of see what's going on, so to perceive the world. But the actions are still sort of logically driven. They're driven by our knowledge of, OK, this is what we tried before, or this is what we think our model says we should try. But there's not really an automated closing of that perception-action loop. And one of the most vibrant areas of deep learning right now is something called reinforcement learning that provides a framework for us to do that. And we've seen advances in the application of those methods. You've probably seen in the news, we can hurt these systems up to an Atari system with no knowledge of what a pixel is. It just gets in these images. Doesn't know what a joystick does, it just knows that it wants to increase the score. So over time, through lots of experimentation, it figures out how to play these games with superhuman accuracy. So we've seen this in these Atari games. This has been done in chess for many years, but we broke a milestone in the game of Go just last year. Now we're looking at advanced, more complex games like StarCraft. So when we're applying these things to the natural world, what is the action space, right? So what's the joystick in conservation projects? What is the reward function? We know it's not just increasing population densities, right? That is going to lead to environmental disaster. So if we want to think about closing this loop with reinforcement learning, these methods that are coming online now, how should we smartly use them? And I think that's a big part of the discussion that should be happening right now in conservation science. Thank you. [APPLAUSE] OK. Thank you guys all so much for those really fantastic presentations. It's really wonderful and gratifying to see this all come together on a full panel after spending a lot of time looking at all of your work. And so I'm just going to make a couple of comments ON what I see kind of coming out of all of your work tied together. And I'll ask a couple of questions, and then open it up for questions from the audience. And I just ask when we do that, that you wait until you actually receive the microphone so that we can get you on the live broadcast recording. And so I really believe that all of your work touches on ideas, theories, and methods of extending human senses and perception through technology, and more specifically, computation to measure and abstract the landscape, and what we have been referring to, Brad and I, as another form of reality that's been crafted through careful modeling and representation. And these methods of mediation between digital infrastructures and physical infrastructures that's been brought up-- or you could say soft systems and hard systems-- begin to create really complex entanglements between landscapes and environments that are already eliding or conforming to various rules and methods of governance. And so your designs, in this sense, are very much acts of resistance that not only attempt to create new interventions, but also shape and push back against existing structures of landscape and control. And so I'm going to actually connect to two different quotes. One is by Eric Winsberg, who wrote the text Science in the Age of Computer Simulation, where he states that "The models we need to construct in order to do our science need to be constructed delicately and from as many sources as are available. Consequently these models are best viewed not as mere solutions to theoretical equations. They are rich physical constructs that mediate between our theories and the world. And so I think this speaks to not only projects that are scientific, but also to design projects. And what I really love about this text, and actually many texts written about scientific modeling and simulation, is just how much that representation and visualization of the simulations and models plays into the reading and the verification and validation of the model. And another code is actually from a paper that Sara Dean and Etienne Turbin recently published, an volume titled Machinic Apprentice. And they state that "Projects of political resistance and social emancipation certainly require technical infrastructure just as they demand much more than armchair critics. Getting our hands dirty with data and their socio-spatial consequences means working with and designing for digital infrastructure. It means leveraging design across scales to create cracks, gaps, hacks, and openings that enable and embolden politics. And from this perspective, politics is a fundamental design problem, and design is indelibly political." And so I think that you guys are all engaging in practices of being machinic apprentices. And so I'm also using the term resistance from before, in the way that Galloway and Thakkar refer to it, as a political act within an ecology of computational and networked systems. And so Natalie, I think that your work really tackles the lack of spatiality in information sensing and sharing in the urban landscape. Alex, your work in developing a philosophy of design and methods for design in what you aptly designated terra icognito. I think very truly it's a great description for the types of anthropogenic landscapes that our attempts of control and formulations of infrastructure have engendered. And I love that you're talking about bending objectivity and subjectivity, and I think that really gets at the heart of what we mean by these different forms of realities. And your call to action that basic landscape architectural practice, and our proclivities to drift, really becomes an opportunity to enhance our practice. And Sarah, your work with the Peta Jakarta team really exposes the fact that flooding in Jakarta is not a topological condition, and really is, therefore, impossible to be modeled or predicted with the types of static IOT sensors and algorithms that maybe we would typically think to use in an urban setting. And so in this instance, the sensory capabilities of humans are put at the forefront and are incredibly valuable. And so your very successful attempts at crowdsourcing real time information is not about prediction, but provides a functioning real time mapping tool that has proven to prevent mortality through already established forms of communication. I think we're all very excited to have the emerji addition to our text language. And David, your work really deals with the converse relationship to what Sarah is talking about, in which sensors and algorithms are much better at rendering a more accurate depiction of the landscape. And I love your statement that we really lack the data to actually know if our actions are meeting our goals. And in so many of these projects, so many instances where the landscape is attempted to be modeled or computed, we're really trying to meet a certain kind of goal that is intentional and is not necessarily any form of accurate depiction of reality. And so I would love for each of you to discuss your work, or relationships between your work, where forms of mediation are used as acts of resistance, and how current and existing computational and machinic systems have provided opportunities, but also how you, now and in the future, really see these acts of resistance shaping and modifying the landscape and not just kind of presenting another kind of synoptic view or form of reality. I can comment on the power line work that I just highlighted. That's been a complicated story because it's been a conversation with the utility company, of course, right? So there's a small organization on the island of Kauai that's a conservation organization. It's an endangered bird. That's why, unfortunately, people-- in terms of policy and human action-- we don't really care about species until they are endangered. And this is an endangered Hawaiian bird called the Hawaiian petrel. It was known this problem existed because occasionally they would find dead birds around power lines. But once Conservation Metrics got involved and we did some of this pilot work, we saw that there was a much bigger problem. And that's not the story that the utility company wanted to hear. And so there was a period where lawyers started to get involved, and there was a lot of digging into how real was this data, and what was the bias there? Was this really being used purely to shake things up, or is this more of just an objective observation? So over time that conversation shifted, because it stopped going from a surprising problem to a useful tool, because with the problem revealed, then it wasn't something that was just kind of going to linger there forever. Then they could actually take actions. They could actually say the problem was localized to these stretches, and there are certain stretches where we can underground. The wires there are certain stretches where that's not really feasible. So let's look at the other ways to affect the bird behavior around the wires, illuminating the wires being one example. So I guess the undergrounding of the wires is one aspect of the changing of the landscape. So that's knowledge of where the birds are flying. And this is a bird that feeds-- they go out to sea to feed at night, they come back into their nests, and they can't see the power lines. That's why that's occurring. So we're actually seeing a modification of the man-made structures on the island because of our knowledge of the flight plans of these birds. I'll add to that real quick, because I think one of the things I was really taken with in your presentation was asking new questions of conservation, and also of the systems that we use, and saying, for example, that systems that started as hunting monitoring also have this potential for conservation. And that's where I get really interested in technology is, we get the solutions to the questions we ask, right? And so I think the kind of common ground between our two bodies of work is that on the one, the question of asking a different question of existing platforms is one way to get a different answer, to say, what else can this protocol contain? What other ends? And then the other mode of that is different types of mediation or different types of technology that ask different questions to begin with. And so I think those are both forms of resistance, of asking not the question that the power company would have asked, but putting that into conversation through new forms of technology. So I think that both of those are resistant technologies, but through very different means, say. One thing, sort of following a little bit on David's comment, the way in which the increased information sort of reverberates through the system is really quite interesting, because you could say each time we add information to the system it's an act of resistance. And then that sort of-- oftentimes the information switches sides, in a way. Like almost every time in Owens Lake, they discovered the birds and they watched the birds and then they counted them. then now the DWP, the city, is counting them obsessively because they want to reduce the water but maintain the birds. So they're trying to prove that they can do that. So they're increasing their model. So the idea of just adding information is not such an easy way of just resistance, in a way. But it obviously has power that we can't resist and that we shouldn't. But we don't really know what's going to happen to the whole system because we don't know how that information is going to play with the other information that's out there. Because you sort of see these endless kind of strange collisions between different information sets. There's the whole visual resource management at the lake, and then there's the birdwatching, and then there's the birds, and the dust. And all these things are having these strange metaphysics. So it's hard to kind of pin it down. It's volatile. I also think it's a little bit unclear, not in terms of how you set it up, but just in terms of what you're resisting to. And I think that's kind of-- in some ways I feel like a lot of the work that is presented is essentially is a series of experiments that are trying to figure out where those limitations lie, like, I don't know, did I know that it would get taken over by an XYZ organization to put out there whatever message that they were thinking of when they-- so I think there is a little bit of-- I see a lot of the work that was presented today as actually being almost like testing those boundaries, like, at which point is that data neutral? Is it neutral? Is it not mutual? Do I need to resist to the information that I'm actually bringing in? Do I assume that a certain audience will embrace this or resist it? So I feel like there's ways that we're all making inroads in this. But I think that the question of resistance I think is an interesting one, because you could say you're resisting through the technology, or you're resisting through the information. So there's different layers that I think we're all kind of tapping into. I think the other kind of-- I mean, in a more, let's say, personal kind of terms in terms of the work that we're trying to do, I think for us these aren't necessarily one-offs. Like we're trying to kind of find ways to have more than one of them talk to one another and trying to understand what the implications are when these things start behaving in much more complex ways that are also completely unpredictable in both the behavior of the people, but then also of the systems. So I think that the scale kind of exponentially increases as a kind of multiple inputs come into this. Well, that actually leads into my next question very well, which is, I think that all of you guys are engaging in experimental practices and methods of testing in the landscape, and very much, a lot of them, in real time. And I kind of wonder what this practice affords, and what the kinds of limitations are within your disciplines, but also, maybe, is there a potential to accept the urban or landscape condition as a kind of ongoing experiment? And what does that mean for how your projects might actually get built or completed or deployed? One thing, working on this project, Owens Lake, it's this constant-- the manager is spending a $1 billion now to redo it all after it's been worked for 10 years. And he's like, this is our first attempt to redo it, because clearly it's not the last one. And it's this mosaic. And it's like they rewrite the code and try in a different spot. And the whole thing is experimental, and I mean, I hope the whole world doesn't become this kind of bizarre, cellular, experimental dust controlled habitat. But you know, it's going to happen here and there that we have this. And it's all experimental, and now that we have these measures of whether they're successful or not, like the fitness measures, it's going to be this constant experiment, because we're really taking the reins of all these different parameters, and we're held accountable. So anyway, in terms of my own practice, I just sort of see myself sort of working within that framework and seeing how my work is going to maybe change the revision, the next revision. I think a couple of things that it's brought up for us is, as a lot of the work that we're doing now is becoming permanent rather than just a kind of one-off installation or something that's in a park for two years, is the kind of post-occupancy-- that's what I call it, post-occupancy, but it's the kind of evaluation of the thing and what information it is gathering, collecting, triggering, has been something that I never really thought we would have to do. But it's part of the learning process for both the work itself, because it has the ability to kind of feed information back to us. I think that's been an aspect of it that we're kind of trying to figure out how to deal with. I think in terms of the other layer of your question-- I don't know how to connect the two-- which is the disciplinary question, I don't know. I really don't know. I mean, I'm an architect by training. I've taught in landscape departments probably more than I've taught in architecture departments. And my office basically builds electronics most of the days. Sp it's like a weird mix of things. I have been trying to kind of think about it, also, just being part of school and academia and education, but it speaks to maybe a different kind of blurring of those boundaries that seems to be happening much more often. So I don't know to what extent do those disciplinary bounds still maintain, or how we are transgressing them is kind of interesting and compelling. But I can't answer that part of the question quite yet. This experimentation is kind of tricky in wildlife conservation because you're playing with lives. And often the people that are involved care deeply about the individual lives. And so that is something that's definitely needed, though. It's something needed to make progress on understanding, and also to-- you need progress on understanding, and/or progress on these methods of automatically optimizing to some reward function. Both of those things need access to an experimental space. And so these game-playing learning machines, they play millions of games and most of them fail, right? So how do we-- is it going to be possible to adopt these techniques without causing some environmental catastrophe? Can we do that the kinds of things that we're doing on the lake, where you kind of-- OK, you have kind of fixed boundaries on an ecosystem, and you come up with a grid where you're trying different things in different areas. Or maybe it should be more of a passive approach, where you know, there's a lot of conservation projects all over the world. Can we use our methods now to understand, OK, this is a reference ecosystem. Now let's look at similar projects all over the world, maybe through the literature, even, just pooling in information about what was tried and what the effect was. That's more of a passive experimentation. But there's an active debate. You know, I talk to colleges quite often. And those who really feel like they want to make progress, often they'll say, gosh, I wish I could do more experiments. But it's really hard to do because some experiments, most of the experiments, are going to fail. So you're going to cause a loss of life, you're going to cause degradation of ecosystems in order to serve knowledge, and ultimately to leapfrog the current trends, which everything we've tried over the last few years-- with many good exceptions-- on the whole, though, all the things we've tried has not really slowed our biodiversity loss. I'll just add that we're experimental because we've never been here before. I think, too, as far as the types of technologies available, but also the rate of change in the world right now. And so the number of tools needed is increasing enormously while we have more tools that we can start to adapt. So for me, in the disciplinary question, I'm trained as an architect, and before that in communication arts. And so the question of process, and starting with ends and possibilities, and ending with technology and methods and means, to me, is how I come at that. For example, in Peda Jakarta we didn't start with Twitter. We ended with Twitter. We started with trying to understand this incredibly out-scaled, complex environment that many groups around the world are constantly trying to come to terms with. So Twitter became the place that that was possible. Rather than going out to say, we're going to do a Twitter project. What should we do? So I think that that's really important. And I think that's embedded in all four of our practices. And to me, I'm looking for ways of engaging the city and engaging urban interface, in my mind, as an architect, but trying to find out where that can happen now with agency. Thank you. If my mental clock is accurate, we may have about seven minutes for questions from the audience. So I guess just wait until you get the microphone. Hi. I had a few questions about the tensions of sensing and technology. One of them was-- I think I can see this across all of your work-- the limitation of modeling in the face of argumentation-specific work. So in the sense that in each project there's kind of a question being asked before the process of looking out to see what exists within the landscape, and how, maybe in a certain sense, that might limit the scope of the things in which you're observing in the world. And I was thinking of that relative to the anthropogenic effects and methods of sensing and how, maybe in the case of conservation, we've affected the habitats in which we're engaging, but then also we've also used anthropogenic tools to begin to engage within those habitats to see how we can resolve some of those issues. And I see it kind of as a constant process. And I'm wondering, at what point do we say that we stop sensing or begin sensing relative to that anthropogenic change in which we're affecting those environments? Well, I'll add a quick start to this, which is that the change that environments are seeing now, or any place is seeing now, is coming from everywhere. I don't think we can realistically separate our effect, things we've affected and things we haven't affected. And so whether you're in an urban or a very rural environment, the impacts that that environment are having on that environment are economically global and environmentally global and going to be changing. And if we left right now, if we all got up and went to Mars, there would still be change that would continue to impact these environments. So I think it's, in a way, a false dichotomy to say, if we impact this-- I think we can ask, are we negatively or positively impacting? But the question of, are we impacting, I think we're beyond that point. I would just say, it's definitely the case that we change the environment by sensing it. It creates change. But I think that yeah, it's kind of, let's all be incensed, to some extent. I don't have a good-- I think it's kind of true it might be a false dichotomy between-- I don't know how productive that thought-- where it takes us right now. So I don't have a better answer. I think there may be a way to-- thank you for the time. I think maybe a way to address a part of your question, I think, is the real time piece, which I know is something you asked earlier on. But the ability for all of these things that we've been talking about to actually adapt on the spot, and kind of evolve, and potentially have the actual artificial intelligence to learn, I think, is maybe a way to start addressing some of those concerns. And I'm not saying that we've done that, because we haven't kind of worked through our work personally that way. But I think you bring up a point where potentially, actually having-- and I'm going just going to give the example of being able to hack the piece again, or being able to go back in again and allow it to kind of evolve or co-evolve with something that's kind of ongoing, or may be popping up or emerging at that particular instance, may be a way to address, I think, a different nuance of your question, where there may be a way to, through time and phasing of some of these things, or the ability for them to learn, their ability to actually evolve over time may be a way to address, I think, some of the concerns that you're bringing up. And I'll also add, from the way you presented your work today, the thing that stuck out for me that I think addresses this, is the difference, then, between the hardware and software, and the ability for something to exist in the world and adapt. And I think that's what you're saying. But I think in a particular way, the fact that we build things that also-- those things live and have phases, but the technology also is in this constant iteration and potential. And so I just wanted to say that that's a very important part of your work as well as, I think, in all these projects. Sensing has so many-- it's such a multivariate thing. I mean, we're talking about sensing leading to action. But sensing of these systems is, I think-- maybe the most common application of sensing of the natural world, up to now, has been to generate empathy, right? So we generate these pictures, or now we can place a 360 camera and you can be in the Amazon jungle and really understand and revere the beauty of the place. And that, I guess, indirectly leads to action, it leads to societal change and changes in policies. And so that's something that's really interesting to think about, all of these different uses of sensing outside of the gleaning of knowledge. In a way, both of these things, trying to go in and understand things so we can save them, or the fact that these systems are being degraded. Both of these things, theoretically they lead to a reduction of the amount of stuff that's out there that's unknown, because in one case, there's just less out there, and the only stuff that remains is the stuff that can successfully interact with us every day. In the other case, we're intentionally going out to get information. So can we have embedded in our objective functions of the sensing something that balances this somehow, something that modulates the knowledge that we're getting out of a system, that keeps it constant so we can get the utility that we need without necessarily having to know everything. That's going to be tough as a species, thought. It's not something we do. I mean, I sort of wonder if the question is really about the quality of sensing and data that we're getting, and how that's instrumentalized. Because I think that even before we had digital sensing, we kind of sensed the whole world, whether it was through a book or a legend, or it was just the part of the map that we never went to. And we had an attitude, and it changed how we reacted to that space and impelled us to take different actions. So it's sort of like more reckoning, the more real time sensing, the different kinds of information, and the quality that's coursing through, and whether that's really qualitatively so different. And it's gonna change the world because of that. Do we have time for another question? Great. Any other questions? OK. Well, thank you so much. [APPLAUSE] 