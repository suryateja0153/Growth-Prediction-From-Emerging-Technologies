 the hello everyone we do still have some people coming in keep on coming in but we're going to go ahead and get started in the interest of time I'm going to try and keep the schedule going on time let me introduce myself my name is Katherine Schmidtke I'm in the sourcing group at Facebook and I focus on optical technology strategy and today I'm going to be talking about optical interconnects inside the data center and beyond so the outline of my talk I'm gonna start by explaining why optical interconnects matter otherwise we wouldn't be here talking about it and then I'm going to explain how data center requirements are different data centers are in general a homogenous homogeneous environment and very high volume and then I'm going to talk about some of the different interconnects that are being deployed and I'm going to use a specific example of Facebook data centers something that I know very well so let's start by looking at a facebook data center this is a diagram of our switch fabric it's something that we've been very open about we've shared in a number of blog posts and I've listed the blog post information at the bottom here so that if you're curious you can go and find out more about it the diagram represents the interconnections between the major components of our data center the compute the storage and the switches and you'll notice those colored lines there these are the interconnects that connect all of the switches and the data center together and those are all optical interconnects there it's all run over optical fiber and just to give you an idea of the scale of how much optical fiber there is in a data center there's multiple tens of thousands of kilometers of fiber in a data center that's almost the circumference of the earth in a data center so there's a lot of optics in a data center this is why it's something that we're taking the time to talk about today this is a pic a photograph taken inside one of the data centers I think this was Lilly oh but it doesn't matter they all actually look the same on the inside pretty much and for those of you who know a little bit about optics you'll recognize that those colored cables they're all color-coded that's om3 and om4 multimode fiber and historically our our data centers have all been multimode fiber starting at 10 gig and then 40 gig that's what we needed for the kind of distances that these interconnects we're going to need but that's changing as we move to a hundred gig and then we start looking at what comes next 400 gig terabytes per second this isn't something that's going to be sustainable over multimode fiber so for the past year and a half or so we've been making some very difficult decisions I'm going to share with you the results of all this decision-making processes we're going to be using single mode fiber and I'm going to run through some of the advantages of single mode fiber and what in fact we chose to use we chose duplex single mode fiber it allows us the lowest cost point and the ease of installation just because it's less fiber you're paying for less fiber you're supporting less fiber in the infrastructure and there's a lot of benefit for that so why did we move to single mode single mode as I said before allows you to increase bandwidth beyond just what we're doing today with 40 gig hundred gig but on beyond to 400 gig and terabytes per second all on duplex fiber source single strand in each direction so this gives us a huge future proofing that allows us to depreciate that fiber plant over multiple generations very important so now single-mode fiber why did we think about that soul so long and so hard why not just go do it and you'll see right away it's more expensive so this is the big challenge the hundred gig market cost challenge this graph shows cost per gigabit per second and this is just a the net links so we could look at some of the other links they would be even more expensive I've tried to focus this just on Ethernet and this data comes from a number of different market reports this isn't just Facebook data this is market research just put it together in one graph so that it's easy to first all read today so we chose multimode fiber a 40 gig that's the red line at the bottom you can see it's considerably cheaper we want to use single mode fiber 100 gig we are actually already so is this we got to the punchline already and hope but it's much more expensive at least what we were looking at a year and a half ago it's much more expensive and the blue line is single mode hundred gig and the Green Line is multimode but we really want to have something in our data centers that's future proof and will survive multiple generations so why why is this blue line so expensive a lot of it is it was designed for for example ten kilometer links and there aren't any ten kilometer links inside a data center so what we've been focusing on is finding the set of requirements that's common to a data center environment and then focus just on that and scale back and it makes a huge difference is over a factor of 10 decrease in cost this allows us to then go and use a single mode fiber and to add a much more effective price point so I that we did that by reducing some of the specifications and tailoring it to our particular environments so what was that and this is OCP this is all about sharing so this is what we've chosen to do duplex single mode fiber as I said before the three parameters that we've optimized for our environment are the reach doesn't need to be ten kilometers all of our links are less than 500 meters and this isn't just Facebook data centers this is true of very very many different kinds of data centers so this is something that's common for the whole community so specifying something that's shorter than 500 meters is something very useful for the whole community another parameter that we've tailored to our environment is the operating case temperature most of these links are going inside a building the Facebook owns that we specify all the equipment that goes in there and it can't get too hot inside a data center because real people actually need to go in there every now and again so having an environment where the case temperature is 15 to 55 degrees C is actually something we do anyway why not take advantage of that and specify that from the beginning and since most parameters are temperature dependent this allows us to significantly reduce the parameter space that the product needs to perform over and then the Third Point is the lifetime of the equipment and I'm going to be careful how to phrase this it doesn't mean to say make unreliable parts because that doesn't work either but the expectation that a part has to live for a service life of 20 years it's it's never going to see that so why design for that that end case because it's not going to happen so this mean in practice what this means is there are and some novel packaging techniques some high-volume packaging techniques that you can bring to bear here that are perfectly adequate for this kind of environment so I've called this to the face book optics choice but I really believe this this is something that we will share with the whole community and it applies to very many different situations not just our data centers and the first thing is that you can tailor the performance to the specific data center environment I talked about temperature and reliability the second thing is when networking at a very large scale that means we deploy a lot of pieces so there's very large volume that's supported and behind this platform and then the second point which is subtle we move fast that means that the implementation of this new design needs to happen very quickly and that leads to my second point here and it's a slightly different business model than how things were done before instead of iterating with small volumes and changing the design a little bit and getting a little bit more volume and interesting and improving their design a little bit more it's much more about taking the big innovative step that allows you to deploy high volume from the beginning so it's a slightly different business model to the way things were before but it's it's very important for what we do and then the final point I've told this whole time about what happens inside a data center but I believe that these principles the approach that we're using here applies to outside of the data center - this is a philosophy that we can extend to the wide area network and other places and it's the idea of collaboration and sharing what we're doing so that we can encourage the whole industry to move fast to meet our needs so that's the end of the specific Facebook piece and now we're gonna do a little panel discussion and I'm gonna bring some of my industry colleagues up onto the stage let me introduce them first and then of course this they've gone now anyway so there's a team of and my colleagues from from the industry who are gonna join me up on stage okay but come up guys and I'll read your names as you come up so from Microsoft we have Jamie got it from Equinix we have we have Tirat C key and from Google we have Vijay and he dared me to say his love named for Sara kala okay and they're going to talk through some of the challenges that they're facing and how they're solving them using the same kind of philosophy so I'm going to pass the baton to you off you go Jamie okay so I'm gonna go through some of the field data from the Microsoft data centers what you're gonna see is some alignment with what Katherine said in particular the bet on single mode fiber inside the data centers but first we'll talk about design intent inside the data center and we're talking about Microsoft data centers which are split between Microsoft own properties and leased facilities so our number one use case today is what we call AOC active optical cable for those of you not familiar with the term AOC is a set of two plugs with a permanently fused patch cord between them it allows us a great cost per bit and no fiber cleaning which our data center techs love and it really is plug-and-play now this optic is only good for in room or inside a row so when we get outside a row we start to see alignment with what Kathryn said little differences here though our number two choice is PS m4 and we deploy these in high volume today PS m4 is a plug that runs over parallel single mode fiber so it's still single mode fiber but uses eight lanes of an MTP 12 connector so we fit our Microsoft data centers with MTV twelve trunk cables and we've cartridges the end with MTP connectors and we use PSM plugs and we believe that a 40 gig and today that is still the most cost-effective way to connect between rows and and a room and even between rooms of the datacenter now CWDM 4 is a very interesting newcomer to the space and for exactly the reasons that Catherine mentioned the typical specs of an LC duplex plug in the past made it a very expensive part at 10 kilometers etc CW m4 has relaxed specs and there's even a CWM 4 light variant now that has even further relaxed specs and it's getting to the point where it's cost competitive with PSN so CW m4 is is something that we're keeping our eye on we don't deploy it in high volume today but it is a part that gives us some flexibility in the future for example if we have fiber constraint in fiber troughs and we can't run MTP 12 trunks to use hundred gate connections we can actually get a 6 to 1 fiber efficiency by cartridge in the ends with LC duplex and running 6e WMS over 1m TP 12 versus 1 in 1 PS m / 1m tv12 so CWDM is an interesting part for the future and it will give us some flexibility to reuse old infrastructure and perhaps reuse fiber conduit and and help alleviate fiber constraint and last on the list here is lr 4 and and you know it is really last for a reason we don't deploy lr 4 in high volume whatsoever when we do deploy lr 4 it is lr 4 light lr 4 light is typically used for Interop with CF v 2 or CF p or other legacy infrastructure the lr 4 distance of 10 kilometres just isn't realistic for Microsoft by the time you're you're reaching 10 kilometers you're outside of the data center campus which means you're into the carrier metro fiber infrastructure and in that case you're usually thinking WDM so the 10 kilometer lr 4 plug it is not a big player in the data center so on to the data the top line here which is 1% today increasing to 5% is the duplex single mode connections in our data center so the 1% in 2015 was dominated by lr4 and for reasons I mentioned before in not a huge use case now because of CWM 4 we're seeing a five-fold increase in the use of LC duplex just in the first few months of 2016 and this is because CW m4 is getting cost competitive with the parallel single mode and we have projections where this could get even higher again CW m4 is is an interesting part still at 5% not a huge part of the infrastructure but 5% of our volume is still a lot of plugs so you know there's significant volume there and it is our fastest growing part in the ecosystem down a little further you see the parallel single mode continues to be our number one use case so that's PS M over m TP is going up to 53% in the data center so that is a single mode infrastructure now important to note that the PSM trunk cable the MTP 12 can be recaptured and used with LC duplex if in the future we find there's cost benefit to doing so so our MTP 12 trunks are single mode we can use them LC duplex or PSM so we feel that we future proofed our data center for the next 20 plus years by running our MTP 12 trunks and at the bottom exactly what Kathryn said we have 24% of the ecosystem in 2015 was multimode and we're aggressively eliminating the multimode use case and this year we expect it to come out less than 1% of the data center ecosystem thank you very much Jamie yeah hi Equinix joined LCP recently we joined the tip project as well and we're working very closely with Facebook and others on bringing the OCP innovation into our data centers our data centers are a little different which is it's a multi-tenant data center it looks like a mall I'm just trying to simplify it we have a lot of tenants who come in and the value there is not just the high quality space and power and colocation but also the interconnection is the most critical part that people come for they come for who can they connect to and this is a quick diagram of our what our data centers look like you have which that we also use single mode fiber I'll talk about and we have a massive number of fiber cost connects all the connections inside the data centers requires some kind of fiber from people's cages too from one to the next and so I'm gonna go to the next slide here and our challenge has been congestion on fibre the the amount of cross connect and their magnetic power of the cloud and the networks is so significant especially the last three years we've seen a significant explosion of the cloud both infrastructure as a service and software as a service deployments inside Equinix and hundreds of customers are now connecting to them directly with massive amount of fiber so these magnets start to look like the old central offices in terms of volume so what we use today is single-mode fiber and you can see a picture of one of the trays these things are starting to get filled up so the first technology plan we're going to still staying with single mode fiber but we're going with high density cabling to and high-density LC LC patch panels to solve the issue of density but our long-term solution is OCP under the OCP project is that we really want to bring in automated optical switching and also Odom CDC technology even beyond their data center so that we can give the control to our customers to activate between each other and what we want to do is collaborate with the community everybody here on this panel to figure out how we build that kind of open-source system using OCP optical and allowed that to happen all the way into everybody's cage and that would be really critical in the future when containers can be spun up in milliseconds so you want to be able to activate the capacity and the data as soon as possible I'm gonna give that clicker here next thank you so for my portion of the panel presentation I'm going to talk about the van network so we saw some of the discussion around what happens within the data center so as you know the data from the data center there it needs to be connected across the Metro to appearing pop and it needs to connect get connected across a long-haul network to other data centers and some remote pops as well so the van portion of the network is super critical and we focus a lot on that portion as well and what I'm going to talk about here is an architectural vision for how we see the van Network evolving I'm going to talk about both the data plane building blocks as well as the management plane building blocks at a high level if you look at the picture you'll see that the way we are looking at these there is a decoupled decoupling between the terminal optics and the terminal optics here are referring to the DWDM transponders so you essentially have a stack of DWDM transponders that provide your transport capability and these are enshrined in boxes that you may have heard of be referred to as the data center interconnect or the DCI boxes so DCI boxes have come on very strong they're in a server like form factor very compact very power efficient and extremely effective at providing the data center interconnect capability with DWDM optics in it and the DCI optics have to run over a line system and the line system depending on the distance that you want to cover it's a set of amplifiers some access some Equalization elements etc so the key part of this vision here is a true B coupling between the terminal optics and the line system so you deploy the line system and then you pick the best of breed of your terminal optics and they run over each other in a completely independent fashion so that is the data plane aspect of it and the other one that wanted to highlight here is a transport Sdn based configuration and management and traditionally all these systems were managed by vendor provided EMSs and they tend to be very siloed and it's very difficult to stitch different vendors equipment together to have an end-to-end intent-based system you can bypass that and you can have a direct configuration to both your terminal optics and your line system and here we've indicated that the collaboration that we are looking for is an open conflict so it started out as a router switch effort and that's been extended to transport and line systems as well so the draft models are already on github so you can take a look at this URL the open config has the draft models for the DWM transport side as well so just to sum up the idea is to have a decoupled best-of-breed building blocks for terminals line systems and all managed with an open config and the value that open config brings to the table is its vendor-neutral and it extends the concept of a configuration from a server to a router to a transport element and that's basically I just want to wrap up there great thank you very much ok I'm miked thank you very much everyone so I wanted to thank the the whole team my colleagues for industry for coming together here today and sharing some of the common challenges that we all face sharing that with you and I'd like to encourage everyone this is all about community building and together we can really accelerate the pace of innovation and we can move the industry forward together thank you very much thank you 