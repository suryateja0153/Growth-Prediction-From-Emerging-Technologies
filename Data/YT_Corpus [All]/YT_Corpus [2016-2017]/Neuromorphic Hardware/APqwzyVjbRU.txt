 okay after introducing the human brain project this morning more from an administrative perspective let me now spend 15 to 20 minutes on telling you what we really do in terms of hardware development those this talk will be substantiated by a few other contributions over the next coming days and particular by Steve Ferber and Johanna shemal i will give you kind of an overview and if you want to see the real thing of course they're also their most over there as you may have seen already HPP is centered around what we called platforms those are computing infrastructures that are useful for neuroscience I would say and you see there are the things you would expect like making available neuroscience data medical data running simulations but then the last three four five and six or in the internal counting scheme there seven nine and ten actually strange ordering this is high performance computing so the goal is to use the best computing traditional computing that we have to run brain simulations there is robotics which is essential is basically the data that we expose our brain simulation to and there is neuromorphic computing and of course the question is being asked all the time why do we really need your morphic computing if we have supercomputers in simulations of robots what's the role or neuromorphic computing we know it's probably a key technology for cognitive computing for analyzing data but is it really useful for neuroscience can neuromorphic computing advanced neuroscience or is it rather so that neuromorphic computing always desperately follows neuroscience and try to grasp some aspects which are useful for cognitive computing well in HB p of course if you want to survive an HP p which is not easy i can tell you you have to you have to make sure that you contribute to neuroscience actively the progress of understanding the brain so how can I say that with these silly little chips how we can we ever contribute to understanding the brain I mean any detailed simulation must be much better any particulars of course gathering more data that's the way neuroscience will advance well I will give you an answer or an argument why maybe that's not quite true this is a you have seen plots like that there's computing power here on the horizontal axis on the logarithmic scale from gigaton XO flops and there's memory requirement to store the brain structure that you want to simulate from megabytes to almost an exabyte well as 100 petabytes there so if you want to simulate a single cell in the Henry Markram style with quite some substructure subcellular sub structure you need a Giga flop of of computing power which is less than a cell phone and a megabyte of memory so that's very easy to do now you can scale that up easily you can go to cortical columns what Henry has done in the past that needs a teraflop and terabyte also that is easy now people do that routinely we can go to probably to read brains rodent brains which requires a bit more than a petaflop we should state of the art of supercomputing and then then move up to the brain the human brain is very easy you just end up at AXA flop so that's an argument for a nexus scaling machine of course you have to pay for the memory which may be harder you need at least 100 petabytes of memory to store the structure so people work a lot on how to store this kind of structure with with layered memory and things like that but is this really useful that's an interesting question because if you see what people do for example simulating the cortical column you see these wonderful pictures of flashing neurons but if you really really analyze what people do they analyze a couple of hundred milliseconds of life of this network why is that because the supercomputer simulations typically run a factor thousand slower than biology okay so to route or to simulate a day of life of a simulated brain you need a thousand days or the result which is the links of PhD theses and so it's it's not really accessible and things are much worse even in that is if you know that there are other things here of course there's much more structure in biology this is the black stuff her glial cells and you can even think of having molecular dynamics which adds a factor of what is it 1 billion or so and that is totally inaccessible there is no way with any computing that we have in the future that we can do that for the entire brain of course you can try to do this multi scale and this is what people do but there is a more interesting thing for for me and for us I think this is the dynamics of the system there's plasticity learning and development and and so if you want to study development all right like which happens on the minute our day maybe even weeks or years ago you need a factor thousand or so on top of the excess game and and that is not there so my claim is it's very very hard maybe impossible to really study this kind of full brain simulations and the dynamics the learning and the developmental processes so what we really need is if you want to run on subcellular detail and plasticity really require advances in what I would call strong scaling so that means for a given complexity of the problem you just you cannot pay for it by adding more processors but you really need faster processors that can solve a given problem in a shorter time and and and he is probably the most important slide that I I want to show that I mean both for neuroscience and cognitive computing this is my claim and I guess most of you will probably agree the only way to ever make artificial neural simulation circuits derived from biology useful is to make them adaptive now they have to change and you have to follow their time dynamics in terms of connectivity that means structure in terms of synapses whether they are there how strong they are and also in terms of neurons neuroses also neuronal plasticity and you have to do this by closed-loop interaction with the data these systems have to interact with the data to learn as long as you haven't seemed you in an isolated complex brain simulation you will probably observe funny things which look good on computer screens but I guess there's not much you learn from it you really have to make these systems adaptive they have to develop in time the time development is the most important power the computer ads on top of pure connect comics projects if you just why make a map of the connections that's great but it doesn't change if you want changes the only way to get that under control is to run a computer at a reasonable time scale so that to me is the argument for new computer architectures for neuroscience of course this also then applies or cognitive computing now that the other important message is it takes time to build chips it's probably fast ability chip but you have to make it work and and it's not only making it work but it needs all the infrastructure like for example the software infrastructure configure it and then you need infrastructure to a software to analyze the data and to interpret it and that is a very slow process so what I'm going to show about the human brain project what you see there of course is not at the moment the result of human brain project because your own brain project is only 30 months old now there's no way you can do that in 30 months so the chips that we use think we call now the brain scale system and the spinnaker system they are not developed in HB p I mean they are coming from previous projects like the facets brain skates project which started in 2005 and the spinnaker project which studying in 2005 I means you need at least currently eight to ten years from chip designed system requires a roadmap and sustained funding now you may say well ok that's it's typically slow inefficient university work and you can do that much faster if a powerful company comes into play and this is IBM and we have seen that from the mandrels beautiful talk this is the right day that I mean you cannot really see this but this is the how the true no system was developed and amazingly they started in 2005 or 2000 for something like that okay so we all started at the same time wait and we didn't know about each other I didn't know Steve phurba I didn't know Dom ann'dra in those days we were starting independently basically magically all in the same here very much pretty much and now we are there that we have systems not run that means for HBP if you want the next generation which we definitely want we better have a roadmap today if you want to be ready in 20 23 which is the end of the HPP project and so this is what we what we want to do in HPP we have the current system we scale them up to a reasonable size so that you can do interesting experiments and we are kind of complementary at the brain scared system spinnaker system in the true north system but the next generation is really not each p were genuine HBP work and i will tell you what what we plan to do just to remind everybody and of course Steve will give you a much more competent and detailed introduction this is the spinnaker chip which is an 18 18 arm core ownership it has integer arithmetic it runs at a relatively low clock speed there's a ramen died and the drm stack on the die and the most important thing is these router in the middle which provides bi-directional links six per chip and and these are really the powerful aspect of the spinnaker system this is six million spikes per second per link can be transmitted in small packets that are optimized really force by transmission and so what you're really have here is a programmable still programmable with standard programming tools but of course you have to take the architecture into account but it's a real-time simulator ok so a second is the second in the day is a day which is wonderful because that's already a big step forward from the fact ourselves and slow down and it's in particular use for lots of a robotics if you want to talk to real sensors and robots then this is exactly what you need and and and this of course scales very nicely the spinnaker people they they they offer these different options there is even a smaller one they're called 103 104 and 105 which of course should really be one 10 to the 3 10 to the 4 10 to the 5 and this is this is then the number of course in the system thousand ten thousand and a hundred thousand it is you will see the system the big system that's currently available has five of those wrecks of 100 thousand processors their brain scale system is what we call a physical model system so it's really modeled after the brain very closely and what what really the brain is a mixed signal system it has local computing if you want to call it like that on the synoptic and the neuron level but then the transmission of information the action potentials the pulse I'd probably doesn't carry much information so this is typically binary and we just copied dogs okay so rather just replacing ions by electrons so it's a mixed signal system is the idea is driven by architecture we at the moment we don't care about devices so we do not embark on memory stir or other things but we really use rather boring old-fashioned 180 meter underneath the CMOS in the past we are changing to 65 but that was what we did in the past essential is biological fidelity a high near on input count is extremely important more than 10,000 synaptic inputs configurability is key because we don't really know what a good theory is what a good network is what good parameters are so we better make these systems both of them's vinegar and brain skates very very very configurable so in a way it's a bit like like like von Neumann and Oppenheimer when they build these first machines they try to make them Universal computers although they were really optimized for your neutron diffusion but soon they discovered that they can do simulations or weather and things like that so and we try to do this as universal as possible scalability the acceleration factor is important this system by choosing the parameters of the analog circuits and the necessary delays and time constants we can run it faster than biology that has disadvantages and advantages the disadvantage is that you cannot talk really reasonably whether two robots anymore but of course it's very good if you want to study this time evolution so a day here is not a thousand days another day but it's more like 10 seconds in order to explore that or exploit that you need long and short terms plasticity synaptic plasticity which is which is there and they're also other ways to reconfigure the chip and for the future we were in particular working on that aspect there are other things here non expert user access is a very important on the objective is to exploit the configurability and the acceleration to do rapid exploration of parameter spaces cover short and long timescales in circuit dynamics and also do computing in the presence of noise just a few pictures these chips all look the same basically they but they look different from spinning as you can see this is a mixed signal chip it looks like a tennis court okay with the two fields those are the synopsis typically 50,000 synopsis on each side in the middle are a few neurons the neurons are in terms of real estate of silicon almost invisible because there are so few of the ratio is 10,000 to 1 so there are a couple of hundred neurons there and then all the rest is communication this is a single chip on a board you can see setups like that in the demonstration room yes what designer is what they are or where they are what they are the synapses are systems that connect neurons and they have weights with a precision of four bits the four bits are stored on local SRAM cells and they have certain plasticity mechanisms implemented like short-term plasticity a short term depression in facilitation and also HTTP the HTTP timing measurement is done by an analog circuit and then is evaluated by a digital lookup table so that you get these typical SCDP curves which then give rise to a shorter a long-term plasticity okay so this is a photo of course now with this acceleration factor communication BTW between chips is an issue and we decided very early on and to go what we call wafer scale integration not to cut the waiver into pieces this is one waiver it contains about 4 to 500 450 chips so we leave the wafer intact and we develop this kind of infrastructure there where we have a waiver that contains now 200,000 neurons and fifty million synopsis of the same type neurons are addicts type neurons two-dimensional neuron water and then there is all this infrastructure there for power supply and input output digital input output through these FPGA boards which you see there this looks very very complicated this actually there's a lot of technology be developed for example you have to bring power into the system you have to bring data and you have to bring data out but we worked on that for the last ten years and then four at the beginning of HBP we both spinnaker group and we we used our computers to generate computer models of the systems that we want to build this is the computer model that was developed in in drawing in Manchester this is the one that was drawn in Heidelberg and the dream was that we would now scale this up was NH PP in 30 months to a large scale system 500,000 ARM processors in in manchester & 4 million synopsis and this would actually be a billiard 20 times 50 million signatures it's the same mistake as on the poster I have to fix this on and the Heidelberg side so as I said these are computer drawings so that's easy to do but the great thing is that these things exist now okay I mean at this very moment we just finished and we have to deliver it in 20 days basically to the European Commission this is a picture taken in Manchester you see the five wrecks there was the 500,000 course and of course Steve Ferber will tell you details about it this is an equivalent in in Heidelberg it's a small room so we have to use a fisheye lens to somehow put this on the on the screen your handles will talk about it and these are the systems that are made available now so who will use this of course the people that are next to the system is very easy but even if you're not next to the system you will be able to use it an NH PP there is what we call collaboratory it's a horrible word but what you can do is from outside you can log into those system you can't get in get an account and then use it and the good thing is that we also have a description language for networks and that also has been developed in one of our previous project and facets it's called pine which you may know and that's sort of the collector common language for both of the systems as it is for many software simulators and probably also other hardware implementations and then of course there's the low level software which is specific for the physical model system and the many call system in Manchester which is a mapping process pretty much like on an FPGA for the physical model system you have to decide where to put the neurons where to put the synopsis on the way for how to connect them and it's the Pegman software for the for the Manchester system so this is already and today you can log into those systems and submit jobs if you want to learn more I mean this is a very short talk there is a very nice documentation I'm actually very proud of this this is an awkward URL I'm sorry for that but what you should do is to google yo morphic guidebook and it will be right on top okay and what you get is this list here it's a it's constant constantly being updated and it's a very comprehensive documentation of the system of the hardware the system's the firmware a loyal high level software it contains benchmarks for neuroscience and machine learning even tutorials okay so if you want to start using this for example there also these small systems one you see over there all my key systems and there's what we call a spiky school so if you want to get your students involved for example you can get one of these things you see the real thing over there here you can even look inside and that's one of the older chips by the way but it's still very nice if you want to get involved in mixed signal and along your a morphic computing get one of these things or get an account you can also log in from outside if you don't want to invest 2,000 euros to get one of those but if you have it there's all the software support you have to run experiments and there is this school which starts from little experiments with single neurons and then goes to larger networks it explores plasticity and then does some fancy applications like the one Thomas Novotny shot this morning for example on on multivariate data analysis so this is very nice these small systems are available as we have seen from through north they available from spinnaker and they are available from the brain scale system so I think that's a very good way to get students involved and usually they love it a lot now there are lots of use cases and I have no time and to describe them here because I want to talk about the future a little bit I I dared to put these two gentlemen there on this slide probably know them it's a trans on women and robert Oppenheimer in front of their strange machine which looks almost as strange as ours and is about the same size so I mean this kind of demonstrates universality to some extent so these all experiments we either have been done or we are in the process of preparing and you see there are abstract things like for example liquid computing balance random networks stochastic inference and things like that but also things which are closer to biology like many columns of this phase detection which is the echolocation of birds even applying SCDP so these are very very different experiments and they are all being done at the moment and the cool things of course at the moment and again you can talk to people over there as a poster on implementing deep learning with spiking neurons posted by lucy bai ling me i will talk about stochastic computing two more and we are even making an effort to implement HTM with spiking neurons of course that's far from being ready but but so we are really trying to explore and exploit the universality very shortly on the roadmap as i said if you want to have the next generation ready by 2023 we have to know basically today what we want to do and that we have done we have developed the roadmap it's in the document which I introduced this morning so very shortly spinnaker to spinnaker to the basic idea is to get affected ten improvement on many many parameters like the number of neurons per chip going from 16 Kate or 120 number of synapses from 60 million to 130 million energy percent uptick event going down by a factor 10 and all this at constant power the power will remain to be one word for a chip and the number of course goes from 18 to 68 so this is the plan of spinnaker is well underway we have a road map with all the tests chips to be developed and so in the next phase we are going to have little boards so like we have little boards of the current systems we will have little boards of the next generation ships the spinnaker too and also the new brain scans chip we do not have the funding I have to say to build the large-scale systems at the moment this will not come from the human brain project because you're removing computing is I also me this kind of a sideline of the projects currently we are not funded for that but provided that we show good results which are seeing we do I'm sure that we will find a way to do this on the physical model in principle there are two directions which we would like to follow one in structure neurons that was a big topic nonlinearities activeden droids dendritic structure dendritic branching active dendrites means also back propagating action potentials neurons is metal detectors for example for HTML occasions there is a test chip that we have that works very nicely but again we don't have the funding to implement that into the large system at the moment at the moment we are mostly focusing in the next generation schiphol is already there's a prototype again over there you see it ask eric miller you show it to you this is going to small and feature size of 65 nanometer and also has some other aspect but maybe the most important thing is i talked about local learning this morning and this is an implementation of local learning so we have these several hundred ships on a wafer like 500 or so and each of these chips will know every little process of a PowerPC processor it will be 400 on the chip and that can do it has it has access to all the activity on the way with the synaptic weights for example the STD be measurement neuron parameters and it can change them by local algorithms it can for example also do rewiring on the fly which is structural plasticity it can be data-driven so really activity or external data driven development and it can emulate all the slow and fast circuit dynamics by slow by fast I mean of course the typical synaptic change in biology on the millisecond 100 micro second level but of course you can now also to slow things which otherwise is not so easy on your a morphic systems like things that happens in minutes or days or years or whatever but all in the accelerated mode so i we consider this a very very important development we will finally bring to the physical model system this kind of of local learning and this is a cartoon drone with your old ship the new one looks different but it means there's this concept of having observables and controls and the observables can be either internal like soon ups evaluation the population rates or any arbitrary internal parameters like neuron parameters for example this can be given to the other processor and it then calculates the control and the control can be a change of the weight it can change the neuron parameters for example implementing things like homeostasis it can trigger stimulus generators or very importantly it can change the connectivity of the network of course the processor can also receive external signals like reward signals or any other control sequence that you might want to send into the system so that is in and I said there is a test ship available which does it is already on a much smaller scale but it has already the full processor so ask eric miller and rose Johannes who is behind the design of this if you want to know details I will close by showing this thing here which is often forgotten I mean technology of course it's an important thing and if you want to build large-scale networks and scale them up now what we do is to build this thing which you see at the right upper corner which are very beautiful but also very complex modules it takes a week or so to assemble them because you have to take the board and you put the wafer on your third line you have to make the contact and you put all the control boards on and we are now working with fraunhofer which is a technology research center in Germany in Berlin the one working on connection technology and they now do a cool thing they laminate our wafers into the printed circuit board you know a printed circuit board is a stack of like 10 or 14 layers and so they send away for down and it will be part of the layer structure and that means we hope by that we get a much more robust in simple structure which is then easy to scale up to larger systems and also mean this is not a cartoon this is really real life and they have some very nice progress and already and i will close by showing you this again if you want to know more go to the guidebook or join us at our workshop on the twenty-second well I mean of course you would typically I guess read data in in a parallel way not not only if your big Network you would use all waivers in parallel to read data in and I would say I mean there are we did closed loop experiments actually Eric Mueller was here did closed loop experiments and we were able to read data in and out on a scale that in biology corresponds to the typical rate you get from visual data for example of course then accelerated okay to discussion 