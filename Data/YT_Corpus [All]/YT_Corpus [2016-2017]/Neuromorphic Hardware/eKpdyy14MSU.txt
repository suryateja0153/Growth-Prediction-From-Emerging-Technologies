 so I'm not very pleased to have as our speaker Thomas Edgar who's professor of chemical engineering and the able chair of engineering at the University of Texas at Austin he received his PhD from Princeton University and he's had a very wide level of professional service throughout his career including president of a square C square he's done research for quite a few companies a most heavily in the micro electronics field in the last last 15 or 20 years and that includes advanced micro devices motorola texas instruments and the SEMATECH microelectronics consortium and he has a long list of honours and to give him time to speak what will just mention is that that includes the election to the National Academy engineering and I look forward to and I'm sure all of us look forward to a hearing this talk and let's give them a warm welcome okay well thanks Richard you know the title the talk says new directions that we are going to talk about new directions along the way but I think it's important for people to understand you know about the past and where things have gone and why we've gotten to where we are today so we'll be engaging a little bit of a historical review and not too much revisionist history along the way hopefully so here's the outline of the talk we're going to again try to help set the the background in stage since many of you may not be involved in this field in fact I would say that the number of really hardcore control and modeling people who are in this field is really less than 10 in terms of people who might be at this meeting so I susumu many of you are here to sort of understand what this field is about and where it may be heading in the future you know equipment control is paramount and we'll talk about the nature of control and the kind of processes that are used today in semiconductor manufacturing and especially because it's a batch process and that makes it have some rather unusual characteristics we'll talk also about why APC was has been readily accepted by the semiconductor industry and what value it brings then we'll get into the number of different examples of you know where essentially advance control is applied then I should also mention that I use the word APC you know some people APC stands for automatic process control in the semiconductor industry they think all controls advance so they say advanced process control but in any event will talk about the controller performance monitoring and then the other part of i think this this area makes it somewhat unique from the control perspective is the idea of factory control where you've got lots of pieces of equipment it is a very highly complex set of operations that go on there and controls part of that but like i said control had faces a lot of adversities and then at the end we'll talk about some new directions as well so here's some history that gives you an idea about how a pc came into the semiconductor industry probably one of the signal events that occurred back in the 80s was the formation of an organization called sematech which was really founded to combat the semiconductor activities especially in Japan and so how can the u.s. compete and so they view this as a defense imperative that you know we need to be able to support this industry so don't rely on foreign suppliers for chips for this for our defense industry so they put a bunch of money into it they said only US companies can be involved and they actually started on this idea of technology roadmaps which is I think a rather you know unusual part of what makes this industry rather interesting then seemed like they worry too much about control of equipment you know you would basically buy equipment from a vendor install in your plant hope it works and of course you know they had their own software in their their own ways of hooking up to you know the rest of the process in fact some of the equipment may have been built in garages in Southern California for all I know so it was really a lots of different suppliers and inventors but then we started seeing some order out of the chaos and they start having workshops on advanced equipment control how do we do a better job of controlling the various unit operations that are in semiconductor manufacturing etc then the the wind kind of went out of the you know protect the national industry and 96 it became an international organization so companies from outside the US were welcomed in and so then they expanded the road map activity to include those companies and that's called the itrs and then most recently sematech moved to New York out of Austin Texas many of the Texas were sad to see it go and I see you can see there I say say end of life well unfortunately what's happening is that a lot of those maine state companies like Samsung and Intel have now just in the last few months bailed out a SEMATECH and so it's pretty it's very unclear where things are headed right now with that organization in fact someone said yeah we're now looking at I mech and Europe is the way we're going to get things done so unclear where this is going to go but we still have these conferences that are held around the world called aec APC advanced equipment control advanced process control and so you see here essentially three different conferences that developed over time the original one is down there at the bottom the one it's held in this case an awesome is held in the US and that's now as you count if you do the Roman numerals there it's a 27th one okay so they've been doing this now since 1998 so that's pretty impressive along the way Asia decided they need to have their own conference you see the APC a cpa pc asia and then Europe has its own one and they've done 15 of them so there is a lot of activity around this whole topic and again I'm not going to ask for a show of hands as how many people ever been to one of these conferences but they are you know they have their own little communities that operate I mentioned roadmaps so here's been the thing that's been so impressive I think about the whole semiconductor operation and SEMATECH especially is that they basically and this is one road map from 2004 so you can get an idea about where they think they are and where they're headed and if you just look at one of the the lines that you see how you know the nanometers there they keep pushing down the size to very small small size IE night nanometers and and so what they're doing is really darat driving what the devices are going to look like in the future and what the device is going to look like in the future is there going to be smaller so we all know this ourselves because you know we buy smartphones we buy other other things and everything has gotten much more compact etc hopefully using less energy as well but and a lot of this has become the companies come together they get their experts they figure out where things are headed and then they figure out what what are the technologies that need to be implemented in order to get there and in the end it's really the consumer this benefit and its really the consumer who's driving this whole thing because you know we want smaller devices and so and and we're willing to pay for them and so the companies then compete you know about who has the best phone or the best other kind of you know best the laptop etc so this is again something you don't see in other industries it's a very interesting characteristic but again as you can see we're pushing things down to you know very small scale and that again is one of the challenges of doing semiconductor manufacturing this is the other part that you have to understand about a fab is that it's a whole sequence of unit operations I just listed some representative ones here involving chemical vapour deposition ion implantation oxidation you get another CBD round you do mask to essentially create surface where you can carry out etching and other activities and so on so it's it's really here you see you know maybe 10 different unit operations well actually in some fabs you have up to several thousand process steps as they build up these layers of material and that might involve to 200 unique processes and so again the complexity of what's going on here is just staggering and it's wonder that they can actually make the devices is that they intend to make it's also helpful to understand you know how this may be different from so many people may you know be more familiar with process control and in the continuous industry you got refining and chemicals and you know we have different objectives in the semiconductor industry I mean first of all it's largely batch and discrete rather than continuous your your primary objective is to end up with a device that has the right electrical properties but then you're going to try to control things like line with and critical dimension and then for economics you're trying to maximize yield and minimize cycle time so you can essentially make the largest number of good chips and fast as possible so you essentially use the equipment that that you have their most effectively the other problem is that once you make something it's hard to unmake it ok or rework it in other words in refining chemical industry they have lots of these things called storage tanks where they can say oh well we missed the target on this one lets blend it with something else well so that's a luxury that you don't have in the semiconductor industry and you know there's lots of measurements in fact there's no no data famine that seems to be appearing here there's plenty of data being produced so when you look at the different unit operations and we'll talk about a few of them specifically you know what's their characteristics well they're very physically and chemically complex they are you know in the end either physical or chemical processes there's a lot of things we can't measure in fact very rarely can we measure what's going on in the middle of one of these unit operations we have to wait till the end of the unit operation very sensitive to process changes nonlinear processes there they all tend to be multiple input multiple output and the other thing is that you know the people who design the chips are not that people operate the plants so as a result you know there's sometimes a mismatch someone says oh yeah this will be a great chip that we can have certain characteristics and then the guise of manufacturing can't figure out how to make it so or operate successfully so those are some of the challenges you face and this just gives you an idea of all different things on in a series of unit operations that could be applied to make a final product and pattern transfer you can see a list of all the different variables I don't tend to really talk about any of them particular here but again it's a very complex multi variable process that requires a lot of attention to details and this gives you an idea about time scales in a typical fab I mean we've got some things happening in real time you know less than a second but you can do fault detection and that can be done over longer periods of time we have something called run to run control our to our you'll hear more about that later that can be spanning over minutes two weeks even so again you know you can hack actually have from the time a wafer goes into a fab until finally when the bear wafer goes in it might be six weeks before that product actually comes out after it goes through all the steps so it's a very again from a scheduling standpoint and Operations standpoint it's it's very complex and we can worry about other things like you know do we control each wafer do we have a bunch of wafers and try to control representative wafers that we can sample but again the big challenge is it's hard to get effort get data in the middle of the operation you almost have to wait most the time till the end so again if you go back and think about this is in a general batch processing hierarchy they have all the things that you would have in a batch chemical process I want to highlight here steps 3 and 4 which is one call within the batch control which means that we actually try to control what's happening during the run and then finally the the mainstay of semiconductor process control is really run to run control in order to meet the quality constraints and again we'll get into that more detail so again some some of the characteristics you know we have to satisfy the final product quality at each step along the way and you know if you do the math on if you're going to have 200 sequential steps and you're going to have ninety-nine percent yield at each step and then you multiply those out nine point 9 the 200 power you end up with fairly low yield overall so that's the challenge every step has to be done very well the other problem is it because you have a different industry that provides the tools plasma etching CBD etc to the manufacturers then they have to figure out how they combine those pieces of equipment with the way they operate their fab and the problem is usually can't modify what the hardware and software and the tool is and that creates a big problems and so that again is another driver for why people have gone to run to run control the other part of this is that it's very complicated operations that can involve both chemical steps and also mechanical steps and a couple of examples will be lithography CMP chemical mechanical polishing or planarization and this just gives you an idea again there's not much going on here but just that they have these tools they have robotics to bring the wafer n they operate on it and then the robotics take the wafer out again under a very highly controlled environment because they can't have particulate matter in there it used to be that you know you have these gigantic fabs and lots of people running around what we call bunny suits and you had to control particular matter and it was just a very difficult situation so the industry has gone more towards you know let's control what goes on in the tool have a lot fewer people operating and in fact let's move everything around with robotics instead of you know having people moving these things around and that's been largely implemented the big fabs today some other things we can do with batch processing that makes it a little bit different is it that sometimes when things aren't going well in the batch we actually can adjust the amount of time you use for a certain process like you know you can decide well i'm going to edge this one maybe a little longer this time so you know we can take advantage the fact that this is a batch process again there's lots of challenges from within the batch control i'm going to talk about a couple of examples of where this is actually useful again you know a whole problem of modeling it nonlinear behavior we don't have sensors that are going to do the job we need during the batch etc lots of disturbances behavior that's irreversible and you know in the 90s basically we had multi wafer reactors that we carried a lot of the steps out on this is a the top plot there is a LP CVD low pressure chemical vapor deposition reactor they would put anywhere from you know 550 to 200 wafers in there and then basically essentially deposit silicon poly silicon for example on the wafers and you know this worked pretty well as long as you had only four inch wafers once they went the six inch wafers they started finding that that the uniformity of the wafer was becoming a problem because of the nature of the heat transfer in the furnace and so gradually the industry moved towards single wafer control so they could actually do a much better job of controlling what happened on an individual wafer and you know the way for size has actually grown over time you know in later after the they had four inch wafers and with the six inch wafers then 8 inch then 12 inch which is currently pretty much the max size although interestingly enough there are lots of 8-inch fabs out there then processing units that people use they can make profitable product and someone said they've all been depreciated so they actually have a low cost of ownership as well but basically this is a way in which we get much better control what happens on the wafer you know there's talk about going to 18 inch wafers or 450 millimeter wafers that one is unclear is I couldn't get my talk to people in the industry about whether they really think that's going to happen it's a very expensive proposition to talk about now also in changing your whole fab to a new generation of equipment so again there's some big economic barriers to doing that so here's an example of what you try to do if you're going to now take those 200 wafers you used to run all at one time for an hour and a half now we're going to try to speed it up because the point is I can't take a long time to run each wafer one at a time I've got to try to speed it up by running at higher temperature and so we do what's called rapid thermal processing and this is something that's familiar to most of you in the room about you know we have a setpoint trajectory we like to be able to follow it and we have this black area they're called the bring in area that essentially our goal is to minimize that so we'd like to track the setpoint as closely as possible you might say well yeah once you just over shoot it well if you overshoot then you hit start damaging the material and it doesn't turn out to be high-quality so we've got to actually bring it in as fast as possible with as little a little overshoot as possible so here would be a typical RTP reactor that's got these lamps that are radiating heat on the reactor you can kind of see in the picture here there's some maybes concentric zones of lamps there and so you know here would be a sensitive system that we actually worked with the Texas Instruments where you can see we've got like four zones that we can independently control the heating from and then we've got a wafer down there in this case we actually had thermocouples that we mailed in the wafer of course you can't do that for actual practical operation but again in the early days you're trying to show that well we really need multivariable control in order to successfully control a reactor like this and in fact and this was part of a DoD program in the early days of semiconductor manufacturing 1993 and you know the student I worked with that basically developed a nonlinear heat transfer driven model and because it's radiation you know you have t to the fourth terms in there so clearly nonlinearities that worked there and you know we'd like to be able to control this thing in real time over a space of you know 10 20 seconds so you know having you know use some of the ideas of model predictive control or DMC formulated as a successive linearization type of problem and then here are the results we got you can see that you know if we're trying to ramp this system up from 400 up to 800 degrees C you can see that we do a pretty good job of being able to get all four of these thermocouples to track the desired profile again a little slower than maybe we wanted but anyway we were able to show you could do it and do it in real time because you know the computer is basically cranking out new new models as you go along as you go up to the higher temperatures so this is an example of what you can do and again this is a sort of approach that's used today you know there's a lot of examples of model predictive control have been used in the semiconductor industry and I'll say more about that later here's another system that essentially lend itself to model predictive control this is CMP chemical mechanical polishing where you've got these arms that basically pushed down on a polishing pad that sits on top of the way / and this is the way again a reducing instead of etching we actually polish the wafer down depending on the material we're trying to remove this turns out to be with the five arms to be a multivariable control problem as well we have to worry about adjusting each of these arms the arms are rotating around as well so it's again a very complex mechanical process and we did basically the student Jaret Campbell who is at AMD part time in part time at UT basically said you know hey he had learned about model predictive control in classes and he said let's figure out how we can apply it to this problem and so it turned out to Lin itself really well and I think having scoured the patent literature this is probably the first application of model predictive control in the semiconductor industry on a actual commercial tool so again you know we worry about you know how we do this in 20 level we're not going to talk about things like run to run control and that's that's part of what you see here we also have fault detection we have to worry about i'm not going to talk about you about fault detection in this lecture just because of lack of time but there certainly are lots of opportunities to look at that again variation is the name of the game here things are changing all the time we've got tools that behave differently you know we're different lots and they get exposed to different tools at different points in time you know chambers change over time okay what's called seasoning of a chamber that after a while the chamber operates you know what you know consistently for a while but then you know you start getting deposits internally and then it doesn't operate quite the same so keeping track of all this is very hard to do you know I have to decide what I'm going to shut down the system that's expensive to do and so on so those are some things we worry about so I mentioned run to run control will go through this again there's some few equations in here that we can we can talk about the whole idea is because we can't measure during the batch it's hard to do you know that previous example I gave you you know we actually had to put thermocouples on the wafer well that's not feasible in an actual operation so you know we're pretty much stuck with saying this measuring at the end of the run and essentially adjust after every run what we're doing with the process and so it does set up nicely as a summing as a discrete-time process and it is good for handling the kinds of variations of problems we run into and we can do run to run optimization as well and so this is basically the sort of model predictive control or turtle model control viewpoint you know we've got a controller which tends to be a real simple deadbeat controller or we can use MPC ewm a you know the process is going to be fairly simplified i'll show you a typical model that they use for rendering control and then we have an observer that basically has to to filter the the noise out of the system and other things that are changing as well and so people typically use exponential filters or Kalman filtering so here's a standard thing you might look at this and laugh say well that's so simple you know even a caveman can do it that's right you know but again with all the other things changing in the fab you know you actually don't want to have another complexity added to it so this has been extremely successful I'm going to show you some some results that obtained that were obtained just by using run to run control but basically you're saying that you know the model is really simple it's linear at the epicentral at the end of the batch your you compare the measurement see against the target and then calculate a deadbeat controller and then we filter the state update and then when you you know wrap all this together then you basically have an integral controller so we're familiar with PID controllers and certainly the process control world this is essentially an even simpler case but again it's applied on a run to run basis so here is one of the early results actually at AMD where they said okay we've gone from essentially manual operator control and now we're going to use run to run control that's going to be now automated the operator doesn't get to touch anything and along the way and so you see some of the results that they got in terms of improving the performance relative to the target you can see the numbers there are big big improvements seventy percent change in terms of being closer to the target and then of course we the whole goal is to narrow the distribution as well and improve the CPK or process capability so all these things came out of one of these early studies on render and controlled everyone started latching on to it saying okay yeah this is the way in which we can do things like increase yield that's going to improve our economics will be able to reduce the amount of scrap that we make because again those are lost products and you can't sell those will be able to run the equipment more often so and have a higher yield in terms of good wafers and then maybe we run faster and then reduce the cycle time so those are all things that affect the economics and of course in a typical fab you've got a lot of interesting things going on terms or when are you actually making good wafers you know how often are you really sending through pilot wafers to try out a piece of equipment average man shut down that's a cost of course you've got the schedule downtown downtime and unscheduled downtime and then some time is idle so you know again the cost of ownership issue is always with you and when you've got a fab that costs anywhere from 2 billion to 4 billion dollars then people pay a lot of attention to how the how much the equipment is getting utilized here's a good example that shows you know how again why you going to something like run to run control wasn't effective in terms of improving CD control on the gate and as you see the plot there you see the the one with the wider distribution was essentially one that the before they actually implemented improve control and the narrow distribution is the one that's closer to where we want to be we also avoid some of the bad areas of you know where we have a bad device but also as we move that curve to the left and don't move it too far we get higher speed devices I don't some of you remember when you're looking at what prices of computers were back you know 10 15 years ago you would see different megahertz ratings you know and of course the ones with look with our slower you didn't have to pay so much for and you could buy you know more expensive ones if you want to have a faster one and so in a sense this is really the economic proposition that if i can make faster devices i can sell them for more money and my profitability goes up so here just a couple of numbers about what happened in the case they just said that they were able to increase the speed by eight percent reduce the variability and and they greatly reduce the amount of bad ships that they had to rework and when you translate that into you know what does that mean in terms of savings or in terms of increased profit and this is like the capacity argument if i can increase the capacity then i can essentially make a lot more money because i can take all those chips that were bad before now they're good and then sell them and in a competitive business when you know AMD was fighting against intel that was one of their you know standard competitors then when they had an improvement then they could sell a lot of chips and and so this made a big argument 240 million dollars a year as a fairly powerful argument for why you need a bunch of people doing a pc and so that argument was actually successfully used at AMD they hired a whole lot of in fact PhD students from my group and other groups so so that was a good thing that happened now let me shift gears and talk about some other processes that are of interest mentioned plasma etching earlier this slide is really to indicate to you that you know it's a very complex process there are very few physico-chemical models that seem to to work in a plasma system there's so many parameters you don't know and so having a first principles model here has been essentially a an ephemeral goal that no one has really been successful going that way and but we do have lots of measurements from a plasma etching system we've got a number of things we want to control those the performance variables we have a number of manipulated variables that we can adjust in order to get the performance variables to where we want them but then we have lots of disturbances you see all the ones that are listed there lots of things going on and we also have a ton of measurements and so here you get an idea this was actually some work we did with Samsung that we could have as many as 13 hundred different sensors or data points on a plasma echar and the question is okay so how do I take those measurements and then translate them into manipulated variable actions and so sensor selection algorithms is essentially paramount I think we've got a lot of work to do on that one you know that's not a problem that's really been I think we'll solved yet so here's another example it's a resist etch process and the whole idea here is that that again as we try to go to smaller smaller dimensions and you know you lay down a essentially resist that essentially was maybe wider than you want it to be and so they figured out that well we can actually use etching to trim that resist and so you see in the diagram there you know how we might want to reduce it from say 65 nanometers to 40 nanometers and and so we're going to do that chemically by using itch and so I'm going to essentially talk to you about some work we did with Tokyo electron and IBM where they were trying to do wafer to wafer control you know you you were essentially faced with okay we have to measure information about the waiver coming in and if you notice that they're saying that the slide here says FF which stands for feed-forward so typically in the industry they viewed things coming in as being the feed-forward information that you could operate on maybe using feed forward control and then the measurements we got at the end of the batch is something we would use for feedback control but of course we have to be able to measure things and in fact in this case scatterometer e which is fairly expensive was used as a way of measuring some of the variables so this project again was to to look at interactions between lithography and edge and it wasn't just worrying about critical dimension CD but also what does the side wall angle looked like and you'll see some diagrams of that in a minute and then finally you know what's the uniformity on a large wafer are we able to get for every device that we're going to make on a wafer you know a consistent result so those are the challenges and again it sets up very nicely as a multivariable control problem here again you see a cartoon of what things look like we've got the photoresist there in yellow and we've got you see where the CD that we're going to be measuring and you know we're going to be edging that down to the CD after the processing on the right side there and again we're going to measure pre and post using scatterometer II and again the parameters we want to be able to measure or CD and sidewall angles so again adding the side wall angle which was found to have a significant effect on the performance of the chip there was essentially a new part of this and of course the other problem is you know how do we model the system so we have to understand you know what the interactions are between the different variables so here again you see again how the profile is going to transfer on to this poly silicon and then we're going to try to understand you know exactly how all the different variables are related so so it's a chemical process you can see again you know we've got lots of little you know cartoons hear about what the different chemistry is involved we did not try to model the chemistry because again that became a fairly impossible problem that really really could be implemented in a manufacturing environment and so we said okay let's just try to understand what the control knobs are the man variables and we'll figure out okay what the control variable is going to be and again depending on whether it's the the resist edge of the poly silicon etch we had different variable relationships that may be operated there and perhaps different manipulated variables one of the purposes of this of the study was to really figure out what are the best manipulated variables to use and a what was essentially a non-square system so again the idea let's stick with the simple models issues design of experiments develop some nonlinear models and then wrap that into some sort of run to run control framework so again it's a multi variable control problem that we can use quadratic optimization on or with constraints that we have to worry about so here you see some of the results that one gets which we did as part of the study you can see the blue dots there are what we achieved for it for us for a consistent side wall angle the the red lines of the ones where we said let's just control for CD and just see where the side wall angle ends up you can see that the performance is not nearly as good for the the red x's that you see in the plot so percent improvement about eighty percent so that was really I think a great result the CD Delta is again getting the difference between the CD on two spots on the wafer when they're the center of the wafer one at the edge of the wafer and again you can see with with MIMO control we can do a lot better job of staying close to the target and again the red x's are you know where we don't control for that and you can see it works very poorly so again this is again demonstrates the value of a pc and then the summary table here is just to show you some of the percentage improvements but again quite significant you know quite quite motivating you know we basically worked on this project and then was handed back to the vendor and an IBM who then then tell us what they did after that point but it was clear that they were going to work on it so another part of this is you know with all these run to run control loops how do we know how well they're doing and so we basically did a controller performance monitoring project with with texas instruments to look at you know what happens I the term high mix fab by the way refers to a fab where you're running lots of different products as well as lots of different tools and so if you see that second bullet there you know typical control engineer in the fab just for this one unit operation and keeping track of different layers that were you're going to do something different every layer you've got different products and you're going to try to track all these different things they all have a different control protocol they follow and when you do the multiplication potentially this person is worrying about 800,000 sis Oh loops so very challenging from a dimensionality standpoint and so we actually done some work on performance monitoring of PID controllers we said well hey integral control we should be able to come up with something simple here and so we did that just to give you an idea about a few equations that are involved the whole idea is that that Zeta is essentially a measure of how well you're doing the closer you are to one the better you are to optimal performance which would be essentially minimum variance and if you're off of you know less than 1 that means you're not performing as well so here you see an idea of what happened over a period of time in terms of performance variation with essentially a particular piece of equipment you can see that then the blue line is really just it's kind of a curve fit of the whole thing so you can see that there are dips that occur but then toward the end of the run of the the data there you can see it's starting to drop off so that indicates that okay the performance isn't doing too well in fact here's a distribution you can see that we were you know above point 8 for most the time but you can see there were you know isolated cases where the performance of the controller went way down where again you know if you're an APC engineer this is the kind of information you'd like to have sent to you automatically through an email saying okay here the loops you need to pay attention to and they can address those because they can't pay attention to the hundreds of thousands other things that are going on but seem to be working okay and so there's a lot of things you can do here in performance monitoring mentioned a few of them you know one of the problems is that when you think something is going bad how quickly can you operate on or can change things and and so that is actually a big challenge we worked on for a while we weren't able to get down to the oh yeah you can find this out in two or three wipers wafers level it's you know more like 10 15 or 20 so that's a challenge to figure out how to do that given the nature of the data you have so let's now move on to talk about factory control because again that's this is actually an Intel fab by the way that's the other rather unique part of a pc and semiconductor fabs that we have to pay attention to so the notion of fab white control came up you know roughly about 10 years ago and how are all these different unit operations are going to work together and you know we have you know islands of control that are basically different pieces of the process but then we still have to worry about what happens on the overall fab and this is just another picture in this case AMD where you can see again the whole sequence of operations and how we take a wafer and go from you know one unit operation that we do a measurement of that and then we have a controller that basically does a run to run operation and then metrology there is again a peculiar term that is essentially using the semiconductor industry that basically stands for measurement but there's a whole industry built up around metrology because it's fairly complex people sell separate pieces of measurement equipment they don't always come with a tool and so that was part of what need to be done here so again putting this all together very challenging but again you know companies like AMD were very successful in doing it and by the way AMD is now called Global Foundries because they split split up the company about 45 years ago so if you think about you know this complex set of unit operations you know as long as we think about a PC we know kind of everything is going on we're not going to change anything on the flaw we're not changing the recipes there's no equipment maintenance that takes a piece of equipment out of service then we can sort of lay this all out solve a no our problem and figure out how it's going to be done but the problem is in a real world and the real fab you've got lots of things going on you got reworks you have some equipment that's available you've got equipment in parallel multiple pieces of equipment that have different behaviors and you don't know which piece of equipment your wafer is going to go through and so how do you adjust to that and so on you've got you know people running pilot wafers it said of engineering lots and so the process flows might start off this way looking fairly simple but then again as things start happening in the fab and you have you know multiple pieces of equipment and you know scheduling things you're making different products then the process flow has become very complex and so this idea of a multi-product multi-tool processing facility came into vogue again in the in the 2000 era and the problem is of course we're trying to keep track of okay multi-product we're making more than one product but have one process well that's a fairly easy problem to solve and if I have a single product and say two processes and I can figure out how to do that but when I've got multi-product multi-process and things become very complicated so this just gives you an idea again of you know you know it is important to track what happens in each tool and which product you're making and so this just gives you an idea animation of you know 201 may make a and B but then it makes makes tool to is used make five and six and so again notice it you know if you decide you're going to measure all these things sort of see rly then basically you have to keep track of what's happening and what the history is of each wafer so this idea of mixing and matching that you've got different pieces of equipment different products and what is the right you know which tool but I want to use what's the best one but things happen along the way tools change there's this idea of control threads that was introduced in the early 2000s that you know let's really keep track of the path that the wafer takes and really try to understand what each piece of equipment is doing along the way and and so that's called the context of where which is you know the conditions in which is operating and what variables are at opera or essentially worrying about so this is essentially an idea was tried early on because of all the sources of variation and if again without defining threads and you lumped all this data together and so here you see a plot of essentially if you were watching all these different wafers coming out from different tools then you look at a plot like this you say well hey this process is not in control okay because things are going all over the place and so this is two tools and two products a real simple simulation but then if we say okay let's try to understand you know the history of each will each each product which tools involved and we can actually essentially say okay there really are four combinations here they're important you know 20 one product one to one product 2 etc and so now if we start looking at the data and segregate it then you see something like this where now okay now things are making a lot more sense I can now track what's going on and begin in the beginnings people didn't really realize that they just said ok we just lump all the stuff together and we don't really have any way of essentially clearing up the data and so this was an example of how it was demonstrated in an actual fab you know without threads they're going to be the the white boxes empty boxes and the dark boxes using threads and it's pretty clear that that threads was a lot better approach but one of the problems and again this was used an overlay in line with that you know we had to basically worry about lots of variation in overlay and here you see and this is a very complex system where you're trying to actually you know create the image and so you can see all the things that can be going on in overlay that's a problem for us and it's a very complex process where you have essentially markers on each level where you're actually trying to figure out okay where is the wafer zishan how is that one layer position relative to another layer and you can see you know they worry about translation magnification rotation so lots of things can happen in this mechanical movement of the prot of the wafer and you know how do we keep track all this again complexity rules again but again we figured out with the axis student named bode is not related to the famous booty by the way but we'll let that go for now but in this case they're trying to control eight different overlay / parameters or variables and you see them listed there and so again very complex process but again one that set itself up very well for LM PC and in fact I should mention this this paper actually won the I fact control engineering prize back in the early 2000s okay so metrology variations as I showed you you know you're not sure what order things are going on if you're not careful about how you track things you know you could basically take the data on the left and then reorder it and the trends you're going to get from that are going to be totally different than what you expect so asynchronous feedback became a big problem you know taking things out order and the problem is that you know sometimes you know stuff happens in a fab so you have to do things out of order you have to keep track of the data and it's very challenging so one of the things that we discovered though is that you know the other problem with threads is if you had too many threads and too many variables then the number of threads you had to keep track of became just astronomical the so-called you know curse of dimensionality and so we worked on the idea of well let's try to keep track of what each tool is doing as it goes along with each wafer or each product and then you know see if we can take all information combined together and using those tools of estimation control figure out you know let's take the data analyze it and mine the data and figure out what's going on instead of doing you know the thread approach because that was going to become hard to do as we went to more more products so again using some sort of recursively squares approach you can actually do this in because we have a simple model the model at the bottom there is really to give you an idea of you know we've got these different steps along the way and if we track them we know essentially you know if we get data at the end that's kind of a summary or essentially a conglomeration of all the previous errors that are in the process then we can go back and D convolute that data and try to figure out what the individual errors are and so again the control laws real simple the deadbeat control law and you know the notion is that you know you're going to try to solve essentially a non-square problem set of equations where you got more equations than unknowns and then solve that problem it's also an ill conditioned matrix and that's a problem as well the A's by the way I'm walking into the details are all ones and zeros so that makes it kind of an interesting mathematical construct you know we have a weighting matrix that tries to introduce a filtering kind of effect and it's a lot like again the linear quadratic problem and then we did some simulations just to give you an idea how well it worked let me just skip ahead here so this is actually a step change in a state that's the blue line the red line is what happens if we use threads Andy WMA you can see we get when the disturbance happens and there's a lot of things happening as we go to different Lots here you know different other disturbances come in it the ewm a because it's actually based on a thread well that thread hasn't had a lot of data points lately then it's not going to give us current information whereas the Jade approach not the jaded approach with the Jade approach basically stayed right on the target very well by in combining all the information and there's just another example of you know as you're tracking the parameters and what happens with you know the estimates of the parameters and so you can see how they they kind of ramp in over time which is again kind of expected behavior for the parameters and this is the part about the threads about if we have a lot of different products and tools you know how many threads do we have you can see that it doesn't take long to get up to like 400 thread so from a practical standpoint it's really hard to to work with the threads and again this just compares the the awm a standard approach versus the Jade approach and again this mean squared error is a lot better there was some work done by you know one of my colleagues jin wang at Auburn on looking at the the singular regresar matrix and also you know the whole issue around poor input X excitation you know your sort of relying on the normal plant operation to give you enough data information that you can actually do the fitting so you know having input excitation in fact some people suggested is that you all keep track of this and actually decide to use a tool where you have that you haven't run in a while so you can get up-to-date information and so again simulation work that we've done with Jen essentially looked at a number of things like covariance resetting Kalman filtering efra which is a forgetting factor approach again the Jade approach seemed to work pretty well disturbance detection is still the issue with that so I mentioned fault detection again there's a a growing amount of activity here I'm not going to talk about it but again the framework or fault detection fits fits in pretty well so again track the health of a piece of equipment as you see here so the last part of the talk I want to talk about you know development of shapes because that's kind of looking down the future you know you could say this is nano manufacturing but really the people who do semiconductor manufacturing today are really doing nanomanufacturing but this is really a slightly different approach you know the whole problem as we move to these smaller dimensions are going to be the exposure systems or the steppers and you know they're there has to be again the ability to to get enough energy in order to create the image you want and that means that as we go to smaller dimensions we need bigger and bigger systems larger in scale and more power in order to do that someone suggested this is where we're headed for the next lithography device and you know when you do the math do the economics you can see kind of where we are here as we go up the 2010 and everyone keeps holding out essentially it's enhanced or extended UV is the way it's going to go but there is still no product on the market and and they do all kinds of workarounds using quadruple patterning and all kinds of other things in order to make the current system work but at some point it's going to break down and we're going to need to you know go some some other way number of my colleagues at Texas say we should be using imprint lithography and so they're working on this idea the whole idea is it's like you know a printing press you're going to essentially print the template that you want and of course there could be some self-assembly involved with this as well this is a this step flash imprint lithography process it was developed and again we're running out of time so I'm just going to show you that the idea is that we don't have essentially the creation of the image using something like UV to do it it avoids that and so there is a cure step but not to create the size of the image and so people are working on this as kind of the next frontier to create you know essentially shapes like this and there is actually an NSF engineering research center called nascent which is also at University of Texas and the focus here is okay one of the new devices we need to make you know like flexible devices or wearable devices and so on and of course lots of new sensors that people want to make now the Internet of Things is driving a whole lot of interest in how we come up with lots of cheap sensors and so you know combining nanoscience with societal needs essentially you know here are the kind of things that are working on I might mention that right now there's no control people working in the center in spite of my complaints to the epi but he says we really need metrology in order to do control and he's absolutely right so again when people can develop metrology new ways of measuring the dimensions and the shapes and so on then that would be good so again just quickly another just commercial for nascent so and here by the way our R to R stands for roll-to-roll so now they're talking about you see that that middle put in the middle figure the the kind of device now they're using so it is continuous processing now is sort of the goal how we get from the batch type of operation so what are the long-term trends I think as I said that we need more sensors new sensors for nano manufacturing the wafer design technology is going through radical changes people say well we used to be able to predict five six years out now we can't predict more in a couple of years out where things are going to be in the future and things like three-dimensional chips are part of that as a way of cramming more capability on to the fixed space that you have I think fault detection because a lot of the control technology is mature that is really I think an area that's ripe for for contributions you know industry consolidation is happening and so the company you thought you know you're working with five years ago is now a different company or its been combined with another one and really we're down to like for big companies that make make make chips Samsung Intel and TSMC being three of the three of the big ones you know the like I said that the flexible electronics are going to really come into play here and then finally I think some people are now dreaming about photonics and so if someone is looking about you know what should I be working on 10 years from now that might be an area that would be worth working on so in conclusion I think clearly we faced insurmountable opportunities so there's lots to be done here I've had the benefit of working with a lot of really smart people bright students and you can see a whole lot of them you know went from doing you know research PhD level research on control into the semiconductor industry and that's been had a big impact and then finally I should acknowledge a lot of the sponsors we've had that basically we work basically you know the student would typically be on co-op assignment working with the company come back to UT again so we had a really good system for doing this so that's been I think a great benefit so with that thank you for your attention and be glad to have time for a few questions maybe 