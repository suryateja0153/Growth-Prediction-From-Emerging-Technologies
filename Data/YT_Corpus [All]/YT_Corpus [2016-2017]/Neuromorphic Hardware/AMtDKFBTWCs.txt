 our first beach randy bryant he is splitting his time between OSTP and Carnegie Mellon where he's been for over 30 years and our second speaker is Tim Polk who is a detailee 20 stp from the National Institute of Standards and Technology and there they will coordinate how they're going to split the this talk oops I almost tripped on the bottom okay thank you very much and it's great to have this back to back presentations of two major initiatives in high-performance computing oh I said I'm Randy Bryant I normally a faculty member at Carnegie Mellon boots have spent the past year at OSTP um and moni of you know that on joy 29th the president signed an executive order creating the national strategic computing initiative and what does that mean and that's a lot of what we're discussing today well first of all it's an initiative so really the NSC I is really the beginning of something and quite honestly I admire the Europeans for having their whole plan laid out in the way they do and we're not at that point yet we're much more in the motive we know generally what we want to do but exactly how we're going to do it remains to be determined so the executive order listed five strategic priorities and I'm going to rearrange and rephrase things a little bit but roughly speaking the first was we want exascale computing the second is we want to combine traditional modeling and simulation with data analysis we want to make sure that there'll be a technology base once current CMOS technology so it reaches its limits and we want to make the whole system work together we want to have all the not just the hardware issues but all the many software application issues resolved and also make this work not just within the government but as a collaboration with academia and industry as well which is a very critical part for a success so I'm going to present this it by pulling out five themes that are related to these strategic objectives but phrased a little bit differently just because I think it's more logical to present them so the first is this idea of are we looking for flops are we looking for petabytes and the answer is yes we want it all and I'll talk about that a little but next is you know it's important for the u.s. to have the leadership role in high-performance computing the third is we want to make it much easier to develop applications for hpc systems we want to make them more usable more accessible by a much wider range of of the u.s. population and the final one is we need to make sure that beyond the exists the current generation of hpc which probably will be using more or less the technology we knew today we want to ensure continued progress in the future so starting with the first one some of you we're at a panel this morning on this topic and you'll see that i'm using my slides in both cases if you think about it now there's sort of two classes of really really big machines out there a one is a supercomputer of course which is what this conference is all about and those are machines built really to solve the world's hardest problems ones that either couldn't be solved or couldn't realistically be solved using lesser resources but over there out in the world to there's quite a few data centers now that are operated by companies like Internet companies Google Microsoft Facebook and so forth that also contain huge amounts of processing power and storage capacity but the really design on one handed a very different scale that they're designed to handle millions of people doing relatively simple transactions but they also now increasingly being used to analyze the vast amounts of data that are being collected and running actually very sophisticated algorithms to do that data analysis so you can sort of plot these two classes of machines on an axis where the vertical axis is how many bytes petabytes of data do these systems have to manage and deal with on a daily basis and the other is how many flops do they deliver in numerical performance and as I argue these two really live in very different parts of the space and there's relatively little in between you're either very good at data or very good at number crunching and not much in between but if you look and you talked to both classes of people you'll find that they each what what the other one has so in particular the world of simulation is also all of science and engineering is becoming increasingly data-driven so not only do we want to run simulations of potential systems of some sort but we want to also take the real world data that based on observations and measurements and integrate that together with the simulation and get a more coherent model and more predictive capability we also are with the system's generating huge amount of data that need to be analyzed to understand what phenomena are rising through these simulations that need to be understood on the data analytics side the the algorithms that are being used are increasingly more sophisticated numerical algorithms that begin to look like the types of computation and their time requirements and capacity requirements that resemble those of traditional HPC so really people would like at least a machine or a combination of machines that could do both and one of the conclusions of this panel this morning was and if we had that we could start thinking about whole new applications of ways of solving problems where we tightly in a optimization gloop built in actual collected data with simulation models and progress toward much for advanced capabilities than we can even dream of today the challenges these things really live in different worlds and they're very different starting from the hardware levels up through how they're programmed and operated so at the hardware level the supercomputer is a heavily engineered system optimized for high reliability in a very uniform and homogeneous operation mode whereas data centers at least historically have been much more driven by cost considerations and scalability considerations and so they tend to use lower quality hardware but then use software mechanisms to guarantee fault tolerance and reliability in you know supercomputer the objective is to for the runtime system is basically to get out of the way and let those nodes just go at full speed doing their numerical computations and have everything sort of just perfectly synchronized so they can exchange data at the right moments of time and everyone is moving at the same pace whereas in a typical stack for data analysis it's a very dynamic runtime that's doing dynamic scheduling and then at the programming level again because of the focus on performance typically if you're programming an MPI or something you're writing code that is describing what each processor should be computing at every point in time where as a framework such as spark her dick gives you a much more data centric high-level view of the computation with relatively little concern of how does that actually get mapped onto the individual processors and I'll say if I sound like I'm sort of overly favoring the data people let me say that you know the performance that these supercomputers get through their mechanisms is way beyond the performance you could get out of a data center style system or programming environment a lot of sort of the raw computing power of those systems is left behind through the the ways they're implemented so the main point is these things really are different beasts and it's no small thing to say how could we create a convergence of these two and so I'll just say and that's why it's something that's a part of a strategic computing initiative it's an unknown many aspects of it and so we have to look to the future and figure out through research and other activities how will we make this happen the second is how will we keep the US at the forefront and you know the old question comes so who's in the lead here is it John Hay to with its 34 pedal ops or is tightened with its 18 looks like the u.s. is about halfway there but first of all let me point out that this has gotten a lot of the press from the NSC I and I described this as just one small aspect of the NSC I and not the main thing and certainly not some sort of we've got to be number one it's you know we got to regain our weed I don't think that's really the attitude within the government at all about this initiative so in particular I just simply say you can't measure how fast a machine is by running one application on it and claiming that's it also you can't measure how a nation's capability is in computing or anything by looking at what's its you know the performance of its best resource what if we measured the effectiveness of transportation systems by which country has the fastest car the one fastest cars it makes no sense well the issue in in sort of the health of a of a computing community is what types of machines do they have is their diversity is there a healthy ecosystem of vendors of users of programmers of software and so forth an enterprise it's actually out there being used to solve important problems and making advances and keeping a research base that will sustain onward progress tonight I like what the predecessor speaker said when how are you going to measure performance it's not going to be based on who's top and top 500 that makes no sense at all but that's nonetheless an objective is to ensure that the u.s. is uh is in the weed and I describe this as healthy competition it's competition that makes us work harder and strive better so in particular from a nexus scale point of view the department of energy has been planning for exascale for a while and has some excellent planning documents that describes their and part of the NSC I is to support the do E's goals in that toward that and as you know they scoped out that the sort of general parameters of a machine they'd like perhaps the most ambitious one to keep the power under 20 megawatts but otherwise it's viewed is sort of pushing at the limits of Technology but in a way that can be successful and also if you look at the do a document again they emphasize this isn't about one machine this is about an entire ecosystem of capabilities of a variety of machines of applications of software of trained people so even within the exascale framework is a much more ambitious goal than just being top in top 500 third is so what about applications and I think that everyone understands that the current systems for writing programs for hpc is really not a good and it's going in the wrong direction so if you look at a system like Titan or a modern supercomputer you basically have to write programs at three different levels of abstraction down at the lowest level there is some types of accelerator GPU say that require their own programming the nodes are typically a multi-core processor sharing memories so there's something like openmp to do multi-threaded programming and then the nodes communicate with themselves by message passing so there's something like MPI to do that and now if we design an application to really take use of those resources we have to carefully place what part of these different levels different aspects of the computation will happen then we go off and buy a new machine and all of a sudden we end up having to rewrite large amounts of code so that's really not good and you can point to many places where either it takes too much work to write the applications or even worse many applications just don't ever get on to hpc because it would be too much work to do it and I like this quote that dan Reed said if that as the performance of these machines approach infinity the number of people who can actually program them is approaching zero so really what we'd like you know in the dream of dreams is you'd like to be able to describe at a very high level the what you want to compute based on abstractions similar to what you have when writing single threaded applications there is some model of what computation is how data gets allocated and organized how you deal with failures and faults we'd like some reasonable abstractions that sort of expose you to some of the realities of those issues without making you think about them and solve them directly within your application program we'd like to sort of have a rich programming environment of libraries compilers auto tuners and all the tools required to sort of make it possible to take these high-level descriptions and map them effectively on two different classes of machines and and you know a wider collection of sharing and better code reuse so that there's not so much redundant effort going on in program development and that goes along with the goal of accessibility of making sure that all the different entities within this country whether they're scientists working in big research labs or smaller ones similar companies at multinational companies or in small companies have access to these resources and if you think about what are the barriers to entry well what right now the software issue is a big barrier to entry so if we fix that or make progress on that that will be helpful but also I'd claim the typical engineer coming out say a mechanical engineer graduating from school has relatively little appreciation for what modeling simulation and data analysis can do and especially how that can be mapped and make use of hpc class resources so it's not just a problem at the computer computation but also getting people who are the ultimate users and people with the applications to know and appreciate what the technologies can provide from an accessibility point of view right now we have relatively little deployment the big companies can afford to buy and maintain and do all the work to he's going but the small to medium companies really don't have much opportunity and similar goes on with the University so there's critical resources provided by the do ii by the National Science Foundation and those are very helpful but their total impact on society is relatively limited so what's the future well if we could think of other deployment models and one of them that's arisen in the data world is called computing and been extremely successful for making it possible for a large class of of enterprises to make use of large scale computing but not really HPC level computing is there a sort of similar mechanisms that would create a sustainable enterprise that would let provide the kind of access we want and as I say I don't think we collectively really know what that model is going to look like that's something that needs to be developed and then finally I think everyone here appreciates the the challenge being faced in the hardware world of keeping up the the progress that we've seen over the years and we've had this amazing period of time for the past about 40 well if you think from nineteen seventy one to two thousand four was this period of remarkable where Moore's lodges kept ticking along and Dennard scaling was still holding and everything was great and we basically a generation or so of people never had to worry about where the hardware would come from but since then and especially looking into the future it's getting more and more difficult so we believe that industry will largely maintain and keep the the CMOS engine going and push it to its limits and they're very destined to do that but there's many other technologies in the distant horizon that might be the successors to CMOS say carbon nanotubes or quantum computing or cryogenic computing but none of these are anywhere close to being ready for commercial deployment so this is a classic example of where federal research should come in that the government funds basic research in areas that are pre competitive and too far ahead into the future for industry to really be investing in so the role for government this is is absolutely clear but of course these new technologies aren't going to just be plug replacements for what we have today we can anticipate that they'll bring about new models of computation potentially new architectures new needs for how they're programmed and how they're operated and so forth and so once these new hardware technologies become more clear that it's going to have to be all the other layers of development built on top of it so with that I'd like to hand it over to my colleague Tim and he'll talk more about the implementation side of the NSC I Thank You Randy see so so now I think you've gotten a really great overview of why we're doing the NSC I and what our objectives are I'm going to talk a little bit about actually what we're doing what we've done so far and how we want to work in the future and as randy said we we greatly admire the preceding talk where we were able to lay out the things very much more clearly I think you'll find that we're we're moving we're exploring and where we're going to be doing this together as we go forward so let's see so so it's the first question is well what does success look like and no it's not just having one machine that's on the top of the top 500 there there's there's some real questions here about how will we measure ourselves I'll tell you I'm a cyber security guy now in the rep my other half of my day job and we're always trying to find metrics to say when is this successful and so we've been thinking about this what would the fur this this success look like quickly it's it's again that convergence of intensive and of simulation and data-intensive computing if systems are available that really can satisfy that need then we will have succeeded yes we want to keep the u.s. in the lead Europe wants to be in the lead we all want to be in the lead but that is a goal for us we're spending taxpayer dollars to help make it happen we want to streamline that HPC application development that we that was a great quote from dan Reed but you know it's it we need to be if we want new areas of science to take advantage of high performance computing new areas of manufacturing new areas of business we have to have many many more people who can program these machines and so we have to find a way to make that just much easier much more accessible the machines people have to be able to get to them one way or another not everyone is going to be able to own these systems themselves run it and operated themselves and we need to do a better job making these systems available and then the last is that as as the the natural limits for scaling for for charge base CMOS I'll as we run into them if we've got new options then we will have succeeded so I'm going to run a little bit through sort of how the NSC I got to hear what we've been doing since we announced it what's next I will say July 29 was when the executive order was issued that was not the beginning there was about two and a half years a very hard work with a really great team of people we had robbed Leyland from sandia we had Jerry blazey from Northern Illinois University we had Michael Johnson and Aaron zulan from Department of Energy this was a real team effort a lot of people put a lot of time into getting to this space along with that we had an awful lot of government agencies working together over the last two years to help shape this and make sure that this was something that wasn't just the White House saying oh you guys should go and do this this was something that the agencies agreed met the right priorities or something that they wanted to participate in that's really really key especially because this is something that we want to go on for quite a while the day after the executive order was issued we actually held our first outreach event we had a private roundtable with academia and the private sector we had 30 of 30 private sector luminaries come in to meet with with dr. Holdren the the president's science director and that was a really great session we have had our first official executive council meeting which is the group of the interagency group that is going to sort of watch over all of this we have had an RFI that went out about science and capable exascale we've had a workshop with over 200 people we've delivered an implementation plan and now we're off to what's in the future so give you a few more details about this one of the key things that the executive order laid out was it laid out the swim lanes for the agencies somebody has to be sort of in charge well in this case actually three agencies need to be in charge Department of Defense Department of Energy National Science Foundation but we also had agencies that we wanted to empower to lead the way on R&D NIST and I ARPA then there were the agencies agencies that I think traditionally been called fast followers we won't want them to be following so much we want them to be right with us we call them deployment agencies NASA NOAA FBI DHS NIH several of them have really great booths I was really impressed to see down on the expo floor so the first executive council meeting we brought together all of those agencies and it was really actually very very encouraging because even though we had it in August the president goes on his vacation in august and many of the leads that the agencies take advantage of that to get a little vacation time in themselves even though it was in August we had several agency heads and we had a lot of Deputies people really wanted to be participating and they sent their best available people and so what was really a great a great data point we created a subcommittee for the national security missions the executive order is really about economic competitiveness and scientific discovery we've been doing national security things to for hpc and if we're going to do what we're calling a whole-of-government approach you can't do it without the national security people so we created a subcommittee and we brought them into the game as well there's a group called the networking and information technology or indie program or night ered and they actually are a group that does a lot of coordination and collaboration in a lot of areas including high performance computing and they were tasked to take the lead on on putting together an implementation plan and they established a task force to do the workshop so the initial implementation plan this is something that was called for in the executive order we gave the agencies in the executive order 90 days to put together an implementation plan then when we lost a month before we had the first executive council meeting that got brought them down to 60 the agencies worked very very hard to put together a plan a plan that covers all those bullet items that that I listed there and a lot of agency specific highlights agencies put a lot of effort into putting this together so we went beyond that first three level bullets of what are the swim lanes who are the lead agencies who are the foundational agencies who are the deployment agencies and started to talk about how are we really going to work together now the first version given that it was only 60 days and there was a lot of inside baseball in it and we're still in the midst of some of the budget pieces this particular the implement a version of the implementation plan is not something that we're sharing yet I've had a number of people come up to me on the floor and say but how are we going to work together if you don't tell us what you're doing we absolutely correct we understand we're going to be working on a public summary for early in 2016 that that provides more light on all of the subject for for all of our partners that we hope to have in this uh this grand ride in the in the so a lot of that was just there I was talking about how are we working together with in government who are on the areas where we're concentrating on high-performance computing but it's more than just having all the supercomputing people talk to each other because there's a lot of other initiatives going on in government that either will contribute to the success of the NSC I or will benefit from the success of the NSC I things like the materials genome initiative they can use better supercomputers to understand the properties of materials we need those better materials to be able to make advances in manufacturing to actually be able to build supercomputers a capable exascale machine that meets those at 20 megawatt power envelope advanced manufacturing initiatives we certainly hope that we're going to be able to benefit both from improvements in manufacturing and we're going to help make manufacturing better nanotechnology initiative again the same thing the brain initiative so there's a big initiative to understand how the brain works well we're also interested in things like neuromorphic computing how do you build computers that that think like the brain thinks if you don't actually know how the brain works on the other hand if you're trying to understand how the brain works wouldn't it be better to have better machines to do your simulation and modeling again precision medicine the Big Data initiative photonics all of these places that are places for synergy within government so we've been working very hard both over the past few years and since the executive order was signed to get our internal house in order so we've we've been building a whole of government effort we know though that we don't have a monopoly on good ideas and in fact we need all the good ideas if we're going to succeed so we're proud of what we've achieved over the last three years in terms of putting together a robust team well coordinated effort within government but we need to make this a whole of nation effort that has been our goal from the very beginning it's also been the way we have traditionally succeeded in high performance computing we've done our best work when it's been business and government working collaboratively so we're looking to establish a similar level of coordination and collaboration with academia with private industry and yes with international partners so far we've had that that in an industry roundtable as I said we've had a workshop and the request for information and we've asked for were more help in shaping and promoting follow on activities a couple of quick themes from the industry roundtable I think that they repeat from many themes that you have heard at other times in other talks this week in earlier today as well programming is a really significant cost particularly in terms of human capital if I'm spending all of my time taking my old code and making it work on the new machines there's an opportunity cost I wasn't able to actually put those really high quality programmers and move them into doing something new and we want to make that simpler so that we can get we can reduce the cost of programming for these applications do E's Co design process is something that has been going on for a long time this is something that needed to be embraced and extended was the recommendation that we were hearing from privates the private sector another one is look the threat model is changing let's be let's really make a resolution that we're going to build cyber security in the right way from the beginning on this round of systems as a cyber security guy of course I wanted to put this bullet on the top here but I don't think that was fair so I put it where I think it really rank it ranked in the meeting another one was that and we've heard this again and again the business models for cloud computing offer challenges and opportunities for us in the high performance computing world and last but absolutely not least the whole ecosystem matters the workshop the workshop now this is what it was was um we I should say people got a very short amount of notice and they were very enthusiastic and a lot of people made changes in their in their in their schedules to be able to be part of this and we were really so grateful because this we saw as the real kickoff for the NSC I we had about 250 participants mostly industry and academia a lot of government people though as well and there were a lot of themes that came out of that I think that those themes again they resonate with other things that you've heard here there were about you know different paths for that the past for past charge based CMOS are it's more uncertain than it has been at any time for a number of years we keep talking about wanting a convergence of systems a lot of people at the meeting set at the workshop said convergence is great that's where we want to get but we can do some of these things now with closely coupled systems we can do different ways don't assume that the only way to get that convergence is to build the one big perfect machine noted and we agree with that there were there were there were a lot of things about cloud computing again as being a place that we better learn our lessons from did they've got business models that work we want to co-opt them and absorb them as ourselves and then we have to engage with with the non computing sectors if we want to really get broad deployment the exascale request for information I know this is an eye chart but basically we had a request for information that went out from NSF NIH and do e and they were really looking for what are the specific scientific and research challenges that would benefit from a 100-fold increase in performance something that was in the EO that i'm not sure we highlighted earlier I'm probably repeating it but I want to say it again anyway when we said capab when we in the EO we talk about get a cheating capable exascale and we didn't define that as being an XO flop in performance we define that as being hundred times the applicator that the application performance on applications of national interest we want to get the 100 fold increase in performance and if that takes less or more than an XO flops that's okay with us that's the point it's all about the applications as you said earlier and you're in yours we have I want to say it just closed on friday the request for for information looking for these impacts and limitations and barriers we don't actually have a deep analysis yet of the responses to the to the to the RFI but we did get well over 200 responses which we were extremely pleased by a very large number from the do ii national laboratories and an almost as large number from academia very very happy with that i would have loved to have seen a larger number of industry responses in foreign responses but you considering the actual target of the RFI i think that that's that that is to be expected i was actually asked to say by the i should go back one the the folks who are running the RFI said said asked me to please say while the RFI is closed late submissions are always accepted and would be of interest so if people did not know this RFI was out there and you feel that you missed it please don't please let us know what you have to say because we are really interested if it's going to be a national whole of nation effort that means we have to be looking at the whole of nation problems so that's been kind of a race through what we've been doing over the past three years to actually bring a national strategic computing initiative into its infancy what are we going to be doing now we're going to be continuing to meet in government we're going to be doing a lot more outreach this is really really important to us we're going to be doing a public-facing a version of the implementation plan because we know we can't ask you to coordinate with us if we don't tell you what we're doing a lot more in depth engagements we are looking to work with you in short we are looking for partners and I hope that you will that some of the goals that we have will resonate with you and that we can look for ways to work together in the future thank you very much all right so I'm gonna again resumed taking questions off of the where you can submit them on the SC application and so the first one I'm going to go is given budget pressures and uncertainty how bipartisan is to support for NSC I is government funding going to drive the initiative I'll take that one so actually I'm actually pretty lucky i'm working on cybersecurity and high-performance computing which happened to be two of the small number of Vice partisan things that are out there we have actually had really good support in the past I think that if you look at there have been a number of public letters that have been sent by members of the Senate I think if you look at those kinds of things you will see that they really break down very they do not break down along party lines at all that I think that we believe that that this is the kind of thing that if we make the case that this can be a bipartisan effort and that we can get support across the board so we believe that we have that potential here okay very good thank you alright so the another question with a large number of votes you kind of already answered Randy but I'm going to bring it up again to give you a chance to elaborate a little bit more it's very short in the way it was term china is ahead by 2x is that a problem um well I think some people rally around that particular one but as I said I think it's ridiculous to measure a country by who has the fastest machine and even how did how you measure those machines so I think we really need to you know in some ways it would be a relatively easy thing for us to run out and build a 35 petaflop machine and claim that now we're number one but that's really we're trying to do something much more ambitious we're trying to really change the entire national landscape and make high performance computing used in a very widespread way so that's a lot more work and that's really what the intention the initiative is there good let's see all right can a single system really handle data analytics and large-scale compute well I think you know I think what Tim just said was the right attitude is we're not sort of trying to pick winners and losers here we're just saying that if there were a type of system that could handle lots of data and lots of computation in a fairly tightly connected way then whole new applications would arise whether that's physically one monolithic machine or a collection of machines that are fairly closely linked to each other that really remains to be seen and I don't think it's I think it's premature to pick one versus another this so I'd like to add one thing to that I think that there are I mean you certainly we could build a machine if we can optimize for simulation or optimized for data-intensive computing we can build a better machine for that application but there are applications that we think are very compelling and they're going to be more of those we believe that really do require some of that convergence and so some of this is you say can one machine really do both well well there are problems that if they don't do both well that that it will not solve that application well and that's where it's all about the applications all right how do you plan to prioritize investments and high-risk high-reward technologies like quantum and neuromorphic so so you know we've highlighted two foundational rd agencies on mist and I ARPA I think we're going to be looking to the agencies that actually are going to be directly funding the research to make good choices but we're also going to be coordinating those activities to make sure that we have broad coverage it would be a shame if no one was funding any quantum computing or any neuromorphic computing research so we are going to be you know looking to spread the wealth until things become more certain I think what we really do is we look to places like I ARPA places like NSF places like NIST to actually have the skilled program managers the skilled researchers to actually help us narrow in and make those better choices but but we're not going to do it in ostp we're not going to make those decisions ourselves because we don't think that the future is certain enough to make those bets so I'm going to ask a follow-up question on that do you see them having a role in the column the exascale systems or is it beyond that that you really I think we're talking beyond exascale I think the general belief is we can kind of turn the CMOS crank well well what are we two more times the coral systems will be one more turn of the crank and then the exascale system will be another turn of the crank and then you know beyond that who knows but i think the the investment in the technology is more looking to ensure that 15 to 20 years from now we'll be able to sort of continue the progress of hpc in the kind of directions we want I would I would also add to that I think that's absolutely correct but the other piece is that we we do hope you know classical computing is not a digital computing is not going to end with exascale we're still going to be be needing to do you know neuromorphic and quantum won't solve all of the problems that we have and so we certainly hope that some of those technologies will translate back and that there will be some benefits it will not be neuromorphic computing or quantum computing that we're doing but perhaps some of the advances that we make in that 15 years will translate back into some of the digital computing pieces as well so but but absolutely true in the next 10 years that we don't expect them to be the the thing that drives it okay so here's a question that a similar for one that was asked of panos any limitations on foreign responses who can can't reply so so um we have been you know we have been very much focused on getting our own internal house in order most of the last three years has been getting government to talk to each other and collaborate the next step obviously is working with our academia are our own business sector but of course on the other hand what is a US business anymore almost all businesses are global we do understand that what benefits that we have will benefit others and we hope to get you know we hope to learn from price and other initiatives as well so I mean we're we're expecting to collaborate internationally it sort of is a walk before we run is where we are very good i'm going to ask two more questions I have to pick out the second one but with all the various EXA star efforts happening in parallel how should the various organizations communicate so it kind of goes back to you said you focused on how to get our house in order so how are we doing that well historically that's what the night or d program is really was set up by the high performance act of nineteen ninety-two to do was to coordinate across federal agencies and it's not easy i should tell you because every federal agency has a different budgeting process of a different set of oversight committees in the house and the Senate and so it's hard to do it but I think one of the objectives of the NSC I is to try to whatever extent possible to try and get those agencies and we've seen fairly a great interest in the agencies when they get together and start talking and really coordinating so I'd say night or do will be the structure within which and then this executive council will be the structures which are are keeping these things together do you anticipate some of the funding specifically being applied to that coordination or you know I mean so a lot of it's going kind of so-so um so night or D is already funded he's funded by the agencies there's there's already a structure for that and things like the executive council we just sort of take that out of Hyde as we say there won't be a specific funding stream for that I think that much more from our point of view the much more important questions of funding streams have to do with the fundings at the specific agencies so that they can fulfill their obligations under the NSC I so that they can complete the things that are in their role and responsibilities so we are much more concerned about that level of funding than the funding the coordination efforts we kind of sort that out at least within the US government as as we roll along everybody pays their own freight to participate in that in that I am somewhat familiar with it yes all right let's see so for the final question how will we ensure that the hardware software integrators leverage the strengths of each exascale initiatives I think of if one's working here and one's working there how will we bring make sure that the technology partners are working as a whole supercomputing 16 and 17 yeah i mean III so we you know we have the co.design efforts we have things that we do to try to make sure that the application developers and the the hardware developers are actually speaking with each other and that they that they're not off writing code from machines that aren't at all what people are building but but I mean it's hard that's a very hard effort it's true and with there being different efforts on different continents yes absolutely trying to make sure that we are aware so that we can use the best of breed for each of each of these solutions I mean say you know we in government no we're not going to have all of the best solutions we in North America probably aren't going to have all the best solutions just here as well and so we do have to be spending that time that's really something that you know only happens through the kinds of relationships between the researchers and between the companies it isn't something that we can completely do from the top down so you know I think it's a lot of it isn't coming upon the people who are here so so I said supercomputing 16 and 17 is a joke but not really this is where a lot of that's going to happen I think great all right well let's thank our speakers again thank you want 