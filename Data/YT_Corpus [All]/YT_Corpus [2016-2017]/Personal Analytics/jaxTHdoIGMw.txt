 the second part of this demonstration which is about the analysis of the year FAS data and we try to find a couple of interesting insights about what are the most important inflects influence factors for fatality in accidents but also to find out while how likely is that a person being part of an accident is actually dying in this accident this demonstration will cover some more advanced data preparation methods we will make a little bit more deep dive into ensemble modern models it will also cover some very innovative multi objective feature selection techniques we will demonstrate how to take a predictive model and sure this actually into prescriptive analytics by combining this with optimization schemes and last but not least give you at least some hints about how to operationalize rapid minor models by integrating them into other business applications okay let's get started this is an overview of the data we collected in total 42 different fields describing the crash itself like for example if the driver or passenger has been ejected we the the road condition at the moment of the crash we describe the people involved the age the sex and others we describe the month of the hour of the crash the atmospheric condition so all this kind of information in total 42 different fields and we collect the data from 2009 to 2013 and we are resulting about 250,000 different data rows all the data was coded also the year the most important column which was the severity of the injury here's the coding for the severity and one of the most important pre-processing steps is actually to do the decoding basically transforming all the numbers in the data back into human readable formats so let's look at the rapid - that is can be done this is the complete pre-processing for a single year of the data set the way how the fa RS data curve is work is that you actually have to download every single year on its own okay so let's look have a look at the first element here the decoding the decoding here is a subprocess consisting of multiple steps in fact one step for each column so let's go inside here as you can see first up here for the first one Eric deployed can go inside again and here I have some help or data containing the decoding for the different numerical values join them to the data set so that actually I can I can transform the numerical values into the human readable form and then after afterwards I do some type cleaning here so I can do the do this for every single column here if I look at them the next one I see it this is a little bit more complex in this particular case I for example also do some other deep pre-processing for more specialized values here for the alcohol for example I say well whenever something is wrong with the data if I don't know the test results let's replace this by blank and then we define that blank is actually a missing value so since those can differ for each column I decided to actually do the pre-processing for each column on its own so that's really the collection of steps we can see here but this may be a little bit more work but I think it pays off for the quality of the models later on afterwards I do some more additional data pre-processing here for example I transform some of the numerical values which are religious zeros and ones equal involvement and speeding into binary attributes then and handle additional and missing values I transform the columns for the IDS for case and personal vehicle into Real ID columns remove a couple of old attributes which are no longer necessary now rename some of the columns and last but not least I'll also calculate a new one the vehicle age by just using the crash year which is part of the data subtract and subtract from that year the vehicle model year so I get the age of the vehicle because that might have some influence of the from the likelihood for fatality as well so after all all those steps I actually add a branching here so this branching is actually checking if variable sets remove missing and if this one is not set well I'm not doing anything then but if this variable is set then I'm also removing the missing value this is an interesting concept so you can actually define those processes for pre-processing like this one here and then you can actually define how this process actually is working by using those process context variables here and then checking for the values of those variables inside of the process so this all here is the pre-processing for one single year cover the years 2009 to 2013 but then also I have a process here for all the years so in this case I actually create a loop here in this case I just iterate over all the different data sets and for each data set I'm actually now checking again if I should remove missing values or not at the same context variable here and then depending on if I should do this or not I executed by using for example the remove missing variable here so that allows now but this loop and this branching here allows for very complex data pre-processing operations or even modeling operations actually but you can build processes but like looping over different data sets or only doing certain kind of pre-processing in case you you want it to be done so after this loop is finished the result is going to be the five data sets for each one for each year and then in the next operation I just append all those years into one large data set I remove the useless attributes those are attributes with constant values in case there are any I don't think actually eventually they have been any but just to be on the safe side and I store the complete a data set again with a branch and the branch here basically defines if what kind of name I'm using without missing or just the complete data so you see another example of how this looping and branching can really create very complex processes so after all those loops here and all the pre-processing have been executed on the server the result is also certain stored on the server and this particular case actually copied the data here to my local machine so just to be a little bit faster and don't need to transfer whatever 50 megabits of data megabytes of data back and forth so this is the complete data sets 43 columns well--why 43 didn't we say 42 well remember we calculated this one addition column the vehicle age here as well as part of our data pre-processing so that's 250,000 about 250,000 rows you can expect them here now I'm probably better ideas to look into the statistics just as we did this before we have a couple of Rd columns like those up here and all the other information we also see those typical problems missing values for example for Echo test results this one here is missing in in more than half of all the cases that's probably not really great and there are a lot of other missing values so certainly we need to do something about that as before we can we can have a look into some of those details here for example the crash hour we can immediately see that most of the crashes are happening actually here in the afternoon region nothing which is probably a big surprise people are driving back from work with tires and there's a lot of traffic going on there that point in time anyway so you would expect more accidents here but those are the typical first insights you can make right away vehicle age also not surprised well they're more younger because then then really old ones okay so and this is this is how the pre-processing looks like how the data looks like but what I would like to do next is basically shifting to a different part of the rapid miner platform the rapid miner server and show you also how can you share those insights with other people as part of a web web-based interface which allows you to do some explorations and explanations and share them with others so you're seeing now the web interface of rapid - server that is a place where you can create interactive web applications completely based on the rapid manner processes so it came without any code involved so by just combining different processes into a new application you can actually build rather complex while apps that you can then share with others so let me show you one I created for this crash analysis here so the first thing is you see is a little bit more like a bi like a report about those 250,000 people in those years you see for example the distribution between fatal injuries like about 33% no injuries about 25% you see what you already saw in studio that indeed between 3:00 p.m. and 6:00 p.m. there's a peak for for accidents I picked a couple of other interesting insights here indeed people have more accidents on weekends so Sunday's Saturday and Friday are those days with more accidents one thing I found surprising was that they actually peak in the summer month that's something I didn't expect I would expect that there are more accidents during the winter but that indeed is not the case and then last but not least we can also answer the very important question are there more means or more females causing accidents and indeed it's in much more males so good that we clarified this one here as well this is as I said before the more like the I like reports but the special thing about rapid minor is that you can combine those elements with other applications keep in mind that every single visual elements you see here is created by rapid pilot process so first of all that means that you can also export this as a web service into other applications we have connectors to tableau and click and practically every bi tool out there but that also means that you can interact with those visualizations here so for example this is here's the number of accidents by stage there's a lot of accidents in Texas so let's click there this now causes the execution of a process delivering the top 10 accidents in Texas and since the process is executed we can do whatever you want to do is inside of this process so you can send out an email you can write the data back into the database you can purchase some stocks you can trigger some web services or some actions in general and as part of other business applications so that is really important because this is one of the key ingredients of what we call operationalization so you're not just getting some predictions but also you can trigger business actions and ends well connect this to other business applications but still again it is a little bit more be I like so let's move on to the next view here which is the first view with more advanced analytics this one here's about influence factors so what is really causing fatality or injury or no injuries in accidents and you see the most important influence factors is indeed the ejection and then the restraint system the speed limit and so on so let's have a look into the details ejection is very important well you can see it here right away if you have been ejected out of your car it's pretty much sure that you're dead while if you didn't haven't been injected then you have a good chance that there's actually no injury as well the restraint system is also very important and I thought this year very interesting so first of all well if you didn't use any restraint system while you have a very high likelihood for fatality in fact 70% here if you use a shoulder lap belt it's much safer so not a big surprise here on them in the two right columns this is how you would expect this but interesting is for example for the motorbikes it doesn't really make a lot of difference if you're not wearing any helmet at all or if you're wearing a helmet so that was interesting also the child restraint systems there is not much of a difference on fatality if it's a rear-facing system or a forward-facing system but having said this I still think that there's a high likelihood for no injury with the rear-facing system so that might be interesting for you as well alcohol test results also relatively important here interesting again no alcohol you have a much higher likelihood to survive and if you really have liked points 2 percent alcohol while this is really a high likelihood for fatality I think of course it's not really the alcohol in your blood per se it's really more about your behavior which led to the accident which is probably totally out of control if you're that drunk age has a lot of influence on the fatality as well if you're elderly person then it's totally more likely for you to die in the accident compared to a younger person with a higher likelihood okay so this is the first time we have advanced analytics there's actually more on the next step of course we can have here a zoom tree and can see that this decision tree here is actually correct and 76% of all the cases but actually the best model has been an ensemble methods using a based in Beijing boosting ensemble and using naive Bayes as our base runner so this was delivering an eighty four point four percent accuracy for those three classes which actually is pretty good let's have a look here on the right side this is one of my favorites this is actually using a multi objective optimization scheme for this naive Bayes here to figure out what are the best features to use for the learner and here the x-axis you see the number of influence vectors and here you see the accuracy on the y-axis and that graph here tells you if you only go with one single factor then please go with a driver condition let's go into liberal they're going to be a 71% correct model already but if you have to then forget about the driver condition in that case go with body type and restraint system being correct in 77 percent of the cases then the same two plus Eric deployed for three the same three plus rollover for four and so on so this really is interesting because it shows you like how well can you get actually by adding more data and more influence vectors but it also can like in this case here between one and two can show you that this can change to complete different feature steps between different feature set sizes which often is sir is interesting on its own and if some clustering results here with the prototypes down there but I think we'd rather move on to what I find the most exciting part of this whole web app which is really about prescriptive analytics and the operationalization of those prescriptive actions or predictive insights so this particular case what we have been doing here is we allow you to make a couple of settings about yourself like the age bucket you're in after sex if you sometimes drink alcohol before you drive typical speed range you're driving at or rather conditions you're driving in so after you make those those settings here then we take those settings we take those the predictive model predicting the fatality for all the 250,000 cases we observed and both together are going into an optin optimization scheme figuring out what is the best vehicle make and body type you could buy in order to have the lowest likelihood for fatality so for those settings up here the best car you can buy is actually a Dodge as a standard pickup and still you have a fatality of 90 percent so that is really the result of the optimization scheme already so let's make a couple of changes here I'd say older person drinking alcohol driving much faster in the rain so wet red streets let's say in the dark and if I change those elements here now rapid minor is again doing the the optimization going actually to through tens of thousands of of different possibilities here and in this case I'm getting the the prescriptive action well in this case by police a Honda and as a large utility car that's probably the best idea and only having a fatality of eleven percent in this case well then let's operationalize this and let's just buy a new Honda which of course in this particular case is a little bit average well artificial but you get the idea I would like to show you two elements or two processes for this one is for this prescriptive and operationalization here and the other one is actually for the multi objective optimization and feature selection we are using down here so this is the the optimization loop using the predictive model and also all those inputs factors we we have defined here in in the web app but of course you can integrate this into other business applications as well so here's just a little preparation for the optimization loop and that's happening down here so now we are going through the potential values looping through all of them apply the predictive models for all the different combinations we are trying out as part optimization plus the settings you have defined as a user here or maybe some process has defined some other process and then we collect actually or maybe even we calculate how well or how large the fatality is predicted this one then is used here to actually store the current values and fatality also the current model vehicle make and the current body type together with the fatality the predicted fatality in case it is lower and after we went through all the optimization we then generates basically datasets delivering the the prescribed vehicle make body type and fatality which is then displayed to the user or used in other applications and last but not least I would like to show the process for the multi objective feature selection we are using an evolutionary algorithm for this one here with all the settings you can see here on the right side let's focus on the key settings here the population size of twelve and fourteen eration in total so let's execute this process here and you will see what's happening in a second okay and now what we get here is a so-called perrito front so here in this case it's on the on the x-axis the accuracy for the report I actually shifted this over and here's the number of attributes on the y-axis as a negative because it's better to have only few attributes so and you can see how this evolutionary algorithm now is actually finding the parameter front of the best solutions so this is what we've every get here so now it's stable so the optimization stops but what we can see is exactly the information we have seen before so if you only go with one attribute while go with a driver condition etc and you as a user can inspect this now and then just select the preferred data set let's go maybe with this one here and this one then is delivered to the user or stored in the repository like we are doing this right now well and that's it's the result thank you for joining me in this rapid manner raishin today and i hope that you enjoyed it I also hope that you saw that rabbit miner is truly a full platform for effortless predictive analytics that means that there's never any coding required but of course you can add your own scripts if you want to we have the enablement of analysts through our wisdom of the crowds giving with the best recommendations for optimal data preparation and modeling and of course we also have those reusable building blocks which further speed your work up in fact we see a performance boost often or 50 times and more because of the very advanced data preparation and reusable building blocks wisdom of crowds and an overall this visual paradigm rapid miner is also very powerful platform think about the data preparation and combine this with loops and branches as we did in all this demonstration here combine this with your own our and Python scripts for example and even better you can push down all those computation into Hadoop clusters and since the latest release that's even true for our and Python scripts which can also be pushed down into spark and we discussed the operationalization of models I think that is very very important because a predictive model alone just gives you insight but you need to close the loop and to prescriptive action and that's really what we did here we use a predictive model combine this with optimization schemes and figure out what is your best course of action and then you can use one of the more than 500 connectors to business applications to automatically trigger the execution of this action from rapid miner we discussed the interactive web apps and that you can also take those individual results and integrate them into other systems as well thanks for watching 