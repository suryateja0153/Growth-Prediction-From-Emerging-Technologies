 testing testing can you hear me good good okay good morning everybody thank you for taking the time to hear a little bit about storage and components for building open compute solutions with hard drives and SSDs my name is Walter Henson I'm with the company you probably don't know but likely use our products hgst has been in the business for over 50 years we were part of IBM's global storage technologies group then we're sold to hitachi data systems and now we are part of western digital we are the leader in enterprise storage worldwide more than half the world's data rests on some form of HD st technology either solid state or hard drives today we're going to talk about putting pieces together we're actually going to build some stuff have it have a good time with that and do a little little little digging into what's going on in the industry as well so as part of western digital HD st is focused on enterprise we have a full lineup of SSDs from extremely fast nvme compliant PCIe we also are the leader in the marketplace for SAS SSD so most of the hybrid arrays or all-flash arrays that you see if you crack them open and pull out a drive it will be an hgst SAS SSD we also are makers of the most hard drives for enterprise in the world ranging from 15 k high speed drives to the highest capacity helium based technologies today at ten terabytes and we're just introducing some new systems as well there's a j-bot a 60 dr j bod that's available super great density very good packaging and select from your favorite flavor of HD st drive to go into that and then at the high end we also have a multi petabyte active archive system based on object storage so a full spectra if you will of technologies for open compute so the theme is what's hot what's cool let's let's build so I'm going to talk about the hot side for a minute despite what you might have heard there is no crossover point that we see in the near future between SSD and HDD especially when you look at capacity HDDs we maintain quite a significant gap in price per gigabyte when comparing HDD to SSD at the same time you can see that SSDs of course they're coming down at a rapid rate and we say that they've moved into the mainstream more and more enterprises are buying and buying into the flash concept this one I find really fascinating this is SSD by location this is a term IDC uses and basically what they're showing here is that yes there's growth in SSDs for all-flash array but it's the server side where the real growth is for things like PCIe or nvme and I see that a lot in talking with customers more and more folks are moving to distributed architectures for databases so you know my sequel the no sequel variants and those are ideal use cases for server-side SSD super low latency very high performance this is a view of interface type what you see here is that SATA is starting to while it's the by far the highest shipments in volumes sad is starting to get squeezed a little bit and there's a PCIe enterprise entry level that is coming soon that will encroach even further on the the SATA interface and then you know yes the faster it is the more expensive it'll be it's kind of like a car but these are just industry averages and you can see you know that that SATA is good enough for a lot of folks and in that particular case you won't pay as much but I want to think about maybe a heat index in a little different way and I want to normalize out price per gigabyte so if you do that and you say what's the average I ops in a normalized scenario you can see that nvme or pcie is is dramatically better in terms of delivering performance for i 0 for mixed workloads and for sequential workloads so you know if you normalize out the price you got a little different decision criteria to make and this is the basis of a paper you'll be able to find at HDS cheese website in about 30 days and and it ties back to work that was done in like the 80s with IBM it was called the the economic impact of rapid response time and basically what this does is I look at pcie I look at SAS I look at SATA and I look at all flash arrays and I look at latency over time and compare latency against worker productivity so if you've got a thousand workers earning a hundred thousand dollars a year and their idle for some amount of time based on latency their worker impact is significant and and that's what this is building out and you can see that the closer you have the storage in this case nvme to the server the less I oh wait time there is there's no network latency etc therefore higher worker productivity so let's build some stuff with SSDs you can do shared storage inside servers with SSDs in the case of something like VMware vcn or some of the alternative technologies basically you create little building blocks and grow those as you need be San is a really cost-effective way to work in web site type environments it's maybe not the best performer so there are other things like pernix or max t'a that can deliver maybe better performance than v san but either way if you're looking at a converge system building it yourself can be very effective compared to some of the prepackaged converged systems you can also do shared storage for things like Oracle their software products that allow for flash to be pooled and presented to something like oracle ASM as a single volume of storage and then ASM takes over does it striping does its replication and you can grow an Oracle environment very very nicely with all flash on the server side you'll hear people like Wikibon refer to it as server sand and in one case one of the largest telecoms in the world has seen that this approach is seventy percent less expensive than going all flash and then for all the modern databases and the scale out architectures server-side flash is great the database itself understands that it's dealing with storage management so every node is is completely replaceable and it basically shards the data across all of the nodes so there doesn't have to be a sophisticated set of storage management features down at the the SSD level so now let's talk about what's cool we've been making hard drives for a long time and one of our talented engineers in the manufacturing line figured out that if he injected helium into a hard drive it would pass our tests much more effectively than air base drives so this filtered up through management chains everything else and about four years ago we introduced the world's first helium-filled HDD we've now shipped over five million of these things they are incredible we can improve density we can improve reliability and most importantly because there's almost no friction in helium we reduce the power consumption required for hard drive so when you're scaling out a massive a dupe environment scaling out a massive stuff environment your number one concern should be optics and as you look at op X what's the biggest driver of the cost of op X all those darn hard drives so we're doing our best to to make a difference in that area and here's just an example this is a this is a SEF scale out and you can see here with a ten terabyte helium we're talking about sixty-three percent better operational costs than with our first generation six terabyte helium drives so density capacity low friction low cost for total cost of ownership so what's hot what's cool what's cold the cold is chasing the long tail and anybody in big data and looking at analytics here's the problem we run into with Hadoop one terabyte a day with Hadoop striping and everything else that it does equals a petabyte in a year so your data nodes sprawl completely out of control so by taking a different approach and being able to use high-capacity drives maybe you can keep more data longer which will allow you to do better analytics if you're ingesting a terabyte a day and you get to a petabyte in a year you're probably going to throw some stuff away that you don't want to throw away but with this approach maybe you can keep it so one of the things that is relatively new in the Big Data world is the concept of tiering right so in june apache hadoop came out with 2.7 which introduced the idea that you could have tiers of storage and the bottom tier doesn't necessarily need three stripes of replication to go sit for seven years something like an object store could be ideal and an object store tends to be very efficient it's easily accessed rights disk base so you can pull stuff back for analysis as you need back into your Hadoop cluster but with erasure coding and object storage you get a very cost effective medium for long-term archival of data so just a little look at that whole Hadoop environment again remember if we're ingesting a terre by today we get to a petabyte in a year that's something like 280 servers if you're running for terabyte hard drives that's a lot if you were to take a different architecture and use storage tiering you could put flash on data nodes and high-capacity hard drives on data nodes and have an archive over here and you could have a very efficient system load times are faster MapReduce cycles are radically improved things like shuffle and join that are small block related are not friendly to a hard drive based data node but are very friendly with SSD so concept here is choose the right tools for the job you don't have to have this classic symmetrical architecture that was that was part of the original design the industry has grown up and we've gotten better at giving you better tools and we say go build so that's it I'm happy to answer any questions that you may have yes okay so are you asking about HDD or SSD or both okay so the question is can multiple servers access a device and the answer is it depends on what type of work you're doing so if you if you're doing a shared storage architecture then yes the server's would be able to address you know 11 device that sitting on a server multiple servers could address that device if you're doing a shared nothing where you're you know no sequel sharted database no the storage devices don't know about each other at all yep I ok so I think I understand your question with with a no sequel can I have that data accessed from another cluster of machines is that where you're going ok to copy right yeah ok so if i have a no sequel cluster and I've got to set a data on it and I've got another cluster over here and I want to access you you do that either through your operating system or through the application so if I've got a Cassandra over here in a Cassandra over here I can set them up so that I could access that data without having to make a copy ah yes okay so is there value for that type of model where I'm remotely accessing a cluster of data absolutely right you're not you're not spinning up a whole nother copy right so you're saving on space you have to watch what the workload is in the primary cluster right if it's heavy workload having other machines access in may not be such a good idea but definitely it's a it's a cheaper alternative okay thank you very much 