 thank you everyone so I'm Robin I work on our internal platform-as-a-service which we call my cross and the aim of my cross is to enable engineers within Atlassian to develop and operate micro services quickly and easily so these concepts of micro services service decomposition DevOps these are all concepts that are very closely related to Alaskan Connect which as you know is about running a developing and operating your own services so what I'd like to do is to give some insight into our platform how it works and some of the lessons we've learned along the way in the hopes that there'll be some takeaways for you when you think about running your own services whether that involves assessing third party hosted passes like Heroku AWS elastic Beanstalk or looking at lower level components like mesosphere kubernetes or even building your own platform components so I'll talk about why we have a platform in the first place I'll talk about how it works and the lessons learnt but before we do that let me give you a really high-level overview of what the platform actually is so what we provide that last few developers is a number of tools and services and an execution environment that allows developers to have their code up and running and available to the wider world in a few minutes and because these teams that build these services are also responsible for operating them we feed back a bunch of information to them in the form of monitoring and logging so that they can manage their service effectively and the platform also encourages and in some cases even enforces it enforces some architectural best practices so that the services can be as healthy as they can be so we haven't invented much here if you peel off the lid and have a look under the covers we're really just gluing together a number of third party hosted components primarily AWS services and also a number of open-source components like elastic search and docker so this the platform went live a couple of years ago and right now we have well over 400 services running on top of it these are services built by at last funds for all sorts of purposes ranging from experiments to try out new technologies and new ideas to ship it projects to internal tooling like wall boards and chatbots to connect add-ons so we've got both internal connect add-ons and public connect add-ons running on the platform all the way to core components of our critical customer facing products so for example a lot of bitbucket pipelines runs on this platform various parts of confluence collaborative editing runs on there as well so the question is how did we get to this point how did we get to the point where we have so many services that we need an underlying platform to manage them all and to understand that I'd like to take a look at how service decomposition has evolved within Atlassian and we'll start by looking at how JIRA and confluence of cloud work today so when you go and sign up for juror or confluence cloud today what's actually happening under the covers is we're provisioning a Linux container for every customer that signs up so when you get your or something dot Atlassian botnet address you're actually under the covers getting a Linux container and within that container we run the full JIRA and confluence or whatever products have been bought we run the full JVM monolithic processes inside of those containers so and those are very similar to the processes that would run on premises if you downloaded the application now there are a few drawbacks to this approach number one is whenever a new customer signs up we have to allocate a full container for them no matter how much they actually use their their instance and that takes up precious resources in our ax problem number two is that it couples functionality in a way that reduces agility so for example if we find a problem in general confluence close to the release point that'll block the release and it will prevent us from getting features out in an unrelated part of the product and then lastly it also limits technology choices because any new features or improvements have to fit within these monolithic processes so they have to use the same JVM stack the same libraries and frameworks and exists within the same resource constraints so memory and so on as those monolithic processes so with all that in mind this architecture has served us very well there's lots of racks lots of data centers and this has scaled us from a few thousand instances to well over a hundred thousand instances so it has worked very well but when we think about our next order of magnitude increase in user counts and also in developers contributing to the software it becomes apparent that we need to move away from this monolithic approach and the really interesting thing is that connect itself was a big step in the right direction towards service decomposition so a couple of years ago I was at Atlas camp talking about who's looking which was a one of the ready early you connect add-ons it's a very very simple one what it does is it shows you who else is looking at a JIRA issue when you're looking at it and that really began as an experiment to see whether we could take a piece of functionality which on the surface is intimately tied to the product but under the covers exists very independently so with a an independent deployment lifecycle independent scalability characteristics and isolated in such a way that if it has problems those problems don't affect the core product and they don't affect other add-ons that are that are running in a similar manner and all that was was successful and of course Kinect has evolved since then and you can do much much more sophisticated things than this so thanks to all of you who have kind of pushed the boundaries of Kinect and made it what it is today because connectors really solidified service decomposition as a viable architectural path for Atlassian clout and so with that in mind last year I was at Atlas camp talking about blobstore which was our first example of taking not an add-on but core functionality from JIRA cloud and extracting that into a shared service so in this case it was attachment storage in JIRA so we removed the attachment storage from the hundred thousands der instances in lasting cloud and moved that out into a shared service in AWS running on top of the platform that I'm talking about today and we didn't stop there since then many other bits of functionality have been extracted ranging from attachment conversion to the shared user administration UI's all of those have been extract into services that run on top of the platform and benefit from those same advantages of connects in terms of isolation and independence and agility and this trend is going to continue so in the not-too-distant future we expect that much of the functionality that constitutes your own confluence cloud and significant chunks of bitbucket and HipChat cloud will be running on this platform so with that in mind you can see the importance of this shared platform component so let's peel the covers back a bit and have a look to see how it actually works and what it actually does and let's start that by looking at it from the developers perspective so an Atlassian developer wants to use the platform what do they need to do so they need to supply two things firstly service artifact and secondly a service descriptor so the artifact is a binary it's the kind of meat of the application it can be a jar file an NPM module or a docker image and the developer pushes those into an artifact store like a docker registry and then they also need to supply a service descriptor which is a chunk of JSON or llamó which describes the service so just to demystify that and you don't need to parse the details of this but this is what a service descriptor might look like so it can start off very simple with just a reference back to the artifact and a bit of naming information but then it can evolve into containing a declarative description of the scalability characteristics like minimum maximum load count the backing resources that are required by the service like a database or a queue all of that will be declared in the service descriptor so once the dev has their service descriptor they invoke the micros tooling passing it in and then Mike Ross goes and deploys their service in AWS and we use cloud formation for this and the first thing we'll do is provision a compute stack which consists of a load balancer and a bunch of ec2 instances in an auto scaling group across availability zones and those will suck in the service artifacts provided by the by the developer and we also go and provision the backing resources so any databases queues any other external resources that the the service requires so and that's pretty much it so once that's done the service is available at something dot Atlassian dot IO either internally within the Alaskan network or visible on the public internet if the developer is opted to do that so that's the the core purpose of micros it's pretty straightforward now I mentioned logging and monitoring as being an important part of this as well so let's take a closer look at that so what we do from a monitoring perspective is we enable service owners to pump out service level metrics to stats D and then beneath that on the node we use collect D to capture infrastructure level metrics and cloud watch to capture metrics across all of the resources that constitute the service and all of that gets pumped into Data Dogg which provides a really nice centralized place for developers to build dashboards that correlate all of this data and they can set thresholds to be alerted on whatever anomalous behavior they may be concerned about and then on the logging front anything that goes out to standard error or standard out from the service we pump through fluent D then into a Kinesis stream and then into an elasticsearch cluster which is fronted by Cabana so again developers can build really rich dashboards to get a lot of insight from their from their log data and the team that worked on our logging infrastructure did some really awesome engineering and we can actually process well over four terabytes a day of log data and get that ingested and indexed in real time which is really really valuable for service owners now in practice this is what the Pointer service looks like so we provide a CLI tool so devs just type micros service deploy their service name a reference to their service descriptor and then all that stuff I just talked about happens a lot of devs like to wrap this up in their build or deploy plan so then you get that really nice flow of just merging to master that kicks off your build and deploy and then your code is up and running at something god bless you in Ohio some teams even do continuous deployment right so every change will bubble through environments go through appropriate testing and end up facing the customer with no manual intervention from the service owner we just and the important thing to note here is that anyone at Atlassian can do this anyone can go and create a new service and in a typical week we'll probably have two 300 engineers interacting with the the platform deploying and managing services so if these numbers sound very big to you and therefore the concept of a platform as a service feels very heavy weight to you the rule of thumb that I'd suggest is you probably want in the order of one person looking at platform stuff full-time for every 10 to 20 engineers you have building components on top of it and I suspect that if you stray far away from that ratio you may well end up missing opportunities or economies that come from having a focus on a shared platform layer right so now you know why we have a platform and how it works so I'd like to share some of the lessons we've learned building and running it and we've found that as platform owners we spend a lot of our time weighing the pros and cons of consistency versus flexibility whenever we consider a change to the platform we want to be sure that it leaves enough flexibility for service owners to innovate and use the right tools for the job but we also want to encourage consistency to prevent people from having to reinvent the wheel or to stray away too far away from best practices so with that in mind I'll split the lessons learned into two sections firstly the areas where we found it to be better to encourage consistency and then secondly after that I'll talk about the areas where we actually favor flexibility above consistency so starting with the consistency areas the first one I've kind of touched on already it's about logging so it found it to be very very valuable indeed to force all services to use the same logging and monitoring infrastructure apart from anything else this means you can correlate that data across services which is great but it also means that when engineers move between services you get some nice economies because they already know all of the peripheral tooling that they need in order to operate those other services another neat thing about logging is that we very strongly encourage developers to log structured data rather than just plain text so for example this is what an access log might look like structured data you can see it's Jason there's a field for the method there's some fields for the headers the reason we do this is because that structure can be understood and indexed in near-real-time by elastic search so then developers can build really elaborate dashboards that understand that structure that query across the data so they can mine it but graph the numbers and so on and so we get a lot of value from from doing this the next area where we like consistency is around upgrades so as a rule of thumb consistency is good when risk is high and for a service risk is obviously at its highest when you're pushing changes to the service so we've chosen to enable a small set of deployment patterns of upgrade patterns that developers can pick from and the most common looks like this so you start off with your service version 1 up and running so you've got a computer's back and a few backing resources when the developer comes to deploy version 2 we actually spin up an entire new compute stack with a new load balancer new ec2 instances the only difference being that the new instance is referred to or suck in version 2 of the service artifact we then do a health check on that new stack so it's a mandatory part of the platform contract that every service has to expose a health check URL that the platform can go and query to validate that the stack considers itself healthy so once we've validated that and only once we validated that do we flip over the DNS so that requests start to come to the new stack and then we keep the old stack running for a little while so you can roll back if you need to but eventually that will go away and you're on version 2 all right we also like to limit the set of persistent data stores that are available to service owners and in fact we restrict it to three persistent stores we have Postgres RDS for relational DynamoDB for no sequel and s3 for object stores and the reason we do this is because we want to be able to provide really good backup and recovery for all of these persistent stores so again you don't need to parse and understand this next slide but this is what our infrastructure looks like for for doing backup recovery and replication for DynamoDB so you can see there's a lot of exit e here and now if we had to do something equivalent for CouchDB MongoDB Cassandra whatever next DB you can imagine that there would be an explosion of complexity and it would be very hard for us to commit to providing top-notch backup and recovery moving on so we we also like to encourage services to isolate their resources so by that I mean if you have service a and service B they'll both come with their own set of resources that will be isolated via AWS security constructs like I am policies and instance profiles and then when service a wants to reach for some data in service B what we don't allow is for service a to go straight to the database of service B instead we encourage it to go through the API is exposed by service B so the reason for this is to avoid the integration database anti-pattern so as you know managing schemer upgrades is hard enough when you have one service talking to the database if you have five different services all integrating at the database that level when you need to do a schemer upgrade it can become extremely difficult to coordinate that across the services so by going through that domain logic you give yourself a level of indirection to be able to be more flexible when it comes to changes at the database layer all right moving on we also find it very valuable to have a common way of storing metadata about services so this allows us to very quickly find are the service owners of any services that use for example sqs queues so we can go and find them if we need to tell them about an improvement or a problem with that flavor of backing resource this also really helps us with cost allocation so we have a common mechanism of automatically tagging resources in AWS to do cost attribution to different parts of Atlassian so that it's not my team that's responsible for paying the AWS bill for all of it lassen and we can kind of separate that out nicely and this is really powerful because it means that when a new dev comes along and wants to deploy a service they don't have to do any kind of up front complex procedures to get cost allocated all of that is handled automatically by the platform we also restrict SSH access to production nodes you can't SSH in and go and change stuff on your life production nodes to avoid you know changes that haven't been through version control and peer review and to avoid ad hoc fixes that will be lost when the instance is auto scale or when we recycle the nodes in a deployment and then relatedly We strongly favor statelessness so this is enforced by the platform because if you have a load balancer spraying requests across your nodes or if your auto scaling adding and removing nodes you can't rely on state being held onto in memory or on disk within a given compute node you have to extract that information into a backing store a persistent store of some kind so this may will feel like a lot of rows and a lot of complexity but really it can be boiled down to two concepts the first is 12 factor so if you haven't heard of the 12 factor app I strongly recommend going having a read about it it's a set of 12 design guidelines put together by some engineers at Heroku and there are really really valuable concepts when it comes to building very scalable and operable services and then the second one is a simile that you may have heard of before it's the idea of treating your compute nodes as cattle rather than like pets and the intent here is to have a really well oiled process to rebuild your compute nodes from scratch so in other words the destruction and recreation of your compute nodes is part of your everyday life so we achieve that because it's part of our deployment deployment process but there are other ways of doing it like through chaos monkey and that kind of stuff and so the intent here is to ensure that by having this very well oiled mechanism of creating your nodes from scratch you're much more resilient to unexpected failure so compare that to a case where you've treated your node like a pet and customized it for months and months and years when that node dies it's going to be a much more sad event than if you would treat it like cattle so that's the the summary of the areas where we encourage consistency let's move on to the areas where we like flexibility and the first one is about flexibility in terms of technology for the service owner so we found nothing turns people away from your platform faster than mandating a particular language or a particular web framework and so this is the the runtimes that we support on ALP as and you can see docker is the most I adopted it's also the fastest-growing and of course in their docker containers service owners may be running JVM processes onload processes or Python it's entirely up to them by running in docker they get control over the version of the runtime that they're running so that's an added bonus but we found that docker is also a really neat way to enable innovation because there are a lot of very interesting stacks that are running in these docker images we have several services that are running Haskell a growing a number of services that are running on go so that that really enables a lot of flexibility for service owners to use the right tool for the job moving on we also really like it when service owners integrate with third-party hosted services so this happens very often when there's a gap in the platform and they need to do something that we don't support yeah so for example we've seen users use mailgun for outbound email cron man for task scheduling and all sorts of things like that and this is very valuable to us because it means that we don't block service owners who need that functionality and they also assess these third-party services and if they turn out to be really good then those become the kind of de facto standard that we recommend to everyone who's using the platform to use rather than us trying to build something equivalent another interesting one so we have to be very flexible when it comes to adapting to problems in the underlying platform so remember it's platforms all the way down our platform runs on top of AWS and every so often we run into your unexpected limitations in AWS so things like resource count limits API throttling that kind of stuff so when this happens we have to react and work around those problems there's actually a team at Atlassian that's built a tool that runs on top of my cross the the monitors all of our AWS accounts for known limits and automatically raises support cases with Amazon when we get close to them because many of those limits are actually soft limits that can be bumped up by Amazon so that's pretty neat or at education this is my favorite one so we're very flexible when it comes to providing training to other teams in terms of using our platform so we have a few tools on this front we do something called a boot camp which is a 90-minute workshop there pretty much every new n Jr goes through probably about 300 people have been through it so far and the commitment we make is that by the end of that boot camp they will have deployed a service to the platform so it's a really nice way of getting the platform in everyone's mind so that when they go and start on their projects they know that it's there and they know it's something they can use we also do a number of rotations so we we rotate people from the platform team onto service teams so that everyone gets a an experience of seeing the platform from both sides we have a rotation within the team could develop run support where their first their first focus is handling support requests and help request from the rest of the organization and we also do kind of bespoke training for teams for entire teams who are using the platform for the first time and have a particular use case in mind so we'll if that is really valuable for building a kind of internal with developer community around the platform we're also very open to getting contributions the platform from other teams and the interesting thing here is that a lot of the components that constitute my cross run on my cross so it's kind of self hosting and this means that people who build services on top of the platform inherently already know how to contribute to it so for example there was one team that was building a lot of services and they got tired of some of the boilerplate they had to do when they were creating new services so they built a web UI that allows you to select from a number of service templates press a button and that would automatically go and create your bitbucket repository create your build plans and deploy your skeleton version of the application so you just have to go and fill in the gaps to get your service running and lots of people started using that so that effectively became a part of the service a part of the platform rather sorry and I won't go through all of the details but this keeps happening we keep getting developers who build tools that run on the platform that are valuable to the rest of the users of the platform so in a sense the platform keeps growing with these contributions and it's a really really useful thing and the last point that I'd like to make about being flexible is about your roadmap so you'll get a lot of requests from everyone using the platform if you're a platform owner about new features that they want or changes that they want in the platform one way to handle that is with those rotations where you have engineers from service teams onto your platform team to help do those things but in addition to that as a platform owner you'll very often be on the hook for organization-wide drivers so things like organization-wide disaster recovery policies they'll usually be work for the platform owner on that front and compliance as well so things like Sox up to the platform owner will very often be on the hook for changes around that kind of stuff and then while you're doing one of that you also need to keep some roadmap room for upcoming technologies that will represent significant opportunities for your infrastructure and two of those that we're really excited about our container clusters so things like kubernetes and mesosphere those look like they're going to provide some really nice efficiencies in terms of multiplexing services across virtual machines and providing more elaborate orchestration constructs and then serverless I'm really excited about service because it seems like a very interesting way to enforce some of the 12 factor concepts of statelessness and isolation to kind of guarantee horizontal scalability and in fact the next talk on this track after lunch is about service so I very much recommend sticking around for that one all right so as you walk this kind of tightrope between consistency and flexibility it's important to keep in mind that the role of your platform is to try and deliver on many of these promises that come with service decomposition so this includes things like resilience and scalability which we achieved through AWS constructs like availability zones and auto scaling groups but also through best practices like 12 factor well so you've got to enable your teams to deploy independently so we achieved that by giving them the tools that they need to deploy safely without any inherent kind of blocking points on other teams you want to let your teams have choice of technology and we achieve that primarily through docker and then you want to ensure that service owners feel end-to-end ownership for their service and we achieved that by also giving them the tools to deploy safely and then having the tools to monitor and follow their service well after it's gone into production so to wrap up if you're heading in the microservices direction remember that your choice of platform is going to be key to your success in this new world because it determines the overhead of Korea I'm running new services so a key question that always always crops up when people work with micro services is when what is the right size for a service when should I take a big service and split it up into smaller chunks and it's never going to be the right time if you don't have a good platform to help you so my final tips to set off in the right direction remember to always balance consistency with flexibility you want to be flexible enough to enable innovation but consistent enough to add value in terms of resilience and best practices and when you're making these decisions between consistency and flexibility you can use concepts like 12 factor and nodes as cattle as your kind of technical guiding directions to help you find the right balancing point and then lastly make sure you keep your platforms barrier to entry very low because your platform isn't going to be worth much if nobody's using it and the best way to ensure that people do is to support service owners from spike or the way through to production so thank you very much I'll be around over lunch for questions 