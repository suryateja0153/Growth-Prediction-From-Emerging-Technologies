 so I'm going to be talking about a lot about the Apache main source project kind of start off a little bit with some containerization thoughts and kind of the micro-services trends that are happening but most the presentations can be out of patchy Mesa sauce and then at the end a little bit I'm going to talk about some of the work that we're doing at a at Mesa sphere so it's an emerging trend that I think we're all seen that's really kind of cyclical upon itself and that's a lot of organizations are moving to micro services and a lot of organizations are adopting ideas behind containerization and when I say micro Service is here you know one of the things that's really interesting about micro services is it really gets us back to building applications with a couple of really really nice properties one is that we can kind of focus our applications these services on doing one thing and doing it well which is related the UNIX philosophy that lets us compose these things really really nicely it also lets us as an organization build applications commit individual applications code in isolation you'll detest the individual pieces in isolation be able to actually deploy them in isolation roll back individual pieces in isolation and that really captures a lot of the organizational structure that a lot of teams have either working in the distributed way or in parallel really well it doesn't mean that it's a free lunch there's still quite a bit of work that you have to do as you start to move to micro organizations but a lot of the benefits outweigh the cons and so most organizations are doing it one of the things I think is pretty interesting about micro services in particular is that the service-oriented architecture which kind of predated micro services had a lot of these same ideas and this kind of as we always see in Computer Sciences the regeneration of a lot of those ideas so an organization that I was at that bid that did the big shift to micro services was Twitter where we used to run the opposite of a micro service and not a service-oriented architecture a giant monolithic application that was written in Ruby on Rails that we affectionately dubbed the monorail and the monorail kind of did everything from the routing presentation logic pretty much everything what's storage that was the only other service that the Mount Rell was really communicating with and the transition that we did at Twitter was we went from this giant monolithic rails application to a bunch of micro services that were all communicating amongst one another a majority of these things were actually written in Scala Java some as well and kind of at the same time what started to take shape at a place like Twitter and then now that's happening in the general industry is people started transitioning less and less from the way that they configure and manage their individual machines is with things like rpm or when you go to launch your application using something like get to actually pull your repo and build it and they're moving more and more to two things that have less moving parts so just entire jars the jar is your entire your entire bundle that you deploy or tarball the tarball is everything you need all your dependencies a static binary that's my little static binary gif or or using things like docker and I think what's really interesting here is you know in the days of using things like rpm and get for doing a lot of our configuration package management deployment there were a lot more moving parts so when things failed they were hard to debug and they failed more often because there's just just more more moving parts and this movement in this containerization realm is really about less moving parts when there's less moving parts there's more deterministic guarantees I think one of the things that's pretty interesting here is the static binary approach is one that doesn't get a lot of love but for those of you that are writing and go obviously you get static binaries out of the box and that's because at Google with the way that they were actually deploying their applications they weren't using an RPM they weren't using git they weren't using docker they were using static binaries and so they were actually when go first created they said well let's just make sure that we can build static binaries but by default okay so the interesting thing about sort of micro services containerization is that they're creating this reinforcing trend on Morgana's ations not just using micro services and containerization but also starting to do cluster management so cluster management in quotes at Twitter in 2010 really consisted of using technologies like puppet for doing the configuration package management and tools like Capistrano for doing our deployment we actually built a tool on top of Capistrano that we called murder referring to a murder of murder of crows and it was to actually get Capistrano to scale to the number of machines that we're actually using there was one really really missing important part in cluster management here and it's why I put the words cluster management and it was people people were the ones that were actually writing a lot of the scripts to run puppet doing a lot of the operations very very manually and it wasn't just a few people it was a lot of people there were a lot of people that were there that were doing cluster management the early days at Twitter so why did we have to have so many people basically at Twitter what what would look like is we take all our machines and we'd effectively go and we'd statically partition the machines humans we go and they'd statically partitions and manage the clusters by saying okay let's take this collection of machines and let's go ahead and run MySQL on them let's take some other collection of machines let's go ahead and run a memcache let's take yet another collection of machines we'll go ahead and run run rails there you know finally Cassandra and Hadoop right so this is this is the standard thing that that we were doing is we were going to wear static paper partitioning collecting our machines and saying okay these the machines that are just going to be used for running these applications and this is where as an organization at Twitter we started running into a bunch of challenges and why cluster management really evolved at Twitter so the first one was just dealing with failures so this is a this is actually a real email from 2010 where somebody had to reboot a machine in one of the data centers and they asked a very natural question which is hey we're there any production services running on this machine so this is as humans doing cluster management dealing with failures and then other humans actually figuring out how to how to get the services back up and running okay so you know what does that look like in the data center well machines fail for all sorts of reasons all the time and humans had to actually actually account for that the next big challenge that we as an organization at Twitter really had to deal with was maintenance and maintenance I really like to call planned failures because effectively you're inducing a failure in the system you just know exactly when that failure is going to occur and so you can plan for it there's a lot of maintenance that actually happens in our data centers not just in a private on-prem data centers but also in the cloud one of the biggest ones is actually upgrading the kernel we all at some point need to upgrade the kernel which means we're going to be shutting down our machines bringing up other machines or bringing that machine back up with an upgraded kernel but of course there's replacing machines as replacing switches PDUs moving regions if here in the public cloud there's lots of different maintenance like events that you might be doing that cause can cause a lot of humans to be running around communicating with one another to try to figure out how they can actually do this so at an organization like Twitter folks unfortunately had the experience where they did maintenance on say the racks the topper rack switches that were on the far far side of this diagram bringing down all the memcache 'iz and of course that means that nobody's tweeting okay the last challenge that I want to address is really utilization so for some organizations this is not nearly as an important one but for a bunch of organizations especially as they start to scale to more and more machines it becomes ever more important so just grab some water from the utilization perspective when applications are running they have different requirements for the needs the computational resources that they need over the course of them actually running their applications so when we kind of do this static partitioning and we divide the resources up amongst the different types of systems that we're trying to run we end up gaining a situation like this so here I've just divided a hundred percent up into a third each so memcache gets 1/3 Hadoop gets 1/3 and rails gets a third and this is now you know rails using you know during during certain parts the day when not as many people are tweeting there's all those free resources that potentially be used for some other application like say Hadoop and memcache kind of follows a similar cycle to something like rails so really what we'd like to do and what humans end up doing is they end up going and say okay well how can we optimize the number of resources that we should be allocating these applications over either different periods of the day or different times so we could get to something like this so Hadoop could say scale up and use those resources which alternately means that we could buy less machines or run more applications so I think what's really interesting about these challenges is less that they exist and more that to deal with these things humans end up actually playing a big part of the process so humans come in and they start having to plan for failure you know how do we want to actually run our application so we can plan for failure and here's another email internal email from Twitter where I've scrubbed again some names and the important part is really at the bottom where it says you know someone is saying hey preferably this thing I'm trying to run is on different racks switches and power feeds that they are fully redundant writes this is humans basically effectively doing scheduling of applications to try to figure out how to deal with and plan for failure for the challenges they haven't run stuff and utilization is even harder because it's one thing to think about how we can share machines between different app pull machines between different applications it's another thing to think about how we can actually share a couple of resources on an individual machine so an individual machine can be truly multi-tenancy running applications from from from multiple applications on the same machine at the same time so in about 2010 at Twitter we looked into actually getting a cluster manager not having cluster management being done by humans but actually having soft lock software cluster manager so cluster management to me there's really two things that really identify what cluster management is all about and the first one is its when you do cluster management via software you start to stop treating your machines as pets but you just treat them as cattle they're arbitrary machines you put a base a small simple base operating system on all of them and ultimately you end up running these containerized applications where containerized application might be a full-blown docker image or it might just be a tar ball or a jar that has everything that you actually need okay and the second really most important thing is that you start to automate everything you're doing the in the data center with software and not humans so you deal with you let software schedule other software or you deal with you handle failures and pure utilization management and all that stuff you do it you do it with them with software one of the things that I think is really interesting about cluster management is that it's not an old sorry it's not a new topic at all it's a very old topic there's been a lot of cluster managers that existed some of the ones some have still been been used since the early 1990s things like Moab torque PBS portable batch scheduling system one of the reasons for that was that in academia the kind of applications that were being built and run in academia were things like message passing interface where they wanted to run that in a large number of machines whereas the kind of applications we were running an industry tended to be some web servers maybe some other one-off servers and in academia they already had hundreds of machines that they were able to work with to actually run these things and in industry were kind of working with ten tens of machines and ultimately this led to a bunch of cluster management like technologies being built and sort of the academic and the lab space around the world and in industry we kind of use SSH or other tools that we developed which helps solve the problems like puppet chef Capistrano and ansible but these days what I think is interesting is that we're all converging more and more both between academia and industry on running similar numbers of machines we're all running more and more and more machines and in general even if we're not running as many machines in industry as some government labs or super super computers are running we're running enough machines that it's enough of a pain in the butt to actually manage them ourselves now one of the things that that's especially interesting about cluster management as it was done in excuse me in academia was that the clusters of managers they were really focused on batch computation so they were really meant to make it easier to run batch computations so what what what we did at Twitter was we ended up building out a new cluster manager called Apache mesos which is really a modern a general-purpose cluster manager that's not just focused on batch computation but it's actually focused on running any kinds of distributed systems or applications containerize workloads whatever it is that you want to run so what I mean by that is I mean that maze was designed to run these kinds of distributed systems as native applications on top of maysa now I'm not going to spend too much time in the presentation at least not right now right now talking about what it takes to run something like these systems on maysa but I'm gonna spend a little bit of time talking about specifically the kind of system that we built on top of meso s-- to run but the twitter application which is all these different micro services ok so sure right so what was particularly interesting about that the the twitter application is if you kind of looked at this picture again there was a bunch of things that we wanted to run which were these stateless services that kind of sat in between the stateful stuff the storage applications and kind of some of the very very front end stuff and there are tons of them and they was growing every day because new new developers in the organization were building a new micro service all the time ok so we introduced to meso so what is meso scanner from a hundred thousand feet so may suppose is a cluster manager it's got a master slave architecture so the idea is that there's a some collection of masters that run and they manage all of the they manage all the machines in the data center and they run an agent process on each of the individual machines ok now a really really important part of meso that distinguishes it from other cluster managers especially those of the past is this idea that maysa has this notion of things called schedulers that's connect to the Masters they register with the Masters and they're the things that are responsible for actually running the jobs and the tasks in the data center ok and I've got a couple of other slides later that talks a little bit about about about how this works okay so the scheduler was a really critical point that really really differentiates it as an architectural technology from some of the other ones ok so again we're getting back to these stateless services so what we did in in at Twitter was we built a very specific scheduler that we run on top of May sews a service scheduler so a service scheduler that we built was really responsible for be able to orchestrate all these micro services that people were trying to run in the data center ok so I'm going to differentiate here now what I mean between orchestration and scheduling so when you build something like a scheduler on top of mesos there's a programmatic API that that piece of software uses to actually communicate with May so it's in order to run its applications in order to run its tasks so a service scheduler is some software which communicates with other software may sews in order to actually run its applications above that the service scheduler exposes a higher-level API that either a human consumes or some continuous integration job consumes and that's where somebody would say write a specification about the kinds of jobs they want to run and then submit that effectively say to the service schedule here orchestrate this job for me and then it uses mesas to actually schedule those jobs ok so one of the orchestrators that I want to talk about first is one one that we built at Mesa Sphere called marathon so these kind of these kind of service schedulers they've got a couple of roles that they have to focus on but the two that I'm gonna I'm gonna explicitly call out here is really you know how you're gonna do your configuration and package management to actually run your application and and then you know what actually what does deployment actually look like so if you use something like marathon on May so so as a developer what you do is you go build your application and then you containerize it so again you either containerize it by sticking it in a tarball or using something like a like like docker to actually create an image a docker image that you can then then run later okay after you do that then you have to put the bits that you want to run someplace so you take out your towel bar your docker image if you're on something like AWS you can throw it in s3 if you've got something like HDFS running or some other distributed file system you can throw it in the distributed file system and then or if you're using docker of course you can you can use the registry ok so that's that's kind of the first step you do configuration package management when you're trying to run services so the second step is then Zen deployment and what this looks like is that again the developer goes and they describe this application that they're trying to run when in marathon using a JSON specification you write out that JSON specification and then you submit that jason json specification to marathon which marathon then uses to orchestrate the actual running of your application by communicating and scheduling jobs directly with meso s-- so in marathons casters are there's a rest some rest endpoints you can use there's also a CLI okay and this is just kind of a quick example of what an example a running a talker container of iya marathon looks like here's the JSON spec and it's pretty pretty simple you effectively are describing the container if there are particular volumes local volumes you'd want to get mounted in you know resources you're consuming and then finally at the bottom the actual command that you're going to run okay so one of the things that's really interesting about what we're doing at Mesa sphere and the way that the maesters was actually built is that there are different ways in which you might want to orchestrate your containers and that's a great thing so in the same way that you can use something like marathon to run to run run your containers on top of mesas you could also use kubernetes so I know the kubernetes talk was just before me so so that's great so in that case the way it works again the two things that as a developer you have to think about is how you gonna do your configuration and package management and then what's your deployment look like the biggest difference here is that for configuration and package management you're are just creating docker images or soon app see specs and then you're ready to write in your deployment script also in this case in JSON and then submitting that to kubernetes and then kubernetes is communicating directly with maysa again to schedule those those those tasks throughout the structured data center okay so one of the things that's really interesting about Mesa in the way that Mesa post is actually built is this idea that you could actually run multiple of these schedulers at the same time so across the same data center you could both be running marathon and you could be running kubernetes so why on earth would you possibly want to be doing something like this so there's a bunch of reasons why what why you might want to do it the people that we know that are doing is because they've got some part of the organization that has decided they're going to be building some things using one one specification language and another part of the organization that's going to be using a different specification language from our perspective we kind of see it like browsers there's Chrome and there's Firefox and there's internet explorer I think and if you guys want to use any different kinds of these orchestration layers on top that's great for us so one of the specific reasons though why a lot of people end up using something like like basis here is to actually is when you actually want to run multiple of the same instances of of a particular scheduler on top so there's a bunch of reasons why organizations do this as well in that place meso ends up kind of becoming like a private cloud for your data center either because people want very very hard guarantees to different organizations for the individual service schedulers that they're providing on top the other one that we've seen people do though as well it's just because they want to run multiple versions of it so they'll start running one version of the system and then they'll start running the newer version of the system and once that one's hardened then they can just start moving everything over to the newer version and this way you didn't have to go and spin up a whole new cluster to try to run the new version you could just use whatever where available resources in the existing cluster that you'd already brought up so from from the the mesas perspective the thing that we see more and more often though is less people running multiple schedulers this way and more people running multiple schedules this way where the kinds of applications that they're building on top and running on top are things like say analytics stateful services and so now I'm going to go even deeper into the presentation and into how my source works to describe how this stuff all how this stuff all fits together ok so we're going deeper so I've got a couple slides where we go deeper and at some point we get pretty deep into how may Sosa actually works and so I hope I don't lose everybody but I think it's really interesting stuff when it comes to the actual architecture of the system ok so going deeper multiple schedulers so in order to actually make this work in may sews my sauce is built around a concept called two-level scheduling so maysa itself has really influenced by the multi-level scheduling ideas that existed in traditional host operating systems for things like user level scheduling and scheduler activations and in that way was really designed less like a cluster manager and actually more like an operating system kernel and that's actually why we call Mesa as a distributed systems kernel or really a data center kernel so what do I mean by that well as a data center Colonel these schedulers when they actually want to run their applications they use this syscall like API programmatically to actually launch their applications to actually run run whatever it is they're trying to run in the data center so what's interesting about putting this level of indirection in-between the schedulers and and all the resources and in the data center is first well it makes it easy for us to actually run multiple distribute systems at the same time right and if you again if you think back to some of the challenges that we wanted to overcome with software it's that software can schedule other software software can manage software we can actually drive up the the resource utilization in our in our cluster and one of the ways in which we can do that is by having this level of indirection which is may suppose in the data in the data center to go to run multiple to distributed systems at the same time and do it dynamically okay so the other thing though that we really get by sticking the slit level of indirection maysa in between the the schedulers and the the machines themselves is that maysa can now start to provide common functionality primitives that for the most part every new distribute system that gets built ends up reimplemented right and and by primitives one of the things I'm talking about so exposing up concepts like principles users and roles exposing up advanced first level scheduling and and fair sharing algorithms enabling things like high availability to be done more easily for the distribute systems that are running on top doing things like resource monitoring introducing concepts like preemption and relocation volume management reservations there's a whole bunch of primitives that we can actually provide the simplest ones honestly are just run this task somewhere in the data center you take care of getting this task from here to there and launching it and when it fails telling me when that thing is actually launched excuse me has actually failed okay so and one of the really interesting things when we were first building maysa was we were recognizing that you know pretty much everybody when they build a new distributed system they all end up reinventing the wheel and we really wanted to provide for from from the meso abstraction layer a way to make it so people didn't have to reinvent the wheel but they could build their new distribute systems directly on top okay and I just I love this picture I just love the idea of actually trying to use a bicycle like that okay so the other thing that that's really interesting about actually building your distribute systems on top of maysa is that it makes it it can make it much easier for people to actually use your your software so what do I mean by this well we're probably all familiar with stuff like this we go and we try to use some distributive system that's been built by some organization and it's so complicated to use and so complicated to set up and the software can't do a lot of its stuff itself because we got to set it up on each machine if one machine goes down we got to come in and deal with the next pieces that we end up getting a whole industry of books about how to actually run run runs on the software so if you can actually leverage an API and you can actually build the software directly on top we can make it easier for people to actually run your applications in the exact same way that it's pretty easy these days to actually download an application on our on our desktop machines and and just run it okay so there's been there's been a quite a few of these kinds of distributed systems that have been built directly on top of mesas SPARC is a big data analytics framework Apache Aurora was actually the the orchestration a service schedule that we built at Twitter that we open sourced a couple years ago marathons the one I mentioned Cronos is a distributed prong with dependencies like scheduler so it's very similar to cron on a single machine except Chrono Cross your entire data center and then there's also been a whole bunch of these frameworks that have been ported on top of mesas so things like Hadoop storm Jenkins and we're actively working on making a Cassandra port or work really well and then of course mesas is actually being used at a bunch these organizations that are using a handful of these these two these two different distributed systems on top obviously I was chatting about about a Twitter there's been other organizations as well the one that I'll call out more recently as Apple Apple actually recently announced that they using a supposed to run all of Syria on top and they actually built a brand new distributed system on top of mesas to manage exactly how they want to run run Syria internally okay going even deeper okay I'll get some water okay so going even deeper so the way mace was actually works under the hood so in in in Mesa if you're actually building one of these distributed systems on top or the the actual the API the the the API that most people end up using is what's called a request offer API and so that the way it works is that when a scheduler wants to actually run an application they can first make a request for resource you can say these are some resources I would like to use to actually be able to run my application okay and a request is really it's a simplified subset of kind of a specification for what it wants to run it's just the resources that it needs at that point in time okay now what what the what maysa then does is it takes all the requests in the system and it takes whatever other information that it knows about and it creates what are called offers and offers are effectively allocations that it makes back to schedulers so based on on on what some schedules might need to run their applications the meso smashers specifically the allocator and the mesas master says okay great here's some resources I can allocate to you so you can actually use these to run your applications and of course we're not just making a single offer or making many many offers at the same time so what the schedule then does is it uses these offers it uses these allocations to decide what tasks it actually wants to run and this really is where this two-level scheduling comes in so if the first level may sauce is really making the allocation decisions about about resources to individual frameworks it could be the same kind of framework just multiple instances of it and then and then those those frameworks specifically the schedulers and the frameworks are deciding what what tasks they they they actually going to run what jobs they're actually going to run based on the available resources that they can use okay once once a they pick a task that they want to run and they say ok this is a task I want to run then they'll submit that task back to to emmaus and say here's the I want to run and again this is where most distribute systems end up having to build this piece in where they they you know if they want to run some tasks on some other machine they've had to build in the componentry to get the tasks to the other machine have the Machine take that task and actually start start to run whatever it is that it needs to run in this case they can just say here here's my task run it and go okay so it's a couple different ways you can actually run run tasks in May sews so you can just give a task with a command in arbitrary shell commands and depending on how you've actually set up the base image on your data center that command may or may not work if you run a test with a command it's pretty simple says here here's the task I want to run theirs again as I mentioned there's an agent process on each of the machines that gets the task when it gets assigned to it and then just just what launches the task okay and of course you can run multiple tasks on the same machine you can also though you can you can launch tasks through what we call an executor in Mesa so it's this extra level of indirection that lets that lets you as somebody who's trying to build a distributed system be able to specify exactly how you want your tasks to actually run so the way that works is again you go to launch a task but you specify an executor an executor is is the first thing that we actually execute and then we past attach to the executor and the executor can decide how it actually wants to run its tasks and again an executor kind of multiple tasks so the reason why this is really really interesting is because it allows you to actually send tasks to something on an individual machine that don't require a complete fork exec so if you wanted to say build a distributed system where you want to just do more things by say spawning threads or allocating memory you could do something like that in this model so we specifically wanted to build something so if you were trying to build a sophisticated distribute system which didn't want to have to completely say 4k exec to do a very very small task like say pull something off a queue and process it that you could actually do something like that okay so again if you're running if you're running with executor x' that you can also be on the same machine running just just top level tasks okay so what we end up doing on each of the individual machines is we isolate with a container we isolate the individual executor Zoar tasks okay so what this looks like is here we've launched one executor it's got one task running a one top-level task and we've put a container around each of these different things so that they're they're isolated from a resource perspective from using one another's resources and the technology that we actually actually use to do that is the same technology that's underlined docker linux control groups and namespaces so before docker existed this is the way that we did it and that's one of the reasons why I mentioned at the beginning a lot of people that use Mesa sauce in the early days they just had tarballs or jar files because there wasn't a whole image that they needed they just had their bits in that form we constructed the container around them and then they unpacked their tarball okay one of the things that's really interesting about this model though specifically with with executor z-- is that since we put these containers around them you can easily pass more tasks to the executor and as we actually pass more tasks the executor will scale up the size of the container dynamically so we'll actually add more resources to to the container so for those are the really familiar with vertical versus horizontal scaling it actually lets you do vertical scaling directly on the machines as well as of course scale to other machines okay and of course as the machines go down we actually vertically scale scale the containers down okay and then finally of course if you don't you're not sending us a tarball or or or gzip you can send us a docker image and we'll just run the docker image directly okay so one of the things like that that comes up a lot when people really dive into the depths of how am i s'posed works is what is high availability high availability actually look like in the system so it was designed from day one to let all the different components of the system fail independently and there's really not that many components the core components again as I mentioned are the Masters and the slaves and the one other component is zookeeper which we currently use for a leader election but those are the three three core components you need to actually be running a mesas cluster so when something like a master fails however the scheduler will just get rerouted through zookeeper to communicate with another master that's elected and all the tasks that have been run everything that's been launched will just keep keep running okay the same thing with a scheduler if a scheduler happens to go down then another scheduler can be elected and and it will reconnect to the Masters and it will find out about any of its tasks that might have say failed while it was down so this is actually a pretty pretty interesting a pretty interesting thing that we built in when we first built in the notion of scheduler failover we actually gave it a time out that you could specify when a scheduler first registered to decide you know how long you wanted to wait to potentially fail over and if that amount of time elapsed and we would just clean up for you and we would kind of kill all of your all of your tasks under the covers so what was really interesting about this was the first time that we put it in place at Twitter when we were running the the Aurora service scheduler on top I think we had I think we had like a five-hour failover timeout and then we had a we had an outage that was like four hours and 30 minutes and so we almost killed every single tasks in the cluster because we didn't have the scheduler reconnect to the master within the five hours and so the failover would have been elapsed when he would have killed everything so then after that we said oh well let's make it 24 hours so we bumped it up to 24 hours and then a couple weeks later we had a failover we couldn't get the schedule to reconnect within about twenty two hours and after that we just bumped it up to double max so at this point the way scheduler failover really works for most people in organizations is either always keep it running or when it when it first disconnects shut down all my tasks that are running but still we have the functionality and and and and you can decide what what you want to do there so the last type of high high availability that we have in the system is what we actually call slave failover so again we've got this agent process that's running that's managing the slaves and we actually have the ability to let that process that manages the tasks go away and if the tasks if that process goes away and a new one comes up then it will refined or it will reconnect any of the other tasks or any of the other containers or any other other docker containers that it's actually launched so this was really really critical to us because the kinds of services that people actually want to end up running in production are things like memcache or Redis or other services where if every single time that our process died we actually killed all the containers that were running that would be really really unfortunate now of course we don't want our process to die very much because we're going to make it as robust as possible but there's one time when we're always going to be bringing down the process and that's when we're actually doing maintenance or an upgrade of the maysa layer itself and so what we were running into was we're running in substitute to the case where we would want to upgrade may sauce in the data center and what we'd have to do is we'd have to go to a machine we'd have to drain the tasks off of it effectively kill the tasks and then upgrade the you know upgrade the machine and so and then put put put put put put the new the new binary on and then you know stuff can get rescheduled on that box which is fine when you have a handful of machines and when you start to get in the hundreds of machines to guarantee the SLA s that you want for when you start killing tasks and have them migrate to other machines it can start to take days if not weeks if not months to actually upgrade a cluster so this was a pretty critical feature we put in okay so I'm going to go a little bit further down the rabbit hole but I'm gonna I'm gonna skip ahead a little bit because we get pretty deep and we get pretty deep in the rabbit hole with a with the different different mesas features so I'm gonna skip talking about how we actually do our first level allocation and I'm gonna jump to some other higher level concepts okay so we've got some other great great great concepts in Mesa sauce that we've had to introduce specifically for running this stuff in production the couple that I want to mention briefly are the first couple ones I want to mention is really resource reservations so it's this basic concept that you can go in and you can say you know what for some frameworks that are running for some schedules that are running I actually want to make sure that resources are guaranteed to be allocated to these frameworks so you can get guarantee so you can you know feel good at night when you go to sleep and you know that there's gonna be a bunch of failures that's at least if there's this many resources this many resources will be guaranteed to different applications so the way reservation is actually working in Mesa is you can specify the reservations on the individual machines and you can say you know I want to create some pre-specified container sizes four for some of the jobs that I want to run on top so you can do that statically which is great what that really gives you is this notion of strong guarantees but the unfortunate part about it is that it has to be set up by an operator when they're starting when they're starting the slave for the first time and it's actually mutable because once you set the reservations you actually have to drain everything off if you want it you want to change reservations so what we've recently introduced in maysa is this concept of dynamic reservations which are pretty cool it's this basic idea that when a framework when it when a schedule inside a framework is trying to run an application the first time that it gets resources that are allocated to it when it goes to actually run its tasks it cannot just say launch these tasks but it can also say reserve those resources for me so I'm guaranteed that those resources will be reallocated to me no matter what after the machine goes down for maintenance after I fail after there's a network partition for a long period of time and eventually you know you shut my process down whatever it is so once we actually get the the resources and the tasks to the individual machines we do the reservations there so reservations are great because they provide the nice guarantees that that we all want when we're actually running in a production setting but really at the cost of utilization so you know when you end up having a big chunk of resources that are reserved for for a particular for a particular organization they might be using very little of those resources which kind of goes against the whole idea of well we're using a cluster manager to actually be able to drive up to drive up resource utilization sort of to deal with that we and this other concept in may suppose that we actually call revocable resources and the concept point wrote revocable resources there's resources that can be used by different frameworks on top that can actually be taken back from the schedulers that have launched tasks with them so it this ends up introducing a really really nice property for people that are trying to build new distribute systems which is the way that they can think about preemption in their head is that preemption actually occurs through Reve occasion so we don't have the notion of priorities in the system we don't like guess who's a slightly higher priority and what semantics that's going to mean it basically means that if you're using resources that are either reserved to you or not marked as revocable we're not going to take them from you and if you're using resources that are revocable you you it's it's highly likely that your tasks will be preempted to get their resources back okay so one of the really really cool things that we're actually working on with the real cool resources right now is over subscription so it ends up happening and a lot of these organizations is they start running their tasks and users come in and they say my tasks requires 8 CPUs and 8 gigs of ram and it uses 2 CPUs and 2 gigs of ram and now we've got all these resources that have been allocated but are actually not being used so what we can do is we can take that remaining 6 CPUs and 6 gigs of ram and we can offer them out out to to other things that are trying to run in the data center as revocable they can use those resources and of course if that first application that was trying to use all 8 CPUs and 8 gigs of ram scales up then it'll actually get the resources back okay so again excuse me this reeve occasion guarantee that we really provide though which is that the tasks will not be killed unless I'm using revocable resources it's tough to always provide that one of the trickiest places when when it's tough to provide that is when we might want to say defrag the cluster because the way that stuff has gotten scheduled over time it leads to a less useful utilization than if we could actually pack some things together and shut some machines down or specifically when we aren't when we want to actually do maintenance so some of the last primitives that we've been introducing into most recently our primitives we're actually doing deallocations so that we can we can introduce maintenance as a software concept in the system so you know one way in which we can actually do maintenance is we can again go around and we can kill machines or shut down machines or drain drain drain uh tasks for machines and then they won't know about it they'll just think that there was a failure and then we'll go from there but we could actually do better so we got this really interesting concept in and may sauce that we call an inverse offer where an offer is an allocation of resources that's made to you an inverse offer is a D allocation of resources where we say hey we'd like these resources back specifically we'd like with these resources back at this period of at this point in time and for this period of time because say we're going to shut down the machine during that period and so you can't run anytime on it okay so so that's pretty cool we can send out an inverse offer and then the framework can say great yep I acknowledge that you're gonna take my resources away and I'll shut down my resources so that lets us introduce something like maintenance in a really programmatic way so we start off by sending out inverse offers then we actually once we've sent out all the the inverse offers we can continue to send out other allocations but just mark the resources that are that are for use there as revocable and then finally once all the tasks have stopped running on the machines we can remove them from the cluster and do whatever maintenance that we actually want to do okay so you know this comes up often when we talk about cluster management but what about my persistent data how do I run my stateful services on on cluster managers and so for that we also introduce a concept recently we called persistent volumes so the idea behind persistent volumes is again that one of these frameworks through the programmatic API so software can go and it can create persistent volume for for whatever tasks it's trying to run okay and so it does that when it gets an allocation of resources through an offer when it goes to launch any tastic and say by the way I also want you to create a persistent volume for this task on this machine and of course tell me about it in the future if the machine ever fails or my task fails or any of those other things happen and again we propagate the persistent volume information all the way down to the slave so it can so it can record that information and we can deal with all the failure scenarios that I talked about earlier so what persistent volumes end up looking like on the individual machine is just like kind of dynamic Reservations have preset out containers we have these containers set up with things like volumes that are preset for those containers so when a task goes to run it can actually use any of the persistent volumes that have been allocated for it but then of course that the task fails then then the persistent volumes going to stick around and the next time that you try to run a task using that persistent form it'll still be there so I think that the bigger picture of a lot of these features is that we're trying to build these things in specifically so we can run long live stateful frameworks so not just all the state list services that I really talked about earlier the things we went after an attack when we first started doing this at Twitter but the stateful stuff as well and really this combination of reservations inverse offers and persistent volumes really gives us the primitives to do something like that okay so just a couple more slides and then I'll take some questions so you know these days it's a pretty fun time all of our other computers are effectively data centers we can spin up data centers worth of resources very very easily on any of the public clouds or we just have them at our disposal at our existing organizations and you know here by data center I really just mean some collection of physical or virtual machines that were grouping together and and and treating as as one one big data center computer because really the data center should just be another form factor just like our laptops a form factor and and tablets a form factor and the the cell phones are form factors and if maysa is really this data center kernel that we're trying to build the natural question is as well what's the data center operating system look like and that's actually what we're doing at Mesa sphere is trying to build what we call the basis for data center operating system so to look at that stack to look at everything that I talked about earlier in the presentation what that ends up looking like is you've got this kernel like layer that we call meso s-- and then you've got sort of this API layer on top and then again you can either directly use something like marathon to launch your apps or Chronos to launch your apps or you can go and you can build your own scheduler and distribute system by using the maysa SDK on top and one of the nice things here is that you know when you end up when you end up building your software on top of this you're abstracted away from if you're running on bare metal or if you're running on on any of the clouds and again as I mentioned before marathon really it ends up being sort of the init for the data center operating system it's the thing that you use to launch your first task and Chronos really ends up being sort of the cron for the further for the data center operating system ok one of the things we built when when we're at whenever actually building this this is open source the the DCOs CLI so the first one the first things that we actually built was a CLI which is just for interacting with this data center computer obviously data center operating systems operating systems need user interfaces so the first thing we built is the CLI and it's pretty cool you can hook into it with any of the the distributed systems you're trying to run on top like something Kassandra and you can actually do something like DCOs package install Cassandra and actually get a implementation of Cassandra running up on the data center without having to actually touch anything which is a which is very very cool and at the same time we've got some even tighter hooks in our CLI with certain applications things like spark as well so if you want to run a spark job you don't have to install spark you can just go ahead and do a DCOs spark run - run run spark applications directly on on your data center okay so I've spoken for about 46 minutes I'll I'll stop now and I'd love to take any questions yeah so I'll repeat I'll repeat the question if everybody heard so I talked about actually being able to do vertical scaling on an individual machine and then obviously we can reach reach a peak of our resources so you can't you can't just arbitrarily take more resources on the machine and you have to actually schedule tasks inside of the container that you've actually launched to to at for us to actually vertically scale up the the container so if there are no if all the resources are actually allocated to other containers or other tasks or other executives that are running then we won't they won't be allocated will they they won't be usable so you won't actually be anything inside that container so the resources are accounted so the TLDR is that the resources are accounted for in exactly the same way as if they were running outside the container we just let you run the task inside and then scale up the container from there does that answer the question I I see so so the question is is could you if the if the resources are heterogeneous which by the way is almost always the case that there's a heterogeneous collection of you know basic machine shapes can uh can you and said then the question is can you if you're running a machine can you kind of like so you usually what I don't what happening is if you're running a task on a machine and actually you wanted to vertically scale-up but you just could never fit on that individual machine you'd have to shut the task down and then run the task on on a different machine but if there the resources available you can vertically scale up and at the same time once you've used those resources you can vertically scale down so the applications which end up taking advantage of this the most tend to be things like analytics continuous integration things things that are you know compilers that are building things oftentimes they're these shorter lived applications that excuse me could take advantage of the fact that you've already started doing some computation someplace and so if you can just run some more stuff there right now you're gonna be able to do it far faster then then if you were to bring up another one horizontally someplace else and then and then go from there and so that's that's that's specifically why we both we both of things in so things like spark for example explicitly take advantage of this one it runs on meso s-- and when you go to run another task and spark RT has some of the data in memory on that particular machine it'll run the task on that machine it'll vertically scale-up it'll use the stuff in memory and then when it's done and over to scale down and and then you know the resources are free again and you know that's saves you a significant time and running your jobs and bring it up someplace else other questions great question how do you prioritize different workloads on the same mezzos cluster you spark jobs less prouder than marathon opera perhaps also within the scheduler okay great so some of the slides I I skipped in the rabbit hole part was specifically how we do the first level of allocation and Mayo's and that first level allocation is how we choose what resources to allocate to some of the different schedulers that are running on top so the the basic idea in the system is we the first level scheduler the default first-level schedules we have is something that we call a dominant resource fairness and and it's a fair sharing algorithm but it has weights so you can actually attach weights to things like a spark instance or Hadoop instance or marathon or kubernetes or whatever it is that you're trying to run to to preferentially give those frameworks that are running more resources so you know for example you can in fact you can run three instances of something like like spark and you can run one instance with a really really high weight and the other ones with really really low weights and and what that effectively enables is you know the one with the high weight can kind of be your production one which always gets the resources and the one with the low weights can just be the you know ones that you throw away so this is only part of it though because there can still be circumstances where things get scheduled in this fair sharing way that doesn't actually meet the the needs of the organization and that's specifically why we introduced the concept of reservations is because reservations really let's an operator that's running the cluster be able to say to some part of its organization I want a guarantee you're going to get at least 300 CPUs and 400 gigs of ram and so I'm going to put this reservation in place or you're going to I'm gonna put this reservation place but you're going to fill it up dynamically based on when you actually run things and effectively what that does is it just gives better guarantees than then you know fair sharing with weights so that's why we've introduced those other concepts yeah I and a lot a lot of the the things that have influenced these decisions have specifically been going and looking at the way our traditional host operating systems perform these same roles and then and then introducing concepts that are amenable to the data center environment to the distributed system environment or in some cases are exactly exactly the same you 