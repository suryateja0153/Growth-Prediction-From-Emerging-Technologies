 all right the third speaker for the first session is I mean by that he received his PhD in the now project also in the nineties right yes mid-90s he distinguished academic career in Duke and San Diego and has been at Google for a few years now where he is a fellow and the technical aid for networking he had a tremendous impact on datacenter Network and he'll tell us a lot about this here's a man telling us about the future of networking okay it's a real pleasure to be with you all here today I spent nine years here at Berkeley as undergraduates and as a graduate student in all sincerity I equate nothing more with UC Berkeley than Dave Patterson so that's one of the principal reasons why I am so happy to be here well I will I promise talk about networking but I'm going to take advantage of standing before you in front of this bully pulpit to give a couple of anecdotes about Dave he's been a real inspiration to me throughout the years yes about systems design I mean the principles of letting analysis data guide what you do letting the trend be your friend seems obvious to many perhaps sitting in this audience but it's the one that's probably hard-won and not popular enough in the academic community I would say but beyond systems design I actually often find myself thinking what would Dave do in many many different kinds of situations so let me give you two examples that have come up one is always being willing and able to learn something new I actually on the now project worked on operating systems I was an operating systems graduate student I showed up my first day assistant professor on the new job and they said I mean congratulations you need to teach networking so I don't know anything about networking I'm an operating systems person I said you're as close as we got everyone wants to take networking we have someone to teach operating systems so I bought the textbook I read it the day before usually showing up to class and I showed up super enthusiastic every day boy let me tell you what I learned about networking folks so and and that has served me well I think over the years on a different notes I have somebody working for me fantastic insightful super bright always has the right technical sort of assessments and person always great still could not write an email to save his life always would come off wrong sometimes come off mean sarcastic etc he got his PhD in computer architecture his advisor sitting in this room and I actually sent them an email I said I want you to write every email like you're writing with the Dave Patterson and he is a man transformed it is it is a true story so so thank you day for lessons on many many fronts being willing to learn new things treating everyone with respects and being able being someone who I could invoke to get the right level of respect out of other folks okay so with that let's talk about networking it's actually a super exciting time to be in networking and networking of course to me is an enabler for computing so computing is at what I think is once in a generation sort of shifts we're at the beginning of it but I think we'll past the tipping point I being biased I think the last one happened around the network of workstations projects moving toward PCs workstations commodity parts to deliver capacity compute capacity we're now past that 20-year trend and moving toward the next one so I'll begin with talking about computing and talk about how networking again in my biased opinion is going to be playing a very important role in defining what computing will look like over the coming 20 years so computing is at a crossroads I think that's if you were to transport an undergraduate back in time today to about 20 25 years ago when I took my undergraduate distributed programming course distributed systems course that undergraduate wasn't be so shocked they wouldn't feel out of place they wouldn't think hey this is so arcane and this is to say that despite literally a factor of 10,000 improvements in computing and networking performance the way we're building distributed systems is mostly unchanged in fact what we've done is we've layered more and more let's say cruft on top of the baseline hardware such that we're certainly not delivering a factor 10,000 difference in performance given that the underlying hardware supports it at the same time the free lunch in performance improvements is over and many of you remember the good days not too long ago when all you have to do is wait 18 months and your code and compile unreconciled would run twice as fast those those were good times those days are also over we had a great talk from Garth I think we'll be hearing more about this storage capacity has increased dramatically but the i/o latency gaps continues to be there in a big way we've actually been assisted that the dates we've had this definitely slow 10 millisecond access time for discs and the reason that's been good for us is that we've been able to disaggregate discs we can at the level of an entire building make all the discs look like they're local to your computation ten milliseconds is an eternity with this new generation of storage devices that are coming along the networking and compute communities are no longer going to have that luxury of being able to deal with 10 millisecond devices anymore and that's going to require some new architectures moving forward ok so that's one big trend in computing as Dave mentioned and we're not gonna argue with Dave today the cloud is the future but let's talk for a moment about what that future might look like first let's go back about 10 years with the emergence of the cloud what I'll call cloud 1.0 here for enterprises building their own private data centers virtualization allowed us to reap substantial capital expenditure savings what I mean by that is that you could take workloads that used to run on say a hundred servers and multiplex them down collapse them down onto 10 right they could get their own operating system image their own configuration they would run isolated from one another and this was a big win where we are today call cloud 2.0 we can now largely outsource the construction of the hardware that it takes to run our computing workloads and we know that takes substantial expertise to build efficient hardware infrastructure data centers I mean basically the large providers of computing infrastructure can do it from a power cooling commodity's of scale perspective to x3x more efficiently than smaller deployments you also get just the right amount of hardware on demand so you don't have to buy the for the max that you need you can basically pay for what you need at any given point in time and this has also been great at the same time the compute model is the same how you configure your systems is exactly the same you pick what operating system you want which patches you need to apply you figure out how many instances of your web server you need your database you need how are you going to do load balancing across them how you going to schedule things in different places you know I could go on and on and on the bottom line is that how we do computing hasn't changed it is true that we now have the benefit of not having to build our own hardware infrastructure and that is a big one well I think we're going is cloud 3.0 and here the notion is to be able to focus on computes not on servers and it doesn't matter if those servers are virtual or real we need to move away from having to think about having servers so we need to move to a model with server lists compute not on where our data is placed how we're going to do load balancing among different servers how we're going to patch the operating system what our security policy is for access control to individual servers etc we need to be able to focus on writing our application and having it scale out scale up down etc rather than on configuring the low-level Hardware so from a networking perspective we should be aiming for cloud 3.0 we in the networking community I think are pretty stubborn Lee also stuck with taking stuff that we know from private clouds and bringing them into public clouds so now rather than having a firewall that's a physical box that you buy you get to buy a virtual firewall you get to buy a virtual load balancer and I'm not sure that's real progress from the perspective of realizing the future of computing ok so let's take it back to networking and cloud 3.0 given the direction that I think we need to be going what are some of the things that networking can be doing storage disaggregation we talked about this I think we've been largely successful with disk right so we have all these bytes most of them are never accessed that's good when they are accessed we have 10 milliseconds minimum and often more with some queuing etc to access it so the network especially within the data center can get to it but with the new storage technologies that are here today flash and neck nvm we're not gonna have the luxury of dealing with pretty slow storage devices any longer what's the network need to do to keep up with that I'll go into some detail about that moving forward transparent live migration so the idea here is that you don't want to be thinking about your operating system needing to be upgraded and patched even though I started in operating systems I would argue that most people don't wanna be thinking about the fact that they have an operating system underneath the hoods this is gonna still have to happen there will be operating systems they're gonna have to get patched what transparent live migration means is that your compute moves from one physical server to another without you even noticing and no downtime now imagine that you have a compute instance with potentially hundreds of gigabytes of State accessing terabytes maybe even petabytes of storage it will take a pretty intricate and pretty fast network to enable you to move that state with hundreds of gigabytes from one physical place in your data center to another with no downtime no notice and basically full availability seamless telemetry here again what you should be doing is getting information about how well your service is delivering say requests to end-users not on very low level metrics that allow you to do load balancing decisions and placement decisions and finally an open marketplace we talked about App Store I believe that we need something similar for the cloud you should be able to essentially buy and instantiate the equivalent of an NFS server in your computing environments with a click of a button and that NFS server should be configured seamlessly access secure etc with all the right credentials and keys and data access without you having to figure out how to go and change every router every switch ok there might be virtual routers in it virtual switches but they're still routers and switches all those should be going away ok so to put this back up one level in cloud 3.0 then that work should be delivering applications not virtual machines you shouldn't be thinking about machines virtual or otherwise we should be focusing on policy what your security policy is in other words who has access to what data rather than how you configure your middleboxes and how you make sure that your packets flow through the right middle boxes to enforce security rules and the like actionable intelligence and even machine learning you shouldn't have to have creeks and queries about what your system looks like and how it's evolving etc you should have help with that and you should actually be getting insight into the behavior of your data without necessarily knowing exactly how to formulate the right sequel query and finally you should be thinking in terms of service level objectives SLO s basically what's your availability target what is your latency target not on placement load balancing and scheduling of underlying servers okay so all this is going to take a neat trick it's going to take the trick of at least providing the illusion that the network has disappeared and one path that we've been following for this is using software-defined networking to make the network more manageable more scalable and much less apparent from the perspective of the applications running on top of them so I'm going to give a few examples of this through Google's experience ok so compute is taking place in these very large data centers here's one picture essentially think of this as a building tens of megawatts of power with tens of thousands of servers Google's mission going back to the origin of the company was to organize and remains to organize the world's information and make it universally useful and accessible all the world's information is pretty big and is growing really fast so it was far beyond the capacity of any single server or tens of servers or hundreds of servers to go get at this data process it make it useful and accessible so by necessity we developed a fair number of pieces of distributed computing infrastructure to go through this data starting with GFS the Google file system in 2002 two systems like MapReduce BigTable and most recently things like spanner that perhaps for the first time show how to provide consistent data replicated across the planet in all these cases as all draw upon in a moment the common thread was we're going to have logically centralized control over the data and compute of an entire building and this ran really counter to the notions in industry prior to this time right can you think of all the disks in a building as constituting one big file system can you basically plug this in and out as you see fit servers in and out as you see fit and the file system grows and shrinks underneath the hoods from the perspective of the applications that are accessing it without them knowing without them having any downtime any data migration etc as new technology becomes available it also becomes transparently available and accessible to the applications ok all this innovation and distributed computing underneath the hoods was delivering and requiring sort of huge growth in data center network bandwidth requirements this is just one illustrative example that I have on this chart here it's over about a six-year period and it shows the aggregate growth in data center network bandwidth over that time period and the takeaway is that over the 6-year period we had a 50x growth it was a little bit less than six years 50x gone growth in data center network bandwidth we couldn't buy for any price network that could meet the demands of our distributed computing infrastructure so what we had to be doing at the same time was developing Network technologies that could keep pace with our distributed computing infrastructure and here's just the set of examples again won't spend time on most of them but we had to for example build our own content distribution network to deliver videos let's say across the planet watch tower Jupiter I'll talk about in more detail our own data center network infrastructure we built our own private wide area network to connect the data centers together called B 4 etc all of these in one form or another used the notion of software-defined networking and so and and they used software-defined networking before we had the term to describe what we meant but the basic idea is and really counter to how networking works prior is to leverage logically centralized control of all the networking elements either within a building or across the planet to provide the illusion of one big network that provides connectivity from port to port and enforces policy now how that's realized which switch which middle box which firewall which load balancer ideally and we're not to the ideal yet to be clear but ideally you don't know and you don't care the logically centralized controllers know what policy you're trying to get at with respect to connectivity what the bandwidth requirements are and it delivers it as new networking technologies pieces of hardware become available you plug it in and rather than discovering by pairwise message exchanged with neighbors what role those elements should be playing in the network they're told so one of the reasons I actually fell in love with networking was the beauty and the challenge of the protocols involved I mean the notion that you could have a network element in the Internet come up without knowing anything exchange the messages with its neighbors and then become a fully functioning member of this global community of forwarding elements was to me really really beautiful and yet it's a really really hard problem and perhaps an impossible one to solve think similarly in terms of congestion control we have this amazing protocol TCP that with that any state figures out exactly what is fair share of global bandwidth this and relearn's it continuously I mean this is to me again a stunning achievement but in both cases were solving really hard problems that are much easier to solve if you have a central entity that knows what the network looks like what it should look like and enforces that policy that's really what Sdn is about take the brains out of the individual elements have a blueprint for what the network looks like and continuously enforce that ok so let me get to storage and computes so let's shift gears a little bit what kind of network did we need to support our computing needs within the building and what I'm going to argue is that you need a balanced infrastructure computer infrastructure and it needs to be running at building scale y balanced so you've got CPU you've got disk you've got flash you've got RAM you've got network you have to keep all these things in balance for your computation to run effectively if some resource is scarce one of these five let's say you're living the value that your data center can deliver similarly while other resources are sitting idle because of that one scarce resource you're increasing your cost and of course both are bad so that's the argument for balance now what about scale what we found is that you actually benefit substantially from running your computation even if each individual computation is moderate in size maybe a hundred servers maybe fifty servers maybe 200 servers you get substantial efficiency wins of having a big domain to schedule across so here's the intuition imagine you have a series of jobs arriving and you can either schedule them in their entirety across ten 1,000 server clusters or the same set of jobs can be scheduled in one 10,000 server cluster without going through the details hopefully you can convince yourself that because of bin packing you'll get substantially more efficiency in our studies perhaps 30 percent or more by having those larger scheduling domain even for the same set of jobs okay so scale matters and balance matters how do we build a balanced data center here I'm going to go back to perhaps even before Dave started his career here at Berkley to Amdahl's lesser known law for systems balance and this says that you need a megabit per second of i/o for every megahertz of computation and parallel computing let's see how that plays out in one of our put typical buildings let's say I have a building with 50,000 servers that's not an unreasonable sized building today tens of megawatts and let's say I fill this building with number of servers 50,000 servers each has 64 cores two and a half gigahertz each core fortunately I'm not a computer architect so I'm going to multiply 64 times 2 2 and a half gigahertz and we're going to determine the bandwidth requirements of each of these servers to be about a hundred gigabits per second accord Dahl's law all io is remote right we don't want to have I Oh local because we wind up stranding a whole bunch of resources so flash think of a flash device is supporting a hundred thousand jobs per second with a hundred micro second of access latency right that's two orders of magnitude better than disk non-volatile memory coming soon finally in the next year or so is going to deliver a million plus high ops with ten microsecond access latency a million four kilobyte I ops that's about 40 gigabits per second of bandwidth so that saturates won't just about saturates one of these hundred gigabit per second links certainly if you want access to two non-volatile memory devices you saturate one of those links so doing the math 50,000 servers each needing 100 gigabits per second at least according to um dolls rule of thumb you wind up with the need for a five petabytes per second network in each of your buildings now maybe I'm wildly exaggerating the bandwidth requirements and some would say I am let's say we need a 10 to 1 over subscription ratio so you only need a tenth of that five hundred terabytes per second if you cut the internet capital I internet in half the entire bisection bandwidth that the capital I internet is substantially less than 500 terabytes per second so now we have this network need that is more than the size of the entire Internet and we have to put it into every single building that we're deploying for computation ok and latency of course is now key I've got these 50,000 servers and I want to make nvm be a building level resource for it to really be a building level resource you can't make it too slow ok with a 10 millisecond disk it's hard for the network to make it too slow or the 10 microsecond and VM device it's really easy for the network to make it too slow right if I've got this awesome 10 microsecond latency device in the data center it's pretty easy with queuing and everything else to at the tail at a millisecond of latency and certainly trivial to add 100 microseconds of latency so now I've made it maybe 10 times perhaps 100 times worse and now all of a sudden I'm not so excited about this aggregating it right now stick it into each server and access it locally but now we've gone backwards limiting our value because we can't build balance okay availability I'll go through quickly we're evolving the network all the time and we've got to be able to do so without taking down the tens of megawatts of infrastructure so I think for me the main takeaway here is that from a cloud 3.0 perspective from a disaggregation perspective from the perspective of blurring the lines between individual servers moving forward the network is going to be playing a disproportionately large role in defining the future of what computing means and that's one of the main reasons why I continue to be very excited to be working on networking since Christo's is already standing up and it's gonna step up the stairs pretty soon I'll skip the details of how we built our network we actually have papers published on this so you can get probably more detail than you like but I will wind up here to show you actually right here that our latest generation of Jupiter Network actually scales to 1.3 petabytes per second per deployments so right in that 500 terabit 2 5 parapet range and I'll reveal not so big secret that we're talking about our 2012 technology here and Jupiter not necessarily what we're doing today so these bandwidth demands are real and data center and they're continuing to grow at substantial rates we've applied these same ideas to our land to our network virtualization infrastructure before and Andromeda ok so I'll conclude here that we need to continue to work hard at making the network disappear logically centralized control in addition to merchants silicon closed apologies and decentralized configuration is a pretty compelling way to get there but that I'll stop and take questions yeah what impact do you see many-core having on this networking scene as we start to get up to say having you know 100 cores on a on a die so this is this is happening today and essentially the notion here is that the 100 gigabit per second let's say bandwidth requirements of individual servers is not driven by any single core and in fact I'd argue that basically no core running at two-and-a-half gigahertz really cares about that level of bandwidth it's really the multiplexing of hundreds thousands perhaps tens of thousands of individual relatively small flows that are driving the bandwidth needs from every server hi David Stern with mimics great talk I'm curious you didn't mention or I didn't catch very much about security or encryption with networking and I was wondering about your thoughts about the Feinstein BER compliance with court orders Act and you know if it goes in the wrong direction what are its Gogol's thoughts I guess on that so I'm gonna punt on the second question just because I'm not qualified to speak to Google's thoughts on the policy questions I'm not saying that to dodge I actually I'm not the right person to speak for Google in that context and I will ask the more general question in terms of sort of security and encryption requirements it is a huge driver for our network architecture overall and what I'll say is that since the beginning of time Google has been a big target for attack we're constantly under attack and as always I mean it's never a question of Hayes or an attack happening its how big and scary is the attack and some of them get really big and scary and it's gotten bigger and scarier as we've become a public cloud provider so it's it's a big big issue I didn't talk about it in detail but certainly it's a driver so you talk a lot about how much about aam dolls lesser-known law which actually we knew about 30 years ago because they've told us but that's why that's why I included it yeah there you go but but what I'm wondering though is that there's been a big trend to trying to move the computation to the data and your comment was we'll all of our storage is away from the compute nodes how would this met this requirement this demand dropped if you were to be more intelligent or could you be more intelligent about placing the computation so you didn't have to move data all over the place like you do now so I don't know about the trend I'm not sure I agree with the trend I guess we can do a pretty good job building general-purpose compute infrastructure and having a fungible pool of that across a building is pretty nice what I will say and I'm glad you asked the question is wouldn't it be great to have these partner devices be built from the ground up to be network citizens and right now we have to sort of retrofit them on our own to make them be network citizens even for disks okay was easy with this ten milliseconds but at the 100 micro second and 10 micro second level we're having to do a lot of work to make them first-class citizens so I noticed you had a closed Network that that private wide area network yes that was originally invented by the phone company can you say a little bit about why you're returning to that topology Oh close apology oh yes it was actually as a 60 year old topology has you notes and many many of the good ideas perhaps all of them in networking did start in the phone company so I feel no remorse in taking yet another one but the origin was building a non-blocking network technology so remember we need to get to let's say five petabytes per second the state-of-the-art prior to us starting down this path was to buy for the biggest switches that you could buy from non phone company vendors and stick them into your data center buying a bunch of small switches and arranging them appropriately in a clove topology turned out to be a great way to deliver non-blocking technology affordably I mean written gear from us ignite over the years a lot of us have been extremely excited that the promise of software-defined networking and I think it's fair to say that most people or most people who have tried to use it haven't been nearly as successful as Google and Google's been outstandingly successful with it can you speculate as to why that is why Google has done so well when others have done less well it's a great question I think that one one reason honestly is that when we started down the path we had a bunch of people and this was before I got to Google to be perfectly clear we had a bunch of people who came from a distributed computing background and try to solve a networking problem that's part one part two we had two rather large spectacular failures that never saw the light of day and they were on the slides they're in the paper the company was willing to stick by the approach despite the fact that these folks who didn't know networking we're trying to build a network that never existed before so that's the two sides on the positive on the flip side the networking industry as a whole if I'm a hasn't really been big on software there aren't a lot of software engineers working at many networking companies so now when you say I'm going to take classic problems in leader election and consensus and high-performance distributed computing and apply them to networking it takes a substantial DNA shift and one that through just luck and happenstance Google was in a better position to affect given the DNA of the company so you know I want to take you back to thoughts on Dave and a particular term maybe we didn't hear enough of which was persistence so if I can take you back to a paper called webOS this I guess a few years ago received the 25-year test of time from HP DC and I think that was maybe the fifth place we submitted the paper thoughts on persistence did we have it right from listening to your talk it sounds like we were close but we didn't go far enough back then so in that paper we had section seven called rent a server in 1996 when we started writing that paper and the idea behind rent a server I see mike devine in the audience as well was hey wouldn't it be great if you could transparently access computing power across the internet on demand and install your computation on it yeah we had the paper soundly rejected multiple times and the part that you might have forgotten David is actually was never accepted to HP DC the program chair invited the paper after after the so you know I I went on the job market without any first author papers and so I I know failure probably as well as most people in the audience I think persistence is probably another really important trait that I learned at Berkeley from Dave from all the great colleagues around me thank you for the question 