 so let's get started my name is nirav Knopf Kothari I'm a principal consultant with professional services you know I've done this I've done this quite a few times I've worked with a lot of and practice enterprise customers who are been through this journey or are about to start this start this journey one such example being migration of conde nast in 2014 you know right from assessment to shutting down a data center I did it in about 90 days 500 workloads little over a petabyte of data and there's a there's a very cool video on our website where at the end of those two minutes they put up a board saying for sale so why am I saying this I'm saying this because you know I've been there done that I've heard a lot of customers requirement more importantly a lot of complaints from the customers about when they embark upon a journey something like this something like a mass migration a datacenter shutdown you know it has been quite a bit of journey for those guys and I've heard it all and I'm going to share some of my experience outside of talking about the application discovery service of course I want to share some of my experience on what happens in the field and how other enterprise customers have done that so what we're going to do today the agenda that we're going to cover is we're going to talk about a typical cloud migration journey our do customers do it today then we'll spend some time talking about what they have told us what have customers shared in terms of their experience either of you know a complex migration or successful migration either ways think of that as wiser customers and then we're going to spend a lot of time talking about what we announced today the AWS application discovery service will try and leave about 15 minutes towards the end for Q&A we should be able to take care of most of the questions you guys might have out there so here's you know before we talk about my discovery service you know I want to build up why why our discovery service is is important what's what's the role that it plays so here's an here's an illustration by no means we're going to talk about migration of course you know doing that in next one hour is not going to do justice with the topic itself so we're not going to talk about migrations but what I want to baseline is here's an illustration taken out of what we call is a calf a cloud adoption framework if you don't know about cloud adoption framework you can go on our website and look at for calf at a very very high level it's a methodology it's a framework that Aidan is professional services consultants like me have developed based on our experience working with enterprise customers and it has got about nine perspectives goes into people perspective in security perspective in culture perspective and DevOps perspective and so on and so forth one of the perspectives are those nine is application migration perspective so this illustration is really borrowed from that from that book with some degree of variance most of the application migrations kind of follow that journey of course every case is unique every workloads different but at a very high level you know that's how that's the journey that most of the migrations do follow it typically starts with strategy you want to really go and define a strategy you know is it going to be my development workload first am i shutting down a datacenter and that's a strategy in itself am i adopting to DevOps type culture you know what am I really doing right so it all starts with a strategy then gets into the planning and discovery phase where you will go identify if data centers shut down there isn't a closure is one of your desired outcome of the journey what's really running in the data center you know what are you really going to deal with you know things of that nature a quick show of hands how many people have heard about six hours Gardner six hours yep sir Gardner published this this sixth hour strategy or what it what it where it goes in in saying is every workload will fit into one of those six bars and ours could be retain retire Rijo Sri architect refractor re platform so and so forth so think about the planning phase to really you know help you go identify every workload in there and based on the properties of the workload such as what's the type of data is it sensitive you know is the application super complex can him do horizontal scaling and is it homegrown and is it tightly coupled and loosely coupled and so on and so forth you will go in a line during the planning and discovery phase every workload to one of those ours right and each R will have a follow-up strategy on what would you do if a workload belongs to a particular R so for instance if we host is the strategy for a given workload are you going to use a tool such as AWS VM import to lift and shift an existing VMware will be K OVA into iwi something like that and then it gets into the build and run phase in build phase which are really doing is picking up one of those application identifying what's the appropriate are and then executing upon that migration you're going to light up that application into AWS and then you will keep doing it unless you're done with all of your applications all of your workloads and after each migration when you're comfortable when you are done through your smoke test you you ad or F eighty so on and so forth you will hand it over for operations you will hand it over to the support organization to go run it optimize it monitor it think of those red greens and yellows Nagi OHP open views right so so that's a classic journey of vibration the first two phrases if you were to think about it the strategy and the planning phase is usually done at a portfolio level ie if you're going to shut down a data center you're going to run a strategy in a planning phase for the entire data center you're not gonna pick one word or one server one application do assessment and then classify it and then do a migration and complete it and then go back and pick up a second application right that's not going to happen so the strategy of the planning phase usually are done at a portfolio level and then they build a phase you know is done at an application level or a workload level right so here's customer wise a lot of customers we've heard a lot of customers say cloud migrations can be complex and costly and it can overrun and it might need more resources and so on and so forth right so some of the things that we heard in the field are there are data center migrations can involve tens of thousands of workloads right some of them you don't even know probably right and if I were to ask quick show of hands how many of you can predict what's in rack number five today in your data center right some of you will but but it is a little complex right and then let alone rack number five in system number four you know can you predict what's running on system number four rack five and what does it talk to right there are so many tentacles that go out of that system some of it's not documented some of the documents not updated some of the tribal knowledge is gone the application owner resigned so on and so forth right so identifying creating a meaningful inventory you know it's difficult and then I have and top of that understanding all of those application dependencies quickly starts getting more and more complex right and then what if you know there's one of those legacy applications in there all right it's very tightly coupled if you change the DNS name or something it might break right things of that nature so and then the last one being you know putting together an inventory if you do it manually right you realize the problem you don't have a good source of information you know most of the CMDB projects have failed right you don't have a good CMDB you can manually go and identify all of this but it's very very time consuming and that's where most of the effort and cost overruns do happen right so some of those some of those voices from customer led us to do this so going back to the picture there there used to be existing a double your stools to help you with build and run for example AWS vm import is a tool that's available that allows you to go pick and VMDK existing we MDK or an OVA file and drop it think of a tool-assisted automated migration into AWS or when it comes to wrong things like cloud watch will monitor your instances things like AWS config will the CMDB CI inventory management for you things like confidence will enforce corporate governance in to your account things of that nature what we're doing today with this announcement is we're introducing a tool AWS discovery application discovery service that will help you in the planning and discovery phase there are quite a few tools from the partner ecosystem that existed in that space as well so it wasn't a void that we're filling in it's just that customers have been asking us to provide something that's done by AWS it's an AWS tool alright so partners like risk networks science logic I don't know if you guys use them have experience with them do exist in the marketplace to where to help you with planning and discovery aspects as well let's talk about the service so the AWS application discovery service is a purpose-built emphasis is mine it's a purpose-built AWS a tool to help you discover workloads right and I say purpose-built because you've seen some of the tools in the past you know they are meant to do much more than discovery and therefore we thought we'll write up a lightweight tool right great product which will which is purpose-built only to assist you in your journey to migrations what does the tool do it a very very high level it does the following first you know it helps you build up an inventory inventory of your hardware your software your carts running out there it helps you map dependencies you got a few application stats and how are they interrelated and do their hair integrations built with third-party partners and does there's one of those integrations go and open up a sales order in your s AP systems and so on and so forth right it helps you to do dependency mapping and then the last thing it'll do is it'll help perform a baseline performance baseline and and it'll really come handy when you perform the migration when you do a migration think about you doing a right sizing of an instance type think about you doing performance testing at the end of the migration so you get a baseline information that you can compare with at the end of the migration to identify whether it was a successful migration or did you lose something did you miss the mark so the overview of the service the way the service is architected two main components of the service one on the on my right your left is a discovery agent that runs on the host it collects information it gathers everything that is supposed to gather and then over a secured encrypted connection it will go and populate the data into hosted database we're calling it out as a discovery the agent can run on both Windows or Linux hosts you know you can the agents will capture system information and application information performance statistics dependencies so on and so forth and what our what we're doing is we're making public API is available for you to go and query the discovery database and retrieve information that's stored for you in the discovery database what we also allow you to do is create an export creative dump of all of that captured data in a CSV or an XML format so that you can consume it in any other tool of your choice okay one of the most powerful thing that I personally like about the service especially the architecture of the service is that API operation and and it's a very powerful tool comes in very handy when you think about the agent side of the house the way the service is architected allows you a lot of flexibility to write up your own agents think about one of those Sun Solaris running somewhere in your environment you can go ahead and write up your own agent and still populate all the information captured by your own agent into the discovery database at which point what you get in return is the AWS provided API back-end all right the analytics part of it all right and the API is that we are gonna make available along with the launch follows the standard database SDK which means all the languages that you use - all the bells and whistles that you're used to using in AWS the case are all available right from day one okay what it also allows us to do is a lot of the existing partners can design their own discovery agents and still consume the rest of the existing ecosystem to consume the data that's available that's captured by the discovery database so let's talk about the agent what's an agent it's a piece of software it's a binary that's available for you to download from the AWS website and and at that point you can use any of your existing software management tools SSE end-of-the-world to go and deployed across your entire environment okay as I said it supports Windows and Linux hosts the agent the piece of software that's gonna run on your host has a very very small footprint miniscule overhead it's not gonna consume a lot of your CPU resources or memory resources or anything of that nature okay for the security conscious folks sitting in a room out there there are two things that are very very important first and foremost it's in proxy agnostic which means deploying an agent won't require you to do any configuration or punch any holes in your firewall to allow the communication it's an outbound communication only which means the agent will collect information and post it on AWS over a standard HTTP or port 443 there is no inbound connection requirement which means you're not changing any of your existing security posture you're not punching holes in your firewalls number two it allows for an offline mode and what what's an offline mode when you set up an agent you have an option to change the mode between online and offline if you set up the agent in an offline mode the agent will capture will perform everything that is supposed to do but not communicate with AWS it will capture all the information and store it in a local CSV file on the host itself what that allows you to do is allows you to go and inspect the elements or the data that's captured from your hosts verify it before you turn the switch back to online it allows you to audit what data is going to go out of your their Center or of your environment so you know for the security-conscious people in the room and those who are very very important points in the way the service is designed what does it capture we spoke about the CSV file in the offline mode here's here's a quick list of what it captures it captures inventory level details around the infrastructure in the application for instance what's the DNS name what's the hostname what's the IP address what's the operating system is it 32-bit 64-bit so on and so forth it'll capture an inventory of all the installed software's and and that second ballot well it really refers to God's if there's an own software installed on your host it'll go ahead and qualified who captured it and it will make it available to you it'll capture all the information about all the applications and processes with very very minor details such as user information which user owns a particular process running on your hosts what group does it belong to list a kernel modules you know operating systems and so on and so forth it will capture system resource specific information such as CPU and memory and disk space and network i/o Xen what's allocated versus what's consumed think about right sizing right it will really influence your decision of how do your right size and instance type when you migrate it into AWS it'll also capture the last one it'll also capture it will also see all the TCP UDP connections on your host so anything that wasn't captured in bullet number two that is cots if you have one of those systems running homegrown custom application out there or if you have one of those applications consuming custom port the agent will still go ahead and capture all of that information it might not be able to correlate it to an existing known application but it'll still go ahead and capture all outbound and inbound TCP and UDP connections so for custom applications will still be to map it out for you and display the information back to you the last thing on the slide it captures Nik information or the network interface card level information think about some of those legacy applications where you used to run more than one NICs because one of the application is going to consume or will be licensed based on the MAC address of an IP address so if you have multiple necks running in an host the agent will still be able to go ahead and capture it and display their information correlate that information for you here's the one that I love the public API description the features or the discovery service itself there's nothing to do with agent alone this is this is the discovery service altogether here are some of the api's for example the first one lists configuration you know if you were to consume that ABI or if you perform that API operation what you get in return is a list of all the hosts regardless of whether a discovery agents running in there or not okay so let me say that again you'd apply the agent Daewon you deployed agent on a single IP address because referring to the previous slide because the agent will will go ahead and listen on all TCP and UDP ports the agent is aware of a host which is communicating with the target host and therefore when you do a list configuration you get a list of here's an here's a twirl or here's a server which has an agent running on it but by the way here's a server which doesn't have the agent it still participates in some way right so that itself can give you very rich information on alright here are 20 hosts which I need to go and deploy ease and so on or here's one of that host which is running under somebody's desk that you were not aware it still participates in an application group right this the second API listed in there get configuration attributes this is when you know there's a host it deployed an agent and you need to know all about the host such as some resources were allocated to it off that how much of it is consumed what are the processes running on it what users are are on the host and so on and so forth one of the one of the one of the great things available is an ability for you to go create or delete tags so the moment you identify ten of those hosts which participate into an application stack you can go and using those api's you can go and tag all of them together you can tag them for future identification you can tag them for migration planning you can tag them with a right sizing information so on and so forth at this point there are about 25 tags supported so think about you articulating your entire migration plan and putting them as a metadata in form of tags with all of those hosts like all other alw services tags are completely optional but they are one of the most powerful things that you can consume and and this tags will apply to your on-prem infrastructure you never had an option to do that you can do that with discovery service from a migration perspective right so about 25 tags and the last thing I call in there is export configuration it's not a real-time online thing ie if you discover a new host at this moment and if you created a dump in the morning the dump will not reflect it it's it's kind of offline in that sense so every time that your discovery database is rebuilt or rehydrated you will have to perform that export one more time before you consume that information but it makes offline analysis way too easy you're a large download you're able to do a CSV or an XML and then consume it in whatever tool of your choice to go ahead and process it analyze it visualize it what have you right there there will be an existing CLI sorry there will be an CLI SDK utility available with this launched to allow access to API operations and that's one of the one of the best part that I like about it right you build once you program it once you code it once using the existing API and then you use it multiple times in multiple engagements right so if I do this once if I write a if I write a small piece of Python code for a given use case for all future engagements that fit that use case I can just reuse it right that's the beauty think about aligning to energize their office automation type culture very very powerful all right so we're going to talk about an example so here's a here's an example you've got a lamp stack you've got an existing lamp stack running on Prem anyone migrate their lamp stack into a table yes so what would you typically do is you'll go ahead and discover this lamp stack ie identify every server every workload that participates either in form of running an Apache running a my sequel running PHP which will make up their lamp stack so you'll identify all of them you will try and understand what are all the external dependencies and by external I mean not the application itself now the stack itself but dependencies such as authentication or DNS so on and so forth you will analyze all of those dependencies your right size and you follow migration when you perform a migration either you will do it on your own you will have an aside partner this is one of us right will help you go and then perform a migration and depending upon which are out of the six hours that I was referring to your migration strategy will change for example in this example they're talking about re-architecting an application so they swapped out sorry they ran their their Apache on an on an Amazon ec2 they replace their existing hardware load balancer with an Amazon EOB they replaced my sequel with an RDS and their players are DNA's around four-30 so on and so forth right so here's a classic example of how a limb stack will be picked up from your own firm and more into AWS we're going to do now is we're gonna see how we did it using the AWS discovery services so here's a interesting enough in preparation of this demo today we spent a couple of days you know quickly put a wrapper of CLI around the existing as the case API is available with the discovery service so your possibilities are endless with those api's but to align with that example of migrating a land stack and I'm going to show you I'm gonna walk you through some of these API operations so here's the first one you know I'm listing all the applications sorry all the hosts by applications and what you see on on the screen is the service was able to find six web servers and why did it qualifies a web server because there was either nginx httpd one of those processes running on the host and then then the system was the service was able to identify three database servers again by virtue of my sequel D process running on the host we identified my sequel D and therefore we're listing it as qualifying it as a database server right so that's the level of details that is going to be available to you here's another example of an API operation I issued a very simple operation called as list me a stack which is lamp or lists me every host that participates in a lamp stack and to qualify to be a part of lamp stack again you know it's running Apache it's running my sequence running PHP one of those and then you will see in the in in in in the response what I got back was here are some machines running Python and then here are some of them which are running engine acts and then here are some of them which are running my sequel D okay still this the service is not the operation is not giving you a clarity on which of those hosts make up one application for you right it's just doing it in a raw level it's just clubbing them together qualifying them or filtering them based on the lamppost and policy which is do you run Python do run httpd do you run nginx do you run mice equally if you do go ahead and miss them okay so what we did was on the first or the first instant of the first seal on the top what you see is imagine a situation where I know only one host which is a part of an application group what I did was I listed all the dependents every host that is talking to this target machine go ahead and list them in return what I got was there is this host with IP address 246 ending in an IP address 246 that's the only machine that talks to the target host so I performed a similar operation on 246 to see are there other instances participating in this application stat and in return I got two to two so I can keep doing this without knowing what's made up what how many hosts are participating in a given application stack all you need to know out there is one of those hosts and you can go ahead query the service to build up an entire application stack without you going to have the interview the application are as out there asking them to provide you a list of what's your prod and what's your dev tell me what hosts right they're never going to get a response right most of you have done done this right the day I send out a template to a customer saying hey fill out your inventory information then Excel sheets never coming back to me right what if there is a non-intrusive way for you laughing in there what if you had an automated way a non-intrusive way to go query a system and asked hey tell me who all is talking to this host and you building up a stag information right that's the power of API and the last API over there is as soon as I identify all of those hosts which participate into that application stack I just go and bag them all of them together as weigh one think of that as where one for Migration right here's my way one of migration it is going to have all of those hosts which are participating into a lamp stack okay as I said you can have 25 tags out there you can go to the 11 are saying here's my way one of migration my way one happens on 26th of April my way one should involve the following application owners my way one should happen on Saturday whatever you want to go and do with tags you can go ahead and build out a migration plan in a metadata format using tags okay here are a few more examples so once I tagged once I tagged and once I'm supposed to go ahead and execute the migration I can go ahead and list all the hosts that have similar tag ok I can query the service does list me all the hosts in the second example over there what I'm doing is I'm drilling it further down to now do right sizing type decisions so I'm listening out or I'm doing a detail to describe on all the hosts that participate or that has a tag as waveone okay there's much more information that's available in output we just did a screen grab to drive the example to make a point that how can you consume information that's already available to either do migration decisions from do migration planning to do right sizing of instance types so on and so forth so how is the service available we announce the service today it's available if you want to go on the area's website it's available for preview starting today you can go and sign up with you need to do is the service is made available only through AWS professional services or SI partners okay that doesn't mean you're gonna spend any extra but what it means is if you were to have any need of troubleshooting any kind of help you know one of them will be able to help you with your discovery service so the way it'll work is you will go online you'll go on our website and then you'll fill out a form with your contact details in the backend depending upon your size and your Asian and the closest available partner or for some services consultant and so on and so forth we will go ahead assign a consultant to you and the consultant will at that point start driving whitelisting of your account in the preview and so on and so forth so from your perspective all that matters is you can go online right now fill out a form about five or six fields and that's it we'll get the ball rolling for you but I want to show you in the next couple of slides is a slightly different view perspective or the service itself so here's a visualization example of the service the tool is a some open-source tool I don't even remember the name it's some open-source - what I did was for this for this demo for this mark we downloaded we exported the information in a CSV or an XML format in this case it was a CSV we imported that CSV data into this open source visualization tool and then every API call that you that you were making on a CLI or through the SDK with it that API we did that filtering into the tools so if you look at what works doesn't if you look at the if you look at your left you will see a query over there it says missed me every host in there which has a source port which has a source as Pythian and destination is nginx right at that point no matter what hosts are there it will go scan the entire CSV file or your entire environment so speed and highlight those yellows are the highlights or there's a text output at the bottom of it that's not important though what's important is you're able to submit a query and visualize your entire addressable migration on a single screen right to give you another example of the same thing here's a zoomed out here's a blown-up version here's a blown-up version of one of those previous screens what it's showing me is all those yellow circles all those over there are the highlights okay and that's really functionality of the open source visualization tool what's important is there are those four hosts which which meet the criteria ie the sources Python and the destination is nginx what's more important is there is one of those Greens somewhere in between now that host doesn't really qualify to have sources Python in destination as nginx but nevertheless it's a critical host that participates in the functionality of the application because it's talking to everybody in there right or if you see those red lines between those two yellow oval shapes you know that indicates there's an active communication between those two hosts or if you see one of those pink triangles you know that that's saying that those triangles are hosts which participate which communicate with your filtering criteria but they don't even have an agent running in there right so it's a very very powerful way for you to when you start thinking about migrations when you start thinking about planning a migration it's a very very powerful way for you to go and identify what's what exists out there right how do they make up an application how do they participate in running an application what are some of those hosts that are running under somebody's desk you don't know off but they still are a part of the application stack right that brings me to my last slide I being an engineer I just wanted to close with that you know the service provides a set of public api's the way service is designed you know it uses a lot of open data format making it very very easy to either integrate with the service or for the service to coexist with anything else that exists in your ecosystem in your environment so give an example can very well integrate with discovery tools that can publish information so the example I was talking about risk Network science logic you know the tool can coexist they can populate they can use their agent you can use one of those agents to populate the discovery database any existing migration tools that you have heard of sure you guys have heard of versa me and cloud in the Orion cloud we logs and system you can use AWS application discovery service to complement those tools you can feed the rich tag information with rich context information to these migration tools to do an automated tool assisted migration into AWS or you can use it with existing migration frameworks think about think about existing sis they might be using their cloud migration frameworks right talk about exchangers and cognizance and second watch and so on and so forth all of these partners have their own methodology of performing a cloud migration the tool will complement their existing cloud migration frameworks even if they would have built automation in performing migrations in the cloud into AWS pretty much go exist with those existing frameworks to assist him performing migrations that's all I had open for questions yeah if you can please because you're going on wire so you can't so so I think something like this is really interesting even if it comes down to like how are you mapping like a microservices architecture can you talk a little bit about maybe the use case where it's not just specifically used for migration but like let's say do you do you envision people running this on their AWS infrastructure for example and then if it's gonna go that far what how are you gonna represent things like s3 or sqs in terms of how you'd maybe map that out absolutely so at this point as I said that the the service is purpose-built for migration into AWS so there are not a lot of use cases that are supported having said that the service in preview today will support instances like Amazon Linux so we highly encourage you to you know sign up for the preview you know one of the asks as a matter of fact is for a lot of you for all of you to go sign up for preview break the service right pick the service help us rebuild it in preparation for GA so yeah a very good use case and it supports amazon linux as an operating system but it's purpose-built you know for a specific target hi the tool especially the API is really cool but the problem with all the familiar names like risk or see me condor in this how do you actually get it into your target data center I know that you try to minimize all the dependencies and the connection requirements but in corporate environments like outbound is not guaranteed or there maybe 30 firewalls in your way there also might be outbound proxies absolutely absolutely it's not agent less it's agent full and I'm kind of wondering how you decided to pick that route versus the other one but because it's an agent how is it actually gathering the information is it using like SNMP or wri or using local things to the hosts one of the problems we have is we don't have shared community strings that we could go use create them first so the third part of the question is is really going to put you in competition with me you know how the agent works so so I'm gonna skip that question given the type of the forum yeah we can we can definitely talk to you offline but there are two other parts of the question first being you know there could be 30 firewalls or there could be a proxied environment out there how is the agent going to be or isn't gonna deal with all of that right so and the agents are I might have said it in one of my slides the agents are proxy agnostic which means forget about the agent if the host can go all bound on 443 using the proxy the agent will do it without any additional configuration the agent in itself doesn't have an option for you to go and put a proxy setting in there right so it'll pretty much rely on the underlying operating system if your host can go out using a proxy the agent will go out using the proxy no changes in configuration you use the same proven secured audited firewalled environment first the second part of the question was why did we pick agent base and why not agent less or whatever it is right so yeah use cases for in favor of both of them right we heard customers talk about a need of having a much of richer data and and therefore we thought of building out an agent based system but that doesn't mean that an agent in the system is bad either right there are customer use cases out there which which require unless in ways if you know they want to don't play it safe and they want an agentless system so definitely thoughts welcome you know we can take that as a feedback and obviously include it in roadmap we can definitely do that you plenty more can you take a pause and we can take one more question from him and then obviously you're welcome absolutely the the agent itself when online will send information we capture information and popular the discovery database every 30 seconds so if your business cycles for three months for a given application profile let it run for three months and we'll give you min max peak Rani's averages all of the above with timestamps back to you okay in Linux do you qualify aches as Linux we quantify what AIX is Linux we qualified as slightly different a really short answer is no for Windows does it work all the way back to Windows 2000 let me so I'm not I'm you know I don't have a good or a bad answer for you but what I what I'll definitely ask you to do is as we get closer to to GA we're gonna make that announcement so stay tuned you have a good guess on when GA will be or anything no no it all depends upon all of you sitting in the room here the sooner you break it the sooner we fix it the sooner we go live so please please go break it what are you laughing I'm gonna be scared no no I want to understand more clearly if I have sort of multiple lamp stacks on the same VM is this gonna be able to differentiate that there's this particular instance talking this particular database and some other host and there's another one or Sulli saying all in the VM it just lists everything that the VM is connecting to no no so if you look at one of these examples one of the first examples here it's going to list everything if you only filter or if you perform an API operation to only filter a lamp stack definition it'll it's just about everything if you were to go and do individual ease pick up a host and see what's dependant what's starting you can nail it down to one stack versus the other stack right and so can it differ so if I have two Apaches running I probably wouldn't but or but that's a say for the sake of argument I did and there's things talk to two different databases on other hosts would it be able to say here's lamp stack number one yes okay yes it will be able to go ahead and say that okay if there was a virtual host with one apache thing what is it sophisticated enough to see that I see you have two lamps tax under the same Apache configuration and well still do that could you just talk to how the product would be priced the per system just give some details no unfortunately not as a price so at this point you know I don't even know if internally we have that nailed out right so that information is not available at this point but but then again you know stay tuned as we get closer to the GA you know we will we will go ahead and share that Intel with you guys sorry questions we can do a target practice you know I'll be like yeah there are no more questions I think we're done thank you so much once again sorry please what if you have things that aren't computers that you not can't necessarily install the agent on like a Google Search Appliance mm-hmm so the way it'll work is will still go ahead and identified as a participating host which has a unique IP address which is communicating to an existing host at which point there may not be an option to go into point agent because it's an appliance right but we'll still go ahead and identify that for you so you can go ahead and understand there's a dependency and then you let us somehow deal with the dependency when you perform a migration you're welcome thank you guys 