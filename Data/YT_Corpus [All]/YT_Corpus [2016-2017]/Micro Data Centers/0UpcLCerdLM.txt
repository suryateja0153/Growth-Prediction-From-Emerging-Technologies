 good afternoon everyone and thank you for coming my name is Sheila GRE I have been working with Netflix for over a year now we have been working on dynamite prior to joining Netflix I was I was working for plastics which is a distributed relational database startup mmm so today we'll be covering dynamite a open source product that we have developed in-house I'll be talking talking about dynamites its architecture stop ology replication and yeni will then come in and talk about the rest of the components of dynamite which is a dynamic manager in the Dino plant so as you know our most important I mean main primary business use cases to stream movies at no matter what so for us availability is very important right so for that we actually moved from a relational database like sequel or Oracle to Cassandra but over time Cassandra became a de facto datastore for all our data but since we were also micro services based architecture new services came up which required really high throughput and low latency kind of datastore and Cassandra could not really keep up so what we really wanted was a new datastore which would be very very scalable highly available that's very important for us and which would serve good throughput and low latency for for the services that were demanding this load a typical deployment in Netflix is basically we use three data regions three data centers or regions what we call it and and each region or data center has three availability zones and the apps deploy in all three regions right and all three have deserts so essentially the data store what it means for a data store is it is going to get a read or a write from any other availability zones so what it essentially boils down to the underlying data store should be active active and we should have a multi data center or application so that was primary use of primary design goals for our new dynamite thing of course radius was gaining popularity it had advanced data structure so it it provided much much cleaner and better data modeling aspects but what we really wanted was active active highly available multi data center I'm repeating this again and again highly available because that's how important it is for us right and so and we did not have bad action replication with arrays for a master/slave kind of architecture and to provide high availability what we actually initially do we internally do is we we run chaos monkey kind of failure testing if you do not know you can just google for chaos monkey Netflix so they have the three times three types of failures one is monkey where arbitrary node just gets killed another is a gorilla which where an entire rack gets killed and a Kong where the entire region gets killed so what we want is ultimately this data so ready we are coming up should be available even after these monkeys are creating chaos in the data centers right so that's how that's how we test our internal services we want to make sure that it's available and the movies get streamed no matter what so having said that we came up with a new product called dynamite what it essentially does is it's it's basically a generic layer on top of non distributed data stores like Redis memcache at M DB etc the primary focus over here is performance again cross data center active active replication and it has to be highly available right we have two types of use cases for dynamic and ternary one is as a cache and there is as a data store so for a cache if four node goes down and you don't find it it I it's fine you can just bring it up from the back back-end data store but when you are using it as a data store for like temporary data and all you don't you it will be it is not significantly problematic but it can be still still do of still not good to lose the data so for that we actually implemented some really good features in dynomite we implemented no warm-up what it means that when a node goes down or it gets killed by a WS and in you know to come up it is going to get its data from its peers so essentially it is it means that it will the client will see not see that they are not see the client for some time it will fail over and when the north comes back it is having the data again enable consistency so some of the applications we have required to read the data that they have written right so for them we reintroduce two types of we introduced the quorum kind of consistency I'll be covering that in the subsequent slides so that way you can actually read the data that you have written and we last week we introduced s3 backups and restores for for kind of for the applications where you cannot afford to lose the data you essentially have backups to s3 nightly backups to s3 so all these features were pretty recently introduced so consistency we introduced in last August warm up we did it in February and s3 backup siani just introduced in last week so that's how rapidly we are developing dynamite over here the current status of dynamite is it open source and fully integrated with the Netflix ecosystem it lives on github alongside dollar the other Netflix projects having said that it is rapidly developing it is young it is just maybe two years in development but that doesn't mean that it is immature it is fairly mature it is stable it has been running in one for running for 1.5 years in production in fact I am on call primary for dynamite right now so that's how high we are that's how stable it is and we have our own thousand customer facing nodes they take around 1 million ops at peak the largest cluster deployment that we have around 6 terabytes of data and so initially we were deploying very fast but now we have started gradually reducing the deployment and we do like quarterly updates along with hot fixes every now and then so the project lives on github the master is the stable branch and if you want to cut right out you should try the master price that's what we run production so let's take a look overview of what dynamite looks like so it's a layer on top of a non distributed key-value of data stores right it can use to be used in multiple data stores but all of our production deployments use Redis underneath we have peer-to-peer shared nothing architecture what it essentially means that all nodes in in the dynamite cluster are similar there is no master or a slave or anything like that there is no leader election or I think both architecture like master slay or peer to peer has their own pros and cons and it depends on what are your use case and what fits your needs right so for us highly Harold VD was important that's why we choose a peer-to-peer architecture it does Auto sharding so it uses consistent hashing internally so our every query that it receives it is going to figure out the key which and hash it into and onto a node which it belongs to and then forward the request to that key to forward the request of that node sorry it is multi data center application and it linear in hats linear scale what linear scale means that when you add nodes to the cluster or when you double the cluster you you your throughput also doubles accordingly without any affecting latency so no effect on latencies what is very important because then there's no point in scaling of the cluster right we have replication so the replication we have is between data centers is you can tune it to be either nor application or no interruption at all or you can have encryption between data centers or between racks the open-source version uses gossiping but internally we do not use gossiping I will tell you what we use in the next slides so let's let's drill a little more deeper into how the topology looks like this is very unique to dynamite like the way we deploy is specific because we want availability zones sync up with the data replicas said that we have so so this is so as I said our typical deployment has like three regions and three zones in each zone but in the figure for simplicity I am showing it over here it's on the left is DC one which has one data center on the right is another data center the first data center has two racks in it so the weight data gets replicated is every rack has one copy of the data right so one rack is the entire data set that you have and that data gets replicated in every rack and every region that you have so depending on the configuration you can either have three replicas or two replicas so in this case there are three nodes in every rack and considering that the token distribution is the same in that way a 1 B 1 and C 1 would have the same copy of the data so we have multiple tracks for high availability and each rack contains the entire copy of their data right so let's let's dig into further as to how the coit is get replicated we didn't dynamite so in the figure you see there's on the top there is Dino plant but I want to mention that it is not necessary that you should use the Dino plant you many of our use cases actually use Python Redis client and we even use Redis ciela extensively to debug issues in the animate and also the protocol that dynamite uses on the wire is straight away the whatever the data store uses internally so so let's say Dino client is sending a request to a one over here the what a one does is basically it hashes the key and figures out who is the token owner for that key and then it forwards it to the local to the dokkan over in the local availability zone and then gets a response and returns it back so this is for a read but if it is for a right along with sending it to the local ad it is going to a synchronously replicate to the rest of the availability zones and the racks it was sent only one one request to the remote region and receiver on the remote region actually fans out the request further to make copies so every node over here has the complete view of the topology as to how the nodes look like and what is the token distribution also it is not required that all racks have the same number of nodes or hauled all data centers have the same number of nodes you can have like a symmetric deployment you can save save some cost on putting lesser nodes on data center which you are not going to use much but you just need it for this disaster recovery so that's how right right also gets replicated all that is a synchronous now let's dig a little more deeper in to tear apart tear out node and see how it looks like right so the ecosystem has a client as you see on the talk which talks to dynamic node using the release on wire protocol dynamite - dynamite is basically either SSL based encryption or no encryption at all dynamite is essentially a process a program written in C it is it's a proxy layer or that's--it's that that runs alongside Radice on that node Redis or any data store for that matter dynamite manager is a sidecar or you know it's it's a process that is written in Java which you use internally for multiple purposes primary primary role of a manager is basically to to monitor the health of dynamite and the underlying data store along with that it also does a reporting of metrics to the internal at last server I think Atlas is also open sourced and it also acts as a admin console for performing functions like as three backup distortion or warm-up and stuff like that or consistency so this this is like a best-effort consistency since this is an AP system we cannot really guarantee strong consistency but this is like a quorum base for applications which require to read the rights that they have just written they typically use quorum consistency so dc1 and Issaquah reminder to to enable consistency that we use these are these terms are basically borrowed from Cassandra but what they essentially mean that in dc1 you guarantee only the only the write or read from the local SG and in quorum you essentially guarantee rights to quorum number of nodes in the years so if you have three racks you are guaranteed you are written the written the written the data two to two racks before you respond so by that by that you can you can say that if you are if there's another write read coming in and it is going to do a quorum read it will all essentially see the latest data it has been written and we do we do checksum calculation on the responses before we return for comparing of quorum responses consistency can be configured dynamically for read or write operations separately so you can either configure read consistency to be quorum all right considered quorum or both and this you can change it dynamically the only caveat over here is this is like cluster all read configuration you cannot do like per node or per connection the code has the has the capability to it but we haven't yet implemented we haven't yet to go on to that length to do it on a per connection but if there's no need it can be like modified it easily so we did some performance set up of performance comparison of dynamite or I mean we wanted to basically wanted to see how dynamite performs under load with different sizes right so the exercise was not basically to say that we are this much fast or we can do this much what we wanted to see was if if a client app or someone comes to us and say hey we want to create a we want a dynamite cluster of this much SLA we wanted to make a create a proper judgment we wanted to make the right judgment and create a cluster appropriately so the idea here is to just figure out what how good dynamite can perform under load so what we did was we had a we create a we created different sized clusters of three node six nodes 9 12 24 48 and so and so forth and we wanted to see how it performs so we use Pappy as an internal load generator tool it's not open source yet so we we it is basically it creates a cluster of cluster of nodes and it throws load on the on the dynamite node so the pappy is configurable you can again add drivers for Cassandra you can add drivers for elasticsearch of dynamite also and so forth so we created a cluster which was specific to one region and we it had three three availability zones we deployed it in US East one every zone in our cluster had the same number of nodes the demo app that we use had performed only basic gate and set operations we did not do hashes or stuff like that because that was not the purpose over here we didn't want to test the underlying reddit's but we just wanted to see how replication performs and how dynamite performs the payload that we use was a 1k and we had like 80% and 20% ratio of reads and writes so this was what was a throughput performance that we saw so as you see in the graph as we doubled the cluster the throughput actually doubled and we were able to get more than 1 million client requests with just 24 nodes in the cluster and this is 24 nodes in the entire region so it means that - eight notes per easy so this is the latency graph that we observe on that so on the leftmost is Dynamat average latency it means that it was only the server side latency that we saw and on the right is client side median read and write latencies so we saw that the client he didn't write we're like sub-millisecond this was probably much better than we expected and this essentially showed that we we got what we wanted to achieve out of dynamite and these are the 99 latency graph so we generally Netflix observe 99th latency graph because the services some of the services are very sensitive to the latency and as you see over here most of the latency is basically taken up by the network the dynamite latest the Surfside latency itself is pretty low for the for the sake of this session I've just limited the press lemmie turd the graphs to just dc1 but we had actually published a post on at fixed tech block back in january which does a whole extensive test of radish with pipelining and with quorum so if you are interested you can go to the net fixstick block and have a look Jonnie will now continue on the dynamic manager so one comment about the latencies I think more than 50% of those latencies were mostly socket latencies and dynamite and Redis latencies so the processes themselves seem to be pretty fast in any case so based on experience we have for Cassandra for operating the last six years we have a sidecar so we took some of the baseline code that we had on a product called priam and without that for dynamic specifically on how we can manage dynamite in a class named production so dynamic manager itself it does talking management for multi region deployments so it creates the tokens based on some in local our code and then uploads those tokens to a cassandra cluster so we can create at any point of time where we want when when when a node goes down it can where the database comes back and takes the token from the data store we created dynamic manager in order to deploy that in AWS so it has a lot of api's that are specific to AWS the way we develop dynamic manager is through Java interfaces so if you do want to deploy denman manager in other environments like Google Cloud or Azure or rather then it's easy just implement those a specific API is dynamic manager does an automatic security group update which means that it opens support in order for the nodes to connect and discuss them and talk between themselves and it does monitor dynamite and there are nine storage engine which in this case is Redis it actually does two things to monitor first it checks if the process is alive and secondly does perform a pink on reddy's to see if the Redis itself can accept data if it does accept data it means that it's it's alive and interconnected client request and it does the same thing for dynamic as well it does perform a ping on dynamite using the ready CLI at the reddish pink and it also takes the dynamic data live if all the cases for the hell secure file it reports back to discovery and is discovery this node is healthy and it's operate able if something happens and one of the health check fails then you report back discovery this isn't healthy we take it out of discovery and then Dino automatically fails over to another node so we don't lose any data in that case dynamic magic performs node called bootstrap which means that the node goes down it comes back we don't lose data we just use Redis underlying replication to stream the data from another node I'm going to show you how this happens and of course for disaster recovery we do implement edge through backups and this is done automatically on the scheduling basis through dynamic manager dynamic manager is fairly easy to community with we use just a streak molest calls to it and you can perform any actions through that so the way that the warm-up functionality works is a dynamic manager checks which other node in another availability zone has the same token and once it finds that node then it uses that know to stream the data use it master-slave replication in the background so during the time that we warm up the known we still keep it out of production until once it's it's it's wound up then we bring it back to discovery so effectively what it does in order to determine if that note is ready to accept data is in sync if actually takes a bit sitting on the masternode you're taking the ready scene for information once the two notes are in sync then that dynamic releases and say this note has all the data it is fully all the date happened before implicated now we convert it from slave to master and we bring it back to production and of course in the end we check if everything is caffeine or to make sure that there's no other problem happening so this is an example here that the note gets eliminated in AWS and then as you can see there is a better world so there is another node in another availability zone that has exactly the same token so the note comes back throughout the scaling activities by AWS once that is up Florida a dynamic manager detects that there is another node that has exactly the same token which means effectively inside the token ring it has exactly the same data and then it says that no that's a master it says itself as a slave once this is done it uses diskless a master-slave replication from Redis in order to string the data to the slave once the data is in sync then we list that back to to production and therefore a discovery knows about it we also have edge three backups so as I said before we have to use cases for dynamite and Ready's one is a circus in one is us data store and for those people that use dynamite as a data store we definitely don't want to lose their data in order to do that some customers have asked that they want to backup their data to s3 these are other use case about data corruption sometimes something genius may make mistakes they may change some data and therefore they want to roll back to the data in a previous day and it's not the third case which I didn't include is refresh and restores sometimes we're going to do testing like a testing so we want to move test the production data to test so this can happen also with s3 backups restores with dynamite so effectively the way we do that is on a scheduling basis so dynamic manager spawns a thread on scheduling basis and then backs the data dumps the data to the drive then it uses s3 API to send the data to s3 donors to backup the data as we said the most important instances are r32 x-large which means that we have 64 gigs in upper node basis so we don't use incrementals because our file sizes are not that big to actually use incrementals we do have a couple of nodes that are used are 3/8 x slots which are 256 gigs but right now this person only uses that the dynamic cluster circus so long story short if if you have data that have very short TTL then there is no reason to do a straight backups obviously and we do perform backups on a daily basis on a specific time while everybody's up in the office of course in case there are high latencies but that's off now everything looks good so the way we do backups is again either through a lightweight thread that spawns on a specific time during the day or through our arrests or well this is some customer comes to us and says I want to persist my data right now because I'm gonna start a test in about 15 minutes then we tell me okay wait we're going to persist your date and you can start your test in case something happens we do Billy's right AOF and Biddy save in the background so we used you know the 14 mechanism that Redis uses for that perspective we check the size of the file that we have persistent and if the sizes above 0 and everything is fine then we send this to s3 the way we store the data industry in our buckets is effectively we have this is the backup this is the region this is a classroom this is the token and this is the date so every time somebody asks I want to restore my data for a specific date we have all the information and we exactly through the token you know exactly where the data will come back again restores can either be performed so if a customer says I want to do let's say refresh and restores right now I want to some data from production to test and we can do that effective once we do that in tests during the start of dynamic manager we can enable for our first property which is a configuration to do the restores or otherwise we can do that through a rest call in that case we stop dynamic process the reason we stopped dynamic process is because we want dynamic manager to inform discovery that this node is getting out of discovery once the node gets out of discovery then automatically the client will failover to another node so the customer will not lose any data or will not have any downtime and after we do that we stop Redis so of course we started in a specific date we start Redis the data are loaded automatically by Redis and then we start dynamite and we bring those anodes back to production so in order to fully take advantage of the features of dynamite we have created the Dino client which is effectively a wrapper around jd's so as Celeste said you can use any radius client to talk to dynamite the the API that we support are on github but we support a majority of them so Dino Dino client uses connection pooling it does a lot balancing so if you do use Dino client there is no coordinator note on dynamite because Dino client is talking aware so automatically if it uses the same has Albert must dynamite so it automatically send the data to the proper node directly it does do effective failover which means that once it detects that the nodes is out of discovery there is some issue it automatic failover so that request is not lost that one tries second try third try it tries to failover the way we have done it to failover once it fails roughly it sprays the request to the other nodes that has failed over and we do that again for extra load balancing you can do pipelining scattered together and Dino client is also if you use Netflix OSS ecosystem it is also integrated with insights which is Atlas the matrix matrix R we use inside Netflix what's going on so this is how the eine kleine the plugins employees talking about load balancing effectively the way the tokens are distributed inside dynamite is follows the same principles as Cassandra but with a difference that every node has a single specific token rains so effectively once data come to dynamite and they belong there hast if they belong to specific token AIDS that this node is handling the data will start back to that node and since Dino is talking aware it will be no tweets exactly note it has to go to cell store the data so therefore there's no need for a coordinator node so again there might be cases that one dynamite slash ready snowed is down and we need to failover and we don't want any downtime in that case we just retry in add mode track in order for us to get the data so some of the features we plan to implement in the future one thing is a multi-threaded support for dynamite of course the latencies are low but we think there might be some improvement on having separate threads for cross region or a cross track replication we are we have were almost done with the first session of data reconciliation repair so for example if some data cannot be sent on other AC and we lose the data we have a reconciliation engine that we plant we we have we plan to deploy in production in a month or so for that we're looking in different persistent data source as well we are looking there the BLM DB which are effectively there are some open source projects that do use the Redis API to use them so we can use that for example as an l1 ready cell to cast as a persistent data store or we can even use them as another solution for people that actually need persistence and noting memory data we're working towards a dynamic smart connector and goes have a nice talk a little day about Redis spark connector as well and also implementing a synchronous calls from the Dino client and of course this is an open source project so if you do have any recommendation about features you would like to see please open an issue let us know and we'll have to to work with you in case we can do that again these are the links where you can find information about dynamite and I know dynamic manager which is a cycler has not yet been open source we plan to do that about a month as well we're in the final steps so again if you do have any questions let us know these are emails but also you can use github to post any issues that you have or questions just to mention we also have a Netflix OSS meetup on 1st of June in Los Gatos so if you are interested you can come over and we can further discuss about that no matter now and we also have open issues on dynamite github so if you if you think this project is interesting and you want to contribute we have issues in attack does need help and you can take a look and feel free to pitch in if you want any questions so are you talking about token collision in that case the two nodes have the same token the one didn't get out of discovery we haven't faced that issue right now in production we have faced that with Cassandra a couple of times but we haven't seen that issue with dynamite right now one of the differences said that Cassandra uses our token rains across nodes and we use a token reads inside a specific node so once that node gets out of discovery that that node is effectively invalidated for that I've right so in any case there cannot be two nodes at the same time in production that have the same token in one availability zone so Zambia course for nodes you mean so so the discovery service is going to say that this node is down right and dynomutt manager actually informs dynamite that hey this node is down and that's how dynamite actually gets its updated topology and later on when an event manager actually when the other node comes up it is going to actually see which know which token it is replacing and it is going to say okay I am the new token now and that's how the dynamic discovery service actually and we use the same same technique for Cassandra as well that we we use that this node has replaced an older older node and this is the new node that you should communicate to no we don't Trustin Ubuntu right now but there are people I think I'm lyin on github that using st. Wes there was anything as issue two days ago and versa fix today for that as well how do you add new notes so the question is how do you add new nodes to the cluster so in the long term we want to see that this is going to scale up linearly like you can just randomly add nodes and it will auto rashard and all but that is that is something that we don't have today the V the way we do it is either we do dual writes and create a new cluster temporarily because many of our use cases are have TTL based data so we use dual rates and all four for such features and if if there is a need that you need more capacity there are two options either like I said the dual rates or you can have you can push a new ami with a larger instance and then you can do a rolling termination of one note at a time break so because we use a highly available system and not a master slave for us we can terminate any node in production and there is no problem because the data will be formed automatically right so there's no data loss in that case so for us a week what we can do is we can push a new ami with an better instance type if we want to vertically scale if it's a nemedian case but it's more like a long-term case so let's edit right we can just double the cluster and do dual rights on the Dino side as a pattern and therefore bring that till the date out and all the cluster you really need to see whether you want to add this mechanism of or Tori sharding or whether you want to just temporally create a double cluster and be done with it right so there are a couple of there are a couple of vendors that would be actually in a meet-up that will talk about how to operate dynamic in production but the reason opens I think to folks have created already an open-source version of what we call dynamic mods we integral Florida inside their github repository so it does have some functionalities for token management it doesn't have a full-blown fortress for warm-up and backups but the dynamic manager is almost ready to get out as a la presse's project that's a synchronous replication and dynamite takes care of replicating data so what is your question around it yeah it is a synchronous so yeah it is I think so so if it is a quorum kind of read or write then the client request is going to essentially wait for the dynamite coordinate is actually wait for those responses for that request before it responds back to client so it really depends on the configuration of the consistency but by default for a DC one it is always going to be a synchronous replication yes so so if so there are chances that since it is a synchronous replication the coordinator node might go down it might acknowledge the right to the client and say that okay I'm done and then the right is not never replicated so that in that case the right is lost so for such kind of use cases we recommend customers to use to nip the DC quorum consultants so that they know that at least two of the nodes have written the right before they go yes we have done I think we did a month ago so we tried lmd be in our DB and it was like plug and play because they used the Redis API no we used in that case with what we want to do really so this was more like for us to do an exercise how difficult it is number one and how fast it is right so for us was just just put 11 DB in a cluster and see how fast is runtime running two of them together so it as I said Italy tries twice if it doesn't find the data it's gonna fail over to another to another available to another node inside the same inside the same region but we keep nine copies of the data right that's idea is three replicas per availability zone so we keep nine copies to be saved that data will not be lost so it's so it depends on the deployment they are our typical dynamic deployment some of the dynamic deployments and and Netflix are three regions three racks and recently we have also started deploying our f2 which means that you have only two replicas within a region some of the deployment have just one region so it really depends on your deployment but essentially it boils down to you have one replicas or availability zone no that's not a difficulty plan so the typical deployment is two or three two or three racks and the local region and if you're if the app is multi data center that you can expect reuse and writes on other region then you have the apps deployed on multiple regions so I'll ask the first question if anyone goes down it comes back it gets warmed up and gets back to discover with the same nodes wants to get back to discovery Dino detects that and will you know it will move back to sending the traffic to that node so why if it falls back to 81 right it will start failing again because we just moved it back we did the shell sack then my mother does a health check it's make sure that it is running if it's wrong running it will not bring it back to discovery if it does BIGBANG discovery Dino will continue getting the data from the remote rocks yes so you inject the dependency of your discovery service in Dino so that's how you effectively Dino right yes it has connection pooling and preconnect surrounded and or the maximum connections that the dynamic node can accept is around when we tested around the eighty thousand so keeping the connection zone that doesn't create a problem for us no no this is not that strong consistency this is a best-effort consistency we use we make sure that the response we get a positive response perform at least two nodes out of three before we return back that okay we are good otherwise if you don't get you get you send that okay we failed and then the next quantum read would essentially mean that you will get the right real make sense yeah we have we have deployments actually depends on what is the customer requirements we have some clusters which has just one one node per rack yeah we can still fail over and on so essentially it means that one reduced node active active throughout so looks at the one to two more questions because we have to wrap so we take so we have buckets in all regions and all nodes in all regions do a background a UI for a background save then we send to the corresponding packets using the token so we have all the data across all cluster right so that's so that's that is a global snapshot but it is not a point in time snapshot right so it is not a point yet they're like this is a difficult problem to have a point in snapshot in a distributed manner and they're actually full cup full blown companies around it so no so in Netflix we are majority of clusters are Cassandra but we ended up having this problem that Cassandra was not fast enough right and there was a lot of community was building using radius right so they wanted a shorty tale of data our data store at an extremely fast wrote it you know or low latency therefore this is what they need to do so we are trying to cover a use case for that so think about Cassandra sitting here the sitting here so high persistence low latency high persistence okay latency a high throughput low latency persistence not on a per request basis right so if if a customer has a lot of requests right per second they can do that with Cassandra or we even have a customer that is behind you that wants to implement some chewing on Cassandra and if you can do that so he's creating a layer above dynamite for Q's as well so there are like different use cases right every customer is different so I think we're almost done yeah if you do have questions we'll be sitting here outside or outside yeah sorry 