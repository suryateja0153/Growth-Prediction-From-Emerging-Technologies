 so first of all I want to thank IDC for giving Microsoft an opportunity to be here and present our part of the story I was specifically told not to give a sales pitch and talk about technology and trends so I'm gonna try to do that and I do have some slides obviously I wanna hear because that's what I work on engineering strategy there so let's get started so when we look at a big compute ecosystem you know there are different kind of people in wall whether it's on Prem or in the cloud your cloud providers hardware manufacturer OEM network providers storage the operating system tools you have schedulers end-users and commercialize switch that's kind of at a very high level the way I look at the ecosystem right and every party has a part to play in each of them the most important is the end user and I see very few end users here I wish I could talk to them more so when I'm talking to my customers or our customers and what we hear from them is most of them are talking about we want to use HPC because we want to develop our product faster I come from a manufacturing background it's to run crash simulations you're not emotive the reason auto OMS use HPC because they want to reduce physical testing and do as much simulation and develop products quicker lot of them want on-demand compute a lot of crash engineers or NV as durability computational fluid dynamics engineers wanted to use do your optimization they want to route thousands of runs generate terabytes and terabytes of data and analyze the trend for the underlying physics that they're trying to simulate they're always restricted by compute I mean even if you have 50,000 cores thought of time it's not enough because 5,000 engineers are using the system imagine if you submit thousand jobs to do a do a full factorial it's going to take forever to finish they want to do high fidelity the models are getting when I started doing simulation a million element model for crash used to be like a big deal now it's pretty standard to run ten twenty million element models which is Lana CFD job with five hundred million elements on one of the CFD codes and we see very good linear scaling we will talk about that so the models are getting bigger day by day a lot of users who under control they want on-demand right so I should be able to go provision by machines I should be able to run my job and I should be able to shut the cluster down we really quality job per cluster and not a monolithic cluster there most of us are used to used to see you know Ron prim Dale sense they only want to pay for what they use and they want mobility so if I have access to the cloud let's say I'm on the road I'm visiting my supplier that manufactures seats in automotive and I'm at their factory and I want to quickly submit the job I want to change the parameter in my input deck I should be able to do that seamlessly through a normal SSH connection in the cloud I don't have to go through VPN a lot of other stuff so mobility is important and remote visualization kind of pleasing to that and without GPU strategy so this is what I hear from the end users right an example of that we have a customer Hendrick Motorsports a NASCAR some of the big names in in NASCAR are those drivers are on the payroll with Hendrix and Duran star CCM plus in in Asia on up 2024 course and you know Hendrix relative to auto om so you know some of the National Labs that are here they are fairly small right there at total IT budget is like maybe few million dollars they can't afford to purchase thousand twenty four clusters and just keep them around just whenever the race is there or when they are changing the design so truly on demand and that's why they use it then there are cloud providers right so what we call hyper scale David was here just before me talking about hyperscale there we think there are only really three players in this space that can truly do hyperscale will requires billions and billions of dollars of investment right see AWS Google and Microsoft and we have I have I have a slide we have about over lasts I would say maybe four years close to 15 to 18 billion dollar investment in data science so obviously what that does is you have enough competition in the market that drives the cost down if anybody has observed trend for block storage in the cloud the prices continue to decline for last over I would say maybe four years now they are started to taper off a little bit but it's so cheap to store data in cloud it's not even funny right so that's that's good for consumers that good for customers and a lot of tall providers what I see is trying to move to higher level services Microsoft will obviously trying to do its part but cloud providers are trying to come up with not just infrastructure like network compute and storage is kind of given nowadays right you can come up and spin up five thousand cores and what an MPI job and I'll talk about that how we do it that's pretty much given nowadays that's like you know that innovation has left left the station but value-added services so if I am an end-user I want to do end-to-end so I should be able to do pre-processing easily in the cloud I should be able to submit jobs easily truly truly on demand and automated workflow to do your entire end-to-end design how can execute that in the cloud that's what I mean by higher level services so you will see a trend I feel going towards higher level services in the cloud eventually there's still some challenges last mile is always a challenge so Microsoft has its own offering called Express route which is MPLS connectivity directly onto your MPLS network that gives us a maximum bandwidth and relatively low latency we also try to localize our data centers in the region that we go we have a jennette works that we call all kinds of things but still connectivity if you to upload a petabyte of data in the cloud it's a problem right so latency is also important in turn or latency to run MPI jobs at scale its lot of us here believe that that's true HPC right and now a lot of talking a lot about Big Data but ultra-low latency super-important and that's a challenge and then workflow automation for higher level services as we talked about so these challenges I see another big important part of this ecosystem is commercialize who is obviously if you are working for a national lab or a university you have might have open source code or you might have developed it yourself then it's a different but most of the commercial industry uses commercialize Suites and one of these is we is with my interaction have been kind of little bit slower to adopt to the cloud and they have their own challenges and own reasoning it's still relatively niche market the industry I come from in autumn or day a lot of simulation providers have been kind of worried that how they're going to monetize this utility model right which I am my user wants to pay for per hour of software they're going to use versus I'm used to leasing it over one year or a paid applies and stuff like that and we as a company Microsoft Office 365 we went through the same transition right we were used to selling office licenses per PC collect your cash and be happy with it we had to change our business model to office 365 where we are charging per user per year so we went through similar pains and we are trying to you know when we work with I squeeze we try to bring some of that experience to them and try to try to encourage them to look at a longer picture the more cores users are going to run in the cloud the better it's going to be at the same time running very very large-scale you know design of experiments or optimization so let's say I'm running a CFD code one thousand thousand cores and I want to run five hundred jobs like that now if my license costs keep exponentially increasing that thing is going to be you know 100 times more expensive than buying a cluster on Prem it's just ridiculous so but I see a change in this market slowly ancestry singly announced there pageview licensed star CCM from CJ adapt co has a utility based licensing per hour so there is change happening in this market and it's inevitable so it's I think we are moving in the right direction and some of these commercialize these are extremely crucial to make this cloud adoption really really mainstream and I my opinion is that we are moving in their direction and they have their own challenges also right so you want to protect you want to make sure ice with intellectual property is protected and they're able to monetize it because we don't want these people going out of business so it is critical so they're they taking their time but they're moving in the right direction good example of that is slumber J so we work with somebody very closely their application intersect for for oil exploration is offered software-as-a-service on top of azure so it uses to MPI it uses our low latency Rd make an activity and they offer that as a service so if anybody here from oil and gas industry you can go check it out let's talk a little bit on agile so this is my sales pitch now so I tried to keep it as technical as possible agile this is the great thing about cloud right 500 features released in last one year it's just phenomenal pace of innovation I joined the team I've been with Microsoft for 10 years doing different roles when I joined HPC team and I said I'm only coming on the team if you let me work on Linux and if you let me work on Linux RDMA and still have absolutely yes so we launched Linux RDMA July of last year since then we have done a lot of great things and will will walk through some of them it's just quick statistics on Azure about 90,000 new customer subscriptions 1.5 million sequel databases it's just 90 trillion storage objects we have authentication system called Azure Active Directory about 551 million users and this is increases by leaps and bounds every year the the 90 million 90 trillion storage objects it maybe took us four years to earn a double that from 45 to 90 90 but we project that in within the next two years it will double to like 180 trillion arm it's just crazy phenomenal growth what is a sure oh sorry Azure this bunch of services but really at the heart of it it computes storage and networking adjure was born as a platform as a service kind of platform so we do that really really well and now we do infrastructure services as well so the HPC piece that I'm going to talk about it's kind of really at the bottom here left corner compute storage and networking and then there are obviously services build on top of that for example if you use office 365 at your company the your credentials are authenticated through something called Azure Active Directory so if anybody is familiar with Active Directory on-prem used to stand up a Windows server you have Active Directory role you had a domain now all that can be done in the cloud as a service so these kind of value-added services on top of that they're like 70 or 80 different services we'll just continue to add them is what we have one of those interesting services which I am not going to go in very detail today but I thought this is a low latency infinite bank and a crowd so I'm not going to talk about a whole lot it's called as your badge as your bass think of as your badge as a HPC as a service where you get a cluster per job on-demand and it's available through different runtimes is available through API is it's available through CLI and you can we have done scale testing I'll just keep you know in the spirit of cloud I will keep doing numbers we have done scale testing on insurance actual modeling up 200,000 cores for example so as your batch is one of those services in Azure that contributes to HPC as well and that kind of is one of the largest European banks that we work with for managing the risks and compliance they you and they use you know 20,000 cores that kind of scale and actually do it all on Windows through our scheduler called HPC pack so they burst on-demand they have a data center on Prem but they bus to cloud when they need to especially at the end of the quarter at the end of the month when they have the regulatory reporting requirements let's come back let's dive in a little bit deep into what is our you know big compute philosophy or HPC we take pride in saying that we call it no compromise HPC I can obviously debate you know what that means but what does it mean for us low latency we think is extremely crucial there is a market for non low latency type workloads obviously embarassingly parallel but anything that is tightly coupled MPI bound needs low latency I mean there is no right there is no surprise on that so we strive to give you lowest possible latency we can dedicated non-blocking network for MPI obviously to InfiniBand close to bare metal performance so we run a virtualized environment but if you talk to our people that that have that do benchmarking on Linux on top of our our hypervisor you'll see very very minimal overhead we obviously believe in the resilient parallel file system so we have different storage options but one of the recent developments is lustre from Intel the parallel file system is available on Azure now with our premium storage and we see really really good performance running MPI workloads for scratch IO right because when you're running half a billion cell model you're going to get lot of i/o depending on the application we used lustre for that we believe in high frequency processors so I cannot publicly talk today what is our next processor is and configuration but if you want to see me in private I can give you the details we try to strive the fastest processors we can find largely memory fact nodes we're gonna double our memory footprint for our compute nodes very very soon SSD build is the base local store is up to two terabytes of local scratch on top of global scratch if you need coming for Microsoft I joined the team to work on Linux I will release Linux RDMA July of last year so Linux and Windows SUSE Linux sent over six five seven or seven one and soon read a hat rewrite is already available in Azure but the Red Hat on our DMA will be available pretty soon and then we can provision I can provision entire lustre cluster with SSH trust without a scheduler now soon scheduler will also be there I can provision 5000 course in 10 minutes completely automated cluster deployment so that's what we mean by no compromise let's dive a little bit on to virtualize already ma we use Network direct stack and we have develop our own stack all in do we support Intel MPI obviously Microsoft MPI on the windows side and we have a platform MPI in prototype from IBM so we work extremely closely between Intel we found them very open to work with us different knobs available in Intel MPI we don't support openmpi yet if there's a significant requirement from anybody in the audience please talk to me we support only certain pair of words and and we and we used Apple transport for MPI communication which is not supported in OpenMP openmpi and if there is significant requirement for it we're willing to contribute to the open source open MPI community and develop the dapper Transport OpenMP I used to support the Apple but they dropped it so anyway it's kind of a quick architecture basically VSP I don't know if guys in the back and see but basically vm buses so you have a host operating system and a guest and basically host is what is running the hypervisor the guest is I didn't Linux Linux or Windows in the guest and those guest kind of need to establish connectivity and register addresses so we do that through vm bus the VSP is and VST talk and once that connection is established then the operating system gets out of the way so application can directly talk to the MPI stack and the network and that's how we get low latency through our our DMA stack what will be awkward today our hardware is getting little older so don't erase eyebrows because we have a new hardware coming in and I can talk to you in private what that would be but today we have what we call a eight a nine instances that are InfiniBand enabled with our DMA interconnect these are Sandy Bridge 26 626 70 processor with 16 core each we don't another thing about you know no compromises we don't do hyper threading for our HPC nodes so you have one s two one CPU mapping or core mapping we have sixteen core on the box if you do a nine a VM with sixteen course you get 16 cores that's how we deliver close to bare metal performance you actually have 112 gig of memory ddr3 I know it's a little bit old but this is a very very balanced system that what we found the CPUs are not wait waiting for network to catch up any things like that so even if it is Sandy Bridge if you compare Sandy Bridge to Sandy Bridge you'll get very close to bare metal performance we have QDR InfiniBand dedicated for MPI traffic only a lot of time people ask me I'm using lustre can they use InfiniBand we want to reserve InfiniBand cuter InfiniBand traffic for MPI only we have a 10 gig front end through TCP and we use that for for for other kinds of traffic for lustrous storage and things like that we are bumping that 10 gig to 40 gig in our next front end for for storage and other traffic's we have 2 terabyte us crash per node again it's available different flavors of Linux and it's available in seven regions around the world for in u.s. two in Europe one in Japan and that footprint will increase very very soon to go even more global and I just said I actually had a slide and I kind of deleted it because I was told not to show that but we have working on next-generation which will be rolled out in couple of months I can talk about that that will be faster processor more memory better InfiniBand just gets better stock quickly on GPUs we announce GPUs so I can talk publicly about it don't make fun of the names the game it's called nv6 enemy 24 and envy 12 they're what we call V stand for visualization I have no idea what M stands for it's there you have 6 12 or 24 CPUs and these are the m60 and video cards we have the first ones in the cloud to offer that for remote visualization and it's available through DDA and you either get fifty six hundred and twelve and two 24 gig of memory up from half a terabyte to two terabyte of local memory and then this this doesn't have our DMA because these are Wiz notes it has 40 gig front and Ethernet video I have compute me M sweet K 80s again one of the first cloud providers to offer that in the cloud again they go from 6 12 24 and 24 our R stands for all DMA low latency network for GPU compute as well so either you get let me go back to the previous slide for a second I don't want to mention you get one GPU two GPUs or four GPUs so you get an entire card you get half a card or you get one GPU depending on your workload what you're trying to achieve our stands for our DMA so it will have in this case we are testing out rocky our DMA work on was Ethernet is the debate on that you know the latency especially through switch hops can you know creep up and maybe want to try it and so we have made rocky available with our DMA on compute nodes with K 80s again here you get one to four for GPU cards each GPU has about I think twenty seven twenty hundred GPU cores and you go from 112 to 224 that should say 112 it's not 128 M apologize and then if you don't want our DMA back-end by the way the our DMA back-end is dedicated you still have front-end 40 gig Ethernet so you have enough network manual to do anything you want with analyzer the good thing about the GPU visualization especially if you use our regular a nines or next generation or a nine and then you have GPU nodes right next to it in the same data center now a lot of time people say I do compute and I have to download all of my d3 plot files to do visualization of my crash simulation I'm not going to download 60 gigs of data per run well you don't have to anymore transfer the data you can map the same luster parallel file system onto vis node and now you have 40 gig pipe going between machine to machine you can with the visualization and post-processing of your nodes anytime so it kind of removes that bottleneck of data transfer so we are one of the largest chemical companies in the world use our a nine nodes with our DMA InfiniBand our DMA with up to 2.5 microsecond latency we use that up to 1600 cores for one job now it's okay up to you know ninety six hundred and twenty eight cores great any application will run on Ethernet some of them still won't because this is required depending on the message size you are passing right but when you start creeping up into 256 512 1024 1600 2000 if you don't have latency you can't run it it actually will run slower then by adding more cores so these guys are ran up to 600 core jobs and phenomenal results and they were able to launch their product in time it's a quick architectural diagram I know it's eye chart I'll make the slides available online but here is how we kind of basically we have compute nodes here with the head node dedicated our DMA for MPI traffic and rest of the traffic is routed through front end forty or ten giegi through a lustre Farrell file system we recommend that you use lustre because it's attached it's like a file servers you can have up to 60 terabyte per server and you can have five of them you have eighty terabyte pool for global scratch we don't recommend storing here data for months and months as soon as you know you're done with your simulation keep it there for a few weeks and then move it to blob storage for permanent storage you can connect to these obviously everything is defined to software-defined networking as David was saying before we have VPNs in Azure or so this is like a small cocoon with an agile data center that is owned by you you can pour the public ports off only keep the private ports for internal communication and you're kind of doing Natick through the head node right so you connect that to you to on-prem using Express route or VPN startup or hyperscale a little bit microsoft has more than hundred data centers but we truly have for Azure I would say twenty eight regions around the world there are two times to our closest competition we have taken approach of obviously massive data centers but we have taken approach of localized data centers as well for example we announced a data center in Germany that has a trusty model where German citizens and T systems locally manages the data nobody outside Germany gets to access the data same thing we have done in China we around the data center in UK we have three data centers live in India Canada we have a DoD cloud that we announced via white our compliance cloud already that's running out of space bigger is so popular we have Destin in Australia Brazil we have a third largest private network in the world and as I said 15 billion investment over the last few years so we massively continue to invest and we run out of capacity all the time because this is so popular and we are expanding like crazy so now every datacenter doesn't have HPC InfiniBand nodes today they are available in seven regions and soon that will double within the next few months tools and kriti you want to talk about luster and different storage options available obviously you have blobs which is the cheapest way to store data but it obviously it's it's kind of useless to do it while the job is running we have a jar files which is SMB 3.0 I won't recommend it for running it for high performance workloads it will we are working to boost the performance of SMB and you can use it through sips on the linux nodes we have Azure data leak which is for big data and it's kind of a service on top of azure blobs extremely fast throughput and there is no limit on your storage account blob has a limit of 500 terabytes for storage accounts so if you have two petabytes of data you got to manage storage accounts and kind of manage it you don't have to do that in actual data leak especially fie IOT connected car projects where we are generating terabytes and terabytes of data pork are agile data leak is a great option because you ingest so much data and then we have lustre so we work very closely with Intel it's available in azure marketplace complete automated deployment fluster and we do that on our premium storage to provide huge amount of data storage space for for scratch quick lustre architecture if anybody using lustre you are familiar with it and I'm running out of time I come from CA background so I work with a lot of I squeeze in my space very closely as you see most of the solvers here listed abacus L is dinosaur CCM fluent radio softest truck from out there PBS pro scheduler univer grid engine Intel MPI you just name it you know most of the support is there now and I have one last slide we just this is just hot off the press we used to do benchmark up to 512 course people saying you know that's like Tony won't talk to us 512 course so thousand 24 so we continue to push the limits this is a we're not allowed to say the name but it's a CFD code running a hundred million cell model very linear scaling and we'll just continue to push the limits as I said we have custom 1,600 core jobs so scaling is not an issue we just have I found finally time to run some of these workloads show you the benchmarks and if we have launched a new site simulation Dodger com no www so feel free to visit that site has a very detailed information you don't have to go from page to page to find information which is very typical in large companies so we created a micro site called simulation or agile comm we continue to update it so with that I'll wrap up and open for any questions 