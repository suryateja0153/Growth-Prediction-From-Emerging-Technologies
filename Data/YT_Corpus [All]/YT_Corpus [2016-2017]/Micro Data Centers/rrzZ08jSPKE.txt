 should we so we're 150 good to start perfect well thank you everybody for coming it let me first say you're seeing one speaker up here rather than the original three that were on the agenda all three have had different combinations of personal and flight issues so min from facebook wound up actually stuck in hawaii which seems like a very good problem to have Hopkins who need on our side or in California and stomach bug respectively so I'm going to be the stand-in for the day my name is Kyle Forrester i'm the founder of big switch i was also sort of one of the i won the handful of people involved in this particular initiative the war that we're going to talk about today first of all how many people here have heard of open compute hardware comes in this country and how many of like more than like the one paragraph but could probably like right like a page about what it is all right so like about half fish I'm just trying to measure where it where I think would be most interesting to spend time let me you know feel free to ask questions anytime please ask from the mics because the whole things being videotaped if you don't mind indulging me I'll give a little bit of background on on big switch in the company so you see where we're coming from but most of this is actually us kind of as a as a user here on our experience in our journey setting up you know setting up a small openstack cloud with the goal of it we actually add this explicit goal can we use the exact same hardware that's in production at Facebook and use that for a small scale openstack cloud for our office the implications of it would be far beyond that but it was an interesting sort of limited time sprint type of goal and I'm going to talk about what we found let me start off a little bit about big switch so you see a bit where we're coming from art our company very specifically takes network designs that are in production at Google Microsoft at Facebook at amazon and we package those designs for enterprise service provider and government use so in some cases it's even the exact same switch hardware that's running at Google or running at Facebook and we write the software on top that's our primary business in some cases the adaptation is more significant right our software runs on top of Dell and we changed a lot of the management interfaces to make it relevant for you know data centers that are kind of operated more traditionally if you've never heard of us before that's normal that companies I'm very very proud the company is on a tear since we launched our products our first bare metal products in 2014 it's a bit no need to go too far it's a really fun time for the company our big news here at the show this week is that we just announced jointly with verizon that for their very large scale OpenStack NFV pod they've selected that to run on big switch switching so you know for us this is a kind of wonderful validation of the work that we're doing we've been part of a number of very very high scale OpenStack clouds over the last 12 months thank you know zooming back from the company more specific around this we do a lot of work with the Open Compute Project were right now the companies were actually the single largest contributor of software back to the Open Compute Project in the form of an open-source version of our switch OS called open network Linux f boss Facebook's switching operating system has now been ported over 2f boss sorry i've also been avid poured it over to open network linux you know microsoft's saic as a roadmap to port over to open network linux so this particular part of our business is working with these hyperscale operators who write their own switching software and run that on top of the the open network linux layer of our open source i basically means that we want up spending a ton of time with the Open Compute Project I think it as a result we've gotten some level of you know wonderful access and and great hardware and got a little bit of time we spend certainly a ton of time with a Facebook wedge yeah certainly with the wedge 40 with wedge 100 and I think this is sort of the natural outgrowth of that work so here's the rack this is these are actually shots and we'll come back to a few of these these are shots of an actual ocp rack that was mocked up to be you know the exact same hardware that's in production they're running up in our labs and our sort of journey of of getting the thing up and running now first time we asked who here has a lot of people have heard of ocp hardware how many people have actually touched OCP hardware good that's great that's actually a lot more than I expected sets about a quarter so some of this you'll probably find pretty remedial but hopefully some of its interesting I think for the next bits more for people who haven't had the chance to actually touch it I'd read a lot about it but this was my first time actually touching the hardware itself how am I going to get through this one okay give me a second look the ocp wreck is is is 21 inches wide and the size matters for Freddie for anybody who hasn't you know what why did they choose this rack sighs all right well I mean the this is actually really painful for us the rack access to sit separately from the rest of our roads caused all kinds of sort of headaches with our ups guys about getting the thing in it becomes really important because for folks who aren't as close to it that a three servers you know I under if you ever played with like yeah like twins or micro Chessie's but this is a really clean 32 are you servers can sit side by side and this rack very very comfortably and probably just as important okay so you might have a little bit of play with the motherboard design but you can fit 60 disks per two are you run over computer may you all right slightly different size but the that kind of disk width layout gives you the actually this really intense density and a 21 at track that in the 19-inch rack you're actually losing a fair number of inches so there is a really I think there's a really good argument to say hey these 21 at tracks are an awesome innovation the downside is for those of us with kind of traditional sort of you know floor layouts power layouts it's a real headache to truck it in but I think there are a couple of cases where you know this innovation should be considered by groups at large well beyond you know the football field-sized data centers so 21 verses 19 inch one of the really there are two really important innovations that I think are kind of just make this thing really nice um everything that a human can touch is green and I might not seem like that big of a deal especially if you're like more of a software person but if you're kind of a hardware person and you wind up like racking and stacking stuff fairly often this is actually a really really really nice thing everything is designed to be field replaceable and field replaceable very easily so you can actually feel replace you don't need a screwdriver you don't need a drill and that's that winds up being like kind of a big deal when you're monkeying around in lab and certainly a big deal at scale but even we're just like a one or two rack everybody on our side noticed it like everybody notice how much easier it was just move stuff around on the rack now part of the innovation here they claim they get a thirty percent increase in power utilization I don't know if that's really I have no kind of comment on that that's a claim of the the power distribution itself is actually DC power that's in exposed rails inside the rack and power inn is actually like vampire style taps that come in from the compute nodes in the storage nodes so on the one hand you get this really nice benefit you actually save a ton of power because you don't wind up going through you know ACDC transformation you don't wind up going through DC DC step down that alone is actually a huge benefit but like don't touch the rack it's really important we had like yeah the doll was actually really awesome has anybody here ever held an ocp debug dongle till you're kind of modeling long i'm guessing what a curious to get your going to might take what was your take the first time you fool around do so this thing is a great idea and again like a lot of the design trade-offs that they made you know I'm trying to show that these are actually these are smart ideas but they came with trade-offs so the goal was to have you know basically one hundred percent ipmi no separate you know no separate serial access and like if you were you know that's around trying to find the right name serial cable or like you know this is actually it's one of these things that looks the idea of doing everything over IP mi is one of these things that looks great on paper and then practically when you're actually monkeying around in a lab and moving around it's actually a complete pain in the neck and so the idea that you know this debug dongle is that you can actually get very quick LED displays on like I mean really simple like you know basic sort of power check stuff kind of come in and out of the different nodes in the rack it's the exact same debug dongle that works across wedge the switches leopard the servers and the knocks the storage so you can literally just go around with one your hand and it goes into all the different at least standard like the facebook path through the ocp stuff so it actually turns out to be a great idea because frankly about like mmm every 45 minutes or so with our first like two days in the rack we're plugging it in and just looking at LED colors coming out I don't know if you've had the same experience the numbering like helps a ton and then the neat thing is it actually has USB access in and out so you're just playing with USB cables instead of fussing around with terminal cables which is really really really really nice so again let me come back it's one of these neat trade-offs it looks really good on paper to have ipmi on the access it creates all these basic like in the lab headaches and I can only imagine it a thousand racks that's a lot more of a 2,000 exit headache even it like 10 racks the stuff is a headache and the debug dongle is that is the way that they had addressed that headache and it's very elegant so so far I hope what you've seen is like hey there series of design trade-offs that they had to make all right hey no power dissipation okay that means that everything human can touch is ok you know their parts that you really really can't touch like hey all I PMI that means the dongle now here's one design trade-off that specifically for OpenStack I actually think is a really really big deal on leopard the servers only have one ethernet port and like there's a lot of control and data stuff that has to go in and out of OpenStack compute nodes I mean at least an art sort of reference topology we literally have like you know seven different logical very separate logical networks control networks right to go in and out of a compute node for a full you know under cloud and over cloud and running all of these on different vlans in and out of the same Ethernet port kind of seems like it's possible but the trouble that you wind up having is startup conditions where you have to do things like you know make sure that the VLAN tagging that you're using for first booze or synonymous or synchronous with a VLAN tagging that you're going to use for separate ap OpenStack API control and like the witch vlans you set up first actually ones that it's incredibly frat like this ordering has to be exactly right or the whole thing falls apart really fast so there's just that very practical headache and then there's the obvious one of it with a single Ethernet port right this is a real single point of failure that's kind of I only don't need to go into that obvious for everybody in the room but this is like for openstack use this was actually like the workflow around getting the under cloud up and running and then the over cloud going this was actually the single biggest non-obvious headache that that we ran into now newer versions of the ocp hardware have options for the mezzanine card with two ports and so suddenly like that's kind of a game-changer right and it's not the exact stuff that's running in prod but like it makes a huge I think you have two ports is plenty for the work that kind of work that we all need to do together so we were able to wrestle through a lot of the first boot stuff and we came to a happy ending right we are actually currently now running up yeah a nice-sized you know a nice little kind of small dev testy size openstack cloud up and running on the exact same hardware that Facebook is using in prod so that was a big point of celebration for us you know just some of the fine print the stuff that kind of scares us about even yeah certainly about using this in like enterprise settings or service provider settings you know the single Ethernet port is probably the biggest kind of scariest thing there we go and then there's some other bits and pieces we had to upstream a patch for discussed nation bug UEFI boot mode is not was not supported on the Installer a fuel installer that we were using so we had to boot up in legacy mode and then there was definitely some like I p.m. i vs we tried the vendor ipmi and we spent a lot of time wrestling with it we finally got it to work open BMC which is you know more the ocp sort of style of doing this is a is it we think a bit of a cleaner path but we timed out before being able to use it and we're solve some of the headaches so it works but with some limp on it so let me finish up with this idea okay so where where to go from here I'm first of all I was amazed and impressed that a quarter of the people here actually touched ocp hardware that's a lot more than I was expecting and that's a great thing if it's specifically the exact version that you know the Facebook team has in prod I would be really interested to hear from you but I think let me and on the this idea of hey what path for everybody else what path you want to take forward with this and there's one that says hey look for special use cases this stuff actually today is pretty pretty damn good the thing that jumps out at you is the density I mean like a half-rack running like just a massive amount of compute and storage and so if what you're looking for is like super super high density and you're willing to live with a lot of other trade-offs especially likes of you know the single point of failure on the ethernet link and some of the headache about getting the thing up and running this thing today might be great so we have one environment on our side it's a in one of our engineering continuous integration environments like it's a CI environment so actually the density of the compute and storage matters a lot resiliency doesn't really matter you know at all for this particular environment at least not not much that's we've already done it once we don't mind paying with the setup headache so for that like really specific case this is actually useful today but would you say it's a general-purpose openstack cloud right for all the different workloads at least that we have running both in our engineering labs as well as our public facing stuff it's not we're not moving our general purpose openstack cloud over to it so this would be very very specialist to use on our end there's the pragmatist argument that says hey instead of using the exact stuff facebook has in production use minor variants right the 21-inch versus the 19-inch rack that at least for us was the biggest decision and there's kind of 19-inch things that are kind you sacrificed quite a bit of density but they're pretty close to the facebook like to the Facebook prod parts and they're certainly ocp designs for them and then whether or not you want to use the exposed power rack you know that's you know so up to you but I i would say for if you're going to take a very pragmatic approach to building a general-purpose openstack cloud and you want to use ocp hardware i would really urge you to think strongly about using ocp ish hardware the enterprise adapted versions of the stuff if you want to get gone today and then the last is if you want to take the idealist approach at least it's been our experience and Bob I saw you when you're walking in I'm guessing it's I'll be curious to hear from you it's been our experience that the ocp community is incredibly easy to work with very very very flexible it's it's easy at this point the community is small enough it's very easy to get things on a road map so if you want to instead take the idealist path and say hey I want to be part of shaping this community to make it more relevant for the general purpose OpenStack clouds I think a lot of that work is already happening with some of the work that they're doing but there's always going to be edges to say hey here's a particular openstack cloud what we'd like to get on the hardware roadmap if you want to take this hey let's modify and mold ocp for OpenStack please talk to me after this is an area where we're really interested as a big software contributor and the form of open network Linux we're really interested in seeing ocp grow and ocp successful and ocp growing out of the you know the couple of use cases and a couple of verticals where it's being used today and if there's anything that I can personally do or my team can do to help you get you plugged into the right people this is something that you know we feel strongly about is probably part of in the mission of our company and we'd love to help out where we can I'll be you know downstairs at the big switch booth after this so thank you everybody on let me in there and take any questions as a as you happen thanks a lot 