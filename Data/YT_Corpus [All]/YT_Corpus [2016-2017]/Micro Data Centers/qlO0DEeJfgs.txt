 okay hello I'm Kevin dearling with mellanox technologies and I'm here to talk about building clouds with Lego and really applying Lego principles to OpenStack clouds with what we call open composable networks so first of all I think many people may know mellanox as the infiniband company that's where we started back about 16 years ago we are the dominant supplier of InfiniBand connectivity for high-performance computing but more recently we've really launched into the ethernet space and we sell a lot more Ethernet today than we do InfiniBand technology and in fact I would say that four out of the top five hyperscale data centers in the world or our customers unfortunately most of them consider what we do part of their secret sauce and don't let us talk about it publicly although a couple of people do so you know Facebook for example and Microsoft have both publicly talked about the fact that they're using our interconnects inside of their data center and these numbers you know I think everybody understands that with over a billion users now in facebook all of these different applications that are driving more and more data really building clouds is all about doing so very efficiently rapidly and at a very cost-effective price points so when we look at building clouds we really see three elements that are required and the first of these is where the Lego comes in and we call this composability and the second piece is programmability and this is where software-defined everything comes into play software-defined storage and the last is predictability and this is really something that I'll talk in some detail about that is where you can get into troubles when you deployed things and the results that people are looking for is flexibility and efficiency and value and really simplicity and all of these things need to come together to make that happen and I think open stock being here in Austin it's great because it really brings all of these different pieces together so I'm going to talk first about the composability piece and the Lego and so you know one of my colleagues here Chloe has a young son and she was talking about the fact that with Lego he's able to build many many different things and one of the first key things is that all of the pieces are really fairly basic and all of those components can come together then using standard interfaces all of the pieces of the Lego actually are very well-defined and connect well together in terms of their interfaces so they can be combined very freely so you can put things together and then really it's up to your imagination about how you build these things because when you'd have the first two principles now you can do automation and really use your own imagination to build whatever you want and to do so in a way that works efficiently and you can scale that very nicely and so here's some pictures of things that you can build with Legos and it's kind of hard to see but there's a little man there standing in front of the giraffe so that's about a 20-foot tall giraffe and a car and other things and really we see many of the same principles here applying to OpenStack and building clouds as when you build look Lego so we call this open composable networks and really this is the first piece this composability we're open platforms need composability and one of the key things that we've been able to do is take our hardware and then build standard api's on top of that and a couple of the AP is that I want to highlight this is for our Ethernet switch the first is called switch dev which is part of Linux and the second is called sigh which is the switch abstraction interface and actually was defined and one of the sister organizations to OpenStack which is the Open Compute Project and both of these are standard ap is so those of you who are familiar with netdev for example when you write an application to an ethernet nic you don't really care whether it's a mellanox neck or one of our competitors next broadcom neck or intel nic you right to the net dev interface and it's up to the vendor to supply a driver that conforms to that api and so a couple i guess it was about six or eight months ago one of the key developers in the Linnet community said boy wouldn't it be nice if the switch ASIC vendors did the same thing and so Dave Miller throughout that challenge and we heard it and so we came back and we contributed switch dev and for our spectrum ASIC to the market similarly working with Microsoft believe it or not in the open compute project we developed sigh which is the switch abstraction interface so when you start looking at a ethernet switch there's a lot of things that you need to do to bring that switch up bring a physical link up bring a logical link up you know you can read porch you can bring you know read turn fans on all of those things frankly there's not a lot of differentiation between individuals which asics so by standardizing all of that we really enabled now the whole market to build components on top of that that can plug and play so a good example was microsoft sonic which was announced that the Open Compute Project month ago and very interestingly that is a completely linux based platform I wouldn't be at all surprised to see them become more involved in OpenStack the other one that we announced was cumulus Linux and I'll talk a little bit more about that in a second the third one that we showed was HPE open switch and so this is the new open source switch network operating system running on top of multiple platforms again metaswitch is another company and then we had our own operating system running on top of that so really with this lego building block concept you have open platforms and you've disaggregated the hardware from the software and what open composable networks is all about is how you put it back together easily in a way that gives you flexibility and scalability bility and automation so to give you an example this is cumulus this is their integration with OpenStack so they have a cloud controller node you can merge that obviously with your compute nodes and then they have a platform that they call the RM p which is their rack management platform and then on top of that it really gives you access to a whole range of SDN controllers so again part of that software to find everything whether it's me takura or nuage or plum grid or anybody's Sdn that conforms with open flow then you can build Sdn on top of this again you're taking individual components and building on top of standard api's if you look at the cumulus viewpoint and the philosophy associated with the cumulus Linux basically it's a hundred percent open and it's linux throughout and then this drives of course value and the cost that I was talking about and so with the cumulus networks on top of the spectrum switch you get the combination of both one of the best hardware platforms in the world and the best software platforms particularly if you want to manage your network as if it was servers so one of the things that differentiates the cumulus networks offering is that a switch just behaves like a server that happens to have a lot of Ethernet ports in our case it's 3200 gigabit ports so it's a lot of very fast ethernet ports but you manage it as if it were just another server in your network so that is appeals to a whole bunch of folks and we'll talk about some of the people that are deploying that a little bit later so the second piece that I talked about was this program ability and software to find everything in particular software-defined storage and storage is really interesting because right now there's three major transitions occurring at once and that always makes things interesting the first is where we see solid state disks replacing the traditional hard disk drives and in particular something called nvme which is non-volatile memory Express so it goes over PCI Express very very high performance solid-state drives and I'll show some of the performance data here in a second the other thing we're seeing is software-defined storage sometimes called scale-out storage or servers and storage so instead of these big monolithic arrays that we're familiar with with fibre channel connectivity today we see scale-out storage happening and I think OpenStack is a great example of that we see things like SEF and neck senta and Zadora and others that are exploiting this scale-out principle for storage and the third thing we see is hyper converged where people are trying to take all of the compute and storage and put it into a single entity and that's another opportunity here where I'm starting to see announcements that for example mirantis and quanta put together something that looks like a hyper converged platform the good thing here is all of these things need a lot faster network it turns out if you break open one of these big scale up boxes if you look at an EMC chassis or if you look at a oracle or teradata or really any big storage platform inside you'll see mellanox so it work kind of in every data center in the entire world running mainstream applications that are critical every day but you don't see us because we're actually the internal network that's moving the data and for those of you are familiar with storage you have something called write amplification you do one right and it spawns five rights in the backend network that's why you need something that's very low latency high performance and has all of the transport offloads when you do that in the cloud and you start to do scale out you actually need it in your network so you'll see that many many of the scale-out storage providers today or using mellanox for their network for the same reason that the big scale up people uses for the networks so I said that faster storage needs faster networks just to give you some idea if we started with a SATA hard disk drive you can see here it took 20 Wars out of disks to saturate a 10 gig link and that means 250 SATA drives to actually saturate one of our hundred gig links which were shipping if I fast forward to SSD is suddenly two SATA drives can saturate a 10 gig link and we need 24 of those to saturate a hundred gig link but as we've progressed here with SAS SSDs we get down to one SAS SSD saturating a 10 gig link and only 10 for a hundred gig link and today with the nbme we're talking about a single nvme drive will actually generate close to 25 gigabit per second and for we've actually seen three saturate a 10 gig link with even some of the newer ones so we've really moved the the bottleneck now so instead of spinning rust that took seven milliseconds to ask says we've moved the bottleneck from the disk to the network and the latency here today used to be on the disk drive this is the the spinning rushed with seven milliseconds and who really cared about the network and the software but as we go to SSDs we're shrinking that as we go to nvm suddenly the network storage and the software associated with it become a critical part of the overall latency equation and so now we're really reworking all of those things at a very low level to actually optimize the protocol in the network and so we're doing things like nvme over fabrics to reduce the latency so that we can get the performance out of the new memories that we have so for example a single nbme drive can generate about 25 gigabits per second so with the 10 gigabit link you're throwing away two-thirds of the performance of that nvme drive so how have we applied this to OpenStack this is just some data that we generated force F here and we had multiple partners that we've worked with super micro quantum and scalable informatics for the three up cited here it's really interesting that we've been able to get in a OpenStack self environment actually show the type of performance that we're able to achieve here anywhere from you know 2x2 7x the performer based on the higher performing networks here we got 70 gigabits per second of throughput with our hundred Giggy adapters with scalable informatics so this is all cept related workloads that we've done the next one I wanted to talk about was neck Senta we actually just did this war Victor's hear from our team in the audience I can see him in the back did this work just last week where we took the new neck sent to edge and we ran that on super micro servers with micron flash drives and you can see here that at 50 gigabytes oh here we were running just 25 gigabits and 50 gig we didn't do the hunter gig we got about 3 gigabytes per second performance frankly this was pretty much out of the box we haven't done any performance tuning yet we need to add another gateway device and I think we'll be able to push these numbers even higher so it's pretty encouraging that when we can pull something together pretty quickly we just published a blog on this today so I encourage you to go to our website and you can look at that so the other key piece that we really are looking at is all sorts of new offloads inside of the network itself and so as we get the 25 gigabit per second we say that 25 is the new 10 we're seeing very rapid growth of 25 gigabit and many many people adopting this technology so our connect x4 LX delivers 25 gigabits per second the connect x4 is our hundred gigabit per second neck these are both shipping today with the Kinect x4 we've gotten TCP throughput of 93 gigabits per second one of the key things that were doing in order to deliver that kind of performance there's a tremendous amount of activity that you need to do not just the normal lro you know large receive offloads and the transmit equivalent of that but really managing at a very very tight level all of the interaction with the cores on top of that there's a new thing that's called dpd k which is the data packet development kit and so with dpd k you're actually running in userspace we were able to demonstrate 34 million packets per second with dpd k at 25 gigabits per second so DP DK is a really great infrastructure for building and FV and I think NFV is one of the areas where we see the OpenStack community embracing both OpenStack and dpd k to develop and FV applications on top of that and I think a key here is that actually managing all of the individual threads and scaling that is very important beyond that we're doing work now with OBS offloads so if you're familiar with the open virtual switch normally you think of that as a software construct something that's sitting up in the hypervisor now it turns out that just like many of the things that we've done to accelerate TCP there's no reason that we can't accelerate OBS as well now some people have some problems with that in the sense of sort of philosophical problems because they want the control in the actual hypervisor itself and so in fact we keep all of the control plane so all of the control is in the hypervisor remains there if there's a flow that we don't recognize we throw it to the hypervisor and it generates it policy and installs the flow into our device and after that we accelerate the data plane activity associated with that so all of these things together when you're talking about twenty five fifty and a hundred gigabits per second you really need to accelerate every piece of the network otherwise we've seen situations where somebody's plugs in a 40 gig Nick but they're not using the accelerators we have so that can be VX LAN encapsulation and D capsulation as well as all the stateless offloads and they only get about less than 10 gigabits per second so they paid for 40 gig adapter and they're only getting 10 gigs worth of bandwidth as soon as they turn on our accelerators and we say oh you need to do this you need to use this as our Iove you can turn on the rocky capabilities we have lots of accelerators to get us up to these performance levels and the other key part of this is that it's not just the fact that you're getting performance but because you're doing the offloads in one of these cases we saw that we were consuming four cores at 25 gigabits per second and when we turn our offloads on for the OBS we went down to zero cpu utilization because we were doing it at hardware so we're giving the cores back to the application processor to be able to run the application because at the end of the day that's what you're interested in if you're in NFV you're not really concerned about moving the data it's about running whatever application you want on top of all that data that you're getting when we go to 100 gig if you extrapolate if you were using four cores you're going to use 16 cores to run your transport protocol stacks that's not what you're paying all the money for an expensive CPU and memory subsystem that is the most expensive part of your server you really want that part to be effectively free in the Nick itself and then use those CPUs to run the application processing so the last piece is predictability and here we talk about our spectrum switch one of the things when you have a network you really need to make sure that you have everything working smoothly in that Network and here we're talking about things like packet forwarding rates the ability to withstand congestion latency and really predictable behavior and I'll talk about some of the things that we see there so spectrum is our new 100 gigabit ethernet switch uses what's called a fully shared packet buffer and compared to some of our competition that breaks their chip up into four pieces that causes a lot of problems so first of all you get very inefficient buffer allocation so if you look at the the data sheet of our device it says around 16 megabytes a buffering and you look at another device it makes a 16 megabytes but suddenly if only 4 megabytes is allocated for any given flow you can imagine that you get much worse performance when inevitably you see what's called in cast or micro bursts occur in your network and when that happens you're going to lose pockets and you're going to wonder what's happening in your network your performance is going to go down and so managing all of those you know individual consumers that you have in your cloud is a challenge so when we characterize this we saw that anywhere from nine x 2 15 times better in terms of the ability to absorb a microburst so this is when multiple devices are talking so let's say you're doing a backup or you're accessing data from a storage spoiler or you might be doing daily backups or hourly backups you're going to have an in cast situation and the question is is how long can you sustain and how much data can you absorb in terms of these microburst and here we saw about a nine to fifteen times better performance capabilities than we saw with our competition namely the Tomahawks which the other thing that was really interesting in terms of predictability and trying to understand what's happening in your network we saw that if depending on the port's that you choose the bandwidth that gets allocated to an individual virtual machine can be very very unfair with the tomahawk so in certain circumstances you'll get more balanced bandwidth allocation but in other cases you'll end up with one virtual machine or one connection sharing all of half of the bandwidth and then all of the other virtual machines that are connected to other ports going to servers are sharing the other half and this will change on the fly if you change your connections but these days you really don't want to send anybody into a data center and ask them to unplug a device that's all happening manually so you're actually moving virtual machines and doing vm migration and so these activities are really transparent to you or invisible to you because you're doing vm migration so when vm migration happens and suddenly your performance because you've changed the physical port that you're connected to goes from something that was very very good to something that's very very poor it leaves you wondering what's happening and it's very difficult to debug in our case we fairly allocate the bandwidth across all the ports so no matter which ports you share and change and whether a virtual machine moves the band width is allocated fairly no matter what and this allows you to give service level agreements and have predictability in your network so the last piece here is some of these things I was talking about are in cast situation in cast is when you have 225 gig ports talking to 125 the same output port sooner or later buffers fill up and it's a question of how well you can manage that but there's another thing that goes on which is just how fast you can forward packets and in particular in many applications whether it's a messaging application or if you have a common node that's actually doing the storage demons for example it's getting lots of small messages for synchronization then the question is is can your forwarding asic and the switch ASIC actually keep up with the forwarding rates and this is what i call a voidable packet loss because it's avoidable meaning if the asa can keep up there's no in cast it's just I have a lot of packets coming into the network and those can be acknowledgement packets or management packets or messaging packets and then can I keep up and keep forwarding them out and what we've seen here is we are line rate across all packet sizes from 64 bytes and up this is very important for telco applications in particular you need both the endpoints that I talked about and the switching network to be able to generate you know full line rate across all the packet sizes with the tomahawk we saw here for example that from you know basically 64 bytes all the way up to fifteen hundred or two hundred bites it was dropping packets so really a critical difference here between the two so all this information is available if you look at the thali report here it's mellanox calm / totally I encourage you to read that and you can take a look at that and with that I want to introduce one of the weight we have I think one of our customers Cambridge is here and they've built an OpenStack cluster but we also have enter here Mariano from enter who's going to talk a little bit about his use case and how he's been able to take advantage of some of these principles oh so this is my last slide actually so this was just the overall integration that we have with all of our partners and then an example of that I think Mariano can talk to it thank you came in hello everybody and thank you for being here and let me introduce enter cloud sweet centre clouds which is a project from enter enter is in Italian companies it in Italian ISP that moved from being an ISP five years ago to a cloud service provider and enter clouds which is its main its flagship product it's a cloud based on OpenStack the deployed in three different regions in milan italy frankfurt germany and amsterdam in the netherlands it was actually the first multi-regional public cloud based on OpenStack in Europe so when we started it was too we started to work around enter cloud switching 2012 and OpenStack was pretty young and the solutions we had there was at the time there was a huge fight for network vendors to get the throne it was the game of Game of Thrones for for the plugins I think it's finally over and what we wanted actually was to be able being able to decide at what level to put our in our control plane and just to use underline levels neutrally so we clearly wanted to move away from locking vendor locking you can see on the on the left image you had this may be Cisco or something out juniper boxes where you both essentially the forwarding plane coupled bolt into the control plane we wanted to move away from this approach because we have seen opus that was doing exactly the same on the server side we come both from the data center services and from the network so we were able to manage this complexity at a higher level so what we wanted to do was to being able to use our choose our Hardware use API to manage the hardware and use the functionalities built-in into linux kernel and on top of that to run the user soft to the management services and eventually now the containers to run application at a higher level so this is a disaggregated process that was started in the data center services like fertilization but it was ready to be imported into the network also so why reinventing the wheel so there are so many operating systems we have evaluated the many of them before choosing and then we decided that we already had the common language that everybody in our company but also in the OpenStack environmental was able to use Linux so we wanted to use Linux to manage all of our hardware and all of our infrastructure both servers and networking so what we wanted actually to do was you know we are a small company I don't know maybe some of you will work for small companies and to run a public cloud with OpenStack may be a challenging task so what you need to do is to be efficient every time you need to have people being able with the same skills to work on very difficult very different aspects of your infrastructure they can range from networking to storage to virtualization to anything that the OpenStack word can bring to so you definitely want your users to have one single common interface to cope with all the infrastructure you are running so ansible is one that we use we also use puppet and so why not bringing all these automation to the networking part so at the end what we think is that we by using mellanox and cumulus we are going to reach the same model this is actually the same model you can have in OpenStack it's not very different you just run different hardware in this case the hardware is just forward in place playing in the hardware just forwards packets when we decided to work with the guys at mellanox we were wondering whether to how to solve our issues we had with our previous cloud platform we were coming from a VPS product and we were using Nova network on Essex it's still running and we were working with sx Nava networker with villains and then our marketing and our brilliant CEO here decided to provide the rate a free trial registration with the email that brought us so for 14 days you could have any birth machine for free so we we got flooded from Vietnam Thailand us anybody with the email and email could access our infrastructure and run the amsa so we we called it the trial disaster because our villains soonly ended so in the next release we'd open with enter clouds which we decided to move to more extensible network topology and so we decided to run the excellent at a time was alpha version so the RFC was not very settled and our main concern was about the fact that vlans the excellent required UDP a lot of UDP tunnel so the the concern was that providing cpu to the action would be kicked away from the customizer so that we decided to work with mellanox because the UDP offload was so affected that we have seen a nine point we used 10 gigabit nics so we have seen 9.3 affected iperf bandwidth which is which was pretty awesome on our nodes also we use SEF as a block storage device so we use the 40 gig Nick's double double dual nics on on top of SEF nodes so it was very important for us to have a large bandwidth also because we use SSDs inside the set free installation a last thing to say is that when it comes to OpenStack you cope with a lot of vendors the biggest one the smallest one and one thing they provide the promise you is they're going to support you but when it comes to real support you need someone helping you to solve the problems once they show up we had an issue with the driver with the overdrive which is the open source driver for mellanox because we were trying to fix some some issues they had and so they are allowed us to send one of our technicians to the Israel are in the office he spent three days there with the best engineers they have and they provided the solution in three in three days so we had the problem fixed it was a pretty big bug to fix and we had it fixed so when you have to face a challenge like running an open openstack cloud there are not many vendors that can help you except the ones they are betting their company on your success and mellanox did this with us so thank you very much thanks so with that I think we've got a little bit of time left so happy to open it up for questions if not there's a question go ahead you can use the speaker microphone for just talk for example you're looking for me today GA yeah right and then one thing another thing is are you guys looking in chi for example to mess with your partner's calico project calico and conda's postura to use those approaches yeah so I'm not the right guy that's way above my pay grade but eras is here somewhere who would be able to there is right behind you so you can answer the question about ml2 and I think Chloe can probably talk to the Metis which questions so I encourage you to come by our booth and have some detailed discussions any other questions so you mentioned that your NIC can do all floats right yes I had a reference to an equinox designed but program black slate is inside that could you talk more about that so I'm not sure there's quite a bit of programmability and our existing I'm not going to talk about the future things that are coming you know there's always a spectrum of programmability but our next today are quite programmable so inside of the device you can have open flow rules where you have it recognizes the flow and it has a set of actions and then you can filter that and loop it back through and do multiple actions we can do end cap and D cap so it already is quite programmable it's not programmable in the sense of our other devices so we acquired a company called easy chip earlier this year which is there's two parts of it one is a network processing company the other is a tile era was the name of the company that is a multi-core processors arm-based processors and so those are infinitely programmable their general-purpose processors or network processors and so we've talked about the fact that will see those technologies come together the mellanox technologies and the easy chip programmable technologies in the future great okay yeah so so the way mellanox works we actually everything on chip is sort of a cache and so then because we have access very rapid access to external memory then if we don't have an on-chip we can go fetch it off chip so the answer is effectively infinite but it's how big the cash is and I actually don't know in the latest connect x4 maybe eras I don't know if you know basically yes so the question was if we can upgrade to your open flow so basically the current design is so that every flow that we cannot address in however will go to the user space back to program to the obvious but internally in our silicon we have some degree of flexibility we can add capabilities that we don't have today with some flexible parsers that can somehow add capabilities that are not available or not designed today yet so not for everything but here's the reason why okay okay so there's no more questions we can go and drink beer 