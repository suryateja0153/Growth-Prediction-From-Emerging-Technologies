 are we good to go now yes okay thanks for coming everybody my name is Mike water so I'm gonna be giving you a kind of a one-year update and I'm gonna be partnering here with Steve wall he'll be up in a few minutes but this is kind of a one-year update last year we gave you a preview of what digital globe was planning on doing and this is kind of where we're at right now and the lessons we've learned we're trying to find hard to impart our knowledge and what we've done and and learn any lessons and get any feedback from you all it'd be great this is a shot of Mount Fuji actually as we have a constellation of satellites an image of the earth so if you've ever seen Google Maps Bing Maps Apple maps any of those we supply almost all their imagery to them so that's what our company does and this is a fun picture it's Mount Fuji as it come over the horizon as our satellites were way off to the side so really awesome so profile view over no Fuji so I'll give you a little bit of quick background on what digital globe does we'll fly right through this you only take a second we uh we have a constellation that like I said a satellite sat image the earth we collect about three and a half million square kilometers of imagery a day which is about the size of India or if you like little things 21,000 times the size of Liechtenstein we don't link about five terabytes to six terabytes of brand-new data a day and that's highly highly compressed we turn that into about 40 to 100 terabytes of new products per day that we ship out to customers so we on average add tens of petabytes of stuff every year to our archive we have tons and tons of tape and over about 60 petabytes of spinning desk disk at our shop so huge i/o company we also have a platform in the cloud where you can actually bring your algorithms to our imagery so we don't have to ship you these hundreds and hundreds and hundreds of terabytes of stuff you can come and torture our imagery in AWS so we have a few satellites up to whom we've been retired we've got four other ones we'll just kind of fly through this here so what do we do in 24 hours we collect that seven days month six months a year we can paint pretty much the entire earth several times so what what happened we needed a new architecture we had a monolith you know insert monolith story here everything integrated through a database it took forever to get anything new into production so that led us to do lots and lots of fun unnatural things probably like everybody else where you'd do a bicycle to a wagon to get it to avoid the production just because it's easier to do that then actually get something new deployed very very project focused at our shop - so if you wanted to get anything done you had to tie it to a project and then every project would twist the monolith to it's own desires and collide with other projects and it was not it was not good but we had a new opportunity with our next satellite world before it's gonna be launching here in September of this year where things can change but that's what it's scheduled for now we got the opportunity from upper management to give us the green light like look we even were tired of this old system the cost to twist the noose the old system to meet the needs of a new satellite work you know they weren't quite approaching the cost of the satellite but it was getting ridiculous and non credible so we got to start over what are we gonna do so we as the enterprise architecture team we knew that building a path was going to be key we had to have something that just let developers do the best work of their life and not worry about his IT provision my my VM yet has IT done my f5 rule yet you know they they should be writing lots and lots and lots of business values what they should be doing so we we surveyed the landscape created some knock out criteria for a pass what we needed it to do at our shop if it doesn't do these it can't work here we bash that against the list of passes that were available Cloud Foundry was chosen as the leading candidate so we did a few quick prototypes we verified what we called these knockout criteria and you know did things like de A's do rolling upgrades all that sort of stuff to make sure it never really went down reported a couple different apps or major languages are Java Ruby Python and report we created little sample apps and stuff like that in simple apps and ported a couple just to make sure it would work built some staffing and pricing models and went forward so kind of our path that we found ourselves on as I know it's a poor graphic we had that the Pioneers was what we did we stood up one team and we called them the Pioneer team and their job was to go get bit by rattlesnakes and step in the cactus and then figure out how to do all this stuff it was really good it was a great exercise they learned a lot and we just started this kind of learn fix adapt cycle went really well they they learned a lot and what they learned we would feed back and fix make better things that were painful we'd go back in and remove the pain and then we expanded it to beyond the Pioneer team to another team and started developing a little bit more code a little bit more code and then once you added a few more people you don't cover way more problems and well I just asked Bob to do that well let's automate Bob now right so we we kept learning and learning and learning and adapting and now we're up to I think a few things don't run well boundary minutes but almost all the apps are built and designed to run on cloud powder so some of the lessons we're learning is vocabulary is important so when you're talking with somebody about developing things and it's they're describing how they're doing it and you're like that that will never run what what are you talking about that can't run in cloud foundry you're talking you're talking about s AP right yeah I'm not gonna do CF push s Appian and be done and they're like no no I'm running that on a VM oh okay well that's we got wires crossed so we come up with this vocabulary of these patterns of apps right it's a pattern one that's a 12 pack trap it's gonna be running in Cloud Foundry pattern 2 that means your VM pattern 3 is bare-metal with all the big pixel data we push around we have huge HPC clusters that are you know compute on this imagery so we have a lot of need for a lot of bare-metal but just this little nomenclature has caught on and everybody knows what are you talking about pattern one okay great you got a whole context around that you know you can just move on it's kind of funny though that we need some people to run and run our operations and stuff now so they've put out some some wrecks for hire and I seen in the wreck must be familiar with pattern one two and three apps that's not the industry no one's gonna know what that means so some of the things we learned and these learn fix adapt cycles hopefully will well our goal here is to help you out and maybe you won't get bit by the same rattlesnakes and step in the same cactus that we did but microservices sprawl fast you give the developers the ability to see you push and push fast and push anything it it goes gonzo so fast and so we had to we always had a plan yeah we're gonna do Eureka we're gonna do console we're gonna do something for first service discovery but it was a massive forcing function too we had to get that up and running quickly because of just how fast everything is for all centralized configuration people were pushing apps and some people were configuring through CF env variable other people were bundling property files into jars and huh you know cats and dogs so we're like no we're gonna go a spring config server we updated the spring config server to have a Postgres back-end it'd be nice if we could contribute that back to the open-source community but it gives it a really nice way of everybody attached into that and grabbing their config we're also wondering that API management is hard and I get angry emails just a few minutes ago on API management still we're learning but we're using we had a legacy product in-house from software AG central site where you can track your services and who's consuming their services and stuff so you have at least some dependency map of who's dependent on what and why so that that's been helpful we're also using a tool called apiary I don't a PR II but it's a good design tool on the web we've bought into it pretty heavily makes it nice for testing your api's and giving them a pretty easy way to do some work down to define your API I'm still learning you don't have any golden magic sauce here yet but we're learning but that's what the tools we're using now and they seem to be doing okay we even have the software AG's product integrated into our pipeline now so when you first time you hit deploy on an app to get it running through our pipeline it'll check with central site to go oh I have no clue what this service is you can't proceed past go until you tell central site what it's all about so we're also learning decoupling code deploy from feature deploy this is critical to people are familiar with it I'll Park the people who aren't so familiar with it it's it's the Zen art I guess of being able to have your continuous delivery pipeline continuously delivering but not actually be turning on new features in production right have a very controlled way to turn on a feature so the code just keeps flowing like it's supposed to but you can go configure that new feature on in like a user acceptance test environment and they can poke around and test it and like it and then you just there's no big big bang production day that's codes already there just go turn the nuclear launch key to on and in your career features live in production so we're still learning there we're trying to integrate that where we're trying to use a feature flipper for Java FF for J and we're trying to enhance it to use our centralized configuration server so we have a centralized configure the apps and centralized configuration of all of our features so that's the end goal of that we hopefully get there some other things we've learned is standard standard standards or your friends some people will kind of be the Wild West is ok just let people do what they need to do but we found a ton of value in having every app your slash endpoint is a very standard endpoint and what it returns who you are what bills you are you know a little bit of information about you that slash status and slash health check slash status will check your immediate dependencies so like your database connection or if you have a dependency on a file system or whatever it is it checks your dependencies like that and a health check to checks your remote dependency so if you depend on a remote service it'll actually go out and hit the slash status of that remote service so it gives us a very standard way when things are deployed I know I can go to slash see what it is go slash data see if it's kind of healthy and go to slash health check to see if it's really healthy so the monitoring monitoring teams are integrating all these calls it's been pretty nice our build pipeline are actually rebuilding our build pipeline we we tried to do everything in Jenkins soup-to-nuts and that didn't work out too well and we're trying to now choose the right tool for the right job Jenkins does builds really good but Excel released from zbo labs that orchestrates releases pretty well so we're we're we're doing that now that's a big big effort to redo our pipeline and one huge landmine if I can keep you from stepping on something at least this was big where I work is we have a bunch of things like that common the config server that's common for everybody to use we created this notion that there's all these common services out there and well wait a minute I'm writing this is that I thought that was supposed to be the one place where the company come to get this so isn't that common well yeah well that makes everything common right you should only write things once you should only write the function for the enterprise once not ten times so technically every service is common right so we we decided that it would have been a much better path to call these config service and things like that the utility services right if the utilities are down everything's down but it just caused so much confusion and chaos to call those common services so what's the current state a digital globe we have open source cloud phone foundry running for dev test we have over 800 services running in there they're not all unique developers are doing their own thing and there might be twenty copies of the same thing out there because they're in little developer spaces supporting their development and it's kind of interesting in fact I don't know if anybody else has any history on this but we think that when we're done we're gonna have between like 60 and 80 micro services for this first kind of big release that's supporting the launch of the satellite and so we're getting an order of magnitude difference between this is how many is gonna be running in production versus this has many it is is running just to support all the developers so be interesting to hear if anybody else has any numbers like that our de A's are just two CPU to 16 Giga RAM 3x overcommit on memory we found what they're running that mini apps we just can't scale with a 1x over commit which just it was ridiculous we're integrated with long stack or elk stack for our logging and we're currently using log drains bound to every app but we're looking at doing firehose and we just actually broke the ground on that on last Friday which makes that a lot easier and right next door our friends are talking about that in another meeting in production we have PCF running in production on OpenStack it's up and running we have a few services that been kind of snowflake Ashley deployed out there because we're rewriting our pipeline and we didn't want to go back and rewrite the entire pipeline to deploy to production to replace the pipeline so we just kind of have in a couple things at the end of deploy a few things to production and running pretty good so far some of the winds that we have is our development speed we once we had those patterns down we could talk about them know the pattern one two three and then four the pattern once we created a template a pin in github where you just basically like download the zip file unzip it create your new repo and you're on your way you change the name of the app and it takes care of its like fill in here four slash data slash all those great he's easy to onboard developers and new team members that way he's a development we've been lots and lots and lots of self-service portals that's been a really really really key if you're thinking about doing this self-service is key but one thing that we've learned is your self-service gets out of control and in order to self-service yourself to a new service you got to go to 30 different places so we're gonna have to have like a self-service portal consolidation effort here sometime the visibility is great in the Cloud Foundry what's running let me just do a CF curl command to the half's endpoint and I'll tell you exactly what's running the monitoring team does that once you can see everything then you can audit everything right so as soon as we find new services we can send off Nimsoft alerts who's doing what and why why is there new things popping up in production or wherever and when you get visibility and auditability you get control of your environment so this has been huge for us as you know what's going on you know what's deployed everywhere and alarms can go off and the audit ability with the Nimsoft alerts being sent out we're actually automating ELQ to the point where it'll auto create a new dashboard for that app as soon as it appears the testing groups are just ecstatic and that's mostly on because of Micra services we used to have a monolith where if you wanted to test the delivery of a product you had to put an order in and wait through the whole thing to go to the through the pachinko machine to see if it actually got delivered and now if micro services you know you can just test that just test delivery on its own some of the other big wins is resiliency holy cow we've had compute nodes fail and OpenStack to burn up CPUs and stuff none of the cloud foundry users even knew right well me being one of the admins I looked at it and go why why do I have new Bosch VMs what's going on start peeling back the layers find a big pute nodes had vanished none of the cloud foundry people even knew and this this happened several times so that we're very happy with the resiliency we've gotten some of the challenges we've had is a synchronization across foundations we're doing two regions if you will we have two data centers that are geographically diverse but synchronizing across these is becoming problematic how do you have one source of truth for you eh eh when you got to different foundations best practices around load balancing without 5s across foundations are kind of few and far between that we can find freely available so we're trying to learn here SSL one thing we'd really like to do is have a domain per developer so you can segregate it off in your own domain but just to the due to the nature of work we do even in development we have to do HTTPS Everywhere and as many of you may know you can only have one served and you have to have a zillion sans subject alternative names in the cert that's just it's kind of a non-starter to rebuild a massive cert every time you have a new developer start but so it'd be nice to just be able to point it at a list of here's a bunch of search go and go serve up all these open-source no support for a che at of the box so these the development environments we wanted to have some level of resiliency but we've had to have to kind of hand roll our own H a you know Bosch deployments and stuff like that and the VCS team have helped us out there it's great stuff developer and DevOps access to spaces that's been interesting because when you're using these log binders launch stats drain binders just some time it kind of takes a while we've noticed to get logs out and when you first deploy in an app and it dies in a test environment it's it was hard to get the logs out so sometimes the developers needed access to these spaces and we didn't want to give every the fine-grained access control just isn't there to just give them kind of you space auditor wasn't enough but space developers way too much power to create a snowflake and all that stuff so that's kind of the things that we're we're dealing with and there's some tension too between the micro service architecture and the licensing models from the vendors there's the tension there the more times I do the right thing if I need to break things down from one service to ten that actually cost me a lot of money to do that no matter which product I'm on change is hard be ever vigilant the old ways of doing things are gonna come back you're gonna have a developer that's trying to cram their own custom-built pearl into their app yes real story I need to package this entire pearl distro with my app what why get somebody an executive management to back you we've had many times where people just didn't want to do stuff and we've had to play the the c XX whatever said so so I'm really sorry but you have to get on the train and for me I'm on the architecture team we had to totally put it on the line and fight for this the organization's had really really really resisted change but putting it out there and putting your badge on the table and you know this this is the penalty if I'm wrong take that away from me that's well that's kind of what we had to do but it's been great and I think that's the last oh just one more that our future needs we actually have a need for a like an OEM style of cloud foundry deployment where I can put it at the customer site and just turn it on for our entire system I don't think anybody's doing that right now but if anybody else is thinking that I'd like to talk to you managing multiple foundations as if it were one that would be sure nice if anybody's had any insight into that or no when you open source tools that help with that we'd love to hear from you and persistent storage oh wait they just did that yeah diego has it awesome and if that I'll bring up Steve and he'll talk to you about that kind of the day in the life of a developer all right thanks Mike so I'm gonna talk to you a little bit about what our delivery pipeline is and how it looks for a developer so when they start off they have their local dev environment they'll check in to github pretty standard stuff and before that we check in the github that could use a dev environment so we created a dev organization so each one of these boxes here is an organization within cloud foundry so they have their own personal space their own sandbox they can use and then they can check in to github which will trigger a build so the first it's it's kind of the the standard build right initially where you do your compile you do your unit testing now moving into the micro service world one thing that we wanted to do is after you did your standard build and created your archive we wanted to make sure that archive was deployable into cloud foundry so we have this functional test organization and so after your after your application passed the unit tests there was an archive we create a space within the functional test environment for your micro service so it's a clean space we deploy that archive into a clean space and in the functional test environment the dependent micro services are mocked out so we want to make sure that that micro service works on its own it stills working within the what infrastructural bounds so it registers with Eureka it goes out to the config surface it uses event service if if it needs to so it'll still work within the bounds of the infrastructure it will hook up to a database but all the dependent services around it or mocked out so if it passes the functional tests that it has that environment is then torn down again so we we use the resources for the period of time we need to use the resources then it gets torn down and if it does fail the functional tests that environment that space is still left intact so that the developers can come or come back around and investigate the logs and do some analysis to see why it failed the functional test so once it passes the functional test then we'll move it on into what we're calling the integration test organization now the integration test organization is a full-up environment so all the micro services within the ecosystem are in the integration test environment and so then it runs some integration tests against your micro services and make sure that it works in a full-up environment the integration test environment is also a place where tests will go or development will go and they'll just do some kind of exploratory tests with their micro services and you know experiment on what type of tests do we want to have in our automated test harness and then if when when from the integration test environment we have a bit more of a controlled environment called the regression test environment now currently we have actually a manual gate we actually have a manual gate right here so somebody has to approve a micro service going from the integration test environment to the regression test environment you know we're still dealing with some cultural issues where there's a group that they want to have control over how things flow into an environment they want to make sure there's no change happening to this environment while they're running their tests and we feel that we kind of have to earn the right in order to make that an automated deploy so you know over time we hope we gain the trust of the community to say yes it's all working it all works well we have this rich automated test harness let us just flow into the regression test environment but currently this is a manual gate right here and then so we got open source here in this environment in our production environment we have pivotal Cloud Foundry that were working and as Mike said this this path right here is one that we are just starting to burn so each one of these are organizations so I'm I'm gonna dive into what each what were the spaces look like within each one of these organizations so we have our organization here and we have one space we're calling infrastructure so this is what we're kind of the utility micro services that Mike was talking about so in that space will have config service event service there's a few other services in there that are there are more utility services and then we have a space per business domain so we'll have inventory and then each one of these MCS and MPs their specific business domains within the the satellite arena now notice we have over here to the side so this would be called our pattern one and over here we have what were our pattern two so that these are services that each one of these micro services could leverage if they wanted so they're all leveraging Eureka and we have this over here running as a VM we're using ActiveMQ for our queuing mechanism we got elk over here and then we're using Postgres for our database so that kind of gives you overview of what our environment looks like digital Globes hiring UCS is also hiring so if you're interested in doing this professionally there's some opportunities for you and there's some contact information and any questions if you get stuck up to the mic or speak loudly we'll address those out I think so I think we got one here thanks guys that great talk today you mentioned earlier in the session the importance of standards and yet standards have certainly gone through a lot of evolution over the last five plus years in terms of standard boards and entities versus de facto standards through code how do you guys navigate between the old and new world we tried to uh we have a kind of like a hierarchy of standards generally you know we try to go with open standards industry standards and then DigitalGlobe standards so we really try if there is an actual industry open standard out there we've tried to use that barring that if there's some satellite industry standard or something in our industry that's fairly standard we've been using that and the last resort is let's build at least an internal standard so when it comes to interoperability and things like that you know we're trying to use oauth2 and trying to latch on the big standards like that one need be there's a lot of open GIS consortium standards that we leverage to we're also not that it's - I don't know what the right word is it's not too hard of a standard we're using how for all of our JSON responses so hypermedia application language where you can actually kind of describe your structure in the response any other questions roll your not wide you said to use that pivotal CF and production as opposed to just rolling it yourself you'd already done it for your dev test environment it's a comfort factor right right now the upper management you know and the the five satellites that are up there or that will be up there you know they represent billions of dollars of investment so when something goes wrong they just didn't want to go open-source at the beginning and they wanted to have a big brother to be able to call something was wrong so we have other environments that that pivotal will probably be of the vendor always a digital globe because of that reason any other questions thanks everybody for coming appreciate it [Applause] 