 um so as most of us know like these days we are talking about these big data analytics there is a lot of data is being generated and everybody is trying to make sense out of this data I think that is the the overall goal now when the data is being generated so this just shows like a lot of especially on the internet side if you see like starting from the web pages to clickers to emails all different kinds of ways data is being generated and people want to analyze those data if you just take only one of these let's say installed apps if you see the trend over the years like you will see that the just the number of apps have been steadily increasing okay so so now you can see that the number of apps have been increasing the data associated with it is also continuing to to increase now that is not just the internet data but we also see the similar kind of things in the traditional hpc side and inside we are talking about the pedestal systems eggs a scale systems people are trying to do not just process the data but also manage analyze and visualize the data and if you take some examples like climate modeling combustion fusion and astrophysics bioinformatics so typically a scientist or engineer tries to simulate these around the job then you dump the data in the file storage systems collect the data move the experimental data to some analysis site and also do visual analogy so these are the kind of traditional things we also see from the scientific side so then the question is what are available currently to process this kind of data so cuz there are a lot of different tools are available or software structure most common are Hadoop spark and I'll also talk a little bit about the memcache d these are the three kind of things I'll just so some detailed examples I'm sure I think most of you must be familiar with hydro how many of you have used at it that's some of them spark okay memcache d not money okay but actually all of us use memcache dena indirect man or I'll give some example okay so so this is the very standard Hadoop many of you must you familiar if not there are different components here the main thing is the Hadoop distributed file system where we actually store the data then we have the MapReduce that is the the computation mode and typically it gets done in a batch mode then there is an H base which is the query and then there is our PC which is inter process communication which takes place between these components and this model has been used over the period of time to scale but what we see basically especially when we are trying to process large amount of data there could be a lot of communication bottlenecks and that's what exactly I want to highlight later on how we can accelerate that part using these high-performance all these infiniband and the modern networks now this is the memcache d before going down into the details let me give an example you can correlate in your life let's say we all travel most of the people here and we buy tickets so how many times let's say you have chosen or search might be from New York to San Francisco you did some search two months back to come to this conference then you found out a very attractive price then you click it then you suddenly get an error message saying sorry that ticket is not available have you seen a message like this okay if so you have used men casting okay that is a very clear thing so what has happened is this the data centers actually use memcache d what it means that the solvers they take some memory from different nodes and collect a distributed caching layer or prepare a disputed caching layer and what it does it it is the general purpose and it typically casas database queries or results of the sum of the previous calls so might be what had happened before you somebody else from try to do a similar search from New York to San Francisco and and that price was cast there okay and then when you try to do the search that result came out but obviously there's a little bit misleading but there is a rule behind it why these guys are doing it that depends on how we actually purchase the tickets how many times you typically do a search and then buy a ticket yet one is 215 is to 110 is to one might be some people go 50 is 21 okay so so we always don't do like we do a lot of search but we don't actually do the transaction so to provide a very good high throughput transaction so so this is the kind of components which have been designed so that they can give you some information it is not like always you don't have to go to the end database because the database load will be very high if each and every query goes there so this is a big component which is used in all multi-tiered 8 centers and over the years as the data centers are trying to grow people are also trying to see how to build larger and larger mantastic so typically in a multi-tiered a center this is how you will see like this is from the internet we might be doing some searching tickets or buying something from amazon com so typically you seem em Cassidy here Rach base here and in the back end here we see MapReduce spar sdfs those kind of components are there now almost here everybody knows about InfiniBand i means this has been the technology has been there for almost like plus 16 years and and these days we are able to go to this kind of very good numbers i showed some of these numbers yesterday like latency of one microsecond bandwidth of almost 100 gigabits per second and the third parameter which will also see is the low cpu ttle addition and especially the RDMA which is the remote DMA technology allows us to actually reduce the the CPU overhead so that has actually led to all these different kinds of operation on rdio married right atomic operations and this has led to big challenges in designing what we are seeing today HPC clusters file systems cloud computing systems grid computing systems etc so in that context people have been asking a lot of questions and we also asked the similar kind of questions like few years back based on our experience with the novice project which I indicated yesterday like for 15 years we have been working there so we started asking questions saying if this big data is becoming very common what are the kind of bottlenecks in doing this kind of processing in the hydrogues tag or spark or memcache d and can these bottlenecks be alleviated by taking advantage of the HPC technologies and especially can the argument based operations be used here and at the same time as many of us know like HPC clusters use like parallel file systems like the store gpfs can we also take advantage of that and if we can do all these things how much performance benefits we can actually extract using the same hardware but if we redesign the software stacks can we get more and more performance out of this and and at the same time how also we can try to do the benchmarking so these are the some of the fundamental questions in my group actually we asked around like three four years back when we started the research along these directions and broadly we also asked the thing saying hpc is there and big data is there instead of their going in a separate direction can they actually converge so that we can actually use the similar kind of solutions on the on the both the sides so so just like yesterday I showed a stack similar kind of issues also happen here if we want to design a high-performance communication entire libraries we have the commodity computing systems multi-core many code networking technology storage technologies so these are the top levels we have the the big data so although heddo MapReduce all the components live here and traditionally what has happened all these are written with sockets so that is the fundamental thing which will try to address and socket said you know is a very general genric network programming it can run over Ethernet it can also run over infineon using the IP over I be more Christian made comment about that in in the last talk so you can always run a socket based application but gradually we will see that that has some overhead and the question is can we alleviate that kind of over a so so here these are taking the different kinds of communication and i/o library components but before we jumped into that we started asking the fundamental questions that can be from the sockets to some of the protocols like RDMA and if we can do that then what happens is the the network protocols become much more faster then it leads to the second questions that can we revisit the overall architecture or the operable designs because these were design these stacks were designed with slow network in mind slow protocols in mind so now if we can accelerate those then there are more opportunities to explore at the top level and then we can combine the benefits to see Vin better benefits so so that was the kind of questions we we ask this is the very high level it is trying to provide the pictorial view that means let's say this is a big data application or middleware was running on chocolates so the question was can we bring a new design which can directly run over verse so that is the like the the lowest layer on the infiniband stack or the rocky or you can think of I war so all those layers that is the lowest RDMA layer and can be directly design at that got layer so that was the first problem we asked so we have done a lot of research i will show you all the results but just like the rabbits project as the results started coming people wanted to use it so we have actually started putting this together and these are public releases you should be able to download so if you just go to this site is called high performance big data and we have a lot of distributions and these are all available for infinite rocky we started with a diamond / apache hadoop one point x but then last year and all hadoop to point x became very common we move there then recently we have also released the argument for apache spark and especially in the hydroxide we also have integrated package we also have plug-in packages then especially the plug-in packages for haughton works and an Apache so that if you already have an installation you don't have to totally reinstall everything so you can just take our plug-in and then put it there and then it will replace the sockets based protocols to to the argument based protocols we also have the idea for mem Cassidy and also just like the impire project we also have been also pushing a lot of benchmarks micro benchmarks so that people can actually evaluate not only our stacks everybody's else tax and then see where the benefits are and this has been like 18 months we started making this available to the community and so far you can see like around 145 organizations are using it we have very close to 15,000 downloads come from our site itself it is people are using so then we'll go into a little bit of detail so this is the Apache hydro to point X so here what we have done is it's a I will go into the much more technical details in the next few slides so we call it a heterogeneous storage so that we have different kinds of mode we can run it on SSDs we can run it with hard disk we can run it over lustre we can also run it in memory or in any combination of this so that is like the new kind of sdfs designs then of course we have the similar kind of a dia- MapReduce together with also luster or local disk we also have a DM enhance our PC and we also provide an interface with the typical job scheduler let's say you have the hpc systems with PBS running or slurm is running we have interface so that directly it runs using the existing HPC framework so let me just touch few of these things we may not have time to go over all these things so i'll start with the HDFS and MapReduce then going to spark some of the memcache d and now we have some of the advanced stuff like even combining sdfs with memcache d those kind of serves designs we are working on so this is a very high level picture we started this work this was the first we tried to ask the question how we can enhance HDFS with RDMA so this paper was published in supercomputing 2012 almost 33 years back so the whole idea was that typically if you take a look at the HDFS these are all written with java these are not c code ok and then what what Ron said is the Java native interface Jane I and mostly in the HDFS many of you might be knowing the right operation is the most time consuming because it makes the replication ok so that is the part we wanted to address so instead of the J'naii running over with like those sockets we try to put it over the over the verse and everything else we kept unchanged ok and and in all these designs we also have not changed any of the api's a is the same from the comp level if you have somebody's using as their face or mapreduce man cassidy api remains the same so we are not forcing anybody except the very end I'll talk about memcache d we are taking some very futuristic look but otherwise if you have any other applications our libraries should just transparent later on you don't need any any changes there so then if this is the Triple H heterogeneous storage also i mentioned about this so so here you can say all different stories is we can combine the hard days SSDs even the RAM d so that you can run a ZFS in in memory also luster then there are different policies to utilize this heterogeneous storage devices you can specify saying okay I want fifty percent of data to be on the SSDs thirty percent in RAM days remaining twenty percent in the hard day so there are all different configuration parameters you can do some kind of hybrid application and similar kind of things we have done also for MapReduce the MapReduce inside we not only changed it to the the basic RDMA based suffle because in the MapReduce the software time that is the most time consuming time so instead of doing it in a two-sided operation over sockets we have changed it to like a one-sided audio-based things we have also done some prefetching catching and at the same time we have also done some in-memory merge advanced overlapping it's a lot of techniques have gone in and those techniques are all available in the in the published papers I not have time to go over it but let's take a look at some of the numbers so if you download our stack these are the kind of numbers you will see there are some standard benchmarks those who are familiar in the head community might be knowing these like random writer Terrigen so this number is taken on the tax template system it is an infinite FDR and here it says like a random writer so the orange line is the IP over IV that means the existing stack is running with sockets over infiniband and when it is running over infiniband its IP over ID and and this is the red line that is the red box those are our argument based designs so here you can see that same hardware just by changing the software after data size is using we can the overall execution time of the random writer we can reduce by a factor of three okay where you can get a 3x reduction in the time here similar kind of things you see also Terrigen by by a factor of four here this is the sort and Terra sort these are the the many common applications get used so here you will see like you can cut it by half again the same hardware we are not changing anything it's just the software we are able to speed it up and then similarly this side also karasov with single HDD we are able to reduce by again forty-four percent okay and so then we went into some of the applications so there are some applications as we sent from bioinformatics some of you might be familiar is a cloudburst then there is a mr ms polygraph so here again we see like this is a pure HDFS this is the lustre wave solution and here we can cut it down by eighty percent and the cloudburst we can give you like a around nineteen percent improvement for the end applications so then the some of you might be knowing like the the spark also has is the equivalent kind of a file system which is called tachyon how many of you have heard of that okay so so tachyon has this in memory capability also so then we try to compare with our HDFS in memory how does it compare with with tachyon so that's what you will see here like this is the HDFS this is the Tachyon our Triple H design is able to like hear random writer 200 gigabytes of nodes this is our running over on the acac san diego super computing systems the gordon system so here you can see that the HDFS our privileged mode is able to reduce the time by forty seven percent here also it's on the sort 200 gigabytes we can give you like another nineteen percent benefit ok so now you can think of even like the spark applications you can run with our HDFS with the Triple H mode you don't need to go through the Tachyon and then you will get the this kind of benefits what what I am trying to show so then some people also have been thinking that let's say if you have only MapReduce application okay should you go through sdfs or can I directly use luster so let's say there you have some scientific applications you have written with MapReduce you don't have to go through the entire Hadoop framework you can totally skip it okay because a lot of hpc systems have the luster so the question is can you directly interact with the lobster so we enable that mode and so here its source again there are different designs we presented this in the last year sigh petty pace there are some luster read based upon our dbase Tuffle you can also have hybrid and internally we have some dynamic adaptation here and let me show some to set of numbers so this is the taken on tack stampede and tack stampede if some of you know the architecture each node also has a hard disk so the question is when you are running this MapReduce job and when you are generating the intermediate data do you put it on the local hard disk or you put it in the luster so there are two kinds of solutions can come so here what I'm trying to show is the local disk is used as the intermediate data directory so here we can see that almost half terabytes of salt in 64 nodes we can cut it by almost half the time it's like forty four percent reduction and similarly here 640 gigabytes of sort in 128 notes similar kind of things we can see forty-eight percent improvement and then on the other hand it though directly let's say you have a clustered where you don't have the local disk so everything can go to the luster your intermediate data as well as the end data of course then there is a higher load on the luster and so benefits are little bit smaller here so you here you can see like it is reduced by thirty percent you are reduced by twenty-five percent kind of things so then let's moving on to spark this is the original design we presented at the hot interconnect 2014 so the broad idea here again you can this is the light the spark architecture there are different kinds of software there is a net lease awful here Java and I was suppose so we actually took it instead of Java sockets we change the communication paths so that it can actually go over the the argument based now these are some of the numbers in the spark literature there are different kinds of benchmark like one is called a short by group by so here you can see eighty percent benefit compared to the IP over IP design ok this is taken on the on the SDS see comet and there are different all these details are there this is a 1536 core experiments it is a quite large job we are trying to run here eighty percent benefit here is also around 57% benefit then these numbers are taken from hdac comet these are the high bench sort and Terra sort we are using again the standard benchmarks just to show the benefits compared to the IP over IV you can see you get you can get with our audio-based spark design you can get around thirty eight percent benefit here fifteen percent benefit so then there is another common benchmark which is called page rank and that is very common like just like on the web pages you see the google and all they do all this page ranking that means you get although who is referring to which pages and there's a piece ranch is a very common algorithm there and when we ran it on like a 768 course on HTC comment so you can make it faster by thirty-seven percent time here and forty-three percent time here once again the key thing is hardware remains the same we are not touching anything to just the pure softwares we have redesigned which is trying to help you with this kind of acceleration so then going to the main cast d when costly again as i showed in the architecture in the beginning the people who are just taking memory from a set of nodes and especially now since SSDs have come the next level of designs are trying to explore saying can we also combine SSDs so that is kind of a hybrid design so i will show you the the first the basic design so this is like a this is taken over again on the on the tag stampede this is the FDR interconnect so compared to the IP over ID here you can see you can reduce the latency by almost 20 x ok but factor of 20 we can reduce and here we try to scale to almost 4k clients it is a huge memcache d we try to simulate and here we saw the transactions per second we can give you the double okay and that's a great benefit for for any of the data centers deployers because you are using the same hardware and you are trying to satisfy more number of users and give the reduced response time that is the idea of thing with the same hardware you should be able to achieve or extract mode and then we have done some evaluations with different kinds of online data processing workload so here it's like a MySQL slap load testing tools so here we see that the again the latency is reduced by sixty-six percent and so that is like a query latency we are trying to reduce my sixty-six percent the throughput we are able to improve by sixty-nine percent and then this is the hybrid design so what we did is not only we take the memory we also took the HDS okay so that you can have your entire capacity we can significantly increase but then there are different matrix you can see even though you have a large memcache be your data whether it is fitting in the memory or not okay so there could be some different trade-offs so we wanted to make sure that first when we do the design so this is like the IQ over ID and these are the 2 r.d language design this is a pure memory only we are aggregating the memory whereas the hybrid is also we are combining the SSDs and the first thing we proved that irrespective of like memory or hybrid we are not hurting anything when we go into the hybrid we are not putting any additional penalty but at the same time compared to the IP over I be here you can see that we can boost the throughput again by x factor of 2 so then we have performed a very detailed analysis with all different configurations like IP / ib memory rd my memory Adam a hybrid or SATA disk then now we are seeing this nvme disks are coming up and then this is the data which is fits in memory and this side is the data which does not fit in the memory because we have done a very exhaustive analysis and here we see that when data fits in the memory our design is gives around like a factor of five improvement over IP over I be and then when data does not fit in the memory we can actually go to the hybrid SSDs and there we see you still the factor of two or 2.5 x improvement we can give compared to the IPO right so then very recently the very latest work this paper just got accepted to ease back to be presented at I to DPS this upcoming I pdps so what we're trying to do this these things we learned from our NPI side you remember yesterday I talked a lot about non-blocking operations non-blocking collectives so we started looking at saying all these calls currently in the memcache d they are all blocking calls so you initiate a query and then wait so you are not effectively putting all the CPU and the network to to stress so we try to explore that can we do some non blocking API extensions so this is like a futuristic look we are trying to provide saying okay if we can go into the non-blocking api you what happens then that's what we see like so these are the kind of the earlier numbers i sold like the default and some optimized the blocking and now if you do all these non blocking and we have a complete solution ready so then you will see you can even use the benefits by even significantly higher like you can get a 16 X latency improvement and a 2.5 x improvement so through this we are trying to show to the community saying we need to totally rethink how these designs have been done so moving forward so then even we are moving taking one step further like many of you know even in the HPC a lot of discussion is going on birth buffers how you can use the word prefers to accelerate the i/o operations the same thing we did here like from the if you have HPCL austere environment so then we said how we can have a memcache debased birth buffer system and they link it with the HDFS okay so that we can try to even explore it more and this we presented last year similar kind of things you can see like the this is the yellow line sorry the yellow bar where you can see that you can even effectively get another forty percent for eight percent benefit compared to what people are trying to do now so these are some of the numbers and and through these things what I want to also stress that there is a big need for micro benchmarks to to to guide these kind of designs so so what is happening what we learned ourselves and also the community is also experiencing the same problem that when you do some design you see some benefit let's at the sort i showed you like forty four percent benefit but wave that benefit is coming from okay from which layer from which operations okay nobody knows so that's what's the question like what is happening at the lower layer where the benefits are coming which design is leading to benefits are bottlenecks which components in the design to needs to be changed so in a pictorial manner here you can say that all the book currently people focus the benchmarks on the top but then nobody knows what is happening here when you try start working on these layers you need to have some benchmarks and the example I give just like let's say you go to like a four-door General Motors people are designing the cars if somebody's is designing tire they have their own benchmarks the tire team has their own benchmarks to design the best higher then the engine team has their own benchmarks and then when you put it together then only you run it on the road okay and see how much benefits we are seeing so that kind of things are are missing here and there is also no correlation between these and this experience also we found out from our evaporates project which I mentioned yesterday like for 15 years whenever when we start i remember in 2001 when we started pushing our new design nobody knew how to compare different mph tags okay like different operations where the benefits are coming from so in that context we have introduced this way to MPI micro benchmarks OMB many of you must feel familiar and over last 16 years this has become like an industry standard these days if you see all the biddings all the bits there is a line there saying okay refer the numbers using OMB so we wanted to do similar kind of things here to to the to the hybrid II community so that we can actually contribute back and let the community start looking at how to redesign these stubs so we have started working on these so there is a set up sdfs benchmark memcache p benchmarks these are actually available you can download and if you just want to evaluate multiple sdfs solution irrespective of your short or any of the higher level you can use these are real low-level benchmarks and you can see how one a CFS compared to the other HDFS similar things you can do memcache d we also have MapReduce pure MapReduce benchmark that means it does not depend on HDFS similarly pure RPC benchmarks so component by component you can evaluate and then these will be gradually released over a period of time so to conclude this what I'm trying to show here like just the wii for the last three years especially in our group we have started asking the question saying how especially how the HPC is moving ahead how we can take advantage of those technologies and then trying to accelerate the big data stack we have of course already made several of these tags available and our next releases will be the cloud era distribution a lot of people are asking for that we are still tuning and optimizing on that our hydro for the cloud era then X place we have also started looking at the Impala and also I indicated like the hybrid e benchmarks MapReduce and RPC will be there and then this new HDFS memcache debauch buffer non-blocking those kind of things also will try to a push so let me just quickly summarize here what we see that some of the initial results what I presented the results are promising that means we can really take advantage although it's we see technologies the RDMA and the especially the file systems etc but of course this is just the beginning there are a lot of open issues the open issues are in the upper level architecture we have not explored that completely and this is the thing what I motivated like if you take let's an old car and you just change your tires you expect it to run a little bit better but not completely all right is the engine is still old ok so that's what is happening even though we touched some of the components and we can isolate it but we need to totally reevaluate it because the overall architecture was written with slower network in mind slow network and slower protocols and now if you can enhance all those things will be able to actually do it much more better then these are all the again though just like yesterday these are standard sponsors and these are all my heroes I always publicly acknowledge their contributions and the final thing I want to stress here to bring this community SVC and big data together since last year we have started a new workshop called high-performance Big Data computing we did it with I cdcs conference last year and this year it will be held with IP dps so we already have a there will be a keynote talk by dr. chaitanya baru well he's a NSF program director we have a very interesting panel already has lined up saying merge or split mutual influence between big data and hvc techniques like should SPC things go to big data or big data should come here okay or how the future will move around so if some of you are attending or planning to attend I pdpsi will strongly encourage you to attend this their full day workshop we also have regular research papers so it will be a full day event so with this let me stop here if there are really quick questions I will be happy to answer these otherwise we can take some of these discussions offline thank you 