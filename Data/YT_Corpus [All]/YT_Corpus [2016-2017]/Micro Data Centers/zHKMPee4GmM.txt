 so yeah font of knowledge more font of opinions and I'm going to share a few of them with you today so we're here to learn about deploying and scaling microservices as Scott alluded to I work for a company called thought works you can find more about us we've got a lovely booth outside we've been sponsoring yeah for many years we're sort of happy to do so I'm also the author of a book called building microservices if you want to buy a copy wait till the end of the session and I'll give you a discount code it's my way of locking you in but we're going to be here to talk about deploying and scaling microservices microsomes is a nice but we end up with lots of little things and managing lots of little things turns out to be quite painful I think Martin observed might have had observed that with microservice architectures we've effectively pushed a lot of our complexity into the operation space and so it becomes quite important that we actually have a decent platforms for managing this stuff the space of platforms around microservices has been changing a lot and so hopefully today I'm going to try and give you some sort of core principles that you can use to frame thinking about deployment of micro services I'm as you can talk a little bit about artifacts as well just briefly but then we're going to use that knowledge to then take a look at sort of maybe three of the bigger platforms being used right now for managing microservice deployments and other sorts of deployments I can't give you all the answers that are going to be true for the next 20 years I'm going to try and just give you some things to think about and that the current state of the art with those principles so hopefully when you get back online tomorrow and find out that five new micro service platforms have launched all of course written in go all of course using docker because you have to have a transit of buzzwords for 2015 you'll hopefully have some good ideas about how to select the right tool for you we're going to start talking about a few simple core principles some of this is stuff around good principles of just delivery that come out of things like continuous integration and the sum of this is very specific to challenges that microservices bring because microservices are lovely architectures look at them hexagons and hexagons nice this is very good this is excellent people pick microservices for a number of reasons more often what they're trying to achieve though is the ability of going fast they want to give their team's autonomy allow those people to make changes and deploy them quickly one of the things you need to bear in mind if you really want to get those benefits of microservices architectures it's sort of holding this tenant very close which is the idea that you need to embrace independent deployability of your services that means you need to be deploying one service by itself into production not deploying 25 different services at once you want to pay take one thing make a change and deploy it this is stuff you need to get right if you're in an environment right now where you are I can only deploy 20 services all at the same time all together you've already got a problem around coupling you should fix it the problem is that when people do have architectures and systems that are so tightly coupled that they need to deploy lots of moving parts at the same time they then end up having to put a lot of complexity into the deployment tooling we start having conversations about things like this so Sam what kind of deployment orchestration tool should i use so i can deploy everything in exactly the right order i think if you need to deploy everything in exactly the right order the right order you've already lost you need to fix that problem first the idea that I only deploy this thing after this thing is up it's also kind of a bit odd in a system where what is up and what is down as a transient idea anyway to get rid of these ideas keep it really simple our processor is the process of deployment when we want they're going to allow to deploy one artifact by itself and then use a platform to scale that out accordingly we should also they look at some core principles about how we get something from our laptop into our production environment probably many of you are familiar with the idea of things like build pipelines ideas that grew out of continuous integration the idea is is there a number of steps that we need to pass our software through in order to be happy that it can go into production so we check our code in some source control tool we have a build process some people might use Jenkins of course what you want to use is go CD that will create some artifact will talk about artifacts in a minute they are actually important in the context of making our deployment process is easy this artifact is then moved we're moving the software moving a version of code through different environments different stages and if it's good we give it a nice green badge yes you're good enough you part your test pass right now we can go to uat did that work yes okay did your performance test pass fantastic now we can put that software into production this shouldn't be new I mean this is what build pipelines are for some of you may have all of these stages completely automated that's perfectly okay and in many domains that works that's what's called continuous deployment in other words it's fully automated it just goes into production in general it's good to keep these pipelines as short as possible now there's some interesting things though we're moving this thing through the system and we're validating it at each point what happens if I rebuild my deployable artifact at every single stage am I then sure that the thing that I built and tested here is the same thing that I built and tested here in other words is the thing that I've validated as part of my tests the same thing I deploy you know imagine the differences that can creep in when you rebuild an application at multiple different stages of watch and it's also crazy to do that if it's the same version of code one on earth and we're building it like 25 different times that just wastes cycles but it also introduces the risk that the thing we actually end up deploying is not the thing actually tested this is another thing you need to hold quite close is this idea of one artifact for all environments the thing you move through these pipelines is your artifact build it once move it through the interesting problem then of course is that configuration does need to change from environment to environment when they talk about that in a minute the other thing you want to think about with all the different places in which you deploy your application is trying to use the same deployment process everywhere the reason this is useful is because I wouldn't want to use one deployment process in my CI environment and then use a completely different deployment process at my production because again bugs can creep in late in the pipeline I lose the opportunity to catch problems earlier using the same process everywhere is quite important and i'm going to give you sort of what my ideal deployment process looks like in just a moment we need though to think about this concept of environment what an environment really is for us an environment is a thing into which we put a service or a piece of software when it comes to services what we're really thinking about is our service exists in different topologies in different environments for example in the uat environment my service might be deployed on to a couple of machines low balanced on top of a database maybe in a CI environment would be even simpler maybe just a single node on one machine in a performance environment that self-same service might be deployed across multiple machines across two data centers in an attempt to sort of mimic what our production in crime it looks like so this is sort of useful okay with its idea that the artifact should be built once and once only but it gets deployed in different topologies from one environment to another okay when it come back to that idea again now we have to deal with a very sad fact that I mostly think about the world from the command line this is my unit of all things on trying to make out make sense of the world I do it for our a command line this is my command line for what every deployment process in the micro service system should look like so I'll break this down for you it is very simple but I will break it down we have the word deploy that's good we're off to a good start we're going to deploy something you can call it go or you know monkey trumpets but deploys are good words because it has meaning next we have the name of the thing you want to deploy I said thing not things the name of the thing I want to deploy I've then got some version of the thing I want to deploy what I normally like doing with these scripts is I have a version number for an exact known version it's a saying that maybe a build labels coming out of your CI tool I have the able to put the word local which means as a developer I pick up the locally built artifact or latest switch with my latest green artifact and then I've got the environment the topology in which I want that service to be deployed this is a universal command line this is a command line that will satisfy what a developer needs when anyone to deploy one service by itself and it will also support the needs of what production needs now when you're doing dev and test quite often you want to deploy two or three or four things together in like ass back to bring up a test environment more touch on some things you can do about that later on but in general this is what I always strive for what I'm thinking about automation this is what I want this is this is something that everybody can use to achieve the same goal but that can vary from one environment to another so you know when I'm in my test environment I've got one set of config and really that's what this thing at the end is when we have that flag that value at the end for environment name is referring to a topology is referring to a set of configuration this is where we could be pulling different configuration sets out of source control this is where we could be determining different look up criteria to pull information from a console node for example for different environment specific configuration sets it's also the place where we're going to want to parameterize our deployment platforms and we'll look at some different ways in which you can define those in a moment so you 80 and production this is should all be fairly straightforward stuff so far this is good so let's summarize that we have this idea the same artifact we have different topology that's important we need to embrace this idea of independent deployability really really important for micro services to get the benefits you want one artifact for all environments and the same deployment process everywhere so when we go looking at both the artifacts and deployment processes we're going to be looking at them to make sure they satisfy these criteria okay so that was a whistle-stop tour this is really you getting a sense of how my brain tix around these things because I sort of make I have opinions these are always good to know why I have these opinions you can talk to me about these late if you don't agree that's completely okay let's thought now that artifacts artifacts are often quite boring because we call them artifacts and they're not like things that Indiana Jones would find the desert instead they're just bundles of stuff and that's pretty much how we treat them but I sometimes feel that we do ourselves a disservice by not giving enough for to what an artifact is especially in the world of services because we end up making choices that make an I was a lot harder when it comes deploying things even in the very simplest way again what would we may be like from an artifact with simple things that would like from an artifact we start off with something like maybe it'll be good idea if it was very easy to create that's nice okay that's good we want something that's maybe easy to deploy that's also a good idea I'm hoping I'm not being too controversial at this point about what I build artifacts should look like now when it comes to microservices what we often doing is giving ourselves the opportunity to mix different technology stacks we don't always take the opportunity but we would like to maintain that opportunity and so when we're thinking about what a good artifact would be for our micro services we also would like something that does the same thing that really abstract out the underlying differences many of you will work with operations teams that support multiple different teams in your organization it's quite annoying if they have to suddenly learn about how to use 15 different technology stacks just because the developer made an arbitrary choice so something that hides that from us is useful and something as well an artifact that is something that the developers like and the operations people like to again developers make decisions about architecture about micro services and they push all of this complexity onto the port operations people and never give a second thought to them well actually I think we can do better by them as well as doing better by ourselves let's start with the most developer-friendly artifact that I see the one that most developers create because it's very very easy for them and that is the tarball if you've ever seen an oil slick you know what tar balls are like and they're not necessarily always fun because they're just giant conglomerations of mess and some poor soul has to sort of pull it apart later on and that's the issue right they are very easy to create anyone can runner to create a tarball or a zip file it's not hard everyone does are they easy to deploy hello to get a tarball deploy properly onto a Linux system I've got to unzip it I've them going to move files into the right location I've got to change permissions I've got to set an upstart script this is often why people have lots of puppet and chef recipes because they're deploying tar balls and so all the deployment work then goes into these scripts that have to be maintained separately that themselves become a nightmare it's stunning how many people complain about puppet and chef and then don't look at the artifacts they're creating in the first place because there are opportunities to make that life a lot there lies a lot simpler does it abstract out a tech stack well maybe if you've done a good job depends is it good for dev and good for ups I would say not operations people they just love it when you send them a giant tarball of stuff without much information about how to deploy it no I don't think this passes muster on this one we could do better now let's talk about technology stack specific artifacts we've got things like nuget packages jar files pips and gems again a developer centric view of the world some of these systems sort of 8 operating system level artifacts but I don't think these are very good for us services either beyond the fact that most of these are often built with libraries not running services in mind they're good for the developer because if you're a developer inside that stack you know that ecosystem you understand what a gem is you can we build gems anyways intermediate artifacts for your services for example and so for developers they like them okay that's great are they easy to deploy all they can be this varies greatly from text actors tech stack I'm managing gems and things in production can be a bit fraught and there's a whole class of security issues around how you manage gem deployments for example now of course if you are using Go Go being the greatest thing in the history of the entire universe then what you are actually doing is creating little static binaries in which case diploma is actually quite easy because go is amazing then of course we got our we abstracting out a technology stack absolutely not because by definition we aren't and I think if you're an organization where your operations team like only know how to you know God of deep expertise in managing wire files and weblogic well then firstly I'd petty pity your operations team but maybe this is acceptable but again with microservices we're trying to buy some options and so we want that maybe not I would argue by the same token it's not good for dev and it's not good for ops and just in case you didn't get that just everything goes awesome that's just a fact major major facts okay so this is these aren't bad but we can do better they set a much better option out there for creating services and it's something that I see very very few people do and that is operating system specific packages like you know allowing us to do things like this so this is me command you know deploying a service our operating systems have tools for managing package deployments that also do things like handle dependencies we have these wonderful tool chains that allow us to inspect what packages are installed that can do rollbacks that can check hashes we can store these things in secure artifact repositories and do signing and all sorts of wonderful wonderful things and operations people know how to use these tall chains and developers ignore them this is unfortunate because look that's not that different there's only a couple of little things we need to make you can pass version numbers in this is very very close to what we need I've seen so many teams move to operating system level packages and then promptly delete virtually all of their puppet scripts for example because suddenly all this work they were doing inside puppet or chef or ansible and now just being done by something as simple as building a proper package in the first place these things though fail on the first hurdle all too often in that they tend not to be that easy to create there's famously a package manager to support building packages for multiple Linux systems that's called fpm which stands for effing package manager they are not easy things to handle them to create but nonetheless if you can get good at it you can actually script away lower complexity are they easy to deploy yes you just saw it happen um the challenge of course is that to bear for developer then to deploy the same artifact implies they're running a production like operating system somewhere where they can do a development work which again is very achievable now we've got things like vagrant and docker which we'll talk about in a minute because of course this is the most buzzword friendly talk avião so I have to mention docker a lot does it abstracts out a technology stack oh yes as an administrator I don't really care what's running as long as when I run that RPM it install scripts in the right location and my service comes to life good for dev good for ops this is where things get a bit tricky developers don't seem to like them operations people do there are some other issues over these packages and how we tend to deploy them they just take the example of rpms you know I build my create my build I've got a couple of services now service versions in my artifact repository I go and deploy them on the machine so I run my you know rpm install my my dev updates or whatever it might be and then some issues occur because well with packages you can declare dependencies you can say things like my service needs version 1.5 of this thing and my others this other service needs version 2.1 of this thing and those two dependencies you both want to put on the same machine don't play nicely together you get clashes now if you've built your packages correctly your packages will tell you I've got a holy mess on my laptop right now with a whole bunch of gem conflicts I can't work out what on earth I'm going to do the issue is this occurs because what we like to do especially we start our journey into microservices is pack all of these things on the same box and so we get these nasty collisions our deployment processes become more complicated because we trying to dodge around each other you things happen like one service suddenly goes rogue and eats up all the resources all the other services and now nothing else is working and we want to make our services easy to deploy independently from each other and it's temptation we have to still pack lots and lots of services on the same machine does not help us every organization i know that uses micro services at scale and by at scale i don't necessarily mean they're microservices take loads of load I just mean they've got lots of them lots of organizations I know that do very small volumes it would be less than 1% of what a Netflix would do they still have hundreds of different types of services because they have complicated domains most those organizations I know end up with this model well they moved having their services operating in separate hosts separate isolated operating systems because this is really good this is a very simple worldview you avoid clashing your void collision between things the problem is that this actually can be quite expensive and painful to manage if you think about an organization like Netflix they run this model they deploy one service predominantly on an entire vm they take a whole am i but their services are like leading 32 gigs of ram right at runtime and so you can have a very very big vm the way normal virtualization works though the more small virtual machines you pack onto a box the more resources proportionally are going to the hypervisor to manage that separation and so you're spending more and more money not to run services but to run the hypervisor you're using to run your services so economically it can be quite expensive and a tooling around virtual machine isn't great you don't get have an ability to really dynamically resize things in a developer friendly way and this of course is why people like docker and people why people like docker in the context of a micro service environment and that is because the cost of isolated hosts creating separate operating systems these separate execution environments is drastically reduced docker is just a library is a system that uses linux containers within its containers rather than having a hypervisor that manages separation of your separate operating systems are separate virtual machines instead the kernel just keeps these things separate much more lightweight much faster much more efficient it's not just that the effort is much reduced because there's some very good tools a great command lines around handing these containers but also you're using more resources are actually going to go to the services themselves what's often overlooked as to what my daughter has been so important in this space a lot of people fixated on its ability to pull down other people's images from docker hub or you know this equivalent of you know you're dumping around a dumpster and you find a thumb drive and you stick it into your laptop and run the programs on it that's all going to be fine that's what you know getting stuff from Dhaka hub is like it's in the same way that people miss the trick with rails and thought the scaffolding around Zamost important thing the most important thing that Dhaka did was made containers easy to use the good command line and came up with an artifact which didn't really exist this concept of an artifact for control containers this docker image is a perfect artifact for us because well let's see these things to create a docker image you to Dhaka fast a plain text file that just determine is what you need to do to bring that into being I think some people can argue how easy that stuff is to create I find it quite easy the more complicated your programs the heart of the docker file is going to be to create is it easy to deploy yes it's incredibly easy to deploy it has the added benefit that it's very easy to bring up production like operating systems on your laptop I can deploy using the same image in my laptop as I can in a production environment do I abstract out the tech stack absolutely it's just a container as part as far as doc is concerned it's just a running thing it has some ports that go in and out it knows it's running or not I can see the logs and apart from that you do not care what happens inside the box good for dev good for apps developers loved ocular it's amazing or look I'm a cloud administrator I'm web-scale on my laptop um the where it gets a bit tricky is this good for ops I did like a highly unscientific survey about a half a year ago now some are developers worldwide to go and just pull their clients and say okay who's using docker out there who even knows the docker exists you know who's in production about half of the clients about 500 of the large corporates half of them were doing invest with investigating dr. in some way shape or form exactly zero we're using in production a big fat zero the reason is because historically docker had a very good experience for a single host you had a lot of tulle trains around managing one host it has absolutely nothing historically to help you running across multiple different hosts so people that jumped onto docker early had to do that work themselves for example in Dhaka this is thing called links which are now great deprecated finally where you can say how one container talks another container those things only work if both those containers are on the same machine who he runs an entire production system on one machine anyone in the room no because it doesn't happen this is the problem this is what's been holding docker back and that's why so many people are now interested in these platforms so this docket image i think is as close as we've found so far to the ideal deployable artifact for micro services and so the platforms they are quite important the problem is that this is a space about as journey as the javascript library space so boogers again let's let's take a look at some criteria try and use that to dry some selection so what do we want my deployment platform well start off with we need to make sure again that we can separate the artifact from topology so looking at how easy it is to define the topology in which we're going to deploy our artifact is important to allow us to have a different CIA vs test versus production setup we also really want to make it easy to handle lots of services I mean if it doesn't do that what's the point again when i talk about scale in this context doesn't necessarily mean load or volume it often just means a number of things what we don't want to do is just have a linear increase in the amount of operations work required every single time you have another service to break that pattern otherwise that will become a constraint on how aggressive you can be in using microservices if assuming they're right for you we also need something to support docker images I mean that's it we've decided now all of you now are going back to your organization's rule dr. izing everything so that's good so dhoka dhoka dhoka dhoka dhoka dhoka running go go running docker in a micro-sized environment so those are other things there's this nice idea so you'll see if you've used puppet chef or ansible or sort of declarative provisioning where you effectively make a statement about what a node should look like a single machine should look like and those tools make it happen and if the system is already in that state and you reapply that script that engine is a declarative started provisioning we think about your deployment platforms you similar ideas this idea of desired state rather than listing a load of command I go to these platforms and I say look I want 15 of these things I just want them distribute across a data center but you work out which machines they should go on and each one of these things should have a couple of CPUs and some ram that's what I think they're going to need and the whole idea is that these platforms these scheduling systems that we're going to look at in a moment can take this information and work out where to place your service that's what we want this is good this is exactly what you need to handle things at scale however we can go further the sort of idea of autonomic computing is idea that systems and sort of maintain their own state so the platform is ensuring that the desired state has maintained at all times this is important once you've got thousands and thousands of nodes if I deploy my service and say I want 30 of these and then five seconds later two of them crash you don't want to have to know that then go back and say now on I really won 30 of them and then something else happens I really want 30 of them that's going to get old very very quick we want our platforms that maintain that desired state for us that suspect he always there that's what you said you want and it's my job as a platform to make sure that's exactly what you get so to add that trial it's a criteria we definitely want things that support desired state and ideally risa bonds things that are also self-healing how this autonomic property so which platforms have I picked for the great melbourne bake-off well of course i haven't been to the word doc you enough so we're going to have a docker swarm dhaka swarm sort of grown out of lips warm is we bad Brad lips form this is there sort of out of the box cluster support docker we've got me sauce which is tried and tested and has but has been used increasingly by people in this space and of course both one of the oldest and the newest tools out there in Cuba Nettie's we can look at the architecture of these systems and it's all look a bit the same after a while unfortunately because it all follows very similar architectures but have very interesting nuances let's start by looking at dock warm we have lots of hosts ease off this could be physical machines or vm set aside wearing a docker engine we have a sensual manager a swarm manager and the swarm manager talks to swarm nose running on those machines those nose in turn talk to the docker engines on those machines as well one of the nice things about docker swarm is that you can use exactly the same command line that you would use to run and configure dock are locally on a machine so to bring up into the bigger images check your logs you know check what running processes are you can run those same commands on the swarm manager and it just transparently works across click on those at present it supports about seventy percent of those interactions it's adding more and so then the store manager based on your commands will go and distribute load across all of these things in a lovely way now obviously you often need a little bit of control in terms of how these no's get distributed so swarm supports a couple of scheduling a few scheduling strategies one of them is bin packing so with a bin packing shed schedule what it does is it looks at the resource profile of the containers and it tries it's best to cram it in and fill up every last bit of you space to really reduce the number of machines required to handle your workload a bin packing strategy would work very well in a dynamic computing environment where you want to turn machines off you're not using you that effect is about optimizing for utilization of your computing resources now that may be fine when you're trying to maximize going to reduce your costs but if what you're actually trying to do is scale out services across multiple separate physical machines to ensure resiliency that's not going to be a very good option for you then you could do something like using a spread showing strategy where when I deploy my service it will scatter them across all the machines there's also a random edging proper strategy for just shits and giggles I think I don't know why you'd use it these are fairly broad brush and you often want to do a bit more nuanced stuff there's a whole bunch of filtering stuff that some also provides so it provides things like affinities dependencies and constraints I'll be honest and say that none of these ideas are things that have come from these are all pre-existing ideas you see many of them in me sauce and other batch scheduling systems these these things like affinities can be useful so you can do things like saying okay when I deploy this thing this container has an affinity for the radius image and that means what I want you to do is put my container on a physical host that already has read it's running and that would allow you for example to hopefully made more efficient lookups to your cash so if this green nail was ready to put my notes there you can also do things like weak affinity so say try your best but if we're this isn't near that's fine I'll work it all out these things actually it can get quite complicated it is very easy for you to tie yourself in knots having lots of dependencies and constraints and affinities and it can make it very easy for you to end up having a hundreds of machines available to you and for reasons you can't understand swarm just won't let you put a service anywhere so in general when you're using these filtering rules keep them very very simple I don't try and get to advanced about the whole thing one of the other nice things is that docker swarm does support docker compose docker composes a tool really designed for dev and test purposes it lets you bring up stacks of machines so this is a really simple example where I'm bringing up a web node and a Redis image that going to run you know together docker compose now works on swarm you can even specify those affinities in that so bringing up stacks for Devon test purposes is also quite easy in this world and all look look how read abilities if ya mall it's nice it's very easy to understand the topology and the relationships between things there are some downsides though it doesn't rebalance so if a whole host goes hmm sorry you have to know that's happened and go back to darker and say no roots go back to swimmers they know you need to launch the more injured images please that's painful likewise even if a hose doesn't go down for container fails it won't restart it it is not self healing it doesn't have an autonomic property so there's always stuff about how well doc aswarm scales and I'm thinking I'm not going to run a 30,000 node cluster on docker swarm that doesn't self heal in this way doesn't mean desired state this is coming in the next version which I think lines up with the next minor point release of docker although I haven't seen the progress and I'd say the first versions of these things can be often a bit tricky so i would let that bed out a little bit before i use it in prod necessary but anyway they're trying to get there but it's not there yet one of the best things about it is it plays very nice with the rest of the ecosystem the fact that the doctor command line works well the fact that you know docker compose works well as a real boon the other thing is this is still new cases are thin on the ground some colleagues are doing some research on this and they said okay we've got this both oye are using it for their for some of their publishing stuff and rackspace bill to pass which they have on top of docker swarm when you dig into it you find that O'Reilly are actually using the Rackspace pads so actually you have their sort of claiming two case studies when it only really want to be had you can throw a rock and find someone running docker on meat sauce or creeper Nettie's it's much harder with docker swarm speaking of metals let's take a look now the most complicated thing I've looked at in quite a while because there's a lot of moving parts and many of them will hurt you again the architecture on the surface look starts looking quite similar we have a missus master that talks all hosting on those things are running slaves I really dislike this tendency we have in computing to you shitty language like master and slave the other tools here don't use it but it's just wrong I mean they just for it lacks imagination um so anyway this how it works and it's all looks good and then things were about to get complicated because then we discover a thing called frameworks hands up who knows what a framework in general is in computing who's used a framework here with lots of people of you I've used frameworks before I've use things like hibernate hibernate is a framework spring as a framework frameworks in me sauce are something else entirely that really just confused me it's a very badly named concept in me sauce the framework is more like a plug-in and it's actually two plugins so for every framework you have one piece which is the scheduler which is the thing that's essentially and basically make sure things get scheduled accordingly and it is an executor and that is the thing that actually lives on the host itself and does the work okay don't call it a framework please and so if we look at this worldly me sauce was built with things like Hadoop in mind so there's a Hadoop framework out there so the scheduler that sits there talk to the master and then the executor actually lives on the node and so when you're you know firing up these Hadoop jobs the schedge that tells you which executors go use and the executor runs that that has that environment locally you can actually mix and match executors on the same host this can be quite useful if you're running a mixed fleet you can actually colleges for dupe jobs and docker containers on the same machine and have me sort of hand a little bit for you so for example no I can use her do now marathon is the scheduler and execute a pair the framework that we're interested in for docker marathon is the framework for running long running jobs it supports docker images and docker containers so this is what we use to actually run these long processes so again it has that pair in it handle deployments now the key thing about marathon is is it maintains that desired state and so if you if when these machines dies it will rebalance the fleet those containers dies it will restart it that's quite annoying when you're actually trying to kill the container it keeps popping them up this pluggable framework adds a lot of power you can actually write these frameworks in all sorts of languages it's like you can run you in a scowl go java pretty sure someone's written wanna know camel and you start thinking or how often do when i do that but like the fact that I get a Hadoop framework that just works and will handle and has a plenty of case that is showing me sauce clusters handing tens of thousands of Hadoop nodes Siri is based on me sauce using a custom framework for Apple built these things are really really powerful there's also some fun stuff you can do a friend of mine yes yesterday last early this week pointed me at this framework which is aromatic and it's a very simple framework that runs short lives docker container why would you want to run shortly of docker containers what on earth would you want to do that for who here has heard of Amazon lambda aw slam de lambda is awesome right based on an event or a call it will spin up a short-lived job for no more than 60 seconds and then that job that thing gets processed gets torn down you only get charged per call someone else somet told me that you can log into those things if you know how and actually just docker containers under the hood aromatic will fire / short-lived ah container now suddenly I've almost got amazon lambda running on my own data center and that gives you some idea the power of what meat sauce can give you with these pluggable frameworks so let's summarize me sauce it's great if you to another workloads okay that's not if you've got a mixed fleet if you're really doing data processing using storm going to make use of stuff like their d sauce support and running large cassandra clusters and you want to run docker containers this thing will handle all of that for you quite happily it is really really powerful it is very widely used there are large numbers of catering out there for it and it's really freaking complicated with lots of moving parts if you want to spin up a sort of production each set up for marathon running docker containers you're gonna need three no to zoo keeper you're going to three nodes for the meat sauce master in another three nodes of Chronos which is the bachelor that marathon needs and then you need three nodes for marathon itself just to get your production quorums at their limit at their lowest limits and that's twelve nodes and now we can start thinking about deploying some services so there's a lot going on I also don't think the abstractions are quite where we'd like them to be flat topology so let's talk about the new and the old kid on the block kuba Nettie's is actually based on some abstractions that Google use internally that you pre heard this system called Borg they haven't open sourced board what they've done is effectively taken the abstractions they use and the tooling around those abstractions an open source that so while the abstractions are a tried and tested and work inside Google we've got a fairly new tech stack that uses these things and again the it uses docker images and containers under the hood to howl deployment although again it being googly it's all a little bit different so again a very similar diagram we have an API server it talks to things and look it doesn't call them slaves it calls them cubelets that's much cuter than slave okay these cubelets are what then handle the deployment of the things I'm saying things because at this point we have to talk about the magic word in Cuba Nettie's of pods so think of pods like pea pods like you could have multiple peas in a pod and that's what it actually gets deployed now i pod is effectively a set of tightly coupled containers these are things that are going to live together and be deployed together on one node so one pod lives on one node it's a key thing to understand so you can also have lots of this career as a great labels metadata you can put volumes inside those pods which can be quite useful when your ship things like say static configuration again if you keep your images the same we're going to bring a new configuration this is a place where you could bring in a different volume with configuration files on it as part of the deployment pods are mortal they're not a long-running concept I bring up a pod when I tear that thing down it's gone that's it and the way to think about poor is poor as a unit of scheduling I will give a job to Cuba Nettie's it might spin up multiple in my schedule multiple pods across multiple hosts now if you're building America services right you will probably have a very very small number of cases where you'll be putting more than one container in a pod you shouldn't really be deploying lots of really tightly coupled processes in general it's not really a very good idea you still have the ability to have affinities between these things they sit on the same machine so I think in general you're using it as a marker service platform you probably can have one thing inside your pod and this is where it gets a bit odd because you now these are abstractions that work inside Google that don't really work as well I think in the rest of the world everyone talks about pods but they're not actually that interesting what's more interesting as services with services i have a configuration that says look there's as soon as it exists here's its name here with the ports is going to operate under and this bit in the middle which you can see here is a selector and that is saying use a selector to find pods those pods are part of my service so if ffensive LY this is kind of like Al Gore's interfaces where you define the interface over here and things that happen to implement that or automatically implementing that interface just by implementing the methods this is a bit of a loose relationship it's a bit confusing I'd also know it's written Jason anyway ah so yeah it's a mapping really of metadata and ports to these pods um Jason is terrible right for this stuff really awful right Jason as someone pointed out is neither machine the human readable nor human writable and we insist on using it for complex things like this the really frustrating thing is that all of the examples I've seen for Cooper Nettie's all use Jason for these typologies and it supports yeah mall which is better so basically when you leave here's a few things I need you to do is stamp out Jason for configuration files a terrible idea but anyway bit more detail so a call comes in to the kuebler it launches a pod if you've got a definition of a service you have another little thing that runs on your nose a thing called the service proxy it takes a look and said oh this thing matches your criteria is one of these services I'm going to handle the networking for you so you think great I just deal with services I can kind of ignore pods there an implementation detail the problem is it's at the pods where you specify the resources that you want your nose to have and you actually is what you scale the pods not the services and so as an abstraction it's a little bit of a clunky abstraction because I think most of us just say I have a service I think about the service from the outside and I want the platform to hide it now I would imagine a lot of people will end up hiding the pod and service abstraction under their own layer it's a little bit it's a little bit of an odd thing so anyway let's summarize q benetti's it is a bit simpler to set up you still have to run an X ed cluster to handle all sort of configuration management and service of scary and stuff like that it's a few less if a few less moving parts though it's it has got a high level abstraction built in it feels a little bit closer to a platform as a service where is you know sort of me so still feels like a lower level clustering solution I'm not convinced though that the abstraction is actually right it's also very new as an actual going concern but there is a lot of support out there for it their core OS have jumped on board red hat with openshift realize that open shift VT was terrible and have now made openshift v3 basically cuba netted as far as I can work out my advice if you're looking at having a platform for managing microservices you're looking to you something that's going to help it make you easier to handle multi host docker what these are the three frontrunners you're seeing at the moment they're not the only three there's amazon ecs which is a hosted service only it's not great i think it needs a lot of work with things like rancher but a lot of these things are noise compared to these three at the moment I wouldn't if I had to start today I wouldn't seriously look at dhaka swarm at the moment I would do a bake-off between me sauce and Cuban it is hopefully docker swarm in the next few months will produce an out-of-the-box experience as good for smaller cluster setups but i think if you know at the moment if you were starting would be one of these two so let's just summarize we talked about some core principles today so we talked about the idea that we need to embrace independent employability and to any platform you pick is going to be based on this idea build your artifacts once and keep the topology separate and use the same deployment process everywhere and again whatever platform you pictured make this possible docker images as artifacts are a very very good artifact choice I would also say big shout-out to operating system based packages as well and we talked a little bit about the criteria for selecting a platform this is a crazy space docker the engine the stuff the single node experience is pretty good is pretty robust when you get into the platform level it's a journey journey journey space um so you have to accept that you know whatever you pick now may not survive the whole future a friend Clarence fact picture I had to try to steal it so you just accept that you are a bit in the bleeding edge um some of these platforms allow you to get in a monkey with things a bit more i think new sources got the chance you can adapt it more i think if you stayed still for six months or a year there'd be another five platforms you could look at so just be where you are in a little bit of a riskier area here it's still significantly better than we were say 6-12 months ago as promised this is a discount code for the book that will get your forty percent off if you go to oreilly com 