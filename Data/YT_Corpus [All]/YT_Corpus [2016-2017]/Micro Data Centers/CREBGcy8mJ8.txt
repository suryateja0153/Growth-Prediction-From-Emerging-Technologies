 so thank you everybody and I think we need to get the session started my name is cliff gross nur and I'm responsible for data center cloud and sdn research at IHS technology and I have with me a very esteemed panel to debate what I think is maybe a topic that we're all probably opinionated on and maybe even already think we know the answer but I think the devil is in the details and that's what I think we're trying to get out of the panel and with the topic we have today so with that I'm going to ask the panel to introduce themselves we can start over on this side Brianna yes all right hey I'm Toby can help i'm one of the founders and the CTO of mesosphere I'm Tim honkin i'm one of the founders and technical leads of the cooper nineties project christopher Williams topia chief architect for project calico hi my name is Mike gelser i'm the product lead for Dockers runtime so that includes the docker engine which is which is the component most people know and also some of our orchestration technologies dr. swarm and some of the technologies that glue things together compose machine our provisioning tool etc hi folks I'm Pino de Candia CTO at me da Cunha the maker of me tonette network virtualization well thank you as you can see we're very well represented here today and what I'm going to do is lead off the discussion with just a few minutes sharing some of the research that we've done recently that I think might be of interest to everyone in the audience and it's something I refer to as the meta cloud which I believe containers are a very important technology to make this happen and so with that let's take a look at sort of our view of how the markets been unfolding and I think we all know history right around moving to off-premise cloud computing and bare metal servers and then we went through a server virtualization phase with on-demand computing and that brought in you know our ability to be very agile and that's what I think that many of the enterprises that I work with have been working on and that's meant they needed to deploy orchestration in their data center and you know some of the statistics we heard there earlier today in the keynote around that certainly bear that out and also that's borne out in our research surveys as well but I think we're actually entering a very rich period now from twenty sixteen to twenty nineteen and I call it the containerized server period coupled with the meta cloud and when I think of servers I think of them being multi-tenant be them virtual machines and containers and the question we're asking is how is that going to work together I think there's still some pretty important details that have not been worked out as we get past 2020 I think we're going to get to a point where we see cloud brokerage and very very huge compute farms be exchanged on a daily basis on the spot market just like we do copper today or other minerals and other elements so I think when we get to that stage I left the question mark because I actually don't know the answer yet as to where we get those so with that what I want to share with you is some research that where we show that where we look at cloud services and before I do that I just want to share with you our segmentation for the off-premise cloud services market where we actually separate out from I something we call cloud as a service which includes orchestrated platforms with containers such as Cooper Nettie's and the other competition that do the same thing and look at services that provide orchestration as well as infrastructure as a service the reason we split it out in our mark in our market research is because I think that's where the real innovation is going to happen in the next few years and that's really where we want to understand the growth and so with that I'm going to share with you this chart and you can see that cloud as a service portion is the blue one the dark blue one actually where the the gray one is infrastructure as a service and the light blue one is platform as a service where developers are provided pre-built application building blocks and of course we have the software as a service on top so the entire market by 2020 we project to be just over 275 billion we're cloud is a service as you can see starts to become a very size of proportion and I don't break it out yet but i believe containers will be a strong driving element of that market segment so i think there's a i believe they're from our research there's a good amount of growth now another element that i think is going to be driving growth in container usage is the fact that when we ask service providers in North American Enterprise what kind of cloud architecture do they want to use is it public cloud private cloud or hybrid cloud where we move workloads from an on-premise data center to an off premise data center it turns out that hybrid cloud is the architecture that is slated to have that respondents to our survey expect to use the most over the next couple of years and of course i'm not sure if anyone if you caught it but i saw a demonstration on google about their latest visualization tool to go alongside cooper Nettie's that allows an enterprise not only to move workloads from one data center to another be on prem off graham and other third-party data centers but also visualize the workload so it's for me the first step of providing the tools to really move to the multi or meta cloud depending upon how you want to refer to it and with that I'm going to leave you I think this is my last data point and then we'll turn it over to the panel I asked North American Enterprise on average how many different cloud service providers do you use and in 2015 the response came back as 10 now this includes SAS providers but before doing this survey I if you had asked me I said oh I think it's three so this is obviously much further advanced than I thought and also expected to grow to 14 by the respondents so with that let me leave you with a couple of final words that I think meta clouds are here and we're well on the road to building them I think that containers are going to be a key enabling technology for this and I think it's up to the panel now to answer the question how far can we go so with that I'm going to ask a question that the panel has actually been eager to respond to and this has to do with one of my favorite topics and that's really power struggles because I think at the end of the day not only is their technology but there is the human side of any technology adoption curve and so a panel was saying that they would like to talk a little bit about or speculate on what happens in terms of who's going to control what going forward is it going to be the upper sorry it is going to be the ops people where is it going to be the DevOps people are the devil so with that I'm going to pass it over to the panel and please go ahead and take your turn okay I'll start here this is a subject that I'm interested in I'm a Linux nerd at heart but I'm also interested in how power works and organizations and and how that how technology changes that and so one of the things that you know we really at docker were a very developer focused company so one of the things that we have have seen over the past couple years of the sort of the docker revolution or the container revolution is that the balance of power is shifting more and more away from ops and toward developers and I'll give you an example relating to networks networks used to be purely an infrastructure concept and now with containers you know a network is something that an application developer actually defines in his or her deployment manifest you know docker compose manifest or some of the other people up up here represent other approaches to orchestration but either way these are increasingly software-defined entities that are not being defined by ops but they're actually being defined by the application developer and tailored to that developers application and so that has huge implications for ob's because if you're in an obstacle you want to enforce certain policies and you're going to be held responsible for security but at the same time you've got developers just sort of operating at a higher level of abstraction with applications communicating the in ways that you can't control and with TLS you may not even be able to see what they're doing exactly and so I think that's a really interesting area where containers are kind of changing the game so well I mostly agree i think i'll put a different color to it though so i've had the experience of both operating infrastructure as well as as vending into infrastructure and being a company that writes applications so sort of across the spectrum here and i wouldn't even necessarily say that it was a power balance problem before say it was an impedance mismatch problem the problem was that the what infrastructure exposed to developers was a very infrastructure centric thing could have either been in in the bad old days tickets or in more modern days in this infrastructure that we're talking bout here in OpenStack we make developers sink in terms of oh I need what segment what network am I gonna put this application in verses that application in how do i connect those two networks together do i create nel through citadel three segments or null three route or how do i connect these things together making devs think in the very twisted mind set the network operators and i'm a network operator so i'll say that um so we've made them think that way on the other side though the operations guys would see these artifacts in the developer i don't know what the developer is trying to achieve he created an l-3 router was that a firewall was that a there's a question can you hold till wait we'll have ten minutes for questions yes Holden was only in place you could be first up so um you know so I end up looking at this as an operating what was he turn create a firewall was trying to create a router what was he trying to do so I man i'm lost sound have the developers intent the developers annoyed because he can't do things like he thinks the way he normally thinks what we've done with dr. composed with KU béarnaise fine-grained policy with with net modules is we've allow the developers to say things i need to talk to things be with this protocol or with these characteristics that's the only thing the developer really cares about doesn't care how the infrastructures put together so he can just define what he wants the infrastructure renders it the infrastructure operator can look at that and say I know exactly what the developer was trying to do rather than trying to yes so I don't think this is a power balance thing I think we've done as we solve the impedance mismatch and now both sides are more empowered and more powerful than they were previous so I think it's a net gain for both sides it's not a power balance on a power shift anyone else so I think the the future of this layout continues to have two well-defined hats and there's the operator hat and there's the developer hat and sometimes the same people wear two different hats and sometimes you have different people wearing the different hats but I think that you know to latch onto the example of networks you as an application developer i think it's i think network is the wrong abstraction i think you you're you want to talk about what can talk to what right draw the graph of your application and and let me enforce the graph and don't tell me how to do it and then as the operator it's my job to figure out how do i implement that right how do i how do i take my network layer and implement the policies that you've described because now i know the intentions and i have the freedom to change the implementation without actually breaking you the user because i didn't expose you to constructs that are that are a little too brittle in small organizations even medium organizations these people may be the same people write the guys who set up the network may be the same folks who are writing the applications and that's totally okay but as you scale and as you move into larger and larger enterprises I think the two have to be different at least in some regards you know when you start talking about certifications and and and compliance and things like that it's a big deal and you can't just give the developers the keys so i think the split is is here to stay at least at the large end of the scale and i agree strongly that policy and description of intent is is very very powerful that's the direction I think things need to move yeah so I agree with at that point you know the two roles are going to stay there just you know a developer cares about you know I want this thing to just work I have my architecture you know just just take that blueprint that I that I created here and and you know get it out there and as an operator you want to make sure you know things stay up and running and you want to make sure there's enough capacity enough machines enough network bandwidth and all that stuff so so those things are not going to change but um but as Chris was saying I think we just have much better tools available now you know so that impedance mismatch is is not there anymore and Devon ops can actually work together better and and speak a better language together so we fixed the interface i think that's that's the main you know the main thing that happened here that impedance mismatch in old conversations you see the dev guys in the Ops guys and there's a table and now you start talking about this stuff they all actually seem to like it and they're actually having constructive conversations rather than throwing flame balls back and forth so yeah I guess I guess I have the last word from my point of view that there is a power struggle because although that we're going in the right direction with the intent expressed in code the fact is that the technology isn't yet quite there so for example we're using technology that cannot express policy security policy yet and so that means that the customers are rushing or sorry companies are rushing to deploy microservices on technology that doesn't quite Express all the concepts that the security team needs or the infrastructure team needs and that's okay because they're moving fast and they're going to do it and I think the power struggle that will see with container technologies and is sort of similar to what we saw with VMs with virtual machines we virtualize and some of the traffic that could be seen by the security team before could not be seen once we had virtual machines containers does the same thing and we might you might say that okay maybe you are in seeing something in monolithic application you weren't seeing the process is a process interactions but actually now you've split up that monolithic application into micro services there are more risks because any of those services and now exposed and can be attacked and then can be then can the attack can be and expand laterally so I do think that there is a struggle and there is sort of a fear of what you put into the micro services based on the on the maturity of the technology to express some concepts well thank you i think that covered a wide range of things i'm going to ask a question now that it's actually dear to my heart and something that i kind of changed my opinion on over the course of the last few months so so if you'd have asked me a year ago I'd have said there's no room for innovation in hardware it's all it's all going to be done in software and hardware is going to be vanilla especially for switching merchant silicon but I've actually now been convinced otherwise that there is room for innovation and hardware and I want to turn this over to the panel to talk about what does this mean for containers in terms of where we might go with innovation in hardware we certainly see innovation and hardware helping on the networking side so we're seeing Network chips now in white box switches that allow us to do we will get to look at packet queues and look at congestion and drop events so we can help if you exactly what's happening in the underlay for flows between applications and we can identify those flows trace them back to the services that they belong to even on the server we see that happening where the hardware look at TP DK and maybe this is really an interaction between software and hardware but we see chipsets coming out from various vendors both for network switches in the underlay but also for the servers themselves to go a lot faster let's really have to do a lot of work in servers and the OpenStack community has shown that on the end the software to enable the hardware acceleration yeah at the container infrastructure level there are also some interesting things going on I'm an open source p.m. so I don't want to come across as plugging particular vendors but you know Intel is doing an initiative they call clear containers where basically they'll spin up a hypervisor and run your container inside that so if you know that I'll give you greater if you're concerned about containers not being fully isolated that you know that's a technology that you can look into for hypervisor level isolation and um and you know I think that that trend is only going to accelerate and other chip vendors are going to get into that and and look more closely as production workloads move more and more torque containers so I think you know that interface between the software and hardware is interesting things like DP DK I mean the FDI o project they just got kicked off in the sea and CF are not in the CNC ethin linux foundation it's a high-performance vector processing packet processing a forwarding paths that's tied partially to DB DK that's that's an interesting data path from the networking site I'm a networking guy I think there are some folks noodling around with exposing doing TPM chains we're even containers you know can authenticate the underlying OS which can authenticate via TPP via TPM to make sure you have a trusted chain all the way from the hardware up through the actual application potentially even tying that into network policy I if you don't have enough trust authentication chain you can't talk on the network that's that's an interesting thing as well the one thing I do hear from a lot of customers though is they don't want to tie their infrastructure to a specific proprietary implementation and burned too many times so i think the hardware innovation is interesting but you know and people might take advantage of it if it's there but i don't see people building infrastructure bedding that a particular piece of hardware innovation that's proprietary is is the basis of that infrastructure because at that point they've just walked away from all of the open-source things that they've been driving toward which is to make sure that they have flexibility of movement so they might take advantage of it but are they going to KI their design off of some proprietary piece of hardware I'm not seeing that going forward but I made me wrong you know it rewind a little bit to the the advent of VMS and at first VMS were a pure software construct and all the hardware vendor is sort of that silly go away and then as VMS gained momentum you you see guys like Intel and AMD coming in with instruction set extensions and new platform extensions to make virtualization harder harder harder boundaries faster and you know more more reliable I I would never bet against intel on this I'm sure that they're over there dreaming up ways to make container isolation more secure you've got clear containers you've got they've got a bunch of brains over there that are working on fun new ways to engage hardware as a way of making this solution better because they understand fundamentally the easier it is to use something the more of it we use right and what does Intel want they want us to use more cpus right I also think that there's a lot of room for cool stuff to happen around the authorization not the authentication stuff expanding the role of TPM and I think this can be neat stuff in in some sense containers take machines that people had scoped for you know it'll run two applications three applications and we're saying it's gonna run 200 applications instead and some of the platforms have to adapt to that sort of orders of magnitude scale change so I think we're going to see some growth there yeah I think I think there's a ton of innovation happening in hardware and if you talk to any of the hardware guys they're actually really happy about containers and cluster schedulers because but virtualization did to most of their products is it kind of just hid them away so you know you couldn't expose the special you know characteristics of this machine because all it was is virtual cores and so on and so these guys are super happy about about containers because what you can do when you have containers and schedulers you can say all right you know this machine has a certain you know GPU on it or this other machine has an FPGA or a TPM or whatever the hardware characteristics are you can then use that to place your applications or even architect your applications right and so the hardware guys are really excited about this I think they're all you know looking at you know what're you know what can we do to enable containers and you know I think that the last point from Tim you know that the containers put a lot of strain on the network so you know the Knicks are just because we're running so many on a most and so the neck vendors are looking into you know what can you do there so yeah we're going to see a lot of hard renovation and I think we can surface it more with with a container based infrastructure okay I actually have one element or thrust here on the hardware innovation side I don't think anybody mentioned and I'm just asking one panel member to be brave maybe if they can talk to that is that the p4 initiative how to Stanford if there any relevance here and maybe I'm off based on that I don't know about it well then something to look at it effectively it's a so project out of Stanford I know it is but I'm not qualified to comment on it no worries for those of you that haven't heard of the p4 it's a project to best basically the way I think of it as a compiler for silicon to indicate how the silicon infrastructure should process pockets making the compiler convery field programmable it's kind of like micro coding for CPUs but now for Network trips ok let's move on and so one of the questions that I think we we should really touch on because I've heard people ask me this and maybe the answer is easy but do virtual machines disappear and if so why and if yeah and if not right I get the idea around if they say oh oh there's enthusiasm on this one that's great if they do stay around how do they get integrated to work together with the containers all right i'll answer first so VMs will be around for a while you know if you look at any of the public cloud providers obviously it's all vm based there is you know there's tons and tons of workloads where where VMs will always make sense you know where I have a very heterogeneous OS environment and so on but but we're also seeing more and more work loads moving to containers so a lot of greenfield development happens in containers it's a really great fit for micro service architecture which which a lot of people are doing they're also great fit for for workloads that you know that are fairly short-lived where the virtualization overhead it was just overhead of booting is too large and so you know I think they're going to be around but we're going to see more and more workloads shifting to containers and you know we probably also going to see VMs actually running in containers which is already happening at places like Google I agree VMs are here for for the foreseeable future you know at least the next decade they they are an important piece of the ecosystem they solve they solve problems in certain ways that are better and different than what containers can do and I think that there are problems that will demand vm solutions for quite some time I do think that the balance will shift towards containers as people go for the efficiencies and the density that that will make these things more affordable but i don't think the m's are going anywhere anytime soon so I ditto however I do wonder over time will those VMs that exist in the infrastructure will they stay what I would call a heavy vm there's a full-blown LS image or do we start seeing more micro or runt VMs that that surface just the bits of the virtual machine that are unique for this application that isn't generally available I'll put on my large infrastructure provider hat I used to wear it so I think and so put it on one of the things you don't want to do in your building an infrastructure is say okay I'm going to have X percent my infrastructure dedicated to X and the other part of the structure dedicated to why because the forecast will always be wrong you end up with stranded assets so at some point you have to say what is going to run on foot um you also don't want necessarily to schedulers fighting over the same set of assets I think there was a there was a cartoon in South Park in the ski instructor saying you're going to have a bad time you know at some point something's got to own the resources in the infrastructure I look back to the late 90s we were building internet backbones nine percent your traffic is IP and it's growing a hundred percent of the year and five percenter traffic's ATM not folks here don't know ATM was I'm showing my age it's not a ATM machine it's actually was a protocol and it's growing at five percent a year do you build an ATM network and put IP on it or do you build an IP network and put ATM on it so I think as things move more and mortar containers you start saying okay at some point there's gonna be an inflection point where you stop running containers on VMS and you start running VMS on containers because the container workload if you believe in this the container volume and and percentage of applications for own containers is increasing so that should be eventually become your native infrastructure in your native scheduler and then containers become an application on top of that if you believe VMS will continue to dominate you do at the reverse way yeah i don't i don't have a whole lot to add to what's already been said i mean basically agree I you know I would point out that especially in the early days of containers there was a lot of confusion I think they were often compared to VMS it described as like a lightweight vm whereas i think they solve a somewhat different problem fundamentally a container is a single isolated process that is still running on the host Colonel whereas a vm is a totally you know hard isolated even hardware isolated you know separate machine that have just happens to be using the same hardware and I think you know that there's a big difference between you know situations where you want one or situations where you want the other so I think VMs will continue to live on for a long time I mean there it's a it's also a conceptual model that's very easy for people to understand whereas I think containers are a little bit more confusing it's like what is it is it a process is it you know is it a lightweight vm i think that confusion could persist for some time so i don't think i have a lot of wisdom to add but i guess i have a slightly maybe a slight nuanced view which is i think that containers will be on VMS for quite a while i think that the VMS will guarantee the portability as we see hardware tricks that make certain that can make containers faster we'll see sort of VMs perhaps come back because they might be cheaper to do and compare and compared to today so the trade-off of vm to container might not be quite as quite as severe in terms of loss of performance but I think certainly all the public clouds are going to be running containers and VMS for a very long time then if you're if you're architecting something you don't to easily move workloads back and forth between public clouds or hybrid cloud you're going to want to have want to have something similar but nor do I can find the role of VMs to act as as homes for fora sorry AZ homes for containers I think just like we have we still have mainframes we have we're going to have applications they're quite complex that are perhaps not so easy to to container eyes and so we'll see that for a very long time okay well we're into the question and answer period so there was a gentleman over there don't know if you still have your question okay so you're off the hook if someone does have a question please come so why don't you get in line and be next then okay so we'll have time with these for two questions if you have one go ahead great so we're talking about how urban it is is trying to capture the developer intent in terms of policy language and then the decoupling it and the operator going and implementing it on what our abstraction means he has right so but the intent is only a portion of the intent really comes from the developer and there's a lot of the intent that comes from a lot of other people right so and this artifact needs to be portable across multiple people right it could be like a load balancing guy it could be a security guy it could be some guy was doing like operations and once a bunch of chef and puppet aliens so this how do we really want to make this artifact truly portable across different personas and then have the final infrastructure keiko implement it that's something I think are you guys thinking about it because that's probably I don't see it spoken lot but I can necessarily required so maybe I'll take the first shot of this question I think that once so so I mean networking so part of the way I think of this is once we can do the same thing for containers in terms of networking that we do for VMS then the part of this discussion that is container focus sort of goes away you have workloads you have endpoints so matter if they're containers are not from a network standpoint from a security standpoint it really doesn't you're right that the application owner only expressed as part of the intent there's other parts of the organization that can impose policy on the workloads and so on so you do need policy management's you need to provide ways in which you can sort of overlay advanced security for example transparently to the application so service chaining of course becomes really important and then but the management pieces is crucial sort of the api's that or whether it's templates or API is that allow you to put multiple layers of intent on top just to finish before I headed office I'll say that sort of our people thinking about this so absolutely I think once for example the trunk port doing trunk ports in neutron or VLAN aware vm is the original name of the blueprint that that makes gives a neutron port to every container and then all of the advanced services that are offered by neutron will be available for containers in particular service chaining at dock or our orchestration system is called dr. swarm and this portability issue comes up with a lot of customers and it's something that we're very actively working on what specifically portability between different developers or different development teams so so I absolutely agree with that that's an important area so this is something that yeah I we're definitely got it up again I have an operation operator hat on the idea is still it's an intent model so the way the way you want to think about is there's different layers of an of policy and policy can trump higher levels could potentially Trump or override lower levels of policy so the highest level might be the operations guys and their intent is we're getting an ssh door knocking attack from this block of addresses somewhere so on every single endpoint be it via or container I need to block inbound 22 from these addresses and I need to deploy that everywhere no matter what the developers intent was or not because I need to save my infrastructure or it could be the security guys below it saying i don't care if this workload is pieces says it's pci or not but he has to pick one of the two if these pci ends up the pci walled garden if he says he's not pci he doesn't have access to pc i enabled resources and if he doesn't say either he can't talk to anyone on the network till he identifies himself which camp he belongs to this is just that's the same kind of intent he doesn't have to think about this workload in that workload and he says you know things in pci there have identified pci or go get this policy versus another policy it's the same intent model just who's applying it and what priority they end up in so I don't have a whole lot to add to that I want to refocus a little bit on this idea of layering of abstractions we've got five people built out VMs we built that OpenStack as a way of managing at least in part applications on top of metal in a way that was more manageable than metal right now we have containers and people sort of like containers in some ways better than VMs do we build containers on top of vm to rebuild containers alongside VMs or perhaps instead of VMs and some of the infrastructure that we've built around VMs maybe we don't need it anymore right maybe we can just throw some of that out because it was useful for EMS but maybe it's time for a little bit of a technological contraction right our complexity contraction here it's not necessarily true there's going to be plenty of places where virtual networks are going to continue to be really important but I think there's a lot of places where people did them because that was how they thought they had to do them and that maybe they don't have to yeah nothing much to add agree you know the everything is intent driven know whether which persona it is and and you get depending on where you live in the stack you get the right abstractions to express your intent ok we have two questions and we have three minutes so what I'm going to suggest is go ahead and ask your question you're next but only one or two panelists that really have something feel very motivated answer it's real quick question can containers be first-class citizens without IP addresses yeah absolutely I don't understand the question I mean I feel like I p is such a fundamental part of what we've built that it becomes actually I'm surprised you came down on that side continuing pods versus containers I'm making a distinction there oh okay sorry so in it to make that distinction right I mean communities has an abstraction that's sort of one very thin step above containers and I think that IP addresses are an important part of the identity at that level of course there are jobs that don't need networking and they don't need IP addresses and that's fine right but I think in the in the abstract like containers this concept they are they network the network is the computer to borrow a phrase I don't need to add anything well those are good questions almost stumped the panel we got one more and a couple of minutes to answer it hey guys my question is about impedance mismatch so Anand aclara tomada like heat the VMS have a certain prescriptive way to be described and in the container space and may sauce or cooper Nettie's they have a different way of being declared that fully addressed how do these come together in a container first model the team also so um so I guess I think of it as two different platform so you need certain we have certain tools for VMS we have certain tools for containers why can't we do the same kinds of templates for VMS that we do for containers maybe because they're too slow to spin up can't we just all get along and do we want to get along I guess my view not heat specific but open second in general OpenStack tends to make people think about the underlying infrastructure rephrase our questions the way we define things in terms of the underlying network the developer nice things about you know l three segments or networks or vlans or we make them think about the underlying infrastructure that might not be the right abstraction going forward it should be more of AI need this much compute I need this many cycles of type X I need this amount of hash space or memory space or storage not thinking about the how that gets rendered we get into trouble when we make the developers think in terms of the infrastructure and then we lose the actual developer intent we get what the developer thought they needed to say to get what they intended I think that's the wrong the wrong constructor so so I've been hearing this a lot that that I ass makes developers think about about low-level details and it's certainly true that as a as a young technology OpenStack made people to you can see horizon is structured around having to choose all these things you have to understand IP prefixes and so on but there are certainly layer as an OpenStack that are starting to abstract that all the way you can use catalogs now to launch applications and I think we'll see more and more of that heat is a template model to launch eunos ok so maybe heat isn't quite as convenient or according to your taste as Cooper Nettie's templates for meadows or a marathon templates but there's no reason why we can't build the same kind of templating on top of VMs sure there isn't yeah there isn't a reason why not ok I think we're at the end of our time slot and I just want to thank the panel for the very brilliant discussion and the leadership they provided here today thought leadership for us so thanks again everybody and on to the next element 