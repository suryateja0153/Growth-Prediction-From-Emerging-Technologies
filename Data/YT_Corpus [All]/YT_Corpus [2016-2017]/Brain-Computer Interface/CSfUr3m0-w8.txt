 ALEX: How is everyone doing? I'm Alex. Thank you for coming. I build software to help understand human behaviour, and one of the sources I base my research on is electricity. You see, the body is an ocean of electricity. Everything we do, it is facilitated by electrical signals flowing through our body, when we speak and move, even when we are simply breathing. The way Rio de Janeiro charge ourselves is of course by resting and feeding. Well, today, we are going to use JavaScript to witness some of that electricity, specifically the ones coming from our brains. So, before we get started, I'm going to need a volunteer to join me on stage. And hopefully, someone without a dirty mind, without a huge head so he can fit the headset here. Who is up for the challenge? Someone there? Someone here. You there? Please. How are you, sir? What is your name?  Mike. ALEX: Where did you come from? MIKE: Holland. ALEX: Thank you. You look like you have a decent kind of head. MIKE: I have a big head, yes! ALEX: It looked like you had a head that will fit this. You can grab it real quick, so you see what it is going to be. Put it on. Not too fast. Let me try to adjust it. You do have a big head. MIKE: Thanks! ALEX: It's fine. This head in particular is medium-sized, if it is me, but it doesn't fit a lot of people. Let's see, adjusting here. Okay, so, wait. A little more. Maybe it is not too small? That's it. It company doesn't go quite down. Can we try someone else! Sorry! [APPLAUSE]. Promise I won't embarrass you! What is your name, sir?  David. ALEX: Oh, yes. Let's see. Perfect do you feel a spike here? Is it too tight on the sides! What about back here? Do you feel it? DAVID: yes. ALEX: It is pretty much all set. The next thing I'm going to do - can you hold this for one second? I'm going to bring down this little plug for the ear lobes. Not a big deal. Just your head doesn't explode. DAVID: I hope not! ALEX: So I'm going to turn it on. Back here. If you don't mind standing up and turning around so they can see the blue light -- or not! [LAUGHTER]. So something that I didn't tell you - my bad - I'm going to have total access to your brain, okay? You have nothing to hide! [LAUGHTER]. So, if you're unsure you've got the right person or not, calm down, just focus, and think of something appropriate for this situation. Awesome! Okay, so - let's do this, right! So what I'm going to do, I'm going to launch my demo here, and this is going to be a quick image. It is going to be like the first thing that flashes as a demo was kind of like - so, relax, right? It is going to be okay. Okay? One, two, and ... that's not it, I'm just kidding! But now seriously, I'm going to go to my node terminal here. I'm just going to launch my note app which is the entry point for this application. I'm going to do npm run visualise. What is going to happen is that node is going to crash! [LAUGHTER]. Typo, of course. Anything that's going to go wrong during a talk is your demo - always! That kind of looks right. Node is going to open the app. It is going to start communicating with the headset. We should start seeing some activity, some data samples. It is going to probably go pretty fast. Actually, give me one second. Going to make sure it is properly turned on here. Give it another try. Okay, last resort is just unplug. Plug it back in. All right. I think it is going to be good. Awesome. So, right now, what we are seeing is we are getting a lot of data from the headset, and if you start looking in here, there I run three - there are around three channels that are not changed values which are four, five, and six, and I'm going to make sure they're making contact. Channels 5, this one, assist fine. Four ... there you go. We are almost there. We have then this one. No, I'm not going to kill you! Yes. This one is a little tight here, right? If you're uncomfortable at any point, please let me know. What about there? You're okay. Oh. Actually, this one is making a lot of contact - not making enough contact. Okay you, we are almost there. There you go. Okay. One more. This one here. Here, here, it is fine. One, two, and three. Four, five ... . I think this is it good for now. There you go. [APPLAUSE]. Making sense. What I'm going to do is now launch the real app. We have some activity here. So you're very relaxed. That's good, okay? Now what I want you to do is just let's start with - try to clench your jaw. Okay. Good. Now try to like blink a few times. A little harder there. Okay, there you go. So we are getting some electricity. Awesome. Great. So what we are seeing right here is in real time this headset, this device, is capturing the electrical impulses from the brain in real time, and I'm receiving it, and, at the end, it appears in the browser. Another way of visualising brain waves is by actual frequencies, right? What I showed you before was a time series and now we are seeing this in frequency waves. Great. So far, so good. By the way, I hope you read the fine print! What just happened? This is what I called NeuroJavaScript. What it is, it is a combination of a lot of things, or the things that you might be familiar with put together, including isomorphic JavaScript, the headset, some data science, of course. Let me break down the way this actually communicates. I don't know if you can see that black part there? There's not a lot of contrast. Okay. Left, you had the headset, and via Bluetooth, it sends a single to this USB donning he will that you're probably able to see right here, which is part of the kit. And that USB dongle -- I hate that word -- connects to the computer, of course, and communicates with my node application via the serial port which is pretty much reading the information directly from the USB which the headset is sending serialised here, ultimately getting to node and that is when the actual work starts happening on the software side. Then, via web sockets, we send this data to the browser. Of course, there is a lot of manipulation done to this data. Will the browser be able to interpret it in a friendly way? As you can imagine, we have seen the data in kind of like, in frequencies, in time series, so there is a lot of parsing between DOM and the data side, algorithms, for example, the FFT for the frequency. But also the amount of data being sent to the client is specifically trimmed down to something that the browser is not going to be overwhelmed by. So, again, we go to the headset, we have got Bluetooth, and I want to break down each one of these pieces. The first one, the headset, is observe BCI, and it stands for an Open Source Brain Computer Interface, which was created, manufactured in New York. But the great thing about it is that it is open source in both hardware and software. This one here in particular was 3D printed height there in the office, and I guess their offices, and you can see here that most of that is the plastic;is that 3D print, and then ultimately, if you don't mind standing up, and just turn back so they can see the board, we have the - we say board, which is actually where all these sensors go to in order to process that signal. And the cable. So putting this together, you can do it yourself by the board, by the sensors, and then put it together at home for an affordable price. A few years ago, when the technology was not there, mostly for medical research, something like this, would be over dollars 30,000. Now it is a few hundred bucks at present and that is the open BCI headset in a nutshell. If you want to find out more, you can go to openbci.org. This is an illustration of how it comes together, the little pieces. It is actually not too bad. There's some complexity, but everything is well documented, and on GitHub as well, including all the 3D design. The USB donning he will, which is part, of course, of the kit, is just based on Arduino, that family. This one uses a radio frequency in Bluetooth to communicate with the board, but it is custom-made at that to washing with that headset. And then we go, we get to the serial port part, and we have note.js. I don't know how many of you have worked with a serial port. I have some friends that were the ones that developed this whole - the idea is that the connector, in this, okay? The serial port, could be swapped for other things like Bluetooth directly to node or Wi-Fi. Right now, we are going - in the future, we will have the ability to use more connectors, and it's being developed right now. On the node side of things, you can simply do an install, that will give you the package that will get you the class that is going to allow you to listen to this event. At the end of the day, everything coming from the headset, and receiving node is converted into an event that can be accessed easily with an sdk. If you're interested in knowing more about the node side of things, you can definitely go to the repo. I'm going to have some links up here later for you to see. But, as you can see, at the end of the day, we get a data sample, but what is this data sample? So this headset in particular has eight channels. You can see them mostly here, not all of these nodes are being used, but eight of them are in very interesting places. It uses a grid system called the 10/20. The odd numbers are on the left, and the even are on the right. When you're doing some research, you can see some interesting activity happening more on one side of the brain than the other. And that is something definitely positive. Great. So, we get channel data, which is like eight channels. Each one coming from the headset in order. You have channels 1, 2, 3, 4, 5, 6, 7 and 8, and two on the back side. We have some auxiliary data that we are not going to go into detail today, but pretty much data coming from the accelerometer. Then we have other data like what is the stopByte number and other fun stuff. We are getting one of these every four seconds. That's a very good sample rate. It provides a very decent high-quality data from the headset. So when you start working at that level of something happening every four milliseconds and then you have to plan how that data is going to be made later and channelled through your app, it becomes like a very interesting challenge, so, definitely had a lot of fun. Ultimately, we get this through the browser, the web sockets. How many of you have used web sockets before? Very cool. Yes, lots of fun. The interesting thing about web sockets is that depending how many events you're spending to the client, you know, it can really affect the performance, right? So, limiting the amount of events from web sockets is going to be something that we need to take into consideration while developing a UI for this. Yes, the UI, have you seen some technologies that you might have heard - one of them is Angular 2 which is web controls. Change detection, and then using some data association libraries based on different technologies. Of course, well, Smoothie is based on canvas. It has web GL, so I'm mixing these technologies, because the type of associations that we need here, it's a lot Demme that not a single library is going to satisfy all the use cases. Okay, so let's see your mind one more time. It is awesome. I'm going to show you again and start explaining to you about implementation of the time series in real time. Great. So, our app is still running. And in here, we see the time series. You can City that the numbers here on the left are - can you blink right now? Did you blink? Burp? I noticed something there, something happened. There you go. So, the numbers here on the right are actually the amplitude, right? You're going to see positive as well as negative numbers. You have a lot of negatives numbers, which is something as good or bad as positive because it is really nothing to do - it is going to - the energy's going to flow, you know? Up and down, so these are just changes of amplitude. They don't mean anything in particular as far as like when they're like negative and positive, just something to have in mind. Great. So what I'm going to do now is that I'm going to show you how this is built, and you can take a break from that, so, you know. [APPLAUSE]. I turned it off. I'm going to start my app real quick. Let's talk a little bit about Angular 2 and some data visualisation. How many of you have used Angular 2? A good amount. I've been been using it for over a year now since it's been developed, and beta-ed the candidate. As you might now, one of the options is to use typescript which a friendly way of developing Angular 2 in particular. The code that I'm going to show you here is based on Typescript, but you're going to be familiar with it if you have used, say, Babel or transpilers. I try to keep, the specific code out of the presentation. So, in here, we have some module imports, and, from the Angular, from the angular API, we are getting some the things that we have availability like Component, element reference, and hooks. I'm going to explain the life cycle so you know how it is connected. This time series is based on smoothie which is the best library I've found for streaming data. Mostly because of a time series, of course, and we are getting classes like the Smoothie chart, and like a time series. Then we have some like app-specific code and, of course, sockets. So, this is the component metadata that I have created for the Time Series component. As you can see here, we have a selector. If you're familiar with Angular 1, please raise your hands, and you've used it before. The will, you're going to see Selector which is not different from the Angular 1, but you have some template URLs, and then we have some providers, kind of like a way of injection. And this syntax here, the add is metadata and definitely available in Typescript. After that, let's talk about what is going on on the view the most important part here is that we have a - whoops. We have a canvas element which has an ID. You can option the width and height. From the Angular world, we are going to target that element, and then we are going to stream the data via Smoothie. Let's see how it looks like. In my class definition, this is the top part, and it continues, and I'm going to show you everything, but we explore a class that takes time series component, we are implementing here some of the hooks, for example, the OnInit, Ondestroy. We have dependency projection here. Basically a reference to the element itself, let's say the view that we are referencing from the component met at that data. We have sockets, we have a service that has some utilities. And then most importantly, we are creating a smoothie chart here. Is and then we have some data containers, in this case, arrays for amplitudes and timeline. So I'm going to run the application in simulation mode. I can walk you through what these things mean. This is the simulator. It looks way better than the real brain waves, but at the bottom here, we do have a timeline in seconds and then we have the amplitudes, and then we have the channels here. So, what I'm doing is all that data is being passed from the node side via sockets. So, when I initiate the component, I start by basically initialising the Smoothie library and I'm listening to the socket events - in this case - just a string made from the node side of things. As I receive this, in this case, I believe it is an event, each like 32 milliseconds or so? No, actually, I'm buffering a lot on the back end, and I'm sending it, and then I'm reusing that buffer, so I think it is more like probably one second of aggregated data, but, yet it is real time, and it looks completely in sync. What we get back is this data object that contains the amplitudes, the timeline, everything I need in order to feel this chart. And, ultimately, I'm using a method that I created, basic, it is what what those lines are moving up and down what you see for each of the channels. It is important to think about the channels rear, right? And be mindful that everything you're doing is like basically at the channel level. So, ngAfterViewInit, think about it as a document function, and the view has a property element which is a reference to the DOM itself, and we are querying the DOM and looking for the time series ID which matches our canvas object. The best - that's how the connection is made between basically the DOM and JavaScript. Also, we have for each one of the lines, basically each one of the channels, eight iterations, we are adding a time series, and they contain a stroke which in this case is mapped to colours I have saved on the front end, so all the different colours are basically being referenced here on this iterator. These two functions are very similar, if you see them. One is pretty much adding the time series, so basically initialising the data stream for each channel, and the other one is appendant, so the one, the adding time series is called ones, and each time, it is, it is basically an append to the smoothie, and serious object. With the current time. So far. Ultimately -- that one component, it is destroyed moving to the DOM, we are removing the socket event listener, because otherwise it can be as lingering, and it can just build up, and use some of your memory there. So definitely it is recommended to remove listeners from the socket event as the component is being destroyed, let's say you're switching tabs, then you don't need to receive events from the other component. These are other things of the same chart, including, let's say, some of the channels, let's say this one. It is for the amplitude. I'm going into an I a ray of eight items, and for each one, I'm using data by naming Angular 2 and displaying the proper value. Angela 2 is great for this like Angular 1 and any other framework. Here, as you can see, this is basically a for-of loop. Angular here is closer to what JavaScript really is. And this is footer, the time value at the bottom. Great. So let me take a pause here and show you a little bit of other type of data that we have, other than the frequency line. We also have a frequency radar. That is basically the same frequency which will just display as a sphere, and we also have some bands, which is very interesting, because, in this type of data visualisation, we are seeing the amplitude of electricity for each one of the frequency bands. Who is here familiar with the different frequency bands like delta, theta, alpha? This is when it gets interesting, because you can start doing something with the information you're getting because right now, you're seeing data being displayed. But according to research, you know, if you're going to get some more let's say alpha as you're starting to get focused and meditate. You can train your brain and see some results from this. Funny story is that, at the beginning, I wanted to start by kind of like trying to measure something in our bodies, like I wanted to see if I can detect JavaScript with this application, like if someone is hungry. So I want to put the headset and see yes, you're hungry, not that hungry. I was reading tonnes of medical papers about research about the different bands and what they mean, and it's kind of like somewhere where I can understand how this is reflecting when it comes to electricity, because it is all in here, right? And I read, alpha should go up, delta should go down. That's not too hard, right? If I fast for let's say half a day or a day, I should be able to see those changes. In reality, I ended up being sup hungry, so I said I'm not doing this again, this is not fun, but this is definitely the part where things start getting interesting and start making sense. So also, the different channels, depending on where they are positioned, they're going to tell you a different story. For example, back here, we have our visual cortex which is processes like the vision, for example. So, if you want to conduct some experiments based on like vision or based on other parts of the brain, with some like cognitive, like data, you can definitely isolate the different channels and just base your experiment base on different parts of your brain. Which is very interesting to me. Awesome. So I want to show you real quick on this data visualisation part for the frequency bands, this one was slightly easier because I'm just using the third-party library based on charts. At the end of the okay, if you include the directive right from the library, you can use the component that the library provides, and with the API, you bind the properties to the data here on the right. On the left to the right. If you know the documentation for that library, you're going to parse the data the way it expects and pretty much how it works which is not a lot of work on the angular side. Of course, if you really want to get performing charts, you need to know exactly which one of these libraries are doing, and not only the Lang or 2 library but like the main data library is probably roll your own which is one of the things I might be doing soon. We talk about all of this in Angular, which is pretty exciting, but what else, right? What are we doing here? I broke down this product into three different phases. The first one is visualisation, which is what I've been talking about here today. The second one is more about experimentation, but in order to start working on experiments, you need to have visibility. That's why I took a step back. I started first on experimentation. I wanted to know what the data looks like. I wanted to learn at the amplitude level, and, lastly, hopefully some interpretation, right? What this means is that you can make sense of what is going on in here. And, actually, what everyone wants to know and wants to do. Everything is possible, right? Some of the things involved like controlling, right, IoT objects. A lot of possibilities. There's some research done and projects that involve like moving a wheelchair with your mind. The thing is, this is technology that just was made accessible to the open-source community like this, without having to be for a licence. It is all open-sourced and encouraged for everyone to give it a try. This project that I showed you here today, all the code is on GitHub. You can definitely like set it up locally, if you don't have a headset yet, you can run the simulation part. If you're passionate about it, you can get a headset. Definitely talk to me afterwards if you're interested, and I can put you in contact, you know, with open BCI. And get involved. Threw meet-ups all over the world. So, please check out the meet-up page, try to find the local meet-up, if you're interested. Go to the GitHub organisation where we have a lot of open source work. And also, you can go to open BCI where you can read about the community, what other people are doing. Thank you! . [APPLAUSE]. BRUCE: Tremendous stuff. Thank you. Lots of questions from the audience. What do you use that for at Netflix? ALEX: Okay. So I stamped at Netflix like a month ago, so, so far, I haven't used it for Netflix, but that's definitely part of the plan. I want to apply it in every area that I possibly can in my life. And Netflix is just an amazing ecosystem, and I can see a lot of folks -- BRUCE: Turning on the TV with your mind? ALEX: I want to wear the headset and for it to tell me what I want to watch, or control the UI, like this is me being a dreamer, but you have to dream big, right! BRUCE: You do. And if you're dreaming about it, a lot of the audience said can you give us some more real-world type things that this could be useful for? We've seen the person controlling the prosthetic arm, controlling a wheelchair? ALEX: Yes. The sensors that you saw on the headset, right, they don't only measure brain waves, they also measure some muscular electricity, and even like heart rate. You can wear this different sensors over your body. There is a lot of opportunity on the muscular set of things, right? It has been proven that just by thinking that you're moving your arm, you know, you're actually Trigg those nerves, and it actually behaves as if you were moving them. How you can see some people, you know, with amputees like actually controlling things with, like, devices that are created by humans. Funny story was I was at a conference, letting other people try the headset. A person came to me and said I want to try it but I have a big head right now. I said, "Okay, let's give it a try." Put it on. We could see that the channels 7 and will 8 back here was like derailed. There was tonnes of electricity compared to the rest of the head. Said the pain is like right here, but the amount of electricity was coming from back here. Put some ice on, said never feel better. Never guessed it would be here. For real-world examples, this is very early stages. I had to take a massive step back because it is not like, okay, there is an API, I can know what I want. I can Google, I can talk, all that stuff. We are waiting for you guys it to help me with that. That's why I'm here today. BRUCE: Can you put it on my head? ALEX: Yes! BRUCE: Don't look at what I'm thinking about! Thank you. How is it? Cool? [APPLAUSE]. Can I take it home? ALEX: No! That's how I have fun, man! BRUCE: Last question. How is the hardware built and extra points if it is 3D printed? ALEX: Can you say it again? BRUCE: How is this bit? 3D printers? ALEX: Yes, basically, it is printed by parts, and you have like all the little like nodes in like one plane and then you have the headset actually split in two in another kind of like, like four printers could be printing inside at the same time because different files are going to print that could print this in, let's say, for files that fit the normal printed home, to the printer, the affordable one. It might take probably up to 40 hours. Next generation of headsets, it is going to just get more simple, and easier. Again, tonnes of work. But, yes, you have to do the gluing, you have to do the sanding. There is a very nice GitHub tutorial about all this, and I also have some videos of the 3D printer printing it also. BRUCE: Excellent. Thank you. ALEX: There you go. [APPLAUSE]. BRUCE: Give it up for the man who genuinely reads minds! Alex Castillo. [APPLAUSE]. 