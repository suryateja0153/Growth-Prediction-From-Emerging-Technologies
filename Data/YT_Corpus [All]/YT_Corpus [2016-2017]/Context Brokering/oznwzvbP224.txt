 I'm going to try and talk to you a little bit about some of the complexities of managing and using life sciences data in this year and in some of the projects that we see coming forwards and I'm going to try and explain some of the joys of having a very complex data sets very complex type of standards public databases repositories and how that doesn't always help when you're trying to bring data together so first of all I'm going to look very quickly at whether or not we can really consider life sciences data in general as big data and then we're going to jump straight into the data sciences maze in the life sciences and throw in a few ideas on good practice as we go and very quickly if I have a few minutes left I'd like to introduce one large new project that is coming online at Imperial so why am I here and why am I not talking about a single big project will I run a core facility at Imperial we manage and help researchers in somewhere around 150 different research groups at any one time manage their life sciences and biomedical data so we in a microcosm see a lot of the issues that are associated with bringing together multiple types of life sciences data so first of all some characteristics of bio data in in terms of the general ideas of big data today so life sciences data have very little structure they grow very quickly but compared with the sort of data that coming out of the Square Kilometre Array they're not that big but as I'll show you in a moment they are getting larger very rapidly and they have very high heterogeneity many many different data types coming from many different instrumentation tie oops and the the types and the technology is a very unstable they're changing on a very rapid basis so we have many different file formats that we have to cope with wildly different sizes differing acquisition rates I'll show a few examples as I go through and there's still considerable manual data collection a lot of very important information hides in people's notebooks and if we try to integrate large data sets together we've got to be able to track that information as well otherwise we lose information so that's still a big problem in the life sciences generically so during the lifetime of a data set it will go through multiple format changes that's true of most types of data analysis but perhaps in some of our areas because the data types themselves are not terribly stable that becomes more of a problem if you're trying to integrate data together and we have a huge range of different analysis methods both computationally statistically and methodologically and they change rapidly too so we have many different metadata standards in the life sciences which is in some ways good and some ways adds to the complexity of what we try to do when we store and use and reuse data meaningfully and we have an additional issue of confidential patient data private data that increasingly has to be integrated at different levels with pure research data in order to actually make sense of the data sets that we are generating so what sort of data are we talking about what do I mean by data well sometimes we're talking about raw data files sometimes we're talking about analyze data files sometimes results which can go across many different data types many different files we need to store hypotheses and hypotheses are meaningless without the context in which they're formed we often have to store standard operating procedures sadly many of our researchers don't like generating them but they're very useful when you want to share and reuse data meaningfully and we also generate software as I'm sure most other people do as well we generate tools and regenerate interfaces unfortunately many researchers still don't believe that those are data assets themselves so they sometimes get a bit lost and we have a large amount of dark data so what's dark data so it's a miscellaneous additional data that isn't necessarily associated with any particular publication but it's still meaningful and reusable and we need to not lose track of that where we go so in terms of biological data sets these are a few of the different types of data sets and that are generated in any particular research lab one experiment may generate many of these different data sets or it may only generate one some laboratories and site specialize in generation of only one of these but many many researchers end up having to produce multiple different types of data sets that each have their own standards each have their own formats are associated with particular different instrumentation types and we need to be able to integrate those data together to form meaningful analyses more and more and that in itself is a major hurdle so going to talk about size for a moment size isn't everything obviously but when people talk about data size in the life sciences this is probably one of the most common graphs that gets put up we're talking about DNA sequence volume here so our ability to generate DNA sequence has got faster very rapidly it's got cheaper very rapidly so when I started do my PhD I spent three years generating sequence and I managed to generate about a thousand bases of sequence entire in during my entire PhD project nowadays you can sequence the entire genetic complement of a human in 27 hours and it will cost about a few thousand dollars so we are generating awful lot of DNA sequencing data we can generate it more rapidly and more cheaply than we actually analyze and store it so we have to consider very carefully how what we're going to do with all of this data and it also needs to be associated with sufficient experimental data that we can actually use it meaningfully with other data types as well so are we really producing big data well this paper at the top is a very interesting one it was published earlier this year where there'd been some basic comparisons between how quickly we are able to generate just DNA sequence information its volume and the projections on the instrumentation we have now where are we going to be in terms of volume now and in the next five years how does that compare with pure bulk astronomy data and the sort of data that Google are playing with and some of some of the information it's a little bit naive but generically speaking it would tend to suggest that life sciences data that we've always tended to think of as well actually we're very heterogeneous that were not terribly big is actually getting big in the in the in the global sense very rapidly so at the moment the word if we just looked at all of the DNA sequences that we know are being sold and that are currently functioning we should be able to generate about 35 petter bases of DNA sequence per year and that's without technology getting faster so we have a big data problem here and even one small lab can generate an awful lot of information so sequencing 170 human genomes is the sort of thing that many labs now do on a fairly regular basis and just that one experiment is able to produce 40 50 gigs of the publication level data the sort of data you need to send public repositories and if you need to store the raw data the raw data from that sort of size project is multiple gigs is a few terabytes in fact so that mounts are poorly quickly computationally some of the types of analysis were doing quite large as well as an example of a recent one that was done by a PhD student not that not that long ago so one set of data just to do some basic analysis took 215 CPU years generated 40 terabytes of storage requirements and that's just one experiment so it's getting big not all of it gets big but there are enough big examples and of course we're not just talking about sequence data we have other high-throughput technologies these are just a couple of them metabolomics data can be mass spec profiles can be NMR spectra and we can generate these sorts of datasets very very quickly instead of one University having one machine we can have one lab having 10 20 of these machines running pretty much 24 7 so you can generate data sets very quickly and they all have to be analyzed they will have to be stored high resolution light microscopy is another big growth area for data sets so instead of just building single images or singles edge stacks people are now building time force series that will run not just for seconds but for days were eventually days we're up to multiple minutes and in some cases hours already and the data just get larger and larger and larger as we go so we've talked about sighs I'm going to talk about complexity so as I've alluded to earlier we have many different data formats many different data types so even within one type of experimentation we have many different data formats so this set of acronyms at the top are just some of the standard file formats that are associated with DNA sequence data there's only a few of them there are about 20 and if you're going to analyze those data and keep the appropriate metadata to tell you how those data were generated and what to do with them that's quite a big challenge in itself because these file formats are not particularly standardized they change over time different software brings in new version so if you want to be interoperable with data you generated three years ago that can be quite a computational challenge another big problem in the life sciences is we've been very good at publishing data for a very long time and that's led to a huge amount of fragmentation so today there are over 1,500 different public repositories in use for different biological data types so if you want to track the different data types associated with a particular experiment that's a lot of fragmentation it's a lot of complication they all have their own formats as well and we have minimum reporting guidelines we have metadata standards initiatives currently there are over thirty four different data types so it's a big morass if you actually want to find data and make sure that you publish your data appropriately as well thank you so many many different repositories one project may require data submission in multiple places each has its own format each has its own standards and data submission is often a requirement for journal publication which is actually a good thing but um lifts leads to fragmentation it makes data very difficult to find and as we become more integrated in our outlook for life sciences and biomedical research we found ourselves having to go back to these public repositories more and more regularly and integrate new data with all data across all of these different formats and that's a that's a problem on top of that we have the clinical data problem some of the data we generate even if we want to make it publicly available can never be openly published so data that is identifiable human data has to be handled separately excuse me so the good news is now that we have a halfway house there are a number of public repositories where you can actually deposit anonymized or sued anonymized human data and it is available for research behind ethics guidelines so you can apply to a database to use or reuse data in an ethically approved manner for your own research so of course we also have lots of different funding bodies and they will have their own ideas about how we should share our data and we use it and we have many different data stages so this is really just saying as we go through any type of experiment the data types change as we go through the analysis sets and the data sizes change but we need to be very careful to decide what we keep and what we throw away what is reusable and what is not so just a reminder that a lot of the type of research we do requires going back to the data sets we already have using them feeding them into our hypotheses comparing our data with other people's data and we cannot do that if we cannot find the data reuse it and understand what the published data are and as we become more integrated in the types of research we do this becomes more and more important so this is part of the the data life cycle in a type of biological systems biology which has a cycle of hypothesis data generation model building model verification on experimental data and it goes round in a circle but as you can see at multiple stages we're bringing in published data and if those data aren't well annotated we can't do that we can't do it effectively or efficiently so I don't know how many of you have come across the fair principles hopefully most of you have they're not mine but they're just a reminder that this is something that we need to be aware of and in the life sciences we've had very good agreements internationally for large data generation objects for many years so the so called bermuda agreement excuse me came out the Human Genome Project and it's an agreement that publicly funded data should be made available as soon as it was generated even before it was analyzed it's been re ratified and improved and grown from the 2003 verification agreement but in general the life sciences are very good at sharing their data they're just not necessarily brilliant at doing it in a in a transparent manner shall we say so I don't really think I've got time to go through this and perhaps this is something that may be useful for some of the discussions and later on but some of the types of things we do I'm not quite sure why the Mac has just eaten a lot of my pictures but hey there are holes in how data are managed so the sort of things my group do our build bespoke data sets bespoke interfaces methods for storing pre-publication data to enable sharing within large consortium that can be anything from MRI scans to confocal microscopy images automated analysis systems that count cells count edges of cells in an automated fashion we work with mobile phone technology so ecologists in the field can go to Africa and can take geo tagged information about insects about parasites particular animals and upload them to databases we also use basic laboratory notebooks to try and encourage researchers to capture data and back it up rather than just write it into into paper notebooks and we work closely in trying to make reusable pipelines to encourage better capture of metadata reusable data reusable methodologies operating procedures that are automatically gathered so you contract the parameters of the analysis that you've done so you don't just end up with a with an empty set of results that you cannot reanalyze or reuse so I'm afraid I've got far too much information here so I'm not going to talk about I think any more of it cuz i will be murdered by these lovely ladies at the front the last thing i want to do before and thrown out of the door is say that there are a lot of very very good initiatives in the life sciences that are working very very hard to improve data reuse data reproducibility these are a few of them some of these are eu-funded project project some are horizon 2020 some are fp7 and then we have the research data lines and software sustainability Institute I hope most of you have come across and um I think that's probably it for now unless you've got another hour 