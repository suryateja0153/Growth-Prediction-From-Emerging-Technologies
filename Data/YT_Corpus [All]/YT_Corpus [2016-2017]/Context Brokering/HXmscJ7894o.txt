 good well good afternoon everybody my name's John Mitchell and I'm delighted to welcome you to the ODI for a very special discussion on a forthcoming evaluation synthesis on the international response to the Syria crisis now the report has been commissioned and managed by the I ahg which is a very influential UN interagency coalition coalition which will hear more about later and the report is going to be published very soon and everybody who has registered for this event will receive the link once it's ready so the vents being live-streamed are so I'd like to welcome all of our online participants I hope you can see me and hear me clearly and we look forward to hearing from you later when you have an opportunity to join in the discussion on the online chat room so all of you are whether your remote or in person here we'll have an opportunity to add to this debate by using the twitter hashtag which is hashtag syria learning so if I could ask you right politely to turn off your mobile phones or put them on silent I think that might be a good idea so thanks for that so I'll tell you how the events going to run will begin with Helen Wedgwood who's sitting on my right here who's going to give us some background and a short introduction to the ihe I athe that's a very difficult thing to say isn't it it comes with practice good and so helen is the director of the evaluation office for the World Food Programme and an I ahe steering group member I'm also delighted to say that sir she's a a member of the our lap steering committee so would take a hell and then we'll turn to James D'Arcy who's here on my left and James is that lead author of the report and I'm sure as he always does is going to give us a lucid and the only author as far as I know oh you're the only less other people of you right well I really couldn't say but I'm sure as always you'll give us a lucid and concise account of the key findings from this report now James of course has a huge amount of experience from Oxfam in the 1990s and he's now vice chair of oxfam gb trustees and of course he was also a head of the humanitarian policy group here at the ODI for several years and we know James very well and of course we're delighted to welcome him back we've got two panelists who are going to respond to this report and we're going to start with Jane cocking here on my right and Jane is currently deputy director for international humanitarian programs at oxfam gb if I got that right Jane okay so are you humanitarian stuff okay okay it's good enough like you're good enough guide okay good so James been with Oxfam since 1997 and she does I can assure you bring a huge amount of experience and wisdom to the table and she's also completed recently completed a review of oxfam's work in Syria and of course we know that Jane's participated in many ldap events over the years and are very happy that you can join us for this one too and our second panelist down on the left-hand side here's dr. Julia bets who also is a peer reviewer of the report and julia has a doctorate in social anthropology from the University of Oxford and she is an evaluation specialist and she's told me that she consults a very wide variety of humanitarian organizations and others and she's currently leading the government of Norway's evaluation of his assistance to the Syrian regional Crisis so please could you join me in welcoming our speakers thank you I think we're going to kick off now so we're going to go to a Helen all James yes let's go to heaven first to put this in perspective do I have to do anything to okay if it ain't broken there we go that's it significant mishap than that actually was it was just the microphone being broken and so let me just I'm just going to take a few minutes I just wanted to for those of you who are not deeply and probably far too closely and intimately engage with the interagency Standing Committee international humanitarian system architecture which which which in 2011 went through a process of launching a whole series of reforms to try and strengthen the accountability and the effectiveness of humanitarian action across across all the players and I wanted to just give you a quick little brief on on this so that you can better put into context the findings the institutional context at least and the findings that James is going to is going to present within this international interagency process since 2011 there has been a commitment to have an improved sort of project cycle management and when a system-wide emergency is is activated at what they call the level 3 the level 3 the third level that is or automatically triggers a requirement for evaluation this is this is new and it's part of the accountability and effectiveness initiatives what we have this little group as as John has introduced which is made up of the interagency who might own evaluation all the entities the heads of evaluation our form the membership of this steering group and we get to kind of discuss together and try to agree across the system okay what should get evaluated when should it get evaluated what is appropriate to be evaluated what is possible and feasible to be evaluated and so you might ask well if the system was supposed to trigger an evaluation within 15 months of the activating the system-wide emergency response why wasn't it done and I think the the literature that you've received for this seminar puts that as one of the questions that it notes that there wasn't a system-wide evaluation many of you will will recall that the the efforts of evaluating for instance the the Rwanda if it weren't a response evaluation triggered a huge amount of attention and effort to improve and address the systemic challenges the tsunami evaluation coalition took a different stance and a different approach and so the steering group kind of got together and in 2013 thought well which was when the when the system-wide evaluation of system-wide level three emergency was triggered it was called by the emergency response coordinator in January 2013 and we thought what there really are significant challenges to doing evaluation in this context of Syria first of all an extremely complex context in conflict the political the multiple mandates the multiple actors the cross-border the multiple dimensions of the crisis many traditional humanitarian actors but also others who are not that not the usual partners in humanitarian response the third critical element was that this was a of complexity was that this was continuing continuing the crisis was continuing it is not just a it wasn't a sudden onset and then back to normal so when the question becomes when is the appropriate time to do an evaluation and especially as we were trying to get in our evaluations to be more results-oriented rather than the process type of evaluations that had been done before and then of course the more usual issues in humanitarian context massive constraints in terms of access massive constraints in terms of data quantity of data available T of data so what did we do we decided to do something different and we set up this coordinated accountability and less and learning initiative the syriac or what was that trying to do the aim of it was to basically to recognize that all of the main humanitarian actors would probably have to do evaluations they what the need to evaluate despite the complexity despite the challenges were not was not going to go away but to try to support that process to try to make it a little bit more coherent and to try to incentivize the doing and the publication and and sharing of lessons coming through from evaluative work why because we believe that that doing so actually does contribute to knowledge and to sharing of lessons and hopefully ultimately to improve the response over the medium term at least so what did we do we set up a portal a repository which I'll nap has been has been very very skillfully and diligently hosting for the last two years at least more than two years and has a significant number of materials in it again to try to incentivize and share we did a common context analysis in fact we did one in 2014 and and again in 2015 which was trying to a reduce the burden you know don't don't reinvent the wheel the cut let's let's try and take expert expert analysis and build on that rather than then repeat the same work and to reduce the burden on the people who are responding the people who are at the you know actually delivering the humanitarian resistance and for whom evaluation often is seen as a as a as a burden we agreed a broad evaluation framework around a set of questions around which stuff we would wish that participants would use to frame their evaluative work and then we also committed at that time that we would do a an analysis a synthesis of what had been put into the public domain through the portal and that we would do that in time for hopefully contributing both to the immediate response improvement but also into the world humanitarian summit and hence and hence this work and hence this seminar today so for me that's really all that I wanted to do is just a frame this frame is where did this all come from and it's and it's been quite an intentional process it does raise from my for me and my colleagues on the on the interagency humanitarian evaluation the I ahe steering group you know some really significant questions the results of this analysis did we actually achieve our objectives did we actually generate sufficient knowledge evidence and important lessons that can contribute to the improved response and obviously James's findings are going to are going to be extremely important to us on what should we what should we do differently from now on this is a still deeply deeply complex conflict crisis and should we continue along this way or should we try to do do something else in terms of supporting lesson learning accountability for improved response so I'll leave it there and look forward to James's findings thank you Ellen thank you very much for that very helpful context and so this is this Syria cool initiative it is different to the Rwanda response evaluation it's different to the tsunami but it's quite similar to what we did after the the haiti earthquake I think we're there was also you know we tried to there was also portal there that was also a context analysis and I think there was a synthesis as well so obviously this situation in Syria is an unprecedented situation in terms of access and to to the people enter data and so let's hear what the synthesis report says James please thank you John Thank You Helen um let me start with just a few remarks about the nature of the exercise um it's not a definitive exercise it was based on what happened to be available in the public domain by way of evaluations or evaluate evaluation related studies i'll say a bit more about them it's a limited sample this this session is billed as being about lessons learned from the international response to syria and so most of our focus will be on the substance of this but there are also some lessons about how you gather evidence about evaluation practice and so on which we may want to we may want to discuss so essentially this was an attempt based on what was a publicly available to summarize some of the key findings and to organize those a little bit to look at the evidence base and what wasn't in the evaluations what gaps there were in the evidence base and as I say a bit of a reflection also on evaluation practice so I am I won't spend much time on this on the purpose purpose rider an overview of the results of learning and accountability f'd efforts undertaken thus far the focus is on the international response to the Syria crisis there are many other actors local actors governments and so on who have been he perhaps the key humanitarian actors in this context so one of the important questions is how well did the international system or the the various international bodies that have engaged coordinate with supplement add to those efforts there's actually not much about that in the evaluations unfortunately and it's one of the things that not having an overall evaluation means that we don't get that sort of bird's eye perspective and that's a real gap it's a real problem i'll say a bit more run that the scope of this the evaluations are they cover the response inside syria and they cover refugee responses most of the refugee related material is on Jordan and Lebanon there's relatively little on Turkey which is odd given that turkey actually hosts the 2.7 million more than any other country surprisingly little on that Iraq and Egypt we haven't included simply because there was enough material to make to make it useful and of course we haven't covered refugee responses in Europe its itself there are some other gaps the focus with regard to refugees is pretty much on ethnic Syrian refugees this there's not enough coverage of the situation of minorities particularly Karrde something your Palestinians Yazidi Turkmen and Christians so ethnic and religious groups to be able to say anything useful about that there is some material on this but surprisingly little in it it's a gap particularly as often those groups are the ones who have been most vulnerable I'll say just a word on methodology first of all this is clearly not a substitute for interagency evaluation nor how are the evaluations we looked at based on a common evaluative framework they haven't all asked the same questions and approach it in the same way they're very diverse and so you have to take that into account in interpreting the results of this it is an extremely diverse set of a report but nevertheless I think there is something useful to be gleaned from it but it means that there isn't a we're not able to do a meta-analysis and a more rigorous statistical sense on the available material the fact that they are publicly available tells you something and it probably tells you something about what they don't say you'll find that as we go along quite a lot of the gaps relate to things that are perhaps too sensitive to discuss in public or a perceived to be but there are questions about why there isn't more material actually after for four and a half years you might expect there to be more publicly available material than there is there are lots of studies lots of assessments and so on out there we've got a list of 300 plus but this is stuff that actually evaluates the responses but we suspect in fact know that a lot of the learning arising from this context is held within organisations and there's an interesting question about how that is always not shared with with others I won't say much about the reference material there's a range of different sorts of evaluations here some are multi-agency the interagency steering committee Standing Committee operational peer review a couple of donors strategic reviews the DC review that I was involved with some single agency reviews of a region-wide program and then some country or program specific reviews plus some that we felt it was important to include like the snap review that the review of the strip Syria needs analysis project looking at assessments review of the impact of the Security Council resolutions a protection meta-analysis and a review of pooled funds as you can see quite diverse so I'm not going to say much more about methodology I've said a word about it we're looking we were looking for patterns and recurrent or contrasting findings really to ask the question what does the available evidence tell us about specific topics and the response overall and we did some weighting of them of the material according to evidence strand if we if we can talk about that if you want if you want to but I'd rather get on to the substance so what we did was essentially goes through the 24 reports and look for common themes and came up with a list of about thirty that are clustered under seven headings and these are the headings so some are context related findings some are about strategy planning coordination leadership there's a there's a cluster on program delivery effectiveness coverage and quality there's one on protection and vulnerability and related material advocacy and principles it's something about targeting and about accountability and community engagement and mobilization there's some management related stuff for organizational related stuff on staffing partnerships and operational efficiency a lot of the management stuff is very organization specific citing quite hard to synthesize and condense so I've tried to draw out just that what appeared to be the common themes and then a final one on on on assessment monitoring evaluation how we understand how we've understood the context and use that understanding to inform responses or not so I will run through briefly some headlines from those seven headings those seven clusters context related findings we could talk a long time about this I I will assume that that the people here or online know broadly speaking about the the nature of the context we know that there has been a catastrophic decline inside Syria it has gone from a middle-income country to a country where at some two-thirds of the population about 12 million people need humanitarian assistance about 4.8 million people are hard to reach with humanitarian assistance and that that phrase covers all sorts of horrors that you you will be aware of there are about 4 million refugees and there are about 6.6 million internally displaced people um there's quite a lot of analysis of context in the reports about but it's often seen from the point of view operating context for international agencies fair enough that was often what what the mitu the evaluations was so there's quite a lot about access and and it's clear that humanitarian space in Syria has been deliberately restricted really quite consciously so by a whole range of means some some of the access constraints were security related but a lot frankly were bureaucratic to do with the denial of permits to do with roadblocks and so on when the political controlling authorities have wanted to block access they have and they've done it consistently there are also issues about capacity constraints in country and there's an interesting discussion that we might come on to about that most of the international assistance had to be routed through the Syrian Arab Red Crescent and and that's a constraint in some way certainly if it's different from the normal picture the question about finding partners who are reliable had capacity and how you work with them I think we will come on to in the late in the discussion so that's been the critical a limiting factor on a coverage inside Syria the protection context you'll be familiar with very direct and deliberate targeting of civilians and civil objects in Syria including the destruction of hospitals about half of which have been destroyed or seriously damaged essentially disregard by all sides of basic norms of humanitarian law in the refugee contexts an interesting situation where the 1951 Refugee Convention was not applicable bart refugees have been given temporary or de facto protection by the receiving countries you'll know about the burden that that is placed on the countries in question including on services but also in terms of competition for jobs and so on the point I wanted to make here was and it comes out from the evaluations is that that grant of de facto protection has been increasingly undermined by assistance deficits on the one hand but also the lack of access to effective livelihood opportunities and jobs in particular inability to access the formal economy really and that's become an increasingly problematic issue and and frankly has reduced the value of the protection offered and is probably we think linked to increasing refugee flows into Europe the second heading was strategy and planning coordination leadership I'll just make a few sort of headline remarks about this most of the reports note a problem related to a lack of overarching strategies that United the various strands of of programs reading the reports there are some fairly clear disconnects within the system and I want to come back to that subject of disconnects when I talk about gaps but I mean and it disconnects between individual agencies but also between for example UN agencies and international NGO partners there are a number of fault lines and they are pretty clear even from reading evaluations that weren't specifically evaluating that question it becomes clear various attempts were made to get something more comprehensive by way of regional strategic coherence so there was this attempt to create a comprehensive regional strategic framework it by ultra it by all accounts didn't really work but it did have some important outcomes the main one of which I think Dana be interested know if you agree main one that I picked up was the the spotlight that was put on national ownership and and and the role of all governments then the need to configure this international response in relation to that role which had sort of not been a focus of attention and and that and that process of trying to produce a comprehensive framework did highlight that and it did make a difference there's a thing called the whole of Syria approach was an attempt to join up the different hubs cross-border and from Damascus trying to work within Syria itself and that has had some traction its had its problems that it's about to be evaluated but anyway the overall point is from the available evidence is actually not possible I think to make any firm judgments about overall strategic adherence of the system except to say where some some of the probe lie and to some extent how they've been addressed most of the coordination analysis is dominated by a long-running confusional or tension between the role of UNHCR and that of a un autre and the role of the HC it's caused a lot of heat not much light it to some extent has now been resolved but one of the big things coming out of this is the need for clearer guidance on the respective roles of UNHCR and Archer in a massive refugee crisis of this kind with Archer of course leading it inside Syria we need some more guidance on this there are real lessons learned there's also been quite a lot of tension between individual agencies over roles who's who's leaning on education you know is it UNICEF is it UNHCR it took too long to sort out those turf battles and actually it's taken three or four years to conclude local memoranda over or letters of an understanding between these bodies to try and sort that out which is ridiculous there may be global memoranda understanding but doesn't seem to translate locally anyway I've said that coordination is now reported improved in most respects which i think is true third heading program delivery and effectiveness the first topic here preparedness actually surprisingly little on preparedness in the evaluations most of it is about the preparedness process in other words what measures were in place was there a contingency plan for this contingent so which is fine but it doesn't actually say well were those measures useful did they actually lead to swifter more appropriate responses but the lesson I take from reading material is actually the biggest single factor in preparedness is the ability of an organization to shift and to shift relatively quickly from one mode of operating to another most of these organizations had programs that were small the development focus they were policy oriented and suddenly they had to deliver these big operational responses that's really hard to do and and the proof in a way is in the in the evaluations it took a long time to change gear too long I I think it's fair to say effectiveness there's quite a lot on effectiveness it's largely evaluated against delivery of output which is of course is a rather limited description of effectiveness as opposed to the achievement of of objectives so it's rather thin analysis for the most part of evaluations quite positive about how agencies delivered against their targets but there's a problem here um the togas themselves kept shifting and partly because funding was so uncertain that everybody kept revising their targets and there were very few bass lines against which to measure progress against objectives so you've got a whole set of sets of uncertainties built into this process to actually very hard even for an individual agency I think to say anything very concrete or at least it appears so from the evaluations about really the effectiveness of their work that's particularly true in Syria for reasons that will come back to the thing I would highlight is that a lot of the target setting itself appeared to be capacity based rather than need space agency started off by saying well the need appears to be this and here's what we think we can kanna should do as UNICEF for Oxfam or whatever it was and then realized they couldn't do it and so they revised it and they revised it partly on based on just resource availability so you had this strange tension between a needs-based target setting and a capacity based target setting one of the evaluations it was a UNICEF regional one said well UNICEF said itself far too ambitious targets so and they didn't meet them so then they revise them down quite dramatically and lo and behold they got they scored a hundred percent you know result well sort of but not really because there's a written other was a serious problem about keeping a focus on the overarching need as opposed to what the organizational capacity to meet that need has been I went too much about the other criteria time lenders agencies were overall frankly rather slow to wake up to the scale of the crisis but there's lots underneath that to be to be talked about huge coverage gaps inside Syria we know about but actually we can't quantify and that's one of the problems fewer outside that outside gaps are mainly to do with refugees host in host communities and I think we again we might come back to that question of why that has been such a problem quality evaluation surprisingly little evaluation against standards which surprised me I thought that would be a fairly straightforward thing to try to do whether that's internal standards or its external ones like sphere just surprising a little little of that um there you go I will move on protection vulnerability advocacy humanitarian principles well the protection is always a difficult one to evaluate as his advocacy the general story arising from the material would seem to be certainly on the UN side that the UN protection efforts were not well joined up that there was a lack of coherence and that it tended to consist too much of if you like normative declarations and statements and too little by way of actionable commitments one interesting point that arises I think from the author evaluation is there appears to be quite limited incentives for the humanitarian coordinators and the humanitarian country teams to take bold decisive action the incentives are stacked the other way it's a high-risk strategy to go public both in career terms frankly and possibly in other ways as well and so I think there's an interesting question about about about incentives in here but there are lots of things to be said about protection and its limits in a context like this unhcr's role a generational by and large comes out with considerable credit on the protection size given that it didn't have a solid legal underpinning in this context but it had a difficult balancing act between trying to maintain a climate of acceptance of refugees and support for hosting them and on the other hand giving the host governments a hard time when I constraints to helping those refugees became inseparable advocacy arm isn't covered as much as it should be it's actually a major mode of operation committee agencies if that's the right word I want not mode of engagement there's a bit about advocacy strategy but not much there's very little sort of systematic analysis of the the the way in which advocacy was pursued and its impact very little i should say that's disappointing it may be the nature of advocacy that is always going to be hard to determine what contribution advocacy made to outcomes but there's not much attempt in the evaluations nor is there much attempt to evaluate against humanitarian principles and that really did surprise me it's something that we were discussing with Helen beforehand apparently this is actually quite common across evaluations worldwide in this context it seems extraordinary particular when you think about the principle of impartiality and how in the issues that have been raised in Syria and I'll come out to that in a minute some issues raised about the independence of UN agencies in relation to governments and whether they were simply too close to government and in order to maintain access and so on traded off too much by way of influence it's a difficult one to judge of course and we don't quite know what's happening behind the scenes again the evaluations don't really get into that in in a useful way fifth topic targeting accountability community engagement there's a reasonable amount on this in the material and some of it's quite useful the limitations of accountability in Syria are acknowledged but not much analyzed and I think it's it's rather thank it's rather taken for granted that it's just difficult and we couldn't achieve it but anyway I will I will skip over some of these things but I think my colleagues on the panel are going to talk a little bit more about them including social cohesion and refugees in those communities on the sixth topic again I'm not going to say much about this one staffing partnerships operational efficiency lots about staffing problems associated with organizational overstretch a lot of these organizations have been responding to multiple l3 emergency simultaneously and even though Syria has been relatively prioritized it's been really hard to find people to deploy often particularly at the sort of senior technical and management level so in un terms the kind of p5 and and and well particularly around p4 p5 level I think but I think all IDI NGOs have also struggled with this I won't I think say more about that page except to say that there isn't any real analysis of the part of the the efficiency and the effectiveness of this you ni NGO local NGO partnership chain again the evaluations were not tasked to look at that but it's a big gap in the analysis which I will show a bit more about in the last 15 seconds that I have sorry John and again I'll skip lightly over the assessment one assessment got better over time in Syria but it took a long time to till nineteen forty at nineteen forty try that one again took 2014 to get anything like a joined-up picture of needs and even then even now actually even with the whole of Syria process it's still not very well joined up there was some bold attempt by particular by the NGOs and a caps and others to do this but but as it says here large amounts of assistants are being delivered inside Syria with very light independent monitoring based on incomplete or non-existent assessment analysis lots of accountability issues related to that and and and and less excusable the same is true in in some aspects of the refugee host responses so now i really i'm going to skip quickly forgive me John I just need another couple of minutes the most significant evaluation gap I've alluded some of them the analysis of the response inside Syria is very thin I know that there's a lot of material that is organization specific on this and organizations have been reflecting at least some of them quite deeply about this but we don't have access to that material publicly I think that struck me was a very little engagement with this question of do no harm impartiality risk management in the context delivering large amounts of aid in a context like Syria where we know that the warring parties have been frankly manipulating the way aid is provided where it goes and so on we know that and yet the question doesn't appear in the evaluations it seems extraordinary to me again I'm quite sure there's a lot of analysis that isn't in the public reports almost no analysis of the funding shortfall problem beside and understand this routinely the appeals have been forty to fifty percent underfunded that's half and yet the evaluations don't say well as a result of that the following didn't happen there's nothing is they take the programs as given and then they evaluate against those well fair enough but what who didn't get assistance because of that and what and I find that extraordinary relates to this question of a needs-based versus capacity based target setting but it's really striking when you read across the evaluations as very little analysis of it the system disconnects let me just say a word on this and then i'll shut up them there's a sort of assumption that the common planning process the the RR r p3 RP you may know the acronyms the sharp is what drives the response and of course it isn't really and it's very clear when you read the evaluations that is at the whole set of organizational factors mandates fair enough capacities competition for resources frankly that are really driving this but because we haven't got an overview evaluation we we've got no real sense of how those are done the dynamics are playing out but it shows that some of the tensions between agencies that I've alluded to and also in the obvious gap in the way that the UN agencies and the international NGOs have viewed their roles and I will simply click through a list of other significant evaluation gaps I will show you a slide that says overall conclusions that really isn't a conclusion except that we lack that system-wide analysis I've said here some acid system worked well highly compromised inside Syria sometimes slow to respond elsewhere and hampered by confusion over leadership it's been undermined by week or late international political engagement and massively affected by chronic underfunding I'll leave it there Don I've got some other slides that I won't show now on issues arising but if they come up in discussion we can look at James thank you very much for a very very comprehensive presentation there huge amount be she's covered loads of methodological issues around evaluation standards and gaps and so on a whole range of issues around protection and advocacy Human Rights etc and then loads more operational issues the usual things I think we come across time and time again needs assessments standards incentives advocacy coordination and so on so forth Jane what do you make of it all John oh thank you James very much indeed for a very comprehensive summary and as you were talking two things came to mind about half the things you you mentioned I thought oh yes I recognize that the other half I thought our yes but and I think that's what you would perhaps like me to draw out so I will resist the temptation of going through your whole very comprehensive presentation but perhaps just try and draw out four points very briefly the first one is as so many times on occasions like this I'm struck by how much is the same as so many crises that we have all engaged in but then how much is different and I think it's helpful as a backdrop to to perhaps just sketch out very briefly which fall into what falls into those two different categories so much the same the high levels of politicization the players involved by proxy the difficulties of access both physical and political the huge protection crisis that this represents the funding shortfalls I would say that the answer to your question funding shortfall so what is tied almost exclusively to that point of access I think that plans are made and targets are set on the basis of an understanding or a belief or sometimes to be blunt best guess of what is needed but then what is physically possible to deliver is then something which is very different and I think that's an interesting point and then the staffing piece as well absolutely it's it's difficult what's different the middle-income environment the Europe's doorstep peace cannot be understated but where and again I put this onto the into the discussion because I think it might help us to tease out some of those issues and say well what have we learnt and what can we draw from previous evaluations and learning with a benefit of hindsight and time is I and colleagues of my generation have been reminded time and time again in in recent months of the parallels with the crises in the Balkans in in in the late 1990s and it's quite frightening I think how little we are taking that into account there are many parallels that I think we could learn from and I think that's something that the operational community if you like could would be very wise to ask the the evaluation community if you're not earn if you don't mind being characterized as such to help with I think that would be draw that parallel very much the challenges to the development of strategy and and what we actually do the access to information is is is obvious for all sorts of reasons but i think what comes through very strongly for us is the diversity of ways of working both within the countries surrounding syria and within Syria cross-border very very remote management operational semi operational working through Sark not working through thought I think it's rare that so many crises are characterized by so many different ways of working and that gives a real challenge to coherence and I think we are perhaps not being sufficiently open about the remoteness of a lot of our management on the other hand I have seen some of the most extraordinarily impressive and innovative ways of attempting to evaluate against quality standards either use of social media the use of photographs the use of digital fingerprints on photographs to double-check I mean some of it is absolutely awe-inspiring but that's one of my yes but is I'm not in the least bit surprised that there's very little evidence of quality standard monitoring and if there is strong if there's any anywhere then think it would be very valuable to to share it because I think that is primarily what is what is getting in the way the operational difficulties that I was surprised the one thing that didn't come through particularly in and I'd be interested to know if it was if it again falls into that category of sort of rather tricky issues that we don't talk about in public but we certainly talked about in private which I completely recognize that point I know that there are many issues that we debate within Oxfam that would not reach the public domain and some of that's legitimate because it is very clearly related to starve security and organizational identity some of it is perhaps less legitimate but I'm surprised that the issues of sanctions and counterterrorism don't come through more clearly just not there no and and yet anybody who has tried to run or try to set up a banking relationship for a program run from Damascus will tell you how difficult that is so that that's a gap I think in in things and then finally and this is partly a very deeply felt point on on my part and I believe auxonne colleagues but also I think it might be a helpful link to another event which took place here on Tuesday the launch of of the HPG time to let go paper which is the extent to which humanitarian principles are used not only as a sort of within evaluation but also as a backdrop and a filter with in decision-making and planning and I think that's something that we are within Oxfam coming to a position on much more clearly that we think we can use impartiality independence and neutrality as ways of program design but then also measuring our effectiveness and that does not mean and that means accepting some pretty tough truth the one reference I'll make to the piece of work that I'm just in the process of completing for oxfam asked the question is oxygen being impartial in Syria and if we take a purist definition on of of impartiality and what that requires in terms of practical action that you have at a given time an overall assessment of all of the humanitarian needs where they are what drives them and so on then nobody can possibly sign on the dotted line however are we at a community level are we to an organizational level are we at the level of the international community as a whole behaving in a way which delivers the levels of impartiality which are possible and desirable I think that is a much more positive answer that's just one example you know taking independence independence isn't who you sign documents with its it's who influences what you do and I think there are so much deeper questions that again I would agree with you it's surprising that they don't come through in the evaluations then it would be very helpful if perhaps in in the debate that follows this conversation we could we could take those forward rather more clearly so those are some thoughts on what was a very helpful and thought-provoking presentation thank you James thank you very much for those thoughts and also very thought-provoking for thoughts indeed i'm going to turn to Julia now how did the report strike you Julia well it's it's fairly difficult to know how to respond to such a broad ranging and comprehensive report so I'm just going to zero in on a couple of areas and I say this in the light of currently completing a bilateral donors evaluation of its assistance to the Syria crisis so it's just a couple of thoughts that are very resonant in my mind at the moment but I actually want to start with the call itself so James you flagged the the diversity of the real and how much was probably hidden rather than transparent but actually I think that having 24 evaluative reports in the public domain in one space where anybody can access them is quite an achievement and commissioning this synthesis as well it's quite an achievement and it's a step change actually from several years ago yeah so I think this is a very positive thing and I you know I commend the IHC for for doing this i think it's beneficial and useful and maybe a bit more substantively i'll just make three substantive points and then i'll beg your indulgence and talk about evaluation just for one moment one of the things that really came out to me from the report was the cross-border work very difficult to do and there's surely a lot to learn from this the very delicate balancing act of the humanitarian imperative versus the very intensive political complexities how do we manage that how do we manage these things remotely and in parallel the very high-level political processes and the use of information and evidence and how do we present that and communicate to influence at Security Council level so I think there's a lot of richness there that could be could be brought out the second point I want to make really is about the issue you flagged James of social cohesion and what's become the resilience discourse in relation to the Syrian crisis and I think that's a very interesting area that we could explore not just from the operational aspects but from the political economy aspects to you flag chain just how differently in each of the three countries the host governments have managed the response and these are all and if you talk to interlocutors in government of Lebanon or government of Turkey you get very very different responses to how they've engaged with the crisis so I think the issue really is what does social cohesion mean in these different environments and how are we defining that how we therefore intending to approach it and apply it and what models are being used and again I think there's some valuable and useful lessons in the individual reports that can be drawn out the final substantive issue I would flag is the control environment and anybody who's worked operationally is very familiar with this the really difficult balance of wanting to respond to the humanitarian imperative devolve as much power as possible engage with local service deliverers and so on whilst meeting very difficult accountability requirements in terms of delivery financial management etc how do we define that balance and what is the tipping point essentially how much risk as do agencies our agency's willing to absorb I think that's something that would merit some further exploration my final point really is I guess it's a return to my home turf and it's about evaluation so James you mentioned and you've flagged very extensively the diversity of material and how all these evaluations have different purposes audiences intense approaches etc and they're all conducted under very difficult conditions let's let's not forget but as an evaluator myself I guess the question I ask is below the structure that Helens flagged to us are we as evaluator serving the needs of our community in relation to this crisis effectively and I just have three little points I want to make which I guess our personal but will maybe cause some reflection there are some clear gaps as James has flagged in the coverage the I hp's do no harm approaches risk in all its dimensions gender efficiency social cohesion and resilience and transition planning and I would add to that political economy factors as well so are we doing the best we can do is evaluate us to serve our community if we don't press to address these issues the second point I want to make is we have a nun lap has helpfully helped us define the humanitarian evaluation criteria but do we unpack those enough when we go to these operating contexts do we try and understand what effectiveness means in this environment and James you flagged the low funding levels and the effects that has on targets how do we then define effectiveness for an agency in that context how do we define coherence in a situation where the international response is not fully joined up so I wonder if we couldn't do a little bit better at saying what these criteria mean and therefore what specific questions we should be asking in a in a particular environment my last point really is that you know the fields come a long way in the last 10 years and we've got now pretty good tools and approaches I just wonder if we apply them all ways to best effect do we as evaluate as always justify the approaches that we use do we think through enough how we need to do this study and I wonder there if I think in every evaluation I've ever done the report is a useful architect artifact for sure but actually the process is often the most valuable thing and I wonder if we pay enough attention to process when we're doing these studies studies and if the methods that we use to unfold the process are given enough thought from the start I'll stop there thank you thank you very much thank you for being so concise and thank you also for drawing our attention to the evaluation issues that you are identified we're going to open up the discussion now to the floor and to our online participants before before we do I just wanted to say one thing no as preparation for this meeting partly are I I spoke to some Red Cross colleagues a couple of days ago who just come back from Homs and after the discussion are actually I felt really rather shocked by the things they had told me I won't go into the detail here but it really there was a very vivid and graphic description of the immense human suffering that's going on in in Syria and so I'm wanting just to duel the panelists attention to this fact that we you know we're talking about a best study here underlying that is some are some terrible things James mention them as well and so I'm going to come back to each of the panelists at the end here and just ask you one thing that we can do better now and in the future to try and make these operations a little bit more effective and successful so ladies and gentlemen please I can see already mr. Bolton John Walton please go ahead I should say please when you have a question do say who you are and who you represent I've already said who you are John so maybe you'd like to say who you represent I guess you isn't it you see yourself 