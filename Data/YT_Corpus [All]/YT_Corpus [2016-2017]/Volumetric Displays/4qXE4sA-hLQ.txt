 no film schools coverage of nab is brought to you by my road real the world's largest short film competition is back shutterstock your source for stunning HD and 4k footage plus high quality music Blackmagic Design amazing solutions for film post-production and television hey I'm Ryan with no film school we are here with Lytro whose presentation at NAB 2016 was standing room only it is a light-filled photography motion picture camera could potentially change cinematography as we know it so we're here with John the head of light-filled video I don't even know if you call it video at this point we're going to find out the scoop on this get a demo and go through this seven hundred and fifty five megapixel cinema camera John thanks for being with us thank you really appreciate your time so why don't we just go ahead and start off with what it means to capture with a light field the way that we always think of a light field is that in traditional photography you're capturing the color and the intensity of light on one flat imaging surface with a light field you're actually capturing the discrete angular information or those actual rays of light so what we're showing here is actually content that comes off of the camera where you're now able to take those rays of light project them into space because you can effectively retrace the light back into the real world the easiest way to think of what you get with a light field camera ortholite row cinema is that is a digital holographic representation of everything that's in front of the camera with light field you're actually taking the raw discrete rays of light so Papa Klee it's as if you have the ability to change these decisions as part of the software process and it's optically perfect in terms of those changes so it's not a simulation is not a depth effect it's actually re retracing every ray into space like you had a CG render and you're actually using some kind of a ray tracing methodology so this is the actual electric cinema technology this is what we showed on Tuesday it is the highest resolution video sensor ever designed if you know of one that's higher please let me know in the technology we have the micro lens architecture that gives you quite literally millions of lenses to do this holographic image the camera is capable of capturing up to 300 frames per second it also is able to achieve 16 stops of dynamic range so we like to really highlight that that the architecture allows you to gain all of these additional benefits in addition to providing exactly what you would expect for industry standard image quality have you even think about sensor size obviously it's a it's that's a traditional thing where people think about super 35 and right right what's the electro approach there so we are a super super super super 35 it is a very non standard optical format the first generation of technology the image sensor is quite literally larger than 1/2 of a meter wide so we have very different optics we have very different methodologies in order to collect that much light but you have the real-world points they go through a main lens and that is the lens that you'll see in front of the actual electro Cinema Camera and then behind that we actually have millions of micro lenses so with those millions of micro lenses you're actually taking the light that would have been previously focused at that imaging surface and you're breaking them into this core angular components so now you have InFocus rays that project back out into the real world and that allows you to with the computational back end where you process all the information back into basically where the original 2d image would have been focused that's the easiest way to think about a life field if that's easy in any way no no it's not object but but I guess one of the first questions that I would have from a technical standpoint how much are you able to differentiate the camera and move the camera in this space because you're still just getting you know no matter how many millions of micro lenses you have there's still a perspective from which you've chosen to put the camera right absolutely correct so actually if you look at the lens in this diagram the one the main lens at the front the left to right or the top to bottom distance actually defines the parallax that you can generate so you have now that's called the entrance people of the lens the entrance people in this centum system is approximately 100 millimeters so if a true optical shift capability of a full hundred millimeters so the electro cinema technology is all about the frustum or the actual field of view from the camera it's not about the vr type immersive experience like the other technologies it's all about taking that density of resolution in giving the filmmakers the image quality that's required in order to produce a cinema level film which is an important distinction because it's still the intent of the director and the cinematographer and where they place the camera you're still there there are adjustments but it's not like the studio could come in later and say oh you know I went for this shot we want to be on the other side of the room because we wanted to see someone's backside or something it's still your creative intents correct unless of course you shoot with two of these they'll be true of traditional 2d cameras as well so you can get the impossible shot the things that you can do they're fundamentally just not possible traditional optics and on the film life that was directed by robert stromberg and it was cinematographer David Stumpf what we're able to do is literally bring them into the actual editorial suite and go through shot by shot within the actual EDL and make creative changes so we're able to provide them on set with a real-time QuickTime Pro res file or an MXF file depending on what the workflow actually is and that goes immediately over to the editorial department they make creative decisions based upon the actual cuts of what's going to the film we sync the EDL directly into nuke studio and then we can actually manipulate the light field directly so we're going to go ahead and actually launch this so when you look at this we actually have all of the metadata embedded here into the tool when we create the file it's actually in EXR file so it's kind of a standard open source file format and then as you look through all of the tabs here you actually have all of the metadata that was captured on set embedded directly into the file so you can't lose the logs it's a direct it's the physical focal length it's a physical everything a bounce the captured scene is now embedded directly into the metadata and that's required for light field projection that's why all that information is exposed here for you so here what you're looking at is the ability to modify the light field if your little focal point let's say I want to change my depth of field I can then apply that so now you can see you have shallow depth of field where the ball is in focus and the boy falls out of focus I can then also move my focal point anywhere arbitrarily in the scene and you can see here that your aperture dates you have your focus distance that is an exact measurement in real-world coordinates for everything is within your actual field of view right now you have the equivalent aperture of an f1 point one lens but then we can go we'll above and beyond that because again we have an exact understanding of where that point is in space so let's say for example I want to create an F point five lens or I want to create an F point three aperture lens or point two or keep going until if you see here we're going a little crazy where you have a depth of field that is so shallow that literally it's within less than a millimeter of actual focus like for a physical lids we would basically consider this to be impossible that is 100% impossible yeah so the ability to do this is actually liberating from a cinematography standpoint because you're able to do things that had previously were up to clean possible and from a focus standpoint you could you couldn't focus this shot with any human because the depth of field is so shallow any subtle movement would require just some kind of a machine precision to do it there's no filter or tilt shift lens or anything that would let you do that actual thing that's correct that's correct now we also can do tilt shift we can do anything that you want you can move your sensor virtually in any any coordinate any direction all those tools are right here things you tilt and shift and everything that you would like so this is basically the general interface this is the engineering design as we get into the commercial release it'll look a little prettier a little more easy to use the other thing that we're able to do here is stereoscopic creation and it's true stereo 3d native capture so it's very important to distinguish it from any 2d 3d conversion or from some kind of a beam splitter rig that it's one lens one one sensor but because we can move the camera dynamically in space you have exact and control from zero interaxial all the way through 104 millimeters of separation in our eyes are physically less than 100 millimeters apart that's correct and traditional cinema production they're anywhere between 20 to 40 millimeters but what you can do here is quite literally I can just click a little button here I can set one camera to this position I can set another camera to the opposite side you're now seeing the equivalent of a 48 millimetre interacts separation this is 100% accurate too as if I made this exact decision at the time of capture but now you have the ability to animate these parameters you have the ability to generate any stereo shape any stereo volume that you would want so the Deaf screen is something that gives you the capability of virtualizing your entire world without actually having to do very complex green screens in green screens are great they let you do compositing and virtual eyes that set but obviously the set of time and the lighting is very challenging so what you can do with the actual depth screen is just by virtue of capturing with the light field you can extract out the background so this is the final composite and clearly this is not supposed to look photo-real this is just a matte painting that Robert the director of the film hand-painted and it fits within the storyline of the actual content itself and then what you do here is if you see this is actually quite literally shot in the parking lot for the soundstage it literally took the camera outside which aimed it right in the parking lot we were then allowed to have the actors perform in this environment without being restricted by the green screen it was giving the giving Robert the director much easier ability to have them interact with that space so with this traditionally this is a very complex key if you note all the hair and all that detail and how would you actually extract this out without the green-screen what we can do is perform the depth extraction process so here seeing this is the depth estimation and from that point you do a rough estimation and from there you actually perform basically you you calculate how all those rays of light are going through the light film you can extract out the background and foreground separately so this is how you're able to perform the final composite so you can see that I can easily dynamically scrub through the volume and the volume here what you're trying to do is just create that slice in space it's giving the algorithm a very rough estimation of where it is pulling the actual rays of light from so once you do that you can also make a different decision if you want to have the girl removed from the foreground and it's really just that quick here and then once you get to this place then you calculate the final alpha estimation which something like this so now you have all that fine detail all the all the fish netting from the Hat and it is basically just this refinement process that allows you to do this extraction but now I have literally millions of rays of lights to assess that gives you not only the depth information but the color the intensity and everything else about the light field so it is looking at the volume the volume is what dictates how to extract out this this very rich and dense information you've basically created like if someone were to think about Photoshop layers you're giving the ability to turn one on or off there are more layers than just the subject and the green screen but everything in between you could set a range that's exactly right that's exactly right so from that point then you can go into your final composite you now have the virtual world here where you have the basically the foreground couple with the virtual matte painting in the background and you can do the same thing by refocusing it just like you would do if it was a live-action play you can see here that is D focused and then I can actually do focus them and have the the matte painting in focus this is again in that processing mode then you do the full raytrace render and all your edges and everything will look perfect and sharp and beautiful one of the most important of these parameters is the ability to control your frame rate and your shuttering so in traditional photography you make certain decisions based upon that the fundamental rules of the exposure triangle those rules kind of start to go away I'm sorry what we are able to do now is you can control all of that through something that we call a volumetric flow vector so we generate that is almost like a fluid simulation of how the light rays not only go through the lens but how they travel over time we have a very exacting understanding of how all that is accomplished which gives us full control over those variables shutters so you can use that for distribution so again you can release let's say a 24 frames per second with the correct shutter angle or motion blur for that scene or for a high frame or for broadcast or anything in between or it can use it creatively so on this particular sequence as I playback here you see this is the original shot you see it was shot on this not very attractive and not very useful JCPenney looking photo backdrop and there are practical lights flashing and doing horrible horrible things that if you're trying to do any keys just fun they don't work and we do the depth extraction so you can see here the rough key that goes into the final actual extraction or basically the the depth screen of that element and then from here we integrate all the matte paintings all the CG elements they're converted into 4d light fields to perform the final refocus and then we can control the motion blur independent we can also go beyond the actual capabilities of the sensor right now you're seeing the equivalent right there of a 1440 degrees shutter angle since we're looking at soldiers here basically if Steven Spielberg decided after the fact that he wanted Saving Private Ryan to have that very distinct shutter look that it had they could have made that decision not only beforehand as they did but with the Lytro camera after the fact that's exactly right so you could have switched to in that case about a 90 degree shutter angle to make that nice hard straw be a very gritty look or he could have inter cut multiple different aesthetic looks if you wanted to jump to that and then a different moment where you wanted to have the the more motion blur and more that dreamlike State which is what we did in this particular shot one additional thing that we like to highlight very quickly is the ability to take all of your CG assets and render them as a 40 light field but the idea is that you're actually getting now the full plenoptic 4d light field which allows you them to do the final composite in 4d so with this now that you can see if you note here that this is now a 4d light fill the little lens lets that is actually identical to the structure that's created optically within the camera that is all part of the plugins that we provide inside a new core inside of whatever the 3d software is so just going into here and you're able to see that now I can refocus on this shot where you have the background all in focus now or because it is a 40 element I'm able to defo 'kiss that I can also and this is just kind of a little key note here let's say I throw this entire scene out of focus completely let's now go and actually change the lens parameters so right now I have 12 blades to my actual aperture let's say I want to do a three bladed aperture not to say you want to do this but as i zoom in here you see you actually have a triangular aperture in terms of the bokeh of the scene and then I can also then change that to be anamorphic if I want so this is the wedding sequence which is a extraordinarily challenging scene and it may not look challenging when you're looking at this final composite but it is the result of multiple plates multiple light field capture and a lot of CG renders that are integrated into this one final resulting image and this is the final sequence as it was composited what you're seeing is this is actually live-action confetti that was depth screened from one take of the actual film and then it was recompose it over a second take you also then have a light field silhouette plate you have a virtual matte painting for the environment and then you have this set extension here which is basically virtual grass and a virtual life line that's part of the story that was told in this project so showing how this was actually accomplished with the light field you're actually able to generate an automated 3d camera track of the world and it provides you with sub pixel accuracy of that virtual position so what you're looking at here is the original plate that was captured and we like to show this because it is quite literally one of the worst things that you could ever do for your scene so we had so the director and SEM tiger for a Robert David had a lot of fun that they choreographed the grips to walk behind the set the camera is literally just moving on bare concrete so you see that it is highly unstable and then they actually surprised us with a confetti bomb which the guy who's launching is in the top right here he's actually in the frame he launches a confetti bomb and if you were to use this with any other camera tracker or any other types of algorithms they just yeah if you look in this representation here you see these little red cross hatches this is showing you just some of the features that are being tracked in the scene automatically and it's giving you this down here on the bottom left the actual camera that's being computed fundamentally the one thing to take out of this is that what we're able to do is extract out the exact position of the camera as it travels in space and over time that once you have that camera information then it is a rather easy task to perform the depth screen and then Rica posit so this is live-action confetti the first take is on the left the second take is on the right so what they wanted to do with the scene is take the element from the confetti and place it on top of the better performance on the non confetti tick so when you watch what we're able to do you perform that depth extraction you overlay it back on top of the second scene and then because you have the camera as it was tracked in time and space you can reproject both into the real world so that they are locked into the actual coordinate that they were captured if you didn't have that bit of two completely different motions that would not actually composite well at all from here we can actually also then show what we're able to do for rotoscope so in this scene realistically we use this on the feet because the feet from a depth key standpoint are touching the actual ground so you still need to separate that from the ground itself but we like to show this here they even with the confetti the artist goes in they lay down a spline on the first frame or whatever frame that they that they select and then you can automatically track that over time as well from this you're able to see how we're enabled able to create the depth P the depth extraction this is the estimation process we then extract out the the couple that is then placed on its own projection path with its own camera track you have a separate camera track for all of the confetti you also have a separate camera for the silhouettes that are then integrated into the shot you then finally have a completely new virtual camera that has all of its own intrinsic properties and that allows you to rephotographed the scene and complete the final composite this is how the final composite looks when you place everything back together so the one element that we like to show here is the fact that everything you see here is actually a full light field element so even though you have the set extension for the grass layer even though you have this virtual painting for the background when you actually composite you're using your same merge notes the same operations that you would for a traditional 2d composite but you're working with the actual 4d light field you can then also completely stabilize your camera so if you look at the original take that you see here and it does kind of fly through a couple of frames you can see clearly the camera is not moving in a very smooth path it's all over the place you can see a counter right what you now can do here however is create a completely new camera and reproject your rays of light through the lens and optically stabilized your scene so when I say optically that is a true optical stabilization as if your camera was smooth at the time of capture as opposed to making a 2d tracking point or whatever you're doing for corner pinning when you would introduce motion blur artifact a still be able to tell that you would stabilized it traditional camera that's correct that's correct so on a traditional camera when you do that you would actually have baked in incorrect parallax so here you're able to see that you are just moving the camera on Z now and you're creating essentially a completely virtualized camera it is not just any one components an entire ecosystem so it comes with the camera it also comes with the backend server architecture allow you to stream the data in addition to process and store we also showed on Tuesday our entire cloud ecosystem working in collaboration with both Google and the foundry in order to make that a reality and the thing that the cloud allows you to do is because we're working with such large datasets you can upload the raw files you can then process you can interact with because we have the software running actually on the cloud compute environment and then not have to transfer the data back and forth between multiple facilities when you say large data files what are we talking here well it is a 755 megapixels sensor so depending on bit depth depending on frame rate each frame when compressed is in the 650 megabyte range so you have for every one output pixel literally hundreds of input pixels so that's something that we like to highlight as the main advantage of the technology is that you have so much additional metadata that you're seeing you can do things for visual effects and for the actual composition that are previously just not possible to do after the fact and you said you're working with Google and foundry is this something that editors and compositors they have to go out and learn an entirely new piece of software are you working with existing software platforms a fantastic question because that's my the third thing that the system comes with is actually plugins to existing industry standard software applications the first launch partner that we were working with is actually nuke and in the future we'll refine that API to integrate into additional software applications with the light field you're actually able to take the content and then re render it to optimize for any distribution outlet or format so if you want to do a high frame rate cinema release you can do that and then rerender it in a separate way for either broadcast which has a very different frame rate or you can then rerender it for or stereo scopic depending on if it's IMAX or Realty or really anything and everything in between but the idea of having a full 4d composite allows you to have the ultimate light-filled master so this is something that is quite unique to light-filled that we also provide the tools to take all your visual effects assets convert them into a blade field finally your composite and then that way if you want to make a decision change or a distribution change you don't have to go back through post and visual effects right now you have 100 millimetres of the ability to move it it seems like with virtual reality and video game applications there's no limit to how much you could potentially change things as the technology progresses what is light rose take on that so for the electro cinema technology it's all about the cameras field of view and it's all about making sure that the decision that was made at the time of capture is as optimal for the Edit or for the scene as possible but everything that ledger does is all about capturing the volume of the world so we also have the ledge roll emerge technology which is a very different hardware design and that's all about that immersive 360-degree experience in order to do they have a very different optical construct and the that our take on that is that we want to exist in this volumetric space we want to capture the volume we want to be able to move around that volume seamlessly so the idea here is that as we start to refine and work with industry experts on this technology we may find that we start using multiple of these cameras for every single shot on a film we've already had a lot of requests for even even for electro cinema can they put that with ten of these in a big radial array and capture the full volume in 360 and absolutely we can globally trigger the entire system simultaneously and this is same thing for electro merge you could have a whole bunch of these units all around a room and capture that full environment is the idea to eventually democratize this and make this more available to independent filmmakers into filmmakers of all levels throughout yes our goal ultimately is to have light field imaging become the industry standard for capture but there's a long journey to get there so right now this is the first generation like any first generation technology that's truly on the bleeding edge of what's possible it is a larger camera but that allows us to capture all of those rays of light and as we move forward we're already working towards the design that becomes a completely handheld battery operated and untethered system so it's just like computers where they started off as the size of a room and now you have an iPhone and it's more powerful your pocket that's exactly right the ultimate goal is to have it be the same form factor that you would be accustomed to for cinema production and that's something that we're already actively working towards it is a very complex development but it is something that will happen sooner than later so it is really unlocking a potential that when you start having this become the same form factor as a traditional camera is really giving so much more control that it will make the actual final product that much better well I will let our viewers that know film school decide whether they've seen the future cinematography but thank you so much for explaining and showing us the electro cinema system my pleasure thank you again 