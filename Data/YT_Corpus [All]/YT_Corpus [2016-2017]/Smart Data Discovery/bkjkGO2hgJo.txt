 thanks everyone for joining us my name is Matt Reid I lead the security business the risk management business at IES and then I'm joined by Navin Dean who is an architect on our team and one of his superpowers is is DLP so he's gonna go over a lot great information around architecture some of the technology itself within Symantec DLP especially around detection and we're going to go through a live a design and sizing exercise and then hopefully we'll have collected some questions and we can pause and take a few of them and I also want to mention these are some some recent pieces of thought leadership that we've we've provided and we've we've created you can go of our social feeds we've got landing pages for all these great documents so I encourage you to go download those and take a look at some some fun infographic based white papers that we've that we've developed so let's talk a little bit about about DLP and a level set and and first just a little bit of you know a little bit of fear factor here in the way of if you're paying attention to what's going on in the news and the media it's kind of clear that that today's security architecture is failing us there's all ever-growing complexity and rapidly changing environment with trying to keep the bad stuff out and trying to keep the good stuff in right so the the huge growth and information and we'd like to distinguish the difference between data and information and treat it like currency Mavin's going to talk China a little bit of that and talking about data classification but that huge growth that organizations create in store in the ever-expanding number of endpoints with the you know the permit are all but all but vanishing between the devices changing between you know people work and play in this connected world it's it's making its making things really interesting out there and it just seems like the sheer you know growing complexity of everything is putting organizations at risk you know and the other thing that's that's kind of that's kind of happened in the last decade comparing back to when I got into i.t i.t used to rule the kingdom right we used to rule rule the organisation from a technology perspective with a fist and in the last decade it seems like the old user has taken over on us so users are all-powerful the threats are different you know doing business is changing and that reminds me of the of the staff issues most c-level executives and organizations have got you know different operational challenge it could be with tools it could be with process or it could be with people or people management so you know the other realization that's worthy of mention is that companies that leverage technology to help them do their business and make money and drive revenue they've built IT companies within themselves and I think we're at a point red or critical crux where because of the things that are going on in the landscape and because of all this confluence of different challenges and different drivers it's it's making things it's making things start to move right the data is growing and moving cloud is very much a thing companies are looking to offload not only the responsibility of data but different workloads across the organization so that lack of skilled staff is also impacting us out there right and then remediation is slow and manual right so there's there's there's definitely some big challenges out there and even just looking even two years ago the amount of new threats the amount of data breaches and you know X filtrations of a PII it's it's it's really significant and so DLP has really become an imperative so for those newer to DLP really we're looking to help organizations identify locate confidential data make sure that it doesn't leave the organization inappropriately so the notion of find it you know what are we looking for let's figure out what's being done to it and then fix it you know the diverses detection versus response and the fix-it is what do we want to do now that we've determined you know what and how or where let's let's take the action you know that's appropriate for the business requirements and then and then even further it's it's about notifying people you know empowering the line of business that can ultimately make the decision about the violation use the opportunity to educate you know these users that have now become our powerful and then ideally bring this back to the beginning and create a closed loop of remediation and an ongoing improvement so the the due care and due diligence so most organizations need some sort of DLP solution whether it's to demonstrate compliance safeguard personally identifiable information or maybe to protect their intellectual property or brand so from a semantic perspective I mean this is one of the strongest franchises that Symantec has got right I mean the the most of most of us know if you're paying attention to semantics franchises you know that you know Gartner's had a eight plus year love affair with the with the Symantec DLP product you know very feature-rich very mature solution that covers all the relevant control points and protecting information and not only you know a really good architecture that we're going to get into here with Navin but it also gives you all the workflow and all the response capability to take action and and do the things that you know are important so this is helping customers cover all the data loss vectors whether its network whether it's storage endpoint and and even the ever-growing migration to the cloud so we're going to go into more of these and do throughout the presentation and it's just worthy of noting that all of these various modules are administered by a single management platform which is called enforce and you can see that there at the bottom of the slide so another note along the lines of alright I bought DLP so now what there there's a list of really critical things that that you must do there are a few organizations that are making these mistakes but we definitely have seen over the years organizations that buy a DLP tool they they install it they get it configured maybe for our vector or two sometimes just one and they get visibility so they get they get eyes and ears into what is going on in the organization much beyond what they had before but they may not do anything about it or they don't you know proceed down the risk reduction lifecycle so if you're if you're planning a DLP initiative or a program you know these are important things to be to be mindful of and we'll touch on these a little bit more as we go throughout it but a really important part of this is is assigning the staff you know making sure that you've got a reliable design and infrastructure which Navin is going to demystified today get all that integrated with your with your other infrastructure whether it's the content management side or a proxy leverage different you know capture protocols or maybe it's on the data classification or you know whatever so we need to get that integrated with the rest of the infrastructure you know get all the advanced technology deployed to detect and respond to things between the performance and then really where the rubber meets the road is is putting those automated responses in there and taking the right action on the right violations so along the lines of of the team this is really important so you got it you've got a couple different groups of the roles that are needed for a successful and amateur DLP initiative if you're if you're regulated if you are protecting employees and customers and IP and you know adhering to to compliance you need these these roles can you know can be overlapped in some case it is but it boils down to these three groups you need the gurus you need some steering committee and you need your responders so the gurus are essentially you know the tech whiz is right where we're the ones that know the DLP solution in and out the the steering committee these are the people that understand the industry and and and how DLP needs to function in an organization you know like yours they're bridging the gaps between IT and a business units right and that's a that can be a tough thing for IT to overcome these individuals in the organization speak both languages fluently and then there's other roles like the Shepherd that we refer to where they've got the credibility and the respect that's often needed to build a maintain support and even different levels of the organization so even beyond you know that translation that translation or that translator that can speak both of those language you need somebody who's really you know that sponsor that can take it all the way up to the top of the organization and even get visibility into the board that could be the the c-level executive for example and then really really important the responders right those those first responders different escalation people maybe maybe some forensic pidove people or some you know some data mavens they're the hands on people that respond and flag you know the business broken business processes address the employee concerns maybe other educational needs and in the in the organization so you know it takes it takes a skilled and an smart person to be able to look at something that's going on in the environment and know what to do right so really important things that we need to be considering if you're developing a DLP initiative these are critical critical aspects of the team to consider and then the the beloved ro ro T or the waterfall as its referred to and this is essentially the supposition here is that when you initially install and get the LP up and running you enter in this realm or into this this phase of of visibility so again I refer to as you know eyes and ears now you're seeing what's going on you're finding broken business processes and ideally you start remediating right you start remediating those red flags that were identifying we're seeing and then you're starting to notify people you're really empowering the lines of business to Retta try to drive the policy now IT is ideally you know moving in a little bit into the background at this point because the DLP architecture or the the infrastructure is up and running and now we're getting this turned over into different lines of business and we're starting to you know work with the people and and really really start to make a difference and then only after through your you know after you're through all of these phases can you get to this notion or or outcome if you will of a prevention or blocking or stopping you know that big data exfiltration so that is what i had to talk about in the way of kind of setting the stage and setting the table with DLP I'm proud to turn this over to - Navin Dean greetings and good morning to everyone hope is all as well and I'll take a minute here to introduce myself a bit been working in the healthcare space for about a decade various physicians concentrating in security I've worked with DLP for about five plus years and also had about thirty successful DLP implementations here with IPs I've been with IPS coming up on three years very similar to to math there and just helping companies architect provide the best so with their DLP program help with other initiatives like business workshops bridging the gap between the DLP program and the the compliance or the different areas of the business for instance the PCI team helping with that payment card industry visibility and also providing them with policies and workflow so they can handle their own incidents that are generated with respect to the DLP policy or the PCI policy there right away here you can see an overview of the architecture of the DLP product suite at the left is the enforce admin platform and the associated Oracle database enforces where you administer the system create policies view the incident reports and remediate incidents in the middle you see the various detection servers which perform the actual detection analysis on emails web transactions files and so on also keep in mind here that data insight is not a detection server data insight is now part of Veritas and that is a separate user interface and solution at the right you can see the network components with which the various detection servers integrate with for example the discover scans various data repositories to identify confidential files the endpoint products leverage a single lightweight agent running on the endpoint computers and network monitor receives a copy of outbound traffic to analyze and that's from that span or tack port and then all the server components are software they're not appliances they're they're software and most can be virtualized so switching city the virtualization architecture here I always recommend VMs this is to the ease of provisioning the servers taking backups or snapshots and restoring from disaster recovery in addition there is only two servers that would require physical devices and that would be the Oracle which is part of the DLP suite however it's running an Oracle database and the network monitor servers and that is due to the packet capture nature also keep in mind here one thing I'd like to add is that you know detection service keep a copy of all the enforced policies in memory so when you're enforcing DB servers are down as long as the detection server does not restart detection will still continue so this setup is very scaled and also autonomous so this way that if there is some type of downtime or or backups being taken for Oracle the detection servers will still function as normal so they would detect incidents and analyze network traffic when the database or servers are down and also keep all of the incidents stored locally so incidents storage is limited by this space available and the on the box itself and storage of queued up incidents on the detection server you know depends on the tribe size allocated on that partition in addition that the incidents are stored in encrypted files on the tribe so once you know the moment that if or server starts communicating with the detection servers the detection server will will push up those cute incidents to beam or server for processing the network prevent the network DLP products which is our data and motion a couple of our products includes that Symantec offers this network monitor network prevent for email and network prevent for web so network monitor passively monitors email web and other communications that contain confidential data and that includes SMTP HTTP FTP I am and TCP based protocol so next product would be that will prevent for email which blocks SMTP traffic or modifies it for downstream routing to an encryption gateway network prevent web into grapes with supported web proxies to black will remove content from HTTP or HTTPS traffic and also blocks FTP so a lot of questions come up so then you know why would you need network monitor and that would prevent why I've been in tandem network monitor catches all protocols and network prevent blocks common risk protocol so one is therefore for visibility of of protocols that are not being sent to your proxy and the other one just blocks common high-risk protocols that are sent through your your web proxy solution I will switch to the network prevent for email flow here so Network prevent for email integrates with your SMTP compliant MTA's to monitor as SMTP traffic and block it out right or modify it for downstream processing so end-user sends an email it hits your MTA or your exchange environment and this diagram shows that MTA sending it to network prevent email for inspection and that is against our DLP policies that we create in the system and if it violates that policy we can take action on that email network monitor flow here captures and analyzes traffic like I was mentioning on your network SMTP FTP FTPS DDP and various protocols so you can configure it at moderate the custom protocols and to use very a variety of Filat filters to filter out low-risk traffic so it provides great visibility and can be scaled and deployed in old infrastructures or legacy infrastructures as well as new infrastructures so the great thing about this is that you know semantics dlp separates itself by adding scalability into the mix and can be incorporated into almost any type of environment distributed or remote environments you know placing servers at that branch office for better performance and reliability being able to provide you know real-time results of scans and monitoring and that's the the value there that semantics brings to the table versus or other ideal piece solutions out there so getting back to network monitor here the the packet capture process you know takes the packets and reassembles them into a message can handle about 80 megabits megabits per second throughput total this is not per neck but the total traffic come in and from all NICs selected for capture if it's more than 80 then you know there's going to be packets that are dropped and a long message wait time maybe some of you have experienced that before and this is where we would recommend using high-speed capture cards like an Indus or an app attack moving to network prevent for web here for for inline active web request management that would prevent web integrates with and I compliant proxy and typically that proxy handles HTTP HTTP and FTP so that I cap is very critical and should be supported in most proxies out there like your blue code web sends your iron ports so network prevent for webserver you know detects data and those protocols and it can reject those requests do a room remove or redact on that HTML content with respect to the the governing policies there or your DLP policies two of our endpoint products single compact agent is installed on the endpoint computer which enables endpoint prevent this prevents confidential data from leaving and points and appropriately and the second one is the endpoint discover which discovers confidential data that's stored on on you know thousands of endpoints for example or or maybe you're taking a targeted risk-based approach and just pinpoint a specific Department of machines and you can specify in the endpoints can if it's just one machine by putting in the host name of that machine in there for scanning that particular machine for data at rest on at the endpoint so note that the endpoint of prevent monitors data downloads on local drives prevents data on USB cd/dvd Network transmissions and in DLP 14 we recently added chrome support for that web transmission there's also printer fax copy/paste network shares and cloud storage applications as well so endpoint discover an endpoint prevent or you know requirements of true endpoint DLP and our logical extensions of the network context layer of Defense both products use a local detection so it can work if it's in disconnected mode providing anywhere anytime protection and the same DLP policies you use for network and storage protection can also be used for for for the endpoint quick flow here you know we have the agent configuration those channels that we can monitor in a DLP console which you know we can we can look at whatever your your risk is if you're targeting a specific risk in the organization that that managers and don'ts charge with government mandated we can specify those particular channels those that are applicable to your industry vertical and monitor that vector Matt was showing us earlier that water flow model this is where we would do that visibility on the endpoint you know make it not visible or not make it transparent to the end users so they're you know are continuing with their regular business work or business slow and not interrupting any port activity loss there so keep it in trance mirror mode see what's going on in your requirement and then once we get those incidents we can then have a good idea on what actions to take holding forward I'll get a little bit detailed with the architecture where we get there but my sweet spot for for for the number of agents to report to a single endpoint prevent servers typically around six to eight thousand depending on the amount of policies and if we're leveraging any advanced detection technologies like IBM on the endpoint note that ADM on the endpoint is not recommended on this time because of uh of resources and the amount of time it takes a process and EDM index which is exact data matching on the endpoint this discovery here but this text you know sensitive data rest on on desktops and laptops use case would be that if someone is switching laptops or someone left the company and you're unaware of what's on that machine you would want to do an endpoint discover scan before you you give it to another personnel maybe in a different Department so it's moving from a clinical area now to a in business area or maybe an IT area so that that IT is who may have clinical data on that laptop that you may not know about and it can pose a risk because it's you know not encrypted it gets lost it could be a big data breach issue there right so point two here for endpoint discoveries that you have to add credentials to a credential store and this will invoke the endpoint flex response to quarantine any type of files that violate DLP policies um note that the credentials are stored in an encrypted folder on the endpoints that are connected so you'd have to be careful about what type of credentials you you use in store there typically you know use credentials that are not access to other areas of the system all right moving to the storage DLP here this is very critical to to most organizations is finding that you know that data a brass because it's just one click away of being data in motion right so scanning your data repositories finding out who owns the data we solve that with storage DLP here so next slide here we talk about you know network Discoverer and how it lets you locate confidential files exposed inappropriately on various repository and you know this includes a multitude of repositories Windows Linux don't unikz sequel Oracle db2 so long been a collaboration platform like SharePoint document Documentum websites extranets essentially any data repository and then network protect lets you copy or quarantine those files for for target file servers that support specific protocol so a use case you know doing a data cleanup a risk reduction want to relocate an appropriately exposed confidential data remove the files in violation of PCI or remove the be confidential over you know X years old so there could be like you know data sensitive data out there that's ten years old on your your data repository you know you want to end the inventory out of that and and then you know kind of quarantine it or put it off to the side for review and then maybe archiving and send it off in this way so those things you know pose a potential risk and then the access privileges is another thing and that's where you know we solved that data inside right so data insight it's a comprehensive solution for governance of unstructured data lets you collect data on information owners the file usage while permissions for files and directories on windows file servers and also then as sharepoint devices so data insight integrates with network discovery it gives you that big picture on ok this I found sensitive data on my network share but what next I mean who owns it this is where data inside comes in and tells you who owns that data based on that metadata access who's been writing to it who's been accessing it who's been reading it so such details includes you know top file user who's complete file access history with visualization of that access permissions and that folder risk scoring that helps prioritize our remediation so in the LP console you'll see all of that metadata information the user attribute the sensitive data and that's what the integration with network discovery data inside and it forms moving on here we have UN DLP 14 some some advanced capabilities here for a cloud one is box and the other is open 365 integration so in Box Cloud Storage targets with Network discover you can discover confidential data and this is on providing that credentials for a box of administrator Co administrator so you can configure a box data at risk an automated response rules to apply visual tag to confidential files discovered on your box cloud storage target and next year the de cloud prevent for office 365 which monitors and analyzes outbound microsoft 365 exchange email traffic you can block redirect modify that email message as specified in your enterprise policies so to add visual tag rule here on the backside we we tagged that metadata for sensitive information and in that visual tag your box cloud story you just search for and software mediate that sensitive data so you know DLP provides that tagging and then your users then can you know we're mediate those confidential files by you know adding some type of password protection and we're just removing that file from that location moving on to the cloud prevent for o 365 exchange here a couple of things I just want to keep in mind that semantic data loss prevention cloud prevent integrates with a separate semantic email security cloud service and this provides the email delivery and also protection such as policy based ink an anti-malware scanning however you know you don't have to get the service you may already have something like six or Proofpoint but the service is available for a great kind of like package here where you get the cloud prevent and the email security dot cloud solution ah one thing to keep in mind the cloud prevent it you don't have to have a Rackspace or as your environment or you can leverage semantics cloud cloud infrastructure to post the cloud prevent servers out there all right I think I think we're ready here to do a sizing and design exercise for you guys keep in mind also a couple of things is that a use case that I would give you that maybe there's an audit going on in your environment may be a compliance audit and there's an audit risk there may be it's an inherent risk or something to deal with me to reality something that's going on with your your sensitive data in your environment and you know this is where a DLP classification doing that that auditing of your environment or your information systems and processes would expose a need or have a need for this type of a solution and this solution is a program and it also helps you know with user user education you know making sure that the users are doing the right thing and dissing forth this definitely it forces you know making sure that you know your end-users are protecting what's most sensitive to that organization so definitely something to consider when thinking about setting up your DLP systems and making sure that your policies are in line within mandates and those charts with confidence in your organization to implement so I wanted to do a little exercise here today and just architect out high-level diagram of what a DLP implementation would look like and talk through some questions here on on what we what we discussed earlier when we start scoping out a DLP engagement we would like to do an architecture session and one thing to add what Matt was mentioning is that you know IITs provides that entire DLP program you know helping out the business side as well as bridging that gap and making the connection with the IT security side so we helped develop that entire DLP program for you target risk do a targeted risk-based approach deployment methodology may be that risk is on data in motion or data leaving externally out from the core to the perimeter we would probably start with you know your network or your network based data emotion products just to take a targeted risk-based approach moving on here to this Visio diagram so once we do that scoping we start off with the Oracle database and that database is required to be physical that is the best practice and storage is typically around one terabyte the Oracle or the Symantec VLP template is applied to the Oracle installation for tables and also for housing the incident information which is encrypted so we start off with the Oracle database and move on to the application which is the infor server and here we make the connection between the application in the database via the import one five to one which is very very typical here right uh what I'm doing here is a three-tier deployment there single to tier and three tier typically what best practice and won't be like to implement here is 3-tier keeping everything separate on its own box and force is virtual as well as all the other detection servers so the only servers here that are physical are the Oracle database and network monitor so we have our database and application stood up now we can start configuring the enforced platform for Active Directory connections and user attribute information pulled over from from LDAP or Microsoft Active Directory and that is typically over 4:3 8:9 here right next we would want to set up our mail for reporting and email notifications alerts so on we we start here with making that connectivity to your relayed over port 25 those mentioning maybe to use case here is - next we want to - network monitor we want to target our data in motion so network monitor is a physical server there is no appliance here Symantec DLP is all software right so here we will start off with network monitor and the way we would we would scale or size network monitor is based on the number of period - Nettie press points right so for example if you have your main DC and then your secondary DC or backup data center we would typically want you know two network monitors right so I'll add my my span port here I'll add a DMZ right and you may have more than one ISPs just for high availability and can business continuity so we would put another internet egress point here for network monitor and most detection technologies can communicate back to the enforce via 8100 by default once we've got network monitor we have visibility into - HTTP SMTP FTP and various protocols that are configured based on your your risk here and your environment is there some type of proprietary application it's using a number port or or communicating out through a tcp port that is uh you know not your typical port 80 or 443 or FTP traffic to anyone we can definitely configure those TCP ports to monitor that traffic next I'll move onto to the network prevent web website right so if you have you know say four proxies we would typically do a one to one so for network prevent web servers to handle and we can definitely do round-robin and and load balancing for for high availability and that's that's we yeah I can't write or Internet content adaptation adaptation protocol once again for eighty one traffic is the port that we communicate back to the enforced platform here we have our you know backup data center what about our primary right we'll have to do the same thing on this and write for for web proxies for that would prevent web servers do that I cap could make that I cap connection here and also with the 8100 traffic back to the enforce so it's kind of duplicating you know each side for for that high availability and in the event that your main DC fails traffic would then flow through your second DC or BL layer to write for that high availability and business continuity so last date and motion product here is network prevent email typically we we get the messages per second out bomb for for scaling and compute a formula for sizing Network prevent for email based on mailboxes messages per second those types of metrics to to report on and to scaled and supporting Li maybe this is a small organization 8,000 8,000 mailboxes a typical implementational specification for this is you know 2 to 3 Network for Becky email servers this example reusing exchange so here we have our network prevent email server communicating to an email encryption appliance and here we're having our exchange send information to our network prevent email server and then to the email encryption and clients and then off to your risk point year or your mail egress point and once again port 80 80 100 traffic here going back to before so notice that you know it enforces the Achilles heel for all your policies your incident remediation process and everything home runs back to your enforced platform endpoint maybe we have about twenty thirty thousand endpoints in your environment and and maybe even have external users right or users that don't don't communicate out to communicate on your domain or your network so maybe those users are out here somewhere in India in a cloud or in the Internet we can definitely stand up an endpoint prevent or in your DMZ which is supported by Symantec and we can specify that port and that package using a unique package and also of configuring secure keys most all of the servers here has capabilities of communicating via one and 20 big aes encryption without keys so it's it's a secure deployment and that only that server can communicate to the enforce server then so each detection server can only communicate to a particular server which is the enforced server for security typically by default your agents communicate will be a port one zero four four three and and provides also keys for encryption as well so sending that information is encrypted network discover scanning your data repositories your for data at rest port 80 100 can be changed this is just a default that I'm using here and this setup here or this example and one last component I'll add to the diagram here is your data insight and your data inside should be placed near your your storage repositories as well as your discovery for better performance right your endpoint can be distributed in the environment once again six to eight is usually my typical sweet spot that I recommend to two customers to have that have agents report to that endpoint server and it could be distributed some of the endpoint servers could be at the other data center for that high availability it all depends on your infrastructure and how we would scope it in for your environment to wrap up here this is a service or an offering that we do to all of our customers is to help with the implementation the scoping process provide the specifications you know they're necessary servers that are required for your environment and also create that plan at target either if it's data and motion data at the endpoint or data and help with that maturity process terrific stuff here Navin thanks so much so we have collected some really good questions we are going to reply to those in the email and I just wanted to wrap up with a couple slides here real quick if you need some help keep us in mind we have a brand new fleet of hardware that we use for risk assessments or what we are calling an insider threat study and it's essentially an evaluation of the Symantec DLP solution so we've got a new fleet of Hardware dedicated to this we have service offerings to support all the different phases of an adoption lifecycle with so whether it's implementations upgrades assessments deep dive health checks as well as managed services so right for you a support model from ad hoc support to project support that teaches you how to fish as the as the professional and is the DLP owner or a more of a I TS own option where we provide a DLP program as a managed service we also offer a non fee based red flag assessments and this is essentially a high level high level console review and benchmark against an existing Symantec DLP installation and then as far as contacting us these are all of our Social Feeds and our our contact information reach out to us follow us connect with us we'd be we be delighted thanks so much for attending Navan thanks so much for going all over all this great information you deserve an award for I think a land speed record on on building a DLP vizio so that was pretty impressive work my friend thanks so much for that we'll go ahead and adjourn happy Friday everyone have a great weekend and we'll see you on another IT s webinar take care you 