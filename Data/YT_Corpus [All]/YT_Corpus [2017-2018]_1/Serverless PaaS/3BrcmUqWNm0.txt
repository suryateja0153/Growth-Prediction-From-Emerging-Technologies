 [Music] the next 60 minutes will be about serverless data-processing give you a bit of an overview of what we'll be reviewing today or looking at they they'll quickly go over some of the technical benefits of cloud dataflow this is a level three hundred sessions session so we'll skip most of the basic stuff and just provide some of you but dive deep into the technical benefits pretty quickly I will do a demo of a new feature that we have to launch just last month dataflow templates it will show you how easy it is to do data processing without operating any servers and launching pipelines without any servers of them deep dive into two particular benefits of cloud dataflow specifically dynamic load balancing and auto scaling I'll do a demo of auto scale and show you the history of auto scaling for particular type line that I will launch alcohol anchor to stage and here it is going to talk about a breitkopf is using cloud dataflow and several other DCP products so to provide video analytics to their customers we'll have a recap and to also do a Q&A if you would like to ask questions we have two microphones in the aisles so at the Q&A session just come up and feel free to ask so most of the customers before they start using cloud dataflow they use something that is called a lambda architecture a collection of tools a doob sparks kafka which is optimized to for specific flows of data for batch processing and for steaming processing once once our customers are switching to data flow they take advantage of a unified one this is one of the major reasons why customers are migrating to cloud dataflow it offers a unified way of doing batch and streaming and you can use it in conjunction with other GCP system product such as pub/sub and bigquery to build your data processing pipeline if you're a fan of open source computing and who isn't you can also use cloud dataflow with with another stack of data processing applications we have open sourced our SDK and became Apache bean which just recently became a top-level Apache project and you can use theme runners on a variety of in a variety of environments including spark Flegg and some others to summarize why customers are using cloud dataflow by the way just by by the show of hands who has already been able to run a few dataflow pipeline ok a good good idea sort of the audience so cloud dataflow first unified way of doing batch and streaming you get access to a fully managed environment you can spend your time on optimizing your pipeline code instead of running and managing server infrastructure we offer you a open source project model so you are not locked into just one vendor you can run beam pipelines in cloud dataflow or you can run and run it in on-premises or other cloud providers with other runners such as parties links and we also able to to scale from maybe a 10 records per second incoming data stream to millions of Records per second architectural a he's a architectural overview of cloud dataflow we are a black box that connects sources and things we support streaming and batch sources we call them bounded unbounded sources and we write into variety of things to offer connectors for sources and things then we also offer a bunch of benefits under the hood including we create the worker BMS for you which actually executes and run your code we optimize the pipeline's that you submit to us you you can keep kind of thinking at the more abstract level and I say if you are a sequel program or think about analyzing a simple query and then submitting into a database that we do the optimizations for you we skill resources for you up and down depending on the workload and we do a redistribution of work between workers when particular workers become what I'm going to talk about two specific benefits later in my presentation but that was going to pay overview of all the benefits that be the pro provides if you are a developer then from a developer's point of view you write pipeline code we support multiple languages Java and Python this code has access to a wide variety of data transformation function in our SDK the buguese a language in the temple so they offer the same functionality in both Java and Python we call it the beam model beam provides abstraction to example if you want to do parallel to process data elements in parallel you can use a transform called Pardo because I do we transform for parallel processing if you want to group things by key in any language Java or Python you use a goodbye key transform and as already mentioned once you go to pipeline and kind of chose your your language your SDK you can run it in a variety of runners you can run it in cloud dataflow et marinatin spark and you can run it in flag so I wanted to highlight four specific technical benefits of cloud dataflow before I do my first demo I will mention the the full management extract aspects of cloud dataflow we create workers for you we allocate the split your data inputs into smaller shards and allocate them to workers go to do optimizations of your execution code example you might start with a pipeline and pipelines can be thought of activity diagrams where you have bubbles of transformations and data flowing from one transformation bubble to the next transformation bubble so you think in terms of these activity diagrams and what you want to do with data you define your transforms and you connect them you submit your pipeline graph to us and we optimize the execution sometimes you don't know ROG steps together if you think that it will be more efficient to do both transformations together sometimes we split them up to achieve a higher level of parallelism yet another benefit of cloud dataflow is auto scaling so think about a workload which maybe starts with just 800 records per second incoming data stream the beginning of the day your users are not doing much to just in generating about eight thousand records kiseki duality just a sufficient number of workers which which are required to process this incoming work see now during the day your users who begin submitting more events it might've reached several thousand five thousand from this example throughout of scaling cloud dataflow is able to stamp the increase in CPU utilization DN T's in incoming data rates and increase the number of workers automatically to a much higher number not only do we increase the number of workers we also decrease them when the workload is about when your incoming data stream is declining and lastly the last feature I wanted to highlight and these are the two features out of scaling the dynamic load balancing which I wanted to kind of give dives audit later it's important that I covered them on a higher level source in Stanek work rebalancing I already mentioned that when you have tell us your data inputs and let us allocate the workers to process the data input some of the data charts which we create be the shards of your input data might might contain data which takes longer to process than all the other sharks we call side shards stragglers and sometimes well actually quite often because of the existence of stragglers your entire data pipeline will take longer to execute because we are waiting for the completion of the of the struggle with the feature that we have dynamic worker balance is actually able to steal tickly work from the worker which is struggling and he allocated to other workers now I'll show it in a demo which I want to do just now and you switch to the demo machine first in the demo I'm going to start with showing you how easy it is to to start a dataflow pipeline we have recently launched a feature called dataflow templates templates allow you to write your pipeline deposited into a GCS bucket and then call on this code whenever you would like and wherever you would like you can initiate the template from the console you can initiate the sample into a dataflow job from Q cloud or through the API in my demo I'm going to use a export of a publicly available data set of packages those of you who know me know that I am really excited about text analytics I love natural language processing I write algorithms for crunching through through from news articles trying to extract opinions from text so I took the I took one of the public datasets that we have how can use news post and dump them into a bunch of CSV files for this demo as you can see in the ccs pocket where I dumped my parking use dataset I have about 30-40 CSV files of the size of about 125 megabytes but one particular file is extra long and you'll see why it's important this file and a delight file CSV is 5.5 Giga bytes long and those are the stragglers I'm talking about sometimes you did is just larger the data shards are larger than other datasets so let me start my data flow pipeline I'm going to go to the data flow console and click on run job and this new UI you're able to choose which template you would like to instant read some of the options are I can do a custom template custom template is the code that you write and place in GCS and you can then share it with others and span sheet it into digital jobs etc I don't actually use a simple algorithm that we typically do for typically share with customers when they start learning about data flow it's the Ward County algorithm it's quite simple it reads from text files counts the number of words and then the statistics on on these from these words I'll give the job a name demo job or one and specify my input GCS bucket I'll use a pattern to catch all the CSV files in this bucket and lastly I will also specify my output bucket where the attack statistic should eventually end up that's all I needed to do to launch a data processing pipeline I didn't have to install any ideas I didn't have to launch any VMs I can basically launch now pipelines through the console using the Stamper Cheng mechanism and within seconds data flow will start showing you the execution steps of this pipeline as I mentioned before it's a little bit simple it reads lines from my input files it runs the tax calculation algorithms it does some mapping work and then applies into GTS files I will come back to this pipeline in about 10 minutes and to show you a couple of other things that I will continue talking about during my presentation slides so this is the code of the pipeline reading from files applying a data transform to calculate statistics on tags in parallel form at the output and writing back into files because cloud dataflow is a parallel data processing service we offer you as the user the ability to run your transformations data transformations in parallel and we call these capabilities purdue's so basically define a transform function we call them do functions do fonts and then you tell the depart do transform to go and execute your do function the do function takes data in a certain shape and this diagram is represented by yellow yellow breaks a yellow squares and appreciates them into data elements of a different shape maybe adds additional fields removes them aggregates them again in the diagram represented as green star since all data flow is a parallel data processing service we're on many many workers that do that execute your transforms the best way of visualizing what your workers are doing your worker virtual machines are doing is to display a Gantt chart in my diagram here I have eight workers working on data charts leaders are some is my input data set that I subdivided into smaller chunks so these eight workers take a data shard they work on it they complete it and they start working on the next data set is a real-life Gantt chart of a dataflow pipeline of four hundred workers running for about twenty minutes during three texts it reads files it then does a goodbye key operation and this is why you see this way clean separation between stages first needs to leave the files once it has entire data set it begins grouping data by by T's and it also writes into files with parallel data processing one of the big problems that I mentioned before is the the presence of stragglers and struggle thoroughly your data charge which take longer to execute than the average execution time for all the other data files of your data set and this diagram here drivers would be the bar in red and the yellow one is kind of approaching the Ducato we're just travelers come from and this is important because if we want to provide you a service which does minimize these three sources that we that we utilize the tech improves the execution performance of your data pipelines we need to be able to detect strugglers and we need to be able to deal with them so the first step here is to understand where they could come from they could be coming from from a uneven distribution of your input data for example if if you if your input datasets are textual files let's pick your favorite language let's say English English has a natural skewed data distribution to to the frequency of water starting with a particular letter so all the all the data charts which contain words starting with the teeth example will be longer than your other data chart where else to data such a strugglers come from well example if you do a joint operation and you have to have two input datasets and you're trying to join each key on the left-hand side which with the corresponding key is on the right-hand side but some of the keys on the left I'm using the left-right technology in joints but some of the keys on the left you will have many more records on the right and this is again another reason where you could have struggles not only does the data or your data could could complete two strugglers you can sometimes also have issues where the infrastructure and you can have issues with networking or perhaps a slow disk something something is going on with vos or DBM restore there are not enough resources on a particular virtual machine that runs your code to to process data quickly yet another reason for data struggles what could you do taught us about this issue well you could start splitting your input data into yet high number of chars that might help in some cases you can try to let your users you being the the bender of a data processing service you could let your users define business rules and try to kind of find you how many workers are allocated to be charged which of the workers are being allocated to the charge and so on not so the complex but can be done you could collect statistics on data before the execution of the pipeline and and create and start you your strugglers course to example so that they can complete faster on the hardware side or the infrastructure side this is especially something that focus on the Hadoop world or you could do backups and restart which help with with this problem well at fault data flow we spend a lot of time thinking about these problems and we deal with them by first detecting struggles and fighting fighting meaning dealing with with with this problem so the first step is to identify stragglers so what do we do the for each of the shards and each of the workers which process these shards we always track the estimated completion time we always estimate how long it will take to finish the processing of an active trial and when we when we hear that the expected completion time exceed the the average completion time for all the other shards there is a flag this is the indicator for us this shard will become a struggler and will slow down the pipeline so what do we do we split them up clicking up means we up the worker which is responsible for processing these stragglers to give back some of the work that there is wantable for the given bags could consist from give me a range of line items in your input data set give me a particular IDs and you input data set that you are unable to process quickly so that I can reallocate it to other water so we continuously do that and in my diagram here the green bars represent work that is already completed I have eight workers the voyage done nine shards the fourth workers did all digital the yellow bars represent shards they're currently actively working on and the these bars represent the estimated completion time and for each of these shards is calculated estimated completion time and we detected for two of them that they will delete and so we will try to allocate work will not try to reallocate the work will take away more from these shards and reassign it to other workers not only do it what do we do it once during the execution of the pipeline we do this repeatedly and here's what it actually leads to going back to my example of a real live pipeline it's actually an example I haven't shown here the pipeline is a relatively simple transformation there's only one type of transformation on your data and a trance on 24 workers I'm talking about the diagram in the upper left quadrant here so 25 workers trying to do a very simple transformation which is simple pardhu but there's a skewed data distribution and two of these charge will take much longer than the rest without load rebalancing the execution of this pipeline will take nine minutes but look what load rebalancing gets to to the pipeline it used the execution time almost fifty percent to about four four minutes and twenty seconds and look how how much more utilized the waters are now be complete and approximately at the same time it's another example of a pipeline that I've mentioned previously 400 workers doing somewhat more involved transformations a read Purdue the region from files doing a group by key writing to files the data is not a problem it is uniformly distributed but the still a bit of a flag between some of the workers in the region part and some of the variation in the waters in the writing part with without load rebalancing the pipeline takes 20 minutes web load rebalancing the savings are not huge but they are nice 25 percent utilization is significantly higher and we've reduced all the slack all the white space between the waters dynamic load-balancing is trade in isolation but it especially is useful when you start thinking about adjusting the workload to the variations in the incoming data stream what we call Auto scale think about the case where you you start with three workers that's my diagram up in the first row here so three workers working on thermal data charge and for the first couple of minutes since we're doing the estimation of the completion time everything looks fine it looks like we can process the data within 10 minutes not a big deal but as time progresses and as data flow is consistently and continuously reactivating the completion time we learn more about the input data set we begin something that the actual completion will be much longer the first couple of records in our data shards we are our typical and the average record you know data shard will cause the pipeline to take readings great so we realize you could definitely use some some additional help why don't we start a hundred new workers and transfer the data just by throwing more CPU on it well we could but if you don't have the ability to divide the work of a life running pipeline your additional workers will be idle there will be nothing for them to do without this ability to dynamically be assigned work and rebalance work auto-scaling just doesn't work here's the diagram here's a here's a screenshot of another real live pipeline which starts with three workers and kind of keeps learning about the properties of your input data and realises yes we can process we can process the pipeline faster if we added more workers and it's great that we have the ability to divide the work so we launch new workers we allocate work to them every worker is working hard there's no white space they are fully utilized eventually a pipeline reaches thousands of workers it crunches through the data and completes and then skills down shuts down let me show you how exactly auto scaling is benefiting you on the example of my pipeline that I just recently launched demo machine all right so here's my pipeline back to my pipeline it completed with then within about eight minutes reason why I know it's completed is the green check boxes in each of the transformation steps for those of you who haven't seen the beautiful console before and one interesting feature that we have in the Google console a Google developer console is the history of auto scaling decisions to check it out when you go back the unseen appeared here's the full history of what went on in your pipeline lead lead from the bottom read from the bottomless screen we started with a pool of a single worker and remember I had about 40 files in my input data set of proximately the same size with only exception of one file which was 50 times longer 50 times larger than all the other files so we data flow kept the estimating and reallocating waters we increase the number of workers to 2 then 8 to 15 and 15 seemed to be a be a kind of a a good balance of nob of workers and available data inputs so we kept working at that level until we process all the records and then shut down hopefully that's this was a useful demonstration of the capabilities of dynamic load rebalancing and auto scaling and with this I would like to invite Ankara to Han from Brightcove to talk about hi how bright Bob is using data flow advice can we go back to the slides thanks sugar hello everyone I am there we go hello everyone I'm uncle Johan from bright coughs I am a software engineer with the video analytics team Brightcove is an online video platform which provides video so end to end video services to companies large and small they're supposed to be snapchat order but kept on disappearing after 10 seconds so we provide video analytics to companies big and small which means that we handle lots of data we deliver about 8500 years of video every month which generates about 7 billion events a day which generates a lot of time series but because we believe that raw data just crunched up together and summed up is not useful we aggregate it down to about 80 million unique time series and this all this is maintained by a pretty small team of engineers about five or six of us so basically half of the team is here so as Sergey pointed out we follow the pretty traditional approach we went with a lambda model so as you can see the top part is optimized for streaming calculation whereas the lower part is more of a batch system events come in through the top from our web players and Roku devices and Android devices and iPhones and they come to our beacon over HTTP GET aggregated in real time and stored away into the real time database at the same time we write all of this down into logs and store and store it to s3 so that we can allocate it every few hours because one of the deal which you make with the lambda model is that aiming system is really optimized for well being real-time that means that it is lossy so to take care of that we have a reconcile database which goes in every few hours and happily Jones through all the logs and gives it to a reconciled database and the API comes along for every API request and says it give me the data for this this request that the user is trying to make merges the two data sets and gives it to the user on the side we have a redshift database and over that we have some BI tools written so if anyone knows the work that goes into managing a lambda model system and everything they would know that it's a little latent hadoop takes a lot of time to churn through data you don't absolutely get all your results immediately plus there are more or less two code bases here so you know every so often we would get a support ticket which says hey my results from yesterday doesn't match the results from today oh why is this ever so slightly different so there's always been for that so all all this is working fine and many years it did and then you know things happen you know most important one is Brightcove is a growing company and more people come online and they watch more video which means that we have more scale and we adding more disks we are adding more compute nodes to a dupe cluster which is now bursting out it seems so we are spending more and more time just keeping the lights on essentially which means that lesser and lesser features get delivered so we have to do something about that but even more important is that there were some new features in line specifically session correlation we we thought was really important session collation basically means that we had to look at a viewing session in its entirety it's a much more resource intensive thing to do and the Hadoop and the homegrown streaming system was just not cutting it it was a lot of work and we wanted to get something better at the same time we wanted to use a more performant execution engine which could support all this and the future things that we want to do as well as a more robust and performant database layer which could take care of all this so we went you know we did did our research and we found that everyone seemed to recommend these slew of technologies step pretty amazing technologies they're open source so you can actually see the source of the product and you can figure out whether they actually wrote the tests that they did the their performance they have big communities behind them so you know that you know if you get stuck you will have someone to don't do and hopefully they will be able to answer your questions so kafka spark cassandra hadoo mesa was all good and we were like yay let's do this sit down so we started plotting and within the first 15 minutes of planning you know beep-beep-beep pages going off in our heads this is a lot of systems to manage and with five people you would essentially be just managing these systems and we had to first of all become experts at these systems to run them establish them build monitoring resources for them so dashboards and alerts and then each of these systems have like you know 500 variables do-kyun just right to get in working in the right way we want them so it's not easy so we were like okay you need to step back look at this again so the basic theory in this architecture is very sound you have a simple thing that you are trying to do Event collector goes to a message queue which goes to some computation thing and gets written to a database and we want to write it to a warehouse and finally serve it out but each of these components are very heavy so we wanted to sort of make them lighter and what we wanted to do was look for some managed solutions for this for each part of the system so that with the five person team we actually managed less and do more so looked around and lots of claims lots of marketing materials but if you're an engineer you thought one thing for sure trust but where if I never trust a marketing material or a blogpost just because it says 100% managed and performant so we went out we wrote lots of performance tests we did our due diligence in making sure that we could swap every component individually without sort of getting locked into the wrong thing and we went step by step to make sure that we got it right so easy pass easy parts first we saw Google and we saw Google container engine best way to deploy application so we went with it we deployed our micro services for the API on top of GK straightforward we knew how to do it run some there are lots of resources to help us out and it's kubernetes so even more community so that that was perfect next we went with how to deal with Kafka Kafka has this magical dependency called - zookeeper which if you have managed it it it's just completely magical in terms of just going down for no apparent reason so we didn't want to deal with it and what we found that pub/sub was a very good replacement for it it is a globally available and durable message queue which means that all the work that we would have to do in terms of keeping replicas alive and making sure the cast car is properly provision for our scale and is all good we don't have to do so three months saved message do you duplication by ID so you can fiddle with the pub/sub and data flow working together you can actually deliver messages again and again and assign them a unique ID and data flow we'll make sure that it processes that message just once so exactly once and finally it because it's globally available very little chance that you get disconnected from every single region of Google everywhere in the world oh pretty much not going to happen so okay next thing that we looked at was our database it's the single most pain point operational burden of of our team like we have spent weeks sometimes just bringing the system up so we wanted to find a solution to it our dataset is time series based so BigTable Cassandra was our first choice and then we looked around and we saw BigTable was there and if you read the Cassandra Doc's one of the inspiration for Cassandra is the BigTable paper so you're like okay Google knows what it's doing it's a pretty well respected paper so let's check it out we wrote our performance test and it just blew past everything that we thought in addition to that it's one button scale up and down so you go into the console you've set the number of nodes to 30 and save and it's done and it provides an interface using the good old HBase API so there's lots of tooling for it and it works very well very reliable and performant so database another one it's the easy button and finally ad hoc analysis and doing bi of things at our scale we get terabytes of data every day so sooner or later a support call will come in which says hey can you find this absurd thing from your data set and we wanted to answer that question and in the past whenever we had to do that it was a couple of days of going to s3 and downloading a whole bunch of logs running it through grep or some other tool that we wrote and giving the answer in if you have used big queries you'll know how easy it is to actually run queries and edits a console you write the sequel query and it's done it's almost as cheap as just storing files in s3 and it's completely managed and it churns through terabytes in seconds so Emma date went for us things that used to take days for us to debug and diagnose took maybe five minutes at best okay elephant in the room spark and Hadoop it's it's a monster system because it's trying to solve a monster problem but it's something that we wanted to tackle so we again looked around and just at this time Google dataflow had come out of beta so so I come into beta so we looked at it we read the paper that was published we read some blog posts we read some docs and we liked every single thing it was right there and we trusted none of it followed by we went in and we you know wrote a proof of concept over a over a hackathon and two days later we were like 50% of our feature set was implemented in it which was great so so we sort of sat down and looked at it even more it for the first time I think it provides a unified batch and streaming model the B model is very powerful so you don't have to maintain - I didn't almost identical date code bases just to do the same thing which is a big win streaming reconciliation which means that whenever pub whenever a message gets picked up off pub sub you know that it's going to get written it's guaranteed to get done and you don't have to do anything of of the sort of retries and all those things and finally its batteries included what I mean by that is it comes with a slew of inputs and outputs sources and syncs so you don't have to implement them there's like text input which Sergei used there's pub sub and on the output side we use big table but there's big query text output many others and lots of windowing operators so we want to do session correlation we have to essentially say window by session and done so immediate win for us and we were already one year ahead and off in our game of implementing all this stuff which is great so putting it back into our original architecture diagram everything inside the dotted box over there is a managed service what that means is that we would never get a page for those services which also means that someone in Google who is very well trained for that job knows how to diagnose debug and fix services when they do go wrong and it's a much linear system which means that when things are not working or when we want to develop something we know exactly where to point that you know what to tweet we know how to reason about it which makes everyone's life easier so what what did this give us lesser pain more features faster and if you look on the side we now have another box which is bi tools and ad hoc analysis so we got more stuff done because we were able to get access to tools which had much lower marginal cost this there's no thinking around this query is going to take eight days to run so let me not do it you just go in run the bigquery query get your answer get on with life all of this took about six months to sort of perfect and get it right there were lots of lessons learn and if if some of you have managed Hadoop a lot of that knowledge transfers over specifically if you want to speed up your data flow or your processing the best way to do is process less data reduce your i/o use a better disk mainly our SSD disk they're much faster you'll get a performance boost like without doing anything secondly is use a schema schema data interchange format protobufs is a great one we started with JSON JSON is great when you're just starting out it lets you we can twiddle with stuff but it's not optimized for machines that means a lot of your computers just spend serializing deserializing and - in addition to that you would spend a lot of time just cleaning up the data so you'll write tons of code just to make sure that the right fields are present the right format is there or something or the other and the same so we use protobuf and we like it and it provides many many features some of them being it's efficient and backwards-compatibility finally being from an analytics background I can I cannot stress this enough monitor everything what makes sense for our workload CPU and observe that logs may not make sense for yours you might have a completely different set of metrics which makes sense to you so monitor them we make lots of dashboards so that we have visibility into everything and that worked out very well for us and finally when all fails you get stuck there's a data flow team which was very eager to help us out in in on every turn so we really liked the team and then before I go into a short little demo this is a future roadmap which I would not be showing over here at all have we been implementing all this stuff using spark and Hadoop adaptive aggregates we analyze our customers requests volume and their requests patterns and automatically adjust how we aggregate our data so that their queries magically become faster lot more BI tools we have started to build a lot more bi tools on top of bigquery which give us which gives our internal customers much more visibility into data which just wasn't there and finally and it's a big one that we were able to unify our analytics and billing pipelines because there is a hard guarantee that your data once it's acknowledged actually gets saved to our database we can be sure that it's good enough good enough that that data exists so the billing system can trust this data analytics system always our previous analytics system always this thing that the billing data was ever so slightly different so you trust the billing and there would be questions about it like why is it slightly different and we would have to explain no longer so that's great everyone's happy everyone joyous so with that i'll show you what our pipeline actually looks like this is okay I'm going to cheat a little bit because demo gods are never create during demos so this is a video of the same Google Cloud console of our production pipeline as you can see we are reading from two pub subtopics at the top performing a bunch of computations on them and churning through data constantly and then finally writing it back to do BigTable the amazing thing here for me is that it's a very simple representation of a complex piece of software each box directly lines up with a piece of code that we have written so even though data flow underneath the covers is performing all these optimizations of you know all different sorts of optimizations we don't have to do those not amazing and we can just concentrate on how things look like so that was how a pipeline looks like and next we have maybe next we have the monitoring dashboard so a mean intuition behind making this dashboard is keeping it simple so that when you wake up at 3:00 a.m. because of a page first of all the black color does not hurt your eyes the second part is that everything is is exactly what you what you need and so the first row is pub/sub metrics for us pub/sub bat log is a good indicator of how we are doing so when you know there's a sudden increase in message messages published to us and data flow starts falling behind and it doesn't keep up for some thresholds so let's say three hours we get a alert saying that hey go look at pub/sub or go look at the data flow it's not keeping up we come here and we see that okay message volume increased pub/sub is backed up because data flow is churning through it it'll be good just leave it if it's if it's Auto scaled it will scale up process everything scale down everyone fine the CPU utilization we monitor that just to make sure that we know that we are spending the exact right amount of money we want to keep these machines as utilized as possible but not so much that they are over overwhelmed and low CPU is generally a good indicator that you know the pipeline just seized up for some reason need some more work so to the point monitor monitoring that helps you go back to sleep and with that I will hand the mic back to [Applause] thanks anchor let's recap we took a look at how easy it is to launch data for pipelines using beautiful templates we deep dive into two specific technical benefits of cloud dataflow dynamic load rebalancing and auto scaling and we also heard from breitkopf how easy it was to build a video analytics solution using data file [Music] you 