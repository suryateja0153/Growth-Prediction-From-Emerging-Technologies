 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu so be talking about my work for the past 11 years so this has been certainly exciting but it also was long in duration so we had to sort of stick to the goal and I'll show you also a couple of things I mean most of this work has been possible because we have a team of people they contributed to both the design of the robot and the research we're doing on the robot so being freely drawing from the work of these other people just sighted them as the iCub team because I couldn't list everybody there but you see a picture later this shows how many people were actually involved in developing this robot so our let's say goal although we didn't start it like these is to build robots they can interact with people and maybe one day be commercially available to be deployed in a household everything we done is on the design of the robot has to do with a platform capable of interacting with people in a natural way and and this is reflected in the shape of the robot that is humanoid it's reflected on the type of skills we try to implement on the robot and overall on the design the platform itself in terms of strength in terms of sensors and so forth there was and let's say hidden reason we wanted to design a platform for research also some when we started we didn't think of a specific application our idea was to have a robot as complicated as possible to give researchers the possibilities of doing whatever they like so the robot can walk as cameras tactile sensors it can manipulate objects we put a lot of effort into this design of the hands and it's complicated and breaks often so it's not necessarily the best platform but it's the I believe the only platform that can provide you with manipulation and at the same time with a sophisticated called motor system in the eyes and cameras and ma B doesn't give you lasers so you have to do with their vision the result is this platform they show here this started as a European project so there was an initial funding that allowed for basically hiring people to design the mechanics and the electronics so the robot and unfortunately the robot is not very cheap we I mean the the overall we try to put the best components everywhere and this is reflected in the cost which doesn't help diffusion to certain extent in spite of these we managed to let's say cell which between quotes because we don't make any profit out of it 30 copies of the robot there's still two of them to be delivered this year so there are at the moment 28 around there and four of them are in our lab and are used daily by our researchers and and given the complexity of the platform we managed at best to build for robots per year and at best means that we're always late in constructions we're always late in fixing the robots and that's because I mean we are a research lab trying also to do to have these the same more commercial side or support side to the community of users which in fact doesn't work I mean you cannot ask your PhD students to go and fix a robot somewhere in the world it was striking a bit the we managed to actually sell a robot in Japan and that's because you know you see Japan as the place of human robots and having somebody asking a copy of our robot there it was a bit strange but nonetheless and the project is completely open source if you go to our website you can download all the CAD files for the mechanics for the electronics all the schematics and the entire software from the lowest possible level up to whatever latest research has been developed by our students why we think the robot is special as I said we wanted to have hands and we put considerable effort into the design of the hands there are nine motors driving each end and although there are five fingers and 19 joints which means some the joints are coupled so the actual dexterity of the kind is soul to be demonstrated but it it works to a certain extent there are so sensors it's entirely human-like we don't have for instance I say we don't have lasers we don't have ultra sound or other fancy sensors that from an engineering standpoint can also be integrated but we decide to stick to certain subset of possible sensors there's one thing that I think is quite unique we managed a longer way to run a project to design tactile sensors and so I think it's this one of the few robots that are almost complete body coverage with tactile sensors there are about 4,000 sensing points in the latest version and we hope to be able to use them I mean you'll see certain things that we start developing but for instance we there was a discussion about manipulation and the availability of tactile sensors we just scratch the surface in that direction we haven't been able to do much more than that as I said II we designed also the electronics and the reason for doing this was that wanted to be able to program the very low level of the controller so the robot this didn't pay off for many years but a certain point we started doing tour control and we started hacking also the low level controllers of the brushless motors and so it paid off eventually because we that wouldn't be impossible without the ability to write the low level software not that many people are modifying that part of the software it's open-source also that part but it's very easy to burn your amplifiers if you don't do the right thing at that level and the other thing is that as I said the platform is reproducible and the moment that our github repository well a number of github repositories which contain whatever some a few millions or lines of code whatever it means just means probably that a lot of students are just committing to the repository is not necessarily the software is super high quality at this point there are a few modules that are well maintained and that's the low-level interfaces which is something we do everything else can be all different in different ranges of readiness to be used in a sense well why humanoids the were illicit beginning scientific reasons one paraphrasing Rob Brooks paper helices on play chess a reason of you know developing intelligence in a robot era human-shaped may give an intelligence that is also comparable to humans but also provides the natural natural human robot interaction the fact that robot can move the eyes is very important for instance has a very simple face but it's effective in communicating something to the people the robot is interacting with and also building a humanoid of the small size the robot is only a meter tall was very challenging from the mechatronics point of view so for us engineers was a lot of fun to the initial few years when we were designing every day was very very funny our a lot of satisfaction seeing that the robot was growing and being the eventually the fact that the platform is open-source I think it's also important allows for repeating experiments across different in different locations so we can develop a piece of software and run exactly the same module somewhere else across the world and this may again give advantages in first of all debugging was a lot easier so many people complaining when we do when we did something wrong and allowed for also let's say shared development so building partnerships with many people mostly across Europe because there was funding available so for people to work together and and this may eventually enable better benchmarking and better quality of what we do as part of the project we also develop middleware so maybe you may think we've been a bit crazy we went from the mechanical design to the research on the robot and passing through the software development but actually this was a middleware that was started before Ross even existed and in fact it was a piece of my work at MIT with a couple of the students there in 2001 2002 so the first version actually ran on cog and run on KNX a real-time operating system later we did a major porting to Linux and Windows and Mac OS which so we'd never committed to a single version and that because we had this community of developers from from the very beginning and there was no agreement on what development tool to use and so we say why don't we cover almost everything in this part of the software is actually very solid at the moment that this has been you know growing not in size but in in quality in this case so the interfaces remain practically the same and I think the low-level byte coding or the messages passing across the network didn't change since the clock time everything else changed is completely new implementation now and butter has portability so as I say the this was a sort of requirement from the researchers not to commit to anything and so we have you know developers using Visual Studio on Windows or maybe using GCC in Windows and other developers running whatever IDE available on Linux or Mac OS and this worked pretty well and there's also language portability we can link so all these middleware is just a set of libraries so we can link the libraries against any language and so we have bindings for whatever Java Perl MATLAB and a bunch of other languages and this helped researchers also to do some rapid prototyping maybe using Python and so forth they said that the project's open source so you'll find if you go to the website this emmanuel not particularly well taken care of it works at least it works with with our students so it should work for everybody and but they're also the drawings so you can go with drawings like those too mechanical workshop and you get the parts in return and then from those you can also figure out how to assemble the components although it's not super easy it's not something you do just because you have the drawings you do in your basement I mean one of the groups in one of our projects try doing that and I think they stopped after building part of an arm and maybe part of a leg that I mean was very challenging for them and you need a very I say a proper worshop for building the components so it takes time anyway continuing on the sensors I mentioned that we have skin and I'll show you a bit more about that in a moment but we also have four source sensors in gyroscopes and accelerometers so if you take all these pieces and you put them together you can actually sense interaction forces with the environment and if you can sense the interaction forces you can make the robot compliant and this has been an important development across the past few years that are allowed the robot to move from position control to torque control and this has been needed again to go in the direction of human robot interaction and so that these are standard for sort of sensors although we designed them as usual we spent some time in designed the sensors and this was a reason of cost the equivalent six axial force sensor commercially cost I don't know five thousand bucks and we managed to build it for one thousand so it's maybe it's not as super rock-solid as the commercial component but it works well in about the skin this was a sensing modality that wasn't available and again we managed to get funding for actually running a project for three years the design the scheme for the robot and we thought it was a trivial problem because at the beginning of the project we already had the idea of using capacitive sensing and we actually had the prototype and we say oh it's 3d elderness we spent three years to actually engineer it to make it work appropriately on the robot so the idea is three deal so since capita capacitive sensing is available for cell phones we thought of moving that into a version that would work for the robot there were two issues first of all the robot is not flat so we can just stick cell phones on the robot body to attain tactile sensing so we had to make everything flexib so they can be conformed to the surface of the robot the other thing is that the cell phones only sends objects that are electrically conductive that's because the way the sensor is designed so we had to change that because the robot might be hitting objects that are not there are plastic for instance so what we done was to actually build the capacitors over two layers this an outer layer and a set of sensors that are etched on a flexible PCB they're shown there and what the sensor measures is actually the deflection of the outer layer which is conductive towards the sensors and in between we have another flexible material and that's another part of the reason why it took so long we started with materials like silicon that were very nice but unfortunately they degrade very quickly so we ended up running sensors for a couple of months and then all of a sudden they started failing or changing the measurement properties we didn't know why we started investigating the all possible materials until we found one that was actually working well the other solution we had to basically design was the shape of the flexible PCB so we had the challenge of taking four thousand sensors and bringing all the signals somewhere to the main CPU inside the robot and and of course you cannot just connect four thousand wires so what we've done on the back side of the PCB this is actually a routing for all the sensors from one triangle to the next until you get to digitizing unions and sorry each triangle digitizes all signals and they travel digital foreign from one triangle to the next until the reach and microcontroller that takes all these numbers and sends them to the main CPU and this says the connection side so it actually enables the installation of the skin on the robot so this is a they say industrialized version of the skin and that's a customization we done for a barrette and those are part of the scheme for the iCub so the components that we just screw onto the akka body and to send make them or to make the iCub sensitive this is another solution which is again capacitive for the fingertips simply because the triangle was too big too large for it the size of the iCub fingertips but principles exactly the same it was just more difficult to design them this flexible materials because they they just more complicated to fabricate on those small sizes and the result when you combine the force or sensors and the tactile sensors is something like this which is a compliant controller on the iCub where you can just push the robot around this is a zero gravity modality so you can just push the robot around and move it freely and this has to be compared to the complete stiffness in case you do position control and another thing that is enabled by force control is teaching my demonstration this is a trivial experiment we just recorded the trajectory and repeated exactly the same trajectory so it's not it's not I mean you can do learning on top of that but we haven't done it it's just to show that the fattie can control the robot in torque mode enables these type of tasks so teaching a new trajectory was never seen by the robot there's another less trivial a thing you can do since we can sense external forces you can do something like this which is we can build a controller where you keep the robot compliant you impose certain constraints on the center of mass and the angular momentum and keep the robot basically stable in a configuration like this one in spite of external forces being in this case generated by a person these are this is part of a project is basically try to make the iCub walk more or less efficiently and as part of the project we actually also redesign the ankles or the robot because initially we didn't think of bipedal walking and so they weren't strong enough to support the the weight of the robot and this is basically the same the same stuff that was shown on the on the previous videos just same combination of tactile and force or sensing used to estimate contact forces we actually added two more force or sensors in the ankles so we have six overall here and this version of the problem now as part to this we also played a bit with machine learning and this the fact we okay for mapping the tactile information and for sensor information to the joints since they're not localized on the joints to the robot we have and also for separating what we measuring with the sensors from the forces generated by the movement of the robot by internal dynamics we have to have information about the probable dynamics and this is something we can do or we can build a model for using machine learning since we have measurements from the joint position velocities and accelerations and the torques measured from the four store sensors we can compute the robot dynamics and this can be done either using pre say computed model from the card or from learning the model via machine learning and so we collect a data set on the iCub in this case was a data set for the arm for a first four joints we didn't do anything from the wrist and in this case we used we sort of customized a specific method which is Gaussian processes to be incremental and also to be computationally bounded in time so we wanted to avoid dysplasia of the computational time due to the increase in the number of samples and this was well was basically interesting piece of work because everything we do on the robot if is inserted in a control loop as to have a predictable computation time and possibly limited enough so that we can run the control loop reasonable rates and this is some of the results and actually we also compare with sort of other existing metals this is just to show that the metal we developed which uses an approximated Colonel works pretty much as as well as standard Gaussian process regression in this case and worse much better than other metals from the literature this was just to have a rough idea that this was entirely doable also by shaping the kernel is possible to compensate for temperature drifts fortunately the force or sensors tend to change response due to temperature not that the lab is changing temperature but often the electronics itself is heating up around the robot so it's making the sensor read something different and but is possible to show that a game through learning you can build the compensation also for the temperature variations just by shaping the kernel to include a term that depends on time this one example of how we done machine learning on the robot although the problem is fairly simple problem there is more complicated is learning about objects so in the scenario we targeting is shown here where we have basically a person that can speak to the robot tell the robot there is new object and robots acquiring images and we hope to be able to learn about objects from just from this type of images this is a maybe the most difficult situation we can also lie objects on the table and just tell the robot to look at specific objects and so forth again the speech interface is it's nice because you can basically also attach labels to the objects that are what is seeing the metals we tried in the recent past we've done we basically applied sparks coding and and then regularized least square for classification this was basically how we started a couple of years ago and more recently we used an off-the-shelf configuration convolution sorry convolutional neural network and again the classifiers are linear classifiers and these I mean as proved to work particularly well but also since we are on the robot we can say play tricks one trick that is easy to apply and it's very effective is actually you see in an object but you don't have a single frame you can actually take subsequent frames because the robot may be observing the robot the object for a few moments four seconds whatever and in fact there's an improvement that is shown this plot there the one on to the right if you increase the I say the number of seconds you are allowed to observe the object you improve also performance and the plot is over the number of classes because we also like to improve on the number of classes there are robots can actually recognize and which was limited until let's say a couple of years ago but now with all these new deep learning stuff seems to be improving quite a lot and that this are experiments in that direction there's another thing that can be done which is try to see what happens if we have since we have again the robot interacting with people for entire days if we collect images in different days and and then we can play with different conditions on the stinkies so for instance the different plots here show what happens if you train and test on the current day so you train cumulatively on up to four days and you test on the last day only and you see of course performance improve as you increase the train set conditions may be slightly different from one day to the next light may have changed just because it was more more a sunny day or a cloudy day and the the other conditions are to test also on past days or to test on future days so where conditions may may have changed a lot and in fact performance is slightly worse in that situation okay and this is a video that shows basically the robot training and some of the experiment on testing how the robot world perceives a number of objects and fortunately there's no speech here but it's basically a person talking to the robot and telling the robot world what is the name for the specific object then putting another object there drawing the robot attention to the object and again telling name this is the Lego becomes faster in a moment okay and then you can continue training basically like that and in the video shows also testing while showing a bunch of objects simultaneously to the robot and here we simply click on one of the objects to draw the robot attention and on the plot there you see the probability that given objects being recognized as the correct one okay I think of to cut this short because I'm running out of time another thing I wanted to show you is basically now we have these ability to control the robot we have the ability to recognize objects we also have the ability to grasp objects in this something that uses stereo vision and in this case what we wanted to do is to present an object to the robot no prior knowledge about the shape of the object we take a snapshot we reconstruct a stereo pair we reconstruct the object in 3d and and then we apply optimization constraint optimization to figure out a plausible location for the palm of the hand and then that will maximize the ability to grasp the object by closing the finger around that particular position thus our let's say definition of power grasp so put a part of the hand of the robot in a region of the object that has a surface which has a similar shape or a similar size of the firm itself and with the orientation is compatible with the local orientation of the surface and this works we mixed results so it works with certain objects doesn't work always there are objects that are in Sycamore difficult for this procedure so some of them will only be grasped with 65% probability which is not super satisfactory if you run long experiments you want to grasp three four objects you start seeing failures it becomes boring to actually do the experiments these are so works well for soft objects for instance as expected we moved a bit into the direction of using the tactile sensors and but at this point we only been able to try to characterize forces out to the force of the tactile sensor measurements so we basically taking the fingertip we have twelve sensors and we trying to and this is another case we we apply machine learning trying to reconstruct the force direction and intensity from the tactile sensor measurements and this basically the procedure is a we take the sensor we move over a six axial force or sensor we take the data and we approximate this again with the Gaussian process just one last video if I can okay so basically put together all these skills we may be able to do something useful with the robot in this case the video shows a task with a robot a screen table and it's actually using the grass component and in the ability to move the object to see the object recognize them and draws them and put them at the given location which was pre specified in this case so it's not recognized this container is just putting things there and there's one last skill that I didn't have time to talk about which is recognizing certain objects as tools in one specific object like this one go an object like to here can actually be used for pulling another object closer and and this again something that can be done through a learning so we learned the size of the sticks or set of sticks and we also learn how good they are for pulling Santa closer through experience by basically trying there or over many trials and the result is that it can actually generate movement push the object closer so they can later be grasped and and that's basically a couple of ideas on how to exploit the object affordance is not just recognizing that but also knowing the certain objects have certain extra functions which may end up being useful okay just wanted to acknowledge the people that actually working on all these I promise that will do that and it's actually the photo around Genoa showing the group has been mainly working on on the iCub project over let's say this is a group last year so there may be more people there just left or some of them moved to MIT okay thank you [Applause] you 