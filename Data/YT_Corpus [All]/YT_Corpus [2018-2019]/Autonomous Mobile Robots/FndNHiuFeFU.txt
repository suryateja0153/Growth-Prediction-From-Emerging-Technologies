 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu I've been getting to play with this robot for a few years now three years of my life basically devoted to that robot it was one of the most exciting technically challenging exhausting stressful but ultimately fulfilling things I've ever done we got to basically take this robot make it drive a car get out of the car that was tough open the door turned valves pickup drill cut a hole out of the wall notice there's no safety harness its battery autonomous it has a walk over some rough terrain climb some stairs at the end it had to do this in front of an audience with basically we got two tries and if your robot breaks the breaks right and there was a there was a two million dollar prize at the end right we we wanted to do it out for the two million dollar prize but for the technical challenge and myself and a group of students just like I said absolutely devoted our lives to this we spent all of our waking hours on this we worked incredibly incredibly hard so just to give you a little bit of context DARPA are our National Defense funding agency has gotten excited about the idea of these grand challenges which get people to work really really hard the self-driving cars were were the first one and MIT had a very successful team in the urban challenge led by John and then it's unquestionably had transition impact into the world via Google uber apple and John John will tell you all about it I think in 2012 DARPA was scratching their head saying people haven't worked hard enough and you know what's the new challenge going to be and and right around that time there was a disaster that that maybe helped focus their attention towards disaster response so in ultimately it was October 2012 that everything started with this kickoff for the DARPA Robotics Challenge the official challenge was was cast in the in the light of disaster response in the the scenario of the nuclear disaster as a backdrop but I think really their goal was to evaluate in advance state-of-the-art in mobile manipulation okay so if I'm the funding agency what I think is that you see Hardware coming out of industry that's fantastic so Boston Dynamics was building these these walking robots and the like this one is the one we've been playing with Atlas built by Boston Dynamics which is now Google oh yeah and and then I think from the research labs we've been seeing really sophisticated algorithms coming out but on relatively modest hardware and I think it was time for a mashup right so they were they were very interesting in the way they set up the competition it wasn't about making it a completely autonomous robot it was there was there was a twist you could have a human operator but they wanted to encourage autonomy so what they did is they had a degraded network link between the human and the robot and some reward for going a little bit faster than the other guy so the idea would be that if you had to stop and and work over the degraded network link and control every joint of your robot then you're gonna be slower than the guy whose robot is making the decisions by itself that didn't play out as much as we expected but that was that was the set up that set up a spectrum where people could do to fully full teleoperation meaning joystick control of each of the joints if they wanted to you know and maybe the goal is to have complete autonomy and you could pick your place on the spectrum right so MIT possibly to a fault aimed for the full autonomy side the idea was was let's just get a few clicks of information from the human eye let the humans solve the really really hard problems that he could solve efficiently object recognition scene understanding we don't have to to do that a few clicks from the human can communicate that but the robot do all the dynamics and control and planning side of things okay so those few clicks should cede nearly autonomous algorithms for perception planning and control okay so technically I I don't I don't intend to go into too many details but I would love to answer questions if you guys ask and we can talk as much as we want about it but but the theme the overarching theme to our approach is when we're controlling perceiving everything is to formulate everything is an optimization problem so even sort of the simplest example in robotics is is the inverse can Maddox problem where you're just trying to decide if I want to put my hand in some particular place I have to figure out if I have a goal in the world coordinates I to figure out what the joint coordinates should be to make that happen okay so we have joint positions in some vector q and we just say I'd like to be as close as possible I have some comfortable position for my robot then we formulate the problem as an optimization say I'd like to be as close to comfortable as possible and some in some simple cost function and then I'm gonna start putting in constraints like my hand is in the desired configuration but we have very advanced constraints so so especially for the balancing humanoid we can say for instance that the center mass has to be inside the support polygon we can say we're about to manipulate something so I'd like the thing I'm going to manipulate to be in the cone of visibility of my sensors of my vision sensors I'd like my hand to approach it doesn't matter where it approaches along the table maybe but it should be might the palm should be orthogonal to the table and should approach you know like this and we can we put in a you know sort of more and more sophisticated collision avoidance type constraints and everything like this and the optimization framework is general and it can and it can accept those type of constraints and then we can solve them extremely efficiently with with highly optimized algorithms so for instance that helped us with what I like to call the big robot little car problem so we have a very big robot it's a 400-pound six-foot-something machine and they asked us to drive a very little car okay so to the point where the robot physically does not fit behind the steering wheel impossible it just doesn't fit kinematically torsos too big steering wheels right there no chance so you have to drive from the passenger seat you have to put your foot over the console you have to drive like this and then our only option was to get out of the passenger side okay so that was you know a hard problem kinematically but we have this rich library of optimizations we can drag it around we can explore different kinematic configurations of the robot but we also use the same language of optimization and constraints and then we put in the dynamics of the robot is another constraint and we can start doing efficient dynamic motion planning with the same tools so for instance if we wanted Atlas to suddenly start jumping off cinder blocks or or or running we did a lot of work in the regarde to make our optimization algorithms efficient enough to scale to very complex motions that could be planned on the fly at interactive rates so one of the things you might might be familiar with is sort of a Honda azzam oh no the famous robots that walks around like this and it does it's it's a beautiful machine they are extremely good at real-time planning using limiting assumptions of keeping your center of mass at a constant height and things like this and one of the questions we asked is could we take some of the insights that have worked so well on those robots and generalize them to more general dynamic tasks and one of the big ideas I want to try to communicate quickly is that even though our robot is extremely complicated there's sort of a low dimensional problem sitting inside the big high dimensional problem right so I start worrying about every joint angle in my hand while I'm thinking about walking I'm dead right so but actually when you're thinking about walking even doing gymnastics or something like this I think the fundamental representation is the dynamics of your center of mass your angular momentum sort of some bulk dynamics of your robot and the contact forces you're exerting on the world which are also constrained and in this sort of six dimensional twelve dimensional uh if you'd have velocities space with with these relatively limited constraints you can actually do very efficient planning and then map that in a second pass back to the full you know figure out what my pinky's gonna do okay so we do that we spend a lot of time doing that and we can now plan motions for complicated humanoids that were far beyond our ability to do it a few years ago this was a major effort for us my kids and I were watching American Ninja Warrior at the time so we we did all these other Ninja Warrior tasks so you know that was that was there was some algorithmic ideas that were required for that it was also just a software engineering exercise to build a dynamics engine that was that provided analytical gradients exposed all the sparsity and the problem and and wrote custom solvers and things like that to make that work it's not just about humanoids we spent you know a day after we got as I'm was doing those those things to show that we could make a quadruped ed run around using the same exact algorithms it took literally less than a data to make all these examples work um there's another level of optimization that's plot that's goin kicking around in here so the humanoid in some sense when it's moving around is a fairly continuous dynamical system there's punctuation that when your foot hits the ground or something like this so you think of that as sort of a smooth optimization problem there's also a discrete optimization problem sitting in there too even for walking so if you think about it you know the methods I just talked about we're really talking about you know okay I moved like this I want to prefer I would prefer to move something like this but there's a continuum of solutions I could possibly take for walking there's also this problem of just saying you know am I going to move my right foot first or my left foot first am I gonna step on cinder block one or cinder block two right there's that there really is a discrete problem which gives a combinatorial problem if you have to make long-term decisions on that and one of the things we've tried to do well is is be very explicit about modeling the discrete aspects and the continuous aspects of that problem individually and and using the solvers the right solvers that could think about both of those together so here's an example of how we do interactive footstep planning with the robot if it's standing in front of so some perceived cinder blocks for instance the human can quickly label discrete regions just by moving a mouse around the regions that come out are actually fit by an algorithm they're deciding there's they look small because they're trying to figure out where if the center of the foot was inside that region the whole foot would fit on the right on that okay and they're also think about balance constraints and other things like that but that we have discrete regions to possibly step in we have a combinatorial problem and the smooth problem of moving my center of mass and the like and we have very good news solvers to do that and you know seated inside that hour I just want to sort of communicate that there's all these little technical nuggets of we had to find a new way to make really fast approximations of big convex regions of free space so you know we have optimizations that just figured out the the problem of finding the biggest polygon that fits inside all those obstacles is np-hard we're not going to solve that but it turns out finding a pretty good polygon can be done extremely fast now and particular way we did it scales to very high dimensions and complicated obstacles to the point where we could do it on raw sensor data and that was that was an enabling technology for us so our robot now when it's making plans so the one on the left is just walking towards the gold okay the one on the right we removed a cinder block and normally a robot would kind of get confused and stuff because it's just thinking about this local plan local plan local plan that's it you know it wouldn't be able to stop and go completely the other direction but now since we have this higher-level combinatorial planning on top we can we can make these big long-term decision-making tasks and at interactive rates so we we also the robot was too big to walk through a door so we had to walk sideways through a door and that was sort of a standing challenge for the guy who started the the program putting footsteps down by hand said whatever I do in footstep planning it will I will never lay down foot steps to walk through a door again that was this challenge we did a lot of work on the balancing control for the robot so it's a force controlled robot using hydraulic actuators everywhere I will again I will go to the details but we thought a lot about the dynamics of the robot how do you cast that as an efficient optimization that we can solve on the fly okay and we were solving an optimization at a kilohertz to balance the robot okay so you put it all together and as a as a basic competency you know how well does our robot walk around and balance here's sort of a one of the examples at a normal speed from the from the challenge so the robot just puts its foot steps down ahead the operator is mostly just watching giving high-level directions I want to go over here the robots doing its own thing now all the other teams I know about we're putting down the foot steps by hand on the on the obstacles I don't know if someone else was doing it autonomously we you know we chose to do it autonomously we were a little bit faster because of it but I don't know if it was enabling but very proud of our of our walking even though it's still conservative mean this is lousy compared to a human yeah so so we knew they were going to be cinder blocks we didn't know the orientation or positions are them so we had a cinder block fitting algorithm that would run on the fly snap things into place with the cameras yeah yeah actually laser scanner and then we walk up stairs you know little things if you care about walking you know the heels are hanging off the back you know they're you know there's special algorithms and they're sort of to the balance on partial foot contact and things like that and that made the difference we could go up there and efficiently robustly so I would say though you know for for conservative walking we it really works well they don't we could we could plan these things on the fly and we also had this user interface that if the footstep planner ever did something stupid the human could chris drag a foot around add a new constraint to the solver ii would continue to solve with a new constraint and adjust its solutions we could do more dynamic plans we could have it run and everything like that we actually never tried this on the robot before the competition because we were terrified of breaking the robot and we didn't have couldn't accept the down time but now that the competition is over this is exactly what we're trying but the you know the optimizations are slower and didn't always succeed so so in the real scenario we were using our we were putting some more constraints on and doing much more conservative gates the balance control I'd say worked extremely well you know so the hardest task was this getting out of the car tasks we worked like crazy we didn't work on it until the end I thought I thought DARPA was gonna scratch it honestly and then but in the last month it was B was became clear that we had to do it and then we spent a lot of effort on it and you know we put the car in every possible situation this was on cinder blocks it's way high you know it has to step down beyond its reach ability in the leg you know this thing was just super solid you know so Andres and Lucas were the main designers of this algorithm you see it's doing i'd say it's super human in this regard right now you know one on a human would not would not do that okay you know but standing on one foot while someone's jumping on the car like this you know it's it's it's really works well in fact the hardest part of that for the algorithm was the fact that the it's trying to find out where the ground is and the camera is going like this right so that was the reason that it has this long pause before it went down okay but there was one time that it didn't work well okay so and it's hard for me to watch this but you know it turns out on the front you saw that little dead kick okay this was horrible okay so exactly what happened but I think it really exposed the limitation of our of the tools the state of the art you know so what what happened in that particular situation was the robot was almost autonomous in some ways and we basically tried to have the human have to do almost nothing and in the end the we got that humans checklist down to about five items which was probably a mistake because we screwed up on the checklist so so one of the things I Tomas - we have one set of programs that are running when the robots driving the car and then all the human had to do was turn off the driving controller and turn on the balancing controller but who was exciting in the first day of the competition and then and we've turned on the balancing controller forgot to turn off the driving controller so the ankle was still trying to drive the car even that the controller was robust enough so you know I really think there's this fundamental thing that if we were if you're close to your nominal plan things are very robust but what happened is the ankle is still driving the car I think we could balance with the ankle doing the wrong thing except the ankle did the wrong thing just enough that the tailbone hit the seat of the car that was no longer something we could handle right so there was no contact sensor in the but that meant the dynamics model was very wrong the state estimator got very confused the foot came off the ground of the state estimator had an assumption that the feet should be on the that's how it knew where it was in the world and that basically the controller was hosed right and that was sort of the only time we could have done that badly and I think that the vibrations and everything I had emails from people of all walks of life telling me what they thought was wrong with the brain of the robot from shaking like that but that was that was a bad thing okay so you know I think fundamentally if we're thinking about plans and that's what we know how to do at scale for high dimensional systems as single solutions then we're close to the plan things are good when we're far from the plan we're not very good and a change in the contact situation even if it's sort of hurt in a Cartesian space very close change in the contact situation is as a big change to the plan there's lots of ways to address it we're doing all of them now you know it's all fundamentally about robustness but ironically the car was the only time we could have done that badly right so we every other place we worked out all these situations where okay the robots walking and then something bad happens and and you know someone lamp you know Lance's you or something you know we had recovery and then even if this even it tried to take a step even if that failed it would go into her a gentle mode where protect its hands because we were afraid to break my hands it would fall very gently to the ground all that was good we turned it off exactly once in the competition we turned it off when we were in the car because we can't take a step to recover when you're in the car and you're the same size of the car and we didn't even want to protect our hands because we didn't wanna get hit we once we got our hands stuck in the steering wheel and Reaper so yeah so anyways that was the only time we could have sort of shaken ourselves silly and fallen you know and what happened it we fell down with our 400 pound robot we broke the arm the left arm or the right arm sadly all of our practices ever we're doing all the tasks right handed but we got to show off a different form of robustness so we actually because we had so much autonomy in the system we flipped a bit and said let's use the left arm for everything which is more than just you know map the joint chord it's over here I meant and then you had to walk up to the door on the other side of the door you know and you had it really backs the implications back up quite a bit and we were able to after having our arm just completely hosed we're able to go through and do all the rest of the tasks except for the drill which required to hear we couldn't do that one we had to pick up the drill and turn it on so we ended the day in second place with the you know with a different display of robustness we were happy but not as happy as if we had not fallen so okay so I think walking around balancing you know we're pretty good but there's a limitation I really do think that's that's a fundamental everybody has that limitation to some extent the manipulation capabilities of the robot were we're pretty limited just because we didn't need to do it for the challenge we know we had the manipulation requirements were minimal you know you had to open doors picking up a drill was the most complicated thing we actually had a lot of really nice robotic hands to play with but they all broke when you started really running them through these hard tests so we ended up with these sort of lobster-claw kind of grippers because they didn't break and they were robust and they worked well but it limited what we could do in manipulation again the planning worked very well we could even we could pick up a board and even plan to make sure that the board now you know didn't intersect with other place boards in the world and we have really good planning capabilities and those worked at interactive rates the kinematic plans but the grasping was was open-loop so there's really no feedback so there's current sensing just to not over to overheat the hands but basically you do a lot of thinking to figure out how to get your hand near the board and then you kind of close your eyes and go like this and hope it lands in the in the board and my in the hand and most of the time it does every once in a while it doesn't you know we we experimented with every touch sensor we could get our hands on that wasn't meant to be a pun and we tried cameras and everything but they were all just too fragile and difficult to use further for the competition we're doing a lot of work now doing optimization for grasping but I'll skip over that for time and so the other piece was hard so how does the human come into the to the perception side of the story right so one of these tasks was moving debris out from in front of the door and this is sort of what it looked like actually in the trot in the in the original version of the competition the trials would come up and throw these boards out of the way and you see the human operators over there with their you know big console of this displays this is what the laser in the robots head sees we have a spinning laser we also have stereo vision but the laser reconstruction of this gives you a mess of points if you asked a vision algorithm you have some of you are vision experts I'm sure in the room if you asked a vision algorithm to figure out what's going on and that's massive points it's extremely hard problem but we have a human in the loop so the idea is that one or two clicks from a human can turn that from an intractable problem to a pretty simple problem right just say there's a 2x4 here and then now a local search can do sort of ransacked type local optimizations to find that the the best fit to a 2x4 to that local group of points and that networks well and so the robot didn't have to think about the messy point clouds when it's doing its planning it could think about the simplified geometry from the CAD models and all the most of the planning was just on the the CAD models so this is what it looks like to drive the robot so the robot you click somewhere saying there's a valve then the perception algorithm finds a valve then the robot starts going it actually shows you a ghost of what it's about to do and then if you're happy with it and if all things are going well you just watch but if it looks like it's about to do something stupid you can come in stop interact change the plans and and let it do its thing it's kind of fun to watch the view robot view of the world right so this is sort of you know what the robot sees it throws down its foot steps it's deciding how to walk up to that valve you know again so when when the right arm was broken this was one of our practice runs but with right arm was broken it had a valve we had a bit flipped now it had to walk over to the other side of the valve and you know there's a lot of things going on a lot of pieces had to work well together to make all this work one of the questions that I'll I'll get before you ask it you've written it down okay yeah that's fine why were the robots so slow why were they standing still a lot of people out there waiting for the human maybe a lot but for us it wasn't it wasn't the planning time the planning algorithms were super fast most of the time we were waiting for sensor data and that meant there was two things there was waiting for the laser to go spin out completely around and also just being conservative or wanting to get that laser data while the robot was stopped and then there was getting the laser data back to the computer had the fast planning algorithms and back so if there was a network blackout we had to wait a little bit and that meant we were standing still but we've actually done a lot of work in lab to show that we don't have to stand still this is now the robot walking with its laser blindfolded and using only stereo vision using one of the capabilities that came out of John's lab and others to do connect stereo fusion so that compared the laser gives gives very accurate points but it gives them slowly you know low radon you have to wait for it to spin around the the camera is very dense very high rate but very noisy and John and others have developed these new algorithms that can do real-time filtering of that noisy data and we demonstrated that that was they were good enough to do walking on and so we'd put all the pieces together real time implant foot step planning real sight balancing real-time perception and we were able to show we can walk continuously this will be the future so you know we had to do networking we've optimized network systems we had to you know build servers unit test logistics politics you know it was exhausting but I think it was overall incredibly good experience a huge success I think the robots can move faster with only small changes mostly in the perception side you know the walking was sufficient we can definitely do better the the manipulation was very basic I think we need to do better there we didn't have to for those tasks the robustness dominated everything so I'll just end and take questions but I'll show this sort of fun again robot view of this is the robots you know God's eye view of the world as well it's doing all these tasks you can sort of see what the robot labels with geometry and what it what it's leaving is points and it's just kind of fun to have on the background and and I'll I'll take any questions yeah you you 