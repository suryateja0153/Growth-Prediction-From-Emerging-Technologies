 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu ok thanks thanks for opportunity to talk so hi everyone it's a it's a great pleasure to talk here at MBL I've been coming to the Woods Hole Oceanographic Institution for many years as my first thing over here at MBL and so I'm gonna try to cover three different topics which is probably a little ambitious on time but there's so much I'd love to say to you I want to talk about self-driving cars and use it as a context to think about questions of representation for localization and mapping and maybe connected into some of the brain questions that you folks are interested in and time permitting at the ends mentioned a little bit of work we've done on object based mapping in my lab so my background I grew up in Philadelphia I went to UPenn for engineering but then I went to Oxford to do my PhD at a very exciting time when the computer vision and robotics group was just being formed at Oxford under a Michael Brady and then I came back to MIT and started working with underwater vehicles and that's when I got involved with Woods Hole Oceanographic Institution and I was very fortunate to join the AI lab back around 2002 and then which which became part of csail and and really I've been able to work with really amazing colleagues and amazing amazing robots and a challenging set of environments so autonomous underwater vehicles provide a very unique challenge because we have very poor communications to them typically we use acoustic modems that might give you 96 bytes if you're lucky every 10 seconds to a few kilometers range and so we and we also need to think about the sort of constraints of running in real time on board a vehicle and so and the the sort of work that my labs done on the more we investigate more fundamental questions about robot perception navigation and mapping we also are involved in building systems so this is a project I did for the also Naval Research some years ago you small vehicles that would reacquire mind like targets on the bottom for the Navy and so this is an example of a more applied system where we had a very small resource constraint platform and the sort of work we did as a robot built a map as it can perform its mission and then match the map against a prior map to do terminal guidance to a target another big system I was involved with is Russ mentioned as the urban challenge and I'll say a bit about that in the context of of self-driving cars so let's see so who's heard any of the recent statements from Elon Musk from text Tesla so you know he said self-driving cars are solved he said and a particular thing that he said that just made my I don't know maybe steam came out of my head was that he compared autonomous cars with elevators that used to require operators but are now self-service so imagine you know getting in a car pressing a button and arriving at MIT in Cambridge you know 80 miles away navigating through you know the Boston downtown highways and intersections and maybe that will happen but I think it's going to take a lot longer than folks are saying and some of that comes from just fundamental questions and intelligence and robotics so in a nutshell when musk says that self-driving is solved i I think he's wrong and as much as I admire what Tesla and SpaceX have done and so so to talk about that instead of I think we need to be very honest as a field about our failures as well as our successes and try to balance what you hear in the media with with the reality of where I think we are and so I wanted to quote verbatim what Russ said about the DARPA Robotics Challenge about a project that was so exhausting and it's just just all-consuming and so stressful yet so rewarding so we did this in 2006 and 2007 my wonderful colleague Steph teller John Howe Amelio for Thole amazing students and postdocs we had a very large team and we tried to push the limit on what was possible with perception and real-time motion planning so our vehicle built a local map as it traveled from its perceptual data using data from laser scanners and and cameras and we didn't want to blindly follow GPS we wanted the car to make zone decisions because GPS navigation was part of the original quest with the challenge and so we my teller and a student Albert laning developed a vision based perceptual system where the car tried to detect from curbs and lane markings and very challenging vision conditions for example looking into the Sun which you'll see in a second in a really challenging situation for trying to perceive the world and so our vehicle at the time we we went a little crazy on computation we had 10 blades each with 4 cores for 40 cores which may not seem a lot now but we needed 3.5 kilowatts just to power the computer at full tilt we fully loaded the computer with a randomized motion plan and with all these perception algorithms we had a valid ein laser scanner on the roof and about 12 other laser scanners 5 cameras 15 radars and we really pushed the envelope on algorithms and and so when faced with a choice in a DARPA challenge if you want to win it all costs you might simplify or try to read the rules carefully or guess the rule simplifications but that would have meant just sort of turning off the work of our PhD students and we didn't want to do that so at the end of the day all credit to the teams that did well Carnegie Mellon first $2,000,000 Stanford second $1,000,000 Virginia Tech third half-a-million dollars MIT 4th and nothing for fourth place but it was quite an amazing experience and in this period of advertising our our failures I think I've time to show this this used to be painful for me to watch but now I've gotten over it this is our we had even though we finished the race we had a few incidents of DARPA stop things to let us continue that's Carnegie Mellon who won the race two behind Virginia Tech Virginia Tech a little issue we were trying to pass Cornell for a few minutes 79 is trying to pass and has passed the chase vehicle for Skynet the 26 vehicle Wow and Talon he's gonna and talus is going to pass very that's a maneuver so so what actually happened so so it turned out we Cornell we're having problems with their actuators they were sort of stopping and starting and stopping and starting and and we we had some problems it turned out we had about five bugs they had about five bugs that interacted and here's how computers I sort of brain of the robots view now back you know seven we weren't using a lot of vision for object detection and classification so with the laser scanner the the vehicle the Cornell vehicles they are it has a license plate it has taillights it has a big number 26 it's on the middle of a road we should know that's a car stay away from it but to the laser scanner it's just a blob of laser scanner data and and even when we pulled around the side of the car we weren't clever enough with our algorithms to sort of fill in the fact that it's a card you have the problem when it starts moving at the aperture problem that as you're moving and it's moving it's very hard to tell and deduce the true motion now another thing that happened was so we had a threshold and so in our 150 thousand lines of code are wonderfully gifted student who's now a tenured professor at Michigan at Olson had a threshold of 3 meters per second so anything moving faster than 3 meters per second could be a car anything less than 3 meters per second couldn't be a car now that might seem kind of silly but it turns out that slowly moving obstacles are much harder to detect and classify than fast moving obstacles that's one reason that city driving or driving say in a shopping mall parking lot is actually in many ways more challenging than driving on the highway and so despite our best efforts to stop at the last minute you know we steered into the car and have this little minor fender-bender but one thing that we did is we we made all our data available open source and we actually wrote a journal article on this and this incident and a few others and so if you'd ask me then in 2007 I would have said we're a long way from turning a car loose on the streets of Boston you know with absolutely no user input and the real challenges are uncertainty and robustness and developing robust systems that really work and but for our system some of the some of the algorithm progress we made I mentioned the lane tracking Albert Wang who's now I think working at Google developed was given very sparse I'd say about ten percent of the recent graduates are more working at Google these days oh okay and then and here is a video for from our qualifying for the national for the for the qualifying event to get into the final race we had to navigate whoops I can't press the mouse it's going to stop so we I had to so we had to navigate along a curved road with very sparse waypoints and so in real-time the computer has to make decisions about what it sees where is the road where am I are there obstacles and there are no parked cars in this situation but other stretches had parked cars in our car and in a nutshell our if our robot became confused about where the road was it would stop it would sort of have to wait and get its courage up but like lowering its thresholds as it was stuck but we were the only team to our knowledge to qualify without actually adding waypoints so it turns out the other top teams they just went in with a Google satellite image and just added a breadcrumb trail for the robot to follow simplifying the perception so this was back in O 7 now let's fast forward to 2015 and right now so of course we have the Google self-driving car which has just been an amazing project and you've all probably seen these videos each with millions of hits on YouTube you know the oops the earlier one of taking a blind person for a ride to Taco Bell this was driving that was 2012 city streets in 2014 spring 2015 and then the new Google car which won't have a steering wheel in its final instantiation won't have pedals it'll just have a stop button and that's your analogy to the elevator and and so I think that the Google project is in whoo cars an amazing research project that might one day transform mobility and it but I do think and with all sincerity sorry I wrote in the Google car last summer I was blown away I felt like I was on the beach at Kitty Hawk you know it's like this just really profound technology that could in the long term have a very big impact and I have amazing respect for that team Chris Urmson Mike Monson Merlot etc but I think in the media and in others the technology has been a bit overhyped and it's poorly misunderstood and a lot of it goes down to how the car localizes itself how it uses prior maps and how they simplify the task of driving and so even though people like musk have said driving is a solved problem I think we have to be aware that just because it works for Google doesn't mean it will work for everybody else so critical differences between Google and say everyone else and this is with all respect to all players you know I'm not trying to criticize it's more just trying to balance the debate is if you the Google car localizes on the left with a prior map where they map the lighter intensity off of the ground surface and they will annotate the map by hand adding pedestrian crossings adding adding stoplights they'll drive a car around many many times and then do a slam process to optimize the map but if the world changes they're going to have to adapt to that now they've shown the ability to do response to construction by cyclists with hand signals when I was in the car we crossed a railroad tracks that just blew me away I mean it's pretty impressive capability but more a vision based approach that just follows the lane markings if the lane markings are good everything's fine in fact Tesla either just have released are about to release their autopilot software which is an advanced lane-keeping system an Elon Musk you know a few weeks ago posted on Twitter that you know there's one last corner case for us to fix and apparently he on part of his commute in the Los Angeles area there's well-defined lane markings and part of it is sort of a concrete road with weeds and skid marks and so forth and he said that he said publicly that the system works well if the lane markings are well-defined but for more challenging vision conditions like looking into the Sun it doesn't work as well and so the critical distant difference is if you're going to use the lidar with prior maps you can do very precise localization down to sort of less than 10 centimetres accuracy and the way I think about it is robot navigations about three things where is the where do you want the robot to be where does the robot think it is and where really is the robot and when the robot thinks it's in the wrong if it thinks it's somewhere but it's really somewhere different that's really bad that happens we've lost underwater vehicles and had very nervous searches to find them you know luckily when the robots that have made a mistake and and so with the Google approach they really nail this where am i problem the localization problem but it means having an expensive lighter it means having accurate maps it means maintaining them one critical distinction is between level 4 and level 3 these are definitions of autonomy from the from the US government from nitsa a level 4 car is what Google are trying to do now which is really you just you could go to sleep the car has a hundred percent control you couldn't intervene if you wanted to you just press a button and go to sleep wake up at your destination musk has said that he thinks within five years you can go to sleep in your car which to me I just five decades would impress me to be honest and the but level three is when the car is going to do most of the job but you have to take over if something goes wrong and for example Delfy drove 99% of the way across the u.s. in spring of this year which is pretty impressive but they fifty miles had to be driven by people getting on and off of highways and city streets and so there's something about human nature and the way humans interact with autonomous systems that it's actually kind of hard for a person to pay attention imagine if ninety nine percent of the card then ninety percent ninety nine percent of the time the car does it perfectly but one percent of the time it's about to make a mistake and you have to be alert to take over and research experience from aviation has shown that humans are actually bad at that and so and another issue is and this is the I mean Mountain View is pretty complicated lots of cyclists pedestrians I mentioned the railroad crossings construction but in California they've had this historic drought and most of the testing has been done with no rain for example and no snow and if you think about Boston and Boston roads there are some pretty challenging situations and so for myself when I first a couple years ago I said I didn't expect a taxi in Manhattan in my lifetime a fully autonomous taxi to go anywhere in Manhattan and I got criticized online for saying that so so I've been um so I got to put a dash cam on my car and actually had my son record cell phone footage this is the upper left is making a left turn near my house and in in Newton Mass and if you look to the right there's cars as far as the eye can see and if you look to the left there's cars coming at pretty high rate of speed with a mailbox and a tree and if you this is a really challenging behavior for a human because it requires making a decision in real time we want very high reliability in terms of detecting the cars coming from the left but the way that I pulled out is to wave at a person in another car and those sort of nods and nods and waves there's some of the most challenging forms of human human-computer interaction so imagine vision algorithms that could detect you know a person nodding at you from the other direction or here here's another situation this is going through Coolidge corner in Brookline and I'll show a longer version of this in a second but the lights green and see here this police officer so despite the green light the police officer just raises their hand and that means a signal to stop and so interacting with crossing guards and people very challenging as well as changes to the road surface and of course adverse weather and so here's the longer sequence for that police officer and here first of all you'll see flashing lights on the left which may be flashing lights you should pull over here you should just drive past them it's just the cop left his lights on when he parked his car but the lights red and this police officer is waving me through a red light which i think is a really advanced behavior so imagine a car that's you know you imagine the logic for okay stop at red lights unless there's a police officer waving you through it you know and then how you get that reliable and now we're going to pull up to the next intersection and this police officer is going to stop at a green light and and so despite all the recent progress and vision things like image labeling imagenet most of those systems are trained with vast archives of images from the internet where there's no context and they're so challenging for even humans to classify so that if you had you know some some data sets like the Cal Tech pedestrian data said if you got 78% performance that's really good but we need 99.9999% or better performance before we're going to turn cars loose in the wild in these challenging situations now going back more to localization and mapping here I collected data for about three or four weeks my commuting this is crossing the massive bridge going from Boston into into Cambridge and the lighting is a little tricky but show me will tell me what's different between the top and the bottom video anyone notice by the way how close we come to this truck you know if the slightest angular error in your position estimate you know really bad things could happen but the top between the this is a long weekend this is a Veterans Day weekend they repaved the massive bridge so on the bottom the lane lines are gone and so if you had an appearance based localization algorithm like Google's you would need to remap the bridge before you drove on it but the lines aren't there yet and how well is it going to work and so this is just a really tricky situation and of course there's there's weather now snow is difficult for things like traction and control but for perception if you look at how the Google car actually works if you're going to localize yourself based on how precisely you precisely know in the cars position down to centimeters so that you can predict what you should see then if you if you can't see the road surface you're not going to be able to localize and so this is you know just a reminder of the sorts of maps that Google uses so I think to make it to really like challenging weather and very complex environments we need a higher level understanding of the world I think more as semantics or object based understanding of the world and then of course there's difficulties in perception and so what do you see in this picture the Sun there's a green light there I realized the lighting is really harsh and maybe you could do polarization or something better but does anyone see the traffic cop standing there you can just make out his legs there's a policeman there who who gave me this little wave even though I was sort of blinded by the Sun and he walked out and put his back to me and was waving pedestrians across even though the light was green so a purely vision based system is going to just need dramatic leaps in visual performance so so to wrap up the self-driving car part I think the big questions going forward technical challenges maintaining the maps dealing with adverse weather interacting with people both inside and outside of the car and then getting truly robust computer vision algorithms just we want to get in a totally different place on the Opera ROC curves or the precision recall curves we're approaching perfect detection with no false alarms and that's a really hard thing to do so I've worked my whole life on the robot mapping and localization problem and for this audience I wanted to just ask you a little question said even know what the 2014 Nobel Prize in medicine or physiology was for anybody grid cells grid cells and play cells and so this is sort of has been called slam in the brain now you might argue and we might be very far from from knowing but I think it's just really exciting to so for myself I'll explain I've had a what's called a no one-armed URI grant a multidisciplinary university research initiative grant with my casa mo and his colleagues at Boston University and these are a couple of Mike's videos and so I think I think Matt Wilson spoke to your group and the notion that in the internal cortex that there there is this sort of position information that's very metrical and it seems to be at the heart of memory formation to me is very powerful and very important and so we have this underlying question of representation how do we represent the world and how do we and I I believe location is just absolutely vital to building memories into developing advanced reasoning of the world and the fact that that grid cells exist to me and they have this role in memory formation is just this really exciting concept and so in in in robotics we've we call the problem of how a robot builds a map and use that map to navigate slam simultaneous localization and mapping this is for a PRT robot being driven around the second floor of our building not far from Patrick's office if you've recognized any of that and we this is using stereo vision this my PhD student hardly Johansson who graduated a couple years ago created a system to do real-time slam and try to address how to get temporally scalable representations and one thing you'll see was the robot goes around occasionally is loop closing where the robot might come back and have like an error and then correct that error so this is the part of the slam problem that in some ways is well understood in robotics which is how you detect features from images track them over time and try to sort of bootstrap up building a representation and using that to locate your estimation and I've worked on this my whole career and as a grad student at Oxford I had very primitive sensors so I for historical slam talk I recently digitized an old video and some old pictures this was in the basement of the engineering building at Oxford I had a this is just the localization part of how you have a map and you generate predictions in this case for sonar measurements and at the time there we had the I'm sitting at a Sun workstation to my left is something called a data cube which for about a hundred thousand dollars could just barely do like real-time camera frame grabbing and then edge detection Wow and so vision just wasn't ready and and the exciting thing now in our field as vision is ready that we're really using vision in a substantial way but I think a lot about prediction if you know your position you can predict what you should see and create a feedback loop and that's sort of what we're trying to do and so slam is a wonderful problem for thinking for I believe for addressing a whole great set of questions because there are these different axes of difficulty that interact with one another and one is representation how do we represent the world and I think that question we still have a ton of things to think about another is inference we want to do real-time inference about what's where and world and how we combine it all together and finally there's a systems in autonomy axis where we want to build systems and deploy them and I have them operate robustly and reliability and reliably in the world so so in slam here's an example of how we said opposed this as an inference problem this is a this is from the classic Victoria Park data set from from Sydney University of robot drives around in this case a park with some trees there are landmarks shown in green the robots position error drifts over time we have dead reckoning error that's been shown in blue and we estimate the trajectory of the robot in red and the position of the landmarks from relative measurements so as you take relative measurements and you move through the world how do you put that all together and so we cast this as an inference problem where we have the robot poses the ODA metric inputs landmarks you can do it with or without landmarks and measurements and an interesting thing so we have this sort of inference problem on a belief Network the key thing about slam is its building up over time so you start with nothing in the problems growing ever larger and let's see the the if I had to say 25 years of thinking about this up through 2012 the most important thing I learned is that maintaining sparsity and the underlying representation is critical and in fact for biological systems I wonder if there is evidence of sparsity is there because Varsity is the key to doing efficient inference when you pose this problem and so many algorithms that basically boiled down to maintaining sparsity and the underlying representations so just briefly the most important thing i learned since then since in the last few years i'm really excited by building dense representations so this is work in collaboration with some folks in Ireland Tom Weil and John McDonald building on connect fusion from Richard Newcombe and Andrew Davison how you can use a GPU to build a volumetric representation and build rich dense models and estimate your motion as you go through the world so this is something we call continuous or spatially extended to connect fusion this little video here from three years ago is going around an apartment in Ireland and I'll show you the end result just hand carrying a sensor through the world and you can see the quality of the reconstructions you can build say in the bathroom you know this the tub the stairs to have really rich 3d models that we can build and then enable the more advanced set of interactions that Russ showed that's fantastic and I mentioned Luke closing something we did a couple years ago was adding loop closed into these dense representations so this is again incy sale this is walking around the Stata Center with about eight minutes of data going up and down stairs have you watched the two blue chairs near Randy Davis's office you can see how they get sort of locked into place as you correct the error so this is taking mesh deformation techniques from graphics and combining it so the underlying pose graph representation is like a sort of foundation or skeleton on which you build the rich rich representation okay so the so this is a sort of end resulting map and there's been some really exciting work just this year from Whelan and from Newcomb in this space of doing deformable objects and then and then really scalable algorithms where you can sort of paint the world so the final thing I want to talk about my last few minutes is how their latest work of using object based representations and for this audience of think if you go back to David Marr who I feel is unappreciated in the historical sense of how I feel that you know vision is the process of discovering from images what is present in the world and where it is and to me then what and where are coupled and maybe that's been lost a bit and I think that's one reason one way in which robotics can help I think with with vision and brain sciences I think we need to develop object based understanding of the world so instead of just having representations that are a massive amount of points or purely appearance where we can start to build higher-level and symbolic understanding of the world and so I want to build rich representations that Neve leverage knowledge of your location to better understand where objects are and knowledge about objects to better understand your location and just as a step in that direction my students Sudeep pillai who was one of Seth's students has an RSS paper where we looked at coupling using slam to get better object recognition by effectively so here's an example of an input data stream from dieter Fox's group there's just some objects on a table I realize it's a relatively uncluttered scene but this has been a benchmark for our GBD perception and so if you combine data as you move from the world using a slam system to do 3d reconstruction on the scene and then using the reconstructed points to help improve the prediction process for object recognition it leads to a more scalable system for recognizing objects so and it comes back to this notion to me that a big part of perception is prediction the ability to predict what you see from a given location and so what we're doing is we're leveraging off techniques in object detection feature encoding and the newer slam algorithms and particularly the semi dense orb slam technique from Zaragoza Spain and so I'm just going to jump to the end here to sort of the key concept is that by combining slam with object detection we get much better performance and object recognition so on the Left shows our system on the right is a classical approach just looking at individual individual frames and you can see for example here the red cup that's been misclassified we get better substantially better performance by using location to cue the object detection techniques all right so I'm going to wrap up and just a little bit of biological inspiration from our bu collaborators I can Bauman's looked at this sort of what's in the where pathways in the on Toronto cortex and there's this sort of duality between location based and object based representations in the brain and I think that's very important okay so my dream is persistent autonomy and lifelong map learning and making things robust and just for this group I made a I just want to pose some questions on the biological side and I'll stop here so some questions do biological representations support multiple location hypotheses even though we sort of think we know where we are when robots are faced with multi modal situations all the time and I wonder if there's any evidence for sort of multiply hypotheses in the underlying representations in the brain even if they don't rise to the conscious level and how sort of experiences build over time and the sort of question what are the grid cells really doing are they sort of a form of path integration or there obviously to me seems to be some correction and my just sort of crazy hypothesis as a non brain scientist is do grid cells serve as an indexing mechanism it effectively facilitates search so a location indexed search so that you can sort of have these pointers to what and where information get coupled together you 