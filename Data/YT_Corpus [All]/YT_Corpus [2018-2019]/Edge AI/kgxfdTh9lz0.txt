 one of the top things that I get asked about is Auto ml in this two-part video let's try using Auto ml vision to build and deploy a machine learning model that recognizes different types of chairs along with a few other items sprinkled in for good measure we're doing it all from the raw data collection all the way to serving the model everything in between welcome to AI adventures where we explore the art science and tools of machine learning my name is Yu Feng Guo and on this episode we'll explore Auto ml vision currently in alpha and get our data into the right format as an input in part two we'll use this data to build a model to detect what style of chair is in a picture one of the things that makes Auto ml so compelling is having a custom model existing models and services like the vision API have no problem recognizing that these pictures have chairs in them but what if you made your own chairs and you needed a way to catalog the various brands of different chairs in your inventory wouldn't it be nice to be able to use a custom vision API sub speak which recognizes your particular chairs that's what we're going to try to do with Auto ml vision Auto ml vision takes as an input lots and lots of labelled photos so get your camera out and start snapping pictures how many pictures you ask well ideally you'd have hundreds of pictures project it would be good and then once you get tired of clicking that shutter option perhaps you'll try doing what I did to get lots and lots of pictures to make it easier to capture data for Auto ml I chose to gather my training data by capturing videos of the chairs and then using the tool ffmpeg to extract out those frames I've gone out over to the Google Sunnyvale campus I took some videos of over a variety of different outdoor chairs I'll also took some video of the tables or position around as well as a bike just to make things a little more interesting let's take a look at some of that footage now here we have some chairs of different shapes styles and colors no video is greater than 30 seconds in length and we have a short clip of a table and another of a bike that's the data that we will be working with the end it states that we want is a CSV file that has one row per image and two columns the first for the location of the image in Google Cloud storage and the second for the label like red chair or table or blue chair to make it easier to organize I placed each video in its own folder so then we can run ffmpeg on each video file in turn within those folders that way once the frames are extracted out you'll have one folder per label filled with images of just that label this is a handy way to organize your images and data and it's simpler than having one massive folder of all the images intermingled next we can upload the images to Google Cloud storage using gsutil replicating our folder structure of one folder per label now it's time to get to work creating a CSV file that lists the path and label for each image I wanted to include in my data set now there's lots of ways to accomplish this but I chose to spin up a local Jupiter notebook and create a panda's data frame that gets exported as a CSV file let's walk through that code now ok so here we are in our Jupiter notebook and I've imported OS and pandas so we can work with the file system and load up pandas that we can use that to export our CSV I've listed out all the folders that I and want to use to pull in the data and I've listed this out here explicitly because perhaps in a different scenario I might choose to use just a subset of these folders to create a different training set so then we have a setup here to create a dictionary of our folder name and then our mapping to an array of all of our files for that folder and so do that first I'm gonna create an array of arrays of just all of those file names each in our array is all the file names of one folder and the outer array is just for each folder so here we have we're listing through all the files in a folder so our data folders here and we'll wrap that in an array so that we can capture that and so we can see here I've printed a sample of one value one image from each of the folders and we can see that they're all different so we look looks like we're in good shape so we can take those to our folder names and all our file names and zip them together pass them to create a dictionary using that and our base Google Cloud storage path we can glue these two together because we want to prepend the GCS or Google Cloud storage path in the final output so what we want is something like this where we have a Google Cloud storage directory path with all the way out to the dot jpg name and then a comma and then the name of the label so in this case maybe we have a comma and then chair black like so and so to do that we'll loop over the entire dictionary and for each file in the array of file names in that dictionary we will put together the base Google cloud platform along with the folder name and file name you know as well as that label so now we what we have with us here is an array of pairs each pair has that path and the label okay so we can take this array and just pass it straight into a data frame pandas accepts that format to create new data frames from scratch and our data frame looks exactly as we expect we've got all our paths and all our labels now we're ready to export to CSV in this particular case we want to export such that we use index equals false and header equals false this is important and can really trip you up if you forget to do this because if you leave the defaults which are index true and header' true the outputted CSV file will have a header row of 0 and 1 and moreover it will have an index column that comes before everything else which is just going to be your 0 1 2 3 4 5 so we don't want any of that we just want the path to the file and a label and nothing else so we use index false and header false so if you organized your folder structure the same way that I have mine you can take this notebook and with a couple of minimal changes adapt it for your use case as well so I'll definitely publish this notebook and link to that down below in the video description alright so now we have a CSV file describing the location and label for all the images in our data set we're ready to train our model join me in part two of this episode and we'll see just how well this model can perform thanks for watching this episode of cloud AI adventures and if you enjoyed it hit that like button down below and be sure to subscribe to get all the latest updates right when they come out and while you head over to part 2 of this video I'm gonna eat this Apple 