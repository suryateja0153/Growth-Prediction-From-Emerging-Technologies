 [Music] hi everybody welcome to our our lovely session here I'm very excited to talk to you all about probably every buzzword in the industry we could possibly jam into one session hybrid machine learning to have ops kubernetes like just tensor photo take your pick like we tried to fit it all in because they go we're inclusion us here like we like to include everything everybody everything every solution all of it all at once so we'll kick things off we'd like to just introduce ourselves so you kind of know who we are and what we're doing here so my name is Rob Schmidt I am a customer engineering manager for Google cloud based out of Chicago I primarily work with our large retail customers to help them adopt the cloud and transform their businesses and over to my left here is I'm Ryan McGuire I'm a CTO of augmented intelligence for a fortune 500 digital transformation consulting company out of Chicago we're partner with Google so happy to be here let rob take the stage all right so let's let's start with a little bit of definition of kind of what we're we're doing so what is the edge like a it's a lot of things it can kind of be anything you want to define it as but for for our for our purposes we're really talking about areas of constrained compute where you have potentially a limited amount of resources in which you can execute your model so you know in datacenters oftentimes people are going to be running just straight tensorflow in an IOT devices that sort of change things with mobile there's lots of different techniques and things we can use depending on where we're going but the fact of the matter is is that machine learning models are going to be running everywhere on all of these surfaces very soon so we need to start thinking about how we start to how we start to transform these these devices and and bring those services to them and we need to do that for for for a number of reasons right you know obviously with Google Cloud we have lots of really awesome vision api's that are very easy to adopt and give you a fair amount of thing but there's there that comes with a cost so for latency reasons you might not be you're going to have round-trip time trying to get anything up to those api's are back bandwidth is also an issue when you have not a lot of if you're on a mobile network or you are in an area where you don't have a good connectivity back to back to the cloud privacy reasons there's reasons to do things on devices and not ship things up to the cloud particularly with things like for example like very very sensitive healthcare data or sensor data or things that are very private connectivity as we sort of alluded to is always going to be an issue especially in IOT scenarios and obviously execution cost a lot of times it can be it can be quite expensive to hurl a lot of data and a lot of of compute up in the cloud to get this kind of stuff but the cloud still has a role and we need to think about how we leverage the Tod for what it's good for and how we leverage the devices for what they are really good for so to that to that end obviously in the cloud you have the the computing power obviously we've got GPUs and CPUs available that you can bolt on and use to accelerate your machine learning processes and get things done faster and that leads right into the velocity so you're able to move and iterate quickly by using things like cloud ml engine and and other tools like that the elasticity you scale up when you train and when you're done you turn it all off so you're not using having to have deadweight resources just sitting out there data storage is really another area where the cloud makes a lot of sense because you have to centralize and aggregate a lot of this data in order to train your models and obviously training cost is really good if we use things like preemptable VMs and other things like and all the elasticity that we're then able to minimize a lot of that stuff so there's really three considerations that we need to think about when we're starting to think about running a machine learning model at the edge one is the model in binary size obviously a lot of these models are going to be running in memory so if you don't have a lot of memory that's something we need to control for and make sure we get that as small as possible so that it's able to run in those in those scenarios obviously that also has an impact to the speed at which it's able to run if you're using a lot of the memory already you're not able to push transactions through it as fast and it also can impact application performance and you know as we look at that you know slow performance in an application is a killer people will not tolerate that these days and and you know you have to start thinking about your device itself and threading so by default tensorflow sets that to the default number of cores on the device but we need to understand that hardware profile so we can decide - we need more quick we need to utilize more cores do we need to utilize more threads in order to move forward so that's all great like we all understand kind of the problem space but how do we actually go there and there's really four phases that you need we need to think about and there's a lot of constraints and considerations that we need to take into account at each one of these phases in order to get our model out so first we're gonna talk about building the model obviously we're primarily talking about tensorflow here and the reason we're talking about tensorflow is because it's a very flexible execution environment and training environment for for machine learning so we're able to actually utilize it to go straight from the cloud to the edge with very little change to the actual machinery that we're running on but when we when we can start to consider the problem and what we're doing we need to understand where we're going to be putting this model at the beginning choices you make it this stage of the game have a very severe downstream impact if you're not considering them so we want to understand where we're going to be running this model is it going to be running on a Raspberry Pi in a closet is it going to be running directly on a piece of machinery you know what what kind of hardware environment what are the latency requirements of this particular model that we're going to be trying to run and understand that like as we make those choices we're going to be having we're going to be constrained in several different ways so whether it's the whether it's the RAM or the compute performance whether it's the size of the model or the choice of the model that we're going to use or build you know we need to understand at that point what we're going to be doing so that we can choose the right model and build the right model so components at this point like there's a lot of different things that we use obviously tensorflow I talked about but having Jupiter and ipython notebook so you're team of engineers and data scientists can iterate quickly and share the data and get everything moving quickly is important cube flow if you're if you're if you're in on kubernetes cube flow is another great pattern that people are starting to adopt because it kind of homogenizes a lot of that like training infrastructure and ipython notebooks and sharing and all that stuff obviously the thing like the big glaring hole that everybody needs to think about is training data and testing data if you don't have enough data there are ways around that but obviously we need to have good quality data so that we're able to actually build this model and get good performance out of it but but in in some cases where you don't have good data or maybe it's not particularly good you can start with a pre trained model from TF hub pull that down and then you can do a little bit of transfer learning on it to accelerate yourself and not maybe be constrained by as much data as you may need so when we look at the actual training of the model first we have to make sure we got enough data as I just said another constraint that we need to think about is how fast do we actually need to do this training how often are we changing the model how often are we looking at actually improving the model so because that has a that has an impact too if you are in an IOT use case in manufacturing and you tell your plant manager that you're gonna have to take a whole machine down for three hours so you can update the sensor model they're not going to be happy with you that's that's money just flowing out the door another thing to think about this point is is what we call quantization quantization is something that you package into the actual training execution in tensorflow and what quantization does is it simply takes the graph so tensorflow executes syn floating-point by default but on an edge device or on a very constrained area where you don't have a lot of GPUs you don't have a lot of floating-point operations that can be really hard to run it's like really hard for mobile phones to run all that floating-point stuff at the moment so what quantization does is it takes all of it and flips it into 8-bit integer calculation now there's a consequence to that so you're going to get the performance and you're going to get all the VIN crease in boosts but your model is going to be less accurate now so when you quantize the model you also have to make sure that you're understanding what that accuracy loss is going to be and what you actually need to be successful obviously in a lot of various industries there's going to be specialized hardware as we're starting to see we now instead GPUs and things like that so we're starting to see some some changes to to how we're actually building these things and that's that's an important thing to understand here it's because you have to know what you're going to be running on so that you can actually make sure that the execution environment is good and obviously one of the big risks of training is always data bias you know oftentimes we bias the data unintentionally we walk in with the best of intentions and wind up doing it ourselves because we're humans and we're biased so understanding that you might not have all the data or you might be excluding certain kinds of data that have an impact to your model is important as well so in this in this as we look at kind of the architectural components that we would use here obviously cube flow can do it can be a training and an execution engine you can use that code ml engine for you know like the fully serverless style of training and execution is another good component too and obviously using TP using GPUs at this point are really are really good because it's gonna it's gonna start to shrink that time down that you're going to need to to actually build this this model out and get it trained and tested so moving on to the the preparing the model phase so at this stage really what we're trying to do is understand that we've got we got to understand what the performance characteristics of the model are going to be a lot of a lot of ways things that we can do are built into the tensor flow libraries themselves but we want to make sure that we understand exactly what we're gonna be doing this model on and how it's going to execute in the environment that we're going to be running it in so a couple of tools that we can use are the tensor flow profiler this is a great built-in tool that can give you actual performance metrics based on the graph that comes out of the training session and then the other thing that we start to look at at this point is the TF flight benchmark library so if you're in a mobile or in a like a sort of an IOT situation tensorflow Lite is probably the destination you're going to wind up at it's built in it what it does is it restricts a number of heavyweight hard hard core CPU operations from the from the model and it comes with a custom protocol buffer that lets you execute it's specifically designed to be executed in mobile environments and so what you do is after you quantize your model you're able to run this this converter called toko and that's what turns it into the actual tensorflow light graph that you can then take and execute a couple other things to consider at this point our model encryption obviously a lot of these models are running in memory and it's not baked encryption is not baked into tensorflow or tensorflow light but it is available to to build there's there's a little tutorial up on the tensorflow website if you are concerned about people scraping the memory and taking your model and doing something else with it so that's something that you also want to consider at this point as well at this point now we got to actually get the model running and there's a number of different ways we could we could actually approach this situation obviously we could run in containers we could run natively on android or iOS packaged it right into our app you know do we have edge TP use or we're using android iot we could have cube flow to play to an edge location like for example if you have a fleet of of different service centers with small data centers inside of them like like you know three or four and you sees from Intel you could deploy cube flow there and then you have a nice really end service at that point but you kind of need to take into account everything that you've got rolling in your data centers and where you're going to be and make sure and think about how often at this point in time you're going to be training that model right so if you package everything into an app and you retrain your model well guess what now you got to push a new app off to the App Store like people got to go and download it and update it so there's often times you need to think about like okay like how am I going to do this so that I'm giving my users the best experience given the fact that I'm probably going to be training the model more frequently than I'm going to be updating the application because that's that's going to be a key thing that happens here the thing about machine learning is it doesn't really stop it has to keep going so with that I would like to bring Ryan back up on stage to talk about the journey that his team has gone through to trying to cute a very similar vision of the one we just talked about thanks for up so we're going to talk a little bit about an example today about cocoa for those of you know Coco Coco was a gorilla who learned sign language actually Coco was born here in San Francisco just down the road at the zoo and over a period of time she learned sign language was able to communicate so we're going to use that as a model today to demonstrate what Rob talked about about some of those flows at solstice you know our mission is to prove what humanity is capable of and what a better way to take machine learning computer vision neural networks and an experience to kind of help those in need to prove what humanity is capable of so we're going to walk through the slide a little bit and talk about the steps that Rob talked about first step being you know building your model using the cloud and the power of the cloud to do that here we had a series of images that we use to train the model to make sure that we selected the right model as Robin talked about with the deployment model of the edge we want to give consideration to the things that we might not have thought through so what operations could go through a tensor light conversion and how would we do that should we use a you know an object recognition that's already trained or should we train ourselves should we use SSD should we use vgg those types of things so in this phase we started to work out a lot of those types of things using the power of the cloud with Google compute engine to do those types of things really important in this space is to have your training set and your test set aligned up so I'll show you a demo in a little bit about using a sparsely trained Saten we'll call her Coco Junior she only knows three ladders and then we'll show kind of a demo of you know Coco senior being trained on a full set of data you know different hand gestures different colors left and right and those types of things to have a more rich and robust model once you have that model and you've got a built you want to training you you want to set some validation points so you know am i comfortable at 50% accuracy do I need a 90% accurate how much coverage do I want so we'll talk a little bit about how we did that there and then like this whole quantities it's a cool word right the edge quantifies this is like where you now get to take the power of the cloud and compact it right a whole world in a teeny dibidibidi space right so putting it out to the edge so getting ready to do that so we use quantization to do that conversion down that tensorflow light and then we use traditional means of CI CD pipelines so we're constantly training the model in the cloud with new imagery and data that we have we have a CI CD pipeline that will demonstrate in the application we're able to make the switch between Coco jr. and Coco senior in near real-time which is really important to the user experience does that make sense yeah who wants to see Coco before we do I want to talk a little bit about the the the differences here so you can see the top track we have Coco Junior who's only trained on a and B with it with a minimum set of observations that model is available in a version in our continuous integration environment where we can pull that down from our storage bucket on the bottom we've taken more of a robust approach using the power of auto ml engine you know imagery and data to really train Coco senior to know the the soup-to-nuts alphabet and interfacing with the application so with that said I'm going to bring up Coco Junior if you could switch over the displays so here we have an Android app displayed here on screen image image recognition cameras on you can see the image process over here which will do motion detection once we put up a hand signal but as demos go let's see if she's working ah she worked she knows hey yay however if we go to H which is this signal no letter detected but to prove out that she knows B you can go ahead and put up B signals so she knows B but she's only 61% certain that it's a B so now what we're going to do is we're going to pull down through the C ICD pipeline that we have available with Coker senior trained so we've got these these buttons that basically toggle versions of the model back and forth inside of the application so that we get the latest and greatest so we're going to go ahead and update here to Koko senior and as you see you can see Coco senior is now taken over but nothing's really changed from the experiment so we can go ahead Oh guys so she knows a but let's see if she knows H oh so you can see that the power of having data and a different training set brings you to the next level of what you're capable of doing I could show you right hand signals we could have a small children's hand a woman's hand a man's hand all these are different signals into your environment when you're thinking about the model that you're creating so your test inputs your data set are very important but let's envision for a second that we took this to the next level and you know we didn't have that training set you could have the power of a continuous learning cycle with Coco that could capture go into training mode and now it's capturing images and you're allowing your user population to feed back into that loop creating your training set you're not going to run that locally at the edge you want that information to go back up into the cloud and we'll go to talk about it a bit in a second I'm gonna call Rob back up here if we could switch back screen I'm gonna click this so Rob is this how it works in the real world today with with Coco yeah but it's truly on that's only half the story as we think about this like like at this point it's like congratulations we've deployed a model like yeah everybody's excited but there's so much more we actually have to do at this point and as Ryan alluded to we've got to get feedback from the users we've got to aggregate the data we've got to organize the data we've got to update the model we've got to keep going through this cycle onward and onward and onward so you know a couple of things that I really liked about the approach that that Solstice took in this model was they they sort of decoupled the app execution from the actual model graph which is pretty slick so they can were able to deploy that out to a CDN basically and have have it going back and forth right there without having to update the app itself so putting that logic into the application was pretty was it was it was pretty smooth I think that's a good pattern that you're gonna start to see people adopt going forward yeah I think it's a really important thing you talk about something that all of us that are intimately familiar with right the software engineering practices you know that cycle is never done you never finish it application you're always constantly grooming it so it's solstice we've adopted an approach called continuous learning which is a mix of agile principles reinforced learning supervised learning which we take it through a cycle very much like we talked about today and then that prior slide going from selecting your model and experimenting training your model the initial flow of that evaluating what are your criteria for getting it out the door you know and going through each one of these steps but it's evergreen it's it's never stopping we're using modern software engineering principles see ICD process you know maybe some a be testing in here to put our model out into production and get our feedback so you can envision a NLP chat bot that is you know making a recommendation for a particular purchase order and asking for validation from the end-user was that right and then that flowing back into the process so that we can elevate the the capability of our machine learning models in a commercialization to go from just reasoning for a human to actually helping make decisions so one of the things that we'd like to do is just kind of walk through between Google and solstice put together this model of like what an ideal state architecture is and how that might fit into a model to give you guys the tools as you walk away from this conference this meeting and you think about how you're going to take forward what you've learned here today and apply it to to your business so we'll start with yeah the the experiment and train section here using GCP we've we've lined up the ability to use bigquery BigTable and other cloud storage as kind of that foundation data layer and we'll talk about layers how that that true feedback data flows into it but you can see you've got your dev cluster in your Prada living side-by-side creating that train model that's constantly evaluating new sets of data and new points of data inside of that anything can add their up yeah I think it's it's a it's an important piece that you need to have that I think people don't think about is they're going through this is is is curating your data and making sure that you have a consistent environment for your teams to actually build an experiment with these models on and and making sure that you're doing that in a safe way making sure that everybody has access to what they need making sure you're respecting user privacy and all the other concerns that we have in software development and then you give them the opportunity to just take go right from that spot to call machine learning and do the actual training execution so you're basically unconstrained emmm now from resource usage and and that's I think the ideal States it's gonna let people move quickly and actually start to execute these these patterns rapidly yeah no I think that's great and this kind of blueprint gives a where do I start and when do I end and how do I start it again right and so it's a really good model to it so stepping out of that it's into the evaluation the deploy stage you can take this you know for what it is at this level or you can even go further detail so put in whatever your continuous integration CCI CD tool of your choice in this spot again using that cube flow to deploy to the edge you know containerized and package that doing the TF light using spinnaker to get it to the edge of ice but inside of this you could get more sophisticated say you had a confusion matrix that you wanted you know 90% along your confusion matrix you want your ROC curve to be these types of things in your continuous integration environment like you do today with your web applications you could set a threshold for coverage right these models are set to have these types of facilities to be able to measure the validation of it your deployment here doesn't necessarily have to overlay the production model that's there you could put that behind an a/b testing platform and say 80% of your routes are going you know to the old model while 20% are going to the new so you can really measure the value of those types of things this is the type of space where we're taking the history of what we've done is software engineers and product folks and bringing that into the model and integrating that into your application how many think about that yeah no I it's it's absolutely something I think needs is needs to happen right I think people need to people need to start taking a lot of the things that's painful about machine learning like the performance testing the regression testing figuring out you know what the limits are and Alessa and automated away we've done that we've done that with regular app deployment like we need to do it in machine learning now because that's the thing that's going to start to let us move faster it's also going to let us start to put controls and rigor around the testing right rather than relying on somebody manually rigging up the test we're gonna be able to make it a piece of the pipeline and it's just going to happen another thing to talk about that that we that this model sort of highlights is this concept of localized training and one of the things that this is really where cube flow shines this is the thing that cube flow is also mapped so one of the things that that's super important is when we look at the cloud right you're going to be training against all of your aggregate data out out up there but when we look at you know when we look at certain scenarios like weather forecasting mm-hmm it's maybe like rain down in Africa potentially potentially I'll bless the model and then hopefully we can get it going yeah the but but it's important right there's going to be regional differentiation there's going to be different scenarios there's going to be different conditions at the edge and you might want to like tuna model at the edge right in certain scenarios to fit this case so a classic I'm obviously I spend my time in retail a very classic use case so this is demand forecasting in stores it's very easy to forecast demand on an e-commerce site because it's all coming through one channel right when I look at a store a store in California versus a store in Florida versus a store in in in Maine versus a store in Texas all going to have different demand and supply cost so if we see if we start to optimize the supply chain and make those predictions we probably want to start training on all of our aggregate order data across all of all of our stores figure that out then we deploy it locally to the store and train against the transaction log that that's coming in from everything that everyone's buying and that localizes it because we might get bad predictions if we're looking at the whole store because all all of our transactions aren't necessarily representative of what's going on in each individual store yeah it's a continuous learning piece you know and you know taking the localization factor to it right like I'm majority of the models like we used an object neural network for animals that have four classes and that's where we started cocoa from and we transition that to to learn the alphabet same type of thing right store you know you have different parameters like you can tweak down at that level but the base is always the same yep yep yep you trained on there and then just use executing another set of training down at the edge the last piece I think is one of the most important factors as we think about the maturity of of models in the commercialization space so how do you get a model to be better how do you take it to the next level how do you elevate it from an MVP to a version 2.0 we like to talk about models and AI going from an infancy state to a reasoning state and then ultimately moving to a decisioning state so that we can augment the human element so that we can help the human get better and ultimately make those decisions on their behalf with a high degree of certainty different businesses have different risk thresholds and tolerances but you know in our experience we don't see those models kind of move forward without taking this feedback loop in or additional data to adjust in the marketplace so this is a really important component as you think about your architecture and you think about your deployment of models is how are you capturing the customer sentiment how are you ensuring that you know what you intend to happen is happening you know compliance and regulation learning about how did you arrive at a particular conclusion and what would you do next time when you're there so it's a really important event and making that loop back into the cloud where you're actually feeding that event data back into your training set yeah 100% and even then this also this also feeds the automation more right once we build the model and we have that object in that artifact we you know we might make tweaks to it we might change little things for performance reasons let's lots of little things like that but really the thing we're going to be constantly doing is training it and training it again and training it again and training it again so if you build this pipeline like this you're able to automate all of that stuff away so now your high-value hard to find hard to hire data scientists and machine learning experts are able to keep going and working on new stuff rather than having to continue to maintain a model over and over and over again by hand right so this is this is I mean it's very much reflective of the changes we've seen in the industry to this point especially as we look at app development and kind of the the the the the way that space has evolved over the past five or six years and bringing that now that same sort of rigor and and decision to machine learning yeah no it's great well you know thanks for allowing us to have the opportunity to come up here and talk to you hopefully you guys have walked away with a little bit more insight and knowledge as you cut a wine dine your time at next and really appreciate it and if you have any questions yeah we were offering four questions we've got we've got it we've got about 15 or so minutes left it looks like and we're happy to take any questions you might have you [Music] 