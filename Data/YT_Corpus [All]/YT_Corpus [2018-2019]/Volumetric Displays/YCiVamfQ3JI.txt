 Ah, so today is the fourth presentation in our scientific visualization workshop series and we're hoping to do this beyond this semester. But here's the calendar for this semester. And things may happen you know, in like the November timeframe, to move things around, but for the most part, at least, especially for the next month or so, we're pretty fixed in the schedule. So, next week Chauncey Frend who is one of our colleagues in the Advanced Visualization Lab who is stationed at the, our Indianapolis facilities and he'll be coming down or maybe presenting remotely. I haven't confirmed that with him. So he'll be talking about augmented reality for visualization. So, we'll have kind of a two things on immersive technologies back-to-back and then we'll start. We'll talk about some other topics after that. Alright, so, so today is immersive visualization or virtual reality with, for visualization. And a quick, kind of statement about what we're talking about then is taking scientific visualization which we've been talking about the last three weeks, kind of focused mostly on desktop tools, and apply that to what we call an immersive interface which is just kind of a fancier way of saying virtual reality. So that's what we're going to talk about today. I wanted to show kind of quickly and we'll see how this works, a quick demo of VR application or maybe a couple of quick demos to start us off, to get us a feeling for what's going on. And so I'm going to bring up a little medium technology thing here. And oh, I have to start Photobooth again, right, so just a moment. I have a camera set up so that the remote people can see what's going on as well. Oops, and I had it set up but then we crashed due to the or oops, lost it due to the crash. There we go. Oops, that's not what I wanted. Oh, here it is. Alright, alright so here's the camera looking at a computer which is showing us what's going on in a head-mounted display. So I have this head-mounted display and this is one of the commercial tools that we'll kind of mention today so that one of the things that we have is commercial tools and handwritten code. So here's one that does a molecular visualization. I haven't used this one too much. I've just kind of played around with it for a little bit. Joe here from Crane played around with it for 10 or 15 minutes before we got started today so he's probably more of an expert at this than I am at the moment. But here's one application. So this is a commercial one. You can download it for free on Steam and so it works both for commercial VIVE and commercial Oculus tools. I won't try to demo that while talking. But I do also want to show one other application if we can get out of here. And that is ParaView. Alright, so we talked about ParaView two weeks ago and so I want to just quickly show you how easy it is now to do ParaView. So one of the files we looked at two weeks ago was from Professor Horowitz and so we'll bring up his data real quick. Load it in. There's the frame. If we add a contour to that at the value we talked about back then and apply that, we get this. I don't know what stage. What stage is this Dave? [ Inaudible Response ] Okay, uniform density of neutrons and so we'll add the connectivity filter which we did the other day Alright, so I'm not going to check, play with the color or anything like that. But so this interface is backwards for those of you watching on the video which is everybody I guess. There's a button down here. It says send to open VR so I just click on that button and it gives me a little warning. And if I go to, oops, I'll have to turn that down. My display mirror, oh here we go. It'll show me what's being shown in the VIVE and if I look at the right direction there is the data. Oh, it's. I was going to say it looks squished but that's because there's a cutting plane on there. I'm going to put the head-mounted display on real quick here. So, there's a cutting plane and I can grab this cutting plane and actually I can move that data so it's closer to me I can fly around the data. I can grab the cutting plane slice it through any way I want really quickly here. I can also grab the data and rotate that around however I want. So I can move the data relative to the cutting plane or move the cutting plane relative to the data. And so again, it's a pretty simple ParaView thing. We can do more in ParaView to get our data a different state. We can animate this one although this one loads pretty slowly. It does a lot of calculations so it's not really pleasant to watch animated. But, it works and it works pretty quickly so, you know, it has some limitations in the speed if you're trying to do a lot of data, and we'll talk about that in a little bit. But otherwise, you know it's a commercial. Actually it's an Open Source commercially-supported package and if you have a VIVE or an Oculus you just press the button and you're ready to go. So, that's something. We'll talk a little bit about those things and we'll do demonstrations for those of you who are here with us. We'll do demonstrations later, after basically we're done talking. And you can try ParaView. You can try the Nanome molecular viewer or some of the other things that I'm going to show today. Alright, whoops, turn my volume down, I don't know what's [inaudible]. There we go. They were [inaudible] right now. Okay, so I need to get focus here, there we go, maybe. There we go. Alright, so this is a lecture series on scientific visualization. We haven't really talked about what virtual reality is so I'm going to spend just a couple of slides explaining what, what I mean when I talk about virtual reality. So, here's a definition for virtual reality. That's a lot of words so I'm going to break it down into some chewable segments. So the first thing is you want to know where the person is because virtual reality responds to the movement of a person. So you have to what we call, track the user. We have to know where they're at. The VIVE has that built in. If, for those of you in the room, you can see I'm pointing to. There's a couple of these stations, we call them Lighthouses for the VIVE and they put out a pattern in the air of light and then the VIVE itself has these little, little dimples on it. And each of these dimples is sensing when that light pattern comes by and that's how it knows where it's at, and the same is true for the controllers. Different virtual reality systems use different techniques. The Oculus uses cameras instead of the Lighthouse but works essentially, similar, similar concepts. So, once we know where the user is we want to display to the user from their perspective right. So we want to augment their senses, usually we're at least talking about vision. Sometimes we're talking sound and touch. And then finally the goal is to put the person into that space right, so we talk about being mentally immersed when I give my full lecture on VR, intro to VR. A mental immersion is not necessarily a requirement of VR, it's the first two. It's the physical immersion that's more the requirement. But we want to make you feel like that everything makes sense to you right. I mean it doesn't make sense that you're standing in a molecule but if you're able to manipulate the molecule, scale it, move atoms around, things like that in a natural way, then we'll call that kind of being mentally immersed and making sense. Well, there's three kind of major or two major and one minor way of, we do that. One is to put the display on your head which is what we're showing with the VIVE here. One is to have a display that the user enters so we call that CAVE or CAVE style. That's this middle picture here and the middle picture. And so that works where there are screens on all sides of you or most, several sides of you, and also on the floor. Although we can also do that for example, on this screen that we're watching here in the room. It's a 4 by 4 tiled screen. If we want to just see something really large and kind of walk around things, we can do that as well. I have spent the last few days getting kind of a basic example running on this screen which I can kind of fire up after we're done. It's not as impressive as the commercial VR that we're going to see so, so don't get your hopes up too high for that just yet. But we have a lot of different software that we can install and that will work out well too. Right, so our motivation then for doing, for bringing this virtual reality concept to scientific visualization, so I'll talk a little bit in a few slides about the demonstrated value of virtual reality as an interface, of it's being natural and people being able to use interfaces for six degrees. You can move things around, it's not like you're mapping a two-dimensional mouse into a three-dimensional world, and with rotations it's even more controls. And then as we've been talking about this semester, you know visualization, we're looking at different tools, different ways of exploring science data. So we want to bring those two things together and make it even better right. So, we want to have, we're looking for then a kind of a suite of tools that we can have that will allow us to explore our data with a natural user interface right, right Eric [assumed spelling]? I can get you that picture. Alright, so why do we want to do immersive visualization? So we just have [inaudible] motivation but I mentioned already that virtual reality gives you this natural interface with the world so you can naturally explore the world. You can actually go inside your data which in some cases may not make sense but in some cases you want to be able to be there, be in the space and look for relationships between things. And so, like I say, sometimes that makes sense, sometimes maybe it doesn't. One question then is, where does virtual reality fit in your scientific analysis workflow right? So, in general, there may be some exceptions, but in general, VR, when you go into a VR experience you're not there to do the analysis. You're there to go and collect data, collect, make observations that you can then just come back from. So, I like to make the analogy of going into the field, it's kind of like a fieldtrip. In one of my past assignments we, we had people who would go to Alaska and go to Antarctica and go to the desert and collect data, find things. And so often they would need to teach their new students who are working with them the lay of the land and so one of the things that you could do is give them a feel of the space before you go there. But that's just one example of what you might do. Really you're there then to take some measurements, get some insights and then come back to your office space, come back to your computer with your statistics packages and other analysis tools and that's really where you're going to do most of your analysis, right. And in some cases you might actually use it to be recon for an actual fieldtrip right. And so as I say so one of the people I used to work with, he would take people to. He would take students to Alaska. They needed to find a valley that had certain properties and in order to figure this out, kind of get a sense ahead of time they would actually bring it up in virtual reality and get, kind of look at some of the valleys and things like that. A lot easier to do now that we have Google Earth, but in the past they would actually you know, rent helicopters, fly around and that was kind of expensive. So virtual reality then for this whole process is more, I like to think of it more of a scientific instrument right. So it's a place where you take measurements, like if you had a microscope, you know kind of a basic, standard, I should say scientific instrument and it lets you get different views of your data right. So if you have a microscope you're getting a different view of your data. And then you can also use it to enhance data. So sometimes the data is kind of messy. It takes kind of a human intuition to figure things out, but maybe that intuition is hard to bring about when you're only looking at it in two dimensions. If you can actually see it in three dimensions you can see relationships between fibers or something like that, then it can make this data enhancement and cleaning test a lot easier. I'll show an example of that as we go on. Again, you can kind of take a, a tour of some data right. So you can, if you already have a bunch of molecules you want to show your students, a different bunch of molecules you kind of take a tour of all those molecules. And then as I've already mentioned a couple of times, you can actually do actual recon for an actual trip in the field right. So now this doesn't necessarily apply to most of you unless you want to argue and let you refer to your management that you want a bigger virtual reality facility. But one of the questions then is for people who have VR facilities is, how much is that facility going to be used? Right, and so in a lot of cases, in scientific spaces, the facility doesn't get used 24/7 or not even 24/7 but 8 by 5 even necessarily right, 40 hours a week. And so that looks like, kind of it's not being put to good use. But sometimes really the question is, is it being used enough where people are getting good benefit out of it right? So, it doesn't have to be used all the time unless of course you're doing something like training where you want the usage. If you're just trying to get people in there so that they can see relationships, then they go back to their desk and then they make use of it, like if you have a microscope. Your scientists don't have to be looking in microscopes you know, all the time in order to be doing their science. They have to look, get some data, make some, try and deduce things and then they go back and figure things out, right? So if you're arguing for a VR facility even if it's not a big one like this, even if it's for a lab full of head-mounted displays, I like to use that as one of the arguments. Right, so one of the issues when we're dealing with scientific visualization in VR is that we tend to have a lot of data. And so if you're doing, you know VR for games and things like that, they basically reduce the amount of information they're presenting to the point where it renders fast. We may not have that luxury or we may not want to get to the luxury of having things render fast by having less data, we might want to see more or a lot, a good percentage of our data. And so we might suffer with lower quality rendering than the games do. So, right now if you're a game company and you want to sell your product on one of the VR stores they basically require that your game run at 90 Hz or your product run at 90 Hz. And so that's, you know, a lot of processing. I think that's about a, I can't remember exactly but about 11 milliseconds you have to do all your rendering. For a lot of you, you have a lot, a big scientific dataset, you might not. You might not get to that point right. You might have a much slower thing. Now the slower you go, the worse the experience is kind of for your brain right. It doesn't look as natural if it's going very slow. One of the benefits of having, of using like a screen that we have here, this tiled screen that we have, a big screen like that, is that you can get away with lower frame rates and not have your body sort of reject that and get nauseous from that. Because it doesn't move around, it stays stable. It's you that's doing the moving and so the images on the screen don't have to react as fast. If on the other hand you have a display on your head, when you turn your head that has, you know what it's, what you see has to change immediately. And so it's actually a lot more strenuous on your brain for the head-mounted display than it is for the CAVE-style displays. So, if you're doing science and you have a big screen you can get away with the frame rate being a little bit lower. Otherwise you have to reduce your data right? You have to reduce your data so that you're not seeing all of it, maybe you're seeing a subsection where there's something important happening or maybe you're just seeing it all sampled down. And so, you can then do your exploring and then again, go back to your desk or maybe once you've kind of perused your data at lower resolution, you can say over there in that corner of my data is where I'm really interested. Well let's look at just that corner but in higher resolution. Alright, and so let's talk about, then the cost, a little bit about immersive displays. So here's a, our kind of CAVE-style display that's up in Indianapolis now, just installed this year and so it's another tile display. Well it's about. It's twice as big as the one you're looking at and it curves around so you can have data on all sides of you. So that's kind of a big facility. It's going to be expensive. Again, if you have a need you can argue for that, that it will help your science. But we also have now a commercial off-the-shelf VR that's priced pretty cheaply. The computer actually costs more than the display at this point so you want to get a nice computer that costs you know $2000 or $3000 maybe. But then at some point especially if you're doing custom code really the cost, the main cost is the people, the people doing the code development, the people processing the data. That's where a lot of your time is going to be and so that's where, if you can save a lot of time in development by having a bigger display. You can also have a win there. Alright, I said earlier that we'd talk a little bit about kind of demonstrated value of virtual reality. So, I have here four research experiments done using virtual reality starting back in 1990 by Fred Brooks. So, in the Fred Brooks case they have a molecular viewer. We'll see a picture of it later. It had a force feedback display and they wanted to have two molecules dock with each other. And so they wanted to compare a desktop interface with that versus the virtual reality display. And they found that basically, I don't remember exactly what the task was, but the subjects in this task were basically able to perform twice as good, yeah twice as good using the VR display. A few years after that at NCSA over at Illinois we had a volume visualization tool and we did a similar kind of test. And people in the, on a desktop interface do the same. You know, how well do they compare with a person using the CAVE virtual reality interface? And they found, I don't remember the multiplier but they also found that the virtual reality interface was better. Kenny Gruchalla, who is now at the National Renewable Energy Lab did a research project on figuring out paths for oil drilling. And so again, I don't remember the multiplier but definitely the virtual reality had the bigger benefit. And the last one listed is Prabhat at Brown University and he was looking at scientists. I don't remember the exact task but they were looking at blood flow through arteries. So, those are research papers but there are also people who I think have just as kind of good a value who talk about specific cases where they had kind of a big win from virtual reality. So Gil Boer [phonetic] who was at Duke at the time, he basically, he was just taking his friends, showing them their CAVE at Duke. He had some visualization of a forest canopy and they were just having a conversation, looking at it and then they were kind of. Something struck him and this is one of the examples I talk about where you, you have this kind of Eureka moment. You were only in the CAVE for 10 or 15 minutes having a conversation. You see some relationship. You go back. Now you're doing statistical analysis with R or whatever. And so the rest of your work is sitting at your desk doing kind of traditional stuff. But it was that moment of insight that you got while you were in the CAVE. Oliver Kreylos has a paper on several different scientists who found benefit using CAVES at UC Davis. And then Gary Kinsland has a similar story to the one from Duke and he was again, he has his date in the CAVE because, you know, he had a relationship with the people around the CAVE at the University of Louisiana at Lafayette. And was again bringing a friend just to show him, hey here's my date in the CAVE and he had really kind of literally just finished a paper on this very dataset. And while in the CAVE with his friend he realized something that he'd totally missed in this other paper and so he went back and he wrote. He got a whole new publication just out of this one little insight that he got out of, literally just being in the CAVE for 10 minutes. So these are the kinds of things, this is. You know, there's no published thing there right. There's nothing. That's not a scientific study in and of itself but that anecdote I think still lends credence to the value of virtual reality for visualization. But, so I want to talk about, kind of a quick tour of some tools that are available for doing visualization with virtual reality. And we'll talk about some examples as we go along. I've broken them down into kind of three categories. So one is a special purpose tool right where it was written just for this task. Another one is general purpose where it's like ParaView where we can load in lots of different kinds of data and then we can just kind of output it to VR. And then the last one I call 'Shanghaied' which is basically where you have a computer graphics program that does a particular task. It's meant for the desktop but there are some tools that can capture the rendering as it's on its way to the desktop. And as they capture that rendering they send that, those polygons to a VR display and then you can actually see the polygons in the VR display. There are some limitations to that. You know for the most part, the tool then doesn't even know it's being displayed in VR. So, if it happens to think well, they can't see this I'll just throw this away, whereas in virtual reality you can turn around and see it. The application doesn't know that and so it may have discarded some of the data. And also there's not really going to be a good user interface for that right, because the, all the user interface is going to be on the desktop. None of that is going to make it to the VR. Alright, so let's go through, a little bit of examples. So here's some old stuff. So I talked about the Fred Brooks experiment. That was with this nano-manipulator. So and I mentioned, or had on the slide in the previous slide this notion of custom applications. You know 20 years ago of course we didn't have off-the-shelf VR applications we had to write everything pretty much ourselves so there was a lot of effort, a lot of manpower, womanpower put into doing that. And so we had to spend a lot of time and it took a lot of time to get to the place where you had a tool that was useable. One of the goals then at that point was to use some of these general purpose visualization tools and add a module such that you could then do virtual reality with that, much like we just saw with ParaView at the beginning of today. And so these were some tools. I have pictures of those. We'll see those in a minute. Here's this molecular docking application that I mentioned with the Fred Brooks study. And so this is at the University of North Carolina. The Haptics force arm came from Argonne National Lab and so they were able to both see and kind of feel the molecular forces as they interacted. Here's one from NASA's Ames where they're doing visualization of wind flow around a space shuttle. Kind of one of the unique things about this then is the old style of the [inaudible] flight. So here's a head-based display but you actually had to hold it to your head, kind of like what you do with your phones now. And there's a, this arm here that is standing out there's a counter-weight out here because there's you know literally like little TVs inside this box here. It's not like we have our smart phone displays that we use today. So this box is pretty heavy so it's not something you want to have to physically hold or wear on your head. And so that, this is an old display called the BOOM but the tracking was really good because they had mechanical tracking. Here's another one I mentioned from Rachael Brady at the National Center for Supercomputing Applications and shows she has this volume visualization tool. Here's a chicken embryo and I think this one is a horse fetlock. But anyway one of the things I mentioned already is this notion that we can't see all of our data right. And so this, the chicken embryo is sampled down data so that we can see the whole embryo there. But this other one, this horse fetlock or knee data, we're only seeing a portion of the data and that's this notion where, well we can kind of choose right? We can see sampled down, the whole dataset or we can find the data that we're really interested in and focus on that. And of course 23 years ago that was even, you know, more the case, even with a computer here that costs over half a million dollars right. So, just to do the rendering they kind of made it so that you can only see a portion of the data and that's what we're seeing with this square in the middle. And you can grab the square and move it around your dataset to see different aspects of it. And there are tools, so one of the things that she was working with researchers who wanted to say measure paths through data so perhaps the brain stem of this embryo and so since it's kind of curly and it's a little fuzzy data you could like drop these little bread crumbs which is where the name comes from and drop them along the path here and then have, at the end have it report the link to that path. And so that was the task that she had and that she did in her study to find out whether people were better at it in virtual reality or not. Here's a quick one. Here's an atmospheric visualization. So this was a desktop tool which, with the source code we could actually add some virtual reality into it. There's Los Angeles smog there. And then the general purpose tools that I talked about, so a lot of them or two at least, two of these have gone by the wayside, right. They were very popular for their time but haven't survived. Actually ABS may still be around but I don't know anybody who uses it. But anyway they have these tools where you connect things and it was kind of like ParaView. ParaView does this internally. You have something that reads the data somewhere, something that processes, adds color, you know finds I don't, isosurface here, so our contours and then feeds that to some renderer. Well, at the time we were able to create a render that produced two side-by-side, left and right views and lets you then interface with them. And the views changed based on the virtual reality system. And so here again, you could take your desktop tool and bring it into virtual reality much like we're doing with ParaView now and see that. And then also the Visualization Toolkit which we'll talk about later in the semester with the web interface for that, but that was kind of a popular visualization, popular toolkit that had lots of visualization features and you could connect together. You could trust kind of the, what it was doing. And lots of people were taking that and putting it into virtual reality. Nothing that kind of lasted because VTK itself was evolving and so things would change and eventually it would break. But, there's more to that story we'll talk about later. So, here are some pictures of the VTK from early days. Alright so those were some early examples. Now there are some modern examples. Again, here's a 3D visualizer so this is earthquake data in, at the globe. You're actually standing inside the Earth and looking out toward to where the earthquake data is. You can generally see that it follows where these tectonic plates are. It does a lot of other things though. It has slicing and isosurfacing much like ParaView has and it streamlines. Another tool from UC Davis is a light R [phonetic] visualization tool. Here we're just looking at points so light R systems generate, you know literally billions of points, maybe hundreds of billions of points. And so even for modern computers with modern graphics cards that's a lot of data. And so the researcher at UC Davis, Oliver Kreylos developed a system that loads in the data where you're looking right. So it focuses in as you get closer to something it fills in the points. So the computer doesn't have to render all those points at the same time. But the one on the left has kind of an AB sampling so it's terrain from two different times of the year and you can see where there's vegetation growth and you can also see where maybe some landslides happened and/or some building took place. And then the one on the right is from powerline observations where power companies want to make sure that vegetation and things aren't encroaching on their lines. Here's a, ESRI-based, so RGIS-based visualization tool for a CAVE. Here's a volume visualization tool which I want to show a little video of this one just to give you a sense for that. So let me break out to, oh right. I lost my VLC so let me start up VLC here. [ Inaudible Question ] So, yes, the, I'll talk about that in a little bit but these couple, last couple of tools, the light R one and the earthquake one, the one that was showing earthquake data and this one, they're in with a tool called Vrui which, at the moment, Vrui only works on, basically LINUX or UNIX-based systems so it'll work on a MAC. He has got it and I've got it to work on in our lab over at 10th and the Bypass. He has gotten the VIVE to work on LINUX with his system so we can look at this, these applications in our VIVE. One of the things I noticed though and you'll, and other people that know this system, when you have that much data you actually notice the resolution of the VIVE isn't as good as you might imagine if you're just playing a game and you're just kind of concentrating on the game or some other stuff or even a molecule. There's not that much data and so the resolution doesn't bother you so much. But once you get to lots and lots of points, that's when the resolution actually starts to become noticeable. So you know, the next generation of head-mounted displays I think will probably address that too, to a degree. But yeah, so some of these tools, and I'll talk about volume visualization at the end for Windows too but some of these tools, the guy who authored the Base Toolkit is kind of a LINUX guy and so some of these are limited in that way. But we'll talk about different opportunities later. So I want to show the video of this which I lost my, I lost my bookmark on that since my machine crashed. Alright, so here's that, that same tool that we're seeing in the picture here. Here's that tool in a CAVE and so it has this interface and one of the things we want to do is be able to kind of. In this particular case is count the number of I can't remember, microbes or whatever. I can't remember exactly what the state is but anyway you want to count the, or the cell nuclei or something like that. And so normally the grad students would have to go and look at 2D slices and say well, there's, you know there are 10 on this slice. And then look at the next slice. Well, there are 12 on this slice but 3 of them were still on that slice and we have to figure out, you know, which ones are which. Whereas in 3 dimensionally you can kind of go and just kind of tag them and you can do much better, a more accurate job of counting the things and also seeing again the relationship between things. And so here's the, part of the user interface for that. There's a transfer function to convert the density of materials into different ways. It's kind of a pretty nice utility for its time so that's one tool that we actually have running over in our lab at 10th and the Bypass. I've lost my mouse, there it is. Alright, so let's pause that. So anyway, so that's that tool in virtual reality or in motion. Here's another example, whoops need my focus. Here's another example actually using the same tool. Here are a couple of researchers who went to the Sahara Desert and they want to see, what are the wind patterns underneath? What were the prevailing winds 500 years ago over the Sahara right? And you can tell that because as the prevailing winds change the shapes of the dunes change. And so what they do is they go and they take this ground-penetrating radar and every meter they take a measurement right. And they go through [inaudible] and it takes them a few days to make these measurements. We reshaped that to the actual terrain and then you can look underneath and see the shapes of these dunes over time. And then they, then they in virtual reality, as I said, you know you can use human intuition a little bit better. It's kind of hard. Maybe they have a computer automatically figure out you know, what are the angles of the dunes in various places? What kind of properties do you want to look for? Maybe with machine-learning today we can do a better job with a computer but back you know ten years ago this is what we had and so you could, with kind of the human's ability to do things, take measurements. And then again, once you have those measurements take them back to your lab and that's when you actually do your kind of analysis. [ Inaudible Question ] Yeah. So this wouldn't. Well, when you say you guys, this is when I worked somewhere else.  Okay, okay.  Yes, at the Desert Research Institute. So, in this case they were researching a desert which isn't always the case but yeah. So here's one that I was not involved in. This is from UC San Diego where they have several CAVEs and different facilities. And so they have a program they call CAL VR [phonetic]. And they were doing some archeology so they start with light R data that you can see here. And then as the ground is being dug away they actually rescan it and then they can like archive where they found different artifacts and they have captures of that, and then be able to visit that in 3D. So you can go back in time, you know, pre-dig, pre-levels of, I don't know, unearthed-ness and see how things were. Here's a similar tool. This is from the super computer center in Stuttgart, Germany. And so they do a lot of. He has a general purpose tool where you can bring in a lot of different types of data. It's kind of like ParaView except features were added based on what the scientists he was working with wanted to do. Here's a picture related to the anecdote I talked about earlier with Gary Kinsland at the University of Louisiana, Lafayette. And so, this is. I don't know if this is the particular data that he was looking at when he had the insight but one of things they've done here is have these like, what they call magic lenses. So they have some general data and then inside these boxes they can see different versions of the data and they can have those overlaid and so you can only. You can like cut away parts of it and see different things. You can tell this is an old picture because they actually use a CRT monitor. Here's another one I want to show in a video form. And so this is another one from UC Davis. And so this is one where she's looking at the surface of Mars and is looking for different RIFs [phonetic] and different properties of different RIFs. But specifically you'll see that there's a picture of the researcher herself in here holding a little remote. And the reason that this is kind of interesting is that you can then use this as an educational tool. So, she was captured in VR doing VR. I'm going to show you a video clip of it as a video but it's actually stored totally 3-dimensional so you could look at her in 3-dimensions. You can actually be in VR while she's explaining things and see her, you know, inside the space with you. So let me go back to the movies. There we go. [ Inaudible Question ] Right, so this is really. She's captured with a connect right, so a $100 game device, so, and you know, that, the connect, I think the Connect 2 is still out and you can still use it and still get it pretty cheaply. There are places that are using multiple connects to try and capture things. There is more sophisticated stuff. Light R systems, you know capture a lot of points but they're not really great at real time right because they're spinning and so, and so they kind of capture from the outside in as well. There are people doing video captures. So one of the technologies that does this, that hasn't quite got to real time is this, you've seen it at football games and basketball games where they have a [inaudible] you know, a really interesting play and they want to show you the dunk or whatever from multiple viewpoints. And so they take cameras like 30 some cameras that are mounted around the stadium or around the assembly hall and then they take that data. They crunch it and now they can watch it, you know kind of in 360 or move around, zoom-in, zoom-out. So that's not really real time so when they do that there's somebody actually operating. Oh this is going, you know, I think he's going to dunk it right, so start recording start processing. And then about five minutes later in the broadcast they'll show you that video. But you know computers are getting faster. Algorithms are getting better so I think those kinds of things will happen. [ Inaudible Question ] Right. Yeah, so I mean if you're good with this. [ Inaudible Comment ] Here, let me just run the video. If you're good with, so for the people listening remotely the question was, what's kind of the state of? [ Inaudible Comment ] Right. [ Inaudible Comment ] Right. [ Inaudible Comment ] Right, so, yeah that exists here with the connect tool. This tool that we have now, this particular tool runs on LINUX so you'd have to have LINUX both sides but other people are doing it with other, you know other operating systems. Let me go and find this data though. Alright so here she is. Here's Mars without a section of the Sun it looks like. Right so this is. I mean you can see it's the connect. She's talking. I don't have volume control on my laptop at the moment. Oh there we go, it's on the big screen. [ Inaudible Comments ] Right, so this is again a video of her talking, explaining things. Let me jump ahead while she's actually interacting with the data. Right, so here's Mars. And so her hand is where, you know, she's seeing it in virtual reality. She's in virtual reality doing this. She's seeing it that way and so when she touches at a certain point she sees it from her perspective. We're just another person seeing it from our perspective and the two perspectives match. And again, we're watching this in video but I could also replay this in a head-mounted display or a CAVE and I could walk around to the other side of her. Now the connect is only capturing you know, one side of her. But I could walk around to the other side. I don't have to have this particular point of view. I can kind of zoom-in. And all this is captured and can be replayed in virtual reality. This is just a video replay of that. And so again, this same tool can be used to have people collaborate remotely as well and. [ Inaudible Question ] No, it's not. It's not commercial-grade quality at this point but it's certainly workable. [ Inaudible Question ] No, so, yeah so the question is, can she interact with us as well? No, this is purely pre-recorded. This is her talking. If I stand over there she's going to. She's not going to look at me she's going to look where she was looking when she did this, right. And so I can move around but, and I can pause her so it's like I didn't hear what she said, I can rewind her, but in this case she's not live. This is all pre-recorded and it's more of an educational thing. But like we were saying, we can do back, you know back and forth and see each other in virtual reality as well using basically the same technology. Alright, so we'll pause that. [ Inaudible Question ] This is Vrui yes. Yeah, if you see those menus that are like that, that's Vrui. Alright, so here's ParaView. So ParaView has a couple of different stages for virtual reality. This is from a couple of years ago, maybe four or five years ago, where our lab teamed up with Idaho National Lab and Kitware which is the company that does ParaView and we wrote in a plugin to make it work in CAVEs and walls and things like that. And so we could bring it into a system that already had VR running and make that work. We had kind of some rudimentary interface stuff that worked. Idaho National Lab was funding this and so once they stopped funding this we stopped developing the user interface for it. But, it generally worked and Idaho National Lab still uses it. Separately and we'll see this in a little bit, the folks at Kitware started working on it for the head-mounted display and with the VIVE and Oculus and we already saw that running earlier. Here's VMD which is a molecular visualization tool. This picture is a little coarse but you can see there's some molecule stuff going on there in the background. And again it's another. It's a desktop tool, this time where VR was kind of added as a plugin feature so VMD knows there's VR in there. You can map. A button presses on your controller to do certain things, start play, start in animation playing, resize things, move things, turn various features on and off in molecular visualization. And again we'll revisit that one in a little bit. I mentioned earlier this Shanghaied notion, so here's MATLAB working in VR and MATLAB doesn't know it's displaying some VR here. So MATLAB is displaying to the desktop and you can actually see that in this picture on the right. In the background here on this side screen we're not doing VR we're just showing MATLAB's desktop. And this data as it's being rendered to this screen here is grabbed and then we take those polygons and we map them into virtual reality so we can actually then walk around that same data. And so that's a specific tool that was done for MATLAB by a researcher at Duke. There are some generic tools that do that from commercial companies so Mechdyne, a company called Mechdyne sells a product called Conduit. A company called TechThis sells a product called TechThis or TechThis XL I guess. And so I call these generic in that the notion is you put this on your system anytime you're rendering graphics to your screen you can capture that and bring that to your computer. The problem is there's a lot of little kind of fiddley stuff whereas if you're rendering in it in this particular way it might not work. Or if you're running in another particular way it might not work. So they actually customize how they capture the stuff and how they process it for particular applications. So it's not quite as generic as the concept makes you think it might be. Alright, so, I'm calling those modern but now let's talk about some really recent stuff. So here's that VTK again. And now VTK has kind of formally introduced and has the ability to run in VR and here we've merged VR with Vrui if you can recognize those menus. And so there are some viewers. There's a volume viewer. There's a [inaudible] viewer which is a simulation product. And it's actually being integrated into VTK releases. So if you have VTK and Vrui, you can just make this work. Here's a program called SEVUS [phonetic] which was recently Open Source so, this guy right here, James Money [phonetic] was in the Department of Defense and he started working on it but then he went to the Department of Energy and Open Sourced it when he was there and so you can get that. And so this has basically time data, so it has light R data. It has drone flyovers with cameras which you can merge. And it can have other data and stuff that you want to keep track of. And you can again go back and forth in time and look at your data at different points in time. Alright, so I said we'd revisit VMD so a couple of features, we saw VMD in the CAVE, that picture. But VMD has had some extra things added on where it works in head-mounted displays. And it does a couple of different things, one is VMD is kind of known for its high quality rendering. The problem of course with high quality rendering, you don't get that 90 frames a second that you want and if you're doing huge molecules so this I think. I can't remember for sure but I think this is an HIV molecule right here which is just massive. And the processing for that, if you want it to kind of be semi-real time, in this case actually the rendering happened in a cluster of graphics cards in California and we could watch that here in Indiana or at VMD's home in Illinois. And what we would do, what the program would do is render that as kind of a big rectangle meant to be wrapped on to a sphere. So it would render it then in the head-mounted display you were just looking at the inside of a sphere and so you could kind of look around and see what was going on. If you wanted to move, you could move but then it had to re-figure out the sphere again. So it would actually get kind of grainy and then it would get high resolution. And so you could get kind of half and half. You can do a little compromise. You can have high resolution data processed somewhere but if you wanted to actually kind of move your head side to side you had to reprocess it. But if you just wanted to rotate your head and look around then you were good. You could look in any direction. And if we can get a sense of that I'll show some movies I have. I can play the movies on the laptop or you can play it on your phone or I can play it in a web browser. And so one of the things that came out of that is you can also do 360 rendering with VMD and so you can make your own movie, preplan a flight, preplan some features that get turned on and off, have the simulation running and then you can actually record that to a movie. And John Stone, the author of VMD has put some up on YouTube where you can actually just watch that and actually let's see if that's still in my browser tab here. There's my Chrome thing there. There we go. Alright, so this is. Oh, I lost it. Alright, so I will go and just search it. It's just on YouTube and if you search VMD 360. There it is, right. And so here it is in 360 mode and I can take my mouse then and rotate it around right, so I can look in any direction. These are water molecules, these red and white things and they are working their way through pores in a membrane, like a skin sort of thing. And then, somewhere on here if you happen to have a VR device you can click on the VR device. If you have this on your phone you can, you don't have to use your mouse to look around. You just use your phone to look around. And I can show you that after we're done talking here, so I'll just get out of here. For those of you who are remotely viewing I have a little phone here so there's a little phone viewer that you can carry in your pocket and put your phone on it and just run up that same YouTube thing. Run YouTube on your phone and you can just kind of look around like it's a little head-mounted display right? I'll show this for the, well it's not on the camera. For those who are watching remotely I'll show you later. So this, and then we can also watch that, that same video in VR. And so in VR I can just kind of naturally in the head-mounted display and I have that up and running too. So we have where there's a YouTube app for the VIVE right, on Steam VR and you can just watch YouTube 360s in the VIVE pretty easily. Alright, so this is I think the HIV one. I'll stop that. Alright, so VMD in addition to having that CAVE view has these lower, you know, consumer-oriented views as tools as well. Alright we've mentioned Vrui a couple of times and we already addressed this so Vrui as it was developed it was meant for CAVE-style VR, big screens. But as I mentioned earlier the author of Vrui got it to work with VIVEs and earlier versions of the Oculuses on LINUX and so you can do Vrui stuff. Here a point cloud dataset of the Temple of Isis in Pompeii that's been excavated and recreated through photogrammetry which we talked about last week. And so you can see that point cloud [inaudible] dataset and you have all the features of the Vrui interface in this, in the head-mounted display. Again I mentioned that the resolution you know, when you're looking at this kind of data the resolution becomes noticeable. Alright, and then we're back to ParaView where we, kind of where we started, where open VR is [inaudible] interface to the VIVE. It also works with the Oculus interface and basically as I showed when we got started, you just start up ParaView. You tell it you want to do VR. You load up your pipeline and you press the button and you're doing VR, so really conveniently done. And I mentioned earlier the notion of volume visualization, that tool that I showed earlier was for LINUX. There are some folks, the team at Idaho National Lab that we've been collaborating with, they've been looking at using Unity which is a game engine and works under Windows of course as well as OSX and LINUX but while it's meant for games it has the ability to actually do [inaudible] shaders in it and with [inaudible] shaders there's a lot that you can do in there to manipulate data. And so they're developing a volume visualization tool inside a shader which they then just use Unity as this kind of a shell to hold that. And since Unity already knows how to do VR it all kind of works together. I contacted them a couple of weeks ago. They're not ready to share it yet so I don't have a running copy of this just yet. And then, this is. We looked at this data two weeks ago when we were looking at ParaView. This is the globular cluster stuff from a Professor Vesperrini [phonetic]. So a couple of our team members took that data and made kind of a fun application of it in order to demonstrate it to you know, kids at the Science Fest right. And so this if you go to this URL down here you can actually run it, again on your phone. Also though we can run it in the VIVE just like there's a program called Supermedium which takes web-based VR spaces so there's a programming interface called WebVR which is like WebGL, it takes graphics and puts it on the web. It takes those graphics, adds VR to it and there's a product called Supermedium which will load those webpages and just let you watch them in the VIVE and so this is one of those webpages you can just watch in the VIVE. And I can demonstrate that this afternoon as well. Alright, so those are kind of the bulk of you know, tools designed for scientific visualization. There's a handful of other commercial software that we could kind of make use of for visualization so this one's called ProspectPro, it's an architectural-based tool. It'll load though any sketch, SketchUp datasets so if you had science data and you brought it into SketchUp it'll load way front models. So if you had a molecule you could load that in. And you can do annotations. And this one lets you share as well so this one actually works across the network so you can create what they call a meeting. Multiple people can join these meetings and all be in the same space and they have these little crazy-looking avatars here with the little, kind of pin-point sticks so you can see where their hands are, where their heads are. And when you're in there you can annotate. So over here in this, in the left-most image these arrows and blue lines were drawn by people in the space, they're not part of the model. They were actually drawn there and those are persistent so if you create a meeting, that meeting and any annotations you have stay there. And then if you join that meeting later, as long as the person whose computer this is being hosted on is up, you can actually see those annotations from the past. [ Inaudible Question ] Right, so it's not cloud-based so it's. It's on somebody's computer and then they join in to that meeting and then it goes, I mean it goes through the company. Iris, IrisVR is the name of the company and then they kind of connect you at that point. [ Inaudible Question ] I'd have to. I don't know, you know 100% the details on that so, that's my understanding but I could be wrong. But I've tried it with like up to three sites at a time and usually I start the meeting one first but that, that may also be just from habit right. We can investigate that and try that. We have ProspectPro on this system but I only have it. I don't have it on the back one so we can't do the collaboration. If you want to stay around long enough I can install it back there but. [ Inaudible Comment ] But we can look into this later too. [ Inaudible Comment ] Yeah, so oh yeah, so Jeff [assumed spelling] is online. Jeff, the question was, although I can't hear Jeff unfortunately. If you were to type in the chat message Jeff, the question was, how does, how do ProspectPro share data? Is it like a cloud-based thing or does it go through IrisVR? So, let's see if I can start the chat up here somewhere for Zoom.  So you're saying you can't hear me?  I can hear you talking Jeff but I can't. It's so muted. It's so low. I can't hear what you're saying.  Alright, I'll type it.  Something is screwed with my settings. I can't find the share either. Actually someone else, are you on chat Essen [phonetic]? [ Inaudible Comment ] Alright, so Jeff if you could answer that question in the chat space. Essen will tell us the answer. And if you have a question about the question we can clarify. Another tool is called Gravity Sketch so this is more where you start with a blank slate and you start designing things. And it has tools to like mirror things and do smooth surfaces and create you know primitive shapes. And so again, this is kind of more for, as I said, building stuff. Maybe if you want to just sketch out some things. I'm pretty sure this one is not a collaborative so this would just be you. You can save the date and load it back but it's not collaborative like ProspectPro is. Tilt Brush by Google is another kind of famous one. This one is continually being enhanced because people are actually starting to use this on a more regular basis and so Google's been enhancing it. But it's really just a paint program with some fancy paint brushes. It does have the ability to mirror things and do, draw straight lines and things like that. So, and you can walk around. I can run Tilt Brush if anybody wants to see Tilt Brush later. And then finally there's Google Earth which if specially when I, as I mentioned before if you're going out into the literal field and you want to do some recon on that Google Earth is a good way to do that, find out where different places are and so that works pretty well. So, that's, from, so at this point we can now do some demonstrations and you can try things out. We have a VIVE running up front. We have another VIVE running in the back. And for a lot of the things like the Nanome, the molecular viewer, the CalcFlow, I forgot to mention CalcFlow. The slide got lost I guess when my machinery rebooted. So there's a couple of commercial products called, well actually so there's some pictures of it here so let me just talk to it here. So, here's CalcFlow, use the mouse. So this is a, a commercially-based tool. You can get it at Steam VR so run it out of your Rift [phonetic] and your VIVE and it shows different ways of doing like mathematical expressions and it'll flop those out for you. The same company did this program called, so this is actually. This was a student project at the University of California San Diego. They started this up, they put it on the Steam store and sold it for like $10. And from that they decided to start a company. You can now download this for free, they've Open Sourced it. And then they, now they're working on this molecular viewer where here you can see they're manipulating this DNA molecule. But it'll load basically any approaching data bag molecule and you can manipulate it. This one is collaborative as well so people, multiple people can be joined in the same session and talk to each other and point out things to each other. Joe who had to leave early today, I'll let him. He arrived early because he was leaving early so he actually did the little tutorial on, in this earlier today, so if anybody wants to try that out we can see that too. So any questions before we go to the demos? [ Inaudible Question ] The molecule, it's called Nanome. So it's N-a-n-o-m-e. And I've heard it pronounced Nanome as well but I don't know how they pronounce it actually. So their company is that, Nanome.ai, I think. [ Inaudible Question ] Right, so the question was about, for those remotely, the question was about standard datatypes that are available. And so there are several standards right [laughter]. For molecules, the protein databank is kind of the main standard, so most molecular tools are going to be able to load PDB files. For light R, for point clouds there's LAS which is a standard and so Oliver's program light R viewer will take that and then pre-process it into his format. So for light R viewer, it has a special format because he's loading in data and taking it back out again. But you can take standard data from LAS and/or just ASCII, come at separate values and process those into his format. And then you can actually take his data and go back the other way too if you want, if say you erased some stuff. You know you go in there, you process it. You want to erase it, some data, like if you did photogrammetry and you had some data points that you didn't want you'd go and delete the data points you don't want, save back the ones you do want and then do the rest of the processing that way. And then for like ParaView, well in the last two weeks we saw the huge list but really just kind of you know, x, y, z plots with data at each, you know, several bits of data at each thing, right. [ Inaudible Question ] So, there is. There are a couple of tools which I didn't show you that haven't been developed in a while but there is one. I can't remember the name of it but basically it is a point-edge visualization tool, a connectivity tool. I cannot remember the name of it but there's a Vrui tool that does that. [ Inaudible Question ] Right, so for those remotely, a recommendation for Vrui [laughter]. Essen, yeah? [ Inaudible Question ] Yeah, let me [inaudible]. I'll give you the microphone. [ Inaudible Question ]  So I'm going to relay Jeff's response to the question. The Prospect apparently does not store the data centrally. So a user who wants to collaborate with others creates a work space and all the data is shared with the participants and they have to copy that data locally so everyone has a copy of the data and they collaborate on that data using the local apps on their machines. And apparently there's some desktop app, it works at Prospect so.  Right, there's a desktop interface, yeah. So if someone doesn't have VR they can still participate in the meetings. So okay, good. [ Inaudible Comment ] What's that? [ Inaudible Comment ] Right, yeah, so you have to shift the data around. And so I didn't say oh, so [inaudible] kind of answered back to this question about data [inaudible]. So ProspectPro will load kind of standard architectural stuff, Revit. It also loads Sketchfab or SketchUP. [ Inaudible Comment ] I'm not familiar with that one. But [inaudible] Jeff, ProspectPro? [ Inaudible Response ] Right. Yeah, it has a couple of different formats that'll load sound. [ Inaudible Comment ] Right, no it sounds like you're responsible for sharing the data on different machines. I guess I, I should have remembered that because I did that myself. He says what? [ Inaudible Comment ] Rhino, yeah, so but they are actually adding. I mean they just added a new, a new file format I think last, a month or two ago so. You know, they're addressing. It's a licensed software so you're paying for that one so you can get some customer support you know on that and tell them what you want right. [ Inaudible Question ] So anything Rhino can love, okay, which includes Step I guess. Right, any other questions, otherwise we'll go and do the demos and we'll stop our recording. Alright, very good and so next week is Augmented Reality and Visualization and Chauncey Frend will be presenting that, alright. [ Applause ] 