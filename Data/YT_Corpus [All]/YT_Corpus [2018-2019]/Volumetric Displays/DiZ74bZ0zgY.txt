 Welcome, everybody. My name is Chauncey Frend and we're today to talk about augmented reality and visualization. I work in the UITS Advanced Visualization Lab. And my slides won't advance. There we go. Okay. I'll come back to that. So the lab I work for is a UITS group. We're been around for about 21 years or so now and what's exciting about our group is we're always evolving in terms of visualization research support. And of course at IU we do a lot of creative activity support but I'm going to focus a little less on creative activity support and a little but more on just scientific visualization and visualization in general. Using augmented reality as a tool today. The context, I guess, for the history of our organization, we started quite a while ago in Bloomington with some like kind of flagship virtual reality systems. So VR was a first class citizen for a long time in the lab. And augmented reality is actually kind of new for the lab as far as support goes. What's great about kind of growing with the university is you get different requests and you start to realize there are different areas than support. So we spread out support and we've gone into all sorts of areas. Our first decade was all about the flagship facilities and then we've gone into distributed visualization type projects. [Inaudible] an example would be this IQ wall for instance. This is built by Chris Ellar, he's sort of a local IU integrator for this working with many different vendors. And there's almost 20 of these around different campuses across IU. I think he's up to 19 now. Tomorrow he's putting one more in at IEPUI. That's exciting. And then now we're starting to really look at increasing utilization. So augmented reality plays a role big time with this considering that, you know, there really are no flagship facilities for augmented reality I can think of. I mean HoloLens would be the closest thing but that's a multi thousand dollar headset that you can wear. It's not really as big of an investment as some of these other virtual realities things. But, you know, mobile devices are leading the way. So we're seeing folks bringing their own devices. So software support is playing much more of a role with AR. And that's most of the time I spend supporting researchers is in helping them get their work flows completed. So our goals for today are just to kind of get acquainted with AR. Some basic stuff if you guys have already started with AR. This will be a little bit of a review. Some history. How I sort of introduce AR into folk's lives as far as using it in their work. If they're a technology person or not. And then how do we make our own AR and how do we be smart about that. And then of course we'll talk about the different platforms that are available now in, what, September of 2018. it'll be different next month. It's very quick. Fundamental forms. How I kind of breakdown a few different technologies within AR in terms of tracking systems. And then some fort of future facing concepts in terms of tracking systems, sub systems. And then I'll have some resources or you at the end. So the roll we play at the university, like I said, isn't necessarily to build big augmented reality contraptions for you to come use like we do with VR. It's more about actually just having workshops available right now because people are dabbling and using it in more sophisticated ways and less sophisticated ways but it's just a matter about spreading the word as far as our kind of core unit understands AR and supports it. And then I document only key work flows through mostly video tutorials. So if I get, you know, within a year's time, say four requests or two requests for the same concept that isn't obviously available through a popular augmented reality tool, let's say [inaudible]. I might go ahead and and spend the time to spin out a work flow and build a video documentation on this and then publish it for the world, you know, not just IU. And then to kind of just break the thing that's slowing everybody up. We do a lot of one on one consulting of course. And I really have to ask people right away when you're working with augmented reality, is why, you know, why do you need to augment data into the real world. You know, why can't just make it a simpler problem and solve it with just displaying it on a screen or using virtual reality, you know. Augmented reality is often used because it's cool and that's fine. For marketing purposes, that's completely fine and of course there are others. You know, young students engage in technology, I understand all that. But when you're working with research, sometimes it's not necessary to sue AR. So if they ask some hard questions. And yeah I'll talk about some available systems for you now. So from the academic perspective, this is where we're talking about. You know, why exactly do we need augmented reality? So the thing people can agree academically with, say VR and AR, is this continuum. This kind of derived from Milgram's continuum which was described a little bit more abstract in the early 90s but I'm going to just lay this all out there because I think people can agree that if you have a line. And on the left side, say it's the real world and on the right side you have the synthetic world, or virtual world, virtual reality. In the middle somewhere you're blending together some ratio of the real world and the fake world. Or computer graphics world if you will. And then you have augmented reality. And there's a lot of terms out there. There's mixed reality, there's augmented virtuality, and a lot of these come from commercial development. So Microsoft is not saying augmented reality for their HoloLens device. To Microsoft this is a mixed reality platforment. Some academics will follow on that because they've been publishing on mixed reality for years and years and they'll have a distinction saying, "Oh it makes sense to call it mixed reality because it's taking more understanding of the real world in playing a role." Instead of just like what they would consider lesser, you know, applications being just overlaying content over the real world. It's a big debate. It seems like Pepsi Coke, you know, when I get around a academic [inaudible]. We talk about what is mixed reality, what is augmented reality. And it seems like everybody's got a way to describe it. It's not a big deal because I think at the end of the day we can sort of agree on this continuum here. And maybe mixed reality fits in somewhere else but not immediately clear to me. And it doesn't really matter. But this is really the question. I mean, not really the question but sort of the statement you have to make when you're building an augmented reality is, are you putting what matters where it matters? So, actually it was IU's slogan a few years ago. It's on big red two on the front of the [inaudible] it says, "What matters where it maters." Like augmented reality. Great. Okay. So a brief history. This is from my perspective quite honestly. So AR was something I did as a student for fun and it developed into something I supported in the lab as I ventured into the AVL. So the term was first kind of ascribed in 1901 in the master key. This was of course before computing is what it is or what it was. And you know, Ivan Sutherland was like this kind of maven in [inaudible] display technology at the University of Utah. And he created what's called the Ultimate Display. That's a picture there of it at the top. And moving on. AR toolkit was one of the first tools I tried to use. I was not a strong C++ programmer but that was the original language that that tool was in. So that was from the late 90s. And then [inaudible] came out, so that's a plug in that was developed initially by Qualcomm. Now is owned by an engineering company called PTC. And that's a very popular -- and then Google introduces Google Glass. That was a really successful marketing campaign but not that really successful of a platform. But it brought augmented reality into the kind of the enterprise crowd's terminology. And then of course Niantic when they brought Pokemon Go, brought the word -- or the term, augmented reality sort of out into the zeitgeist of everyone. So most people can tell you what augmented reality is thanks to that video game. And then the HoloLens come out in 2015. there's a lot more history of AR but those are the main ones that stick out in my brain. I actually lifted this chart from our virtual reality support talk. It really does apply to AR as well. So this is actually pretty important moving forward with this talk. There's three main use cases that you can kind of point to for augmented reality and this simplifies the support end of things and how you design it. So the simplest and easiest use of augmented reality is finding a preexisting application that already exists, a mobile device app or an app off the windows store for the HoloLens, and data that's already in the app. So basically this is a totally created for you experience. That's really what it is. It's just an experience that you're borrowing and using and maybe just integrating into a teaching curriculum and that's the fastest and easiest way to get involved. With a little bit more sophistication, maybe some programming. There are viewers. There are augmented reality viewers out there. This is like what furniture manufacturers are starting to do where you can augment furniture around, you know, your space, that scale. Or you're maybe introducing your data, which would be the furniture and then the viewers, you know, whatever the viewer is. Or for a doctor, you know, if you have like volumetric data, like say DICOM data, which would be, you know, an output from a CT scan or an MRI machine. And it slices pictures that have to be converted into some sort of volume rendering. Or, you know, there are viewers to do such things like that and we're going to cover some of that. And then of course something our lab's been helping with for the last 20 years with, VR and AR, which is building your own custom application and using your own data, you know, with that. So that's going to take the longest amount of time, the most investment from your end. But you're going to have full control over what the end result it. So that was type one, two, and three. And it kind of just increases in complexity and I'll reference that as we go through it. Okay so this is a type one case. So this is the easiest case and this is something that the [inaudible] department showed me a few months back. It's made by the World Wildlife Fund and this is an AR kit application so it exists on an iPhone. It's free I believe. And this is a teaching application where you actually would plot a landscape on a table. I got a video here, go ahead and play that. I'm going to ask Robert Ping. Can you see this video by the way? You can see it? Okay.  I can see it. Great.  Excellent. If you can hear it or not, it's not that important. I really just want you all to kind of see what's going on here. So using an iPhone. Right so. A simulated experience in the real world. The benefit of why augmented reality with this probably comes into play when you're talking about teaching because you can display that. Multiple people can look at the display and you can kind of -- it's not a shared experience. Like I can't look at the same thing as someone else with another iPad or iPhone but I can at least kind of crowd around one screen with someone and say, "Oh you see that? Try this, try that." So it's somewhat of a teaching tool. It's just one example. There are many. If you just search augmented reality on android and IOS store if you don't know that already, there are lots of apps out there that use a variety of back ends to do some somewhat sophisticated AR and somewhat not so sophisticated. There's a lot of people tinkering and publishing on that. Okay. So this is an example of a type two. So this is like a viewer. Trimble is a commercial operation that used to -- they basically sold their product, SketchUp to Google at one time and then Trimble bought it back from Google or maybe the contract ended. I don't know. But it's -- Trimble is basically into BIM. Building Information Manage? Is that what BIM stands for? Management? And BIM data is something that's generated usually by some sort of, you know, architectural group and they're going to informing the contractors on how to actually construct a building or are maybe where maintenance can access certain areas and Trimble Connect is an extension of their product series that allows you to do this on a job site with a Microsoft HoloLens. And it is a shared experience. You can have multiple HoloLens all looking at the same design. I do have a video on this one too. This is actually one of their customers using it so that was helpful for me to realize this wasn't ilke vaporware, it's actually somewhat useful. Now we don't have this available at IU but I did want make people aware that does exist. You could of course build other things SketchUp than BIM data and probably augment them on a site but you can then again use things other than Trimble SketchUp to do that. But it is interesting to see this sort of outlet, you know, and this market being engaged. Okay. Moving on from there. This is one I actually found recently. Some of you involved with geography, geology might find this interesting. It's a data platform and it's an open source project called, it's really just called HoloLens Terrain Viewer. And Esri, the maker of ArcGis, published this in 2014 and they evolved it as they got access to a HoloLens and then they stopped suddenly in like 2016. but it's open source [inaudible] so you can download this and use it. I was surprised that [inaudible] so I'm going to contribute to the project and update it for the latest copy of Unity and the HoloLens SDK. But I was able to get at least their networking feature working so I'm going to do a live demo for you here. I'm turning on the HoloLens right now for those watching online and I'm going to stream my screen actually. So let's escape out of this. Go to the device portal. I think this will be it, Bill, you can probably see it just through the screen share. Is that true? Can you see what I'm seeing?  No.  No? Okay.  I think it has your other screen. It has your [inaudible]. It has your -- yeah. There. Yeah.  Now I've gone full screen it. Can you see it?  Yeah.  Okay. So ArcGis is a resource for folks working with terrain data and what's nice is they have a huge data collection that you can pull from. So in the HoloLens here I can see my cursor but I put the app up here on the wall. So now I've got this sort of platform. And when I finish out fixing up their code I'll be able to actually attach that to the table. But for right now I can just make a verbal call so the interface is verbal site and say show Hoover Dam and that's loading in from the web. So just a little misalignment there. Let me step back. Show Los Angeles. All right. So the idea here is that you could have multiple HoloLens all looking at the same but you can pull down data from the web and it's just going to go ahead and build it out for you. Show Machu Picchu. Show Machu Picchu. There it goes. So it just acts as a data platform but you can extend it from there. If I open up their github repo. Which I'll share a different screen. Actually no. I can just leave this. Maybe I put it -- This I just wanted to make clear on how kind of malable this project is. They have this configuration where prior to building the app you can put in your voice commands. And its simply a C sharp file here. So you can see they've got Hoover Dam as the explicit thing. You can say show Hoover Dam and then you use the coordinates just from GPS coordinates. And they actually recommend using Wikipedia's as a resource for certain like major landmarks. You can use Google Earth as well. Okay. Let me make sure I'm sharing the right screen. [ Inaudible Comment ]  Okay.  Yeah. So that's an interesting open source project and there's a lot of them. Microsoft has the MR toolkit. Mix reality toolkit so this is a collection that anybody can contribute to and that Microsoft is sort of arbiter of what actually goes out publicly, but it's been branched like a ton of times. It's really popular repo on GitHub. And they have a medical example where you can actually take DICOM data and actually convert it to work in the HoloLens. What's great about this package is you can also use it on their windows mixed reality headsets, which are VR headsets. And, you know, you can compile for the HoloLens or those headsets like very quickly like I want to say less than 10 seconds of adjustment in terms of the configuration. So again, another video. I'm going to pop back out. Drag this over. There we go. You see that okay, Bill?  Yeah.  So really, they're just moving around a sort of a viewing volume of this DICOM data. There are of course lots of ways to view DICOM data and there are some more advanced virtual reality systems that in my opinion, actually render it a little bit better. But if you're just interested in the HoloLens this is sort of an entry point. Or Windows Mixed Reality. Okay. You get the idea there. Let's go back to screen two. Get rid of this. So at IU, some examples to show you today. This one is a project that I worked from with mechanical engineering technology. This is actually technically a produce school but the problem to solve was that students in class were having a hard time -- these were, I think, freshman, sophomore, as far as their undergraduate progression goes. Having a hard time conceptualizing the 3D shape of objects that they were milling, machining. Like, machining, levying, whatever the part was for. But in the end, they were assembling a miniature steam engine. They would hook up to an air compressor that would simulate the steam and it would operate if the tolerances were proper and it was assembled properly. So to help the students actually generalize the 3D shape of these objects, we created an augmented reality application that they could install on Android devices. And the purpose was essentially just to look at the blueprints and just have the parts augmented off the surface. And I did a video of this one today as well so I'll bring that up. Okay. So the student actually built this completely herself. I did a lot of one-on-one consulting with her. So this is, again, a type -- you know, the third type of completely custom map, custom data as far as an AR application goes. So again, I asked the professor, you know, why didn't you just use regular parts on the blueprints to demonstrate the 3D shape? And he said, ah, augmented reality's just kind of cool. So, again, the cool factor of augmented reality really does play a role.  [Inaudible] question is just Android, iOS [inaudible] developed?  That's right yeah.  Is one easier than the other [inaudible]  So the question was does Euphoria, which is the plugin that is involved in this application, does it work on Android, iOS [inaudible] and yes. It's a cross-platform plugin and it only really works on mobile devices. You can think of the [inaudible] as a mobile device. Magic Leap is another new headset. We don't have those at IU yet, as far as I know. Our lab's looking at maybe getting one. And the interesting thing is, like, Euphoria, I just heard the other day they're adding support for Magic Leap now that they're SDK [inaudible] so it's just a -- sort of a rapid cross-platform plugin and really it comes down to just image recognition. And you, again, are limited to mobile devices. So I couldn't run Euphoria on, say, you know, a desktop. And then here's another example.  [Inaudible] then it's easy on which platform?  Yeah. It is easy to implement on all of those platforms. It's easy to just to develop for, so it's a little bit more difficult to compile an iOS application than it is for an Android device but, you know, it's not much more for those. It's going to be the same amount of time to develop. Is that what your question was?  Yeah.  Yeah. Okay. So this one -- what's interesting about this project, and I'll come back to this concept, is we used digitization. So this is a service that Jeff Rogers talked about, I think it was a couple weeks ago, two weeks ago. And we actually were building a training tool for folks learning how to work with medical, biomedical devices. So you could imagine a defibrillator kit having a augmented reality training scenario. And the students that worked on this, again, they were a team I worked on with one-on-one and -- oops. The purpose of this was to build as simple of a scenario that they could maybe pitch this to companies local in Indianapolis and it dilled out the same scenario for like maybe a MRI machine or something like that. So at the time, Euphoria was really the dominant and most reliable workflow and it was sort of the easiest to teach, so that was the choice to use an image target. So we're looking at the real devices there on the left side of that screen, and then on the right is an augmented version of those that were 3D scanned or digitized. And very large buttons titled 1, 2 are the steps. And there's actually audio feedback that kind of kicks in. So when you click the buttons, it'll annotate and then give you some sort of narrator feedback what to look for.  Is this still use [inaudible] for the [inaudible]  Yeah. So that sheet he rolled out, this was what you just anchor everything to at. As we'll talk about later, tracking systems with augmented reality are really a Wild West show right now. What was neat about this was these students, some of them it was their first year ever in college and right away within the first three weeks, they're using a, you know, 25,000 dollar Creaform ghost scan and they're learning how to 3D scan objects and then I'm teaching them how to program and see sharp. So rapidly they became pretty good implementors of this concept. Oh yeah. And then this was before the HoloLens was available so they bought this off-the-shelf see-through hardware. In my opinion, I wouldn't recommend it for anybody. I don't even remember the brand. And they tried to get that working. It kind of worked but lining up things with optical see-through headsets is tricky. But luckily there's devices like HoloLens which they're pretty much solved a lot of those problems for. You don't even have to think about it. So kind of going more advanced, this is -- what's cool about this, the last couple I've been showing you are that type three where you're building your own application with your own dataset. This one, we started like that. So we built our own application to do previsualization of prosthetics. So the story goes that Dr. Blacey here worked with a team of informatics folks in our laboratory to help them build a workforce for 3D printing out prosthetics. So, unbelievably, if you're going to get a prosthetic for your leg, you'll go to a different type of doctor versus if you have a prosthetic for your face. You'll have to go to a dentist, a prosthodontist. And Travis asked me one time, "Hey, how can I maybe previsualize these sculpted prosthetics?" I don't know. They probably didn't make that clear that when they 3D scan a patient's face, that patient may have had cancer at one time and had a portion of their face removed just to save their life. So this poor guy, Shirley Anderson here, he's actually missing his entire lower jaw and he's wearing an application in this photo. So Travis would like, since they're working with digital files, an iteration loop, is really what he was asking for. "How can I, as a doctor, meet with my patient, build an application, and if they have complaints about it, respond to that feedback? But then also before responding to that feedback, just augment on their face what it might look like if we made changes." So that's called previsualization. It's utilized in a lot of different areas. That would be what we were looking at with Trimble Connect. That's previsualization of a construction kind of terrestrial environment. So this student's very ambitious, Mark Sporliter, he reached out to me and he said, "Hey, I could scan my head and build sort of a one-size-fits all for different size heads, 3D printed head gear and we could build an application where we extend maybe the HTC Vive, which is a VR headsets tracking system, to work HoloLens. And at the same time, I just so happened to stumble upon a GitHub open source project from the University of Rochester that allows you to make the Vive tracking system, kind of a sub tracking system to the HoloLens and calibrate them together. So we worked together to just kind of build a quick mock up here and that's what you're seeing in this photo is Mark actually with the head gear and the tracked object there and a previsualized sculpted nose for Mark. I actually have two quick videos for this one. So this is the scenario, all right. So the doctor, you can -- as you might imagine, can wear the head gear while the patient kind of just sits there but -- oh yeah. Come on mouse. Yeah. I don't know why my cursor doesn't want to show up. All right. And then this is, early on in the development of this project, this was a successful test. We were able to extend the HoloLens to work with this open source code. So you can see there's latency built in. You know, this would be something later to fix. There's a lot of extra features we could have gone with. But, you know, as we went along with this project, Apple announced face scanning, like, real-time face scanning in their ARKit project. You see what happened. We started extending this, just kind of just boiling the ocean, working very hard to build a prototype throwing hardware at the problem. And then Apple says, "Oh, no, we've got this, you know, free solution through our, you know, our free plugin [inaudible] iPhones. And it is face tracking, so Travis is on that now working with Apple developers at IEPY to use -- get previsualization with facial tracking. So then what's nice is you -- you're able to actually render the -- I don't have any examples of that yet, but to actually render the facial prosthetic and then as you move your face, it will transform with your face's morphology. So again, bring your own device. That's the model we're following. Like I said, there is not really a flagship augmented reality solution that the AVL has really deployed over the years. Of course, we're probably going to keep up with some of the high-quality sought-after platforms that are somewhat expensive, a few thousand dollars for these headsets. This one is like 3 to 5 depending on the software but the Microsoft HoloLens and these are I think about $700 cheaper than that. So this is the magically one. These would be emerging head mounted displays And then of course as I've been already kind of talking through there are lots of mobile devices, tablets and smartphones that can do augmented reality. Still do augmented reality on desktops by the way. It was originally the form that ARToolkit used, through just C++. It was a webcam and a desktop. People built kiosks. I was involved in a few projects where we used kiosks through just the desktop. And as we can imagine, you know, this type of headset, As it becomes more commoditize and just gets better and better, if we start seeing this in consumer's hands, people just kind of merge into the same kind of realm as these devices right? They won't be emerging in same platforms or advanced platforms. They'll be more of just a regular mobile device. So our role in the AVL really comes down to just really keeping an eye on useful workflows, staying away from commercial products. If we can go open source, let's go open source. I'm really excited about new web technologies using, say, JavaScript backends where you can distribute widely over the Internet just as web applications for augmented reality. So in terms of folks coming to me asking, "How do I make my own augmented reality?" This is a tool -- couple tools here, HP Reveal, which used to be called Aurasma, and Wikitude. And these are what I would consider viewers. You have some control because there is some sort of publishing step you have to take when you're building out content. You can think of HP Reveal as a platform that allows you to essentially put any 3D, 2D text, you know, any kind of media data where you want in terms of an image target, okay? And they have a publishing function within their own application so that they have their own ecosystems. That sounds a little weird, but what that means is if you downloaded HP Reveal, you can search through HP reveal and find thousands of publicly accessible projects that people are building. And, you know, a lot of this is just quick mockups, you know? Say like I wanted to augment this IQ wall. I might actually just have HP Reveal set up with a selection of different IQ wall designs and just kind of put a target on the wall and click one and render the IQ wall and then chat with folks as you're in the room. It's a quick way to do it. Same thing goes for Wikitube. But the other problem that Wikitube solves it the other types of tracking. So it does image tracking as well, and it's a little bit more involved in terms of developing, but you don't have to program using Wikitube Studio. But it will do location-based augmented reality, so you can do GPS locations, beacons if you've ever heard of those. Those are usually devices that emit some sort of RF that talks to your mobile device. You can actually have AR pop up when the presence of a WiFi SSID is around. So these two are what I consider onboarding tools, folks to get started building augmented reality that don't want to learn programming. Now in terms of, like I said, like building your own app type three development, the base package is almost always Unity these days. And from Unity there's a variety of plugins. Not from Unity, but from other venders, other open source projects. All of these are commercial except for ARToolkit. Euphoria is a dominant one. Like I said, it's set up to work on mobile devices and it's for image tracking. And ARToolkit works with image tracking, but it'll also work on a desktop. So you can actually publish out to Mac, PC, Linux as a standalone application which is something you can't do with that or this. Basically any of these. The back-end of this now is C sharp and Unity, as you can imagine, if you're familiar with Unity. Microsoft HoloLens has a plugin that used to be outside of Unity and is now built into the package itself. What's nice about that is you can hybridize other types of packages. You can actually hybridize this with that and do -- and gain image tracking within the Microsoft HoloLens, which we'll talk about later. AR Core is an Android deployment for competing with ARKit. Literally, they've reacted to ARKit by producing AR Core. So from what I've read the Google engineers were already dabbling. And they're like, oh, well let's just hurry up and get ours up to the same spec. We'll just kind of tit for tat meet Apple. Just the way they do with every -- ever their mobile product. So Apple, if I didn't make it clear, makes ARKit and that's what I'm showing here. So iOS versus Android. And these two modern packages that are only recently deployed, they do a type of tracking called aerial learning tracking. They use a webcam or maybe a laser scanner built into the device and they just keep an eye on the room, the environment that you're in, within a certain range. And you can kind of find flat surfaces. So that earlier demonstration I showed you with the whole wildlife fund, where it had the terrain on the table, that was an ARKit example. It could have well been developed in AR Core, but as you deploy it, you can actually click on the table and that locks your content in that spot and then you can walk around within a reasonable range, like a few meters in every direction, as long as you kind of continue to come back to that spot and your content will stay locked in that position.  Is there [inaudible]  You know, that does ring a bell. Someone said something about it.  That [inaudible]  Oh okay. Is that recent? Yeah. So the question was is ARToolkit still an open sourced project or is they -- are they bought out in commercial? They did go through a big branding thing and if you're saying DAQRI bought them, I believe you.  Yeah. I think DAQRI bought them and then opened, then put it back out [inaudible]  Oh, they did. Okay. So DAQRI bought them, put them back open source. That's good but still -- you still have now the commercial connection so you have to be suspicious of such things. Because as the product gets better, the toolset could just disappear one day. That happened with, as folks would go back maybe a few years developing augmented reality, they might have used one called -- people used to call it Mataio. That was the way to pronounce it. But it was actually Metaio was, yeah, Apple bought it. And then it literally disappeared the next day and they had a webpage up on their developer form saying no more support would be offered and your terms of agreement are terminated. You know, basically they just stopped letting you use the plugin. Horrible thing. But, I mean, that's an illustration of what you have to be worried about. People make companies. [ Inaudible Comment ] What's that?  [Inaudible] more [inaudible] next week will honor [inaudible]  That's awful. Yeah. This -- yeah, we're nipping at the heels of Silicon Valley and things need to be monetized. I understand. But, you know, academic research needs to be respected and needs to continue to live on. So choosing the right tools in the beginning is always a good idea. All right. So I'm an analyst for the laboratory and when I work with somebody that wants to work with augmented reality that may not be familiar with it, I've got to bring reality into the conversation in terms of limitations of systems. I've probably confused you, if you're not aware of these plugins, on what they can do as far as mobile devides versus what type of tracking and what does that mean. So this is a chart that I broke down and it's pretty busy. At the top here, we have augmented reality tracking systems and then I'm not sure why the IQ wall grayed out a couple of these but they shouldn't be. They're all equal. But you have these different zones of types of tracking. So you can all imagine probably marker tracking, this is what we're used to, right, if you're familiar with augmented reality. Those are, like, 2D QR codes can track, and as long as the device camera can see that, you can track off of that images. 3D objects can be markers to be tracked. So these are anchor points, you know, starting points for your content to stay and let you spatially examine or interact with it. And then beyond that, there's body tracking. You know, that's not as common but you can augment body parts. In fact, something I don't highlight in this talk today, projection mapping is a type of augmented reality. You're still putting data into the real world in some sort of visual fashion. And folks are doing that with projection mapping where you can track a person's body with, say, an Xbox Connect and project onto their body. They've even done this with dancers. It's actually happened here at IU now a little bit. I can't recall exactly the name of the project so I'm going to have to look that up for next time. And then location-based. There are a variety of tools to do this. You can extend most of these packages to do location based if you're based in Unity because it's a cross-platform compiler and most mobile devices have GPS's in and Unity has base-level support for location-based detection for events. So that's kind of interesting. And then aerial learning is this new sort of magic area. So in just the last four years, we've seen an explosion in tools, hardware and software tools that can do aerial learning and they all have different ways of doing it. I made a GIF for you all to kind of express what I mean by aerial learning if you're not already aware. You have some sort of display device that can detect the real world and actually assign geometry to it. So what you're seeing here is a view through the HoloLens. Oh, I should've set that to repeat. Let me reset the slide. And what you're seeing here are polygons of the IT 414 laboratory and the AVL in Indianapolis. And as I look around, it's just annotating the room with these polygons. But what's nice is that's a spatial map. That's a 3D geometry collection that I can extract from the HoloLens and utilize so that I could augment in that space or just detect to have occlusion. You know, that's another thing, is if I have a piece of furniture here and a hologram behind it, some sort of augmentation, I want that furniture to occlude it, to block it when I have it in between my line of sight and the object. All right. So some advances techniques I'd like to discuss today include a few tutorials, all right? So these are some resources for you and also just some pointers. So on the left there, you're looking at tracking system fusion. So you're seeing a view through the HoloLens. And I'll try to explain as we're going along. And this is a long tutorial. It's 32 minutes long. And I'll explain what's going on here. The HoloLens does aerial learning out of the box. It already can accomplish this in an automated fashion. So there's an -- a map happening in this Unity scene. You're looking at the Unity editor here and I'm aligning and image, okay, to the top right of my doorframe in my office. And the purpose of this is that, if you're going to augment a physical space, let's say this room, and you want maybe a tour group to come through and pair a headset or maybe even your work -- your research study groups coming in, you're going to curate locations of your visualizations around the room so you can collectively walk together and see stuff at the same spot in the space, you know? It's hard right away to have a canned example for a room with a HoloLens, which you'd think would be simple, right? If I'm going to augment a museum, why can't I just deploy a whole bunch of HoloLenses and let people pick them up at the door and walk around my museum. Well, the HoloLens starts tracking and understanding the room at runtime, so therefore it has no way to calibrate to the real-world space unless you pre-capture it. You can use image targets everywhere, right? That's not really a sustainable way to do it though. So what you're looking at here is I've pre-mapped the room, let's say my museum or my laboratory, and I'm aligning the image target for initial calibration only. So I'm applying a simple shader to the environment so that we just can see the polygons. And at runtime, I take the HoloLens and look at this target on the wall and it does an alignment behind the scenes. It's actually moving the environment and kind of [inaudible] it together so that they line up. This is a view through the HoloLens. So then as I look at that, that -- those green lines appear and I'm able to see that, you know, things are generally lining up. And this is an iteration you have to go through and just make sure you don't move your marker and you can follow this tutorial and then sort of anchor data into your space specifically. I'm going to fast forward in this video to the final result.  That's why you do that. You have to have specific versions of soft -- of compilers and things like that to?  No.  When they first got it going, you had to have like this version.  Right. That's actually pretty cleaned up now, so the HoloLens, I recommend always having the latest. The reason for that is if you have code existing older copy of Unity and you try to compile into an older SDK or a newer SDK, you're going to get stopped by this and it's because this goes through Windows updates and it keeps itself up to the latest SDK. Unless you force that to stop, you're always going to have to continue to keep on that. So that's not great but that is what we have to live with. I'm going to fast forward here. The end result was just I want to put a cube on my desk in my office. So I initially calibrate. I see the green lines. The HoloLens uses the image tracking initially and now it's switching to spatial mapping. So as I look around the room, it didn't see my cube on the desk. There's a problem. So I go back and look at the target. Okay, now things have caught up. And as I look back at my desk, there's the cube. Okay? And then now you don't even have to look at that image target anymore because the aerial learning system takes over, or spatial mapping. There's a lot of names for it but essentially what it -- the HoloLens does, SLAM, S-L-A-M stands for simultaneous localization and mapping. That's the algorithm. That's the name of it. And what these devices are doing is using some strategies to map and figure out where the camera is in that map at the same time, as fast as they can. Okay. So like I said, that's a 30-minute tutorial. If you liked that, just email this help which I'll have that email up at the end and I can point you to any resources that make sense for you. All right. Now, I mentioned that's a technique called tracking system fusion. So we did a marker tracker and a spatial or aerial learning tracker and put them together and we get this nice scenario where we can recreate the same experience every time by just looking at that image. Now, on the right here, we're doing -- attracting subsystems. We're still doing a fusion between two different tracking systems, but what's happening here is Patrick here is actually building out cubes in a VR headset using a VR system's tracking system which is a really accurate, but still not portable kind of fixed tracking system. And the HoloLens is a somewhat portable tracking system. You can kind of fire it up anywhere and it works. But how does the HoloLens peer into that tracking system and have then co-locate properly so that the data actually is accurate. Actually, I switched this video. It's of Bill now so since he's doing these talks here. There we go. I'll rewind it. So this is an open source project from the University of Rochester. The initial calibration isn't shown but it's quite simple. You turn the headset on and turn the VR session in. It renders a handset in the space and you click the handset by putting your hand in that space. And it basically aligns the HoloLens with the Vive tracking space. And all you can do with this is just build cubes with their base-level project. You're just setting two vectors and it builds a cube around you or a cuboid, six-sided. And you can, of course, extend this and that's what we did with --  That's what I was doing.  What were you doing? Oh, nice.  Yeah. [Inaudible]  So then there you go. I deleted his stuff. No, I just exited the application. So Bill could actually see me in that app. I was represented by a green cube floating around beyond his trackspace. That's kind of neat. So you actually extend the VR track spaxce too. It's not as useful but, I don't know, that's arguable. I guess it depends on what you're doing. Oh, come on cursor. Am I using the wrong mouse? I'm using the wrong mouse. That's what's going on. All right. So I want to mention a few things here though before I move on. So tracking system fusion is like an area of like -- I don't know. It's a proud area for a lot of developers, because once you get it working where you can kind of hybridize a sort of type of tracking with another and it's not easy, you've become this like center of, how did you do that? You know, a lot of people want to know how to combine, say, facial tracking with spatial learning. I don't know. It just depends on what you're doing. Or like the military. They would want to visualize, say, telemetry data if they're standing on the ground looking up at, say, a UAV flying around, how the heck do you track miles up into the air in real time of where that's going? My friend Colin is here today. He and I graduated from School of Informatics and Computing in Indianapolis. He's a beekeeper. So, like, a project that he might want to do as a beekeeper, because he's somewhat professional at it. How do we track bees? You know, how do we, in real time, see an augmentation of these bees moving around, you know? That can be sensed by a system but maybe it could become a subsystem of augmented reality. So this might be an area of engineering that would behoove you to kind of carve out a niche. Because if you need to track a certain type of activity and you build a system to track that, it would even be better if you could extend that to become a subsystem, subtracker for an augmented reality system that's popular, say a headset or a tablet, so that you can sort of extend it beyond its basic tracking. So I think augmented reality subtracking systems will be interesting. There's a tour around campus that Indiana Geological Survey Group does. [Inaudbile] invited me to do it. Eric you did it. And where they were doing a limestone tour around campus, of these buildings were built with limestone cover on the outside and this is a very historic area of the world for limestone. So how the heck do you stand on the ground and look up at a building with certain parts of limestone everywhere? Details that you -- the tour guide is emphatically trying to communicate to you what those are, how to think about them. And really what they want is just a digital annotation up there, right, to explain. Maybe even have videos floating and everywhere. But how do you anchor augmented reality graphics to those spaces, right? So there may be an opportunity there for some sort of software innovation or hardware innovation to extend the commonly used VR -- or AR system to track, you know, within a -- like an outdoor tour type environment. So this is the thing to think about. This is an area that, you know, everyone that comes to me that asks to do some sort of fanciful type of augmented reality, even if it is scientific visualization, how do you track beyond just the limitations of the devices? So lastly here another advanced technique is digitize the occlusion. So when you're tracking an object, this is the type of object tracking that you can do is just track 3D objects. What if you could actually, when you spin it around, have the 3D geometry occluded. Now, that may not be obvious what I'm talking about. Let me play this video. I'm using the correct mouse. There it is. All right. So this is a bloodletting bowl that was digitized and over here on the left side of the screen, you can see the 3D representation is there. And then shortly here, I'm kind of showing off it's a magic trick, right? So there's a 3D representation that was pretty photorealistic. Matched up with the real world one. But what's nice is if I advance this video through, you'll see that the technique I used is to turn off the shader color and just turn on the occlusion function of the shader and then you get this ability to build annotation but it properly response. So as I rotate this bowl, you'll see that the text kind of goes in front of the bowl. Yeah right there. So that's incorrect, right? But by digitizing the object, now we can use the occlusion capability of a shader and block the annotation and then it doesn't seem odd to the user. They could be pretty fairly accurate. I mean, most folks aren't even going to recognize that little gap there. So, again, what matters where it matters. If -- one of the projects that keeps coming up, that I'd love to find a home for, is let's look at our HPC systems at IU. We have these amazing supercomputers. I'd love to somehow visualize on that surface or within it or around it or maybe just on the data center, you know, some sort of relevant real time data. You know, maybe core temperatures. I don't know how interesting that is, but maybe do a visualization with core temperatures.  This is [inaudible] I just for the National Park Service I [inaudible] geomorphology [inaudible] 1958 [inaudible] they're using them airbrushed [inaudible] and stuff. And they're using them on the site with the [inaudible] and a lot of good [inaudible] this would be really awesome. So [inaudible] goes to two techniques.  So digitization does this too. It's like all of a sudden you now pay attention to these things that were created maybe decades ago that were airbrushed and nice and [inaudible] artifacts, right? So you digitize them. This is one application of visualization you can apply, right? So you could stand back with some air display and use that object as the tracker [inaudible] spatial map and then annotate, animate, interact. There's lots of options.  So they are sandbox [inaudible] playing with objects [inaudible] but AR on VAR sand there's a whole other possibility there [inaudible]. But I love the idea that [inaudible] combine.  Yeah. There's a big initiative to digitization, within our organization and Cyber DH to sort of like standardize, and also the universities are trying to do this as we do more 3D scanning of objects. It's like, maybe we could standardize the metadata, right? So then an AR viewer for that would be just let's have it ready to go. With any digitized data that comes in, we understand the standard. Let's visualize it and then let you, you know, direct the annotation, I guess, of how that may be displayed. I don't know how relevant some of the metadata is to be visualized but, you know, there's opportunity there to make a standardized viewer, right, for standardize digitized things. So it's a nice little hookup between two different areas. I'm going to show you another example. This is less of an advanced technique and more of just an idea. So if you have, say, technical items, I'm going to share my -- oh let's see here. How were we doing this Bill? I can't remember. I need to share contact from a camera and then switch cameras. So that's my laptop camera and there's me and then Bill's going to be the cameraman. So I've got an older Android tablet. This is like an older generation Galaxy Samsung tablet and we've got an old video card. But you can imagine this is a new video card and I have maybe many, many more in the closet somewhere or maybe I'm a mechanic and this is some box of a part at a shop or I'm in a chemistry laboratory and I've got, you know, all of these little vials of Aquila solutions. Like, what is exactly is that and can we hook it up to a database and actually annotate what this technical information is instead of me going back to a computer console and looking things up. So what we're looking at here is -- I'm showing the camera so folks can see. Is I'm holding the video card up and this application is just an example of what you can do. So I can touch the screen and it will annotate what we were just talking about, metadata associated. And you could do this with lots of parts. Hook it up to a database and have it live on. So Gary Motes is involved with the IT infrastructure for paleontology, the Indiana Geological Survey and it's something he's trying to do where you may eventually be able to look at a fossil and then actually have on that surface of that fossil some specific metadata that just pops right up. And they have like millions of fossils. Some of them are large, small, microscopic. So going along with that, I have some examples from the paleontology group. So these are just image targets, so pictures of these extinct animals. Please don't ask me what they are. So if I hold this up, this is a digitized jaw or mandible. My battery's running low but Bill if you can touch that screen there, you can show the -- or just spin it, just drag. Yeah. So there was a feature there to actually drag across the screen.  Okay.  Let me close down my battery notification. All right. And there's a couple more here. Should last just enough longer. So this is actually a mastodon too. Thank you. And what's great about this is this was a starter, so this was a prototyping exercise that I went with -- went through with Gary and his graduate students and we produced that application and we didn't really go much further than that. We just have a stand [inaudible] Euphoria. It's available. You can do that and I'm not going to stop you or anything like that. I say go for it and I'll do my best to support that user. I'm probably not going to be as capable of supporting more advanced requests. But what's really neat about that is Unreal tends to look a lot better than Unity right out of the box, right? The shaders just -- they just seem more vivid and more professional looking.  So then it's -- Unity you said [inaudible] than if somebody [inaudible] said the active [inaudible]  No one said that that's the standard. I'll be clear. But if you go to a conference and I'll say Triple AVR and there are some AR applications and VR applications. I mean, run a survey for yourself. You'll find that it's just so dominantly popular, it's kind of -- yeah.  [Inaudible] do that [inaudible] an open standard.  Yeah.  Expertly [inaudible] of that [inaudible] there's a [inaudible]  Yeah.  So --  I do sense a shift.  [Inaudible] have some [inaudible]  Hey, John, see, this is Robert. If you could repeat the questions the people are asking in the room, that would be great.  [Inaudible] repeat this other thing. It's important to keep alive to that [inaudible] there is no Unity [inaudible]  Yeah. Because we might find them -- you know, they sell off to Microsoft. You know, question.  My biggest struggle [inaudible] doctor and it's coming from her side [inaudible] program. The second is the divide between pushing to iOS, pushing to the Android store, making that jump between Unity but getting it to [inaudible]  Yeah.  Because right now [inaudible] people in my tablet and they don't know I'm actually just running it straight from Unity.  Yeah.  And so [inaudible] nice. Now I've got [inaudible] bunch of times where it had deployed in iOS or deployed Android.  We really need not to be in the [inaudible] okay. So the -- [ Inaudible Comment ] Okay. We need not to be -- in the AR community of developers, we need not to be building local apps at all. Like, there's no reason other than performance to grab from the mobile device. And mobile device, it's arguable if they have enough performance anyway to -- you know, a desktop for VR makes sense, right? Because you can do a local app with a desktop and you can put tons of horsepower in a desktop. You're not going to do that with AR. So we need not to be building local apps. We need to be shifting through web distribution, building web apps. The technologies there [inaudible] the biggest building downtown in Minneapolis is based on React JS. Like, Salesforce Tower is filled with engineers that are using React on a daily basis and Reach has lots and lots of avenues for open source AR. And, you know, that's something I'm planning on mapping out for support here and then we also have a developer within our lab that's working with Angular, which is sort of a Google spinoff, right? And that also works with a library called 3JS, which is an extension of web -- I guess OpenGL from kind of the older 3D graphing days. But it's the same concepts and now it's in the JavaScript world. So if you're proficient with JavaScript, stay with it. You can keep an eye out there for something that makes sense for you but -- because I think that if you use a web app, it's going to live on and on. X3D is a great example because it is a -- it's essentially a web technology. You can build and deploy it almost like an HTML page. It's XML syntax.  Something that you had [inaudible] 3D [inaudible]  Eric is --  [Inaudible] I think that's why.  Eric is on their advisory board, right?  No.  No?  But this is an ISO standard, okay? So it's [inaudible]  Right. That's it.  It's been around for 22 years [inaudible] VR now so --  That's right.  Couple of [inaudible] and survivability [inaudible] something like that, we really -- and it's tough because from the practitioner's side, it's like the landscape is changing so quickly and to actually do anything of value, you have to, you know, use what's available and doing -- and because business and commerce is pushing development more than -- or I don't think more but [inaudible] in academia is really tough. So like okay, do I wait two years for academia to come up with an open source [inaudible] version or do I leverage Euphoria and Unity?  Right.  Because I can actually get something out the door in six months and people are going to use this.  Well, I mean, we're not 20 -- in 2018, we're not talking about a spatialized internet yet, right? I think it may be not called spatial internet or spatialized internet. But the point is that spatial technologies like VR and AR are getting wrapped in technology back-ends now for, like, the production of web application or websites. And it's not going to be long before that's a major conversation point and it's not going to be [inaudible]. It's just going to happen because a company that likes to manage a content service that has a back-end that they manage, or a researcher that wants to deploy their research, it's like you don't want to have to go through this red tape of publishing a local app and then having to be at the mercy of Apple or Google to have it published out there and then maintain the application and you're at the mercy of the updates that come into these operating systems. So with a web application, you can at least deploy it the [inaudible] you know, the actual render in a browser changes. But it's sort of standardized more towards the open source community because there are so many web apps out there. They don't want to advance it too far because it'll break people's, you know, business, right, or what they do. And I just think it's a better way to go about it, you know? So then, you know, there are some hard problems to solve. We don't really have a standard solution to stream bytes of 3D data to a spot efficiently like we do with video. I mean, YouTube can stream down a 16k video right here or to my phoen and it works. I can't really do that with a two million polycount dataset that I generated on the supercomputer. You know, that doesn't exist yet. So, you know, compression, decompression algorithms. We have to go back to the old days of computer science compression streaming problems and get it down to that two gigabytes per second standard that the internet lives by. So -- or not two gigabytes per second. I forget the -- I'm not that adverse in network standards, but you know what I mean? It's internet standard for how much bandwidth you're allowed to use. So yeah. Question?  [Inaudible] using [inaudible] tracking and what's your [inaudible]  There's two forms of it. I'll be clear. What was that?  Repeat the question.  So the question was Euphoria the plugin uses object tracking. What is my opinion on it? Is that -- that's right?  Yeah. And this one example with the digitized object, was that object tracking as well?  It was and it was what we call primitive object tracking. So Euphoria had an initial type of object tracking early on where they generalized the shape of an object to a sphere. It's really bizarre but they would have this ovalized sphere that would do its best to understand an object. So they had a scanner application local app you download on your iOS or Android device, put a marker that you print out from a -- just a eight-and-a-half by eleven sheet of paper. Put your object on that. So already had a limitation to the size of the object because that's how big your scan space is. And then just walk around that object. And it didn't actually digitize it, per say, to the actual shape of the object. What it did was encode the image of the perspectives into some sort of spherical observation. So it was like a database file. In the end, you don't actually get a 3D object out. You just get this like collection of perspectives out that are in a database and that's how the object tracked. That was their initial wave. And that actually works quite well but, again, it's limited. Now they have their new object tracking which is immaculate. I mean, it's like you could 3D print out an object and then just take the same 3D print file, the STL, and use that as your source and it'll track it. The problem is they put it behind a paywall so if Euphoria actually charges for that feature, you can't just take it and use it.  [Inaudible] about [inaudible] studio.  Okay.  And 240 [inaudible] with their licensing [inaudible]  Yeah. So good luck. It's unfortunate. I don't actually have a functional object tracker in the lab of that more sophisticated thing just because I don't think it's worth it to pursue these things, mind paywalls unless it's really important for some IU researcher to have. Any other questions.  Speaking of paywalls, I guess [inaudible] iOS apps [inaudible] like this, what do you -- I know some people have done that, but that you can get the apps on there that had paper licenses [inaudible] paper licenses.  That's a very good question. So we -- you have to have a provision profile and IU has a service that's not through our lab, in UITS, to do just that. You know, to -- for folks to deploy one app to one device. And you provision the device. There's some sort of handshake between the operating system and your X code project, which is now called something else. I forget. I'm not an iOS developer but there's a gentleman in our laboratory I would introduce you to who might even introduce you to somebody else. But we do have a way to do it and it's free for -- as far as you can tell. Yeah.  So that your [inaudible]  What's that?  He's a crane.  You're a crane. Okay. Well, so there is a way -- there is a mechanism through Apple to -- I think it's like a hundred dollars. It's not that expensive. And you get a [inaudible] profile and then you basically manage that and the devices you have access to so then you can have a range of iOS devices, build your app and check it on all those. That's the whole point. So that's sort of a commercial, I don't know, catering that Apple does. You know?  And [inaudible] upload to the iOS [inaudible] without having a MacBook? I know it used to be that.  Nope. You still have to use iOS or -- [ Inaudible Comment ] Yeah.  Yeah. That's why all that stuff -- like, every time I -- oh.  Well, even when you get in -- even when you get in the world of web app development, it sounds glorious when I talk about it, but it is so painful to go from, say, angular to an AR app just because the documentation's sparse and it's changing every day and it's not really ready for primetime. That's why I don't have many slides about it because --  I appreciate somebody at the entry level [inaudible] Apple used so earlier [inaudible]  Each P reveal.  P something.  Yeah.  But those [inaudible] one or two like simple problems. Because I asked like [inaudible] multiuser environment.  Yeah.  [Inaudible] not to just be able to see it all easily without having to [inaudible] iOS. These couldn't solve some of those small problems.  I was going to do a demo of that. It's --  And you have some HPs. [Inaudible] my first HP printer in 1996 [inaudible]  Oh sure. Yeah yeah.  So I could never --  Well, this wasn't --  [Inaudible] myself without reviews.  It was not developed by HP. It was a [inaudible] and it was a really -- yeah. Very successful thing. Tassi actually showed me it a long time ago. What's --  They bought [inaudible]  Yeah. Yeah. So HP owns [inaudible] This is the nature of AR right now.  Yeah. This is [inaudible]  You can still use it. It's still got all the functions and I don't see any paywalls yet but -- yeah. But I was able to like -- I was sitting on my couch and I just took a picture of this slide and I was able to take a 3D model that I happened to have an OBJ on my phone and then upload it. And then it did augment right off the top of the slide. So as long as it took to upload was basically the length that it took. You know?  Okay.  It wasn't very long to just -- I like that. You know, if you can prototype, that's a conversation point that lets folks around you react to. That's really important.  It's actually [inaudible] as well. We augmented a whole deck of cards [inaudible] with [inaudible] so it was like I had all the [inaudible] and then [inaudible] Queen Elizabeth wearing the same outfit that she was in [inaudible]  Oh cool. Yeah.  [Inaudible] but yeah.  Yeah but the great thing about it is it also solves the software maintenance problem. So once you deploy it publicly within their ecosystem, you just let it be because it'll work because HP maintains it. Well, if you trust HP. But I could look up stuff from a few years ago. They had a DeLorean someone had animated and made it look really cartoonish and it was like published in the [inaudible] and it was still working really great. It had a lot of features to it. It was interactive. So and HP Reveal now has a studio extension off of it so you can add interactivity to it as well. But just from your cell phone alone you can take a picture of your target, upload your model or text or whatever, assign the two and you save it, and then what it is is it's sort of unlisted and then you can publish it out, kind of like you would a, you know, publication on the internet like YouTube or something.  And then share that with --  Yep. So then you're -- hey, download HP Reveal and look for this keyword. And then they can find it and try it out. Yeah.  Do they have Google Glass here or not?  Do we have Google Glass? [Inaudible] does not. There are other UITS groups that explored it. Yeah. It's actually found a home and a niche in warehouses and site management in terms of parts, you know? Have you ever used one?  Well, I was actually at Google X [inaudible] and they showed us it.  Okay.  So we had [inaudible] just putting [inaudible] interview but I got a couple seconds with it. I just kind of want to try again.  Sure. Yeah yeah. Have you tried the HoloLens yet?  Yeah. We have the HoloLens at [inaudible] as well.  Awesome.  But interesting [inaudible] Google Glass. They're apparently coming out with another one in the next, like, I don't know how long. It's --  I really --  [Inaudible] too [inaudible] not [inaudible]  They're not even focusing on [inaudible] world.  Ah. They're just [inaudible]  They're focused on [inaudible]  Okay.  For a long time, there were some rumors that the Magic Leap One was going to be called Google Glass because it's got a -- almost a billion dollars. Actually, I think it has over a billion dollars of Google money behind it.  Yeah. Have you guys been talking [inaudible] at all?  I have. I just chatted a couple times with an engineer a year ago and then recently I've just been talking with them about their, you know, Unity stuff and then like, I don't know, as far as -- I wanted to know about this right here. So this is a magnetic tracker. This is unique about the Magic Leap, is it -- it's -- there's a company out there called [inaudible] and they make VR trackers and just trackers in general for people to do research with. But it basically just emits a electromagnetic field. If you're familiar with the old flock of birds tracking system, it's similar to that. And basically from your head, you're getting emitted, you know, a range of electromagnetic interference and then you have a handset that's tracked in the full 6 degrees of freedom then with it. So that's unique compared to the HoloLens. And it does spatial mapping just like the HoloLens. The field of the view is taller but I don't know. It's -- for 2700 dollars, I think it's not a bargain for an individual developer. For IU, it might be okay.  [Inaudible] this year [inaudible] AR VR [inaudible] focus more on AR [inaudible] the Unity the same stuff. But one of these comments was related to the communication in general. And he described our limitations with VR really comes down to the field of view.  Yeah.  And really it's nice [inaudible] aspect ratio between the HoloLens and some of the other things that when you're trying to [inaudible] lens, you're trying to look at something that you're --  You don't want a rectangle, right?  Yeah. So [inaudible] look at what your [inaudible] detail and see in context with the rest.  Now [inaudible] pushing -- well, not pushing, but, you know, the AR was really helping right now with that experience. I was just curious about your experience because you seem to have a lot more experience with VR.  The -- yeah. I mean, field of view would be nice if it was just full vision like -- I mean, I could argue a VR headset is full vision but it's technically not. But it's enough. You don't need -- like the Pimax VR headset, it goes like beyond my peripheral vision. I can turn my eyeball and see there but I can't actually see if I'm looking straight forward. So what's really cool though is we're in this HMD arms race with VR and they're doing like -- the next gen of it will be eye tracking. And there's lots of benefits you get when you have a nice eye tracker because on the rendering side, our eyesight, we defocus a lot of what we see anyway, because we're using field of view but in an animal way. Our eyes aren't always in the same focal length and stuff's out of focus, stuff's in focus, and we rapidly go between them. So if a computer can actually keep an eye on that and have that data available for the renderer, then you can actually build rendering applications where stuff in the computer graphics is out of focus just like eyesight. So they're trying to get to this kind of realism threshold where, like, you know, we'll look at digital content and it's out of focus in different distances. Then you also have to have a display that defocuses properly. So the -- like a fighter pilot outfit. You have collimated light coming through the headset. So that means everything's in focus at all depths and that's how computer graphics are in a VR headset right now anyways. Everything's in focus out forever. So like if we have data about where you actually are looking, whether you're looking around closer or further away, then you can respond and then if the field of view isn't there, then it's -- that doesn't help. But the point is the AR will benefit from these innovations happening in the VR space and then I don't know. I don't know. These headsets will probably converge probably. You know, they'll have a nice headset that is a reasonable cost but it has enough bells and whistles that you can do VR with it or AR with it.  [Inaudible] watching the videos [inaudible] my question never came to mind while watching your view from the [inaudible] normal.  Yeah. Well, when the content has realism involved -- say if I'm actually going to render a clump of grass in front of you or a miniature of your [inaudible] the mountains or something. It's like, if that looks pretty realistic and the computer graphics are meshing with that, it would be nice if we could kind of get them together. And the other problem is the color black doesn't work in AR. Like it works for video through on a tablet but if you're using that headset the color black is basically 100% transparent. You know? It's like a projector. If you project on the wall, measure black and there's nothing showing up on the light meter. Question Tassi.  Yeah. So [inaudible] of measuring [inaudible] if you solve too many [inaudible] which is [inaudible] goes to another lens and -- do you [inaudible] within the [inaudible] transitioning from female to male. And they actually show that the higher their estrogen levels were the more they get [inaudible] VR where actually [inaudible] they do suck at it more.  So it is better to have more framerate -- frames per second?  Yes.  Okay.  And so yeah.  Yeah. So VR right now is standardized at 90 frames per second. [ Inaudible Comment ] These are 60 and these are whatever the -- Apple I think it mandates around 60. I might be wrong about that. If Jeff Rogers is listening, he can correct.  And do we have to tell [inaudible] better at it than I used to be. But, you know, I --  But, I mean, these you don't wear on your head. These you do. I mean, this is where it's important. The dumb -- kind of the directive for this framerate that Tassi's asking about is really -- just comes down to the manufacturing choices they've made as far as what hardware they're going to use. And the controllers for these displays are based on like little Pico projectors, you know, and light-fueled emitters that are over here in the side. And you miniaturize hardware, you get a limitation that seems old. So I think 60 right now is a nice sweet spot but you're right. It should be 90 or more. I mean, cheap televisions are 120 hertz at Walmart for 100 dollars now so we need headsets that also go to 120 hertz. Once you go that far, you're dealing with other problems other than the refresh rate. I think there's a thing called persistence of the display. So as the LED or LCD turns on and off with these refresh rates, it needs to actually, in between those two frames, be completely black or not be lighting up the screen at all somehow. Because they find that if you put an LCD in front of your face, when it flips through the frames, you're actually still emitting light in between those frames and that'll cause people to get sick because you get this kind of subliminal flashing that's happening. It appears like it's flashing, especially in dark scenes because it's supposed to be dark but you're still getting light, you know? So we're never -- the displays they're using aren't necessarily meant to be in front of your face quite yet, but I'm excited for the next gens, a lot of these. [ Inaudible Comment ] 1700 hertz?  1700 yeah. [Inaudible]  Okay. [ Inaudible Comment ] I also find latency is not much of a problem in AR compared to VR. Because VR you're, you know, you're immersed and you don't really want the virtual world to wobble or not or come unlocked. If you have something that's somewhat laggy in AR, it's not as big of a deal because you still can anchor with the real world. That's a win, you know? That's something you can kind of take to the bank and say, "All right. Latency's a little bit more forgivable." So -- because I got to these VR conferences, like IEEE and, I mean, they're -- they're still engineering lower and lower latencies trying to get to that really perfect spot and it's unfortunate that that's still a problem. But okay. Thank you all for coming and great questions. We're definitely over time but I'm going to let you go. [ Applause ] 