 Thanks hi everyone my name is Calvin I'm the master's candidate of Applied Science at the human Media Lab Queen's University Kingston Ontario Canada I'm also the co-creator of the grid drone system which I'll be introducing you all to today so this is grid drones which is composed of 15 small tangible drones which we call bit drones on their own arranged in a three by five configuration but they're not just individual drones flying together in a formation they're actually in a rudimentary programmable matter display so grid drones is an effort towards the creation of the ultimate display which I'm sure many of you are familiar with it's a step beyond augmented and virtual reality virtual reality all the way to a real reality but the idea that physical computer interfaces with one day fully embodied digital information is not new in 1965 ivan sutherland first conceptualized such an interface so the ultimate display would of course be a room within which the computer can control the existence of matter a chair head displayed in such a room would be good enough to sit in handcuffs displayed in such a room would be confining and a bullet displayed in such a room would be fatal in 1991 two Foley and Margolis coined the term programmable matter programmable matter could be assembled into clumps of arbitrary size and determined not by technological limits but by economic ones by their definition programmable matter would consist of small parallel cellular automata nodes capable of geometrically shaping themselves in 3d space to create any kind of material structure this concept of programmable matter was then further refined in claytronics in which Katims or clay tronic atoms form the material necessary to construct 3d tangible objects the functional requirements for a clay tronic interface would be that each Kadim should be self-contained that is it should contain everything it needs for computation locomotion and actuation no static power should be required for adhesion after attachment between Katims and coordination should be done through local control no external computation necessary Kadim should also contain no moving parts grid drones builds on the functional requirements of these definitions so bringing things back to the present this is of this of course is the MIT Media Lab to inform dynamic shape display which allows for the physical rendering of 3d content and for tangible interaction with digital information there's some exceptional current research in shape changing displays and modular robotic interfaces that really toe the line between what could also be considered rudimentary programmable matter interfaces so a prime example of a swarm robotic system is Harvard's kilobots which are capable of acting as a swarm robotic display seen in this short video featuring the construction of a two-dimensional star zooids from stanford shape lab are similar similarly diminutive robots and are a prime example of a tangible robotic swarm interface they are seen here interpolating a user's manual input to form a circle another example of a self-configuring robotic swarm is M blocks from MIT csail capable of autonomously creating structures these small cubes utilize the inertia from a rotary mass for propulsion and magnets to maintain physical connections between them and most recently here is a short clip of Intel's drone 100 project which quickly became Intel Stone v one drone 500 project here was deployed at a music festival earlier this year for live visualizations and as you can see it's a highly capable volumetric display composed of hundreds of individual drones which brings us to the rationale behind the creation of grid drones to start with there are two approaches as to how we can interact with digital information in the future either we can augment ourselves which is what we're currently doing for the most part or we can alter reality which is what we just saw several examples of grid drones aims to alter reality but obviously this is a very difficult thing to do from a programmable matter standpoint none of the examples portrayed fulfill all of the criteria necessary swarms such as zoo ODEs are tangible but two-dimensional while intel's drone 100-500 and beyond are not tangible and are very sparse due to turbulence and thrust streams from nearby drones with M blocks and in form we have 2.5 D displays which essentially means that 3d structures must be structurally stable because the individual elements are not self-supporting in the design of grid drones we focused on the following features career drones is naturally haptic due to being composed of self-propelled physical elements that can exert force albeit not a large amount but enough to provide a resistive force when they are being moved all primary function functions revolved around the tangibility of grid drones so most interactions are manual certain non manual manual functions such as parameter specification was done using smartphone app of course being small nimble drones they are entirely self levitating and self powered they are capable of 2.5 D surface transformations but because they are entirely mobile and self-supporting they can also perform 3d transformations such as rotation translation and scaling operations grid drones also has the ability to record tangible interactions as animations and replay them in the implementation of grid drones it can comprises of five main parts so there are the input devices the drones themselves a UI layer for enacting different interaction scenarios the flight controller and the 3d positioning system so these are the bit drones which make up the grid drone display as a whole what you see on the right is the basic viktor encore well on the left is pictured it's pictured securely fixed inside a lightweight carbon-fiber and nylon cage these cages were designed so that the drone could be tangibly manipulated and also double as a protective cage for the delicate components inside here's a slightly larger image it shows the main parts of the drone so each has four brushed cordless motors with translucent propellers for propulsion for RGB LEDs to illuminate the drone a flight controller with integrated microcontrollers and I am use as well as Wi-Fi connectivity and up at the top you can see there are five infrared reflective markers with a unique pattern specific to each drone these markers are triangulated by a vikon motion capture system pictured here affixed to the ceiling which provides position and orientation information of the drones the left and right hands are recognized from a marker pattern affixed to a device worn by the user or by marker patterns affixed directly to the skin this provides the system with information on when the user is touching or pointing a simple smartphone app was created in order to control non tangible aspects of bit grid drones such as specifying parameters or summoning tangible menus out of the drones themselves the backbone of grid drones is a 2.4 gigahertz Wi-Fi network ultimately the system is mainly a centralized one with a flight controller computer running grid drones OS connected directly to the network this is a system diagrams of the grid drawn architecture so first the vikon provides drone positions to the flight controller the flight controller constructs control packets for each degree of freedom using multi-layered PID control and then these control packets are then sent directly to the bit drones which can also provide status messages as queried by the flight controller the user then manually interacts with the drones or sets parameters using the smartphone app this information passes through a UI layer which runs the various interaction scenarios and specifies the necessary spatial arrangements of the drones in space so as I highlighted earlier the point of grid drones is it's a tangible display which accepts both the UNAM annual and by manual input there are four ways in which to group grid drone elements using manual selection you know manually a user can either select all perform a contiguous or discontinuous selection or perform a traditional lassoo select by manually a user can also select a rectangular set of voxels between touch locations so here our user double chat taps a single bit drone to select all elements and group them grid drone behavior at the sentence specified from the smartphone app in this case parameters were set in order to create a pyramid shape when a control point was dragged upwards grid rooms also allows for arbitrary groupings within itself here the user performs a discontinuous selection where only the corners of the grid are Union manually selected and grouped alle su select is also possible using a ray cast from the vector as indicated by the index finger the list su selection process is terminated by additional click with the thumb 3d transformations are done entirely through by manual interaction with the grid drone display with a by manual interaction the user can either rotate the grid drones about all axes or translate along all axes in 3d beginning a bimanual interaction is as simple as the user reaching out and grabbing two drones moving them in space relative to each other allows for 3d rotations about all axes and additionally lateral translation is possible by simply moving the two drones together in the chosen direction of movement so there are several examples that we have constructed and tested with the grid drones including 2.5 D NURBS surface modeling tangible animation design and flying Lego bricks the most obvious application for grid drones is in 3d 3d modeling the NURBS surface application runs in the grid drone UI layer which allows the user to program a topological relationship between groups of bit drones these relationships are programmed using the smartphone app or a relationship can have a strength ranging between 0 to 100% 100 would represent a hard material relationship whereas 25% would represent more of a soft material relationship users can program multiple overlapping groups allowing for complex behaviors to emerge so here's the NURBS surface application in use first the user double taps to select all elements then a topical topological relationship is applied roughly 50% in this case by pushing and pulling the bit drones which represent control points deformations can be applied dynamically throughout the entire surface aforementioned 3d transformations can subsequently be applied in the form of 3d rotations and translations here a raycast select is used and topological grid drone relationship set control points do not necessarily have to be individual individual drones but can be can consist of a line of drones as selected through a ray cast from the index finger in this case a ray cast selection is used as a line control to construct an archway it's also possible to animate these grid drones models and replay the animations using the grid a user can program an animation by setting a topological relationship beginning a recording and then moving the bit drones a recording menu is summoned from the smartphone to either record a new animation with the red button or playback a previously recorded one with the green button the recording can be played back through the system and the user can then move it around space using a for mentioned 3d translation because manual interaction would be would conflict with the live animation the model has to be grabbed via a closed fist gesture and then translation and acted through the extension of the index finger and then motion of the hand so in February we worked with Lego to create a tool for creating and animating robotic structures based on bit drones grid drums although the drones are pretty safe we had to make sure such an experience was absolutely childproof both for the sake of the kids as well as the drones we determined that separation from grid drones behind a window was the best option to substitute tangible interaction with the drones themselves we created an embodied controller to directly control them from behind the glass each embodied controller is connected to the grid drones Wi-Fi network and has two integrated i''m use one for each wing they are also equipped with lights and an internal battery the Gir drones UI layer uses the IMU data from each wing to determine the absolute orientation of each wing as well as the relative orientation of the wings to each other the body of the controller is constructed of a modified Lego base plate on which Lego Bukh bricks can then be attached a scanning table was also constructed with a USB camera and Intel nook PC running a custom OpenCV application to determine relative LEGO brick location as well as their color using an embodied controller the user can scan a LEGO creation and then the system will do its best to recreate it once assembled system allows for full and body control with direct mapping from the controller to the grid drones themselves the bending of the controller wings will result in the relative bending of each wing in the flock while rotating and tilting the controller will similarly result in a tilting and bending of the flock we took the system to the 2018 Lego World Expo in Copenhagen were approximately 250 children use it each child responded positively to the exhibit without exception just last Thursday we took the system to Madrid where it appeared on a live show called el hormiguero and was piloted by guest star rick astley this demonstrated that the system is highly reliable but it does have some issues which brings us to the limitations of your drones this was not an unfamiliar scene so namely the battery life is relatively good for small drones at about 7 minutes but it's not long enough for sustained use as a display this is simply limited by modern technology modern battery technology and there's not a lot we can do about it right now the resolution of grid drones is limited by the cost of the drones the physical volume and the enter of the interaction space as well as the size of the track volume as enabled by the vikon motion capture system which also contributes to the cost due to the proximity of the drones they can't fly over top of one other as they cannot stay in the law thin the thrust streams of drones over top this creates issues with true dense 3d models smaller drones would be better so that the density of the display could be higher but the drones are completely custom designed by us custom-built and also have to remain serviceable so one way to circumvent the issues with battery life would be dynamically replace drones as their batteries run out if we can recharge drones at the same rate that they require a replacement then this would be a solution for longer use cases of the display wireless power transfer would negate the need for better batteries or dynamic replacement but is pretty far off in terms of weight and power requirements for this project smaller drones will increase the display density of grid drones which we're currently working on and being able to hover over each other will allow for true 3d tangible construction 3d group selection is only possible in a 2d grid at present and alternate alternate selection techniques will have to be explored for true 3d drone configurations so we're also working on a physical midair connection which will allow for a much denser display pictured here is just four drones but we've done it with as much as six and we're hoping to scale that number up but there's a lot of control systems issues that come along with flying larger flocks of connected drones and that's it thank you [Applause] all right so let's line up for some questions dan is already over there I knew it hi Daniel Ashbrook University of Copenhagen this is super cool I really liked the idea of being able to actually grab and physically manipulate these things but something that I was thinking about is in the in the hand wavy future when you have a thousand of these things they start to get in the way of your body and I'm curious then how do you envision being able to manipulate a drone that is in the middle of the flock you can't actually physically reach absolutely so that's kind of why we started exploring things like Ray casting for selection obviously there's a lot of work done with 3d selection in virtual reality we find that there are issues with bringing this stuff from virtual reality studies to a physical thing as you mentioned but that definitely warrants much further research and studies with this system with something like that we might envision the user actually going into the display itself doing interactions at a much smaller scale and then taking a step back and maybe interacting gesturally with something of a much larger magnitude well thank you all right hi I'm you from Media Lab um I really like your project but like um when I saw the sweetie display Oh in a sweetie err I saw that you're trying to display some imagery using an LED color changing but have you consider about two instead of you know like a structure I mean because are you using a strong tattoo I maybe you use a structure a skeleton structure but what would be another possible way to display an image on one like module of the drone or a display or shape well to be honest we haven't thought too much about that just because what we're really focused on is each the idea of each your own representing Khadem representing a clay tronic atom so we're more concerned with minimizing the overall size of the drones in increasing display density than actually trying to provide more more contextual like more contextually relevant information on one drone so the focus again is really on managing all of the drones together instead of creating higher display resolution on the drone itself which is kind of why we just stuck to one color so that we would remain in that vein of research all right this thing speaker once more and we're wrapping up the session just want to save 