 Hi, thanks for tuning into Singularity Prosperity. This video is the eleventh in a multi-part series discussing computing. In this video, we'll be discussing what cognitive computing is, current cognitive computing initiatives and the impact they will have on the field of computing. [Music] The human brain is truly an amazing machine: able to operate in parallel, malleable and fault-tolerant, having 100 billion neurons, with each neuron having 100 to 1000 synapses, synapses being the connections to other neurons, this equates to 100 trillion up to 1 quadrillion synapses, all only requiring 20 watts of power in the space of 2 liters. As discussed in a previous video in this series about computing performance, the human brain is postulated to equate to 1 exaflop of performance, in other words, 1 billion calculations per second and there are many initiatives to reach this exascale performance by 2020 in supercomputers around the world. For us to simulate the brain, that being every neuron and synapse in the brain with these exascale systems will require upwards of 1.5 million processors and over 1.6 petabytes of main high-speed memory, using power in the order of megawatts per hour and taking up the space of entire buildings. All of this as compared to our brains that require just 20 watts of power in the space of 2 litres and will still outperform these machines at orders of magnitude faster. On the petaflop K-supercomputer in Japan, running Neural Simulation Technology, NEST, algorithms requires roughly 4.68 years to simulate 1 day of brain activity, that's 1,700 times slower than the brain. Japan's Post-K exaflop supercomputer aims to increase this to 310 times slower, simulating 1 day in the brain in 310 days. While these simulations will aid us in unlocking secrets of the brain, due to the vast architecture differences between modern computers and biological brains, these exascale systems will still be limited in functionality. Every computer in the world today is based upon Von Neumann architecture, having computation and memory fairly isolated with a data bus connecting them, whereas, biological systems have memory and processing tightly coupled together. While Von Neumann architecture is still the best choice for the majority of computing applications, as seen by the drastic performance differences in brain simulations, a more biologically representative architecture has to be implemented, neuromorphic architecture. First and foremost, neuromorphic architectures will allow us to accurately and in real time simulate aspects of the brain, however, while this is one goal of this new brain inspired architecture, our brains aren't the perfect machine in any regard. They get bored, distracted, fatigued, are biased and are not perfect decision makers - they can be inconsistent and prone to errors. This then leads us to another goal of neuromorphic architectures, to be paired with our devices and accelerate the field of artificial intelligence, that is to take the best aspects of the brains functionality and pair them with current computing Von Neumann architecture. This all is encompassed under heterogeneous architecture which we discussed in a previous video in this series, where multiple compute devices and architectures work in unison together. Let's look at this in terms of the two halves of the brain, the left and right brain. The left brain is focused on logic, encompassing analytical thinking, language and other such tasks. While the right brain is focused on creativity, encompassing pattern recognition, learning, reasoning and so on. The right brain is clearly more abstract than its left brain counterpart. Equating to computing, left brain tasks are best suited to be handled by traditional computers, while the right brain is what neuromorphic computing aims to handle. The left brain performance is FLOPs driven while the right brain is driven by converting senses-to-action or what some call, SOPs, synaptic operations per second. Under HSA, heterogeneous architecture, the melding of these two halves then is what will lead to truly intelligent robotics and machines, that are able to operate in real time. Computing devices based on neuromorphic architectures will be able to truly learn and reason from their inputs, especially when paired with optimized software algorithms. This has been the epitome of our discussions in this computing series, hardware and software tightly coupled together to yield massive performance and efficiency gains. One such field of computer science that has gained tremendous steam in the past decade and is the basis of how our brains operate is machine learning. By creating nodes, essentially neurons, assigning weights to them and then feeding in large sets of data, these nodes begin to interconnect amongst each other, like synapses connecting neurons, into vast neural nets. These neural nets are referred to as machine learning models which can then be applied to our devices and also continually adapt by processing more data. This was an extremely quick overview on machine learning and a much more in-depth discussion will be had in this channels AI series. Coming back to heterogeneous architecture, while neuromorphic chips paired with machine learning models will be able to learn and reason, on the Von Neumann architecture side, these traditional compute devices as we all know excel at repetitive tasks, so in this case, executing the models produced by the neuromorphic chips. Neuromorphic chips paired with traditional computing technologies are leading to a new era of computing, cognitive computing. The first step in a long road in emulating consciousness in machines. So, how are we to design hardware that resembles the human brain? Well, first let's take a brief neuroscience lesson. The basics of the composition of a neuron are: the cell body, axon and synapses. Translating to hardware terms: the cell body is the processor, axons are a data bus and synapses are the memory - with all three composed to form a neurosynaptic core. Essentially neurosynaptic cores are the nodes in machine learning neural nets but represented through physical hardware rather than software abstraction. This alone would present a significant speed up in performance but neuromorphic architecture revolutionizes computing in many other ways. As Dr. Modha an IBM Fellow working on IBM's neuromorphic chip, TrueNorth, states, "IBM's brain inspired architecture consists of a network of neurosynaptic cores. These cores are distributed and operate in parallel. They operate without a clock, in an event-driven fashion. They integrate memory, computation and communication. Individual cores can fail and yet, like the brain, the architecture can still function. Cores on the same chip communicate with one another via an on-chip event-driven network. Chips can communicate via an inter-chip interface leading to seamless availability like the cortex, enabling the creation of scaleable neuromorphic systems." Now let's decode what this wall of text means: 1) Neuromorphic computing devices will operate without a clock and in parallel. This may be the most radical departure from current computing architecture that neuromorphic architecture makes. Like with signals in the brain, neuromorphic chips will operate in a clockless fashion through an event-driven model. This is what is referred to as a 'spiking' neural network, where neurosynaptic cores are only activated when signals reach a certain activation threshold. This as compared to traditional computers that continuously run until power is shut-off. Parallel operation means that multiple neurosynaptic cores can be activated and trigger other cores at the same time, similar to how multiple neurons in the brain are always firing. This clockless, parallel architecture allows for vast decreases in energy consumption and increases in performance as we'll see later. 2) Due to the design of neuromorphic architecture, they are scalable and tolerant to faults as the brain is. If some cores stop working, the neural net can adapt and route through other cores, in the brain this is refer to as neuroplasticity. The neuromorphic chips are also designed in such a way that they can scale larger and larger. This scalability is in terms of adding additional cores on a board or interconnecting multiple boards together. This is representative of the multiple different regions of the brain working together. 3) Neurosynaptic cores are tightly coupled between memory and computation, just as the brain is. We'll cover this more in-depth in the next section as some additional background context is needed. Now that we have a basic understanding of neuromorphic architectures we can discuss the two biggest players in the race right now, IBM with TrueNorth and Intel with Loihi. IBM TrueNorth was first conceptualized on July 16th, 2004 with the goal to build brain inspired computers. 7 years later, in 2011, the first TrueNorth chip was produced, simulating 256 neurons and 262,144 synapses all in 1 neurosynaptic core. Progressing forward another 3 years, in 2014, IBM released a TrueNorth board with 1 million neurons and 256 million synapses in 4096 cores, having approximately 250 neurons and 65,000 synapses per core with performance of 46 billion SOPs per watt. This second iteration of TrueNorth was able to reduce its size 15-fold from its predecessor by using 28 nanometer transistor architecture and power consumption by 100-fold, requiring just 70 milliwatts per hour. IBM have remained fairly secretive since 2014 on the specifications of their next iteration of their TrueNorth boards, however we do know their next goal is to create a system of 4 billion neurons and 1 trillion synapses interconnected amongst 4096 TrueNorth boards and all only requiring 1 kilowatt of power. Furthermore, IBM’s stated final goal is to create, "a brain in a box", in the 2020s, consisting of 10 billion neurons and 100 trillion synapses all able to fit in the space of 2 litres and still use just 1 kilowatt of power. They say this will be achievable once transistors reach the 7 nanometer and 5 nanometer node sizes, which is already beginning to happen! As a side note, you can learn more about TrueNorth and how it will be programmed through IBM's SyNAPSE University, SyNAPSE is a software abstraction layer that IBM has developed for their architecture, similar to what CUDA is to NVIDIA GPUs. As of this year at CES 2018, Intel also entered the neuromorphic computing race with their chip codenamed, Loihi. The current specifications of this chip that are known is that it is a 130,000 neuron and 130 million synapse system, fabricated using the 14 nanometer transistor node size. Both of these neuromorphic initiatives are aimed to radically transform machine learning, allowing for real-time, low-power processing, that being: training, learning from data and inference, applying the learnt models from data on edge devices. As you can see, massive strides in neuro- morphic computing are beginning to be made, whether research and development only expected to accelerate into the 2020s. On top of these 'right brain' inspired clockless computing devices, AI ASICs and other traditional compute, Von Neumann architecture devices, will play a major role as well. To list some of the many: Intel Nervana, Intel Movidiue, Nvidia Volta tensor cores, Nvidia Drive PX, Apple A11 Bionic Neural Engine - the list can go on and on. The compute devices just listed can be considered to represent the left brain and when paired with right brain devices as we discussed earlier will produce massive performance and efficiency gains. We've already discussed some of these devices in past videos and will discuss many more in this channels AI series, self-driving series, etc in the future! [Music] Beyond the shrinking of the transistor, new materials, 3D integrated circuits and the many other innovations we've discussed in past videos in this series that will enhance the entire field of computing, one type of computing device that we haven't discussed is the memristor: So, we're focused here on brain inspired computing. The goal is not to replace humans but to take advantage of some of the tricks that brains use, and brains look very different than modern digital computers. Instead of the separated memory and processor that goes through sequentially and does an instruction at a time, brains instead look like these vast networks of neurons with extremely dense interconnections called synapses, and the kinds of operations that brains do, they do at thousands of times less energy per operation than digital computers, so we want to take advantage of some of that. We're also taking advantage of a technology that's been in development and research at HP for a number of years, memristors. So there's three parts to our work, number one we're mimicking this architecture that I just talked about, this vast network of interconnecting neurons and synapses - we're doing that with the memristor technology. Second, we're actually doing all of our computation in those memristor arrays directly, so this way we're avoiding fetching data which is very energy consuming and time consuming, instead we're bringing all of the computing to the data directly and so that's a big deal. Third, we're actually reproducing the key operations that brains appear to use which is matrix operations, a whole lot of very simple multiplications and additions. You're actually trying to collapse all of this system down into a single chip, the one that we were just seeing? Yeah, that's right, we can scale all of this hardware down to the size of roughly this chip right here! As you just saw, memristors are a technology that works exactly like the brain in terms of memory and processing on the same level to avoid data fetching, and are able to mimic brain operations, in other words, the same operations used in machine learning algorithms, matrix operations. These memristors essentially act as a streamlined neurosynaptic core that we discussed in the previous section, and function in nearly the same way: 1) they're clockless and only execute when an activation threshold is reached, 2) they're parallel, multiple different branches of memristors can execute at the same time, 3) fault tolerance, memristors model neuroplasticity in the sense that they can route around broken branches and rewrite themselves, 4) they're scalable, HP has shown that the large memristor array, the Dot-Product Engine, that we saw in the display stand, can be shrunken down into the size of a chip and interconnect amongst other chips. Integrating this new memory-compute technology into neuromorphic chips will significantly increase neuromorphic architectural performance, HP claims that memristors will yield a 100,000 times greater throughput in machine learning. It is to be noted that there are other types of non-volatile memory in development as well that mimic brain circuitry such as phase-change memory, however, memristors are the closest to commercial deployment. Another field of research that can significantly increase neuromorphic computing devices performance and efficiency is analog computing, also called 'dirty' computing since analog signals are so difficult to work with. Memristors actually already implement a form of analog computing by using a physical process to encode themselves: First, you're you're using a physical process, Ohm's law, to do a multiplication. Instead of relying on digital technology where we're having to pull all the numbers into the processor and then we have to push that result out, and here once you have set that value it's always there you pay that energy charge one time, you never have to move that weight again, and then you can use it over and over and repurpose it. So what I think of is that deep embedded system, not just the exascale, but at the complete other end of the spectrum that deep embedded system in a spacecraft, in a deep embedded system at the bottom of an oil well, something that is so hard to get to - you have this ability for this neuromorphic and neuroplastic system to be constantly changing, adjusting, learning and be that incredibly efficient engine. So I think that's what's so amazing, that sustainability of this technology! Beyond this application of analog computing, other applications include having the ability to process multiple 'senses', for example extrapolating raw signals from multiple sensors, a camera and a microphone in real-time, running them through a memristor array and activating simulated neurons. This is actually how the brain works, a mix of analog and digital signals that activate once a certain threshold is reached. It is difficult to pinpoint the trajectory the field of cognitive computing will take due to an ever-changing landscape, with exaflop simulations expected to be possible soon and more players entering the neuromorphic race every year bringing: new neuromorphic compute devices, new compute techniques, different heterogeneous architecture pairings, new AI ASICs - the list can go on and on. However, with all that being said, one thing is for certain, the 2020s will be a transformative decade, bringing new developments and research towards all three facets of cognitive computing: 1) Brain Simulation. Realistic and in real-time simulations of our brains will aid us in better understanding our bodies and mind leading to developments in mental health initiatives and neurological disorders such as Alzheimer's and ALS, cures to disease and infection, faster cures to new types of bacteria and viruses, innovations in gene editing such as CRISPR and more - many of these topics will be covered in videos focused on biotechnology in the future. 2) Artificial Intelligence. Software neural nets coupled with neuromorphic hardware which mimics how the brain functions and has high energy efficiency and performance, will radically transform and accelerate artificial intelligence initiatives. We'll see this impact of cognitive computing first in our edge devices. These first two facets of cognitive computing act as a positive feedback loop like much of technological innovation does. Brain simulations and research will lead to more advanced neuromorphic devices and architectures, leading to more advanced machine learning models, leading to better brain simulations - and it goes on and on. This then leads us to the third facet of cognitive computing, brain computer interfaces. This may sound more like science fiction, like a plot out of Black Mirror, than reality and while I agree this facet is the farthest from real-world implementation, it is an inevitability in the coming decades and preliminary work has already begun. The integration of biology and technology encompasses many subtopics such as: mind augmentation, mind transfer and uploading, artificial consciousness, cybernetics, etc. These topics as well as the ethical concerns and issues they pose are best left for future videos on this channel, but mentioned here to satisfy curiosity and show the impact that these early neuromorphic innovations happening today will have on the future! At this point the video has come to a conclusion, I'd like to thank you for taking the time to watch it! If you enjoyed it consider supporting me on Patreon to keep this channel growing and if you have any topic suggestions please leave them in the comments below! Consider subscribing for more content, follow my Medium publication for accompanying blogs and like my Facebook page for more bite-sized chunks of content! This has been Ankur, you've been watching Singularity Prosperity and I'll see you again soon! [Music] 