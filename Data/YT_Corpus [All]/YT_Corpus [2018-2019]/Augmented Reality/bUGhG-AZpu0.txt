 thank you so much for joining us my name is Chris I'm a designer and prototype er working on immersive prototyping at Google and I'm joined by Ellie and Luca and today we're going to talk about exploring AR interaction it's really awesome to be here we explore immersive computing through rapid prototyping of AR and VR experiments often that's focused on use case exploration or app ideas we work fast which means we fail fast but that means we that we learn fast we spend a week or two on each prototyping sprint and at the end of the Sprint we end with a functional prototype starting from a tightly scoped question and then we put that prototype in people's hands and we see what we can learn so this talk is going to be about takeaways we have from those AR explorations but first I want to set the table a little bit and talk about what we mean when we say augmented reality when a lot of people think about AR the first thing they think about is bringing virtual objects to users in the world and it is that that's part of it we call this the out of AR but AR also means more than that it means being able to understand the world visually to bring information to users and we call this understanding the in of a are many of the tools and techniques that were created for computer vision and machine learning perfectly compliment tools like AR core which is google's AR development platform so when we explore AR we build experiences that include one of these approaches or both so this talk is going to be about three magic powers that we've found for AR we think that these magic powers can help you build better AR experiences for your users so we're going to talk about some prototypes that we've built and share our learnings with you during each of these three magic power areas during the talk first I'll talk to you about context driven superpowers that's about how we can combine visual and physical understanding of the world to make magical AR experiences then le will talk to you about shared augmentations and this is really all about the different ways that we can connect people together in AR and how we can empower them just by putting them together and then Luca will cover expressive inputs this is about how AR can help unlock authentic and natural understanding for our users so let's start about context-driven superpowers what this really means is using AR technologies that deeply understand the context of a device and then build experiences that directly leverage that context and there's two parts to an AR context one is visual understanding and the other is physical understanding with AR core this gives your phone the ability to understand and sense its environment physically but through computer vision and machine learning we can make sense of the world visually and by combining these results we get an authentic understanding of the scene which is a which is a natural building block of magical AR so let's start with visual understanding the prototyping community has done some awesome explorations here we've done a few of our own that we're excited to share just start we wondered if we could trigger custom experiences from visual signals in the world traditional apps today leverage all kinds of device signals to trigger experiences GPS the IMU etc so could we use visual input as a signal as well we built a really basic implementation of this concept this uses a our core and the Google cloud vision API that detects any kind of snowman in the scene which triggers a particle system that starts to snow so through visual understanding we were able to tailor an experience to specific cues in the environment for users this enables adaptable and context-aware applications so now even though this example is a simple one the concept can be extended so much further for example yesterday we announced the Augmented images API for our core so if you use this you can make something like an experience that reacts relative is digital to device movement around an image in the scene or even from a known distance to an object in the world if you think this concept is interesting I highly recommend checking out the ARV our demo tent they have some amazing augmented images demos there the next thing we wanted to know is if we could bridge the gap between digital and physical and for example bring some of the most delightful features of ear eaters to physical books the digital age has brought all kinds of improvements to some traditional human behaviors and ear eaters have brought lots of cool new things to reading but if you're like me sometimes you just miss that tactic the tactility and holding a great book in your hands so we want to know if we could bridge that gap in this prototype users highlight a passage or word with their finger and they instantly get back a definition this was a great example of a short form focused interaction that required no setup for users it was an easy win only made possible by visual understanding but as soon as we tried this prototype there were two downfalls that we noticed and they became immediately apparent when we used it the first is that it was really difficult to aim your finger at a small moving target on a phone and maybe the page is moving as well and you're trying to target this little word that was really hard and the second was that when you're highlighting a word your finger is blocking the exact thing that you're trying to see now these are easily solvable with a follow up UX iteration but to illustrate a larger lesson and that's that with any kind of immersive computing you really have to try it before you can judge it an interaction might sound great when you talk about it and it might even look good in a visual mock but until you have it in your hand and you can feel it and try it you're not going to know if it works or not you really have to put it in a prototype so you can create your own facts another thing we think about a lot is can we help people learn more effectively could we use AR to make learning better now there's many styles of learning and if you combine these styles of learning it often results in faster and higher quality learning in this prototype we combine visual aural verbal and kinesthetic learning to teach people how to make the perfect espresso the videos explained I'm sorry we place videos around the espresso machine in the physical locations where that step occurs so if you were learning how to use the grinder the video for the grinders right next to it now for users to trigger that video they move their phone to the area and then they can watch the lesson that added physical component of the physical proximity of the video and the actual device made a huge difference in general understanding in our studies users who had never built anis never used an espresso machine before easily made of an espresso after using this prototype so for some kinds of learning this can be really beneficial for users now unfortunately for our prototype one thing that we learned here was that it's actually really hard to hold your phone and make an espresso at the same time so you need to be really mindful of the fact that your users might be splitting their physical resources between the phone and the world and so it applies to your use case try building experiences that are really snackable and hands-free speaking of combining learning and superpowers together we wondered if AR could help us learn from hidden information that's layered in the world all around us this is a product that we prototype that we built that's an immersive language learning app we showed translations roughly next to objects of interest and position these labels by taking a point cloud sample from around the object and putting the label sort of in the middle of the points users found this kind of immersive learning really fun and we saw users freely exploring the world looking for other things to learn about so we found that if you give people the freedom to roam and tools that are simple and flexible the experiences that you build for them can create immense value and now we have physical understanding this is our ability to extract and infer information and meaning from the world around you when a device knows exactly where it is not only in space but also relative to other devices we can start to do things that really feel like you have superpowers for example we can start to make interactions that are extremely physical natural and delightful humans have been physically interacting with each other for a really long time but digital life has abstracted some of those interactions we wondered if we could swing the pendulum back the other direction a little bit using AR so in this prototype much like a carnival milk bottle game you fling a baseball out of the top of your phone and it hits milk bottles that are shown on other devices you just point the ball where you want to go and it goes we did this by putting multiple devices in a shared coordinate system which you could do using the new Google Cloud anchors API that we announced for Iroquois yesterday and one thing you'll notice here is that we aren't even showing users of the pass through camera now we did that deliberately because we really wanted to stretch and see how far we could take this concept of physical interaction and one thing we learned was that once people learn to do it they found it really natural and actually had a lot of fun with it but almost every user that tried it had to be not only told how to do it but shown how to do it people actually had to flip this mental switch the expectations they have for how a 2d smartphone interaction works so you really need to be mindful of the context that people are bringing in the mental models they have for 2d smartphone interactions we also wanted to know if we could help someone visualize the future in a way that would let them make better decisions humans pay attention to the things that matter to us and in a literal sense the imagery that appears in our peripheral vision takes a lower cognitive priority than the things we're focused on with smartphone AR be any different in this experiment we overlaid the architectural mesh of the homeowners remodel on top of the active construction project the homeowner could visualize in context what the changes to their home was going to look like now at the time that this prototype was created we had to actual manual alignment of this model on top of the house you could do it today if I rebuilt it I would use the Augmented images API that we announced yesterday and we much easier to put a fixed image in a location the house and sync them together but even with that initial friction for the UX the homeowner got tremendous value out of this in fact they went back to their architect after seeing this and changed the design of their new home because they found out that they weren't going to have enough space in the upstairs bathroom something they hadn't noticed in the plans before so the lesson is that if you provide people high-quality personally relevant ways to create value personally relevant content you can create ways that people will find really valuable and attention-grabbing experiences but when does modifying the real environment start to break down you may be familiar with the uncanny valley it's a concept that suggests when things that are really familiar to humans are almost right but just a little bit off it make us feel uneasy subtle manipulations of the real environment and they are can sometimes feel similar it can be difficult to get right in this specific example we tried removing things from the world we created this AR and visibility cloak for the plant what we did was we created a point cloud around the object attached little cubes the point cloud applied a material to those points and extracted the texture from the surrounding environment now work pretty well in uniform environments but unfortunately the world doesn't have too many of those it's made up of dynamic lighting and subtle patterns so this always ended up looking a little bit weird remember to be thoughtful about the way that you add or remove things from the environment people are really perceptive and so you need to describe the build experiences that align with their expectations or at the very least don't defy them but as physical understanding always critical all points in this section have their place but ultimately you have to be guided by your critical user journeys in this example we wanted to build a viewer for this amazing 3d model by Damon Padilla at school it was important that people could see the model in 3d and move around discover the object a challenge though was that the camera feed was creating a lot of visual and noise and distraction people were having a hard time fishing the nuances of the model we adopted concepts from filmmaking and guided users by using focus and depth of field all which were controlled by the users motion this resulted in people feeling encouraged to explore and they really stopped getting distracted by the physical environment so humans are already great at so many things a art really allows us to leverage those existing capabilities to make interactions feel invisible if we live äj-- leveraged visual visual and physical understanding together we can build experiences that really give people superpowers with that Ellie is going to talk to you about special opportunities we have in shared augmentations thanks Chris so I'm Ellie nadigar I'm a software engineer and prototyper on Google's VR and AR team Chris has talked about the kinds of experiences you start to have when your devices can understand the world around you and I'm going to talk about what happens when you can share those experiences with the people around you we're interested not only in adding AR augmentations to your own reality but also in sharing those augmentations if you listen to the developer keynote yesterday you know that shared AR experiences is a big topic for us these days for one thing a shared reality lets people be immersed in the same experience think about a movie theater why do movie theaters exist everybody is watching a movie that they could probably watch at home on their television or their computer by themselves much more comfortably not having to go anywhere but it feels qualitatively different to be in a space with other people sharing that experience and beyond those kinds of shared passive experiences having a shared reality lets you collaborate lets you learn lets you build and play together we think you should be able to share your augmented reality x' with your friends and your families and your colleagues so we've done a variety of explorations about how do you build those kinds of shared realities than a our first there's kind of a technical question how do you get people aligned in a shared AR space there's a number of ways we've tried if you don't need a lot of accuracy you could just start your apps with all the devices in approximately the same location you could use markers or augmented images so multiple users can all point their devices at one picture and get a common point of reference the kind of here's the zero zero zero of my virtual world and you can even use the new AR core cloud anchors API that we just announced yesterday to localize multiple devices against the visual features of a particular space in addition to the technical considerations we've found three axes of experience that we think are really useful to consider when you're designing these kinds of shared augmented experiences first of those is co-located versus remote are your users in the same physical space or different physical spaces second is how much precision is required or is it optional do you have to have everybody see the virtual Bunny at exactly the same point in the world or do you have a little bit of flexibility about that and the third is whether your experience is synchronous or asynchronous is everybody participating in this augmented experience at exactly the same time or at slightly different times and we see these not as necessarily binary axes but more of a continuum that you can consider when you're designing these multi-person AR experiences so let's talk about some prototypes and apps that fall on different points of the spectrum and the lessons we've learned from them to start with we found that when you've got a group that's interacting with the same content in the same space you really need shared precise spatial registration for example let's say you're in a classroom imagine if a group of students who are doing a unit on the solar system could all look at and walk around the globe or an asteroid field or look at the Sun in expeditions they are one of Google's initial AR experiences everybody all the students can point their devices to a marker they calibrate themselves against a shared location they see the object in the same place and then what this allows is for a teacher to be able to point out particular parts of the object oh if you all come over and look at this side of the Sun you see a cutout into its core over here on the earth you can see your hurricane everybody starts to get a spatial understanding of the parts of the object and where they are in the world so when does it matter that your shared space has a lot of precision when you have multiple people who are all in the same physical space interacting with or looking at the exact same Ottoman tada jex at the same time we were also curious how much can we take advantage of people's existing spatial awareness when you're working in high-precision shared spaces we experimented with this in this multi person construction application where you've got multiple people who are all building on to a shared AR object in the same space adding blocks to each other everybody's being able to coordinate and you want to be able to tell what part of the object someone's working on have your physical movement support that collaboration like if Chris is over here and he's placing some green blocks in the real world I'm not going to step in front of him and start putting yellow blocks there instead we've got a natural sense of how to collaborate how to arrange how to coordinate ourselves in space people already have that sense so we can keep that in a shared AR if we've got our virtual objects precisely lined up enough we also found it helpful to notice that because you can see both the digital object but also the other people through the pastor camera you are able to get a pretty good sense of what people were looking at as well as what they were interacting with we've also wondered what would it feel like to have a shared AR experience for multiple people in the same space but who aren't necessarily interacting with the same things so think of this more like an AR land party where we're all in the same space or maybe could be different spaces we're seeing connected things and we're having a shared experience so this prototypes a competitive quiz guessing game where you look at the map and you have to figure out where on the globe you think is represented and stick your push pin in get points depending on how close you are we've got the state sync so we know who's winning but the location of where that globe is doesn't actually need to be synchronized and maybe you don't want it to be synchronized because I don't want anybody to get a clue based on where I'm sticking my push pin into the globe it's fun to be together even when we're not looking at exactly the same AR things and do we always need our spaces to align exactly sometimes it's enough just to be in the same room this prototype examples of AR boat race you blow on the microphone of your phone and it creates the wind that propels your boat down the little AR track by us being next to each other when we start the app and spawn the track we get a shared physical experience even though our AR worlds might not perfectly align we get to keep all the elements of the social game playing talking to each other our physical presence but we're not necessarily touching the same objects another super interesting area we've been playing with is how audio can be a way to include multiple people in a single device AR experience if you think of the standard magic window device they are it's a pretty personal experience I'm looking at this thing through my phone but now imagine you can leave a sound in AR that has a 3d position like any other virtual thing and now you start to be able to hear it even if you're not necessarily looking at it and other people can hear the sound from your device at the same time so for an example let's say you could leave well notes all over your space might look something like this this is a cherry so notice you don't have to be the one with a phone to get a sense of where these audio annotations start to live in physical space another question we've asked if you have a synchronous AR experience with multiple people who are in different places what kind of representation do you need of the other person so let's imagine you have maybe a shared AR Photos app where multiple people can look at photos that are arranged in space so I'm taking pictures in one location I'm viewing them arranged around me an AR and then I want to share my air experience with Luka who comes in and joins me from a remote location what we found we needed a couple of things to make us feel like we were connected and sharing the same AR experience even though we were in different places we needed to have a voice connection so we could actually talk about the pictures and we needed to know where the other person was looking see which picture you're paying attention to when you're talking about it but what was interesting is we didn't actually need to know where the other person was as long as we had that shared frame of reference we're all here here's what I'm looking at here's what Luke is looking at we've also been curious about asymmetric experiences what happens when users share the same space and the same augmentations but they've got different roles in the experience so for instance in this prototype Chris is using his phone as a controller to draw in space but he's not actually seeing the AR annotation she's drawing the other person sees the same AR content uses their phone to take a video they're playing different worlds in the same experience the kind of artist versus cinematographer and we found there could be some challenges to asymmetric experiences if there's a lack of information about what's um the other person's experiencing for instance Chris can't tell what luca's filming or see how his device is drawing the looks from far away so as we mentioned previously these kinds of different combinations of space and time and precision are relevant for multi-person AR experiences and they have different technical and experiential needs if you have multiple people in the same space with the same augmentations at the same time then you need a way of sharing you need a way of common localization that's why we created the new cloud anchors API if you've got multiple people in the same space with different augmentations at the same time the kind of AR lamp RT model you need some way to share data and if you've got multiple people in different spaces interacting with the same augmentations at the same time you need sharing and some kind of representation of that interaction so shared AR experiences is a big area we've explored just some parts of the space we'd love to see what you all come up with so Chris has talked about examples where your device understands your surroundings that gives you special powers I talked about examples where you've got multiple people who can collaborate interact now local we'll talk about what happens when your devices have a better understanding of you and allow for more expressive inputs thank you Andy my name is Luke oppressor and I'm a prototyper and a technical artist working in the Google ARMD our team so let's talk about the device that you carry with you everyday and the one they are all around and how they can provide the meaningful and authentic signals that we can use in our augmented experiences so Eric or Trax the device motion as we move to the real world and provide some understanding of the environment and this signal sir can be used to create the powerful and creative and expressive tools and offer new ways for us to interact with the digital content so data represent who we are what we know and what we have and we were interested in understanding if the user can connect more deeply if the data is display around them in 3d and 2ar and physical aspirations they can look at the distance data so we took a database of several thousands world cities and we map it in an area that's why there's a football field we assign dot to every city and we scale the dot based on the population of the city and each country has a different color so now you can walk to this data field and as their core tracks the motion of the user we play footsteps in sync you take a step and you hear a step and on ambisonics sound field surrounds the user and enhance the experience and the sense of aspiration of this data forest in fly paths our display up up in the sky and the pass through camera is heavily tinted so that we can allow the user to focus on the data and then still give a sense of presence and what happens is the user has he walks to the physical space start mapping and pairing and creating this mental map between their the data and the physical location and starts understanding better in this particular case the relative distance between places and what we discover is also that the gesture they are part of our digital life everyday a pinch to zoom it's now in AR something more traditional it's a it's actually moving closer to the digital object and inspecting it like we do with the real object and and pan and drag means taking a couple of steps to the right to look at the information so physical expiration like this is very fascinating but but we need to take an account all the different users and provide the alternative movement affordances so in our a user can move everywhere but what if we cannot avoid or he doesn't want to move what if it's sitting so in this particular case we allow the user to simply point the phone everywhere they want to go tap on the screen anywhere and the application will move the point of view in that direction and the same time we still have to provide audio haptics and color effects to an answer the sense of physical space the user has to have well traveling and so we found that this is a powerful mechanism to explore a certain type of data that makes sense in the 3d space and to allow the user to discover hidden patterns but can we go beyond the pixels that you can find on your screen we're fascinated by the special audio and the way to incorporate audio into an IR experience so we combine air core and the Google resonance as decay and resonance is this very powerful special audio engine then recently Google open source and you should check it out because it's great and so now I can take audio sources and place them in the 3d locations and animate them and describe the properties of the walls and the ceilings and the floor and all the obstacles and now as the air core moves the point of view it carries with it the digital years there is a resonance used to render accurately the sound in the scene so what can we do with this so we imagine what if I if I can sit next to a performer doing an acoustic concert or a classical concert or a jazz perform so what if I can be on stage with actors and listen to their play and be there so we took two amazing actors Chris and Ellie and we asked them to record separately as lines from Shakespeare and we place this this audio sources a few feet apart and we surrounded the environment with an ambition exam field of a rainforest of the reigning and then we later on we switch with a lot of reverb into the walls [Music] so now now the user can walk around maybe with his I suppose a nice pair of headphones and it's like being on stage with these actors so we took this this example and we extended we we observed that we can build in real time a 2d map of where the user has been so far with this phone as its walking around and so at any given time when the user hits a button we can programmatically place audio recording in space where we know that the user can reach with this phone and with their ears [Applause] [Music] [Applause] [Music] and suddenly the user becomes the human mixer of this experience and and different instruments can populate your squares and your rooms and your schools and and this opens the door to an amazing amount of opportunities with a our audio first experiments so let's go back to visual understanding Chris mentioned that the computer vision and machine learning can interpret the things are around us and this is also important to understand what the body and in turning into an expressive controller so in real life we are surrounded by a lot of sound sources for all over the places and naturally our body and our head the moves to mix and focus on what we like and what we want to listen to so can we take this this intuition into the way we watch movies or play video games from a mobile device so what we did we took the founder camera signal fed it to Google mush mobile vision that gave us a head position and head orientation and we fed it to to Google resonance SDK and we said ok you're watching a scene in which actors are in a forest and they're all around you and it's raining so now as I leave with my phone far away from my head I hear the forest as I'm taking the phone closer to my face I start hearing the actors playing I warn you this is an Oscar performance [Music] all our company here by man according to the scrip here is the scroll of every man's name which is thought fit through all Athens to play in our interlude before the Duke and the Duchess so now now what is interesting is that the tiny little motions that we can do when we're watching and we're playing this experience that can be turning to subtle changes in the user experience that we can control so we talk about how the changes in poses can become a trigger to drive interaction in this Google research app called self-esteem oh we actually exploit the opposite the absence of motion and when the user in this case my kids are stop posing the app takes a picture and so this simple mechanism that is a trigger by computer vision create an incredible delightful opportunities that apparently my kids love and research is doing incredible progress in looking at an RGB image and understanding where the body pose and skeleton is and you should check out the Google research blog post because their post estimation research is is amazing so we took Ally's video and we fed it to the machine computer algorithm and we got back a bunch of 3d poses and segmentation masks of a valley and this opens the door to a lot of variety of experiments with the creative filters that we can apply to this but what is more interesting for us is that there also allows us to understand better the intent in the context of the user so we took this pose estimation technology and we added the digital carrot that now tries to mimic what the human character is doing and this allows them now to bring your family and friend in this case my son Noah into the scene so that act and create a nice video but this also like Emily like Ali mentioned before we should consider this situation because this is an asymmetric experience what you don't see here is how frustrated my son was after a few minutes because he couldn't see what was going on I was the one having fun taking picture and video him and he didn't see much you can only hear the lion roaring so we need to be extremely mindful as the developer about this imbalance of the light and so maybe I should have casted the the image of the the phone to a nearby TV so I can make my son first class citizen in this in this experience so all these AR technology and the physical and visual understanding are ingredient that allows us to unlock all kinds of new expressive input mechanism and we are still exploring with just at the beginning of this journey but we are excited to hear what you think and what you want to come up with so to summarize we share a bunch of ways in which we think about AR and various inspiration that we have done we talk about expanding our definition of they are putting content into the word but also pulling information from the world and these are all ingredients that we use it to create this magical AR superpowers to enhance the social interactions and to express yourself in this new digital medium so we combine air core capabilities with different Google technologies and this give us the opportunity to explore all these new interaction models and we encourage you developers to stretch your definition of they are but we want to do it this together we're going to keep exploring but we want to hear what tickle you what tickle your curiosity so we can wait to see what you build next thank you very much for coming [Music] 