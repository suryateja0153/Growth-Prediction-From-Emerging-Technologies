 human vision is amazingly beautiful and complex it all started billions of years ago where small organisms developed a mutation that made them sensitive to light fast-forward to today and there's an abundance of life on the planet which all have very similar visual systems they include eyes for capturing light receptors in the brain for accessing it and a visual cortex for processing it genetically engineered and balanced pieces of a system which help us do things as simple as appreciating a sunrise but this is really just the beginning in the past 30 years we've made even more strides to extending this amazing visual ability not just to ourselves but to machines as well the first type of photographic camera was invented around 1816 where a small box held a piece of paper coated with silver chloride when the shutter was open the silver chloride would darken where it was exposed to light now 200 years later we have much more advanced versions of the system that can capture photos right into digital form so we've been able to closely mimic how the human eye can capture a light in color but it's turning out that that was the easy part understanding what's in the photo is much more difficult consider this picture my human brain can look at it and immediately know that it's a flower our brains are cheating since we've got a couple million years worth of evolutionary context to help immediately understand what this is but a computer doesn't have that same advantage to an algorithm the image looks like this just a massive array of integer values which represent intensities across the color spectrum there's no context here just a massive pile of data it turns out that the context is the crux of getting algorithms to understand image content in the same way that the human brain does and to make this work we use an algorithm very similar to how the human brain operates using machine learning machine learning allows us to effectively train the context for a data set so that an algorithm can understand what all those numbers in a specific organization actually represent and what if we have images that are difficult for a human to classify can machine learning achieve better accuracy for example let's take a look at these images of sheep dogs and mops where it's pretty hard even for us to differentiate between the two with the machine learning model we can take a bunch of images of sheep dogs and mops and as long as we feed it enough data it will eventually be able to properly tell the difference between the two computer vision is taking on increasingly complex challenges and is seeing accuracy that rivals humans performing the same image recognition tasks but like humans these models aren't perfect they do sometimes make mistakes the specific type of neural network that accomplishes this is called a convolutional neural network or CNN CNN's worked by breaking an image down into smaller groups of pixels called a filter each filter is a matrix of pixels and the network does a series of calculations on these pixels comparing them against pixels in a specific patterns the network is looking for in the first layer of a CNN it is able to detect high-level patterns like rough edges and curves as the network performs more convolutions it can begin to identify specific objects like faces and animals how does a CNN know what to look for and if it's prediction is accurate this is done through a large amount of labeled training data when the CNN starts all of the filter values are randomized as a result its initial predictions make little sense each time the CNN makes a prediction against labelled data it uses an error function to compare how close its prediction was to the images actual label based on this error or a loss function the CNN updates its filter values and starts the process again ideally each iteration performs with slightly more accuracy what if instead of analyzing a single image we want to analyze a video using machine learning at its core a video is just a series of image frames to analyze a video we can build on our CNN for image analysis in still images we can use CNN's to identify features but when we move to video things get more difficult since the items are identifying might change over time or more likely there's context between the video frames that's highly important to labeling for example if there's a picture of a half-full cardboard box we might want to label it packing a box or unpacking a box depending on the frames before and after it this is where CNN's come up lacking they can only take into account spatial features the visual data in an image but can't handle temporal or time features how a frame is related to the one before it to address this issue we have to take the output of our CNN and feed it into another model which can handle the temporal nature of our videos this type of model is called a recurrent neural network or our nm while a CNN treats groups of pixels independently an RNN can retain information about what it's already processed and use that in its decision-making rnns can handle many types of input and alpha data in this example of classifying videos we train the RNN by passing in a sequence of frame descriptions empty box open box closing box and finally a label packing as the RNN processes each sequence uses a loss or error function to compare its predicted output with the correct label then it adjusts the weights and processes the sequence again until it achieves a higher accuracy the challenge with these approaches to image and video models however is that the amount of data we need to truly mimic human vision is incredibly large if we train our model to recognize this picture of a duck as long as we're given this one picture with this lighting color angle and shape we can see that it's a duck but if you change any of that or even just rotate the duck the algorithm might not understand what it is anymore now this is the big picture problem to get an algorithm to truly understand and recognize image content the way the human brain does you need to feed it incredibly large amounts of data of millions of objects across thousands of angles all annotated and properly defined the problem is so big that if you're a small start-up or a company lean on funding there's just no resources available for you to do that this is why technologies like Google cloud vision and video can help Google digests and filters millions of images and videos to train these api's we've trained a network to extract all kinds of data from images and video so that your application doesn't have to with just one REST API request we're able to access a powerful pre-trained model that gives us all sorts of metadata here's how easy it is to call the cloud vision API with curl I'll send this image to the API and here's the response we get back billions of years since the evolution of our sense of sight we found that computers are on their way to matching human vision and it's all available as an API if you'd like to know more about the cloud vision and video api's check out their product pages at the links here to see how you can easily add machine learning to your application thanks for watching [Music] 