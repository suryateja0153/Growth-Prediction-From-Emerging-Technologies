 good morning I'm sure you've all been seeing a lot about mix reality today especially if you've gone to the expo floor you see mixed reality kiosks all over the place so perhaps you're wondering how to get started with mix reality or perhaps you're already developing mix reality apps and you're wondering about ways you can take better advantage of what tools are available for you my name is Peter priests and I'm part of the XR team at unity and for us we use the umbrella term XR to indicate mixed reality virtual reality augmented reality and for us the X is not just a mutable term for a letter that changes but it's our part of our commitment to cross-platform technology so the way I'm going to do this talk today I'm gonna assume that some of you may be new to unity or mix reality so I'm gonna have a brief introduction to what unity is and then I'm going to dive into some specific topics that I think are pertinent to developers who are making this reality applications and I'm going to cover some of the newer features in unity and then I'm going to come over come up with some recommendations for developers that'll make their lives a little bit easier what is unity so if you go to the end of the website you'll see like these fancy terms that like the world's leading interactive content development platform so that sounds like a lot of buzzwords but what does it mean I'm gonna break that down bit by bit in the past we used to describe unity as a cross-platform game engine I imagine most of you aren't game developers in here so why are we here well developers used to use unity create games 3d and 2d content maybe interactive with scripts written primarily in c-sharp but along the way we have evolved into something much bigger we certainly still support games and games have very specific real time interaction and rendering requirements but unity is now used for a wide range of non gaming markets and they all use unity because they are creating applications that have those same real-time interaction and rendering requirements earlier part of that phrase is world's leading well that's not hard purpley unities development platform has massive reach running on nearly three billion devices and it's a tool of choice for VR and AR development just last year alone unity was had unique downloads of unity apps of 24 billion downloads so still what is unity for some developers unity looks like this it's an integrated environment allows you to create complex and interactive 3d content you touch components which provide functionality to objects and the components can define everything from how the object renders to how it behaves with things like physics but to other developers unity looks like this it's a fully scriptable engine using c-sharp and features a rich API that allows you to manipulate objects and interact with services nearly every platform you can think of unity is on it desktop mobile console and VR and AR we currently support 25 different platforms and this number is definitely growing and we work along side large partners like Microsoft Google Facebook and Apple to ensure optimized support for all their latest hardware unity is driven by three core principles the desire to democratize development to help in solving hard problems and to enable success we work towards all of these with our support for VR and AR the unity editor is rapid iteration makes new development easy and accessible our highly optimized rendering pipeline can help you achieve exceptional frame rates with unity you can create X our applications that run on multiple devices without having to learn a new API and unity has been there from the beginning with Windows mix reality we started two years ago adding support for hololens early in development and we had something called a Holland's technical preview which was available too early developers and we did the same thing with Windows mix reality we've been supporting hololens officially 2016 and we added an official support into the unity mainline for Windows mix reality on desktop last year so I want to talk about how easy it is to take an application it's already written in unity and enable VR and essentially it's one-click this is part of what we believe that solving hard problems there's a lot of things that happen under the covers with that one click but essentially you simply enable VR in your application and the application runs on VR now there's more complexity than that but this is all it takes to get you up and rendering for Windows mix reality and to demo that I'm just gonna show you how easy it is so let's see how I do with a live demo of this simple application here is just an object that has a simple script attached to it and if I hit play that's why we have anticipated things alright so we'll just jump to the end result so this is I can find my cursor this is this is that check box you just saw on that screen this is virtual reality supported so if I turn this off and hit play we'll just be running in the editor I'll drag the window over and so there's an a simple object animating this is an incredibly simple this is a programmer project no artists involved so I'm gonna hit stop and I'm gonna go back over here and I'm going to turn on VR now as soon as I do this I get little drop down of the virtual reality SDKs that are supported since I've selected a Windows uwp as my target I only have one API available which is Windows mix reality if I was targeting some other platform I have access to other api's you could target oculus vive AR k AR core those kind of things so and over here this is the windows mix reality headset running and I'm running it the shell right now so I'm going to tab back and run that same application and basically I've just launched it makes reality up from unity where's my app there's my object it's underneath me so so hope you can see that so anyway that's all I had to do to enable mixed reality and a unity application back so that's great if you're developing on desktop and what if you're developing on hololens well if you're developing on hololens things are a little different it's the desktop application is actually running on the same desktop computer that you're running the editor in the hololens is a standalone computing device so normally the process would be test in the editor perhaps using some subset of the api's and then you would deploy to the device and that involves creating a visual studio solution that you then build and then deploy to the device so the iteration time can be several minutes and we want it to way to improve that so last year we added a feature called holographic emulation we have two modes we can run this in one is holographic remoting and this allows us to connect to a remote in player on the hololens and still gives us the ability to still run the device in the editor so this is awesome for iteration time it means you can test your app iterate make changes to the application make changes to your script and you don't have this long development time so incredibly powerful for developers the other option is something called simulation which allows you to even if you don't have a hololens device you can run on a virtual machine Hollande's directly from inside the editor so even developers who don't have access to a whole lens or perhaps someone else in your team is using hololens device because you have limited numbers you can still work on your application so anyway as I said this is pretty awesome for developers rapid iteration you can actually the what's rendering on the from the editor it's exactly what's rendering on Holland so there's no fakery there most of the API is supported so with the hololens that means you have access to the hand gestures and those will actually be transmitted across the network to the to the unity editor you have access to spatial mapping so you can read in surface data from your environment and make have your application reason from that data the only limitation is currently right now speech input from the hololens is not supported and also the photo and video capture from the hololens will actually use the photo and video capture from your desktop machine and if you've done any hullins development you may know about world acres those are currently not supportive or not sure if we'll be able to add support for that there's some interesting Network burden that happens because of the size of the data set so the Holland's it's a pretty powerful computer but sometimes you want more for your standalone applications so the CPU is a 1 gigahertz processor Atom x5 the you've got 2 gigabytes of RAM and if you've already done Holland's development you know you have to really trim down what your application can do to get it to perform well in homeland so achieving high frame rate is really important for any sort of immersive immersive environment so what if you wanted to take advantage of the type of remoting I just showed you where I indicated you can do from the other I actually have a demo that I can't get working right now so I'm gonna talk over that but I've got some videos to show you some interesting things but what if you wanted to be able to take advantage of the same sort of connectivity you get of connecting to the hololens from the editor and running your application from your standalone application in other words what if you wanted to use the full power of a desktop computer on the hololens so in 2018 point to of unity we're allowing you to do just that you can create a standalone remoting application so this is going to connect to the hololens the application will run on a full powered desktop computer connect to the hololens over a network connection the user or user is wearing the hololens we'll see the full rendering environment and you get the power of the desktop on the hololens so I want to show you two examples of some companies from very different industries who are already using this sandal and remoting to create applications which were not previously possible the first one is meta biz so meta vis is a technology company that was created to leverage augmented reality and artificial intelligence to transform medical and surgical of visualization so medicine has traditionally used 2d grayscale images to understand complex 3d anatomy the first CT scan was done nearly 50 years ago but today we still show grayscale images the resolution is higher but there's no better way to visualize the data so medical imaging often creates very large 3d textures 512 by 512 by 512 volumetric textures this is far beyond the RAM capabilities of the hololens so they were able to use stand-alone remoting to allow them to offer full resolution versions of this volumetric data by running the rendering on a desktop computer what you're seeing here is the render X software running in an operating room this is all captured directly through the hololens and this is allowing doctors to have surgical visualization through reconstruction of traditional medical imaging data this is pretty awesome and like this could potentially transform medicine another early adopter this technology is Volkswagen virtual engineering team in Wolfsburg Germany so they've used augmented reality to help save time and development costs they have tools that allow Volkswagen engineers to work on a virtual vehicle to change its equipment as they wish and even design new components virtually they can able to see the results of their work immediately the complexity of CAD models that they're using can contain tens of millions of triangles this geometric complexity is beyond the capabilities of the current hololens so they were able to use stand-alone remoting to be able to show these full resolution models directly on hololens device and using high-quality shaders I should mention that unity is now partnering with Pixies which is a CAD software technology company and if you visit the unity booth you can find out a little bit more about how you can directly use cad data inside unity so this is where I do my remoting demo so actually I'll do this demo and I'll do it with mixed reality hardware so I want you to imagine I'm showing this done hololens one of the issues with the hololens that it's difficult to demo in a room like this is i have to have a Wi-Fi connection to it and you're all using the Wi-Fi as well so it makes the connection a little bit unstable so I'm just going to demo this with the same mixed reality hardware but I want you to imagine that this is running on a Hulland so think about how you could take advantage of this type of rendering capability on a whole lens so I've got that same model which autumn ember how many triangles it was so it's just some some CG like very simple CG constructed geometry that I'm animating and I have some simple not terribly simple but photorealistic shaders on it but instead of rendering one of them I'm gonna render 10,000 of them which gives me a triangle load of about 15 million triangles and so here's that same scene and basically I'm running a full like I'm basically running at 90 frames per second right now so and this is essentially the same view I would see on a hall lens so I want to talk a little bit about stereo rendering right now so in a traditional rendering environment we render the scene from a single view we take the objects in the scene we have geometry associated with those objects and we transform them into a space appropriate for rendering we do this by applying a series of mathematical transformations to the objects where we take them from a locally defined space into a space we can draw on the screen this series of transformations is sometimes referred to as the graphics transformation pipeline and it's an essential part of plastic technique and rendering so VR devices introduce the requirement of defining two views to provide the stereoscopic 3d effect that creates depth for the viewer so each of you represents an eye well the eyes are viewing the same scene from a similar vantage point each view possesses a unique view and projection matrix and they may even see the object slightly different because of lighting calculations in order to render the view for each eye the simplest method is to run the render loop twice each eye will configure and run through its own iteration of the render loop and at the end of the scene at the end of the process we'll have two images that we can submit the display device and what you're seeing right now is how what we call multi pass rendering works in unity we render all the left eye and then we render the right eye and we combine those into a single unified view there are some performance implications of this and so last year we looked at ways to improve this and we came up with two techniques and in 2017 point two last year we released support for something called stereo instance rendering and this is works for all XR devices running on DX 11 so if you're running a unit a Windows mixed reality device that's definitely on dx11 because that's required for support of it so this is some extensions of some work we did previously to support single pass stereo and other types of devices and those we used a double-wide texture way single pass instance rendering works as we use a instead of two separate textures or or a double-wide texture we use a dx11 texture and we're able to multiplex the draw calls using the GP some of the GPU extensions in the LX dx11 and essentially we do two things instead of rendering through our owns our whole scene graph twice we only have to try out traverse the scene graph once and the other advantage we get is instead of issuing two draw calls one for the left eye and one for the right eye we issue a single draw call and then we allow the graphics processor to split those across both eyes so this halves the number of draw calls required and that's the performance gain so to enable single pass instance rendering in unity it's just as easy as enabling a checkbox so I should point out that we default to multi pass right now because some applications may have custom written shaders and they don't implicitly work on stereo single pass instance stereo rendering so there's some shader changes that you might have to make so this is an idea of what some of the performance differences are and you can see that most of the performance gains are in the CPU not on the GPU the graphics processor is already sort of working at full efficiency regardless of how we do it and we find that a lot of mixed reality apps are in our end up being CPU bound so this is a huge performance gain for them the interesting thing is if you start out CPU bound and you make some optimizations and you find your GP better GPU bound it so it's a little bit of a ping-pong back and forth to try and make your performance go up but you'll notice that one of the things on here is the single pass stereo and single pass instant stereo aren't as significant gains and we think the reason for that is a lot of graphics drivers are already multi-threaded so they can split up some of this work but either case it's a pretty significant performance gain so this is a little bit of the more technical parts of my talk if you are if you do have shaders custom shaders that you've written and you're looking to take advantage of single past instance rendering there's a couple of changes you'll need to make to your shaders the simplest one has just turned on the instancing for your materials in unity and then we have a bunch of macros that you'd want to use in your shaders to help make support for single past instance rendering easier your vertex fragment vertex shader will need to have an additional output parameter that will specify the which index of the texture array is being used and so macros will do that for you and if you have any screen space or post-processing shaders they'll also need to do some special things because you can't simply assume that the UV coordinate so you'd normally deal with are the correct ones when we're doing stereo instance rendering as I said one of the core principles of unity is solving hard problems and we try to do that with VR so as I showed you in a demo converting an existing application into VR is easy as ticking a checkbox well that's not really true there's a bunch of more things you have to do to make your applications really a VR application and dealing with input is another one of those so what happens when you enable that one of the things that happens when you enable that checkbox is that unity when you run your application unity finds the main camera in your scene now simple demo applications will only have a single camera but sometimes you'll have multiple cameras in the scene so we try to figure out which one is the main camera intended for the HMD and then we control that camera via input from HMD head-mounted device this is the way that VR has worked in unity since we had support for it several years ago but this required developers to add their own support for handling controllers so we wanted to create a system that worked for controllers as well as the head mounted devices last year we added a new component and unity called deep tracked pose driver so this works for the head mounted devices as well as controllers and you can use it to define various types of ways you're going to drive the camera pose from the head mounted device you can have it tracked with the left eye the right eye a center eye or in the middle of the head position and you can specify if you're driving controllers with it whether it's the left controller or right controller and there's a couple ways we're going to extend this for other types of input devices so this will rip this is enabled on a game object that replaces the implicit camera update that happens by default now if you think about this one of the things that can happen in unity is is the camera position is controllable in the editor so you can start out it can start out at a zero zero zero position but developers and content creators can move the camera around to get a view of the scene so what happens with that if the headset is controlling the position of the camera the way we made this work is that we capture this transformation that's on the camera initially we call that a reference transform and then whenever we read input from the headset we post apply the transformation of that camera so that the headset matches what the expectation is of where the camera was initially set in the scene this is sounds a little complicated and it can actually lead to problems for developers because there's some confusion around this one of the issues is this only applies to that node so if you have other nodes in your scene graph that you want to control say controllers they can be in a different reference frame than the headset so you're running your application and your controllers are five meters over to the right why is that so this kind of has led to some problems for developers and so we actually have a recommendation of how to set up a scene graph using some VR components that I want to show you now that well I think will avoid some of that confusion and we're actually going to be discouraging people from using that implicit reference transform and that's why let's see on this other slide here I this is a checkbox here that just sort of says enable that reference transform and we're going to be probably deprecating at some point in the future just because we think if it adds complexity and we think there's a better way of doing this so this is a kind of how we imagine this is a very simple slide view obviously your scenes would be much more complicated but this is the basic essence of how we imagine you would want to set up the what we're calling an XR rig so you have some root node and this is the thing that you want to move the player or viewer I call it player because that's the way unity refers to things our background is a game engine but the user this is what you use to move them around the scene obviously if they're wearing a head said pull lens or a mixed reality device they can move but they're going to be have limited movement and one of the things you particularly if they're if it's a mixed reality headset where they're tethered to a computer they want to explore different areas of the scene so you're going to need to have sort of way of teleporting them around the scene so this would be the node you would use to do that so you have complete control over where this position is and we also recommend you use this to represent their position ground height beneath that you have a child node which we call the floor offset so there are a couple different ways of dealing with coordinate systems and mixed reality and some of them depend on which type of device you're using if you're using a hololens everything is going to be what we call world tracking so the device when you first turn it on its current position is going to be considered zero zero zero and whichever way it's looking sort of the yaw that's going to be the orientation not accounting for a pitch and roll with a desktop immersive devices they can act that way as well but they can also act as a coordinate system within a reference frame that they specify when they do the setup and this is the if you view your initial configuration you set up a boundary for the device in that case the coordinate system will be zero zero zero is the ground of the center of that boundary area so unity supports both of these and will implicitly choose the best one for the device but this is what this node is for so if your application needs to run on hololens and a mixed reality device or you're running on a immersive device and you don't know if the user is set up a reference frame this will allow you to deal with that floor offset either way and then finally we have nodes for the camera and any controllers and you can just add the track pose driver on these and use these to drive the headset position on the hand position so that gives you input about where the headset is and where the controllers are but you need to do more with them put the net you're going to want to be able to use the buttons on the controllers as well as potentially give haptic feedback to users so you can't do that to the track pose driver and there's two different ways of doing this in unity so there's sort of the the cross-platform way and then there's a Windows mixed reality specific way and there's advantages and disadvantages so using the cross-platform way as use the existing unity input system and you've got just some API is like get input good access this will work across to any type of device any type of controller the downside of these is you have to kind of look up what the the button mappings are and what the access mappings are is they're all documented and I'll give you some links at the end of this talk where you can find those but there's also some interfaces we provide inside the unity WUSA input namespace and these give you much more control and more specific input from the devices so you can access any of the buttons exposed on the controller is not just a few of them and you also get access to the gesture recognizer which is an important interface API for the hololens so this is one of the interfaces inside the exercise of the XR WS a namespace and this is the interaction manager so you have two ways of dealing with this as well so you can use a polling method where applications will query this each frame and find out what the state of the devices are now when you do polling this will actually give you forward predicted pose information for controllers and the head so you get the you want to know the state of the controller you can pull it you can find out what buttons are down and you can find out the position of that controller in space but what I mean by forward prediction is one of the issues one of the hard problems in VR is latency so you have the computer application running and rendering objects in the scene and then at some point it appears on the headset and the latency for that whole process can be 30 to 40 milliseconds and if we if we don't adjust an account for that latency the experience is really bad for the user so there's a bunch of prediction that goes on so then when the scene renders it'll actually render of what the predicted headset position is and then at the last moment there's also a fix-up based on corrections to that prediction so when you use this polling interface this will actually give you forward predicted pose information based on the controller's current position and their velocity and using this API you can find out whether touch or menu or select or thumbs to here's one of the joysticks if you haven't seen it so you can find out the state of all these interfaces on this there's also an event-driven interface and this will give you information about the source and head pose but not forward predicted but when the event occurred so this is important for handling the intent of user because it uses the state of the controller and the hand at the time the event happened so we have event handlers were detected lost released and updated events so the Preston released events are always going to be bracketed somewhere within a detected and lost event so when the headset sees the controller it'll do it detected and when it's out of sight it'll be lost the same is true in the hololens when the hands are visible you'll have a detected event for the hand and a lost event when the hand is no longer visible next topic is a little interesting so when we started out with unity we originally had three different scripting languages c-sharp something called boo and something called unity script and unity script that was a JavaScript Lite language and to make things but our lives are much more sane now because we only still support a single scripting language which is c-sharp developers seem to prefer this it's much more powerful and allows a better application sharing and we used the mono compiler and the mono back-end to support c-sharp when we first added support for Windows 8 Store apps though we couldn't use the mono runtime windows 8 store already had a dotnet back-end but there were some API limitations that we were not quite able to get around so things like suspend thread get thread contacts virtual Alec these weren't available to us for applications and so what we ended up doing is a reported unity to dotnet this is a tremendous amount of effort on our part it's something like half a million lines of code to make all this work and there's a couple issues with this there's a slightly different API surface than a 3.5 on other platforms and any developers who have an application that works on some other unity some other platform through unity and then they for Twp this is can be a significant courting pain because they may find some of the api's you're using don't exist or they're slightly different our asset garbage collection had to be written for dotnet and some features we actually couldn't implement at all so it was end up being a subset this is still a lot of resources to maintain so two years ago we introduced something called il to CPP this was actually for other platforms other than uwp where we didn't have Excellus to any any dotnet and we couldn't use mono so what il - CPP it's a unity developed net runtime so we compile the c-sharp code using mono or some other c-sharp compiler but IO and then I'll - cpp converts the il code into C++ code and then we compile the C++ code into whatever native binary files whether it's an exe an apk and xá pá whatever for the specific platform you're working on so the biggest advantage of Eyal to CPP is that you can create scripts which compile across all platforms without any modification it's the same net back-end this gives us the ability to have a robust performant and consistent API service across platforms there are a couple limitations right now that we're fixing very soon the first is that there's currently no managed c-sharp debugger for il-2 CPT code so if you're building a project using alt CPP and you want to debug it you're gonna be debugging the C++ code that's that's developed that can be kind of difficult we have some we have some blog posts that describe how to do that a little easier but we're actually fixing this in unity twenty-eight point twenty eighteen point two which will be releasing later this year and it's in beta right now so we've already got this working on so this is going to be integrated with Visual Studio 2017 video Visual Studio Tools for unity and we're also going to make this work with JetBrains writer so the idea is you can build your code write your code in c-sharp compile it run it run through I it'll run through il-2 cpp automatically but then when you're debugging your application you'll actually be able to set breakpoints in your c-sharp code and step through their code just as you would with the.net back in the other issue is the full builds are slower in iterations a little bit more cumbersome since any code changes are going to have to require recompiling the C++ code and as of 2018 point to full incremental build should work with that and I also recommend if you're doing this just run your builds on an SSD and make sure you don't have anything like malware scans going on and that drive so that'll improve the performance and the other thing you can do is you can just do your iteration directly in the editor which I showed you will gives you full access to the device and all the controller input so if you want to enable il-2 cpp basically you just go through and in the project settings that you can see right here there is a scripting runtime and you can change that the ILC il-2 CPP and if you want to use some of the WinRT API functions just set your API compatible compatibility level to dotnet 4.6 now if you've got some other API you're using from other vendors you can just take the WMD files and drop those into unity into your project along with the dll's and then you can call those as well so we're going to be encouraging developers to use il to CPP sometime later this year we're probably going to be deprecating support for the.net back-end in favor of il to CPP so il to CPP is something we really want to get right so I encourage everyone if you're doing stuff in unity take a look at it please give us feedback this is we're really committing to making il-2 cpp excellence excellent the.net support that's in unity 2017 point 4 we're gonna continue to support that for at least two years so you're not the Spile dotnet back-end is not going away magically you have plenty of time to look and try to do the transition dial to CPP so at this point I want to give you just some really simple guidance if you're doing Windows mix reality development some tips that I think will make your life a bit easier the first is to take a look at il to CPP which I just talked about it's available now and if you have any feedback please give it to us we really want to make this excellent and we're committing to supporting this and also don't use that implicit reference transform on the camera that's going to make synchronizing with controller input more difficult and your life will be a lot easier if you just keep that camera zeroed out and and handle moving the user around the scene via higher-level node in the scene graph so this one is interesting so if you're debugging and you hit a breakpoint and you're running in full-screen you'll basically just see this uwp tile filling your window and you won't see the visual studio in the background so you may be confused what's going on there's really no reason to have full screen enabled for a mixed reality immersive at all that really does is just set some initial settings on the windows and because of the way unity works its kind of hard for us to make that network for you WPF so I just recommend turning off the full screen by default there's no real reason for it the to have that enabled and it'll make development for a mix tree little bit easier the second thing related to that is also a debugging thing right now the way unity detects when the user is wearing the headset is we actually check for a window focus from the mixed reality core window that's associated with the headset from the OS level and so when you take off the headset that window loses focus so the same thing can happen if you actually tap out to another application like visual studio so if you're if you're debugging an application you hit a breakpoint essentially from the mixed reality application it says hey some other so I'm neither app has focused I need to stop rendering and so it'll pause rendering which makes your application go am i rendering or not so there's a toggle you can just set which enables you to always continue running in background regardless of whether the app has focused now that may not be what you want to ship with but it'll make debugging a lot easier and I also recommend us use single pass instance stereo rendering unless you have some custom shader that you can't adapt this is basically a free performance gain for you and so this other one is a little tricky I want to explain this briefly so Windows uses Windows mixed reality uses a process they call late-stage reprojection to compensate for movement of the user's head between them time a frame is processed and when it's rendered and when it appears on the display we usually call that the time to photon which is when your photons hit your retina and so to compensate for this latency there's a couple things that happen first of all as I mentioned there's this prediction that we use which says based on where the headset is now and based on its current velocity and we know about how long it's going to take the render this frame we can predict where the headset will be and so will render the scene from that future predicted point of view but there's a little bit more that goes on because when the device actually does composition of the scene on to the headset it does this additional process called late-stage reprojection so it takes where it thought the headset was going to be and where the headset actually is and then it'll adjust the image by reprojected it on to the display to correct for that error between the prediction and reality on the whole lens this rear projection is done using what they call a focal plane the desktop applications can actually get better quality rendering by using per pixel reprojection and to enable this there's this little flag which says do depth buffer sharing this basically gives the Windows mix reality runtime access to depth information as we share it so it can take a look at each pixel know its depth in the scene and do correct reprojection for that so enable this for desktop applications and you'll get better reprojection on the hololens what the depth buffer sharing will do was actually give the device API a little better sense of where the average holograms are from the user and so every reprojection will happen on a plane level but you don't have to set that plane manually unity does have API is a set that plane manually but this is a much easier way to deal with that and if you're developing on hololens there's a couple considerations you want to give to performance and I'd recommend you studying the in the unity quality settings setting the fastest quality settings for the Holland's so this will maximize performance and reduce power consumption particularly you want to avoid things like soft shadows and using shadow cascades they require a lot of resources from the hardware and they're not really necessary for the type of rendering one would do on a hololens so I talked about remoting earlier but there's an additional I mentioned this briefly so inside the unity editor in the holographic emulation window you can change the type of emulation from remoting to something called simulation this is basically a built-in Hulland simulator in the editor so this will give you access to everything that you would normally have on the hololens the way you do it is you enable it inside the editor and you can drive a virtual human human around with a which I don't have a controller but I knew I liked an Xbox controller so you can drive the human around and you can simulate the hand positions using the controller so you can completely create your whole lens application without having a physical hololens obviously you want to develop at some point on that before you ship a whole lens application but this is a tremendous aid for iteration and you can even load virtual room information using this so you can specify a couple different room environments so you'll have access to spatial data as well and if you're developing desktop mix reality applications you can also do this without physical Hardware there's a simulator built right into the mixed reality portal so if you open up the mixed reality portal on the left side there's a little developer button and you can simulate ahead simulate the headset and simulate controllers that way and you can drive the virtual human around inside the mixed reality portal using the keyboard and mouse so it's waz D keys on the mouse for steering and also I mentioned the interaction manager for accessing input and there's a little tip I want to mention so if you're reading mixed reality input using interaction manager particularly on hololens there's one of the source parameters is something called handedness so you can tell whether it's a left or right hand so if you're working on hololens that will always report unknown this is just the windows api doesn't give us the information for hands if you're working with these controllers you will actually get left and right because these can distinguish but if you're doing an application it might work on either I would recommend not using the hand information to correlate events so if you get a pressed and released event don't correlate those by the hand information each of these events will actually have an ID associated with it that ID is guaranteed not to change in the latest version of unity at unity if you're using a hololens will actually synthesize hand for information based on where we detect the hand but that the reasoning based on reasoning to figure out which hand is visible can actually change as we update our knowledge about the state of things so if you see this hand we go or if you started over here and we go we think that's the left hand then we do this we go oh that's the left hand so this is actually the right hand so the reasoning can change so I always use the source IDs to correlate events so I you've heard me talk about like twenty seventeen point two and twenty eighteen I want to explain a little bit about how unity releases updates so earlier this year we've sort of changed the way we do updates we used to have a very traditional version models and users would pay for each version and last year we switched to a subscription model and we've changed our release model a little bit and I think in a way that will better service developers who want to have either a the latest features or be a very reliable robust version of unity so we have two development streams what we call the the text stream and the long-term support stream so right now we're in the twenty 2018 text stream this is where we introduced new features last year's version 2017 is what we're calling our long-term support stream so we do three three iterations per year a point one to point two and a point three and then the last one becomes what we call our long-term support stream and this is a version or branch of unity where the only changes we make to it are bug fixes we don't introduce any new features which would destabilize it so you can either have the very robust version that doesn't have new features or new features so you're probably wondering well I don't know which one should I get and the answer is get them all because we introduced something called unity hub this is an awesome tool it's a front-end for unity and you can actually choose which versions of unity to install and you can sort of experiment with tech print tech release features and then go back to working on the LTS stream so this is kind of cool I just tested this out last week and I had to download three different versions of unity so click download download download go and get a cup of coffee and I come back and they're all set up on my machine so so that's really awesome and this will basically let you see when new versions are available and you can even choose to launch a project with a particular version from the hub yes yep the question was this is is this available and it's available now and I'll have a link later on to how to get this the other thing we're doing this year into our text stream is we're introducing a package model for how we release features to the text dream so in the past it's one of the problems we struggled with a unity was how to get new features available to developers as quickly as possible without destabilize destabilizing the product but also in a way that coordinates with everything else and because we're very concerned about testing and making sure everything works well together our lead time for getting features available to people can be slower than we want so what we're doing this year is we're introducing packages so this does a couple things first of all the core install of unity will become slightly smaller so there would be some features of unity that are now available with packages the way the package works is you just enable the feature that you need in your application so additional API is will become available it'll download the package automatically into the editor and we're actually going to be moving a lot of unity features into this package model coming later this year we're going to be taking our support for XR and making this a package what this does for us is this will let us develop some of our XR features independent of the regular release cadence of unity so we can get features new features out for testing much faster to users and also do fixes much faster and we're actually going to be doing a internally a new API that we're gonna allow us to support a much broader range of devices using this package model so this is my call to action slide if you're not already just check out unity and and here's what you need to get started so you need Visual Studio 2017 so download the Unity hub there's the link right there it's unity3d get unity download and there's a big link for the hub it'll make installing unity easy and check out the windows mixed reality site there's another link there which will give you a landing page which will tell you all about unity mix reality and if you want to learn about pixies go visit our booth in the expo hall at e 21 some people there from pixies and unity that would love to talk to you about how to get CAD integration in unity and get one of these headsets they're awesome they're amazing and yeah I can't tell you how how awesome it is to work with these devices that's so cool and finally give us feedback if there's anything you want to see us do or you think something we're not doing well let us know particularly as I said we're very concerned about making il-2 cpp perfect for you and a lot of you are very committed dot net developers and maybe find the idea of like this other back-end really scary but we want to get it right and we want to make it excellent so we're very committed to supporting that so at this point I have time for a couple of questions so I think the recording so I recommend go using one of the microphones I my first question is uh the il-2 CTP back in is there like a specific benefit of using it for a more development yes so the the biggest development is simply a consistent API service across multiple platforms now if you say I don't care about any other platforms why would I want to use il-2 cpp we actually think it will be actually more robust as well just because it's less code for us to maintain there's also some performance gains because you're not using just-in-time compilation but I mean those performance gains are kind of kind of minimal and there's a speculation that it could make your applications a little bit more secure but that's already you're already taking care of security through the apk so and another question I had was regarding like if you if you use like UI canvas how does that like work with the headset oh right so the question is how does the UI canvas so one of the one of the components is unity is something called the UI canvas and this is a component allows you to do UI composition this does work with the headset normally if you're creating a what we call a flat desktop app you have a way of setting up this canvas to work in screen space mode well there's no screen space mode with a br AR device so what you end up doing is there's an option on this where you created world space canvas and you basically position that canvas somewhere in your scene in 3d space now could track with the camera if you want there's a bit of nuance to making that look right you don't want something rigidly attached to the headset because that actually is not a good experience for the user but you can have it a float and then kind of slowly move to follow the user but you want to move set that canvas up so it's in 3d space so all your UI is there so it does work and that's how we recommend using that UI canvas with VR AR apps so is it easy to like have it so that you know you still have that screen the locked-in view for a traditional game and at the same time share that code for a VR game so I think your question is like like how would you deal with a game that is both VR and then on are you could do both I think there's a lot more than simply changing the UI like as I mentioned a beginning talk it's very easy to enable VR but there's a lot of user interface considerations for making a good VR app and it's more than just simply changing your UI from screen space into world space there are a lot of other considerations my last question is um regard to input at least with mixed reality it's like you get that pointer that comes out from the Hat from this like is that all built-in or is that something up to calculate so the it's not implicitly built-in there if you go to the Microsoft site the Microsoft has something called the mixed reality toolkit there is a way of getting the geometry and mesh data for these controllers into something called a gltf file that is loaded dynamically and there's another deal you all you need there's examples of this on the Microsoft site of how to do that so yeah it's actually pretty easy to access and the there examples will actually animate the the thumbstick so if you the user sees is some move is the thumbstick this model will actually animate and so there's other things you can do like you can highlight things if you're doing a tutorial to tell the user like you know press this move the joystick this way to spin the car model or something so that's easy to do yeah I mean my question was exactly like if you're not like touching something directly but like pointing to it in the distance is that built into that interaction framework so oh the interaction manager so you what you do is the in the in this controller there's two bits of posed information there's what we call the grip pose and the pointer pose the grip pose is sort of going to be the position at this pointer or this controller and its orientation in your hand and the pointer pose will depend somewhat in these controllers which are almost all the same but if you look at this this way you see that this has this angle and this has this angle so the pointer pose will have a different orientation and that gives you the ability to construct array of to see what the users pointing at thank you so much yeah quick question about the remoting what is exactly is happening from the render machine to can you talk a little bit about what's happening from the render machine back to the hololens in particular like you know the calculations are being done and the events are being passed all of that but what's happening on the right turn so right there's a there's a network stream set up between the two devices and from the desktop machine we're basically transmitting frame information which is the rendered scenes as well as the some additional information meta information about those frames about like where they are in space and what time for what the time sample was that they were rendered on back from the device we get information from another stream coming back and we'll get surfaced data coming in from the device so we wearing a hull lens you can see the triangle mesh data from the room you'll also get hand and hand information and head pose information so all your input stuff is there as I said the thing we don't support right now is voice we're working on making that work but so you can actually have all your input working from a ho lens application except voice right right so there's always as you mentioned there's latency but we we actually account for that latency so there's latency in the time that we render the frame to when it gets transmitted over the network to when it actually gets composited on the device reprojection actually takes care of that latency so the device can actually go oh I know the time frame that this this frame was rendered for like this particular time slot and so this is where the headset now this is where it was and it can correct the image and that'll work even like if for some reason your application performs very badly and you only get 10 frames per second the cool thing about reprojection is reprojection happens at device refresh rate so on the hololens at 60 frames per second where it does its own refresh so your image is stable regardless of what your application you know animation will not you know if you you're running animation you're only going to get 10 frames per second of animation if that's what your app is running but the reprojection will happen at 60 frames per second so your images will be very solid and stable yeah hi I have two well I have a small remoting question and it may be a bigger remoting question the small one being is there a plan from you or probably more specifically Microsoft to be able to enable remoting not over Wi-Fi but over a wired IP over USB connection yes absolutely and it does work and that was part of my demo today but I had a little problem getting it set up because I updated my homelands - there's a new pre-release OS for it so you can get RS for redstone for for the hololens and so every part of my demo worked except when I tried that over the USB here wasn't working so there's a few little finicky bits of it but yes you can actually do it over wired connection so obviously it requires a tether on the hololens um there's two advantages one is you get the better bandwidth over the USB connection and the second is you can supply power at the same point so if you really wanted to do some custom thing for people where they were wearing hololens running this application for an extended period of power you could actually take advantage of those but that's only in the preview OS for them no no no this actually works with rs1 so oh okay yeah so it's yeah okay so while working all working today and been working okay yeah eight-zero yeah but that work it okay next is so you said in 2018 you can build a standalone app that can support their moaning unity can help build a list and exactly how do you do that um let me let's see if I can go back in my slide I think I called it out almost there there it is right there so it's basically on the build settings and this is in 2018 point to right now so you can try this out cool thank you sure my question is very related I'm just wondering if I can build that standalone app for like iPhone or all of the Unity platforms or does it have to be like desktop uwp I kind of make my phone be the rendering engine for holographic remote is your question in regard to remoting understand so remoting right now this type of runing is only available for Windows mixed reality the hololens there are some other ways that we support remoting but they're intended on other devices like Android there's an Android remoting player but I don't know how lot about that but well I wanted hololens to I want to wear a holo lens but render on my phone render on what on my phone No so the rendering has to so the base app has to be au wpm okay thank you hello thank you for the good talk I have a question about UI canvases well I'm working on a project that is doing data visualization and charts so basically for some data it makes sense to be 2d and we're using UI canvases is there any performance penalty in terms of how many UI canvases can you have on the scene in world position so your question is about are there performance considerations to having multiply for example say five canvases 10 canvases are they are all these campuses present at once or you enable and disable them it depends it they they can be present at once within the scene so one of the one of the in order to get UI performance to improve UI performance the UI system does what we call batching so it looks at all the things you're rendering and determines whether because the UI actually has its own shader and determines which kind of things it can render as a single pass and the UI campus actually supports full three dimensionality so objects can be in front and back and layered and if they're layered it has to kind of keep them in a certain order and so if the if none of the UI elements changes in a between two frames then it'll actually save all the bashing information and it can render that UI that much faster the idea is to reduce the number of draw calls used for UI if you've got multiple canvases they can't batch information between them because it doesn't know what their spatial relationship is so the one thing I can think of is that does inhibit the ability to do batching but if you need to have the canvases of different locations and you're seeing there's really that's your requirement so you can't it doesn't make sense to combine them yeah I don't think I don't think there should be any significant performance limitations of having multiple canvases one of the other thing that's I think I can talk about this we worry we don't have support for this yet but in the future sometime probably later this year we're gonna be adding support for there's a compositor that happens at the end when the reprojection kicks in and we can actually one of them but one of the common things people want to do in VR is have their their UI rendered at a different resolution than the rest of the scene this is a performance consideration maybe your UI is going to be much higher resolution and so there are ways to have the compositor deal with the UI in a separate plane and we're gonna be adding support for this this will probably part of this new sdk rewrite we're doing internally but the idea is that I think that'll also improve performance because it allowed the UI to be composited into the scene of a different it's a little bit more efficient and you can get higher resolution but to answer your kind of giving you some additional information but they answer your question I don't think there should be any performance overhead of multiple canvases other than whatever limitations it it encouraged for batching okay thank you very much this may be a little bit more of a general UI canvas question but would be applicable here as well I am I used to work on the UI team so I'm set up for this there's been a lot of talk here at build about the the Microsoft 365 and being able to richly embed content from across the graph and such is there going to be a good way to like include one of the customs animal control as a component in the the UI canvas I don't know of any plans to do that for customs animal control's okay all right well thank you very much 