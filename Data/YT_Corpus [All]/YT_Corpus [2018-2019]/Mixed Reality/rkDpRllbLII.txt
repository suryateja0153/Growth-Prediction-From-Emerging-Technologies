 Windows Mixed Reality Motion controllers represent the position, orientation, and motion of a user’s hand. This information is determined by combining data from the sensors in the headset and the controllers. All of this information is available to the developer in one coordinate system, common to both the headset and the controllers. If the user moves the controllers out of the camera’s view, then Windows will continue to track its position by combining the signals from the controller’s orientation sensors with previous visual observations from the headset cameras. This allows users to throw an object, grab something behind their back or reach for a new item over their shoulders. When Windows loses visual tracking on the controller for longer periods of time, it will body lock the controller, moving it in unison with the user. Developers can still received data from the controllers orientation and velocity sensors. The triggers, buttons, touchpad and haptic rumble all remain fully functional.  Developers can use any of this controller data to make their app more robust when the controller is out of view, allowing users to continue interacting with your virtual world. With a mix of input from the headset and controller sensors, along with thoughtful design choices, you can create engaging and interactive Mixed Reality apps that allow for full movement with the Mixed Reality Controllers. 