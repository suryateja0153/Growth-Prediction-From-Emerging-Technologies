 [Music] welcome so today we are going to do an interesting topic so you have been learning a lot about deep neural networks very deep neural networks and like real recent variants so we have covered down model switch world as recent as event 2016 and 17 coming down from the recent CV Pierce now one thing which comes into mind is that all of these models which we were learning down there were champions of solving a very pertinent problem which is called as the imagenet challenge okay great so so you had the standard sized images of two to four cross two to four and there were 1000 categories of objects which were present in these images and the whole objective over there was to train down object train down the network which can actually very effectively with a very high accuracy classified on these multiple categories of objects so that was going on pretty good now the challenge which comes out is that you have a network which has been designed it has been trained you have your weights available but then till now we haven't made use of any of these bits so when we were downloading all the models we were downloading just bear models which read from your model Zoo or all from your thought vision models library you are downloading only the architecture we never downloaded these weights over there directly it was just the architecture definition which was coming down on the other side we did look into the weight space complexity computations and what is the total time complexity computation as well and one thing which you would remember is that if you are looking down into weight space over there and we try to download a model with the weights then the total download size is also larger so that was because the weights have to be downloaded and kept down onto your space and the moment you do it your RAM also explodes out quite a good bit now what evidently comes to your mind is that say we are trying to solve the same kind of an image net problem or yell object detection problems when we were doing it done with smaller size images from C fired just bloating it out to four cross two to four we realized that the accuracy which we are going down going up to was not that what is compared to a image net problem we're still start down at about belly eighty percent yes we are doing with lesser number of sample images we are also doing it for lesser number of classes but then is there a way that I can import all the weights which are present down over there for a model which has been trained for imagenet problem which were still natural images and was trained for possibly more number of classes so the granularity of training was really high so can I actually bring it up and then train a model for lesson number of classes and and can I make it faster so that's the whole purpose of something which is called as transfer learning so you have a model which was trained to solve one task in one domain and then I'm bringing it down to solve another task in another domain and under what conditions can I actually do that so this is what we will be doing in today's lecture which is called as domain adaptation and then domain adaptation in deep neural networks is very specifically so the whole purpose of transfer learning is to do it via domain adaptation that's what I would while this itself is a very big field but we are going to touch upon smaller aspects over there as far as what we do from a very practical point of view okay so if you look into what the situation today is is that you have a real big extensive library of these deep neural networks available you have your different sources of data so they come down from geo x from Amazon Mechanical Turk sand that's how image net itself was created you also have your e-commerce data available and coming down and then on the other side of it you have your really great compute systems you have your GPUs you have your grids and HP C's and everything present down and then you have very standard data sets which you could use for creating your networks over here now once we had all of this we did do a lot of interesting work so like we got done self-driving cars we got down photo tagging on Facebook identification of objects we got down Prisma for generating artistic equivalents of these images we also got down say these mill postal sorting systems as in one of this major examples of Linnet a very practical example of unit now where we are actually missing at this point of time is so these were people who solved very specific problems they really curated and got down a large amount of data can I use the same sort of a model with lesser amount of data so can I use a model which these guys have trained separately for their purpose using a of data which is available to them can I use that in order to solve my problem where I have lesser amount of data available now this is what we are looking at today okay so let's look into the organization how it goes down is that you have so initially I will describe you what a domain is and what what our task or a problem is at hand then we get down into certain origin and notations we go into something called as the classical approaches for domain adaptation and then get into certain application scenarios and then come down to an end node so generative models with a diversity autoencoder disease which I will be covering down later on point of time with a very extensive lecture so there is one one whole series of few lectures which we have in the slick next week where we are going to work down on generative models itself and how these neural networks can work as generators so it can you give a class label and generate an image out of it so that's roughly speaking of any shorter or very sweet and simple definition of a generative model as it goes down okay now let's look into the challenge now I would take out this very specific challenge at hand and this is a problem which we have already solved off which was the vessel segmentation problem in drive data set so you had your retinal images present over there and then the whole object was that can I segment out these bases using some sort of a deep neural network and what we had used over there was a fully connected DubLi network in order to do that okay now so if we train the model with that and then we give an input to it which was from the same data set we got done a pretty good output coming on now the challenge is that drive is a big data set and we use it with trained our models now say that I get down images from a local hospital which has an ophthalmoscope okay now they might not be using the same sort of a retinal camera and ophthalmoscope which has been used for creating drive then these people were from the European origin whose images were taken down in Indian origin we have a different kind of an image coming up so now how will they do so this is a very pertinent product development problem which we have so what we try to do is these were all healthy images people were healthy they they were not supposed to be having any diseases and that's how they were taken now now on the other side of it you have this state dataset which is from diabetic patients now the moment you have this one and you try to feed it through this network this is the kind of an output which it generates now quite clearly you can look into it that this is a horrible river this is this is really bad because what comes down is that majority of this background region over there where you don't have any vessel it shows them as white so that technically would mean down that all your vessels have ruptured and you have blood flowing down everywhere now that's not the case one of the reasons is that that definitely has been a shift in the nature of the data and because of the shift or the change in the nature of the data is why you are getting down this really bad result now this this this makes it a very pertinent case for domain adaptation for us so let's look into demystifying what actually went wrong over there now what came out is that once you have your source domain so so domain is this drive data set over there and what we are plotting down is the histogram of the pixels which represent these vascular region or the vessels over here now in my drive data set this solid red line is what is the histogram of the pixels underneath the blood vessels okay now this dotted lines which I have over here is what is the histogram for my target domain which is for the state data set so this is what we call as target domain which is where we are going to employ our model once it has been trained on this one which is called as the source domain out from my drive data set okay so that's the definition of source and target and it's it's universally accepted unanimously across the field of domain adaptation now when I look into the histogram of the red Channel it's it's more or less overlapping there is some part of a non-overlapping area when you look into the green Channel yes there are major shifts but not so significant over there whereas when I look into my blue Channel what I would get down is my blue channel is completely offset and drift off and this is one of the reasons why when I was working down on the same one so while here it was having lower intensities of blue here it's having on my target domain higher intensities of blue and this is one of the reasons why it just drifted out and then went down somewhere over here in my background regions in order to that they are the ones so this is a very intuitive and simple explanation but that's not always the case so there can be different kind of ships which are tractable which may not immediately be defined so there are multiple ways of actually coming to a solution to this one so how it gets defined is that the original data on which you are going to train this model or on what it has been trained on over here for us it is dry that's the data from that one is what is called as the source domain okay now you have a classifier which you train it down and this classifier is a deep neural network for us and this is what is a classifier trained on the source domain now if I'm working on that one I get down a good result but the moment I get down from a different one which is on my target domain I get a very bad result coming down now the whole idea for domain adaptation is that you take at least one example they can be more than one example from the source domain and sorry from the target domain and feed it to this source domain and try to modify the weights of this source domain now once you have modified this whole domain weights so you have weights which are initialized by training it over here then I am going to retrain so do a feedback feed-forward and then you find out your error you do a back propagation feed-forward error back propagation so these weights are going to update now you have a model which has been adapted in that way and this is called as a model which has is adopted to the target domain now if you put down a example over there you would see that the results are quite intriguingly similar over there okay so this is what comes out okay well and good let's look into some of these rotations common notations for this problem so the data the input data is typically what is called as XS so X has been for my data and Y has been for my class level and the subscript s is just for my source to denote that is from my source okay now what I have if I try to look into a joint probability space then this is what our joint probability space will be looking like so say I have two different features and then these pluses are what belong to my blood vessel region and the red circles are what belong to my background region over there so I have red circles and a blue plus s over here now if that's the case then together if I plot it down then this is how it would look down if I am just looking at two different features so this is a hypothetical feature space on which I am looking now any kind of a classifier is what is this margin which is going to segregate between these two glasses okay great now on my target domain when I get it down what I would be having is a feature space which has a very different topography as such and obviously there are less number of pixels available because you have a lesser number of samples available in your target domain so what we said from the definition is that your target domain may have as less than just one single sample present over there now my whole problem is that using this lesser number of samples I need to draw this sort of a line over here now obviously if I take this classifier line over here and trying to place it over here so that will be a line which comes down along this way now the moment you have that kind of a line coming down you see that majority of these will be wrongly classified and that's wrong so the whole purpose of domain adaptation is actually to somehow bend this classifier line and bring it over here so this is one of the ways in which you are going to modify the classifier obviously there is another way in which what you can do is you can modify this data points and do some sort of an affine transformation and place it back onto this domain so that's also valid way but then that's not something which works around with the classifier and we are not going to touch up on that one okay so we look into a particular field which is called as visual domain adaptation now this has been there on the field for quite long time and there are multiple approaches of doing it out so one of these ways is what is called as a feature augmentation so the whole idea is that you try to get some sort of a transformation of this input feature space onto a higher dimensional feature space such that from your source domain to your higher dimensional feature space when you are coming down so you are bringing down your data points as well as your linear classifier or them or the classification margin over there to a higher dimensional space you do the same thing for your target domain as well such that these two classifier margins merge over there in some way and then it makes it easier so you train a classifier with this data over here do this transformation and you get your classifier transformed over here for a target domain data which comes down over there you have another transformation which is defined now this get down your different data so this green crosses and orange diamonds which are over here so they will fall down in proper way and you have a very high classification so for details you can definitely read down through these papers which are present over here so this is one of this early and very simple approaches for it the next one is what is called as a feature transformation and the whole idea is that you keep sort keep on mixing some amount of unlabeled data from your target domain onto your source domain such that you can have some sort of a gradual transition between these two domains now at a point of time it was a very popular method but today it's it's no more used except for problems where you have real data scarcity and it makes it complicated to transform whole models and try to learn it out over there now one of my personal favorites and what a lot of people actually use today is what's called as a dictionary learning approach so the whole idea over here was that say I had let's let's go down into very simple one so say my source domain had side view images of human faces and they had clipart images of bags or or some objects over there and my whole purpose was to segregate human faces from bags okay now in my target domain what I get is I get front view images of human faces and I get camera images of backs or from my phone camera I've just taken out and I want to segregate it off now it is quite rational that for this for this network which was trained on the source domain since the nature of the data was probably different it's it's not so easy for it to adapt although we as humans have known to adapt it but we have years of training in order to adopt it now what will you do over there so what typically is done is that you need to have the source domain data available to you and the target domain data and you would start creating a common licken dictionary what that would mean is that you put all of these side view faces front view faces and everything so you train a network which took only side view faces and this clipart images and this is trained and capable you also have your source domain data now you create a newer class of faces where your side view faces as well as your front view faces you create a newer class a set of newer class over there for bags where you have clipit representation of bags as well as you have camera images of bags now you create this car dictionary common discriminative dictionary and then make it discriminate between these two glasses so that's what comes on in a dictionary learning kind of a method okay now there's also another method called as domain resampling in which what you do is that if you have multiple features over there then you create subspaces of features and then try to resample out from these subspaces of features mix them with some different affine transformations and come down to a common form of higher dimensional space between your source domain and your target domain so that's one part of it which is about mixing down data from different domain then then trying to update your model based on it now we'll get down into few application scenarios of where it comes down so one of it is in face recognition say that I have faces where the challenge is that I can have faces under different poses so they can be side view there can be front you can I recognize really these ones then I can have people with headgear with uncontrolled background and all of that I can have say one of these views of my face given a straight view but then under different emotional expressions over the can eyes because all of these people are still the same people who are over here and then I can have sketches now can we match down from sketch to a real image that's a real target problem over here so that would be of immense use and in forensic sciences or in crime investigations as well now these are practical challenges associated with one of these problems in face recognition itself then we have this standard benchmark datasets on webcam DSLR object recognition where the object is that you have these very high quality product level images taken down from DSLR as you get down on your flip card or Amazon snapdeal any of these places then you take a camera camera phone image you just snapped it out from one of the bags which was lying somewhere you like somebody's clothes now can I match it exactly and find it up so this is another challenge which the community is facing today now where I would come down is one of our earlier works which we call down as to man adaptation within stacked autoencoders and the whole idea over here was the examples which I were showing down so if you have your drive data set on which you have trained it out and then on your state data set how will it work on so as one extra example which do get pointed out and not many people do refer to it is a very pertinent question at this point so I have my drive dataset on which I have trained it out of the crate and I'm saying that I want to modify that model which has already been trained on drive onto my steer data now see if I don't do that what will happen so if I say that okay I have the whole state dataset why not train a whole model which state it does attend just work it out so that's what we had also tried doing over here so what we did is that say if you have your model which was trained down over here which is called as H source and then you directly plugged in one of these test samples onto it and this is what you get down really bad result over the exactly opposite in some of your sense know what they're now on the other side of it what I try to do is that I try to train a model you exclusively using only limited number of samples present instead at us and then that's really small and if you look into the number of examples available where you're vessels are Mac that as low as 4 just for images are present now this is what the performance will come down which is also pretty shabby now on the other side of it if I take a model which has been trained on Drive and then I just modify it over here I get down my newer data set which is on my target domain and this is where it comes down so you can see effectively that because it had learnt very nicely to discriminate out vessels and everything I just needed a bit of modification to match down exactly to the newer domain over here and you get done pretty fine polished out results over there so that's why domain adaptation is actually used in a big way in the community okay now what we had done over then was using a concept called as systematic dropouts now on one side while you have looked into these aspects of dropouts or randomized reports to avoid overfitting the whole idea over here was to work down on something called as a systematic drop-off and the purpose over here is that some of these neurons so say you have your inputs given down over here and you have your intermediate hidden layer outputs being created now some of these neurons will be something which will be aid in producing out your output correctly or they are the ones which will be positively reflective of features some of these neurons don't care anyway in whatever changes happened down there they are just garbage neutrons but there will be some neurons which will negatively impact your classification over there the whole purpose over here is to stop down so this is a switching layer it's just a zero one zero one kind of a switching layer which finds out those neurons which negatively impact your classification and just tops it out and on the backpropagation side it will try to update the weights only of these selective neurons so that's what we have done down in this particular kind of a model for adapting where DPL networks for from a source domain to a target domain so with that what comes out is interesting over here if you look and the final part which is just for your classification losses now this blue curve is the kind of a curve which you get down on the source domain when the classifier is trained now if you look down into your green curve this is where is the performance of your target domain classifier which is where I just use for images from the state data set in order to train the classifier to find out the blood vessels over there now the final accuracy the saturation actually saturation error is somewhere around 0.7 close to 0.7 that's almost this starting accuracy for my drive dataset where I had a lot of data so that's that's obviously you have more more data more labeled samples of itslearning or very good but then what you would be interested to look at is that instead of training this one with limited number of samples if I take a model which was trained on Drive and then I modify it first here then might starting error over there is itself low that's much lower than the starting edit for directly starting over there and then this final error is somewhere which is located very close to my Drive data set so this is what you saw that if you are trying to modify your network which has been trained with large amount of data more number of images on a source domain and then just used for a target domain then you can actually come down to much lower errors over there and this is quite good because now this would reduce for most of your actual engineering problems you can actually make it faster within very few number of epochs you can modify your network you don't need to keep on running for 100 C box 284 even thousand epochs you can now just modify your network within four extra five epochs to any books or maybe maximum till hundred epochs and still get down a very good performance coming down so this was one of these examples with retinal images another example is working at our own digital pathology this where the whole idea which we had put down was to take a network which was trained on standard image net kind of problems and once you have those networks what you need to do is so there was obviously a voting and a multi-view rule applied over there but then nonetheless these networks over there which were already pretend for just imaginary problem which is natural image classification and what I'm showing over here are medical grade images these extra biology images it takes pathologist years of experience to in order to actually come down to understand patterns very visible and pertinent patterns of diseases on these slides of tissue samples over there now we could modify very easily just within less than ten ebox to make a network workout to actually be a pathologist I equivalent so that's a real real case of using it dumb so I can refer you down to this particular people from HB 2017 where we had details of it not really given down on how to modify it down for your work and purpose ok so if you are more of interested in looking down into other issues of domain adaptation how to solve it out this is a whole list since the slides will be available so I'm not just going to stick down onto this one you can have your look at it at a later on point of time stay tuned in the next subsequent ones we will be actually looking into adapting a classifier for our purpose we will download a whole train model over there and then see how domain adaptation actually comes down to and convergence and compare and contrast it with a standard model where you are not using your domain adaptation to Train so these are interesting points which will be doing on the lab sessions so still then stay tuned and thanks 