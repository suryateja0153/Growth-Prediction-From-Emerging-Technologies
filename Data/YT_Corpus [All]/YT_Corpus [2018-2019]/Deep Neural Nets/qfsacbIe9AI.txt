 [Music] so in this module we'll look at dropout now so the intuition that we have developed in the previous module which was about on symbol methods is what that is that on symbol makes sense in most cases because you don't expect the errors of these K models that you're using to be perfectly correlated and we saw that whenever they are not perfectly correlated you are going to get some advantage okay now how do you do this in the context of neural networks so remember what was bagging multiple instances of the same network trained on different subsets of the data what is the problem with this in the context of neural networks each of these neural networks has very complex training each of these is going to take time and they're going to train k of them is that fine right so you decide okay sorry okay so one option that you have is you train several different neural networks having different architectures right but this is going to be expensive because I have to Train K of them the other option that you have is you train the same network but on different subsets of the data this is also going to be expensive so whatever on sampling techniques you can think if in the pink off in the context of neural networks which are essentially these two techniques different architectures and take a non symbol or tray in the same architecture on different subsets of the data both of them are going to be expensive right so now how do you go about it and it's not just training time expensive it even if we manage to train it at test time again when you're given a test instance you have to pass it through all of these complex noodle networks each of which is going to take some computation and then take the on symbol of the outputs right so even at test time it's expensive it's not just that that training time it's explained okay so now dropout is a technique which addresses both these issues which is use trained time computation as well as test time computation okay so it effectively allows training several neural network artifical are architectures without any significant computational overhead so we will see how that works and it just not training time as I said it also allows us to do this quickly at test time okay so again let's see so again here okay I'll get to it when I know so drop out actually refers to dropping out units from the neural network okay so this is my original neural network and I'm just talking about one neural network forget about on symbols just one neural network is what I have now what Robert says is you drop out some units from this neural network that means two up out some neurons and when I drop out some neurons I am also going to drop out the incoming and the outgoing edges otherwise where are they headed right so I am just dropping out so basically what is effectively happening here I am getting a new network architecture wait at least that is clear that is what dropout effectively does but I have already made a case that I don't want so many architectures that because it's a headache to train all of them and again at test time I have to pass it through all of them right so I need to still fill that gap but drop out says that drop some units and you'll get a new architecture but how does that simplify life we will see that and now each node is actually retained with a fixed probability for the hidden nodes and even further input nodes so then we were not wrong in actually dropping out the visible node because you can do dropout at the visible nodes also okay anyways yeah so for the hidden units you would drop them with a probability 50% and the input units you will drop them with a probability of 20% typically it again is some hyper parameter that you will have to tune but typically this is what you do and I hope you see that dropping nodes from the hidden unit from the input unit is same as corrupting the input data right it's same as adding noise to the input did that fine okay so this is the idea now let's see how to actually implement this idea okay so suppose a neural network has n nodes using the dropper to idea each node can be retained or dropped an example in the above case I have dropped some Phi notes to get a thin network so if there are n nodes what are the total number of thin networks that I can get from it and so that means I can get to raise to n different neural networks am I happy about this or sad about this sad there's just too many neural networks how can I train them actually right so how do I do this I'm just creating a lot of suspense without giving you the answer okay so first trick is share the weights across all these networks okay we will see what that means and the second trick is sample a different network for each training instance okay none of which is clear at this point I can see I can read your faces I'm good at it problem okay so let us see how to do that so we initialize all the parameters of the network randomly or whatever may be used and start training when I start training I'll pick up the first training instance or the mini batch or whatever I am doing we apply dropout resulting in this network okay what will I do and they forward prop Carlo forward propagation right okay okay now we compute the loss and back propagate how some weights are missing rate how do I do back propagation now I've deliberately dropped up some of these connections they did not participate in the forward propagation this back propagate which are the parameters will update now only the ones which actually participated right so I'll just do back propagation just look at the red arrows I'll just do it over the paths which are actually present in my network fair enough right that's what you meant by normally okay that's normal okay okay so I'll just do it over the and weights which actually participated that's fair enough that's the only thing you could obviously do okay now I take the second instance again I apply dropout and quite naturally I'll get a different thinned network as you see the figure three in this slide okay what would I do now forward propagation then compute the loss name back propagate to compute the loss okay and then back propagate again back propagate only to the active nodes so these are the nodes which will get activated okay okay so what is happening here is now trying to relate it to what we were doing in bagging right where we are trying to train these different different networks on different subsets of the training data right do you see something similar happening here there are many such thin networks each time I am sampling a different network and updating it right so it's equivalent to training these large number of networks on different subsets of the data right but then the problem is that some of these networks may never even get sampled there are 2 raise to n of those my amount of data is definitely to be less than tourists to him so some of these networks might just not even get sampled then what is happening or they would get sampled very rarely right for example what's the probability that again I'll end up with the same network we are computing it okay good it's very less okay I'm fine with that at 7:30 right so it's very less right so it's quite likely that this network will never be sampled again that means for that network the parameters are getting updated very few times am i fine with it yes I am why because the same wage will get updated for a different network I am just using the same weight matrix throughout remember that my W matrix or w1 w2 is the same throughout it's just that at different depth subsets different instances I am just tutting some portions of this w1 and I'm not touching the other portions of w1 okay so now what would happen so I have shown you two training instances right what would happen to the weights which were active for the first training instance as well as the second to second training instance they'll get updated twice and which are active only once only once right so over a period of time many of these weights are shared across all these networks that I am sampling right so even though a particular network is sampled only a few times its weights will get updated many times why are these other networks which are similar to it do you get that how many if you get this okay good so what is happening I'll just repeat that I have this one weight matrix I am sampling a thinned-out Network which only uses some of these weights so for that training instance I'll update those weights now I know that the likelihood of the same network getting sampled again is very less but I don't care about it because I could sample a different network but I am sure that some of these weights will again repeat in that right and in that I told they'll get updated so even though each of these networks is seemingly getting very few updates overall all the weights shared by these networks are getting updated as much as they should be is it fine hadrian gets this idea okay fine and while I am also taking care that similar things like early stopping or weight regularization l2 regularization where I am not allowing a thing way to continuously grow or something like that because these weights will be off for some networks is that fine you see the connection between early stopping l2 regularization and this is that okay and so each thin network gets trained rarely or sometimes even never but I'm not worried about it because it's weights will get updated through some of these other thin networks so this is all finite training time at training time what is happening is this is one of these blue guys and reduce on with the probability P that means the weights going out of it who are available with a probability P there are other times they were not available now what do I do it testing I cannot okay I cannot take an on symbol of D ok the answer would have been that at test time instantiate all these 2 raise to n networks pass the training passed the test example through all of them and then take a non symbol right but of course that's probably expensive so what would I do at test time what's the simple trick that I'll do so he says that just use this network and just use the final mate matrix that you had no but then you have you're saying out of those two rays to enter sample some small number of those and do it okay I see the drop out uses something very simple than this what it says is that each of my nodes was present only P fraction of the times in the training data okay that means one way of looking at it is that so imagine that you could think of this as the analogy is that all these nodes are participating in a discussion right where they're trying to see how to do this job properly but with probability P they all sleep off right so at the end of the meeting you will trust each of them only with probability P so that's the simple trick with dropout uses it says that just scale their weights by P because that is how much I trust this node it only participated in P faction of the decisions so that's the confidence that I have in it so if it's saying that with W and wait do this I'll only do it with P into w1 wait does that make sense okay there is again a squared egg with vacuum kind of explanation for this okay which was there in the quiz last year which is very convenient it does not really give you the true picture because okay you can derive some math and so that okay this is mathematically proper but that again works in very specific conditions but at least if you get the intuition that is finite what we are saying is that these nodes will leave an active a few number of times so I'll only trust them that much and I will just scale their weights by that factor so at test time I'll just pass my test instance through one network which is the full network with the weights scaled according to the rule which I just said that's exactly what dropout does how many if you understand this and the final interpretation of dropout rate so what dropout actually does is apply some kind of masking noise to the hidden units right since the same as seeing that you're computing the hidden unit but then you're masking it off okay so what's the effect of this I'll give you the answer and I like I like you to think about it the answer is that it prevents the neurons from becoming lazy what do lazy people do they depend on others yeah actually they depend on others no so let me answer that give the answer for this and then tell me whether that's still contradict okay so let's see right consider this layer of neurons all of these are collectively responsible for what happens to this guy right now you see what I mean by neurons becoming lazy I could just see okay I will not give my input these other neurons will take care of it right they will adjust their weights so that they eventually it will fire or not fire or whatever right you see that could happen but now these neurons cannot rely on their neighbors because they don't know where then when their neighbors are going to ditch them right they'll suddenly drop off okay and now I was waiting for my neighbor to actually do something and he's not going to do it so I have to be alert always do you get the analogy right so these guys are collectively responsible for something and they know that some people in the collection are going to betray them so each of them has to be more careful so the more technical term for this is that does not allow the neurons to co-adapted wait so it does not allow them to get into this mutual agreement that you take care of certain things I will take care of certain things and together we will do the job right you do question one I'll do question - I'm okay it doesn't allow them to do this okay so let's just concretize that intuition a bit for so essentially a hidden unit cannot rely too much on other units as they may get dropped out at any time each hidden neuron has to learn to be more robust right it has to do the job as if it's the only guy responsible for the job okay and let's consider one of these neurons a chai and let's see that a chai learns to detect faces sorry it learns to detect an O so I'm trying to do phase detection whether an image is about a face or not and a chai is the feature which fires if there is a face somewhere if there's a nose somewhere in the image is that fine okay now if all these guys start acting lazily okay this guy is going to detect a nose that means definitely face will be there so I don't need to do anything hey what would happen now suddenly this guy is going to go away dropped out then these other guys need to do one of two things either add redundancy that means one of them should also take responsibility for detecting a nose or do it in a different way take responsibility for detecting the lips or the eyes or some other part do you get that right because you know that I cannot cope with my other neurons I cannot say that okay in these front-facing faces you just detect the nose and we'll be done and we'll all keep quiet wait I don't know whether you will do your job properly so I will have to add more redundancy you detect a nose I will also detect a nose or you detect a nose and I will detect something else which helps detecting the feature right so that's why these networks become more and more robust as you add this robot okay okay so that's all that I had to say I still don't know whether I've answered your question or not all of them try to detect nose see as long as that helps reducing the final loss it's fine it's just the case that you would have some planing images where the nose is not visible maybe that person is drinking something right so for at least for those training instances someone else has to take care that you detect from the the other images said otherwise a loss would not be zero for that training instance so as long as you have some training instances see if all your training instances can be detected just by detecting the nose then there is nothing wrong in all of them trying to detect the nose so if the training gait is like that it will happen but the hope is the training data is not like that okay is that fine so we will end here [Music] [Music] 