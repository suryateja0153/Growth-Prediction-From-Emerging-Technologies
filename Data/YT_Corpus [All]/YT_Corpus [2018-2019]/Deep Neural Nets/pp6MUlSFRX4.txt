 Good afternoon everyone. Good afternoon to those who are in the room and to those who are watching us remotely. Today we have our final intern project of Anderson Avila who is PhD student in Institut National de la Recherche Scientifique.  Very good.  In Canada, under the supervision of Dr. Tiago Falk. Today he's going to talk about his intern project, Audio Quality Assessment with Deep Neural Networks. Without further ado, Anderson, you have the floor.  Thank you Ivan for this nice introduction. Thank you all of you for watching my presentation. This is the outline of my presentation. It starts with a little introduction on Audio Quality Assessment. Then we dedicate some time to explain our dataset which took a massive amount of time to develop, and it's a really nice dataset. Then we discuss our proposed approach for the plan of Audio Quality Assessment, and then the results in the conclusion. So, what is Audio Quality Assessment? The idea of the Audio Quality Assessment is to estimate the quality of the audio based not just on the signal attributes such as SNR, noise level, speech level or reverberation, but also considering the human factor, the human subjectiveness. So, at the end, we want a model that can give us a number that we will represent the quality. Why this is important. So, many multimedia services network systems, they need to be monitored, and we can not just look at the quality of the services without considering or taking into consideration the end user. What we could do was to have a feedback from the end user to monitor ourselves, but this is not always happening. So, an alternative is to have a model that can tell us how good is our service, also considering the perspective of the user. So, at the end, we don't want to predicting a voice call and avoid the user having problems of not hearing each other or not being able to synchronize the conversation. So, all these things can be monitored and improved if you have a feedback from the user. So, these are some of the possible existence of network impairments that can hinder a call or can compromise the quality of the audio. So, latency, jitter, packet loss, and on the signal side, you can have speech distortion, such as, additive noise, reverberation, clipping. So, all these kinds of artifacts can hinder communication based on audio. So, how we can actually assess the quality of the audio there. We have two alternatives. One of them, is to have people assessing the audio files. So, we can have a speech being recorded, then people can listen to it, and then each participant can give a score, then you get what's called MOS. So, MOS is just the mean or the average of all the scores, and this is important because it eliminates the subjectiveness of each individual. So, each person has their own reference of quality. It depends on many different aspects, for instance, the person is an expert, they might be more rigid. If the call or the audio or the content is important, it may also influence the score. So, with the MOS, we can eliminate these factors. So, subjective listening test is actually the most reliable one, but it has some drawbacks. So, it's time-consuming, it's laborious, it's expensive, and I think one of the things that is really complicated is that it cannot be done in real-time. So, if we have a model that can do this for you, you can actually evaluate your systems on the fly. So, another alternative to subjective listening test is to compute MOS using an algorithm. So, we replace the human by a computer based algorithm. We have two types of algorithms. Can be signal based, so the algorithm looks at the signal and extract some attribute operands of the signal to estimate the quality. We have a parametric base, which use network parameters to estimate the quality, and you can have a hybrid one. So, we can use the signal without reference to estimate the quality, or we can use a double ended one which use the reference. So, you have the degraded signal, the clean signal and then you compare the two. That's what PESQ does, we are going to talk about these metrics in a little bit. So, these are some of the mostly known metrics or measures for Audio Quality Assessment. We have PESQ, a new version which can be used for wide band signal, POLQA. These are all developed by the ITU-T organization, and you can see that, for instance, PESQ is more strained for codecs and compressions, not really for noise and reverberation, or the kind of environments that we can have in the signal. We actually have some limitations, therefore this is actually the motivation for our project. So, although PESQ is widely used, it has some drawbacks. It was not really trained for distortions introduced by the speech compression, but it is intrusive. So, it needs the clean speech or a reference speech to compute the output. So, our objective is to have a new algorithm that can be trained on noise reverberation, audio pipeline distortions, and also that is not interesting. So, these are the two main things that we're looking for. When you look at the literature review, where actually in this project, we're focusing more on deep learning approach to build our model. When you look at the literature review, we see that there is a lot of work on using deep learning for predicting mostly intelligibility, but not really for speech quality in general. Most of these approaches are based on a speech recognition, so they actually estimate the quality considering the performance of the speech recognition systems. There are two approaches that I thought was.  Most of the papers are from this year.  Yes.  Except one. So, it's a new thinking in the community.  Yes. Those are papers that are recent papers. They're focused on deep learning to solve this problem, but most of them focus on intelligibility. Intelligibility is actually just one factor or one aspect of quality. So, this paper I thought was interesting, although it's also focused on intelligibility. But, the idea here is to have a spectrogram of your speech signal, and then use a CNN and their kernels as a filter to estimate the MOS. It actually eliminates some of the engineering features that we know by using a CNN as a filter and as a kernel. This one was also a paper that I thought was interesting. It's an end-to-end non-intrusive speech quality assessment model, and it uses a spectrogram and a LSTM to estimate the quality. So, I thought it was interesting that they built actually their own cost function, they customized it, and to get the correlation between each samples or the sequence of samples here, they estimate the quality by frame. So, usually what we have is the overall quality for the whole utterance, and in this paper, they actually were able to export this information to estimate the quality of bi-frame and then get the overall quality. We'll now talk about the data generation. So, we had to spend quite a few time to try to generate more realistic dataset with different impairments. We had initially a clean speech, 2,000 files, then we convert it with different impulse response, to simulate reverberation distance talk. We had the reverberant speech samples, then added noise, we had like four categories of noise and with different SNR, and also different speech levels and noise levels. We're going to show all the statistics on the data in a few slides. At the end, we had like 10,000 noise samples and to train our model we divide it this way. So, I want to now talk a little bit about how we labeled the data. Before I just-  One question, from previous slide. Upper on that corner, third line the random noise level of 50 dB SPL, deviation of zero degree or it's a typo?  Yes, it's a typo. It should be 10 dB.  Okay.  Okay. So, this is just a model, how the quantities perceived by the humans. They actually are divided in three events. The first event, is the physical event, is the waveform which are the ears. We perceive the sound then, we have the perceptual event, where we actually extracted the features such as loud noise reverberation. So, we automatically extract those information from the audio and compare with the desired features. So, the design features is the internal reference that each person has, and then we can make a judgment. So, after doing the judgement, we were able to estimate the quality ourselves, and then maybe have a description of what effect was played a role on this mission of quality. So, basically, this is what will happen when we have people rating our speech file, our samples.  What is PJDD in the boxes?.  This is the perception, to judgment, this is the description. So, we have the perception of the physical event, which are wave-forms. We measured the features internally. So, that's the perceptual event. We compared them with the desired feature which is your internal reference, then you can make a judgment how good is the quality. So, the thing is that people have different desired features. So, we have different expectations, then we'll do the description. So, the idea of the MOS is to minimize or mitigate this device.  [inaudible].  So, he's like considering the desired features with reference and here's just considering the degraded signal. So, here it would be like the non-intrusive and here intrusive because you have reference.  I see.  So, I thought it was important to before talking about our listening tests or our listening quality tests to make clear what are the effects that people are listening to. So, we have the reverberant speech, with RT60 ranging from 300 milliseconds to 500 milliseconds, in our discussion was the most common range and had a close talk and in anechoic speech. So, in terms of SNR, this is basically the range of our SNR, we're going to check the distribution of SNR in a few slides. So, we have also noise and speech level are varying. So, all these variables actually are being assessed by people.  I have a question. So, SNR changes as a huge range, but your reviewer doesn't have a huge range, why is that?  Well those are the RT60 that we had at hand. I think one of the things that we discussed is these are the most common type of RT60 that you can find in rooms like such an office, in-house.  So, the higher that this.  I mean there are some, like a treated office will be 200 or 100, and then my living room is a thousand, so why not go from a 100 to 1000? From 0 to 40 dB SNR, so that's a pretty good range. Just curious.  Because this is the impulse responses we have. You guys gave us.  Also we have like the Skype pipeline, which apply enhancement to all these impairments, and actually we divide the data-set between 50 percent with using the Skype pipelining, 50 percent not using it and we see like there's an improvement. It's like improvement using the Skype pipeline.  What does it include the Skype pipeline?  Is the ACG.  [inaudible].  Also the enhancement.  We have actually those some non [inaudible] speech signals which means zero. Yes, yes comes at zero.  Yes.  You do have zero on it.  Yes.  So, when you include the AGC, what does output of this, I mean this is it like SNR being adjusted or what happened after the Skype pipeline was written out?  When you look at the distribution of the MOS, at least you see a slight improvement. It's not huge, but it's actually doing something in terms of enhancing the signal.  So, if you don't do the AGC, I would assume the signal would saturate, in some of these sounds will saturate, and the more successful because of that? I mean what is the reason of doing the AGC, why do you need that?  To analyse the performance of the voice measurement AGC.  You need ADC because somebody can be at one meter, somebody can be at four meters. This is four times difference in the level of the audio, the speech signal and you need to equalize it, before you send it to the other party. That's the first thing. We did not do normalization when you compute the RMS and the entire file, eventually multiply it simply because AGC introduces its own distortions. So, the gain goes up and down based on what he thinks when we hear speech rotate, we wanted those distortions to be implanted into the signal. So, to get users to judge how much did it take with this or not.  Did that affect the mass? I wonder. Like he says there was some change in the mass like how?.  If there's from time to time actually it's symbolic. But, in most of the cases it sinks.  So, AGC had a big impact more than [inaudible] ADC was an improvement but my suggestion was because I didn't really think it works.  So, in general, AGC with the voice activity detector slightly works quite well. If the VAD misfires and you have a noise signal and consider it as a voice substance to wrap the voice flow, and that doesn't sound well. Or if somebody speaks from far away, and the wave is not far, then the voice still stays weak. This also is not very pleasant.  But in the realistic case the AGC is still there. Like in real case when people are really using this type.  This user processes will create the Skype pipeline, The call which is [inaudible] I think Benjamin.  Yes.  Okay so, this is what coming to the participants. So, for each condition we had 10 participants. So, we average over 10, and then we get the subjective scores. So, not just getting the scores, but also asking the participants to evaluate each one of these effects. Then we're going to see how they relate to MOS.  So these effects can be on or off.  Actually they're always on. So, you have like mixed effects here. Not like its exclusive effect.  Okay.  So, this is actually the OHIS, which is the outsource company that provides human resource for us to do this kind of experiment. So, people, the first thing that they do they read the guideline, they go through a training where they get familiarized with each one of the imperiments, noise, reverberation, et cetera. Then they go to a qualification part where they go through the experiment and then we use a good standard file so we know what to expect from their ratings. So after that, they go to the test itself, they listen to the file after that, these buttons are enabled so they can rate from one to five and then we ask them which device they're using and also which imperiment was mostly disturbing. This is a little bit of the Data Exploratory Analysis. This is our MOS and this is a PESQ, and this is the SNR. The SNR is supposed to be like a normal, so we actually added a little bit more low SNR so we get the MOS to shift a little bit to the lower values. The reason is because we want our model to actually learn how to rate also this kind of situation. But people were generous when they were rating. I was thinking that because people were not using our service, they were working for us so they tend to be, I think that may play a fact.  If they thinking with the actual number for that mass, how do you derive that number?  We have to derive it because remember we need to average over 10 people or even more to eliminate the subjective needs or the individual factors.  So, my question. Okay. So, why aren't they underrated, aren't supposed to be correlated? PESQ and MOS?  So, PESQ is pretty behind.  PESQ.  So, that means PESQ is behind?  Well PESQ, the way I see it is that PESQ, you see mark close to the SNR solution, you look at only at the signal, PESQ doesn't really look at the subjectiveness of the.  It supposed to look, right? I mean.  I don't think it was trained for this.  It's not trained for this.  It's trained at to it's only to evaluate the distortions causing the speech compression. That was the initial idea behind. So, you have codecs we different abitrary, different cortical, corting salvalitmus which means spelled them innately relatively clean signal from a form or cost of microphone. You compress it, and then you decompress it, and then you have to judge at the quality. So, this is why even the traces of noise make best to give you a low score because these things that the kodthec introduced those huge noises there.  I don't know if my request. We have brought in BESQ and Poker remember like in that expel, they were quite reasonably correlated.  So you'll see BESQ and MOSQ from the judges have 0.7 correlation. There correlation is the same thing, it's just the ARCAS absolute value, BESQ is more strict and gives lower score when you see noise. When the voices is natural, you may settle more for givic.  We use this for speech enhancement  Yes.  Evaluation takes.  I don't have the full chapter why BESQ is bad, but we don't have anything better than that. So, this is the better thing.  Okay.  Just this point about the MOS. I think if your people are using our service, there would be more restricted with the MOS. I think the fact that they are kind of getting paid to do this, they might be more generous. That's one assumption.  So, then you compare this with expert ratings?  No.  Okay.  Okay, so a little bit more about the data. So here we have MOS in SNR. You see that MOS tends to be correlated with the SNR. So I'm not saying that the costs for the SNR, but it plays some effect listener. So, the noise level, it seems like when the noise level gets higher people tends to give a lower MOS but is still like lower is still like three, right which is that's why you get this shift MOS distribution. For me, the way I see this is people are more concerned with the speech, so they really can't hear it. So, if the noise is tolerable they don't really care much.  So, wait a minute. What is this? Is that all 10 numbers and that's the average?  Yes, so yes exactly.  It doesn't look like.  No this is actually the MOS, is the MOS already.  No. What is that line and what are those dots?  The line is like the correlation, between the two. The trend.  How they they vary together.  I think less into a year you have interactions right, between your variants so correlation doesn't make.  Yes of course. I mean just to have an idea so here's not the just the SNR there is. That's why I showed this slide here so people know that these things are on all the time. So, you cannot really just analyze SNR and say that's the only fact.  Go back to that. So, each dot should be a file right?  Of course.  Each dots should be a file yes, that's right.  So, each file gets ready for 10 times. That the value MOS should be the average of that.  Yes yes.  This is why you have every tenth.  The curve should just be the mean of that.  What is that thing around the curve that.  Should be a confidence symbol where about this.  Yes.  But, in general noise level per say is not indicative because under the same noise level, if we have a level valvet or softer speech signal you'll get different results. SNR is more indicative and you see that it is actually.  Yes.  More.  Yes, but I was curious to see how these three things plays out considering you know, because these things are changing together we don't have fixed speech or a noise, so these are all effects on the outcome.  So [inaudible] might not hold if you look at the middle and the third picture.  What is that?  Noise level and speech level.  Yes, I think the fact that you actually see, like, of course, there are interactions, but you cannot deny that there's a correlation between.  In general, yes.  So-so, the speech level doesn't mean anymore than the noise level, but the fact that they're delivered at the same absolute playback level, it shows that, okay, if you crank up the noise, it doesn't really matter as long as the speech level is okay.  Yes, that's the point.  But I don't see that in the third picture. I would expect the third picture would be the same or similar to the middle part. No?  No. Because, imagine you have noise here, but, you don't really care right? Because you are being able to understand or listen.  Okay.  So, that's why as the speech level increases, MOS is increased, so, you have a better level of speech. So, it's more comfortable for the listener. I think we saw that Chandan right? I think one of your comments was that, that's why you're always saying like let's lower this pitch level because that's what people are getting bothered with to get the lower MOR.  Yeah. Low volumes have the biggest impact on the MOS.  Yeah.  So, I'm just still concerned about this distribution here. It looks like the dots are distributed all over the place, right? Yes, it's just one of the abilities because you know they have other things going on here. So, you cannot just say, this is noisy MOS.  That's why it's difficult to estimate here.  There's a lot of interaction here. We have all these things but [inaudible] have a Rawlsian networks properly simulated with a lot of [inaudible] packages-.  This is Xiao Gu money when they go.  -those are the files. I know people have elevated down and we're computing the mean range so that seems to be at least 40. Let's say you have a file that is 20 DBSNR. What this file they can be distortions from the speech enhancement is called AGC, they simulate to skype through stimulated network. Maybe, packages groping can sample files which brings the quality value. So I think the bottom line here is that there were way more factors impacting the perception of the quality than the personal which is quite frequently used in scientific papers as the quality measure. Yeah, I mean the point here is just to say there's some trend data and we can actually extract some information but we cannot say like this is causality between speech level and MRI and here this is also i think is interesting so this is actually what people reported as more disturbing. So, and then actually when they report the noise was more disturbing noise was actually increasing in terms of disturbance we have a decrease in MOS. So that's what people thought, so, before was the ground truth, right? And here with reverb so as Ross said maybe we should have trained here but these noise, but this is what people said. We have three judges of thin clinking cold air is noise in the file you'll get 0.3 [inaudible] because each just one, there is a noise in the fall is a resolution of one bed because you have 10 judges think Lincolnian this 0.3 noise level. Nobody went this isn't this the wrong way to blockade because excessive should be the only thing waiting everything should remain the same for every point you have to understand the trend if you wonder study that trembling in Los Angeles. That is in the silence tree but this is kind of more practical thick you have 10,000 miles. I understand. Combinations of everything in there. I mean, you cannot be a nice and noisy place. Still you can say that, when pretty much everybody hears the noise in the file. This means you lost one and a half points at MOS. All the other factors at play and just not just that's fine. You read to basically eliminate them wherever [inaudible] this is the trend like. Yeah. I think, the problem I think we can do that kind of exploration, Jeff would have more the more time you get the data ready? I will keep everything the same distinct device level and get the [inaudible]. Yeah, but we don't have that in our data, that's the thing. The way our data was generated we have mixed effects in the day that's what people are hearing. We can what we can do is when plotting it we can maybe get the SNR with one range and then get the other- but the SNR is random so, if given within that range we're going to have some variability. But actually, if you just took all the files that were only noise was it doesn't look nice. But that's the thing we don't have that, because people have the option to- to that's, you see this is not is not exclusive, so you can hear noise and because that's what is in the file Noise Regulation. Have mixed effects. What if you keep only the files, No [inaudible] , and no [inaudible]. That's [inaudible] because you have-.  [inaudible] People who are totally Reuters people will list the laws the lower volume double-negative [inaudible] but [inaudible] more than that so, we still have to do some. Awesome. I guess, the question you're asking here is for people to tell you what you already know, you put the noise in the [inaudible] in that you're just asking people how well are you able to correlate your own judgments of what their condition was but in the end, you probably don't care about whether people thought there was too much [inaudible].  I think this does actually. Natural speech hurts most the perception of the world. You see you can get two points for natural reverb is less than a point, yes, at some point more and more people [inaudible] but they don't punish that much and MOS is somewhere in between. Which might just be an indication that remote my math is not something people think about money will all school. Maybe. And that's again that there is a ground truth you know how much reverb was actually eliminate. Joe maybe just less annoying for humans or even a thousand seconds is here this is this is true yes which is pretty much [inaudible] so they don't get much [inaudible] by the distortion.  So, let's go to the proposed approach. The first approach that we thought was using I-vector. So, the vector has been widely used for recognition tasks. So that was one of the motivation. So, has been used for a speaker vilification, language recognition, emotion recognition and the question was, what about speech quality? So you use I-vector for that? I-vectors they are known to carry speaker and also channel variability. The way you can think about the I-vector Is, you extract several observations from your speech file, usually MFCCs. The bottom line is where want to map those features into a utterance level vector, which is the I-vector. So we want to map them in this total variability space. So the way we do that is, first you compute the GMM-UBM model. So, this background model we have bunch of speech files and it's going to be a reference or like a mean in this space here. After computing the GMM-UBM, you extract some statistics from each one of your observations here. So, you get what is called the super vector. So, the super vector is composed by the components of your GMM, and the features that you have in your feature extraction. So, these super vector here is what you are going to use to compute the I-vector. So m here means independent super vector, which comes from the GMM, and you have the dependent super vector. So the dependent super vector depends on your speech files. Every new speech file, you are going to compute the offset from the GMM-UBM, which is mean I-vector. To do that you have also to compute the low-rank liability matrix, which is the T-matrix. We use the UBM and the Baum Welch statistics to do that. So this is the framework for the I-vector effort, preparing this framework, we extract the I-vector for each utterance in the train set, in devset, and in testset. So here are some plots of the I-vector projecting the clean speech, which is the dark blue dots here and MOS below 0.5. So, you see that it projects the I-vector and can actually separate the two sets. So, here's considering MOS between two and three. When we consider actually the two levels of MOS with the clean speech, we see that the data gets a little bit mixed up. There's some clustering here, but there's some limitations. So, then we thought maybe you can use I-vectors combined with a DNN MLP architecture to find a better representation to compute MOS. So this is the architecture that we proposed. We have I-vectors of 400 dimensions. I have a two hidden layers, 200 hidden units first layer and the second layer used, dropout for as regularization and relu as the activation function and then we predict the MOS with this model. So, the second approach was using what is called a Q cepstral coefficients. Again, it has been used in bass cleaning speaker verification to detect natural and unnatural speech and has been very successful in that. So that was one of the motivations. The other one is that, it's psychoacoustic motivated. So, the idea of the constants cepstral coefficients is they say that this Q or the quality factor increases as the center frequency increases. So as you go up, this factor will increase because the window length that you extract your samples they are fixed. So what they propose is to make this Q factor constant and change this, so that this doesn't change. So, the motivation for that is that we're going to get a spectrum. There is a better resolution for high frequencies and low frequencies. So, that's how it would look like. So, you'd have this spectrum using their model, the CQT. So, this is how it's computed. So, we compute the spectrum but we are using the Q transform, the power spectrum, the log, and this is actually a spectral feature. So, they had to change the scale, the log scale and they do that by using this uniform and then they compute the deception. We use two approaches for these features. The first one using cepstral and the second one using this spectrum. So, if we remove the signal process computation of the spectrum, we know that cepstral features are very well established. But if you let the DNN look at directly on the spectrum, would you get better results? There was actually the question here. So this is our evaluation. We use evolution setup. So we use the Pearson correlation and the mean-square error to compare the models. As baseline, we use MOS and we compare also our results with SRMR, P563, and PESQ. So, SRMR is also a measure for estimating reverberation and we want to see if they can also be able to estimate the quality for the impairments that we had and P563 is another metric, is [inaudible] metric.  What's this P563, is this-  Yeah. It's a [inaudible].  Recommendation, feedback [inaudible].  Mmhmm.  Okay.  So, all the results are based on the test set.  [inaudible]?  No, we tried but we didn't get rid of it. I didn't get a graph of it, but-  You didn't get what?  We didn't have the code for that.  I had it. You should have asked me. Oh, really? Oh my God. Okay.  That's two hours [inaudible] .  So, maybe next summer. Okay. So, this is the results for the I-vector, and actually it's pretty similar to the results we get with the CNN on Cepstral Coefficients using the Q constant, and one thing I was thinking here is that, so I-vector is projecting Cepstral features or MACCs to the total variability space, and the CNN is basically doing the same thing here. So, that may explain why the results are very close. So, this was a bit surprising for me. So, actually removing one signal process computation we get better results just looking at the spectrum. And then, we have [inaudible] to compare with the SRMR and P563. We also explored with the help of David and Harness, another structure using MEL features in a full connect MLP, and got a pretty good results here. So, the other thing that we explored was using these same structure here, which is computing the MOS on a utterance level, but using-  The frame.  -on the frame level, here? Okay. So, what we did was to compute here the - we use all the features here on the frame level as an input for an ELM and extract some statistics from the ELM to get the results. What we found is that the ELM actually helps to decrease the variability of the data from 0.28, 22 to 0.17.  [inaudible] CNN.  In the- how you get the results per file because the fully connected network gets a segment of the thing. In this case. So, you got you have a segment movement get a certain number where you call them equations. How you compute them here for the entire frame?  For the utterance we just take all the frames over in the utterance and that's what's passed a bit [inaudible] passed the ELM.  Here, yes I know. But Here?  But that's also the that's per utterance results.  How do you get the number for the utterance here mileage? What do you do?  It's just correlation-.  He's telling you get a correlation per frame? [inaudible].  All the frame-  Per segment. So, you cannot just network or I mean call a segment of 25 frames and then next, next, next, next.  Right.  And you have a bunch of numbers which is variable in. How you compute the moment for the utterance here?  As the result network, we just looked at all the frames that were in the utterance maybe at 25 frames between 25 actual physical frame-  Segment by segment?  [inaudible] . So, each of those what we call the 1450 Vector Frame. Each of those frames we just worked at the core, what was the result, what was the loss estimation on that and which is the correlation over plus all those frames.  All of them??  The utterance level before the ELM, then the utterance level after the ELM.  So, pretty much this is one solution?  Yeah.  This just tells us how much [inaudible] Okay.  [inaudible] those are, everything's utterance based on clubs and that's also utterance based.  So, pretty much the all [inaudible]?  So, my question is what is MSE [inaudible]?  Sorry?  MSE is compared between one or two things?  Yes. The ground truth which is the MOS and the inner prediction.  MOS is one number, right? And-.  Yeah, but we have- okay in the test set you have, in my case I have 1500-.  On the [inaudible]?  Yeah. So, ELM is-.  [inaudible] 0.17 ACP-  No.  Guess estimation of how-.  This is just the Pearson correlation. No MSE. So, in estimation is about 0.2 meaning if the best is 2.5 or mass is 2.5 then is estimating the 2.7 to 2.3.  Roughly yes?  That's what I mean sir.  Yup.  And also do we have a estimation of how well humans do?  No.  Humorous mean [inaudible] .  Yeah, well we should get the data couple days ago.  No, we have the data right?  Yeah.  But we didn't find people have even 10 members.  Yeah that's right.  Output this range for those 10 people.  Yeah.  I'd love to hear one more line humorous [inaudible]. One point. So, technically 10 people on average 10 people get around 0.3 meter square, square of 10 times less, so technically this means that if you will get the 0.17 or 0.16. You are as good 50-ish people doing most tests?  If it's [inaudible] but nobody knows that. That's empirical.  Still not bad.  What was the difference again on the second and third models? [inaudible] .  Okay. So, this one is looking at the cepstrum, so you compute the spectrum and then you compute the cepstrum and then use the [inaudible]. So, this one is just an spectrum, yeah.  So, you let the [inaudible].  That's right, yeah.  Okay.  So, just to conclude we develop a new audio quality data set with labeled and I think it's very realistic data sets and we can explore a lot of things on this data. We explore also the effects of mixing impairments on MOS, we explore three approaches to estimate MOS and we show that our methods outperform all the baseline methods. So, the next steps. So, we want to investigate- maybe the combination of I-vectors with Q Cespstral features, see if you can get better results. We definitely need to utilize our Algorithms in new data sets and we want to explore more new network architectures, end-to-end DNN. So, we want to explore a little bit more the DNN plus ELM and CNN plus ELM. And so, I'd like to say a special thank you for the Skype teams of Gehrke was a great help, Ross as well, with great discussions I learned a lot in this internship. I'd like to thank you all the people that supported, funded, Johannes and Scott, Hannes Gamper, David Johnston, who he helped a lot also with technical. Your technical expertise was really nice to have you guys, and the whole Audio and Acoustcis Research Group especially Van was really nice was always there when I needed and that was a great time for me.  Questions. It was actually quite nice discussion during the talk, but if you have more questions, please ask comment.  I think one of the main things to do in this exactly to do what, [inaudible] was suggesting, right, is to get expert opinion on this, because people are giving some MOS scores. But it might just be random. I know how reliable they are because these people are just people.  But there's not the point though.  No, you also want to get an expert opinion to see.  What is an expert opinion? I play a sound file that has five different distortions. You can say based on my expert opinion, I think that's 500 milliseconds revert. That might actually impair your judgment because now you're basing your judgment based on what you hear in a file, not what you might proceed when you have an actual phone call with your-  That is fair. But at least it's going to give you metric to say how much is the variance around. Basically, if the 10 people marking the MOS score varies from one to five, then it is this nice.  Like you said, it was one point standard deviation, which means typically in MOS, Skype MOS scales. Our variance or standard deviation is one.  Right.  But here I don't know how much was the standard deviation, right? So the standard deviation is about five here, right, and it's useless.  Yeah.  I think 0.7, 0.7.  Is it 0.7?  Yeah.  So, the reason I was suggesting getting experts to check some of the ratings because it looks like you have a central bias. So, things are kind of centered around three to four relevance for now. If you ask a bunch of experts, they shouldn't have this bias.  Yeah.  So-  You'll hear more impairments, the regular impairments.  Well, it's critical whereas a naive writer will tend to rate the mean of the central tendency bias of string.  I think this kind of will never be really realistic. We tried to do as much as we can. One thing that I was thinking is different when you're listening to an audio, and when you are having a conversation using Skype service. So, then and if you're paying the services is even worse. But there you're getting paid to do that. So, I think that also my-.  No. Definitely data is so important, right? There are companies who were just selling data to ITU-T, and their job is to just collect data for MOS evaluation, and you're doing it for three months, it's a good achievement.  Yeah.  Right. They're are doing as much as they can.  One question. How much cleaning of the data that you have? Are there any consistency checks that you find bad writers have thrown out?  We didn't actually did much of that actually. We didn't have standard alone.  So, if some letters vary too much time from the mean so they got standard.  Okay.  That was actually one of the problems and we stop getting the hits at seven point because most of the work is done by-  You mean like during the test or after? It could be post test.  Yeah, the post processing, right?  Yeah.  We didn't have time to do post processing, which I had to kind of.  What were you doing during the test? And [inaudible].  So, when you had 10 judges, and everybody disagree on everything. You still calculated the average?  Yeah. I think there's a lot of variability in this data for sure.  But it help if you then look at each judge of the special schema, multiple samples, [inaudible] samples, if you look at what actually is ranges and then-  What we have done this is the motion detector.  So one of the thing is like if you check the ITU-T recommendation for distress, it says should start with the conditions that you want to do this experiment. So, the condition would be very clear. People should not have second guess about what they are hearing. But this is not realistic. You're not going to be an environment with only reverberation or only noise. So, that's what I think was the challenge of this dataset, to have something realistic at least on that side of the impairment.  Maybe one consistency to check that could be added would be you also ask the same rater, given the same file, and make sure they're rating at the same, that they're off by a huge number.  Okay.  They're not reliable.  Yeah.  One small comment since you collected what people used to listen to, the sound rules, there might be some interaction between- if they are listening over speakers, and the speech level is low, then just rounds in your environmental noise. The noise levels being more or high doesn't have much.  Yeah. That's right yeah.  Or just seeing-  But also data mining we can do on those.  Yeah.  One hundred thousands journals.  Yeah.  Is that-.  All the procedures just drawing out all the speaker based ones and see what is.  Is there no way to buy this kind of data from somewhere?  It is which is more costly, $10,000.  Or whatever, people would have done this more rigorously, right?  No.  I mean, there's this company which does POLQA. Which develops all this PESQ and all that. What is the company's name? It's in Europe or somewhere?  HEAD acoustics.  HEAD acoustics.  No, Opticom.  Opticom.  HEAD acoustics is also-.  It also makes control software, but they don't. So, are selling $50,000 or $100,000 equipment to do lab tests.  They don't sell data?  Opticom will sell you POLQA. No one actually sells data to do this.  You don't know if it suitable. The data is suitable for Europe?  The actual Skype pipeline, so whatever they did is very, very Skype relevant.  Yes. Skype relevant also.  So, more questions?  I had a comment close to what you just said, I think all this averaging might be queuing the application because we really don't want to average all these opinions, if your intended application is for a telephone versus quality versus Skype in a conference on quality versus Skype at home quality. So, definitely what's-  Yeah, this is something I think [inaudible] was saying, right? But then you cannot call it MOS anymore. You've to call it something else I guess.  But the average must be more-  Yes, on one hand. On the other hand, do you really get valuable information? Is more tailored to your application? I guess that's the-  I don't know because the IT2, they say should have like 20 people. We got 10. These enough for 10, like in our experience.  So, your deviation of the average is way lower than humans. So technically, presumably you get better results down a bunch of humans, and I just estimated 0.16 mean square error will be added to one of a single human is equivalent that this 0.16 is equal to test with 40.  Okay.  Square root of one over n is the number if they need the statistics. Okay. But that's pretty much means that each measurement with this tool is equal to MOS with 40 humans.  Okay.  More questions?  Again one question to this slide we show the correlation between SNR and MOS. So, that was the correlation between the SNR, how you generated the data?  Yeah, this the actual.  So, would be interesting to see the correlation if your compute SNR at the output, which also then contains like takes the impairment of the processing, and all these artifacts, and cache logs, and these things into account, and that will probably expected to be this so much higher correlation.  So, SNR are what? The input of the pipeline or the output?  The input.  The input.  So technically, it's actually SNR is here, because the Skype pipeline cuts this.  Yeah. Actually, 50 percent of the data is without the pipeline. So, we actually could check the difference at least.  In the call of 20 dB SNR, the files with 20 dB input SNR, and the files with 20 dB input SNR but process it by Skype by a time which means the output is let see, assuming 10 dB, SNR improvement. They're actually at the 30 dB vertical. Yes, Sebastian is right. If you measure exactly, and that's pretty straightforward engine. But still in SNR, it's is a poor quality indicate. It's the easiest thing.  That could actually be added to the list of estimators just SNR predictor. But the SNR at the output.  It's actually hard to compute real time, CPU compute here.  Because we have the clean speech file, and we do the clean speech and then estimate.  Not practical to computer in our own system.  Yeah. Just as a reference point for comparison.  One more question. So, use the constant 10 samples or 10 ratings per file. But a reality that should probably be adaptive because different noise levels, different impairment types will require more ratings or less ratings.  Okay.  So, humans are not consistent across all different types of files.  Okay.  So, you can use your data to figure out which, how many samples you need.  Okay. For impairment?  That's right..  Okay. There is a lot of data to play with for a long time.  More questions. If not, let's stop speaking.  Thank you. 