 in this video I will be using a really cool data set to build both a linear and nonlinear regression model with Kerris I'll explain exactly what this means in a second but first I would like to touch on the data set this data set contains some statistics on YouTube videos including the number of views likes dislikes and also the number of subscribers on that videos channel how I got this data set is actually super cool I along with my dad developed a web crawler which was used to scour the videos on the home page of YouTube and also the results of videos that appeared when a particular search query was entered into the YouTube search engine some examples of these queries include sports politics and gardening a web crawler is simply an automated script that browses the web in a methodical manner oftentimes retrieving data an important thing to mention is that our dataset consists mainly of very popular videos so it isn't necessarily representative of all YouTube videos for our purposes though it will suffice in the video description there will be the link to a github repository that I made which contains the supplementary code and data set for this video if interested you can download the files by clicking the green clone or download button in the top right hand corner and then clicking download zip' you will then need to unzip this alternatively it is much easier for you to simply click on the code and look at it directly from github the goal of our model will be to predict the number of views based on the other three parameters which are likes dislikes and subscribers this is a prime example of a regression model but what does regression mean well regression just points to the fact that our model is continuous and that for any inputs we can get an output for any number of lights dislikes and subscribers we can get the predicted number of views this is different from classification problems which I'll be talking about in the next video in this series again with the cool dataset linear regression means that our model will essentially be a straight line and it can be defined in the equation y equals MX plus B with wiving the output M being the weights X being the input and B being the bias even if these values must be matrices in the case of a neural network this will be a neural net with no hidden layers it will have an input layer and an output layer connected by weights and we will also have a bias for each output if you are unfamiliar with the structure of neural networks you may want to watch my video where I explain justice it will be linked in the video description our linear regression model will look like this it'll have three inputs one output and one bias let's go ahead and implement this in Python and see how it performs before we do that we will need to install a couple more packages in addition to what we have installed in the last video so in your command-line make sure your environment is activated and type the following commands Conda install pip which will install type in your virtual environment pip is just a package management system that we will use to install some other packages from here we will type pip install pandas then pip install SK learn then pip install matplotlib we will see exactly what these packages do in the implementation next we will need to upload our data set into jupiter notebook i would highly recommend that you make a folder within your home directory in jupiter notebook so that your interface remains clean and uncluttered you can do this by clicking new in the top right corner and folder from here you should rename your folder by selecting it and clicking rename in the top left-hand corner next you should open the newly created folder here you can click the upload button in the top right hand corner and select both the inputs or stats videos excel csv and the outputs or stats videos y all csv after doing this you can click upload next to the files and you are done from here in jupiter notebook we will import all of these modules for some we just import the module and for others like I plot we must type from Matt pot Lib import pie plot this is because pie plot is a sub module of matplotlib that doesn't get imported with a simple import matplotlib additionally for some modules like tensorflow we type import tensorflow as TF this is simply so that when we use censor flow we don't have to write out tensorflow every time instead we can just type TF next we need to assign the data set to variables i do this with the function in pandas called read CSV where we pass the path to our file in quotes because we have the CSV in the same folder as our Python script we can simply pass the name of our files in this case they are stats videos XLV and stats videos while CSV so now we have two pandas data frames containing our input data and our output data by the way this is why we call the variables DF 1 + DF 2 after this we will split up our data into training and testing data with a module from SK learn called train tests split if you remember from my neural network videos training data is the data that we use to minimize the cost in the testing data is a separate chunk of data reserved for testing our models performance remember overfitting occurs when our model learns our training data too well and as a result it suffers in generalizing which is the main goal of a machine learning model in the first place therefore it is important that we can test whether or not our model is overfitting one way to do this is to reserve some data for testing that we don't use in training our model when we do this with the train test split function it randomly samples our data which is ideal by defining the test size as zero point two we are saying that we want 20% of the data reserved for testing and the remaining 80% of the data reserved for training when we passed the F 1 we will take random 80 percent of it assign it to X chain and it will take the remaining 20 percent of it and assign it to X 10 the same occurs for df2 now into white rain and white test once we do this we scale our data scaling data is usually a good practice when you have inputs for different features that vary greatly or outputs for different features that vary greatly I'll explain why this is in just a moment but first let's look at our data you can see that we only have one output parameter views and so scaling it won't offer any performance benefits besides making the optimum hyper parameters like the learning rate more consistent between models for different data sets and in turn easier to find in my implementation I choose not to scale the outputs but you very well could note that if you were to do so when using your data spur diction's you would need to undo the normalization which you can do by performing the inverse operations of the normalization that you initially performed on your data however our inputs are a different story we have likes dislikes and subscribers and as you can see the values in dislikes and even likes for that matter are orders of magnitude less than subscribers in a neural network this would result in an under-representation of these parameters in the output when the weights are first initialized and most likely when training has been completed this is because training doesn't always find the perfect combination of weights for different inputs which can be called the global minimum but instead it adjusts different weights from where it begins and gets what is often a local minimum or a minimum of the cost in a certain range of weights this would be an example of a global minimum and a local minimum in these graphs we can see the correlation between dislikes and views likes and views and subscribers in views you can see that dislikes actually has a stronger correlation to views than subscribers says from this we can tell that by standardizing we would maximize our chances of reaching the global minimum or at least something near it because we are making sure that the data is more less equally represented in this implementation I will be using the dot scale function from SK learn a processing this scales the data so that it centers around zero and has a standard deviation of 1 note that the transformation is not linear a graph of our data should roughly resemble this normal distribution graph the next step is to actually define the structure of our neural network first we define our model in the command model equals sequential after this we begin adding layers for the first linear regression model I will only add one layer with three inputs likes dislikes and subscribers and one output views and it doesn't have an activation function note that it does have a bias the predictions of this model would be a straight line but in four dimensions whatever that would look like in the next line we State our optimizer Adam and our loss function mean squared error all that you need to know about optimizers is that they state how we update or optimize our weights and that Adam is an alternative to stochastic gradient descent that simply performs better stochastic gradient descent is simply we're using derivatives we update all the weights to a subset of the data again Adam is simply a different alternative to this that modifies the process slightly and brings many performance benefits in this instance because our data has such large outputs I set the initial learning rate to 100 and even this as we will see is not nearly enough additionally mean squared error is the cost function which we defined in the neural network series it is the same thing as Y minus y hat squared from this point we train our model we do this by saying model dot fit and passing certain parameters first is the training inputs than the training outputs in jupiter notebook we can see all the different parameters by typing shift-tab in my implementation i say the number of a box or the number of times that all the data has been cycled through validation split which I'll explain in just a moment and verbose which controls whether or not our model prints certain things like accuracy at each epoch I usually like to have this off validation split is a random sample of our data in our case a random 10% that is used to check the accuracy of our neural network on something besides the training data at each epoch it is different from the testing data which we only use once our model is done training although I use the default batch size of 32 and don't explicitly define it in the dot fit function it is important that I mentioned batch size and give you an overview of what it actually is during each iteration the model calculates the predicted output for 32 data points calculates the cost for each of them averages this value and uses derivatives to update all the weights the model then repeats this process until it has cycled through all the data points this would be one Epoque note that in our case we will have many iterations and every Pok if we use something significantly less than 32 then we won't be using a representative chunk of data during each weight update anything significantly more and we are being inefficient and performing way too many calculations before we update the weights 32 is a good sweet spot that many programmers use I've all this set to history so that I can graph the accuracy of the model on the training data and on the validation split over the a box this is exactly what I do in the next lines don't worry about the specifics all of that you need to know is that the red is a validation loss or cost and the blue is the training loss or cost once I run this you can see that the model hasn't learned much yet but it is on the right track with both the blue training data loss and the red validation split loss going down when I make the learning rate 1000 the model continues to improve when I update the learning rate once more to ten thousand you can see that the model stops improving and therefore we can further improve its performance only by optimizing other parameters next I add an activation function specifically relu which is one of the better performing activation functions as you can see this has no significant effect on the performance of our model now that we have trained our model let's compare its performance on the training data and testing data so let's run our model with its current weights on our training data and testing data by typing model dot predict in passing X train and X test and then assigning this to Y train predictions and Y test predictions respectively now that we have done this we must find a good way to measure the performance of our model after all we can't measure the accuracy as a fraction of correct and incorrect predictions as we can in a classification problem a good way of measuring our models accuracy is the r-squared score r-squared is a statistical measure of how close the data is to the regression model it will range from 0% to 100% which in our code will be displayed from zero to one with zero meaning that the model explains none of the correlation between input and output and one meaning that it explains all of the correlation I implement it in these lines don't worry about the formatting in the following two lines but just note that I print out the are two score which in order to be calculated takes two arguments the actual data outputs and their models predicted outputs for both the training data and the testing data if you remember we often call these Y and y hat respectively in this instance the r-squared score is slightly better on the training data when compared to the testing data so we are likely overfitting a little bit overall our model is performing quite well despite the fact that it is very simple let's see what happens when we make our model more complex there is no guarantee that the performance will improve as our linear regression model is already performing quite well in the actual relationship in our data set between input and output is simple even linear meaning that a more complex model may not be necessary I'll begin by adding 13 outputs to the first layer there is no real reason why I chose 13 as opposed to any integers near it perhaps because it is how old I am I also add four hidden layers each with 13 inputs and 13 outputs I then add an output layer with one output which will be the predicted views this is what our model looks like next I define the optimizer Adam the cost function mean squared error and the learning rate which will be 0.003 after training this model for 6,000 iterations you can see that both the training data loss and the validation split loss converged quickly and then the model began to overfit with the training data cost continuing to improve in the validation split cost beginning to go up again note that when you run this code it is most likely that you will need to adjust these values in order to achieve similar results neural networks performed differently every time you run them especially when data is randomly sampled we could rerun the training and limit the number of a pox to around 100 when our model was doing the best or we could simply use the early stopping function we implement it in the following way you pass several parameters the most important of which are the loss or cost that you want to monitor the min delta which sets a minimum for how much the cost must improve in an epoxy which allows the cost to not decrease for a certain number of a box and verbose which controls whether or not certain things are printed out again if you'd like to see all the different arguments that can be passed click inside these parentheses and click shift tab on your keyboard I assign all this to a variable which I named early stopper and pass it in as a callback which is an argument that the dot fit function takes as shown on screen note that I very well could have implemented the early stopper in the linear regression model however I essentially nailed it with 500 a pox and a learning rate of 10,000 immediately from the graphs we can see that the linear and nonlinear regression models perform quite similarly next let's compare the performance of the two models in more detail with the r-squared score first I need to calculate the r-squared score of the deep neural network that is exactly what I do in these lines as you can see they are quite similar in the next lines I do as best I can to visually represent the model and its performance I make a scatter plot with the actual number of views on the x-axis and the models predictions on the y-axis note that red is the training data and green is the testing data I also make a straight line with a slope of 1 on a separate graph to show you what a perfect model would look like remember a perfect model would have the same actual output and predicted output from this graph we can conclude that our model is decent there are several other important ways many programmers improve their models performance these include batch normalization weight regularization dropout data augmentation and several others I will talk about these with a data set for which the performance of the model improved substantially in future videos in this regression data set there is no such improvement and yeah that's it for this video stay tuned for similar videos on things like classification problems convolutional neural networks a cursive neural networks and more which will all feature cool datasets 