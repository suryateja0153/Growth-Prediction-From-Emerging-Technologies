 Good morning everyone. Thank you for attending this class. My name is Debadeepta Dey, I'm a Researcher here in MSR and my background is actually the intersection of Robotics Reinforcement Learning Machine Learning in general. Today, I'll be talking about and trying to give you a quick overview of what is the state of the art in AutoML, for Neural networks. Automatic Neural Architecture search and I'm hoping to make this much more informal discussion oriented, so feel free to ask questions. This is obviously not supposed to be just a talk that I am giving, this is supposed to be much more classroom style. Then hopefully, we can all learn and also like this and as you will see this is a pretty hot topic. There's lots of open problems and this is also very Beck company they've definitely very Microsoft relevant, because we have Azure ML and we would like to have AutoML offerings. As well and got to potentially make us generate a lot of revenue. So, before I begin, but even more so how many of you would say would be at least a per people present in the class as a show of hands would be comfortable with let's say convolutional neural networks are recurrent neural networks. That you have used it or heard about it or at least a very basic level understand, one, okay, good and maybe you haven't played with some in tensor flow pipe CNTK et cetera, good. How many of you would say that you are comfortable with terms like MDPs, Bomb DPs, Hidden Markov Models. Basic self let's say reinforcement learning, Policy gradient methods, reinforce, okay. The reason I'm asking is because reinforcement learning gets used quite a bit in many of the papers that you will come across today, at least in Neural Architecture search papers. It's a little bit important to at least understand the basic self policy gradient methods in order to appreciate what those papers are doing and ultimately we will, I'll make the case that, RL is not may not be the best batter dime for doing Neural Architecture search in spite of a lot of high-profile papers that you would find if you do the literature search. All of the latest results are actually confirming that which is not very surprising but sometimes that's how communities move. We have collective DNA and memory. So, just to formalize what is Neural architecture search. So this for example, is the table which shows you the architecture for DenseNet. Now, DenseNet is, how many of you know what DenseNet is? Okay. So ResNet? Okay, much more people. Okay, great. So ResNet and DenseNet are not that different so ResNet, came out of MSR issue about three or four years ago now, it has definitely a great architecture but the basic idea was that, hey skip connections and the ability to predict identity is a good idea. So, ResNet looks, let me see I use this part of the board because that part of the board will be obstructed by the DICE. So, let's say I have a few layers and I have an input image or anything some input X let's say center of edge, usually a convolutional Neural network this is a standard feed forward network these are these things can be like convolutional, pooling. Like Max-pooling, Average pooling. Then some sometimes there are like reduction operations because we want to downsample usually the pooling will do it but in order to, sometimes have we will have like dimensionality reduction operations. Same over and over repeated and then what ResNet said, "Hey, it's good to have this skip connections". Why? Because then the network is allowed to choose its own capacity because if the network realizes we actually don't need this layer we can just make this bypass this completely. Learn nearly like very tiny weights are zero rates here, learn identity there and that allows you to have a lot of flexibility. Then DenseNet came around I think CVPR 2017 and you know what instead of connecting why don't we connect every layer to, instead of it being connected to everything to only the one before it, let's connect it to everything before it, right. So, this guy gets connected to this as well as this describes connect to this, this and this and so on. That's what the DenseNet did, right because which is and then the showed really great results but it requires like for you to have done a lot of work playing with these kind of networks. Then if you think of there's a why are these specific numbers that thinks they are right like why is this Dense block output side seven cross seven? Who said that the best thing to do is to have in this Dense block on the fourth Dense block we should have 16 of these layers, then 32 of these layers in DenseNet 169 and so on and so forth. I was having this when I was visiting MSR in New York, and I was talking to John, John Langford and John Langford does a lot of theoretical ML as well. John, looked at this block we were talking about you're actually starting to think about AutoML and this is April of this year and John was I have no idea how I would come up with this, right. I wouldn't be able to come up with this either, why is it that this specific set of things in sequence gives you a great result. You could be I read builds upon like you know we found somehow magically ResNet works really well as opposed to things which came before ResNet like VGG, AlexNet et cetera. So we connected everything to everything and that's how it came about. But, it seemed and we just did the work and found that it performed really well but that seems very unsatisfying, if I give you a new data set on which is not images which is not imaginary which is not, let's say MS coco datasets and many of you work in production environments where your datasets are your customers own unique dataset or datasets that you encounter as a side-effect of the product that we work on which are not those standard datasets that academia vision Machine Learning community or speech language community have the standard datasets on which they are hacking away. So, you would like to like not have spent 10 years off your life gaining intuition and in neural network architectures in order to come up with great performing state of the art beating neural architecture on your own. It seems not just the very unsatisfying not the right way to make the field progress. So ideally, we would like to just do things automatically. I should be able to do like the dream is let's say I were to come up with ImageNet I'd give you ImageNet or some some big dataset that you care about. You want to just throw it on let's say Azure and say, come back in two days and we will give you the state of the art architecture. Right or not even two days, like one day I like, or maybe you set a budget like, I'm willing to spend $10,000 in compute and this is a very important dataset for me or my client and I want the best performance on validation or held-out test set in once this $10,000 runs out. In fact, there are competitions now that are being run which are simply models like this where it's like if you give what is the best architecture or how well can you do in a $100 worth of Cloud Compute. As you can imagine this is a very important problem, because the vast majority of the world do not hold PhDs in deep learning Neural architecture hacking or have not spent like 10 years building a lot of intuition so if you want To be able to truly democratize all of this and also find good principles of how should that table be generated without having to just write things by hand, which seems very inefficient. So, that's why this field or subtopic has become quite important and to show you how important and hard this has become, I am going to show you this webpage maintained by Frank Carter and his group. This is a very good website by the way, if you want to keep up with AutoML stuff, AutoML.org, they're doing an excellent job. So, this is the following list of papers related to Neural Architecture Search, and these guys are doing a lot of work just to compile every paper that's coming up on archive. Not every paper or not all papers are as of good quality and then some of those papers are much more serious than others. I highly recommend reading this survey. I'll put it in the slides. It's a good way to get a quick introduction into what's actually happening. Hopefully, we'll take away a lot more from this class as well. But, what I want you to take off is, this survey came out in August and we are like middle of October. Already there are three more papers and if you look at this list in 2018, like in the first 10 months of this year and this list is going on and on and on. Let's see. Yes, it ends here. Right. So, in 2018, Neural Architecture Search has really taken off and in fact, if you look at 2017, 2017 is big but not as big as 2018, and 2016 is even smaller and 2015 is downright, there were just two papers. Right. Then 2012 nothing happened before that and then 2009, 2008 to et cetera. Right. So, 2018 seems to be the year for Neural Architecture Search and I'm not surprised because, hey! We're actually starting to do really well on many tasks individually with our individual combined experiences and expertise and armies of Researchers and Engineers. So, there's this big effort now to automate this. So, let's see if I can get my cursor back. Got it. Yes, and if you want to not read the full list, which you should not, is go read the survey article. It is very well written and I highly recommend it. So, before we go and dive, I'm going to talk about four or five papers today which are representative ones and as it happens in almost any field, many of these techniques have Epsilen differences across papers just like, oh! I did it this way and this other paper just twists that and does it this way. They'll find that it gives a little bit of benefit here and there. So, what I have tried to do in this class is actually pick four papers which are like different techniques. Think of them as the principal axis of different techniques. Two of them will use policy gradient methods and then the rest will not. Right, then the other three will not. But the first really serious papers, all of them actually came from Google, [inaudible] group and who were trying this from 2016 onward and they all used reinforcement learning. Some of them also used evolutionary search. So, let's give you a very quick primer on what I really mean by reinforcement learning, which would be relevant to this class. Now, obviously reinforcement learning is a topic of its own. People spend their lifetimes on it doing research. So, I'm not going to be able to cover anything almost in 15 minutes that I have reserved for this, but I want to just give you enough background so that you can appreciate why are the techniques that are being techniques, why were they chosen, why if they may not be the best or maybe they are the best, we just have not got the right formulation, but all the initial results, serious results came from using Model-Free RA. Okay. So, we will do a quick digression into slides that I have. Let's see. I think there's a way to make this full screen view. Perfect. So, this is a friend of mine from my grad school days. He gives a really good lecture, so I stole some of his slides, but all the work was done by Jeff. For these slides, you must have heard like people who have maybe at least heard about reinforcement learning or planning of the storm like Markov decision process. Right. So, Markov was this Russian mathematician who really formalized a lot of the groundwork on which reinforcement learning is based and planning method are based. So, what is a Markov Chain? Right. So, these are very simple constructs, but they are very powerful. So, a Markov Chain has a finite number of discrete states. Let's say in this simple example, you are at S1 and S2 are your only two states, we have probabilistic transitions which you can think as the more general form of deterministic transitions. For example, if I am at S1, then with some probability 0.9, I will spontaneously moved to S2 with probability 0.1, I remain at S1 and similarly for S2 with point 0.8, I might go to S1 and remain on S2. Right. Then this Markov property is that in order for you to know what is the likelihood of you ending up at any other state, all you need to know is what state you are currently in. Right, and you don't need to know history of previous states. It doesn't matter how much time you have spent at S1 or S2 before, as long as you know what state you are in right now, which is either S1 or S2, it completely determines the chances of where you are going to be next. Right, and that is often called as the Markov property. Not a lot of things in the world are Markovian, right. For example, weather is not Markovian, our stock market is definitely not Markovian. Right, like you just knowing the stock price today doesn't tell you anything a whole lot about stock prices tomorrow. In order to actually to have any chance of doing good modelling, you want to know the history and probably even more information. Right. Similarly with weather. But, it turns out that a lot of things for which Markov assumptions hold, admit really efficient algorithms. Then there's this thing called Hidden Markov Model. What Hidden Markov Model is exactly the same as Markov Chain, except you don't know what state you are in, like the state is hidden from you. For example, a good example that HMM get used a lot in for formalism and to model things is, speech to text translation. So imagine, all you get to observe is the audio and the computer's task is to translate this from the sounds waves that are coming in from picked up by your microphone to text. So, you don't actually get to see what were the words that were being spoken, you have to somehow infer them from this observations that you are getting. Right. There's this new thing you will see which is called O. For example, if you are at S1, for example, if you were actually saying the word. Let's say Tuesday. You actually don't get to see that what the word Tuesday is. You have to infer that that is your task, but you get to hear the sequence of sound or audio waves that the microphone gets and hence you actually don't know whether you are in S1, all you get to see is O1 then you get to see, I don't know redness there, so you get to see O2, and you have to then figure out from the sequence of observations you are getting what was the sequence of states that you might have been in. It turns out that there are algorithms which are very much like dynamic programming which gives you what is the most likely path sequence of states you wouldn't be in given the sequence of observations you saw. In order to solve HMMs. That was immediate. But again, note that both in HMMs and Markov chain there are no actions. State transitions are beyond your control. When you would get speech, you get the speech somebody spoke. You don't get to control what is the next word based on where you might be. MDPs. Now, this is the fun stuff. The MDPs have everything. So, they have states and in this example I'm again, showing discrete states, but in all of this continuous states are just as fine. You can represent continuous states and you have actions here. So, if I'm in S1 and if I take action A1 with point three probability I go back to S1 and with point seven probability I go to S2. Similarly, for S2 so on and so forth. You have this new concept called rewards. So, rewards are if I'm in state S1 and take action A1 and I were to move to S2, I might get some money, some credit. For example, if this is a video game, if S2 is the state that is the winning state. Somehow if I'm, for example, I don't know, chess. Chess has a lot of states, Go for example, has a lot of states and my my aim is to somehow meander through state-space by taking actions. Where actions are I get to move my chess pieces and end up in the state where I have the subset of states where I captured the king of the opposing party. So, that would be the state I want to go. If I were to be able to somehow successfully achieved that before my adversary, I get a big reward which is I got to win the game. So, and this is what you would see in reinforcement learning is that there's this aim of maximizing the reward, the transitions you get, as you move around in state-space. Sometimes you also get rewards incrementally. For example, in chess, a good surrogate intermediate reward might be how many pieces of the opposing party you are capturing. So, you might have heard of this term called sparsely rewards, winning the game all the way at the end is a very sparse reward. We might want to get some intermediate reward and that's where people come and go and get into all reward engineering, but ultimately no matter what you do, you are defining an MDP. An MDP comes up with what is your state-space, what's your action space that you can do at every state. Then you have a transition model in video games typically, for example, in chess. In the game of chess, my transitions are deterministic. If I choose to move a piece, my action is moved pawn to E4, you move the pawn to E4, it's not with some probability you move the pawn to E4. So, it's deterministic. It just means with probability one that action will succeed. There are many cases in which things can be probabilistic. We'll get into that later, but yes. So, you have transitions and then you have rewards for transition, and all of the states are visible to you. You get to know which state you are in. Like in chess you know what is the state. It is the relative position of all the pieces on the board. That's all the information you need and usually it's Markovian. It doesn't matter how you arrived at the current position in order to win what matters is what you are going to do next. So, you'll probably see your game-playing policy does not have to depend upon history which is very nice because, otherwise, your policy tends to become very hard to find because your history your state-space will blow up. So, let's see. Then we have the partially observable MDP where it's the same as MDP except now you don't know which state you're in. Again, you will find you have this tinkered observations and observations are only, for example, chess is not a PAMDP it's an MDP, [inaudible] is not to PAMDP it's an MDP. A good example of PAMDP is for example, is a robot motion planning for localization of, let's say, self-driving cars. When you're in downtown Seattle you might find that your GPS becomes very noisy or do ellipse uncertainty becomes very big, you are between buildings GPS doesn't work. So, your location is no longer directly observable. But you might have a belief. You might have an idea based on, "Oh, I saw the Starbucks, I see Capitol Hill signs that," so, because that's your observations, that's all you can see. Your GPS is now pretty useless, but based on what you are seeing you may reach out your observations. You may be able to have some belief over what states or what set of sequence of states you might be in. You might be like, "Oh, if I see the Space Needle and that on my left I must be in this neighborhood and whatnot". So, this is, again, you look at your observations try to figure out what state you're in, and then if you are given the belief of what state you are in or states you maybe in, you try to plan your path to go to your friend's house, for example. So, PAMDPs in general, are much, much harder to solve than MDPs. Usually, you can't solve more than 12 problems in a principled manner. So, that's why we have job security. Lots of research goes here. Yes. Here's a very helpful chart. So, depending upon audio states completely observable and whether you have actions or not, you either have a Markov chain, HMM, MDP, or PAMDP. If nothing else take this away from this class this will help you out a lot. If you're not reinforcement learning or planning experts, this comes out as very handy to have a mental model. Okay. So now, I will leave that on the board. So let me see, if I have this. So we're going to focus on MDPs. An MDP is defined as, in order to define a particular MDP, you have to define the state space, the action space, the transition, probabilities between states, and the reward function. Right? So this, and usually for completeness, you might have like S naught which is our probability of S naught which is what is the stock state you are going to start and it may be a single state not if maybe a probability distribution over a bunch of states. So if I define this I have defined an MDP. What does it mean? Let me see, for people who are maybe familiar with planning, what does it mean to say that solving an MDP? Take a guess. You might hear this word a lot like, we have to solve this MDP. What do you think it means?  Maximize the reward.  Absolutely. Right? You want to come up with a policy where a policy is there are two kinds of, there are four kinds of policies, but in our policy is a mapping. The simplest policy is a mapping from states to action. What I want is, give me policy pi which I'm going to just use the symbol pi which is very common notation in reinforcement learning literature which says, if I'm in a current state S, what should I do? To do what? To maximize my expected reward where when you take the policy pi, so let's say you start out at S naught. Let's say you took action one then and let's you have like, let me just define an action space, you get to go up down left right, and I don't know your playing some Tetris, or some like grid search, like room navigation game, but there's a pot of gold hidden away, and you have to figure out what is the path I should take in order to go there, right? Like a shortest path problem so which which is exactly an MDP. Now, I am going to take action A1, I will get some get some reward, then I will let say I will hit S1, then my policy might say, "hey what am I doing here?" I am going to take action A1 again, I might go to S2, I might take A0 this time so on and so forth right? So this is basically like, I'm going to call this the squiggly, there's a more mathematical name for it which I never can pronounce. So this is essentially a trajectory. So I have a policy which I lay out from S naught and I go all the way, like I play it out and let say I play it out for n steps or some horizon let's say capital H until I reach the goal, or time, or am like, stop that's my budget, and that gives me a particular trajectory, and every time I do a transition, I get to see your reward. My aim in solving this MDP is to come up with the policy such that, the sum of all the rewards is maximized. Right? That, give me a policy pi such that if I were to take these actions according to pi so at S naught, I do pi of S naught. Pi of S naught says," Hey, you should take A1 here". Take A1 at S1, I do pi of S1, and it gives me this. Right? My aim is to maximize A1 to H. So pi would be argmax over let say I have some parameters theta off my policy which is what I'm writing here, argmax over theta, whatever theta star I get, the pi star corresponding to this is the policy which maximizes it. Now, there are various ways, what I have actually done is jumped a lot ahead and said my policy is parametrized by some parameters theta, and these theta are usually also like in deep reinforcement learning, the theta comes from the parameters of your neural network, and in linear policies, you may have a linear regression, a classifier if you're doing discrete number of action steps which all it is saying is that, give me the theta such that, if I were to play that policy out, I will maximize this. Right? So usually these, for example shortest path problems are a special case of solving MDPs. What is your MDP there? Your MDP there is like let's say the great world of states and actions are what how you can move between them, let's say you can go left down or upright and you want to minimize the cost of going from a start state to the end state which can also be posed as maximizing the negative of that cost, and you do dynamic programming, and in indeed solving MDP is the optimal way to do it is to solve big giant dynamic programming problems. But there's usually a big issue with that. Go ahead.  What is T here?  Oh! T is the transition.  I mean in this particular grid research.  Grid search. Yes. It's we find out that particular, it's basically a table which says that, if you are at state S1 and you take action A1 which is like let's say go up. Right? Let say this is my grid, I am here then T basically tells me if T of S1 and this let's say is called S1, and if I were to take action A0, what is the next state S prime that I will end up at? In this one, if I take the action up, I go up, if I go here so on and so forth. T just defines me where I go, my dynamics.  But in some problems, there could be several states you can go. Right? So in probabilities.  Yes. So the one I'm showing you here is a deterministic state transition probability that if I take A naught, you go to S prime. But it could be like you actually get probability of S prime or distribution. Right? For example, in robotics it often happens that if I give a command to the robot, the robot doesn't execute that exactly, instead of actually going all the way straight up, it kind of sort of goes up, so you actually end up in different states. So let's see. Okay I'm already at 11. The thing that happens is, and this is a very common confusion whenever I'm interviewing interns for summer internships at Amazure, I often ask them this very simple question that, okay, define me an MDP. They all define me the MDP just fine. If they are in reinforcement learning background, and I ask them, tell me what makes a problem a reinforcement learning problem. Like what is missing in or do which makes, so the shortest paths problem that I described to you, is a planning problem. Right? I can just do dynamic programming in a discrete space, you can also do it in continuous space which is a detail. But I do dynamic programming and I can solve it. So then what happens when, I why RL? Why does the entire field of RL exist? Like who can tell me what Is missing in an MDP to make it an RL problem?  Possibly feedback.  Sorry.  Feedback.  Feedback? Can you tell me more.  Basically, you receive some sort of feedback after you make an action, after you make a transition, and based on that feedback, you take the next action or take another transition in trying to increase, I mean trying to maximize the rewards.  Okay. So the intermediate reward, you can get that in MDPs as well. Right? So the reward is known in an MDP setting. Right? So for example, if you are making progress towards your end state in the shortest path problem, you can get an intermediate reward. Right? And also because you know exactly where you are, you know that. Right? It's usually something else. Reward is usually not the problem, although that was a good guess. There's not a lot of symbols there, so by process of elimination, there are three left.  E is not known.  Sorry.  E is not known, transitions.  Yes, that's right. So, this thing, if you don't know what what T is, then it's a reinforcement learning problem. So, think about Chess or AlphaGo, everybody has seen Go and AlphaGo, and people have been deep mind used their deep reinforcement learning, but doesn't matter as much right now as a reinforcement learning. Who can explain what is not known, how the T is not known there?  Because the state is a state of the board. There aren't so many states.  So many states is fine, it just means that my state space is really, really gigantic, right? But how is the transition not known?  [inaudible] the rewards exactly.  You know the reward.  I know, opponent moves, right?  Exactly, like when I take an action, let's say you are white and you put a dot on the board and you took an action and you got to see the next state of the board, but what you don't know is what black will play, right? Because you don't know that, that makes it like you don't know transition, you don't know the exact state of the sequence of states that will evolve result as an effect of what you did, right? That's why it's no longer a planning problem and that becomes a reinforcement learning problem. All the reinforcement learning problems have this flavor of interacting with a simulator. In the case of Go, the board is the simulator itself, which is very nice and which means that if I can solve my play in the simulator, then I can come up with a good policy in the simulator, I have actually solved the real world problem. It's just because so happens that video games like Atari, Go, Chess, et cetera, all these things have the world is what you have defined it to be. Yes?  Yes, but analogy with chess is not really a precise. You see, more than Chess game, you use Alpha-beta, so I know the transition. Before I move, I do Alpha-beta and I find the possible responses. In fact, my Alpha would be influenced by that.  Hold on. I don't think I understand the Alpha Beta part.  The Alpha-beta. So, there's a RL in the modern like AlphaGo for instance. But it's only tip of the iceberg. It's old machine learning in the heart of it, it's Alpha-beta search meaning that you make a move and you look what the possible response would be, up front.  Like Monte Carlo Tree Search.  A certain depth.  Okay. Yes, yes.  So, for your estimate, in fact, your own reward.  Yes.  Clearly, you have some heuristics and for others that's called strategy, how you define those very finite number of moves, and then you get. RL is using all the full optimization of this process, not more than that. In other words, Chess is not RL problem.  I would disagree because you still don't know the transition.  I know you disagree.  Yes, but whether you formulate it as an RL problem or not is a separate issue, and actually, I will show in Neural Architecture Search that it should not be formulated as an RL problem, it's just that a lot of people did, right? If I assume that my Chess, I have the state, actions, transitions, and rewards, and say that I don't know the transition because I don't know what thing is, then I can do RL, right? Even in AlphaGo, for example, you do the value function estimation by doing every time before you have to play, you simulate forward many, many steps, right? Then, the neural network is guiding you to cut down, for example, your branching factor.  Excellent depth, let's be specific, the question was, what makes a problem an RL problem?  I would argue that Chess is absolutely an RL problem.  Well, this is exactly we disagree.  I think this is pretty binary, actually, I don't think this is a philosophical debate.  It's not a philosophical debate, it seems pretty binary.  Yes, yes.  It's not.  I'm pretty sure it is.  Okay.  So, okay, so now that we have at least got a flavor of what makes a problem a reinforcement learning problem, now, we are going to go across one method very quickly. I don't know how much time I will have to fully derive this. So, I'm going to write down. Remember, we want to maximize the full reward of what policies that as they go across, what is my cumulative reward? I want to maximize that, and I'm going to write it down as J Theta, right? I'm going to not use discounting terms is my sum of my RI, T is equal to one to my big horizon. I want to maximize that. My policy goes from S to A. I'm going to have a stochastic policy and policy gradients, where instead of going from states to a particular action, I will go from state to a probability distribution over [inaudible]. What I would want to do is take the gradient of my cumulative rewards, and then P where I'm going to use squiggly for my trajectories, R of squiggly. I'm going to use another pen. Where R of Squiggly is? Remember, I go from S naught, I take some action, then I go to S1, then I take another action, so on and so forth, and then I'm just going to sum up all the actions I receive that I'm going to is this big R over here. So, why do I want to take gradient of this? Because so that I can do gradient descent in the parameter space of my policy and until I come up with our gradient ascent, until I come up with the policy which is really good, right? Which is like a parameterization, for example, if I'm using a neural network, then my neural network gives me really good rewards when I use that to do policy. So I can then rewrite this as gradient of Theta, sum over R Tau, where I'm just using this is the space of all possible trajectories that you could take through your state space. Now, this seems really bad because the set of all possible trajectories is exponential in my horizon, and then I might have a very large state space, but things are not as bad as they look because this term, probability of you taking a trajectory given your current parameterization of your policy, can be written down as. Okay. So, again, we return down as this, right? It is just like I've gone through a particular trajectory, and I am writing down that what is the probability that I'm in the beginning state times the probability that I will take action A-naught given that time in S-naught, given my current policy and then I keep multiplying it times multiple probability that I will be in S-naught given that I took action a-naught and S-naught. So on and so forth. Then, this thing can be written down as just this term. Okay. I can move now this term inside, like this gradient with respect to Theta, and you will see that this will come out as sum over all T gradient of Theta probability of this given Theta times R-squiggly. I can just move that inside because the other term will be 0, and then it is gradient of Tau. I'm going to multiply and divide by- Right? I'm just multiplying and dividing by this quantity. Then, okay I'm going to use this. I can take this summation and this thing out and then just write it as an expectation. Again, I'll probably just squiggly you and Theta. So, this is just like rewriting it again as expectation. Then, you'll see that because I'm writing this term, look at this term. Gradient of probability of a trajectory divided by the probability of the trajectory, this I can just write it as log of probability, because if I take gradient of this then it will give me this term back which is a really nice trick, because once you have written it as in terms of log, I can then expand this out. Remember a probability of this given this is this term. So, if I take log of a bunch of things which are products, I get additions which is very nice because most of these, it'll come across as like this, log of sum over all the timesteps, b of St given as St minus one, at minus one plus sum over t log Pi at, St times R of squiggly. Which is just you see I'm collecting all these terms separately and then collecting these other terms separately. If you see, if I put the gradient term inside, none of these depend upon Theta. All of these will go to zero, and only this depends upon Theta. So, if I rewrite everything a little bit nicely, I'll get, that Theta is [inaudible]. There are variants of this depending upon how you write it. This is called policy gradient theorem. This basically says how you are going to take gradient off your policy with respect to your parameters of your policy Theta. And remember your policy has to be stochastic such that you do well, it's gradient with respect to this reward function, but all you need to see is that in order to compute this take gradient with respect to Theta of your policy parametrization, which is very nice. One very nice thing to note here is that you're not taking derivatives of your state distribution. This comes in super handy, which is very nice that the math works out that way. If you keep that in mind. I actually haven't given you an algorithm, I just told you how to take gradients. If I know how to take gradients, then Theta-new is basically like my Theta-old. I can come up with basically a gradient descent style thing plus some step size Alpha times gradient with respect to Theta J-Theta. I can do this and keep on iterating over this until I'm sick of it, and I converge or I find a good policy. So now, rather than wondering we haven't done actually any neural architecture search up to now, but don't worry. If you understand the policy gradient theorem, the first two papers are 10 minutes. We'll leave that thing over there. Here's some studying material, we'll go over this, back. So, I want to begin with this one. So, what happens here is you can pose Neural Architecture Search. It may not be wise to do this, but this is what happened in 2016 as Neural Architecture Search was posed as a policy gradient problem, like it was an RL problem. So, let's go through the state space. Your state space is the space of all possible Neural Architectures or you define some state space and all the possible architectures that it admits is the space. So, every S is now a particular architecture. Now, your actions are basically you get to select. You get to move from one state to another, so you can transition arbitrarily almost, except that you don't know what actions would result in you getting good rewards. To make it even harder, so you know the reward function very well. The reward function is "I want you to give me ultimately a good architecture that results in very good, let's say accuracy on the dataset I care about." I know that for a fact. So, if I pause my problem as policy gradient reinforcement learning, then I'm going to have a policy, and my policy here is going to be, they're calling the controller which is an RNN. What this controller will do, is it will sample an architecture A with probability P. Remember, why is it sampling architecture? Because our policy is probabilistic. We need a stochastic policy to make the policy gradient theorem work. So, that's why people are like, "Okay we're going to sample an architecture. We're going to sample an architecture with probability P, we are going to train a child network with that architecture all the way, do your regular neural network training until you see your accuracy or negative of your errors and then you compute the gradient. " Now the question becomes, "How am I going to compute this gradient which is here? Which I gave it here because I have that expectation term and let me-" Okay. So, what this paper has done is basically written out that math in again, but instead of expectations they have replaced it with the sample estimates, right? So, this is what we are trying to maximize. So, instead of squiggly they have written it in this, and just makes it, it takes more space. I want to come up with parameters, theta such that my expected reward is maximized off my trajectories that I will take, and this is that same derivation of how it actually came about to be, this is also the algorithms that this admit called reinforced, and there are many variants of reinforce today, and since I can't evaluate this expectation exactly, I'm going to turn it into a sampling problem, I'm going to just sample, and this is the sample estimate. Right? So, what I'm going to do is, I'm going to use my controller to sample architectures, train all those architectures, then go see what the rewards I got from that, plug that back in here, right? Where your K equals one to M are basically all you're things that you have been sampling, this is just one by M to get the average, which is your sample estimate of that expectation, and then take a gradient step off the RNN and this I know how to do. Why do I know how to do? Because I can do back prop. Right? I know how to take gradients of my parameters in a neural network. So, I'm going to replace. So I have my controller is going to be an RNN, which is going to sample architectures. Now, the question becomes, how are you going to sample architectures from an RNN? So, this is where the domain expertise comes in. It is not actually that hard. I'm going to unroll the RNN, and at each step of the RNN, I'm going to sample in sequence like as I'm unrolling the RNN, the filter height, the filter width, stride height, and stride width of a convolutional layer. So basically, a layer to definition is given by this. Right? Like so this is one, two, three, four, five. These five numbers define me a convolutional layer, like what is the height, width, what is my stride height, width, and the number of filters it will have. So, if I can sample this, then the next five unrolling will be another layer. Right? So, I can stack up like, okay, I sample, I run the RNN let's say 20 times, so I get a 20 layer neural architecture. Right? And then I can take the 20 layer neural architecture, go and train it on another GPU, see the reward I got back, comeback and plug it into my policy gradient theorem, like estimate by doing sample estimates because I'll have many of these, and then take a gradient step the on the RNN, the controller that is happening. That's exactly what they did, they exposed neural architecture such as this reinforcement learning problem, and in order to help speed things up, this is where all the engineering comes in, you are going to have the parameters of your RNN are going to be saved in like many parallel parameter servers such that your controllers have many replicas. All this you need to do is because the hardest part in all of this, the most time consuming part is coming up with many samples to get this expectation. Remember, our sample here means sampling an architecture, training it all the way to the end. So, for example, if I give you ImageNet, this will take you three days. Right? So, one sample is extremely expensive. So, hence if you have 8000 GPUs, and your manager is okay with you burning for $20 million on that, then go do this. Right? Because it has this big take, and run this. This is all the engineering you need to do to have any hope of getting enough samples and doing this. Then there is some engineering that you need to do in order to reintroduce skip connections. Right? Because this is important again because this gets a little bit tricky is, we know that on images at least on ImageNet C410, resonate and dense net style architectures which have skip connection tend to do well. So, if you design a search space which does not admit skip connections like in the first one we saw, then that is going to be you know that you are leaving a lot of like human expertise out, we know skip connections help, really they dramatically speed things up and all lots of good stuff happen. So I'm going to introduce the sixth unrolling where which we are going to call an anchor point whose aim is just to sample if you are in layer N, which of the N minus one layers to connect to, right? So and that's something you just sample the skip connections. So this defined- now you are back into dense neural space. So, all possible skip connections to things before you are now a part of your search space. So, this is where search space hacking comes into play because if you make your search space too big, you are not going to, your search space is gigantic you're not going to able to search everything. Then when you are doing- so a lot of- okay, now there's more hacking needed in order to make sure that networks that you are sampling are valid networks. Remember, just because I come up with a few numbers, because all the RNN is doing is sampling five to six tuple or five tuple at a time to define layers and connections, but those things may not actually work together. For example, in layer N, if I say that the number of filters and the stride height width is certain it expects a certain tensor size to go in, right? Because and if that doesn't go in, it just because the layer N minus one is completely different shape and just not compatible, you can't do the matrix multiplication, right? So in order to make things actually work, these are the hacks you do. Like one of the hacks is you must have skip connections, they are concatenated along the depth dimension, that way you don't have to worry about, if for example if you wanted to do summation this won't work, it might happen that you have like tensors which are completely different shapes, how do you sum them? Then if a layer is- but if you're like just concatenating it works out just fine no matter what you have as long as you pad everything to make the concatenation go through, right? Like if something small comes and something big is there, you just pad everything by zeros, make it the same size, concatenate them so that now you have a nice big tensor and then you feed that forward. Okay. So, you do all this and you use a lot of electricity, and you come up with, so this is on C410, so the C410 is this image data set, it gets used a lot in all these NAS work because unlike ImageNet, it's much smaller than ImageNet, so you actually, remember all the evaluations that you were doing? Like sampling architectures and training them and then taking the gradient step usually can be done in let's say half a day if you have very fast GPUs and you architect them well, right? So you don't have to wait for four days for ImageNet, right? So, yes, various variance of their algorithm, they come up with good results, right? Like 3.65 which is pretty at that time was good, and these are the number of parameters that the architecture that they found which performed well had. Okay. Then they also did something on language modeling task. Just to show that our method doesn't just apply to images it can work on Penn Treebank which is language data set. I'm not NLP experts so I can't tell you a lot about it but they show at least that they have very- so in here they have this term called perplexity, think of it as a surrogate for accuracy and lower is better and they achieve actually one of the lowest. This was great, right? So at that time, this actually started off this Neural Architecture Search, the massive amount of work, the exponential gradient we are seeing in this number of papers and work that is being done here because they are like, there was suddenly by just doing automatically complete search, yes with a lot of GPUs, we can find actually human beating architectures automatically. This is great news because that means I don't have to have 10-15 years of Neural Architecture hacking experience in order to develop intuition. Okay. But one of the things that actually the details are in the paper is they used actually like almost 8000 GPUs running for months and that's at least what they reported and that would mean I will take up all of Philly for those who use Philly right now, it only has 10,000 as of today, right? So, this will be hard for me to do even if all of Philly was dedicated to me for six months, right? So. Fortunately, things have become much, much better and efficient. So there was this other paper that came out in February this year, the first versions and is called Efficient Neural Architecture Search or ENAS and it achieves actually much. So this beats like the first NAS paper that we saw. These numbers in the first NAS papers were around 3.6, 3.74. This is much lower on CIFAR-10. On Penn Treebank, it's like the previous NAS had 62.4, and this is the best part. One single Nvidia 1080Ti GPU, such takes less than 16 hours, for them it was thousands of GPU hours. So this is like almost three magnitudes reduction in such time. So this is okay. Now, with one desktop GPU, I can actually achieve better results than what they could do before. What is the trick? The trick is that I'm going to share computation along all my experiments. So, okay. If you noticed in the previous work, what was the big time-consuming step? The big time-consuming step is not actually doing that computation once you have all the rewards and the samples of all your rewards coming in, because that's so very tiny step, updating the controller takes like probably a minute or two. The big time's things was training all those things in parallel to completion to get your samples. Like I sampled one, send it off to a computer, go train, sample another architecture, go train, sample another architecture, go train. Now all these things take a lot of time and a lot of compute. So this is why their number of GPU hours is massive, and because everyone is doing forward prop and back prop independent of each other. Now, this idea will sound like barbaric and totally crazy, but it somehow works here, is that what if I don't send everything off and run them independently. What if I have this big giant computational graph and I sample from that big giant computational graph, but share all the weights, and do all the training and backdrop for- so basically imagine- so this is a toy example. So let's say this is my big computational graph and this basically defines my search space and I'm going to say that I'm going to sample this like this red sect of arrows is one network. Then- so that I'll put outside and I'll put it over here. Then let's say I come up with a blue set of arrows, which may be like this. Some other sample from this, that's my another network and so on and so forth. Then, but, instead of training them all on different computers with different GPUs, I'm going to train them all together and just share the weights. So, for example, if this edge is common between the two sampled architectures, they will actually share the same tensor. They will not share the same parameters, sorry, not the tensors. They will not actually be completely independent, but this seems very bad. Because you should be like- but, if I were to actually train them separately, the set of weights that I will get be connecting one and two maybe useful only to that architecture because it has a certain topography, and it will specialize and it should not be very useful to this, and this is why like if you were to just suggest this to me, I would be like that sounds crazy. That is going to make everybody really suboptimal because the things should specialize for their architecture, but somehow that's not true. Then this was like a big head scratching moment where- and okay, they don't have proofs, theoretical proofs here, but they showed that like, hey, our motivation is basically multitask learning like with the neural network, how you can give it many different tasks and it tends to generalize better if you make her do multiple tasks at the same time, and based on those multitask motivations, we started along this path and we find that we can actually find- would do very well. So, for example, like here is how the dude like the sampling, and here again, they're still doing policy gradient methods. They still have a controller which is RNN. But like the layered- so this is like for recurrent neural networks. This is how a particular cell looks like in them. This is the search space. Coming in and you have all these operation options and they're showing like what got sampled, and you can read the details. The details are really boring, but yes. So, for example, if you want to design convolutional neural networks automatically, then you can- you fix this skeleton and outer structure and you can sample what layers each should be doing this. This is the other trick that they also did is that instead of us- okay, there's this term called macro versus micro you will hear a lot when you go across NAS papers. What macro means is, search means, I give you nothing. You start with like a very small network, and I'm not going to enforce any topology order. Like you can grow the network. You can do whatever additions, subtractions you want. You can sample from this big giant search space. Any arbitrarily shaped big network and that's fine, and we'll like search in that space. In order to make things a little bit more tractable, what people started doing is called micro search. So they fix- so for example, oh no, I dropped it. Okay, I'll use it this way. For example, okay. Remember resonant? Resonant seems like a good idea. What if I define my search space to be like this, that hey, image comes in, I have this is- I'm going to fix the resonant architecture, the outer skeleton. This will be my macro architecture. I'm going to fix it. I'm not going to let this be a degree of freedom, and then what I'm going to say is that what should go here? All I'm allowing the network to, like this whole sampling business search procedure to do is tell me what kind of layer I should have here, what could you have here, here, here, so on and so forth, and in order to make things even more tractable, all I'm asking is that, I must say that all of these will have the same layer, but what that layer type should be, whatever layer that should be. I'm sampling here, I'm going to replicate it exactly here. So, these are all ways of reducing the search space. But notice, notice that I am- I in order to do this micro or cell search because you are just searching the cell and you are fixing the outer skeleton design fixed based on something good that some human expert found. You are relying on the fact that there is a human expert who has given you a good macro. Like if for images, like in a lot of research has been done for language. Lot of research has been done, so you may have good macro architectures that you can focus on. Whereas if I give you a completely new data set even or maybe like data sets in different domains for which there is not a lot of prior work, you may not even know what is the right macro to begin with. So there is this like in order to make things tractable, there is human domain knowledge injection into a lot of NAS papers to make the search space good. Good, meaning like we know that this kind of thing works, so what is the arg max within the scope of these kind of things. That's what a lot of papers do. So they use that trick, they use the trick we're going to share parameters, we are not going to run everything on different components, and notice as- okay. They come up with like- so this is called ENAS, various tips and tricks like cut out and in order to come to that number, but they're extremely efficient, right? Okay. So this is for example, this is a discovered network. ENAS is also able to do like in a Macro, so they show this is their Macro one result, and I don't know, I would have never hand designed this architecture by myself. This is actually, does really well on a lot of image classification tasks. Notice how the skip connections are all over the place. Why they would be is just something that the automatic search procedure found good. These are cells like remember, the cells thing. These are weird cells that are discovered automatically. Again, I don't think if you had asked me to design one, I would have ever come up with one, such wonky ones. Okay. So, these two papers were reinforcement learning based, right? So, where they basically said, we are going to frame this as an interaction problem. We don't know what the transition is going to be and our rewards are differentiable. So, we are going to have this, rewards are not differentiable because I don't know how to differentiate reward with respect to the parameters of the controller. I only can differentiate my reward with respect to the individual thing, the networks that I'm going to sample, so I need to use policy gradients. That's how these papers came to be. Now, there are a vast amount of papers which actually don't use reinforcement learning or specifically policy gradient. So, one is called Progressive Neural Architecture Search. I'm going to skip a lot of the details because we are going to run out of time, and give you the intuition, where the intuition is I'm going to grow "Networks". There is no Controller, there is no RL, but what I'm going to do is I'm going to start from a small set of, these are parent networks which may be like almost one or two layers. I'm going to evaluate them, see which one of them do well. I may add then based on which ones do well, I may expand them. So, this is almost like will remind you of Genetic Algorithms or Revolutionary Style Algorithms based on, if they are good parents, then the ones which did well, the children of them might be promising, we're going to only then see which children did well, then take only a case subset for the next set of parents for the next generation. Then again expand and then again, cut it down, all the non-promising one, keep only the promising ones, and at each step like these kind of steps, I'm just adding layers, right? I'm sampling layers and adding them, and remember all the big time goes in Neural Architecture Search in like especially, okay, I have all these like networks here. Each dot is of specific Network Architecture. Training them takes a lot of time. So, what they said is, we're going to have this "Predictor" which is going to try to predict. By just looking at the parents performance and the mutation that was added in order to create the children, what the performance can be like without training them. Now this becomes like this with the chicken and egg problem. So the Algorithm looks very much like, you train this for a little bit, gather a little bit of data, train another predictor it could be another Neural Network or a tiny Neural Network, which predicts how well these things would do based on their little amount of training we have done on them, and past history of other fully trained network's they had, and take only the top K. That's how you big tick the top K without fully training them. That's what saves you a lot of time. It's a very simple procedure and compared to like NASNet, which was the first paper and whatnot. Then, so, this is progressive NASNet. They do well, their search time is much lower. It's an idea, I wouldn't get hung up on the numbers too much. They also tried on ImageNet. They don't really search on ImageNet, what everybody does is, they search on CIFAR, they find a good architecture, and then they just add, replicate all the cells. They make the network bigger, and then they train it on ImageNet. So, that's the standard trick because doing the search directly on ImageNet is going to take you a lot of time. Okay. So, I'm going to take three more minutes and run through DARTS because this is my favorite paper so far. This is under review at [inaudible] right now. The reason I know it's under review, because [inaudible] if you know how the conference works. It's completely open review, except the authors that are anonymized. This is a really cool paper. No controllers, no intermediate performance prediction which has problems because you can diverge. It outperforms everything else before it as far as I can tell. It achieves the state of the art on CIFAR-10, and as well as on, I think Penn Treebank in a single GPU day. Very nice and this is where not casting problems blindly as RL problems really helps. The trick here is, I'm going to do a continuous relaxation of a discreet optimization problem. Remember, my aim is to somehow go through the space of Neural Network Architectures and find a good one. Neural Network Architectures are discrete, they may be similar to each other, but it's not continuous space. Like, if this is one architecture, the architecture next to it differs from it in let's say a particular layer or a particular parameterization of some layers or whatnot, and even it's hard to define what does "Next" mean, right? Like it may mean small added distances. Here, what we are going to do is imagine this is like a cell, and again their DARTS mostly does cell. I don't think they do macro as far as I could tell. I am going to try to figure out within this cell, that okay, if there are four things I am going to have in my cell, I'm going to try to first, what should be the operations that connect this cell. I'm not going to make an assumption of sampling from this and from all possible operations coming up with certain cell architectures and then evaluate them by training them. I'm going to assume that all these operations can co-exist. So, this is kind of the mother cell. What I'm then going to do is, there are weights, then put weights on these operations. That how likely is it that one and three should be connected by a convolution layer, or by a pulling layer, or by a reduction layer or something. Then I'm going to put Alpha one, Alpha two, Alpha three weights over them. This gives me another graph, so which is the continuous relaxation of this discrete derivative problem otherwise I would have. Then, I'm going to run this Bi-level Object of Optimization. I want to find good Alphas, remember if an Alpha's weight is very good, that means this green line is thicker, if the Alpha's weight is small, then it means that this probably is not the right operation to connect between these two. What I'm going to do then is first run an optimization over the Alpha. Like Randomly initialize the weights, do back prop over Alphas only, holding all the other weights constant, and see which ones start to win out, and this is just a visualization of these Alphas. Then I'm going to stop my optimization over Alpha, and then depending upon which ones have whatever weight, optimize over the actual Neural Network Architectures that are admitted by this particular instantiation of Alpha. So, this is an architecture except is that, I'm going to connect one to three very strongly via pulling, very little by let's say a Convolution Layer, and very little by a Reduction Layer. I'm keeping all possibilities on the table at all times except that the weightage gives me which one is more likely than the others. Given this architecture, then I'm going to actually optimize for the parameter values inside it, and then I will go back and forth. I will do a little bit of Alpha then a little bit of the weights, little bit of Alpha little bit of the weights, so on and so forth until one of these Alphas starts to become like really matter a lot, and then I'm going to take that out as the ArgMax architecture. So, the nice part about this is that you don't have a separate controller to train via doing policy gradient things. All the architectures search is amortized like the forward and backward propagation operations are amortized across all the architectures all at once, which is very nice. Similar to ENAS tricks, but even nicer because you are keeping all possible architectures all at the same time. Then at the end, you are just taking the ArgMax out. So basically, this is what the algorithm looks like, don't worry, is that update the weights and by descending down, like to hold Alpha constant, update the weights and this is just your standard back prop operations. Then you update the architecture Alpha by descending down the space of Alphas, hold W constant and which is again just back prop, but on this metagraph. Then at the end once you have found which one is the, you replace all the edges by just the one which is the ArgMax, the thickest line that remains. If you see, they really have some of the best results, in just four days of GPU search time oppose they get to 2.83. ENAS is pretty comparable, but I think the final numbers are better on this one, even though ENAS just searched 0.5 days instead of four days here, but so far I think this is the best method because the numbers are just really, really good on everything and you only need a single GPU. There are already extensions on DARTS, that I'm seeing people start to work on. In general, it's a good idea. Do not do RL, look at the numbers for RL, the search cost which is what should really worry you, is not just the final numbers on CIFAR, this is burning all affiliate dedicated to you, for six months. That's the kind of thing you are looking at and that's just for like CIFAR, and just not a very good way of finding architectures. Then there are many open problems like all the current papers, these show either CIFAR 10, 100, ImageNet, vision or one NLP dataset, RV is the entire field converging to a few datasets, and then there's also this subspace hacking by assuming that we have a good macro, and going around doing cell search in order to make the search space tractable. So, what happens when you don't know what is the right macro architecture? You can't do micro anymore. Here's some teaser results from our group, where we actually grow neural networks by adding layers. There's lots of tricks we use to make this happen, including for people who were at the exploit day yesterday, we use a lot of exploit trick algorithms, we directly search something called the Pareto Front, which is as much accuracy, you can get lower in terms of parameters, if you plot that, that gives you a part of Pareto Front. So, we searched around the Pareto Front as we are training, and we show that we can come up with extremely, ours is macro over there with small amount of search, pretty good numbers. We are much more general than even DARTS is doing. DARTS is doing micro, we are doing macro, and we are able to get good performance compared to DARTS and all the other Google Papers. These are teasers, we are still running lots of experiments, but yes, happy to take more questions, we are overtime, but luckily we don't have a class right away. Well, I think my take away has been by just looking at NAS, is that the days of human domain experts, like doing a lot of engineering and trial and error, are soon going to be over, and should be over. Like we should not be sitting there, like okay maybe I should replace this layer with this; send the job out, come back in two days, see what happens. It's just very unsatisfying especially as we start using neural networks everywhere for new datasets and tasks.  So, the methods that use parameter sharing, do you know like how do they compare to the ones that do not do shrank for sharing in terms of performance? Are they actually losing anything by sharing at all?  No, that's the surprising thing right. They actually don't lose, they do better. If you have DARTS you can assume is doing parameter sharing, because all the networks are there, in NAS, the efficient NAS is doing parameter sharing, they actually do better and their such time is like 100 x less. Which is the super counter intuitive result, but if you believe their results, and I believe they are good people that that's true.  Is it possible that the span of like sharing and overlaying on top of multiple architectures is kind of reinforcing some signal in the architecture, that gives you more confidence that this is the right one?  Yes definitely. Because if certain kinds of rate values, like parameter values, because one way to think about is like all these different architectures at enforcing regularization on the same common edges. They're not letting the edges over-fit to the different architectures if you had gotten, if you had trained them independently, and so there is the multitask regularization affects are coming into being, and because these weights have to do well across all the architectures that share that edge. So, it might be actually helping, I don't even have to do a lot less regularization by hand. And also I don't want to give the impression that this is all magic and look everyone should just implement DARTS, and we are done. In order to get those numbers, there's a lot of domain engineering that goes on, like you know the reason if you saw in those tables like cutout this with path augmentation and drop path, this is all tips and tricks that only if you go and look at their Python notebook, you will find, okay this is not as simple as just fit and then come back, you have to do all these tricks to get those numbers. But that's just like researchers who are oh mine is better than yours, getting into that game, right? Yes.  You might have mentioned this already, but what is it exactly about DARTS again that allows it to train so quickly?  The fact that implicitly sharing all the parameters, and the weights on architectures. So, they have this continuous relaxation of this discrete search space, so any given like subgraph is a specific architecture, but they are all kept. They are all kept all at the same time. It's not that you sample from this train, and then come back. Everybody is kept all at the same time and you do this like alternating minimization over the architecture parameters, and the parameters over the network rates that admits them. Yup it's the sharing that makes it. Cool, feel free, I'm in the same company so feel free to ask me, email me questions and whatnot, and I think the slides are already up on the SharePoint. So, you can get the slides, that survey paper is very nice. If you are interested, it gives a lot of open problems out. It is very honest I like that, yes. 