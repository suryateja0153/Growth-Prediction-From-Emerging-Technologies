 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu my topic today is they're discussing sensory representations in deep cortex like architectures I would say the the topic is perhaps toward a theory of sensory representations in in deep networks as you will see the our attempt is to develop a systematic theoretical understanding of the capacity and limitations of architectures of that of that type the general context is well known in many sensory systems we see information is propagating from from the periphery like the retina to visual primary visual cortex and then across many stages up to a very high level or maybe the hippocampal structure it's not purely feed-forward there are backward massive backward or top-down connections that I can't connections and some of those extra features I'll talk about but the most intuitive feature of that is simply transformation or filtering of data across multiple stages similarly in auditory pathway in other systems we see a similar structure or aspect of similar structure as well a well known and a classical formative system for computational science is bellum where you have information coming from the mossy fiber layer then expand anonymously into a granule layer and then converge to a poke poking yourself so if you look at a single poke in G sell the output of the cerebellum as a unit then you see there is a first and expansion from one thousand to two orders of magnitude or higher in a granule layer and then convergence of two hundred thousands or so parallel fibers onto a single poking poking yourself and there are those type of modules many many across the cerebellum so again a transformation which involves in in this case expansion and then convergence in the basal ganglia which is not which I wouldn't categorize it as a sensory system nevertheless more data to motor nevertheless you see cortex converging first to various stages of the basal ganglia and then expanding again in to cortex hippocampus has also multiple pathways but some of them include a convergence for instance convergence to see a three and then expansion again to cortex but there are other multiple pathways as well different stages of sensory information propagating cause across them okay and finally the artificial network story of deep neural networks you all of you may have heard input layer then sequence of stages purely feed-forward and at least the canonical leading leading network is one that the output layer has object recognition object classification task and the whole network is studied by back prop and supervised learning for that what I'll talk about is the more in in the spirit of the idea that the first stages are more general-purpose than the specific a classic task in the output layer okay so there are many issues number of stages that are require the size of them why compression or expansion in many system you see that the fraction of active neurons is is small in the expanded layer that's what we call sparseness so high sparseness means small number of neurons activate any given stimulus it's just terminologies somewhat confusing so highs partners is small number of active neurons one important and crucial questions is how to transform what are the filters the weights that are good for transforming sensor information from one layer to another and in particular whether random weights is good enough or maybe even even optimal in some sense or all one needs most structure the more learned a type of synaptic wave this is a crucial question perhaps not for machine learning but for computational science because there is some experimental evidence for at least of some of the systems that are that are studied that the mapping from the compressed original representation to the space representation is is actually done by randomly connected weight so one example is all facto a cortex the mapping from of a factory representation from the olfactory bulb so come gloom ever lose layer to the period from cortex seems to be random as far as one can say similarly in the cerebellum the example that I mentioned before when one looks at the mapping from the mossy fiber to the granule cell again enormous expansion by a few orders of magnitude nevertheless it they seem to be random now of course one cannot say exclude exclusively that they're random they're not subtle correlations or structures but nevertheless there is a strong motivation to ask whether random projections are good enough and if not what does it mean structure what kind of what kind of structure is is appropriate for this task question of top-down and feedback loops recurrent connections and so on so that all I hope to at least briefly mention later on my talk okay so before I continue most of our large part of my work of the of the talk will be based on published and unpublished work with dr. Abadi who was until recently a postdoctoral a sports fellow at Harvard University when to do practice medicine any a Franklin master student at Hebrew University so you know who all you know here or at Harvard or Akoni PhD student at Hebrew University and and then we from Penn University okay so here is our formalization of the problem we have an input layer the note is 0 typically it's it's a it's a small it's a compressed layer with dense representation so here every input will generate maybe half of the population on average then there is a feed-forward layer of synaptic weights which expand to a higher dimension layer which we call cortical layer it's expanded in terms of the number of neurons so this will be s 1 it is fast because the F the fraction of neurons that are active for each given input vector will be small so it is expanded and spouse that was the that will be the first part of my talk discussing this then a later on I'll talk about staging cascading this transformation to several stages and ultimately there is a readout will be some classification task our one will be one classification our two will be another classification rule etc each one of them with synaptic weight which I'll learn perform that task okay so that's so we call that a supervised layer and that's the unsupervised layer okay so that's a formalization of the problem and as you will see we'll make enormous Lea simplifying abstraction of the real biological system in order to try to gain some insight about the computational capacity of such systems okay so the first important question is what is the statistics the statistical structure of the input so the input is kind of n dimensional vector n or n 0 when n is the number of units here so each sense of event evokes a pattern of activity here but what is the structure that the statistical structure that we are working with and the simplest one to that we are going to discuss is the following so we assume basically that the input are coming from a mixture of so to speak a mixture of Gaussian statistics it's not going to be Gaussian because for simplicity we'll assume they're binary but this doesn't matter actually so imagine that this is kind of a graphical representation at or of high dimensional space and imagine each in the inputs the sensory inputs imagine that there are a clustered around templates or cluster centers in so this would be the centers of this of these balls and the the input itself is coming from the neighborhoods of those templates so each input will be one point in this in this space and it will be originating from one of those and some of one of those states so that's a simple simple architecture and we are going so in in real space nothing it will be mapping from one of those states into another state in the next layer and then and then finally the tasks will be to take imagine that some of those balls are classified as plus let's say though the olfactory stimuli and some of them are classified as a appetitive some of them as aversive so the output layer the the readout unit has to classify some of those spheres as a class and some of them are - and of course depending on the how many of them are any dimensionality and their location this may or may not be an easy problem so for instance here here it's fine there is a linear classifier on the input space can do it here I think that should be some mistakes in yeah yeah so here is in a case where the the linear classifier at the input layer cannot do it and that's our debts and that's a theme which is very popular both in computational science in and system neuroscience studies in machine learning and the following the following question comes up suppose we see that there is a transformation of data from let's a photo photoreceptor layer in vision to the ganglion cells at the output of the retina then to cortex and in several stages how do we gauge how do we assess what is the what is the advantage for the brain to transform information from one let's say from retina to v1 and so on so forth basically after all in this feed forward architecture no net information is generated at the next layer so if known if not information is generated the question is what what did we gain by these transformations and one one possible answer is that it is reformatted we live formatting the sensory representation in two different representation which will make subsequent computations simpler so what does it mean subset computation subsequent computations simpler one notion of simplicity is whether subsequent computation can be realized by a simple linear linear readout so that's that's the that's the strategy that we are going to to adopt here and this is to ask as the information as the representation is changing as you go from one layer to another how well a linear readout will be able to perform the task okay so that's the input that's the story and another said there is an input unsupervised representations and supervised at the end okay okay now I mean I need to put two into those notations bear with me this is a computational talk I cannot just talk about ideas because the the whole thing is to be able to actually come up with a quantitative theory that test ideas so let me let me introduce notations so at the the centers at each layer you can ask what is the representation of the centers of this stimuli and I'll denote the center by a bar okay and me U is one mu is index of the patterns some u goes from 1 to P P is the number of those balls number of those fields or number of those clusters if you think about clustering some sensory data so P will be the number of clusters I from 1 to n is simply the neuron of the unit activation its mu and n is the layer so 0 is the input layer that's up to Lal ok so this would be 0 1 the mean activation at each lair from from one on will just kept to be constant two BFF goes from zero to one the smaller F is the sponsor the representation is we will assume that the input representation is dense of this is 0.5 and again will assume to be constant PCT constant across layer except for the first level there is expansion you can vary those parameters and actually the theory accommodates variations of those but that's the simplest architecture you expand and dense representation into a sparse higher dimension and you keep doing it as you go along okay so that's notation okay now I how do how do we assess what is what is the next stage is doing to those clusters okay so as I said one measure is take a linear classifier and see how Lin a classifier performs okay but actually you can you can also look at the statistics of the injected sensory stimuli at each layer and learn something from it and basically I'm going to suggest looking at the two major statistical aspects of the data as each layer of the transformation one of them is noise and one of them is correlation so what is what is noise so again noise will be simply the the radius or measure of the radius of this of this field so if you had only the templates as input the problem would be simple problem would be easy as long as we have enough dimension you expand it you can easily do linear classifier and an end and solve the problem so the problem in our case is the fact that the input is actually the infinite number of inputs or exponentially large number of possible inputs because they all come from from from Gaussian or binarized version of a Gaussian noise around the templates and at the node the noise by Delta zero means no noise one the normalization is such that one means that they are random so Delta equals to one means that basically you cannot tell of the input whether it's coming from here or from any other points in the input space the other thing is correlations is more subtle so I'm going to assume that those those balls are coming from kind of uniform distribution imagine you take a template here you draw a ball around it take a template to draw but everything is kind of uniform distribution you distributed the only structure is the fact that data comes from this mixture of gaussians or noisy patterns around or centers okay so that's fine but as you project project this project those those clusters into the next stage I claim that those centers those templates get new representation which can actually have structure in them simply by the fact that you you you put all of them into this common synaptic weights into the next layer and I'm going to measure this by hue and basically long q0q is basically a kind of randomly uniformly distributed centers and I'll always start from that at the input layer but then there is a danger or it might happen that as you propagate this information or this representation to the next layer the centers will look like that or older than the data structure data look like that so on average you know the distance between two centers on average is the same as here but they are clumped together it's kind of random clustering of the clusters and that's can be induced by the fact that the data is feed-forward from and from from this representation that can pose a problem if there is no no is then there is again no no problem you can you can differentiate between and so on but if there is noise this can aggravate the situation because some of the clusters become may become dangerously close to each other and will come to it but anyway so we have this Delta the noise the size of the of the clusters and we have queued the correlations how they are clumped into in each representation and now we can ask how Delta evolved when you go from one representation to another how you evolve from one representation to another and how linear classifier performance will change from one representation to another so the simplicity of this of this assumption is allows for allows for kind of a systematic analytical exploration or study of these ok so ok these are definitions let's go ok so what will be the ideal situation that this situation would be ok so the ideal situation will be that I start from some level of noise which is the my Gauss my-my-my-my spheres at the input layer I may or may not start from with some correlation the simplest case would be that I start from randomly distributed center so this would be zero and the best situation will be that as I Papa gate the sensory stimuli Delta the noise will go to zero okay as I said if the noise goes to zero you are left with basically points and those points if there is enough dimensionality those points would be easily classifiable it also be good since if the noise doesn't go to zero to have also kind of uniformly spread clusters so it will be good to keep Q to be small okay okay so let's look at one layer so let's look at just we have the input layer the output layer here and and the readout the first question is what to choose for this input layer so the simplest answer would be choose random okay so what we do is just take Gaussian we just IID Gaussian weights in this layer a very simple with zero mean some normalization doesn't matter then we project them into each one of these guys here and then we add threshold to enforce the sparsity that that we that we want okay so whatever the the activation here is whatever the input is here here is the threshold make sure that only the F the with the largest input will be active and the rest will be zero so there is a non-linearity which is the first extremely important if you map one layer to another with a linear a linear transformation you don't you don't gain anything in terms of classification so there is a non-linearity simply threshold non-linearity after an input input projection all right so how we're going to do this so so it's straightforward to actually compute analytically what will happen to a noise so imagine you take two input vectors with some hanging distance apart from each other you map them into by by by by convolving them Toto speak with with with the Gaussian weight and then threshold in them to get some sparsity so every the sparsity the smaller the f is that the sparse on it is so this is the noise level the gun or malaises Gaussian I'm sorry reduce or Hamming distance in the in the output layer versus the input layer well if zero start then of course you started the origin if you are random in the input will be random there so these points are fine but as you see immediately there is an amplification of the noise as you go from from the input to the output right so you start from point two but you get actually after one layer two point six okay and actually for loss for the span silk is so this is a relatively high sparsity or at least you go from here to here by increasing sparsity namely f becomes smaller and as have become smaller this curve is actually steeper and steeper so not only you amplify noise but you also the amplification becomes worse thus parcel the representation is so that is the kind of negative result the idea that you can gain by expanding data to a higher dimension and make them more separable later on it dates back to David Mouse classical theory of the cerebellum but what we show here is that if you think not about clean data set of point that you want to separate but you think about the more realistic case of you have noisy data or data with high variance then then the situation is very different so a random expansion is actually amplify snows and that's that's a theme that will actually we will live with it as we go along random expansion is doing the separation of the of the templates but the problem is it also separates two nearby points within a cluster it also separates them so everything becomes separated from each other and this is why noise is is amplified now what about the most subtle thing which is the kind of overlap between the center's so on average the center's are as far as part as random things but if you look not on but you look at the individual pairs you see that there is an excess correlation so overlap between them so this is overlap between the Centers again on average it is zero but the violence is not zero on average it's like random but the valence is different is larger than range or than random and there is a there is an amplification there is a generation of the of the of the success overlap although it's it's nicely controlled by sparsity so as positive goes down the dis correlations go down okay so that's that's that's not a tremendous problem the major problem as I said is the noise okay by the way you can you can you can nicely do an exercise where you generate you look at the this cortical layer representation and you do as BM or PCA you look at the eigen value spectrum so if you if you just look at random spouse points and you look at the SVD this is the angle values and number rank rank then that's what you find is the famous mung mung gain so a store distribution but in our case you see there is an external power in this case the input layer is hundred so the extra power in the input layer in the first input eigenvalue now why is it so so so the the what Hugh is telling us what Nancy okay telling us is the following you take random set of you take a set of random points and you project them into higher dimension the start with hundred dimension and you project and in a thousand dimension on average they're random but actually so you you would imagine that that that it's a perfect thing you project them with random weights then you'll be imagine that you just created a set of random points in the expanded mention representation if this was so then if you do SVM or PCA on dit on this representation you will find what you expect from PCA of a set of random points and this is this one in fact there is a trace of low dimensionality in the data okay so so I think that's an important point which I would like to explain you start from a set of points if you don't threshold them and you just map them into one thousand dimensional space those hundred dimensional input will remain higher than 100 dimensional just before ten ten and so on but everything will live in in Han hundred dimensional space now you add threshold a high threshold being by sparsity so those 100 dimensional subspace becomes now thousand dimensional space because of the non-linearity but but this non-linearity although it takes on the dimensional input and makes them thousand dimensional it's still not like a random there is still this thousand dimensional cloud is still elongated it's not simply uniformly distributed and that this is the this is the signature that you see here in the first largest hundred eigenvalues there is extra power relative to to to the to the random okay the rest is not zero so if you look here you know this is this goes up to thousand the rest is not zero but so the system is strictly speaking thousand dimensional space but it's not random it is has increased power in hundred channels if you do a read out linear classifier read out what you find in this again in when you when you expand with a random okay with random with random weights you find that there is an optimal an optimal sparsity so this is the readout arrow for a classifier the output is a function of of the sparsity for different level of noise and you see that in the case of random weights there is a very high very very high sparsity is bad there is an optimal sparsity of sparseness and then there is a shallow increase in the error when you go to a denser representation one important point which I want to emphasize a coming from the null this let me skip equations and this is what you see here the question is can I do better by further increase the layer so here I plot the readout error as a function of the size of cortical layer can I do better can I make the if I make the kernel dimensionality infinite okay can I do better okay well you can do better if you start with zero noise but if you have noisy inputs then basically the basically the the performance saturates and that's kind of surprising we we were under the we were expected that if you go to a larger and larger representation eventually the error will go to zero but it doesn't go to zero and that actually happens even for what we call structured representation and that's the same for different types of Linda with our perceptron pseudoinverse SVM all of them show this situation as you increase the size of cortical layer and that's one of them a very important outcome of our study that when you talk about noisy inputs you can think about it as kind of more generalization tasks then there is the limit about what you gain by expanding representation if even if you expand in a nonlinear fashion and you increase the dimensionality you cannot combat the noise at least up to some level beyond some level there is no point of further expansion because basically the arrow saturates okay let me let me since times goal goes fast let me talk about the eternity so if random weights are not doing so well what are the alternatives that anything is to do some kind of unsupervised learning here we are doing it in a kind of a shortcut of unsupervised learning okay what is the shortcut we said the following imagine that this layers the learner knows about the representation of the clusters it doesn't know the labels in other words whether those are pluses and those are managers which one are pluses and minuses but it does know about the statistical structure of the input and this is this s s bar these are the Centers okay so we want to encode the statistical structure of the of this input in this expansion of the of the weight and the way we do the simplest way is the kind of head rule we do the following we say let's first choose a recruit or allocate a state a spawn state here randomly chosen to associate a to represent each one of the clusters so these are the are the randomly chosen patterns here and then we associate between those randomly chosen representations and in the end the actual Centers of the classes at the input so this is s bar and R and then we do the Association by the simple what's called hey bro so this happy enroll Associate Justice Center with randomly assign a state in the in the cortical layer in in kind of simple summation without the product for the hypodermis sofisticated ways to do it but that's the simplest one of doing so it turns out that this simple rule is has enormous potential for suppressing noise so it's again this is the input noise than the output noise the humming this distance of the input and the output properly normalized and you see that as you go to higher and higher sparseness toulon low F this is basically the the input noise is completely quenched whenever an F is large when F is 0.01 for instance this is this already stably when at point of five is here and then and so on so forth so sparse representation in particular are very effective in suppressing noise but provided the inputs have kind of unsupervised learning encoded into them which embed into them the cluster structure of the input okay the same or similar thing is true for Q for these correlations if you look at the this was was a random correlation and this is a function of F and this is Q the correlation it's a stoical is extremely suppressed for for sparse representation basically it is exponentially small with one over F so it's basically zero for sparse representation which means that those centres look like randomly distributed essentially and with very small noise so you took this spheres and you basically map them into random points with a very small radius okay so it's not surprising that in this case the error for small for small F the error even for large very loud noise value with aro is basically small as zero nevertheless it is still saturating is function of the network side of the cortical size okay so the saturation of performance as a function of cortical size is the general property of such systems nevertheless the performance itself for any given size is extremely extremely impressive I would say when the system is passed and nose level is kind of moderate okay okay let me skip this because I don't have time let me briefly talk about extension D of this story to two multi-layer so we are now briefly discussing what happens if you take this story and you just propagate it as you go along the along the architecture so let's start with random eighths so the idea is maybe something is good happening although initially performance was poor maybe we can improve the performance by cascading such layers and the answer is no in particularly the noise level this is now the number of layers go to discuss before is here and you see the problem becomes worse and worse as you continue to propagate those signals the lowest the noise is amplified and essentially goes to one so basically you'll get just random performance if you keep doing it with random waves and the reason is very the reason the reason is basically that if you think about the mapping from one layer of noise tune to lab tune another layer of noise there are two fixed point and zero and one the zero fixed point is unstable everything goes eventually to one so it's it is a nice I think it's a nice this system gives you a nice perspective about this a distant deep network by thinking about is a kind of dynamical system for instance what is the level of noise at one there and how it is how it's related to the level of noise it previously so it's connect iterative map Delta n versus Delta n minus 1 and what's good about it this once you kind of draw this curve what one layer is mapped to another layer you can know what happens to a deep network because just iterate this you have to find what are the fixed points and which one is stable which one is not in this case the one is stable the zero is unstable so unfortunately from any level of noise that you'll start you eventually go to two one okay correlations is similar story but I and the arrow will go to 2.5 okay so that's very bad okay there are cases by the way that you can find parameters where initially you improve like here but then eventually it will it will go to 0.5 now if we do similar if we compare this to what happened to the structure wait if you keep doing the same kind of unsupervised hebbian learning from one layer to another and and I'll skip the details you see the opposite so here are parameters parameter value in which one stage of expansion stage is is actually increasing the noise and this is because F is small is not is not too small and the load is large and the noise is starting so you can have such iteration but even in such situation eventually the system goes into into stages where the noise basically goes to zero and if you compare the the story why it is so two kind of iterative met in my picture you see that the picture is very different you have one fixed point at zero you have one fixed point at one you have intermediate fixed point at a high value but this is an unstable fixed point at both of them a stable fixed point so if you start from even from large values of noise eventually you will iterate to zero so it does buy you to actually go into several stages of of this deep network to make sure that the noise is suppressed to zero similarly from the correlations even if the parameters are such that initially correlations are increased and you can find parameters like that eventually correlations will go to almost zero okay and this is comparison of the grid out MO as a function of the layers we structured weights and I compare it with the readout error of infinity wide layer kind of a colonel with infinitely white colonel and you can see that that at least for here I compare the same type of unsupervised learning but two different architecture one is deep network architecture another one is shallow architecture infinitely wide and I'm not claiming that we can show that there is no kernel or shallow architecture which will do better but I'm saying if we compare the same learning rule but with it but with the two different architecture you find that you do gain by going into multiple stages of non-linearity then by using an infinitely wide layer ok I'll skip this I want to go briefly to to two more issues one issue is the recurrent networks why recurrent networks are the primary reason is that in each one of those stages that I have a fail to if you look at the biology on most of them maybe not all of them but most of them indefinitely in neocortex you'll find massive recurrent or lateral interactions between each one of the layers so again we would like to ask what is the what is the what is the computational advantage of having this recurrent layer now in in our case we had an extra motivation and this is remember that I started in saying that in some cases there is experimental evidence that the initial projection is random so that we asked ourselves what happens if we do this if we start from random projection feed-forward projection and then add recurrent connections think about it as from the or factory bomb for instance to perform cortex perhaps random fitful projections but then the Association recurrent connections in period from cortex our structure how do we do that we start we imagine starting from random generating a presentation initial presentation by the random projection and then stabilizing those of presentation into attractors by the recurrent connections and that actually works pretty well it's not it's not it's not the optimal architecture but it's pretty well for instance noise which is initially increased by the random projections will quenched by convergence two attractor and similarly Q will not go to zero but will not continue growing but we'll go to intimately and and the arrow is is pretty well so if you look at the in this case the arrow really goes down to very low values but now it's not layers now it is the number of iterations of the recurrent connections so you start from just input layer or put project random projection and then you iterate the dynamics and it goes to zero so it's not the layers it's just the dynamics of the I conversion to attractor okay my final point I have three from four minutes okay my final points are before wrapping up is the question of top-down so we can't move really talked about it but incorporating contextual knowledge is a major question how can you improve on deep networks by incorporating not simply the feed-forward sensory input but other sort of knowledge about this particular stimulus and it's important that we are not talking about knowledge about the statistics of the input which can be incorporated into the learning of the feed-forward one but we're talking about inputs which our knowledge which we have now on the network which already has learned whatever it has learned so we have a mature Network whatever the architecture is we have a sensory input gross feed-forward and now we have additional information about context for instance that we want to incorporate with the sensory input to to improve the performance so how do we do that it that's it's a nut it turns out to be non-trivial a computational problem it is very straightforward to do it in in Bayesian framework where you simply update the prior of what the sensory input is by this contextual information but but if you want to implement it in a network you find that it's not it's not easy to to find the appropriate architecture so I just briefly talked about I talked about how we do it so so imagine you have again this sensory input but now there is some context different context and and imagine you have an information that the input is coming from that particular set of a part of state space so basically the question is how to amplify selectively a specific set of states in a distributed representation so usually when you talk about attention or gating on questions like that we think about okay we have these neurons we suppress those and amp or maybe amplify other ones or we have a set of axons or pathways we suppress those and amplify those but what about a representation which is more distributed where you have to really suppress states rather than no.11 populations so I just want one go again it's a complicated architecture but basically we're using some some sort of a mixed representation where we take the sensory input and categorical or contextual input mix them non-linearity use them to clean it and and propagate this so it's a more complicated architecture but it works beautifully let me show you here an example you have a flavor of what we are doing so now the input we have we have those 900 spheres or templates but they are organized - with 30 categories and 30 tokens per category okay now the the tokens which are the actual sensory inputs are represented but let's say 200 neurons and you have a small number of neurons representing category maybe 20s enough so that's important that you don't have to really expand dramatically the representation okay so this is the input and now we have very noisy inputs if you look at the readout this is layers here and there is read out arrow if you do it on the input layer or any subsequent layer here but without top-down information we structured interactions and all that I told you this is such a noisy input where the performance is basically 0.5 okay there is nothing that you can do without top-down information in this network you can ask what what will be the performance if you have an ideal observer that looks at the in noisy input and makes maximum likelihood maximum likelihood categorization well then it will do much better also not zero so these are this level okay this is this higher error is in virtue of the fact that this network is still not doing what an optimal maximum likelihood observer will do okay so this is the network this is a maximum likelihood without both of them without extra top-down information and in the network that I kind of hinted about if you add this top-down information by generating mixed representations you get the performance which is really dramatic dramatically improves and as you keep doing it one layer from another you really get a very very nice performance okay so maybe did summarize the day's gonna the whole the whole there's one more before summarizing yeah okay before that okay so to two points to bear in mind one of them is that what I discussed to you today relies on on on assuming either random comparing random projection to unsupervised learning of a very simple type of kind of hedging type the output can be happier nor perceptron or SVM and so on you could ask what happens if you use learning rules more sophisticated learning goals for the unsupervised wait some of them we've studied but anyway that's something which is important to to explore and and another very important issue for thinking about object recognition in vision and and in other real-life problem is input statistics because what we assumed is a very simple mixture of Gaussian model so you can think about the task of the network is to take they're in violation away from the center this spherical valuation and to generate a presentation which is environed to that but this is a very simple invent the invariance was simply restricted to this simple geometric structures more problems which are closer to what real-life problems are will have inputs which are essentially have some structure but the structure can be a variety of shapes each one of them correspond to an object or cluster or a manifold representing an entity a perceptual entity but how you go from this nice simple problem of this spherical environment to those problems it's of course a challenging problem and that's the work which we are now ongoing work also with us with so Eun Chun and then Lee but it's a story which is still at the stage of unfolding you 