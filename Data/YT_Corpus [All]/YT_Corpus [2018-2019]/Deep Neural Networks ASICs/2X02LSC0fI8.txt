 Hello, everyone. Good afternoon. Welcome to the AI Infrastructure and the two-hour session. This session we will have two talks. One is about backend optimizer for deep learning computation. Another one is for AI print form. So, I'm Jilong Xue from Microsoft Research, Asia. So, I will start first talk. It's glad to introduce our recent project called the Wolong: A Backend Optimizer for Deep Learning Computation. In recent years, deep learning has gained many success in the areas such as vision and speech and NLP and so on. Along with this success, there are many emerging innovations. In one side is a deep learning side are continuously inventing new models and workload patterns. For example, beyond the RNN and CNN, the typical models, there are also new workload patterns like GAN, reinforcement learning and graph neural networks. So, in other sides of the hardware vendors are actually inventing new accelerators for speed up the computation. For example, beyond the CPU, GPU and FPGA, there are some more and more new basic accelerators and high-speed network links. So, using this situations, there were deep learning frameworks were proposed. For example, like Tensorflow, NTK, Caffe2, for user to easily write a DN program, and the computer on different hardware devices. Typically, these frameworks epsilon and deep learning computation as a dataflow graph, the node, it's work is to permit operators and the edge represents the tensor data. So, using dataflow graph as a programming IR, you do get efficient, expensive and programming flexibility. However, it also brings the challenge of how to effectively do graph computation over different devices. So, we think a systematic way is to leverage our compiler and optimize our infrastructure to bridge the IR framework and the hardware. This is because compilers can combine both the information from application pattern and the hardware feature to make a best match. As it has a global view of the dataflow graph, so the compiler and optimizer can also optimize for distributed computation. Thus, such infrastructure is critical for both the training and inference scenario. Based on this observation, we started our research project one year ago. So, we proposed a system called the Wolong, which is optimizer stack for deep learning computation. The motivation of Wolong is to adjust deep learning performance organization problem in a systematic way. Specifically, we're going to leverage the general graph organization, passes and the leveraging the principle for software and hardware co-design and just-in-time compiler. The right figure is the architecture of Wolong. Wolong has three optimizer module; the global optimizer, local optimizer and tensor placement and optimizer. So, the global optimizer has a global view of the computation graph and conducts a distributed organization through graph partitioning and leverage of the hardware features. The local optimizer mainly optimize the subgraph after the graph partitioning for a specific device level using the JIT compiler, and the tensor placement optimizer mainly optimize for tensor layout and replacement. All these optimizations, Wolong designing automatic and transparent way. So, Wolong generate target user, optimizes the communication and the execution and the memory efficiencies. So, first let's look at a case of global optimizer. The subtitle is a fast distributed deep learning computation over RDMA. As I introduced it before, a deep learning computation is modeled as a dataflow graph. To execute the data flow graph in distributed servers. So, DL framework first need to partition the graph and the dispatch each sub-graph to corresponding servers. After partitioning, F0 user end go across two servers. We need to insert a pair of standard and the receiving nodes. So, these parallel of node are responsible for a real data transmission during the computation. Worth to mention, this kind of mechanism is generally to express both modal parallelism, the data-parallel pattern, which are a common way to scale deep learning computation in distributed servers. In such a general framework, the communication has to leverage a general communication library. However, in most deep learning scenarios, the tensor transmission in such a framework brings significant overhead, which becomes the major bottleneck for scalability. To demonstrate this, we use a simple micro benchmark, we just send a tensor with different side from one server to another server. This figure measures the latency of different tensor size. As we can see, you measure the same network, there is up to 10x latency gap between the general RPC library and Nobel optimizer of the program which carefully leverage RDMA. So, to understand the overhead of RPC, let's go to a little bit more details. We observed that the major overhead of RPC is caused by memory copy, and it is inherently unavoidable for such a general library. This is because the general IPS is designed for dynamic data structure and it's a lack of knowledge about the actual data placement and the message size. As illustrated in the right figure, a message is sent from sender has to be first copied from application memory to the RPC managed memory, and then serialize into a larger buffer to better leverage the batch efficiency. When it comes to memory, disorientation and memory copy also happened in the receiver side. To adjust this overhand, we cannot leverage such a general library. Instead, we should use a right time optimizer, which can dynamically decide the memory placement according to the specific computation workload and the net network topology to completely remove the memory overhead. Just in our distributed graph optimization, we combine both the properties are computed in a graph and RDMA network feature. First, for the tensor structure, unlike the general messy type, the tensor data in deep learning computation has several special properties. First, it consists of a plain byte array with sufficiently large size often from several kilobytes to megabytes. Due to this, we actually do not require data serialization and deserialization in this transformation. Due to excess pattern of tensors already sequential, we also do not require extra batching, but all this information can only be decided in the right time. So, second, with RDMA hardware and the protocol, we can manage both the local and distributed memory in a unified view. We can do efficient the memory copy between host memory through the one side RDMA and read and write primitives, and efficient memory copy between device memory to host memory with GPU direct RDMA. This global graph optimizer for distributed computation is a systematic way to address scalability of the other work, this is because the first global optimizer account has entire view of and the control of the memory placement among the devices servers. Second, it is capable of making global optimizer strategy for this tensor data placement in the right time. So, based on these observations, we designed a new communication mechanism in our optimizer for distributed computation over RDMA. In this mechanism, it has two phases; the graph analyzing phase and the graph execution phase. First, before the graph is executed, there is a graph analyzing phase. In the sender side, our optimizer first detect physical memory address of the sending tensor, and it tells the information to the tensor manager to reallocate it as the RDMA compatible memory. So, this can dynamically avoid the memory copy in the sender side. Similarly, in the receiver side, as your graph is analyzing, we can know the receiving tensor side beforehand. So, we can pre-allocate the receiving tensor as RDMA compatible memory, to avoid the receiver side in the memory copy overhead. So, after the graph analyzing phase, in the execution phase, when the sender know there is ready to send and source tensor, it can just issue a one-sided RDMA right command. RDMA unique will directly copy the source tensor to test the tensor's memory. In the receiver side, since the RDMA didn't have the notification mechanism, so the receiver operator will continuously pulling off a megabyte until the value turned to one, which will mean the whole tensor data has already arrived. So, it just simply passes a reference of tensor to the downside operators. As we can see, this communication mechanism of neural optimizer, we can completely remove the memory copy overhead in the both sender side and the receiver side, and most importantly without affecting the usability. So, this is the performance evaluation for our optimizer, the communication mechanism on both deep learning frameworks and a real training workload. So, our experiments are conducting a cluster with eight silvers to evaluate the performance we implement our optimizer in TensorFlow. The left figure shows the training throughput of TensorFlow and our optimizer version, our set of deep learning benchmarks including RN, CN, and the fully-connected work. As we can see with our transparent organization, in same hardware environment, the performance can be improved from 1.8 times to 8.1 times. We also demonstrated the benefits our end-to-end sequence to sequence translation model using public data sets which is showing in the right figure. The y-axis is the perplexity which measures the convergence quality. The lower, the better. The x-axis is the training time. As we can see with optimization, the current density can be sped up by two to three times. So, after demonstrating the benefit of our global optimizer for distributed deep learning computation, now let's look at our case of local optimizer. So, we are mainly talking about the kernel fusion for deep learning on GPU. The local optimizer is merely optimizing subgraph unless specific device. Currently we mainly target through GPU. So, as I introduced before, deep learning framework often model computation as a graph of operators. Using graph has several advantages. First it, has a good expressivity to represent arbitrary neural network structure through just composing the graph with the primitive operators. Second, it is flexible to run the graph on multiple devices or multiple servers through just a different graph partitioning strategies. However, this flexibility comes with a significant overhead. Since the typical DL model contains [inaudible] even more operators, scheduling overhead is a major bottleneck for the computation. So, to schedule this overhead, we are introducing the overhead like kernel launch overhand and the cross operator communication overhand. Also, since the primitive operator is the two fine-grade, it is hard to leverage vendors library for a specific computation workload. To demonstrate this, we compared the execution time of a 80-step LSTM model in TensorFlow and CUDNN. CUDNN is a vendor provided library from Nvidia. In TensorFlow, this model is constructed by about 1,600 operators and the framework has to schedule out of these operators in this subgraph during the computation. However, in CUDNN, since it is highly optimized for removing scheduling overhead, it can order magnitude performance improvements for the same workload. We also tried to manually fuse all the kernels into one CUDNN kernel. It shows that we can get similar performance with CUDNN. So, now let's have a high-level comparison of deep learning frameworks and the hardware specific library in terms of programming flexibility and the computation efficiency. You see this is the general challenge of encountered DL frameworks. First, the deep learning frameworks like TensorFlow, PyTorch, and the CNTK, has designed for user to easily program their deep DL models. This is has mostly embrace the flexibility and expressivity through using the graph operators to model computation. But they target less about the extreme performance of organizations. The other side, the DL libraries are designed by hardware vendors to push the extreme performance boundary. However, they are mostly hand-tuned and highly optimized for submissive sub-module, it is impossible to handle customized or new network structure, which means they have good efficiency but poor flexibility to spot arbitrary model. That's if we have a compiler that can dynamically generates a library like a code in the right time based on input data flow graph for deep learning frameworks, we can get both the flexibility of the overwork and the computation efficiency. So, under this situation, we propose the compiler stack for optimizing deep learning computation so-called Wolong. Wolong compiler takes input as a computation graph and it has two optimization layers the graph level optimizer and the target specific data compiler. The graph level organizer applies a set of target independent graph optimization passes through recovery rewriting. So, commonly sets as CSEE, constant folding, and so on. So, spatial organization in the user layer is operator batching, which can automatically batch same type of operators to better leverage the best efficiency, which I will introduce later. The second layer is the target specific JIT compiler which can fuse all the compiler subgraph into efficient kernel, commonly mainly targeted to GPU device. The organization in this optimizer include static shape inference and type inference, and static memory planning, and aggressive kernel fusion. So, here is the general execution flow of Wolong compiler. The graph level organizer works before the graph execution and the JIT compiler works during the execution. First, in the graph level organizer, when getting an input graph, it first detect some subgraph that can apply optimizations. For example, if the graph contains same type of operators, the operator batching patch will automatically merge these operators into a larger one through graph rewritting. So, after the graph optimization, we start the real execution. For a subgraph that is the first time to be executed, our compiler will conduct a shape inference and a type inference for this subgraph, as well as a static memory planning process based on the graph and the GPU features. After that, we automatically generate a fuse the kernel code for the whole subgraph and use the JIT compiler to compile the code into a new kernel. Finally we replace the original operators in the subgraph in this new compiled kernel and then execute it. So, in later iterations, if the graph and the input shapes has never been changed, it will hit the cache and directly execute it from pre-compiled kernel. Since we dynamically fused all the operators of the subgraph into a single kernel, it can remove all the scheduling overhead. In the following slides I will introduce a case of operator batching and the details of a JIT compiler. So, first, let's look at how the operator batching in graph level organizer works. Here we introduce the case of Mat multiplication merging. As we know, Mat multiplications is a very common operator in DL workloads. It has the mathematical equivalent to lose. If two Mat operator shares one same input tensor, we can concatenate the other inputs to merge these two MatMul as a larger one. As the outputs of the merged MatMul is equivalent to the concatenation of the outputs of the two original MatMul operators. Based on this issue, we can actually automatically detects this pattern in our optimizer and rewrite the graphs through Insert Concat and discrete operator to automatically merge the MatMul operators. So, after graph rewriting, we can do further optimizations. So, our tensor placement optimizer will get this information and it will track this input tensor and allocate them physically continuously across the memory so that the Concat operator can overwrite the memory copy during its computation as illustrated in the right figure. With such kind of a graph level rewriting optimization and memory placement optimization, we can transparently optimize the graph for better performance. So, after the graph optimization, let's move to the JIT Compiler part. So, in this talk we'll really focused on how to do the kernel fusion in our JIT compiler. Compared with other optimization frameworks, our goal is to conduct a very aggressive kernel fusion. Ideally, we would like to fuse the whole subgraph into a single kernel to completely remove the scheduling overhead. First, let's start from element-wise kind of fusion. This type of operators includes the binary operators like the and, sub, multiply, divide, and so on, and the unitary operators like sigmoid, and so on. For example if we want to compute a sigmoid of x1+x2, the computation graph constructed by your framework will contain two element operators and a sigmoid operator, and so in the left figure. So, the right figure this is a corresponding element-wise data dependency graph. As we can see for element-wise operators, there is no cross-elements dependency for this computation. So, we can simply assign the computation for each element to a single [inaudible] and leverage the [inaudible] register memory to shares their intermediate results. So, for this workload, the generated code look nicely in the figure. It's relatively trivial. Since the fusion and device kernel is really simple. So, many other compiler [inaudible] also support this kind of fusion. However, fusions doesn't element-wise operator. We think it's not enough because the element-wise operator only occupies a very small fraction of the other operator sets. That's our goal is to do aggressive kernel fusion to fuse arbitrary operators. To support arbitrary kernel fusion, there are some challenge we need to address. First, unlike the element-wise operator two non element-wise operator often has cross data elements dependencies. We have to carefully control the dependency for each convenience thread. Second, grade level global synchronization inquiry is a critical permitive forces kind of workload fusion. However, current GPU architecture has fundamental challenge to support a general global synchronization. Certain some communication between operator has to rely on the device memory, which may increase the memory usage. We have to carefully optimize the memory usage and the buffer allocation. To get better understand let's look at an example for fusion to continuous MatMul operator. Since each thread off the second MatMul has to read almost out the data element from other thread of the first MatMul. So, a realistic way to do the fusion is to store the results first MatMul into device memory and the second MatMul can directly read the inputs from intermediate buffer. So, the right figure is pseudocode for this kind of fusion. We can see, we insert a global barrier between the two MatMul function call. So however, the fact is in GPU architecture this code is really challenging to run. So, to understand why the challenge is happening for GPU architecture. So, to first we need to understand the- let's look at how the current DL frameworks schedule dataflow graph in GPU. So, typically the DL framework uses CPU to schedule operators and the GPU is responsible for executing each operators kernel. So, the GPU often consists of several stream processors, which can concurrently execute block task of GPU kernel. Let's say there is a graph with only two operators, MatMul and the convolution. The convolution either read the result of the MatMul. To run this graph in GPU, DL framework has to schedule this two operators one by one. For each operator, we need to explicitly split it as multiple block tasks according to the data size, and schedule them on to multiple GPU stream processors. Due to the limited number of stream processors, the blocks has to be implicitly execute in several batches like this. So then, the second operator is scheduled in a similar way. As I mentioned before, if the graph contains a lot of operators, the kernel launching and the operator scheduler will become significant overhead. So, a straightforward way to remove this overhead is fusing all the operators of several graphs into a single GPU kernel. For example, if you want to fuse the MatMul and the convolution within a single kernel so we can compose them as two-stage. The first stage do the MatMul and the second stage do the convolution. So consequently, the other block paths also include a two-stage. However, since this two kernel has cross-validated data dependencies we have to insert a barrier between the two-stages in the kernel. So, for the outer blocks it also has the- so each block has to enter a global barrier. However, as I mentioned it before, due to a limited SM resources, only the first several blocks of the kernel were real as scheduled. They will be blocked in waiting for barrier. So, as the GPU has no block contacts switch the later several blocks will never be scheduled. So, this kernel will fail to execute. So, to address these issues we proposed to use the persistent threads and the virtual block mechanism. To fuse the same two operators into a single kernel, refers to the limited number of physical blocks off the fused kernel according to the available resources so that all the blocks can be executed concurrently. Then, we treat the original blocks of each operator as a virtual block and assign them to physical blocks in a round-robin way. We implemented this through remapping their block ID in the code generation time. Then we are free to insert a barrier between the two-stage of virtual blocks since all the physical block already active. With this mechanism we can fuse all the kernel into a single GPU as long as we can generate each operators self code. So, till now to fuse multiple operators we are actually generating a statistic sequence of code for each operator. For example, for the left dataflow graph with three operators, they will be sequentially assigned to physical blocks like this. However, since the sigmoid, let's say the graph, since the sigmoid operator and the end operator are small operators and they have no data dependencies, if we use a counter DL framework they can leverage data parallelization. So, in our fusion frameworks there is some underutilization for GPU resources. To further push resource usage we support the kernel packing mechanism, which you can pack the small kernels into a more compact way during the virtual block assignment. So, we can pack the end operator into the sigmoid operators batch. In this way we can exploit the graph level paralyze and removes kernel launch overhead in the same time. Okay. So, this is a way of how to generate to fuse the kernel coding over compiler. Basically, first for each operator we need to register its kernel source code and some other meta information to our code generator. We can only fuse the operator that has been registered. For arbitrary graph to be fused, we first conduct shipping inference and the memory place planning paths and then generate the function cause for each operators. For example, for this graph we can first generate the sigmoid and then the MatMul and the final is for the real operators in this way. So, during the process we will automatically insert a barrier we need it and the packing kernel if possible. Currently, with all these spots a lot of operators we are still in the more into the framework. So, now let's look at our memory performance scale of a kernel fusion. For the RNN inference benchmark with 80-steps LSTM as we show you at the beginning. Currently, our compiler can automatically fuse and then compile the whole graph with 1,600 operators into a single GPU kernel. The right figure demonstrates the performance comparison with original TensorFlow. The bar with the title OpBatch means we only apply the operator batching optimizations in the graph layer level optimizations. The fusion bar is the final result of the kernel fusion. We can see with our compiler we can get about 10x speedup for this workload. So, in conclusion we think compiler infrastructure is critical for both cloud and the edge AI because they can automatically and transparently optimize both the distributed training and the local inference on different devices. I also think our system innovation is the key to bridge the application and diverse hardware. Since the current DFL already abstract a common IR, which means the dataflow graph IR and we can leverage this to co-design software and hardware for extreme efficiency. We also have developed a Wolong prototype to demonstrate the initial improvements. It's up to eight times speed up for training and 10x speedup on the inference workload. Thank you. That's my talk. So, do you have any questions?  They're the two protects?  Yeah. It's two parts, but it can integrate into same platform. Because Wolong is mostly target to the framework, so computation framework, and the pi is telling you on how to schedule job to the different servers. So, after pi schedule into a specific servers or GPUs so Wolong can apply its organization. So, it can be integrated into a same platform but two different system.  So, why two?  You can treat them as two modules in the same system.  Two module?  Yeah. But actually it's two different system.  Okay.  Thank you.  [inaudible].  Hold on.  Thank you.  So, the global miser part is mostly for multiple servers and the local miser is for just the single server.  Thank you.  So, when you are applying this technology to the parameter server, just for example. So, do you do any compression on the traffic between the nodes and also the RPS?  So, yeah. Currently, we are not applying the compression stuff in this work, but the compression is, I think the technique of compression is orthogonal to this technique. So, we can also apply the compressor into the transformation. It can further reduce the communication overhead. Yeah. Thank you. Okay, if there's no other question let's welcome to the next speaker.  Good afternoon. My name is Fan Yang from Systems Research Group MS, Microsoft Research Asia. Today, I'm going to introduce OpenPAI: The Open Source Initiative for AI Platform in China. So, here is the background. So, we all know that AI becomes one of the major focus and the very heated topic across academia and the industries. We believe that there is these major opportunities to democratize AI through the innovations on AI infrastructure that can lower the bar for new commerce to facilitate AI education and to speed up the AI research. Also, that could potentially help accelerate the penetration of AI technologies across industries especially for the traditional industries. So, we feel that the current status of the AI platform is still in early stage. We find that there exist a ad hoc ways to build and deploy an AI platform. It's relatively easy to build a small-scale platforms with narrowed, specific purpose. But we actually need to help the entire industry. We actually need a platform that works in different environment and application scenarios. That includes the on-premise, cloud, and even hybrid hosting environment. Also, that the AI platform not only can support a domain service speaker application like an image or video, speech, language, but also they need to support them all in one platform, and also for the AI applications in a certain vertical domain. This AI platform need to be highly compatible, need to be extensible, it has a very good manageability and efficiency. So before I dig into the details, I need to emphasize again the importance of the AI platform. The AI platform actually provide the infrastructure support for the advance of the AI technologies. It actually support the Deep Learning algorithms and frameworks, and the AI platform also should manage hardware in a easy way. A good platform can boost the innovation and the productivity of different kinds of AI research field. It allows the researchers and the AI practitioners to focus on the innovation itself, instead of the hassles of infrastructure construction, deployment, management, and optimization. A good AI platform could enable result sharing. A good platform can help build a community for mutual knowledge sharing and to help each other to leverage the previous research result, and thus can speed up the innovation. So, an open platform for AI R&D and education, we feel that it actually helped the co-development of AI innovation, education, and the platform evolution. So a good platform, we can leverage this platform to design the AI course project to perform training to grow the AI talent pool. Also, an open source, the platform can add open source that the platform can help to encourage the results sharing and for collaborative innovation. So here is the overview of the AI platform that in my mind. The underlying infrastructure layer including hardware and the various of the Deep Learning technology we have already built. Upon that, we need a management layer to manage the heterogeneous cluster for resource management and scheduling. We also, as Gino mentioned, that we also need a platform to host as a system-level intelligent and optimization. Also, we could have the platform to support Deep Learning and the intelligence exploration. Based upon the platform, you can build, share the results for practice or for education, and also the platform can petition a host that the label, the Deep Learning data. Also, you can leverage the platform and also leverage the tools for management integration and the AI lifecycle management, or also compiler optimization. The cognitive capability built on the platform, including natural language, understanding visual perception, speech and recognition, as I mentioned, across different domain. So let me focus on the middle layer, the platform to manage the heterogeneous cluster. So we open sourced a PAI called the OpenPAI, a platform for AI. So this platform is GPU cluster management infrastructure. It has the following features: First, it's open. So it's open source, it's licensed under MIT license, and it's actually operated in the open collaboration model. We actually work with various partners, I will mention the details later, and to develop and evolve the platform together. The OpenPAI is actually extensible. It not only support all the major Deep Learning frameworks, it also, I mean, extensible to support a new workload. It also supported, diversify their hardware including FPGA, GPU, and the ASIC, given that the drivers are available. OpenPAI has a modular design. So, you actually embraced the microservice design paradigm. So, every service run in a container. So this microservice choice can help us replace a certain component, PAI component are very easily, for example, if you would like to try a different storage option, you can switch from, for example, HDFS to cluster FS. PAI also have a good efficiency. It supports fine-grained the GPO scheduling, it also support high-speed network including RDMA extension. PAI also has a relatively good manageability. It actually provide a web UI for platform operator to manage and monitor the platform. It also help the Deep Learning user to manage job, a submission job, or even monitor the hardware utilization. PAI provides some fault-tolerance capability. So you don't have to worry about the failure. Also, PAI learns a lot from Microsoft's internal practice. So, we actually reproduce the production environment and the realistic industrial usage case for a Deep Learning model training scenario. Here is a high level overview of the OpenPAI architecture. The underlying is the host environment that including Cloud. Private Cloud, or on-premise cluster. That's running on top of heterogeneous hardware, GPU, CPU, FPGA, RDMA, etc. On top of that, we support, we leverage Docker and Kubernetes to perform container orchestration, and we provide tools to deploy the platform easily and also to upgrade and maintain this. Also, we have job management, resource management, etc. So, one of the key things in market as read, is that, we try to provide a standardized interface to the deep learning workload, and to the platform management tools. So, that as long as the interface are centralized, all the deep learning workloads or traditional big data workload can leverage this centralized API to run on this open pipe platform. This would ensure the portability of the workload. Meaning that once you successfully run deep learning job in one PAI cluster instance, it can apply to running on other PAI deployment as well. Also, this interface and the runtime support enabled the innovation of high level AI service like AutoML, etc. Here is a little bit more details of the our current implementation. For example, as I mentioned I use Docker, we use Kubernetes for cluster management. On top of that, for storage, our current storage option is HDFS because that is one of the most popular distributed storage solution. We use Hadoop with our own extension of GPU support for complicated GPU scheduling as well as multi-tenancy support. We now support our deep learning workload and some of the big data like Spark and we are also developing our own Auto ML, our reasons and the platforms that I will mention that briefly later on. So, with this platform, we believe we can build a open ecosystem. So, our initial engagement with China AI Community is to have a community tool with the AI platform, we can have a lot of shared resource and tools and the build the cognitive ability, so that we can help in research in AI education as well as gross talent pool. The Microsoft Research Asia now work with four major Chinese universities to have China Open AI platform and aliens. Those four universities including Peking Universities, Chinese University of Science and Technology of China, and Xi'an Jiaotong University, and Zhejiang University. I would like to provide a one case study for the schools of Information Science and Technology of USTC. This is the previous practice, they have 300 AI researchers and students, with approximately 400 GPU. Their current pain point is major about operation overhead because that those are AI scientists, they are not the infrastructure system researchers or IT professionals. So, they feel that to operate on a relatively large-scale GPU cluster, it's a lot of work, and for each students and researchers to run assertions of deep learning workload on the cluster require a lot of work. You need to make sure your version is correct, for example, you have to have the right version of TensorFlow, etc. You have to be careful not to screw up other people's environment like the corrected version of Cu-DNNeven GPU driver or something like that. As a result, that the previous cluster utilization is relatively low. So, we use OpenPAI deployment. We have, as I mentioned before, we have a web UI interface to help to monitor and manage that GPU cluster, and we actually lower the learning curve for the AI researchers to run deep learning workload. The research results can share the cross team with Docker image and the job configuration files that require different kind of resource like, for example, number of GPUs for the job. The new user like a new student when they try to learn, to try to run some workflow that they can just jump start with some pre-build Docker images. There are GPU cluster utilization improved with our scheduler mechanisms. They leveraged the platform and the ground their workload, and we are happy to see that the OpenPAI platform can help them win some of the top and prestigious award in CVPR. So, to summarize a platform, OpenPAI essentially, a platform for AI innovation. It provided much needed a playground for AI research and development. PAI actually represent a class of realistic and the production environment, where AI researchers should target that, because that, as I mentioned before, Microsoft internal production environments very much like OpenPAI. It's a hassle-free platform for non-professionals. By non-professional, I mean, IT professionals or system researcher. We have a relatively low operational overhead, and we provide a WebUI-based job in management tools, so that AI researchers can easily use the platform to run their AI workload. So, the researchers can focus on AI research itself instead of a tedious IT stuff. With the platform, we can help the results sharing and reproduce the research result. As you might know that, it's very hard for AI researchers to reproduce other people's workload because that they often require a lot of parameter tuning. So, our platform can help you with Docker image that include all the parameter you set for the paper you've published. Also, the platform can help the system research itself. Here, I'd like to give a few example of the research opportunities systems group are working on. For example, we try to explore it, to understand what is the right system primitive for deep learning scheduling in large-scale GPU scheduling. For example, in the big data area, big data jobs are expressively MapReduce task and later on they usually have a data scheduling that. Before the big data area, if you look at the distributed process, they usually, the scheduling primitives, usually timesharing migration, et cetera. So, what is the right primitive before deep learning's workload is one of our research focus. Also, we feel that because deep learning workload usually require large data and larger neural model size so as to have a relatively good accuracy. So, we feel that how to manage GPU memory more efficiently is one of the potential opportunities we can exploit. For example, we can leverage a very traditional system concept like virtual memory and some of the technology like memory compression essentially to optimize the GPU management during deep learning model chaining. We have identified several case where certain deep learning technology like the Auto ML, have reasons may have some very subtle interplay with the underlying systems cluster scheduling. How to optimize those deep learning job that can target for achieving the good models, having a good neural model instead of only speed up one specific deep learning chaining possession. It's one of the interesting research topic, and also like cross job optimization and how to identify fairness in the case of deep learning workload especially for the GPU. Those are very interesting research opportunities that the system research itself can help. So, to conclude, we have a platform that can facilitate the research on artificial intelligence. It provide an efficient and ease of use AI infrastructure that can help to put the AI research into practice, and the platform itself is open collaboration model. We actually developed the platform with the university together, and we have an open community. We start from China and we certainly welcome university and the researchers or even industry, the practitioners across the worldwide to join us to develop a platform together. Okay, thank you. So, after that I would like to spend a few slides to introduce another piece of work our team is working on which is called a Neural Network Intelligence. So here, I would like to show just a few slides.  Listen to this.  Here is the overall process of the deep learning developers daily practice. So, first, they usually can mine or search some code for certain deep learning neural model practice from GitHub or related website, and our Neural Network Intelligence platform can index those popular code and neural models, and recommend this neural network model architecture to the user. The Neural Network Intelligence also provide a lightweight open source to package for user to run their own training sessions to find a good neural model. Finally, we have visualized the tools to edit, search, and debug the neural network model intense during the Auto ML training session. We know that the the traditional Auto ML algorithm has been developed from a simple Hyper-parameter Tuning, to more advanced neural architecture search including, for example, evolutionary-based search reinforce learning-based search or other more advanced technology. Then later on, we have seen that people are starting to apply neuro-evolutional transfer learning in this kind of Neural Architecture Search. So, our NNI package actually support all these kinds of technology. Also, we provide a markup language for Search Spaces Specification. For example, those markup showed in orange, it's a way for users to specify their their choice during the search space. So we feel that using a markup language actually provide deep learning researcher a very handy tools to specify certain Neural Architectures Search. Okay. So, we are going to open source this package in maybe one or two months depending on the legal process while going through. We hope that in the future, people will use this NNI package to apply a certain Auto ML research so we can enable more rapid innovation in the Auto ML research itself. Okay. So, that is all for our talk. Thank you. Any questions? Yes.  So, I am interested in the Auto ML part, so could you tell me more about like what kind of Auto ML functionality to support, and what's the usage over consumption from the customer,and is it quickly growing feature and so on?  So, we decide to open source the NNI as a lightweight package. So, if you only have one GPU server, you can use it. If you have for example 10 GPU servers, you can use it. If you happen to have hundreds or thousands of GPU cluster that managed by open [inaudible] , you can also use it. So, it's quite flexible. The package itself does include a certain a typical or classic Auto ML algorithm. For example, a random search, grid search, or based method. But we also provide standardized interface allowing deep learning researchers to develop their own Auto ML algorithm, which is actually one of the active research topic right now. So, this is one of the target customers we try to support, AI innovator. The other part of the customer is that, some of the people actually do not have too much knowledge on deep learning itself, and they would like to, for example, try some CNN model. But in order to better leverage the CNN model, they have to do some fine tune. For example, one of the very typical changing practices is to return only the last few layers of the CNN model so as to have a better classification result. Then, we have these tools that can help you automatically. You don't have to do anything. You just leveraging some of the pre-configured package. So, this is just like a Microsoft custom vision of Google's Auto ML, except that you can run the entire package open-source and own your own cluster so you can play with. You don't have to be forced to locking into any cloud offerings. I hope that answers some of your questions.  Thank you.  [inaudible] support the FPGA?  We do not support the FPGA yet. Our design actually, taking FPGA into account but we need to verify that with the driver. So, I assume that if the driver doesn't have any problem, then we don't have any problem because that we treat GPU as just one specified co-processor, and the FPGA is no different than that. Okay. Thank you. 