 [Music] hello I'm Magnus it's them for the developer show we're here at the tensorflow debt summit there next to me I have sac stone hi Zack hi Magnus so I hear that you're the person to talk to about TPU is that correct oh there's a huge team working on TPU is all across the company but I'm definitely involved I'm really excited about bringing a lot more performance to the tensorflow community cool so can you describe what the TPU is some what you used for sure so Google has developed a system of Asics boards and entire supercomputers to accelerate performance and machine learning so we needed this for internal purposes initially there was concern several years ago that if every Android phone user spoke to their phone for a few minutes a day with algorithms of the time on big fleets of CPUs Google might need to double the number of data centers until that didn't make sense Wow right and so there's this crash program to develop a chip that was specialized to accelerate some of these machine learning workloads and so that was Google's first TPU Wow which was announced that at Google i/o I think two years ago so then last year we revealed that we have the second-generation system the TPU v2 that's also available now in cloud as a cloud TPU it's just recently gone to beta and so this supports training and inference it's 180 teraflops per device and then these devices can be connected together into these TPU pods they go up to eleven and a half pedda flops and so eleven and a half exactly let me put that in context for you so if you're training of image recognition model let's say ResNet 50 it's the sort of standard benchmark right now is state of the art not too long ago and if you want to train that to 75 or 76 percent accuracy which is what you'd expect from publications on the subject that might previously have taken you days a few years ago when that paper is published now that's down to about 12 and a half hours on one of these cloud GPUs and on the full TPU pod you can do that in 12 and less than 12 and a half minutes that's amazing yeah so I bet you all wondering out there how can I get my hands on one of these two that's right well they're in beta right now so you have to request quota at the moment but soon we'll lift that requirement and you'll be able to just fire up a cloud GPU as infrastructure just like you would a virtual machine so it's not like you can go and buy one and install on your local hard you've got a cloud.google.com flash TPU oh and you'd fill out the quota form or if you're already in touch with Google Cloud in some other way just talk to your contact at Google cloud that's amazing yes 180 teraflops ladies and gentlemen another possibility we also have this tensorflow research cloud which is a set of a thousand of these so together that's 180 heta flops 180 180 petaflop oh my gosh and those were making available at no costs through an application process to top ml researchers out in the community because what we're really trying to do here is accelerate the rate of progress in machine learning and so we know that performance is one of the main gating factors a lot of the amazing results you've seen across vision and speech and language and robotics and other things have really been driven by this massive increase in the amount of available computation and reduction in cost and so we're trying to make mo acceleration universally accessible and useful affordable for everybody and available at scale in the cloud that's great so 180 teraflops all the way up to 180 petal hops this is Magne system here with sac stone our after tensorflow dev summit and this is the developer show [Music] 