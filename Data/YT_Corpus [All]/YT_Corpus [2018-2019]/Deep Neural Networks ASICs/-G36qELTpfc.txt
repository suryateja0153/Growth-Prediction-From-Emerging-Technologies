 [Music] welcome to cloud on air live webinars from Google cloud we're hosting webinars every Tuesday my name is Wes and this is Mikael and today we'll be talking to you about cloud TP use you can ask questions at any time on the platform and we've got Google or standing by to answer them so let's get started as I mentioned we're here to talk about the cloud the cloud TPU pod a supercomputer for deep learning that packs 11 and a half petaflop sub performance my name is Mikhail Shrestha I'm a machine learning specialist within Google clouds custom engineering team and Wes is our cloud TPU product manager in terms of our agenda for today we'll start with what RTP use we'll move on to the why and highlight a case study with eBay how they're using cloud TP use to enable visual search well then jump into how and I'll walk through a short demo of how you can access cloud to be used today on Google cloud platform and Wes will conclude with a sneak peek into the future of the cloud PU pods and we'll end with Q&A so Wes let's jump right in and let's start with around what was the motivation behind developing cloud TP use yeah thanks so as you can see here on this slide this shows how models have increased in size and have increased in accuracy over about the last five to six years and this has been really quite phenomenal how fast these things have grown the amount of compute needed to bring this kind of performance has been astounding and so from that we realized that we needed to build something that was more specialized to deep learning yeah yeah very interesting I guess in today's current current landscape the multi-purpose hardware accelerators they might not be enough to really further push AI when we talk about new fields such as autonomous systems robotics and things that are very important like safety and health care where every percentage point really matters and we want to get to 85 90 95 percent that's really where we believe that specialized hardware purpose-built for ml can really advance AI moving forward so let's talk about exactly what these hardware accelerators are and what they look like yeah so these are the tipi use tensor processing units and Asics and they're specifically designed to do machine learning and ASIC is application-specific integrated circuit and these are literally just that they're specifically designed to run deep neural networks and do the math that powers deep neural networks yeah it's so we see a lot of generations here why didn't we stop at version one three years ago oh sure so in 2015 and the slide you can see that we started with a very simple processor we wanted to see what we could do with it this one specifically just did inference so already taking a train model and being able to output what the computer thinks it's seeing and we soon realize that this this is amazing it's awesome it's a good thing to keep pursuing them so we made TPU v2 and we increase the amount of compute that it did quite phenomenally to 180 teraflops and then we kept going TPU v3 that we just announced 420 teraflops per device and you can see on that device that there's four chips and they're all working together yeah great so here here we have some snapshots of exact the chips but today we're really here to talk about the pods so let's jump in and really talk about the different granularities all the different methodologies around using these yeah so there's some terminology here so we talked about unlike on that last slide you can see here with one board which has four chips on it and then as as you get into a larger configuration that board is able to talk to other boards and other chips in a very high-speed way so that they all basically act like one accelerator and then we call that full configuration a full pod where we can take 64 of those devices and hook them all together and network them in such a way that they're all essentially acting like one and that's that's where we get a full pod and then we also offer the ability to do that in smaller chunks and we call those slices so it's not all or nothing we do have this small granularity the full pod but we have a lot of different slice options a quick question why not more combinations or permutations of those slices I see three here in the middle right these are kind of the optimal way that they can communicate together so when you when we talk about like how they do their actual matrix math the these are the optimal configuration in which they can operate at full speed I see so a lot of terminology we have cores we have chips we have devices slices pods I think the key takeaway is really around the number of course that's really the key metric and that's where the naming convention really follows when we have the v2 followed by the number of cores available to you as an end user yeah exactly so now let's dive in a little bit into the low-level what is actually happening on these chips right as we go down and we focus in on one chip this is kind of the heart of the TPU and as you can see in this animation that the matrix math is done from one corner to the following corner and this is how this is how the the tipi use function they do very simple linear algebra and matrix math and how that is computed is essentially like a heart would be pumping blood and that's where the systolic array gets its name I see so they're not too versatile but for matrix multiplication specifically they're super fast and I guess you see everyone should see the TPU demo com feel free to take a look it actually has a voiceover and a deeper explanation around this visual as well as comparisons across your more traditional hardware such as CPUs and GPUs so we are here to talk about full pods so taking that performance that Wes just mentioned and really scaling it up so Wes can you talk about how this scales up yeah so hinted at this a little bit earlier but this is how they communicate at a full pod level to do a propagation through each and every parameter calculation in a linear fashion so it scales perfectly so as you get up to a full pod you don't have to change any code all you have to do is increase your best batch size I see and hardware might be a little bit intimidating but as a data scientists and someone who focus on ML code what really makes me smile as that line linear performance scaling without code changes so that's great where I can code once and then scale seamlessly yeah so Wes I think we're excited to announce that these pods are available to the broader community can you add take us into some of the specs yeah so these pods are available today and we have them in alpha and when you utilize a full pod you can get about 11.5 pedophile of actual DNN compute and your models can be extremely large so up to 4 terabytes of hive and with memory and then of course they're all connected in that 2d torus which basically means that every chip kind of acts as one chip all together at the same time and there and I should I should mention that it's a training and inference oh yeah I mean this is fascinating is one of the internal end users being able to just log into a web browser log in to my Google Google cloud platform account and to be able to provision a full supercomputer without worrying about capital expenditures setting up the hardware and being able to train these these massive models it's really fascinating we're really excited to enable all our customers and partners to be able to leverage these as part of their ml processes as well moving forward so we'll jump now into a little bit of the why around this and really around the business applications and and first off you know why pods deep learning overall really takes a lot of experimentation it's more of an art as much as as much as a science so when we talk about pods right away the quick win is rapid experimentation number two we talked about this around coding once and seamlessly scaling so you're really a lot more productive as a data scientist so it which allows you to amplify your work across the organization number two are also at a point where these models are getting very large and very complex not only can we not fit data on one machine there there are moments where we actually can't fit the actual model on the machine we're getting to the point where there's millions even billions of parameters so being able to leverage all these cores all this memory allows us to experiment with very very large models in a lot of merging spaces and finally as a lot of industries and customers move into more of a personalization service really creating these rich customer experiences when we talk about recommendations visual search localized translations for each region and even very important things like medical transcriptions that's really where we believe having up-to-date fresh data that really is able to stay intelligent as your users evolve and their habits change it really gets you one step closer to a continuous learning loop moving forward so that's a little bit around why why we truly believe that pods can help out today and how can you help out even how can you start even sooner so Wes can you go over some of the reference models that are readily available in optimized for TP use yeah so TP use have set of what we would call optimized models and these are ones that are available on github today and they cover these four basic sections of deep neural networks so that you can do pretty much whatever you want and you can get started with these today so these are all available on github and you can do image recognition and object detection machine translation and language modeling with transformer as well as speech recognition and image generation but you can see like for object detection and recognition we have various models available and these these are been in the industry you know as long as long as we've been doing this and it showed on that first graph that we had yeah and while these while these might be familiar names for folks who already immerse in the deep learning space I do want to I do want to dive a little bit into this for folks who are just just joining the deep learning community in traditional ml usually just select the machine learning technique you might choose regression or classification you might use a technique like random force or decision trees but there's one additional layer in the process when you talk about deep learning and that's actually architecting these models and what this is what these reference models are referring to how many layers deep is the model how many nodes per layer what is the app what are the actual properties for each of the nodes are they short term long term or they recurrent or not so when we talk when we talk about names like amoeba net inception transformer these are actually ready to use models that you can apply today to your business problems in the image space whether you're looking at product quality on a factory line or stock-outs on a physical floor space so I just want to kind of clarify that these actually are ready to use they're not just machine learning techniques these are fully architected models that are ready to use on your data and then moving forward we wanted to highlight one customer that was an early user of TPU pods eBay yeah so eBay came to us and they they wanted to be able to speed up their training currently they had done a massive amount of on-prem work and it took them quite a while to do training and sometimes even took months we were able to provide them with some new model and new model support as well as some TP use and we're able to increase their training by over a hundred X as well as increasing their accuracy 10% which accuracy is kind of the name of the game how high can you get it yeah and I mean that the performance results are really amazing but really let's talk about the size of this you know why we're tipping pods really really useful here and if you look at just the catalog listing of eBay one of our one of the staples of Silicon Valley 1 billion product listings at this point as well and for the test that we did with them they actually ran a model on 55 million product images and to be able to predict over 10,000 product categories so this is a very very large scale problem that would not be able to be solved without custom hardware accelerators yeah and the largeness of how much memory is available on the TPU pods is key absolutely so now let's jump in into how do you actually get started before we dive into that I want to talk about you know who is this for really how are we positioning TPU pods the easy answer might be this is for the deep learning researchers and the data scientists and that answer is correct it's really it's gonna allow you to scale and amplify your work but really the broader mission of Google cloud is also to enable the developers the data analysts the business analysts the 20 plus million out there that want to leverage machine learning so TP use today if you know Python especially with these reference models that Wes mentioned you can get started today on practical pragmatic business applications also we understand that there you might have already done a lot of work on developing a ml infrastructure we really want to make sure we have minimum disruption so if you do want to leverage TPU pods for training large models you can still take that model and still serve them within your existing CP or GPU serving infrastructure now how do we actually provision a TPU pod everything is through Google Cloud you log in to the Google cloud platform console and really actually it's just one command there's a nice utility tool where in this case it's CTP you up and then you specify the size that Wes mentioned anything from the normal TPU device with 8 cores all the way up to a full pod or 512 but what exactly does this CTP you up command do and it does a number of things number one it's it provisions a host VM a CPU this is pre-installed with tensorflow today it's with tensorflow version 1.1 - but you can specify the version of your needs and what this what the CPU actually acts is acts as the tensorflow client this has a 10 to flow code it actually serves up the tensor program to the TPU pod it also takes all the images that are stored in Google Cloud storage bucket and feeds it into the TPU pod and the TPU pod essentially acts as the tensorflow server in this case also while you're using this it's important to note that TPU pods are accessible through two AP is currently the TPU estimator and chaos for those who are from more familiar with that framework and there's a number of tools that are expanding around being able to monitor and profile your TPU workloads TPU profiler allows you to take a look at a lot of metrics like utilization to make sure that TP use are being used most effectively and TPU compatibility checker is a plug into tensor board with a screenshot in the right side of the slide to make sure that your actual tensorflow code is optimized for TP use there's a great YouTube video in the bottom right from our colleague Brennen during the tensorflow dev summit that talks a little bit more around all the tools available to you as you develop on TPU pods so now we do want to walk you through a practical demo and first going through the end-to-end demo architecture so first we start with the raw images and those are those are hosted on our object storage web service called Google Cloud storage in this case I'll go through an example around image net second we really want to pre-process data whether it be feature engineering on tabular data data augmentation for images we want a quick way to get your data ready for for training and in this case also converting to TF records and that's really where Apache beam or a managed service on cloud dataflow comes in finally once those once the data is ready in Google Cloud Storage you submit that to the TPU pod using the architecture with the host VM and the TPU that I mentioned before and finally the model is outputted into a storage bucket for you to use as needed or even export and use it as part of your current interest finally there's a lot of different options to serve one of them is to serve those models on our serving infrastructure on cloud machine learning engine so this is a great aunt end pipeline and we also suggest that you have you have a method to orchestrate and automate this using cloud composer which is our managed Apache airflow solution I mean if you were you were watching the webinar before us with Anand until around cue flow pipelines AI hub really see a reference architecture like this a great asset to reuse across the organization so we so we really are gonna put in a lot of work to take this pipeline with TPU pods and really add it into aq flow pipeline and make it more discoverable as part of the broader Ohio AI hub community so now let's jump into the actual demo so for those who are not familiar with image net it is a image catalog of indexed and labeled data that is widely used within the research community in this case I'm going through a couple examples here and for those of you who who are big Disney fans I know the Lion King is coming up in 2009 so you have just a quick example here for the for the lion cub image we actually have over 16 1600 tagged images of lion cubs but again I believe image net at this point has millions and millions of tagged images across 20,000 categories that is great for research and then you can use a lot of these models and then apply them onto your own custom data set also I talked about a lot of the pre-processing of the data we actually have a lot of a lot of toolkits available directly in the TPU repo on github to be able to do that things like downloading the image image in that data set that I just talked about and transforming it in TF records done for you automatically if you have your own images across your your unique product catalog your unique texana me there's actually a JPEG the TF record Python script that's readily accessible for you now let's let's jump in into google cloud platform and I mentioned that with one command CTP you up you can actually provision a host host machine pre-install with tensorflow as well as a full TPU pod and this is the screen for Google compute engine where you have your VMs then we also take a look at the TP use available here here right now right now I have a pre installed a host VM with a TPU I'm actually going to run a quick example just to show you how quickly this can be done by anyone and I'm gonna bring up our quick cloud shell this can also be run on a local command line instance or your local development environment where the Google Cloud SDK is pre-installed I'm gonna run this command and I won wait for it to fully fully provision but as you can see already with one command the cloud on-air TPU VM host VM is is being instantiated as we speak now I'm gonna actually ssh into an existing my existing virtual machine and to really get started with the reference models the only step you need to prepare for is just cloning the github repo in this case i'm cloning one of our resonant models within the TPU model's official repository and it comes pre-installed with a lot of a lot of files that are ready for you to train and evaluate those models moving forward i'm going to quickly move to google cloud storage bucket i mentioned we are gonna we are gonna save the model in a google cloud storage bucket for you to be able to monitor the usage as well as be able to export that as needed i will create models 8 as i have all testing here as i prepare this prepare this model i'm going to set some environment variables change this to eight and I'm just gonna run the training the training Python code again Python is what you need to get started and really we're off to the races so we're off to the races if I scroll down I know we have a lot of noise here but I just want to show you one piece here where right away we've noticed I've used a quarter pod here so it's it's realized there's 128 cores 16 TP workers available for me to train this again I the code is written for one TPU or all the way up to a full pot here and as I now go back to the new bucket that I just created now what why all the magic is happening I hit refresh this is where the model training is occurring the model file is gonna be exported and all the checkpoints for all the updated variable weights are gonna be safe and I might just wait about five 10 seconds just to see one or two files there's the graph that we're gonna use and will will will continue to see the checkpoints continually be saved as we move forward so that's how easy it was for me to get started I actually created I created the TPU or provision to TP I should say with the CTP you up command I cloned the github repo and then I ran the training job now as I move forward I also might want to look at what is my performance here so I am going to open up cloud shell one more time and use one of my other buckets from a previous model that I wrote to actually run tensor board so we have tensor board here I'm going to bring up the UI change the port and this will bring up the tensor board dashboard this is a completed model that I had to run earlier this morning I took about 25 minutes to run through and a couple of metrics around really the power of the TPU pod first the images per second fluctuated between about 66,000 images per second to 68,000 so that's really utilizing the the performance there when we talk about the loss again I can see my loss training and also my final evaluation data point there and seeing the loss increase up until the optimal manner and most importantly what what what was my accuracy so accuracy can be measured in a lot of ways I think in an image classification top one or top five accuracy are great standard benchmarks in this case as i zoom in the top one accuracy for this example along with the hover over was actually 76% and top five accuracy if you're okay with the right result being in one of the top five highly probable classes was actually 93 percent so moving back moving back to our slides here really I spun up a quarter pod was able to get to almost 68 69 thousand images per second utilizing the full TPU pod reach the accuracy of 76 percent in under 25 minutes and overall West is going to talk about the prices but it costs me less than 50 dollars to do this yes this is quite fascinating where this would not be possible just a few years ago right the 25 minutes is phenomenal and that's just utilizing a quarter pod and if you wanted to scale that up to a full pod you can drop that one early yeah absolutely so let's let's dive in a little bit more into the pricing and what's available on GCP yeah so you can see in the chart here we start out if at the top with TPU devices and you can see the price for v2 and v3 there and then all the other ones that are kind of shaded in white and then down down to the full pod there those are the prices per hour for each individual slice of a pod and honestly like when you start to get into like the quarter pod and half pod and full pot arena you're really focusing on time like those times are gonna be phenomenally fast so if you're if your business is concerned about time like though those are the places that you want to think about increasing the throughput of of your training absolutely so I think we also wanted to get a give you a sneak peek into the future so Wes can you talk about some of the new upcoming items on clotty PU yeah and if you know we talked about earlier how the models that are currently supported are out on github that we also have an experimental section and you'll see some of these already in the experimental section so data parallelism with distribution strategy as well as pi Gorge support these are all coming in the very near future and TF tensorflow 2.0 is going to have a lot of extended functionalities for TPU and so with that stay tuned for live QA and we'll be back in less than a minute great welcome back everyone so we're gonna go through a couple of questions from the audience the first question is how do I get access to TPU pod as les mentioned we're announcing that they are available to the public today if you go on our public TPU documentation website you'll see the instructions there also feel free to reach out to a Google account team and we'll get you access from a tactical standpoint there might be steps for you to go into your Google cloud platform project and request quotas if you really want access to to a high number of course yeah second question can you run custom models on TPU pods we've gone through a lot of this session talking about reference models ready ready to use models in the spaces of image language and video but yes you can run custom models on TPU pods there are certain libraries within the tensorflow framework TPU rewrite and TPU estimator that allow you to port over your existing tensorflow models into models that run efficiently on TPU pods they just take a little bit of extra work cool I'm gonna take the next question yeah so next question is what are petaflop s' so when we think about like really large compute we start talking about how big and how fast they go so when you talk about a slot that's a floating-point operation per second and when you talk about a petaflop you're talking about a thousand teraflops so that when you go and you do this compute you can do it that much faster so it's just large large-scale supercomputing and the next step up from that is exaflop so right now with TPU pods were at 11.5 peda flops so that's the amount of throughput that you're able to get with your matrix math and let's see and then the next one starting at the smallest TPU size what changes do I need to make to scale to a TPU pod well that we've made that very simple and on that one slide we talked about how there's no code changes the basic change that you have to make is bash size and that's because the memory is increasing so you want to increase the batch size and that's how you keep the TPU pod fully loaded so every time that systolic array goes and starts chunking through all of your all of your layers in your DNN it's gonna always be full of data and so that's that's that's how you that's how you get all the way up to a full pods no change in the batch size that's like three point code changes for the actual training code minimal to none but you might want to look into changing hyper parameters to fully utilize that the larger larger slices yeah and last question here how much of Google's infrastructure is running on GPUs well as you've probably seen over the past couple of years as you start talking to things that Google makes a little bit more we're doing actually quite a bit on TP use it's so much more efficient power efficient compute efficient and density wise that we're able to pack so much more compute into into our data centers so for cloud TP use that's something that that we run most of what you see for machine learning on the products that you use today and for resources you can go to these links here yeah absolutely that the first link is really our public TPU documentation and also answer the question of how do I get access to TP use so those will be the instructions the second link is a great slide deck from our developer relations team that's been put together TPU demo com talks a little bit around the the low-level architecture of TP use and compared to other traditional hardware accelerators and finally the github repo that gives you access to a lot of our official optimized models experimental models and a lot of those toolkits that allow you to pre-process your data to be fully utilized by TPU pods yeah and stay tuned for the next session how to build custom data visualizations in data studio and thank you very much thank you so much [Music] 