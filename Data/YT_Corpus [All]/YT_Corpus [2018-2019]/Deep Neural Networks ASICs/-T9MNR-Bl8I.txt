 [Music] hello everyone good morning thanks for thanks for coming to this session I'm internal Chakraborty I'm a product guy with Google cloud platform and together with my colleagues here in this session we're going to cover three things one is the key challenges for edge and Google clouds offering to address those challenges second is how do we as Google cloud and Google in general how do we think how are we thinking about offering an integrated AI platform and finally we'll have some demos for you so let's get started so let's start with a simple dictionary definition of Internet of Things by the way the counter is not working here the Internet of Things if you look at the dictionary the definition is interconnection of computing devices embedded in everyday things via the Internet essentially it is little things connected to the Internet and hence Internet of Things and in this path of interconnection they're really three interacting units there's a you know network of objects that are either that connect wired or wirelessly to Internet and the network of objects could be devices can be appliances cars and and so on and so forth then there are gateways which acts as bridges between those devices and the Internet and finally cloud where you want to store the information which you're receiving from the devices and gateways for processing it and then analyzing it to derive intelligence as you all might already know that is estimated by 20 end of 2020 they'll be around 25 billion such installed IOT units and and across and these IT needs would be installed across the consumer segments as well as the business segments so that's an enormous number of devices connected to the Internet and we are already generating more data than ever with these Internet of Things devices and so as we as and the data generation is continuing to grow so for when Google around last year we announced clarity core to really help customers enterprise customers and developers to manage this massive number of devices and be able to easily ingest influx of data from these devices so clarity course is a fully managed service which allows you to securely connect and manage your devices on Google cloud and since their announcements last year the market reaction the reaction from our customers and partner and partners has been really positive it was it's been phenomenal we have customers in manufacturing in a smart parking management healthcare oil and gas services who are connecting their assets on the field to Google chlorotic or so that they can connect all these devices securely and then use the rest of our Google Cloud analytics and AI services for ingesting storing data and really deriving intelligence and as we as we started working with quadratic or our our focus has been on four critical areas one is intelligence so once you collected all this data from the installed units you know what do you do with those data so we provide we want to make sure that we make it really easy for you guys for customers and partners to derive intelligence from the data that you collect second is scale so clarity platform and clarity core is is a service as a service right so whether you connect one device or you tend to connect hundreds of thousands or even millions of devices you just connect your devices and we scale internally as a customer as a developer you don't have to worry about adding new infrastructure or adding a new VM instance as you add more devices so you really have focused on scale second third is security we we wanted to make sure that each each IOT devices there can be individually authenticated and it makes and it makes a secure connection to Google Cloud and finally we focused on the building up a robust partner ecosystem so we've been working really hard on these four critical areas and we identified early on that there is an emergent need for running local services close to the devices itself to the edge devices itself and in fact all these four critical areas are relevant for edge devices as well and most important of most important of those four was intelligence at the edge so let's take some example use cases and if you have a camera pointed at your parking lot where you were you're using the camera to count real-time the number of cars that are parked in the in the parking lot it doesn't really make cost-effective it's not really cost-effective and doesn't really make any sense to send every frame of the video from the camera to cloud to analyze and count the number of cars that are there in the parking lot what if you could just do it in the in the camera itself if you have a factory with robots there could be regulatory reasons there could be reasons for which you want to protect your IP and you don't want to send all the data to Google Cloud you might want to filter some of the data on the edge itself if you have if you're doing object detection using a camera or some other device that there are for privacy reasons you may not want to send all the sensitive data to cloud but you would want to you would want to run some of those analysis and intelligence on the device itself so based on on these for all these following reasons of right about before I'm very excited that we announced two new products for edge computing at NIH intelligence one is cloud ith cloud ith is a software stack which extends the AI and data processing capabilities of Google cloud to the edge and second is HTP you which is a Google design high-performing hardware chip to run machine learning inference on the edge device itself so let's get a little bit more into the details of these two product so with all righty edge as I mentioned it's a software runtime that you can run in your IOT gateway devices you can write it on a camera you can run it on any connected device that has some compute capabilities essentially a device that has a CPU and cloud RT edge software runtime allows you to run machine learning inference on the edge device itself it has built-in support for HTTP you so that you can have lightning fast machine learning inference on your edge device itself you know we thought about edge you can also locally store some of it your data you can process it you can do translation you can do some aggregation and filtration before you send the data back to cloud Android things is one of our Google's managed operating system for IOT devices which has got a secure boot so it's designed from ground water ground up for security and and software software update and cloud out the edge runs on Android things and we planning to soon support I have release have support for Linux versions as well and it securely connects your edge devices to Google Cloud at the same time we've also added enhance security through Hardware root of trust and cloud right edge seamlessly works with Google clarity core and rest of Google cloud IT platform for hassle-free provisioning so it's a complete software stack to run machine learning inference on the edge device itself and you know there are there two key components that enables clarity edge two for real-time AI on the edge device itself there's a tensorflow light based runtime that allows you to run those machine learning inference on the edge reducing latency and then increasing the versatility of your edge device and because we use tensorflow light runtime which is an open source you can you can use cloud IT edge with edge TPU for really for lightning fast and high performing performance but if you have a device which doesn't have an HTP you or your use case doesn't require the horsepower of HTTP you chip you can still run clarity edge and do local inference on your edge device which just has which has a GPU or just has a CPU itself so it's pretty flexible that way and the second component is IOT core that securely connects your edge device to Google Cloud so you can do you can you can manage your software you can do software from software and for more updates you can exchange data and also exchange machine learning models between your edge device and the cloud so combined with these two key components with clarity edge offers you is a is the software runtime format for running machine learning inference but also to orchestrate your models and data exchange with Google Cloud let's take an example say you have a factory with a high velocity assembly line and you want to detect anomalies real-time and you have a camera that has our edge TPU chip and you have our that as it's running Android things and as cloud right eh software runtime running on it and is you have you have collected historical data from your assembly line production which you can then use which using cloud machine learning engine you can train a model to accurately detect defects you can then push down that model to the camera via our quadratic or and cloud riding edge software and using HTTP you now you can do real-time detection of anomaly on your high velocity assembly so with clarity edge or as a software an edge TPU chip as a hardware chip together with our cloud platform we really are you can really build an integrated or a eye solution with our IT platform now let's look at edge TPU and to tell you more about HT PU and their motivations for its building edge TB chip let me to invite my friend Olivia tamam who is the tech lead for HDPE you in the googly eye team Olivia thanks Antonio so good morning everyone it's a real pleasure to be here with you today to talk about HTTP you and actually what we want to do is not only to present you what HTTP you is but we also want to explain why we have developed HTTP you at Google and to do that we want to walk you through the four key motivations that you see on the slide so the first motivation that we heard is we want to bring high quality AI to the edge and buy high quality I mean high speed and high accuracy so at Google we are developing machine learning models for the edge one of our flagship models for image classification is mobile net and it's not the unique features but one of the nice things about mobile net is that it's a highly configurable model and you can easily trade more computations for more accuracy so just to get an idea of the relationship between the two let's look at this graph where you see on the x-axis the computational cost of running one inference at the edge expressed in millions of multiply add x' and on the y-axis you have the accuracy so if you're shooting for a moderate amount of accuracy like 60 percent you're going to pay 60 million multiply ads that's 120 million operations but if you want to burn that up to 75 percent accuracy you're going to have to pay ten times that cost 1.2 billion operations and that particular graph stops at 600 million multiply ads you can invest more computations to get even more accuracy so the point here is that the computational cost of high accuracy is very steep and what we want to do thus in order to sustain high-quality AI at the edge to deliver high performance but because this is also about embedded applications we have to do this at a very low cost and naturally in a small physical and power footprint and that was basically the first charter that we had with edge TPU so the second motivation that we had relates to what Indrani just explained we are building an an to an AI infrastructure cloud to edge AI to hardware and the key goal is to make it as easy as possible for our customers to deploy their AI based solutions by piggybacking basically this infrastructure so it involves work across the three stacks AI software and hardware naturally and as you all know we have already been developing a custom AI hardware solution for the cloud cloud CPU so by introducing HTTP you the other thing we wanted to do is to complement that infrastructure so again to make it as easy as possible for consumers to they provide a solution now we think actually HTTP U is going to bring a lot of value to our customers in terms of helping them deploy AI at the edge but we also are developing an open infrastructure so we see HTTP you as only one option that we are making available to our customers and customers remain completely free to choose any hardware solution they want so the third motivation that we had is more technical and it relates actually to a fairly fundamental point about custom hardware design which is actually very often overloads a simple one that's very often often overlooked and it's the fact that custom hardware design is all about leveraging in Hardware the properties of the algorithms so in this case a AI algorithms so that can be a challenge for hardware designers because it means they have to develop a fairly intimate knowledge of the target algorithms of AI algorithms so that can be a challenge for most organizations but for Google it can also be an opportunity because all hardware designers have access to abundant AI expertise and therefore they can know what are the latest trends in AI and that can help them decide what kind of features they need to bring within their hardware designs now that's one advantage of having both expertise hardware and AI within the same company but it's not the only one what we can do and what we are increasingly doing is that we can also go the other way around and start adapting AI algorithms so that they work as well as possible on the custom AI hardware and the last point is that probably one of the biggest misconceptions about custom AI hardware is that it's primarily a hardware task while in fact the biggest difficulty is the software layer which you use to map AI algorithms onto the custom hardware so what we are really striving to do is to develop a holistic approach across the three stack and in both as a conviction but increasingly better on or experience this is what is going to yield the best results in terms of custom AI hardware design or fourth motivation relates to this one - the one I just presented so it's not only that all hardware designers have to have access to the latest trends in AI but it's also that AI is evolving extremely quickly this graph that you see here has been collected by colleagues at Google AI and it shows the number of AI related research articles which have been submitted to archive over the past decades and the key message here is that this curve is exponential has an exponential basically shape so there is just underlines the very rapid progression of AI so that again can be both a challenge and an opportunity it is a challenge because as you know the timeline of hardware is very different from the timeline of AI research but it can become an opportunity if we learn to tackle in hardware this rapid evolution of AI if we do that then what is going to happen is that the performance of the cousteau Mei Hardware again expressed in speed and accuracy is now going to progress more or less at the same pace as that of AI algorithms and as you can see that's very fast so one way to think about it is that increasingly AI algorithms are playing for custom AI Hardware more or less the same role as transistor technology is playing for general proposed architectures and when we set on to design in HTTP you from the beginning we had that objective in mind which is how are we going to tackle this very rapid evolution of AI and what we've designed is actually what we've created is not just one design but it is a design approach exactly for that purpose that's also why when we think about the first ship that we are presenting today even though it's achieving a very good performance per dollar ratio which we think is going to help disseminate high-quality AI at the edge we also think about it only as a first step in a long journey which is going to be largely driven by AI and we are already well underway we were second design and we've already started to work on the third design and at every step what we are going to do basically is to improve the AI support improved performance and power and bring more functionalities so those are some of the key motivations that we had actually to design at GPU and now to present you in more details what we are going to provide rural customers in the coming month I'm going to invite AJ who is the hdbo product manager to present you that thank you thanks Olivia money everyone I'm a janitor the product manager for STP use will go over quickly on what we're releasing and how users can have access to what's being announced today so that is the HTTP you one thing to note here is that the one-cent that is shown is indicative of the size of the HDP oh it does not indicate the cost but in all seriousness though when we're designing the HTTP use we were striving to achieve a very high performance per dollar and I'm proud to say that we've been able to achieve that our goals have been met and we've been able to do that at a relatively low power footprint and also physical footprint on how big the HTTP is and a combination of the high performance per dollar low-power as well as the low physical footprint allows broad deployment of HTTP use in multiple applications and it helps our goal to democratize AI in terms of the HTTP you itself the first generation that is being announced now and the roadmap is an inference accelerator and it's the architecture was optimized for vision applications of course it's an accelerator it can be used for multiple models but it was optimized for vision it can run concurrent state of and what what we can do with an accelerator at the edge is essentially run multiple state-of-the-art models concurrently for example on a high resolution video without losing frame rate right the as an example of what Olivia mentioned earlier the moon walnut v2 we can achieve several hundred inferences per second on the highest accuracy versions of those models so what we're announcing is not just the hardware though what we are trying to do is bring the leadership and the expertise that Google has in hardware software and AI and combine those into a solution which customers can have access to so as internal pointed out earlier we already talked about the cloud royalty edge and the HTTP you and how they interact we are adding components which make use of our expertise in AI as well as software so in AI for example we will be releasing at the time of the product launch which is in October when early access customers can have access to this the product will be releasing a set of precompiled models these are going to be compiled to tensorflow light which is Google's open source AI framework for the edge intense flow light allows low latency at a small binary size for your edge applications we will be announcing models for example there are mobile net base models and other architectures as well and it'll also be integrated with ml kit ml kit is a beta product that Google announced a few months ago which which has an SDK which allows users to have commonly used AI applications at the edge and if you have more complex needs ml kit also provides an API for you to serve those models onto your edge devices a next step after that is we would enable transfer learning on those pre-compile models that we are going to be providing so what is transfer learning transfer learning is a method where you can repurpose an existing model which is made for one use case and customize that based on your own data sets for your specific use case so that's a very powerful method and it really helps because the investment needed from users in terms of the time the effort and the expertise in machine learning is vastly reduced so the existing precompile models can be Reaper you can bring your own data and reuse your data for your specific application needs in a future update we will also be releasing the tensorflow Lite compiler for HTTP use so you can customize you can start your model design from scratch we also want to make it easy for customers to deploy HTTP you at your applications the idea being we would like you to be able to envision your use cases that serve your business needs and using the products that we are going to be releasing you can iterate and prototype you know you use the models in your hardware applications use the transfer learning function to optimize those models for your use case and test it out in your system and when you're satisfied with the results you deploy that in your real applications and we are also announcing and making available the HTTP you development kit and there are two versions of development kit depending on where you are in the ecosystem if you want a fully integrated solution with the HTTP you connected to an SOC what we are now saying is a system on module so what you see here is a system on module it has the HTTP you ASIC connects to and a CPU and also a crypto for security that module connects on to I'll take a break there the HTTP you is that is at Red Square that we are seeing there and this is to scale by the way in terms of the famous one sent the HTTP arm connects to a base board and the base board essentially expands the i/o connectivity options that are available so there's a whole slew of connectivity there that you have USB a port USB C HDMI there's a meeping interface to connect your camera onto it it's got a Wi-Fi interface and Ethernet and a five volt DC power to power the devices so if you need a fully integrated solution this is what you use the SOC will have Android things pre-loaded onto it it can connect onto the internet you can download the precompile models that we mentioned in the previous slides and you can run inference locally it also will have the IOT edge solution so once you runs inference locally your application can decide what to do with the local data and also use IOT edge to load back the data as you need back to the cloud for storage or retrieval or for further analysis we are also announcing the edge TPU accelerator it's a USBC form factor device and if you have an existing system that has a host interface available you can just connect the HTTP you accelerator using the USB C on to your systems and accelerate your machine running workloads so both of these solutions have been designed internally at Google with partnership from external vendors for four different components it's been designed by the Google aiy team which is a team within the Google AI business function alright and to show some functionality of of the boats and what we can do with it I'd like to invite Preston onto the stage Preston is our head of IOT solutions and he'll help us showcase what a quick example of what the HTTP you can do great so as as I said let's take a look at some of the performance capabilities of edge TPU so I'm going to mostly be off stage and run you through this demo what we do is we have two systems set up here one with edge TPU and one without and so we wanted to show you some of the performance differences as well as how you combine the inference on the edge with sending data to the cloud so give me one second to start up some things and actually if I could have you guys come over here because you are accidentally actually in camera not as planned so if you can come over to the with the other folks on the far other side so what we've got is um we've got two cameras set up yeah just you were your seats were in frame so we've got we've got two cameras set up and what we're using our high-resolution cameras high frame rate cameras these are 1080p cameras filming at 60 frames per second and you can see already that if you look at the top frame that is a system without the TPU the frames per second in which we're able to compute and analyze the video are already down to like 13 frames per second whereas the frames per second with TPU are still at a very high frame rate processing of 45 frames per second now when you're new I was going to invite my colleague Matt one of the IOT engineers who helped us with this demo on stage and so you can see that as Matt walked on stage we've we've detected Matt we see him on stage and you can see that already the frame rates starting to drop a little bit without the without the TPU now all of this inference all the ml that's detecting Matt on stage is happening entirely on the edge so we're not able to realistically stream this kind of high frame rate high resolution video directly to the cloud so we need to be able to compute on this information it's so rich strictly on the edge but since we have now some information we're interested in we can detect essentially some information about what we're seeing so one of the things is we can now look at just a simple metric of how many things have we detected that we're interested in the frame and we may average this out over five seconds locally on the edge that we have a simple metric that we can then send to the cloud and once we're sending information on the cloud we can start to visualize it in things like a dashboard so I have starting out here a very simple dashboard that I can just see that we have one one person on stage right now so but as a few other colleagues gradually joined Matt and we increase that number we can start to look at what that performance is so in this case we're not only going to send just the count of people what we're interested in some more information about the performance of the actual system here so I'm gonna switch briefly to the dashboards for a moment so you can sort of see what that looks like so here we're looking at three pieces of information the top is the number of objects detected the second graph is the frame rate and then the third is sort of a gauge of the instantaneous frame rate you can see that with edge TPU we're still holding steady at about 30 frames per second of effective processing now this is not just one model running per frame we're actually running a complex set of cascading models so we're doing many inferences per frame to achieve the information we're interested in and so as you can see that as the number of of people in frame goes up the inference really starts to drop without a TPU and why does this matter well if you're looking at something like this very sort of high frame rate high-resolution video it does it lets us do a couple things one is it's letting us detect you know that there are still people who might be very small in the frame right so if you're thinking of looking at say very small defects on equipment surface defects or small parts high resolution allows you to still provide the machine learning full information and we're not only using high resolution on the camera input side we're actually not extremely down sampling that video to feed to a model so we're able to feed high resolution directly into the model because of the computation power of of the edge TPU and as these as these numbers increase you'll continue to see sort of frame rate dropping on the left but we want to be able to sort of also show that why does a frame rate matter if we're losing frame's what are we losing so this model is not only detecting things like the the person on stage but we're able to detect a degree of information about what their orientation is what what their attention is looking at and we can count these as essentially glances at the camera now these are glances that lasts only a moment and so if we look now at the way we've counted glances between these two systems you can see that many glances are simply missed by the system without a TP you because they're so momentary and if you think about applications of the TPU in places such as high-speed manufacturing lines or other places where momentary information that might exist in an acoustic signal that will be dropped or missed these essentially act as as sort of dropping important events in the system you're trying to analyze and so as you as these systems are unable to keep up you're essentially losing critical information and again all that we're interested in on the cloud side here is these counts and metrics we don't have a need to sort of send the complex or potentially sensitive information that's happening on the edge directly to the cloud for processing we can keep that processing happening strictly on the edge so thank you very much colleagues and with that I will return the presentation back to Internet [Applause] Thank You Preston so as you can see Thanks say thanks person so as you can see wit with HTP you the the number of inferences and the infants infants you can do at a high frame rate is much higher than what you can do without HDPE you and we've used we've run multiple models in this demo this models were internally built within Google just for the purpose to showcase the capabilities and the horsepower we have with HDPE you we don't plan to make it publicly available but you can use in your your own model you can train your own model using cloud machine learning engine and then use rhtp you do where you need we have use case for such high processing you know real-time analysis so you know we've since as we started working on this we've been also engaging some of our early customers and and the feedback from this customers has been really positive LG CNS for example has has used a cloud machine learning engine and our HT PU and clarity edge on some of their cameras to really improve the productivity by detecting defects in their hive high fast assembly line real-time right so they're really excited about using our integrated platform to improve productivity in their production line smart parking is one another of our customer who are using is an HTP you and also looking at plot ith and overall platform to really revolutionize the parking experience by pushing intelligence to the edge with with the entire platform so we pretty excited with where we're headed with overall our edge story and and and the positive feedback from the customers is really encouraging as well now we understand that for for when we build a platform and as we start offering our edge solution we really have to work with we really want to work with our partners ecosystem so we've started engaging with a couple of our key partners for example force in semiconductor companies such as arm nxp are working with us to build the system and module with HM as I showed in a slide using the HTP you which which can be used by other device makers we are also working with some of our key gateway device makers such as hitachi ventura hearting nokia next calm and act on who are evaluating our platform to take those HTP us sawn into their industrial gateway so they can push the intelligence to the edge and really offer it to the customers and take it to the market we're also working with some of the edge computing companies such as a d-link you know kelvin olya analytics smart catch tracks who are building smart solution using our cloud iot edge software HTP you and the overall integrated AI platform for for the edge solution as well so we're working hard on the partner ecosystem we're gonna continue to work with a lot of more partners and we're very excited with with the partner response we have gotten so far so as you can see just to sort of summarize we are really working towards an integrated AI solution so that with our cloud IT platform what you heard today is our edge offering so HTP you chip which is our high performing hardware chip for machine learning inference on the device itself cloud iot edge which is a software stack for running machine learning in inference on your edge device which you can run on HD which is optimized for HTTP you and can also work when the devices that doesn't have HTTP you and this works seamlessly with our clarity platform which includes clarity chorus as a managed service pops up cloud machine learning engine and other services as well so we are very excited with our what with our integrated AI solution with for with the platform and we can't wait we can't wait to see where enterprise developers and customers can build with with this with this platform so next steps we are we're offering early access to this is an early access program with cloud ith and HTTP you you can sign up for access on on these sites you can either go to cloud.google.com slash iot - edge or cloud.google.com / h - TPU and our plan is that we will initially give pride access to select customers get their feedback and then by fall we'll start offering the HTTP you board to more more broadly and to collect more feedback so we can refine a platform and really make it says that it's seamless and it's integrated for for enterprise developers and customers to build integrated ki solution so with that I really want to thank you all for coming over here we we will be waiting here for a couple of more minutes if you guys have questions you can come to on the side of the stage and ask us questions but really excited with where we're heading and thank you for your time [Music] 