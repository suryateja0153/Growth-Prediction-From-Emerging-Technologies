 Thank you everyone for braving the cold, and the snow To be here This is 6.S094: Deep Learning for Self-Driving Cars And, it's a course  where we cover the topic of Deep learning Which is a set of techniques,  that have taken a leap in the last decade For our understanding Of what artificial intelligence systems are capable of doing And self-driving cars, which is systems, that can take these techniques,  and integrate them In a meaningful, profound way   into our daily lives In a way that transforms society. So that's why both of these topics,  are extremely important And extremely exciting. My name is Lex Fridman, And I'm joined by  an amazing team of engineers, In Jack Terwilliger, Julia Kindelsberger, Dan Brown Michael Glazer, Li Ding, Spencer Dodd and Benedikt Jenik, Among many others... We build autonomous vehicles,  here at MIT, Not just ones that perceive,   and move about the environment, But ones that interact, communicate,  and earn the trust, And understanding of human beings   inside the car, The drivers and the passengers, And the human beings outside the car the pedestrians and other drivers  and cyclists. The website for this course:  selfdrivingcars.mit.edu if you have questions, email at:  deepcars@mit.edu Slack: deep-mit For registered MIT students,  you have to register on the website And, by midnight,  Friday, January 19th build a neural network,  and submit it to the competition. That achieves the speed of 65 miles per hour On the new deep traffic 2.0 It's much harder and much more interesting than last year's  for those of you who participated. There's three competitions in this class: Deep Taffic,SegFuse   DeepCrash There's guest speakers,  that come from: Waymo, Google, Tesla And, those are starting new,  autonomous vehicle startups In Voyage, NuTonomy and Aurora And then use a lot today from CES. And, we have shirts! For those of you who braved the snow and continued to do so  towards the end of the class there will be free shirts. Yes, I said free and shirts  in the same sentence, You should be here. Okay. First: The Deep Traffic competition There's a lot of updates, and we'll  cover those on Wednesday. it's a deep reinforcement learning  competition. Last year we received over 18,000 submissions, This year we're going to go bigger! Not only can you control one car,  with your neural network You can control up to ten This is multi agent deep renforcement learning. This is super cool! Second: SegFuse - Dynamic  Driving Scene Segmentation competition Where, you're given the raw video, The kinematics of the vehicles,  the movement of the vehicle, The state-of-the-art segmentation. For the training set you're given: Ground truth labels, pixel level labels Scene segmentation, and optical flow. And with those pieces of data, You're tasked to try to perform better than the state-of-the-art In image based segmentation. Why is this critical, And fascinating,  in an open research problem? Because, robots that act in this world, In the physical space,  not only must interpret, Use these deep learning methods to interpret, The spatial visual characteristics of a scene, They must also interpret, understand,  and track The temporal dynamics of the scene. This competition is about  temporal propagaton of information, Not just scene segmentation. You must understand the space,  and time. And finally Deep Crash Where we use deep reinforcement learning, To slam cars thousands of times, Here, at MIT, at the gym. You're given data on a thousand runs, where car Or a car knowing nothing is using a monocular camera's Single input,  driving over 30 miles an hour, Through a scene, it has very little control through Very little capability to localize itself It must act very quickly. In that scene you're given  a thousand runs, to learn anything. We'll discuss this,  in the coming weeks. This competition will result in four submissions That; We evaluate everyone's  in simulation But the top four submissions, we put head-to-head at the gym. And, until there is a winner declared,  we keep slamming cars ...at 30 miles an hour. Deep crash, and also on the website  is from the last year, And on GitHub there's DeepTesla. Which is using the large-scale naturalistic driving data set We have to train a neural network  to do enter and steering That takes in monocular video  from the forward roadway, And produces steering commands, Steering commands for the car. Lectures: Today we'll talk about  deep learning, Tomorrow we'll talk about  autonomous vehicles, Deep RL is on Wednesday, Driving scene understanding So segmentation That's Thursday. On Friday, we have Sacha Arnoud, The Director of Engineering at Waymo. Waymo is one of the companies,   that's truly taking Huge strides in fully autonomous vehicles. They're taking the fully L4, L5,  autonomous vehicle approach. and it's fascinating to learn, he's also the head of perception  for them. To learn from him; What kind of problems they're facing? And what kind of approach  they're taking on? We have Emilio Frazzoli, Who's one of last year's  speakers Sertac Karaman said Emilio is  the smartest person he knows, So Emilio Frazzoli's the CTO  of nuTonomy An autonomous vehicle company, that was just acquired by Delphi For a large sum of money. And they're doing a lot of incredible work in Singapore, and here in Boston. Next Wednesday,  we are going to talk about the topic of our research,  or my personal fascination, is deep learning for  driver state sensing. Understanding the human,  perceiving everything about the human being inside the car,  and outside the car. One talk, I'm really excited about, is Oliver Cameron on Thursday. He is now the CEO of  autonomous vehicle startup Voyage. He's previously the director of  the self-driving car program, for Udacity He will talk about: how to start  a self-driving car company, For those, who said that MIT folks  are entrepreneurs. If you want to start one yourself, he'll tell you exactly how. It's super cool! And then, Sterling Anderson! Who was the director previously, Tesla Autopilot team. And now is a co-founder of Aurora, The self-driving car startup, that I mentioned, ...that has now partnered, with NVIDIA and many others. So, why self-driving cars? This class is about applying  data-driven learning methods, To the problem of autonomous vehicles. Why self-driving cars are fascinating, And an interesting problem space? Quite possibly, in my opinion, This is the first wide reaching,  and profound integration Of personal robots, in society. Wide-reaching, because there's  one billion cars on the road, Even a fraction of that, will change, ...the face of transportation, and how we move about this world. Profound, and this is an  important point, ...that's not always understood. There's an intimate connection,  between a human, And a vehicle, when there's  a direct transfer of control. It's a direct transfer of control... That takes that, his or her life, into the hands, Of an artificial intelligence system. I showed a few quick, Quick clips here, you can Google  first time with Tesla autopilot, On YouTube and watch people,   perform that transfer of control, There's something magical... About a human and a robot  working together, That will transform,  what artificial intelligence is, In the 21st century. And this particular autonomous system, AI system, self-driving cars,   is on the scale. And the profound,the life-critical  nature of it, is profound. In a way that, it will truly test  the capabilities of AI. There'a a personal connection, That will argue throughout these lectures, That we cannot escape considering the human being. That will argue throughout these lectures, That we cannot escape considering the human being. That autonomous vehicle, must not only perceive and control It's movement through the environment. You must also perceive everything  about the human driver and the passenger And interact, communicate,  and build trust with that driver. Because,... In my view, As I will argue throughout  this course, An autonomous vehicle is more  of a personal robot, than it is a perfect perception  controled system. Because, perfect perception  and control, For this world, full of humans,... Is extremely difficult. And could be, two-three-four decades away. Full autonomy. Autonomous vehicles are going to be flawed. They're going to have flaws... And we have to design systems,  that are effectively caught That effectively transfer control  to human beings, When they can't handle the situation. And that transfer of control...  Is an... Is a fascinating opportunity for AI. Because the obstacle avoidance, Perception of obstacles,  and obstacle avoidance, It's the easy problem. It's the safe problem.  Going 30 miles an hour Navigating through streets of Boston, It's easy. It's when you have to get,  to work, and you're late. Or you're sick of the person  in front of you, ...that you want to go  in the opposing lane, and speed up. That's human nature.  And we can't escape it. Our artificial intelligence systems, Can't escape human nature,  they must work with it. What's shown here,  is one of the algorithms, We'll talk about next week, for cognitive load. Or we take, the raw,... 3D convolutional neural networks, Take in the eye region, the blinking,  and the pupil movement To determine the cognitive load  of the driver. We'll see how we can detect  everything about the driver, Where they're looking? Emotion? Cognitive load? Body pose estimation? Drowsiness. The movement towards full autonomy ...is so difficult... I would argue That it almost requires  human level intelligence. That the.... As I said, 2-3-4 decade out  journey... For artificial intelligence researchers,  to achieve full autonomy Will require achieving, solving,  some of the problems Fundamental problems of creating intelligence. And... That's something  we'll discuss, In much more depth, In a broader view in two weeks, For the artificial general intelligence  course, Where we have Andrej Karpathy,  from Tesla, Ray Kurzweil, Marc Raibert,  from Boston Dynamics Who asked for the dimensions of this room,  because he's bringing robots Nothing else was told to me... It'll be a surprise. So that is why I argue  the human centered Artificial intelligence approach In every algorithm of a design  considers the human. For autonomous vehicle on the left,  the perception Scene understanding,  and the control problem, As we'll explore through  the competitions, And the assignments, of this course Can handle 90, and increasing ...percent of the cases.  But it's the 10, 1.1 percent of the cases  as we get better and better, That we have to... We're not able to handle  through these methods And that's where the human, perceiving the human is really important. This is the video from last year. Of Arc de Triomphe  Thank you Didn't know it last year, I know now. That is one of millions of cases, Where human to human interaction  is the dominant driver. Not, the basic perception  control problem So why deep learning in this space? Because deep learning Is a set of methods,  that do well from a lot of data. And to solve these problems Where human life is at stake, We have to be able to have techniques That learn from data,  learn from real-world data. This is the fundamental reality  of artificial intelligent systems That operate in the real world. They must learn from real world data. Whether that's on the left  for the perception, the control side, Or on the right, for the human The perception, and the communication, Interaction And collaboration with the human, And the human robot interaction. Ok. So what is deep learning? It's a set of techniques, if you allow me the definition,  of intelligence Being the ability to accomplish  complex goals, Then I would argue, definition of understanding Maybe a reasoning is... The ability to turn complex information Into simple, useful,  actionable information. And that is what deep learning does. Deep learning is representation learning, Or feature learning, if you will. It's able to take raw information, Raw complicated information, That's hard to do anything with, And construct hierarchical representations  of that information, To be able to do  something interesting with it. It is the branch of artificial intelligence, Which is most capable and focused,  on this task. Forming representations from data, Whether it's supervised  or unsupervised, Whether it's with the help of humans,  or not; It's able to construct structure, Find structure in the data;  Such that you can extract Simple, useful, actionable information. On the left, From Ian Goodfellow's book, Is the basic example of  a misclassification. The input of the image, On the bottom, with the raw pixels And as we go up the stack  as we go up the layers, Higher and higher order representations  are formed. From edges, to contours The corners, to object parts And then finally, The full object semantic classification,  of what's in the image This is representation learning A favorite example for me Is, one from four centuries ago. Our place in the universe, And representing that place  in the universe, Whether it's relative to Earth, Or relative to the Sun. On the left is our current belief, On the right is the one,  that was held widely, Four centuries ago Representation matters! Because,what's on the right Is much more complicated  than what's on the left. You can think of,  in a simple case here When the task is to draw  a line that separates, Green triangles and blue circles In the Cartesian coordinates space, on the left The task is much more difficult.  Impossible, to do well On the right, it's trivial, in polar coordinates. This transformation is exactly Whan we need to learn,  this is representation learning. So you can take the same task, Of having to draw a line  that separates The blue curve, and the  red curve on the left . If we draw a straight line,  it's going to be a high There's no way to do it  with zero error. With 100% accuracy Shown on the right, is our best attempt. But what we can do with deep learning, With a single hidden layer network  done here, Is form the topology,  the mapping of the space, In such a way, in the middle, That allows for a straight line to be drawn, That separates the blue curve,  and the red curve. The learning of the function  in the middle, Is what we're able to achieve  with deep learning. It's taking raw, complicated information, And making it simple,  actionable, useful. And the point is, that,  this kind of ability to learn, From raw sensory information Means that, we can do a lot more,  with a lot more data. So, deep learning  gets better with more data. And that's important,  for real world applications. Where edge cases are everything. This is us driving, with two  perception control systems. One is in Tesla vehicle,  with the autopilot Version one system that's  using a monocular camera, To perceive the external environment, And produce control decisions. And our own, neural network Running on adjacent TX2,  that's taking in the same. With a monocular camera,  and producing control decisions. And, the two systems argue,  and when they disagree They raise up a flag, to say that  this is an edge case That needs human intervention. There is... Covering such edge cases,  using machine learning, Is the main problem,  of artificial intelligence, and... When applied to the real world, It is the main problem to solve. Okay. So what are neural networks? Inspired very loosely, and I'll discuss About the key difference  between, Our own brains and artificial brains Because there's a lot of insights,  in that difference. But inspired loosely by  biological neural networks, Here, as a simulation of a... Thalamocortical brain network, Which is only 3 million neurons, 476 million synapses...  The full human brain, Is a lot more than that.  A 100 billion neurons, ...1,000 trillion synapses. There's inspirational music,  with this one That I didn't realize was here,  it should make you think. Artificial neural networks,  yeah... Let's Just let it play... The human neural network is,  a hundred billion neurons, right? 1,000 trillion synapses. One of the state-of-the-art, Neural network is ResNet-152, which has... 60 million synapses. That's a difference, of about... A seven order of magnitude difference That's a difference, of about... A seven order of magnitude difference The human brains have,  10 million times more synapses, Than artificial neural networks. Plus or minus one order of magnitude, depending on the network. So, what's the difference, between A biological neuron, and  an artificial neuron? The topology of the human brain  have no layers. Neural networks are stacked in layers They're fixed, for the most part. There is chaos! Very little structure in our human brain In terms of how  neurons are connected. They're connected, often,  to 10,000 plus other neurons. The number of synapses,  from individual neurons That are...  Input into the neuron is huge! They're asynchronous. The human brain works asynchronously. Artificial neural networks work synchronously. The learning algorithm  for artificial neuron networks, The only one, the best one... Is back propagation. And we don't know, how human brains learn... Processing speed,  this is one of the... The only benefits we have  with artificial neural networks is... Artificial neurons are faster. But they're also extremely power inefficient, And... There is a division into stages, Of training and testing with neural networks. With biological neural networks,  as you're sitting here today They're always learning. The only profound similarity,  the inspiring one The captivating one, is that both are, Distributed computation at scale. There is an emergent aspect  to neural networks, Where the basic element of computation: A neuron, Is simple. Is extremely simple. But when connected together, beautiful Amazing, powerful approximators  can be formed. A neural network is built up  with these computational units, They're the inputs, There's a set of edges,  with weights on them. The edges... The weights are multiplied  by this input signal, A bias is added,  with a nonlinear function. That determines whether the network  gets activated or not Well, the neuron gets  activated or not. Visualized here. And these neurons can be combined  in a number of ways. they can form a feed-forward neural network, Or they can feed back into itself, To form... To have state memory. In Recurrent neural networks. The ones on the left, are the ones  that are most successful, For most applications, in computer vision. The ones on the right are very popular, and specific. One temporal dynamics,  or dynamics time series of any kind are used. In fact, the ones on the right,  are much closer To the way our human brains are Than the ones on the left, But that's why, they're really hard to train. One beautiful aspect,  of this emergent power, For multiple neurons being connected together Is the universal property That with a single hidden layer These networks can learn any function Learn to approximate any function. Which is an important property to be aware of, because The limits here, are not in the  power of the networks The limit in... ...is in the methods by which We construct them, and train them. What kinds of machine learning, deep learning are there? We can separate into two categories. Memorizers, The approaches, that essentially memorize patterns in the data. And approaches that,  we can loosely say Are beginning to reason To generalize over the data,  with minimal human input. On top, on the left are the,  quote/unquote "Teachers", Is how much human input  in blue, is needed To make the method successful  for supervised learning, Which is what most of  deep learning successes come from Or most of the data is annotated  by human beings, The human is at the core of the success. Most of the data,  that's part of the training Needs to be annotated by human beings. With some additional successes, coming from augmentation methods, That extend that... Extend the data, based on which  these networks are trained And the semi-supervised  reinforcement learning, And unsupervised methods, That we'll talk about,  later in the course, That's where the near-term successes  we hope are. And with the unsupervised learning approaches, that's where, the true excitement, About the possibilities  of artificial intelligence lie. Being able to make sense, of our world With minimal input from humans,... So, we can think of two kinds of  deep learning impact spaces. One is a special purpose intelligence. It's taking a problem,  formalizing it. Collecting enough data on it,  and being able to, Solve a particular case,  that provides value. Of particular interest here  is a network That estimates apartment costs  in the Boston area. So you could take the  number of bedrooms, The square feet, and the neighborhood... And provide as output, the estimated cost. On the right is the actual data, Of apartment cost.  We're actually standing, In an area, that has over 3000 dollars  for a studio apartment Some of you may be feeling that pain. And then there's general-purpose intelligence. Or something that feels like... Approaching general-purpose intelligence. Which is reinforcement,  and unsupervised learning. Here with Andrej, from Andrej Karpathy's,  Pong to Pixels. A system that takes in,  80 by 80 pixel image And with no other information is able to beat, Is able to win at this game. No information except  a sequence of images, Raw sensory information, The same way,  the same kind of information, That human beings take in, from the visual Audio, touch, sensory data. The very low-level data,  and be able to learn to win. And it's very simplistic, And it's very artificially constructed world, But nevertheless, A world where no feature learning  is performed. Only raw sensory information  is used to win. With very sparse minimal human input. We'll talk about that on Wednesday. With deep reinforcement learning. So. But for now we'll focus  on supervised learning. Where there is input data, There is a network we're trying to train, A learning system,  and there's a correct output, That's labeled by human beings. That's the general training process  for a neural network. Input data, labels... And the training of that network , that model. So that, in a testing stage, A new input data,  that has never seen before, It's tasked with producing guesses,  and is evaluated based on that. For autonomous vehicles,  that means being released Either in simulation,  or in the real world, to operate. And how they learn,  how neural networks learn, Is given, the forward pass, Of taking the input data, whether it's from the training stage In the training stage, taking the input data, Producing a prediction. And then given that  there's ground truth in the training stage, We can have a measure of error,  based on a loss function. That then punishes... The synapses, the connections,  the parameters, That were involved with making  that wrong prediction. And it back propagates the error,  through those weights. We'll discuss that in a little bit   more detail, in a bit here... So what can we do with deep learning? You can do one-to-one mapping. Really you can think of input  as being anything, It can be a number, a vector of number, a sequence of numbers A sequence of vector of numbers... Anything you can think of,  from images to video, To audio, to text  can be represented in this way. And the output can, the same,  be a single number, Or it can be images,   video, text, audio. One-to-one mapping on the bottom, One-to-many, many-to-one,  many to many, and... Many to many with different starting points for the data. Asynchronous. Some quick terms, that will come up Deep learning is the same as neural networks, It's really deep neural networks,  large neural networks It's a subset of machine learning,  that has been Extremely successful in the past decade. Multi-layer perceptron, deep neural network , Recurrent neural network Long short-term memory network  LSTM Convolution neural network and  deep belief networks, All of these will come up to the slides... And, there is specific operations, Layers within these networks of Convolution, pooling, activation,  and back propagation. This concept that we'll discuss, In this class. Activation functions, there's a lot of variants. On the left is the activation function, the left column, And the x-axis is the input, On the y-axis is the output. The sigmoid function,  the output. If the font is too small, the output is... Not centered at zero. For the Tanh function,  it's centered at zero; But it still suffers from vanishing gradients. Vanishing gradients is  when the value, The input is low or high. The output of the network,  as you see in the right column, There, the derivative of the function is very low. So the learning rate is very low. For ReLU, Not, it's also not zero centered, But it does not suffer from  vanishing gradients. Back propagation is  the process of learning It's the way we take goal from error, Compute as the loss function, At the bottom right of the slide, Taking the actual output of the network  with a forward pass, Subtracting it from the ground truth, Squaring, dividing by two, And than using that loss function. that back propagate, Through, to construct a gradient,  to back propagate the error. To the weights that were responsible, For making either a correct, or an incorrect decision. So the subtasks are there,  there's a forward pass, There's a backward pass,  and... A fraction of the weight's gradient subtracted from the weight. That's it! That process is modular, So it's local  to each individual neuron, Which is why it's extremely,... We're able to distribute it  across multiple, Across the GPU.  Parallelize across the GPU. So, learning for a neural network, These competition units  are extremely simple. They're extremely simple to then... Correct when they make an error, when they're Part of a larger network, that makes an error. And, all that boils down to, Is essentially an optimization problem. Where the objective, utility, function is The loss function,  and the goal is to minimize it. And we have to  update the parameters The weights, and the synapses, And the biases to decrease that loss function. And that loss function is  highly nonlinear. Depending on the activation function's  different properties, Different issues arise. There's vanishing gradients, for sigmoid. Where the learning can be slow There's dying ReLU's... Where the derivative is exactly zero, For inputs less than zero. There are solutions to this,  like leaky ReLU's And a bunch of details,  you may discover When you try to win  the deep traffic competition But, for the most part These are the main  activation functions And it's the choice of the neural network designer Which one works best... There's saddle points,  all the problems From your miracle, non-linear optimization That arise, come up here. It's hard to break symmetry, And stochastic gradient descent Wthout any kind of tricks to it, Can take a very long time, to arrive at the minima One of the biggest problems in all of machine learning And certainly deep learning,  is overfitting You can think of the blue dots  and a plot here As the data, to which we want  to fit a curve We want to design a learning system  that approximates The regression of this data. So, in green, is a sine curve Simple. Fits well. And then, there's  a ninth degree polynomial Which fits even better, in terms of the error But it clearly overfits this data If there's other data That it has not seen yet that it has to fit It's likely to produce a high error So it's overfitting the training set This is a big problem for small data sets And so we have to fix that, with regularization Regularization is a set of methodologies That prevent overfitting Learning the training too well,  in order And then to not be able to generalize To the testing stage And overfitting, the main symptom Is the error decreases  in training set But increases in the test set. So there's a lot of techniques and traditional machine learning That deal with this; Cross validation, and so on... But because of the cost of training for neural networks Its traditional to use what's called a validation set So you create a subset of the training That you keep away For which you have the ground truth And use that, as a representative  of the testing set. So you... Perform early stoppage, or more realistically Just save a checkpoint. Often. To see how, as the training evolves, The performance changes on the validation set, And so you can stop, when the performance In the validation set is getting a lot worse It means you're overtraining on the training set. In practice, of course, We run training much longer And see when,  what is the best performing What is the best performing Snapshot checkpoint of the network? Dropout, is another very powerful regularization technique. Where we randomly remove part of the network Randomly remove some of the nodes in the network Along, with it's incoming and outgoing edges So what that really looks like, Is a probability of keeping a node. And in many deep learning  frameworks today It comes with a dropout layer So it's essentially a probability That's usually greater than 0.5 That a node will be kept. For the input layer The probability should be much higher, Or, more effectively, what works well is just adding noise What's the point here? You want to create enough diversity in the training data Such that it is generalizable,  to the testing. And as you'll see with  deep traffic competition, There's L2 and L1 penalty, Weight decay, weight penalty Where, there's a penalisation  on the weights that get too large The L2 penalty keeps the weight small Unless the error derivative is huge And produces a smoother model, And prefers to distribute When there is two similar inputs It prefers to put  half the weights on each Distribute the weights As opposed to putting the weight on one of the edges. Makes the network more robust L1 penalty has the one benefit That, for really large weights They're allowed to be, to stay. So it allows for a few weights to remain very large. These are the regularization techniques And I wanted to mention them because they're useful To some of the competitions, here in the course. And I recommend to go to playground To tensorflow playground To play around with some of these parameters Where you get to, online in the browser Play around with different inputs, different features Different number of layers, and regularization techniques And to build your intuition about classification Regression problems,  given different input data sets. So what changed? Why over  the past many decades Neural networks that have gone through two winters Are now again Dominating the artificial intelligence  community CPUs, GPUs, ASICs, So, computational power  has skyrocketed From Moore's law to GPUs There is huge data set,  including ImageNet, and others There is research;  Back propagation In the 80's, The convolutional neural networks LSTMs, there's been a lot of interesting breakthroughs About how to design these architectures How to build them, such that  they're trainable efficiently Using GPUs. There is the software infrastructure From being able to share the data, or get; To being able to train networks, and share code And effectively view neural networks  as a stack of layers As opposed to having to implement stuff from scratch With TensorFlow, PyTorch and  other deep learning frameworks And there's huge financial backing from Google, Facebook, and so on... Deep learning... ..is... In order to understand, why it works so well And where it's limitations are... We need to understand where our own intuition comes from About what is hard,  and what is easy The important thing  about computer vision Which is a lot of  what this course is about Even in deep reinforcement  learning formulation Is that visual perception for us human beings Was formed 540 million years ago That's 540 million years  worth of data An abstract thought Is only formed about  a 100 thousand years ago That's several orders of magnitude less data So we can make,  with the neural networks Predictions that seemed trivial Trivial to us human beings But completely challenging and wrong to neural networks Here, on the left, showing a prediction of a dog With a little bit of a distortion and noise  added to the image Producing the image on the right And your network is confidently 99 percent plus accuracy, Predicting that it's an ostrich And there's all these problems  to deal with Whether it's in computer vision data, Whether it's in text data, audio... All of this variation arises In vision, It's illumination variability The set of pixels and the numbers  look completely different Depending on the lighting conditions It's the biggest problem in driving Is, lighting conditions, lighting variability. Pose variation Objects need to be learned from every different perspective I'll discuss that for  when sensing the driver Most of.... Most of the deep learning work  that's done in the face On the human,  is done on the frontal face Or semi frontal face. There's very little work done  on the full 360 pose Variability that a human being could take on. Intraclass variability for the classification problem, For the detection problem... There is a lot of different kinds of objects For cats, dogs, cars, bicyclists, pedestrians. So that brings us to object classification. And I'd like to take you through  where deep learning Has taken big strides for the past several years Leading up to this year, to 2018 So let's start at object classification Is when you take a single image, And you have to say... One class, that's most likely to belong in that image. The most famous variant of that  is the ImageNet competition ImageNet challenge. ImageNet data set is a data set  of 14 million images With 21,000 categories And...  For, say, the category of fruit There's a total of  188,000 images of fruit And there is 1200 images  of Granny Smith apples. It gives you a sense, of what  we're talking about here So this has been, the source Of a lot of interesting breakthroughs in deep learning And a lot of the excitement,  in deep learning It's first, the big successful network At least, one that became famous In deep learning is AlexNet in 2012 That took a leap of... A significant leap in performance on the ImageNet challenge. So it was one of the first neural networks That was successfully trained on the GPU And achieved an incredible performance boost Over the previous year on the ImageNet challenge. The challenge is: ...and I'll talk about some of these networks... It's to given a single image, give five guesses, And you have five guesses to guess For one of them to be correct The human annotation is a question  often comes up So how do you know the ground truth? Human level performance is  5.1 percent accuracy, on this task. But, the way the annotation  for ImageNet is performed, is There's a Google search,  where you pull the images Already labeled for you,  and then the annotation that Mechanical Turk, other humans perform Is just binary:  Is this a cat, or not a cat So they're not tasked with performing The very high-resolution semantic  labeling of the image. Okay. So, through,  from 2012 with AlexNet, to today And the big transition in 2018  of the ImageNet challenge Leaving Stanford and going to Kaggle. It's sort of a monumental step Because in 2015 with the ResNet network Was the first time That the human level performance  was exceeded And I think this is,  a very important Map of where deep learning is. For particularly what I would argue is a toy example Despite the fact that it's 14 million images So we're developing  state-of-the-art techniques here And in next stage, as we are now exceeding Human level performance, on this task Is how to take these methods into the real world. To perform scene perception,  to perform driver state perception. In 2016, and 2017 CUImage and SENnet has a very unique new addition To the previous formulations that has achieved An accuracy of 2.2 percent error 2.25 percent error on the  ImageNet classification challenge. It's an incredible result. Ok, so you have this image classification architecture That takes in a single image, and produces convolution And takes it through pooling convolution, and at the end Fully connected layers and performs A classification task, or regression task. And you can swap out that layer to perform any kind of other task Including with recurrent neural networks of Image captioning, and so on... Or localization of bounding boxes Or, you can do  fully convolutional networks Which we'll talk about on Thursday Which is when you take an image  as an input, And produce an image as an output. But where the output image,   in this case,is a segmentation. Is, where a color indicates what the object is. The category of the object. So it's pixel level segmentation, Every single pixel in the image  is assigned, A class, a category, where that pixel belongs to. This is, the kind of task, That's overlaid on top of  other sensory information, Coming for the car in order to Perceive the external environment You can continue to extract information From images in this way To produce image to image mapping For example to colorize images And take from grayscale images to color images Or you can use that kind of  heat map information To localize objects in the image So as opposed to just classifying that this is an image of a cow R-CNN, Fast and Faster R-CNN, And a lot of other localization networks Allow you to propose different candidates For where exactly the cow is located in this image And thereby being able  to perform object detection Not just object classification. In 2017 there has been a lot of cool applications Of these architectures One of which is background removal Again mapping from image to image Ability to remove  background from selfies Of humans or human-like  pictures of faces The reference is, with some incredible animations, Are in the bottom of the slide, And the slides are now available online Pix2pixHD There's been a lot of work in GANs In Generative Adversarial Networks In particular in driving GANs have been used to generate examples That generate examples from source data Whether that's from raw data Or in this case with pix2pixHD Is taking coarse semantic labeling  of the images Pixel level, and producing Photorealistic, high-definition images of the forward roadway This is an exciting possibility For being able to generate A variety of cases  for self-driving cars For autonomous vehicles to be able to learn To generate, to augment the data And be able to change the way different roads look Road conditions, To change the way vehicles look cyclists, pedestrians. Then we can move on to recurrent neural networks Everything I've talked about was one-to-one mapping From image to image, or image to number Recurrent neural networks work with sequences We can use sequences to generate handwriting To generate text captions  from an image Based on the localization, as the various detections, in that image. We can provide video description generation So taking a video And combining convolutional neural networks With recurrent neural networks Using convolutional neural networks  to extract features Frame to frame And using those extracted features To input into our RDRN ends, to then generate labeling A description of what's  going on in the video A lot of exciting approaches for autonomous systems Especially in drones Where the time to make a decision is short Same with the RC car traveling 30 miles an hour Attentional mechanisms For steering the attention of the network Have been very popular For the localization tasks and for just saving How much interpretation of the image How many pixels  need to be considered In the classification task So we can steer, we can model the way A human being looks around an image To interpret it And use the network to do the same. And we can use that kind of steering To draw images, as well. Finally the big breakthroughs in 2017 Came from this Pong to Pixels The reinforcement learning using sensory data Raw sensory data And use reinforcement learning methods Deep are all methods of which we'll talk about on Wednesday I'm really excited about... The underlying methodology of deep traffic, and deep crash Is using neural networks  as the approximators Inside reinforcement learning approaches. So AlphaGo in 2016,  have achieved a monumental task. That when I first started in artificial intelligence Was told to me is impossible for a system to accomplish Which is to win at the game of Go Against the top human player  in the world. However that method was trained on human expert positions The Alphago system,  was trained on previous games Played by human experts. And in an incredible accomplishment AlphaGo Zero in 2017 Was able to beat AlphaGo, And many of it's variants By playing itself, from zero information So no knowledge of human experts No games, no training data very little human input And what more, it was able to generate Moves, that were surprising to human experts. I think it's Einstein that said that intelligence That the key mark of intelligence is imagination. I think it's beautiful to see an artificial intelligence system Come up with something that surprises human experts Truly surprises... For the gambling junkies, DeepStack And a few other variants Have been used in 2017 to win a heads-up poker. Again another incredible result! I was always told an artificial intelligence would be impossible For Deep, For any machine learning method  to achieve And was able to beat a professional player And several competitors  have come along since We're yet to be able to beat To win, in a tournament setting, so multiple players For those unfamiliar  heads-up poker is one-on-one. It's a much much smaller, easier space to solve. There's a lot more human-to-human  dynamics going on, For when there's multiple players. But that's the task for 2018 And the drawbacks! It's one of my favorite videos I show it often, of Coast runners. For these deep reinforcement learning  approaches The learning of the reward function The definition of the reward function Controls how  the actual system behaves And this will come... This would be extremely important for us, with autonomous vehicles Here the boat is tasked with Gaining the highest number of points, And it figures out that it does not need to race, Which is the whole point  of the game, In order to gain points But instead, pick up green circles That regenerate themselves, over and over. This is the... The counterintuitive behavior of a system That would not be expected When you first designed the reward function And this is  a very formal simple system Nevertheless Is extremely difficult to come up with a reward function That makes it operate in the way you expect it to operate Very applicable for autonomous vehicles Of course in the perception side As I and mentioned with the ostrich and the dog A little bit of noise, with 99.6 percent confidence We can predict That the noise up top is a robbing, a cheetah, Armadillo, lesser Panda... These are outputs from actual  state-of-the-art neural networks Taking in the noise, and producing a confident prediction It should build our intuition, to understand that we don't That the visual characteristics, The spatial characteristics of an image Did not necessarily convey the level of hierarchy Necessary to function in this world. In a similar way, with a dog and the ostrich And everything and an ostrich Network confidently, with a little bit of noise Can make the wrong prediction Thinking that school bus, is an ostrich And a speaker is an ostrich They're easily fooled But not really... Because they perform the task  that they were trained to do, well So we have to make sure we keep our intuition Optimized to the way machines learn Not the way humans have learned Over the 540 million years of data That we've gained Through developing the eye through evolution The current challenges we're taking on First: Transfer learning There's a lot of success  in transfer learning Between domains that are  very close to each other So, image classification  from one domain to the next. There's a lot of value in forming representations Of the way scenes look,  in order Natural scenes look, In order to do scene segmentation The driving case, for example. But we're not able to do  any bigger leaps, In the way it would perform  transfer learning The biggest challenge  for deep learning Is to generalize Generalize across domains. It lacks the ability to reason, In the way that we've defined  understanding previously Which is the ability to turn  complex information Into simple useful information. Convert domain specific, Complicated sensory information. That doesn't relate to  the initial training set. That's the open challenge  for deep learning Train on very little data, and then go and reason, And operate in the real world. Right now, you'll know,  it's very inefficient They require big data They require supervised data Which means they need human. Cost a human input They're not fully automated, Despite the fact that  the feature learning Incredibly the big breakthrough Feature learning is performed  automatically, You still have to do a lot of design, Of the actual architecture  of the network And all the different  hyper parameter tuning needs to be performed. Human input Perhaps a little bit more educated  human input, A former PhD students, postdocs faculty Is required to tune  these hyper parameters. But nevertheless, human input is still necessary. They cannot be left alone. For the most part... The reward. Defining the reward As we saw with coast run Is extremely difficult For systems that operate  in the real world Transparency Quite possibly it's not  an important one But neural networks,  currently, are a black box. For the most part. They're not able to accept Through a few successful  visualization methods That visualize different aspects  of the activations They're not able to reveal,  to us humans Why they work,  or where they fail And this is a philosophical question, For autonomous vehicles, That we may not care  as human beings If a system works well enough. But I would argue that, it will be a long time, Before systems work well enough, Or we don't care. We'll care, And we'll have to work together  with these systems And that's where transparency, communication, ...collaboration is critical. Edge cases. It's all about edge cases. In robotics, in autonomous vehicles... The 99.9 percent of driving is really boring, It's the same. Especially highway driving. Traffic driving. It's the same. The obstacle avoidance, the car following the lanes... ...centering. All these problems are trivial. It's the edge cases. Trillions of edge cases, They need to be generalised over, On a very small amount  of training data. So again I return to:  Why deep learning? I mentioned a bunch of challenges, And this is an opportunity! It's an opportunity,  to come up with techniques, that operate successfully in this world. So I hope the competitions  we present in this class, And the autonomous vehicle domain, Will give you some insight,  and an opportunity to apply... In some of these cases are  open research problems, Wth semantic segmentation  of external perception, With control of the vehicle,  and deep traffic And, with deep crash, Of control of the vehicle,  and under actuated High speed conditions,  and the driver state perception. So with that, I wanted to introduce  deep learning to you today, Before we get to the fun tomorrow  of autonomous vehicles. So, I would like to thank: Nvidia, Google, Autoliv, Toyota. And, at the risk of  setting off people's phones: Amazon Alexa, Auto... But, truly, I would like to say,  that I've been humbled Over the past year, by the thousands of messages  were received By the attention.  By the 18,000 competition entries. By the many people across the world,  not just here at MIT, That are brilliant,  that I got a chance to interact with. And I hope we go bigger, And do some impressive stuff in 2018. Thank you very much,  and tomorrow is self-driving! 