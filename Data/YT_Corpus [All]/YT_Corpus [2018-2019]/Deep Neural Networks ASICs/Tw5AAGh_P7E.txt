 let's begin with the obvious question why should one care about distributed training training complex neural networks with large amounts of data can often take a long time in the graph here you can see training the resident 50 model on a single but powerful GPU can take up to four days if you have some experience running complex machine learning models this may sound rather familiar to you bringing down your training time from days to hours can have a significant effect on your productivity because you can try out new ideas faster in this talk we're going to talk about distributed training that is running training in parallel on multiple devices such as CPUs GPUs or GPUs to bring down your training time with the techniques that you will talk about in this talk you can bring down your training time from weeks or days to hours with just a few lines of change of code and some powerful hardware to achieve these goals we're pleased to introduce the new distribution strategy API this is an easy way to distribute your tensorflow training with very little modification to your code with distribution strategy api you no longer need to place ops or parameters on specific devices and you don't need to restructure your model in a way that the losses and gradients get aggregated correctly across the devices distribution strategy takes care of all of that for you so let's go over what are the key goals of distribution strategy the first one is ease-of-use we want you to make minimal code changes to in order to distribute your training the second is to give great performance out of the box ideally the user shouldn't have to change any change or configure any settings to get the most performance out of their hardware and third we want distribution strategy to work in a variety of different situations so whether you want to scale your training on different hardware like GPUs or GPUs or you want to use different API is like carrots or estimator or if you want to run distributor different distribution architectures like synchronous or asynchronous training we want distribution strategy to be useful for you in all these situations so if you're just beginning with machine learning you might start your training with a core CPU on your desktop tensorflow takes care of scaling onto a multi-core CPU automatically next you may add a GPU to your desktop to scale up your training as long as you build your program with the right poodle libraries tensorflow will automatically run your training on the GPU and give you a nice performance boost but what if you have multiple GPUs on your machine and you want to use all of them for your training this is where distribution strategy comes in in the next section we're going to talk about how you can use distribution strategy to scale your training to multiple GPUs first we'll look at some code to train the resonate 50 model without any distribution will use the cares API which is the recommended tensorflow high-level api we begin by creating some datasets for training and validation using the TF data API for the model we'll simply reuse the resinate 50 that's prepackaged with Kara's intensive flow let me create an optimizer that we'll be using in our training once we have these pieces we can compile the model providing it the loss and optimizer and maybe a few other things like metrics which I've omitted in the slide here once the models compiled you can then begin your training by calling model dot fit providing the training data set that you created earlier along with how many epochs you want to run the training for fit will train your model and update the models variables then you can call evaluate with the validation data set to see how well your training did so given this code to run your training on a single machine or a single GPU let's see how we can use distribution strategy to now run it on multiple GPUs it's actually very simple you need to make only two changes first create an instance of something called mirrored strategy and second pass the strategy instance to the compiled call with the distribute argument that's it that's all the code changes you need to now run this code on on multiple GPUs using distribution strategy mirror strategy is a type of distribution strategy API that we introduced earlier this API is available intensive 1.11 release which will be outraged shortly and in the bottom of the slide we've linked to a complete example of training M NIST with Karis and multiple GPUs that you can try out with mirror strategy you don't need to make any changes to your model code or your training loop so it makes it very easy to use this is because we've changed many underlying components of tensor code to be distribution aware so this includes the optimizer batch norm layers metrics and summaries are all now distribution aware you don't need to make any changes to your input pipeline as well as long as you're using the recommended TF data API s and finally saving and checkpointing work seamlessly as well so you can save with no or one distribution strategy and restore with another seamlessly now that you've seen some code on how to use mirror ID to scale to multiple GPUs let's look under the hood a little bit and see what mere strategy does in a nutshell mere strategy implements data parallelism architecture it mirrors the variables on each device a GPU and hence the name your strategy and it really uses all reduce to keep these variables in sync and using these techniques it implements synchronous training so that's a lot of terminology let's unpack each of these a bit what is data parallelism let's say you have n workers or n devices in data parallelism each device runs the same model and computation but with a different subset of the input data each device computes the loss and gradients based on the training samples that it sees and then we combine these gradients and update the models parameters the updated model is then used in the next round of computation as I mentioned before mere strategy mirrors the variables across the different devices so let's say you have a variable in your model it will be replicated as a 0 a 1 a 2 and a 3 across the four different devices and together these four variables conceptually form form as single conceptual variable called a mirrored variable these variables are kept in sync by applying identical updates a class of algorithms called all reduce can be used to keep variables in sync by applying identical gradient updates all reduce algorithms can be used to aggregate the gradients across the different devices for example by adding them up and making them available on each device it's a fuse algorithm that can be very efficient and reduce the overhead of synchronization by quite a bit there are many versions of Al Gore reduce algorithms available based on the communication available between the different devices one common algorithm is what is known as ring or reduce in ring all reduce each device sends a chunk of it's gradients to its successor on the ring and receives another chunk from its predecessor there are few more such rounds of gradient exchanges and at the end of these exchanges each device has received a combined copy of all the gradients ring all reduce also uses network bandwidth optimally because it ensures that both the upload and download bandwidth at each host is fully utilized we have a team working on fast implementations of all reduce for various network topologies some hardware vendors such as the NV Nvidia provide specialized implementation of all reduce further hardware for example Nvidia nickel the bottom line is that all reduce can be fast when you have multiple devices on a single machine or a small number of machines with strong connectivity putting all these pieces together mirror strategy uses mirrored variables and all reduce to implement synchronous training so let's see how that works let's say you have two devices device 0 and 1 and your model has two layers a and B each layer has a single variable and as you can see the variables are replicated across the two devices each device received one subset of the input data and it computes the forward pass using its local copy of the variables it then computes a backward pass and computes the gradients once the gradients are computed on each device the devices communicate with each other using all reduce to aggregate the gradients and once the grains are aggregated each device updates its local copy of the variables so in this way the devices are always kept in sync the next forward pass doesn't begin until each device has received a copy of the combined gradients and updated its variables all reduce can further optimize things and bring down your training time by overlapping computation of gradients at lower layers in the network with transmission of gradients at the higher layers so in this case you can see you can compute the gradients of layer a while you're transmitting the gradients for layer B and this can further reduce your training time so now that we've seen how merge Saji works under the hood let's look at what type of performance and scaling you can expect when using mirror strategy with multiple multiple GPUs we use a resonate 50 model with image and data set for our benchmarking it's a very popular benchmark for performance measurement and we use nvidia tesla B 100 GPUs on Google cloud and we use a bad size of 128 per GPU on the x-axis here you can see the number of GPUs and on the y-axis you can see images per second process during training as you can see as we increase the number of GPUs from 1 to 2 to 4 to 8 the images per second process is close to doubling every time in fact we are able to achieve 90 to 95% scaling out of the box note that these numbers were obtained by using the resonant 50 model that's available in our official model garden repo and currently it uses the estimator API we're working on cara's performance actively so far we've talked a lot about scaling onto multiple GPUs what about cloudy pues TPU stands for tensor processing units these are custom ASIC designed and built by Google especially for accelerating machine learning workloads in the picture here you can see the generations of TP use on the top left you can see TPU v1 in the middle you can see cloud TPU v2 which is now generally available in Google cloud and on the right side you can see TPU v3 which was just announced in Google i/o a few months ago and is now available in alpha and in the bottom of the slide you can see a TPU pod which is a number of cloudy pews that are interconnected to each other using a custom network TP pods are also now available in alpha so if you want to learn more about TP use please attend Frank's talk tomorrow on cloud abuse in this talk we're just going to focus on how you can use distribution strategy to scale your training on TP use it's actually very similar to what we just saw with mirror strategy but instead we'll use TPU strategy this time so first you create an instance of a TPU cluster resolver and give it the name of your cloud TPU resource then you pass a cluster resolver to the TPU strategy constructor along with another argument called steps for run which I'll come back to in a bit that's it once you have the strategy instance you can pass it to your compiled call as before and your training will now run on cloud TP use so you can see the distribution strategy makes it really easy to switch between different types of hardware this API will be available in the next sensor flow release which is 1.12 and in the bottom of the slide we've provided a link to training resident 50 with the estimator API using TPU strategy so let's talk a little bit about what TPU strategy does tpo strategy implements the same architecture as mere strategy that is it implements data parallelism with synchronous training the course on a TPU there are eight cores on a single cloud TPU and these cores are connected via fast interconnects and this means that you can do all reduce really fast to aggregate the gradients coming back to those steps for a run parameter from the previous slide for most models the computation time of a single step is small compared to the sum of the communication overheads so it makes sense to run multiple steps at a time to Malta is these overheads so setting this number to a high value like a hundred will give you the best performance out of the tip use the tip your teams are working on reducing these overhead so that in the future you may not need to specify this argument anymore and finally you can also use TPU strategy to scale to cloud TPU pods which are as I mentioned in alpha release right now tip your pods consists of many cloud CPUs interconnected via fast Network and this means that all reduce across these different cloud TPU pods can be really fast as well so that's all about cloud TP use I'll now hand it off to my colleague Magnus to talk about scaling on to multi node with GPUs thank you all right so that was how we scale on multiple GPU cards on the single node what about multiple nodes we have multiple computers because the fact is that even though you can cram in a lot of GPU cards for example on a single computer sooner or later if you do massive amounts of training you will need to consider an architecture where you can scale out to multiple nodes as well so this is an example where we see for worker nodes with four GPU cards in each of them and in terms of support for multi-gpu multi node support we have currently support for pre-made estimators in tensorflow 1.11 which is subject to be released shortly and we are working very very hard with some awesome developers to get the support into chaos as well so you should be aware that cater support will be there as soon as possible and however if you do want to use chaos with a multi node distribution strategy you can actually achieve that using a little trick that's available in in the Charis api and that's called it's a function called the estimator to model estimator to model s model to estimator TVRs estimator model to estimator that takes a Karass model as an argument and then it actually returns an estimator that you can use for multi node training all right so how do we set up a multi node training environment in the first place this was a really really difficult problem up until the technology that's open-source now called kubernetes was released and so we even though you can set up multi no training with tensorflow without running kubernetes he will certainly help to use kubernetes as the orchestration platform to fire up multiple nodes and kubernetes is available in most clouds GCP and I think AWS and others as well so how does that work well a kubernetes cluster contains of a set of nodes so in this particular picture you can see three nodes and each of them is a worker node and what tensorflow requires in order for this to work is that each of these nodes have an environment variable called TF underscore config they've defined okay so every single know that you have in your cluster needs to have this variable defined and in this TF config you have two parts first of all the cluster part which defines all of the hosts that participates in the distributed training okay all the nodes in your cluster and the second one is really to specify who am I what is my identity within this cluster so you can see the task here is zero so this worker is host one port one it's one that's host two port and it's two meaning that is host tree and there at that port okay so that's how you need to configure your cluster in order to do this so that is really cumbersome to go around around to all of the nodes and actually provide a specific configuration and key and kubernetes provides so how do you configure this kubernetes provides an excellent way of doing that through its deployment configuration Yama file so you can actually distribute the configuration the environment variables to set on the respective nodes so how do we how do we integrate that with tensorflow well it's part of the initial support and this is just one way of doing it there are multiple ways but this is one way that we've tested you can use a template ending called the Jinja you create a file called a ginger file and there is actually such a file available in the tensorflow slash ecosystem repository observe not the tensorflow tensorflow repository this is the echo system there will be a directory under that repository called distribution on the score strategy that contains useful functions are to use with distribution strategies so you can use this file as a template in order to generate automatically generate the deployment channel for the kubernetes cluster so what would that look like for a configuration like this where we have three nodes well it's really really simple the only thing you need to do in this file the ginger file is the highlighted configuration up here you set the worker replicas to three nodes the rest is just code that you keep for for all of the executions so we set up to do make sense so this is actually a macro that populates TF config based on this parameter up here alright so that's very simple but what about the code we've now configured the kubernetes cluster to be able to do this distributed training with tensorflow but there are also some stuff we need to do with the code as we had for the already the single node as well so it's approximately the same as for single node the multi-gpu configuration so this is the estimator lingo so I provide a config here you see the wrong config which is a standard estimator a construct and I set the trained distribute parameter to T if that country distribute collective all reduced strategy so not mirrored strategy for multi node configuration its collective all reduced strategy and then I specify the number of GPUs I have available for each of these workers that I have my cluster and that's it given that I have that config object I can just put that as part of the config parameter when I do the conversion from chaos over to an estimator and I now have multi-gpu multi node multi-gpu in each of the nodes configured for tensorflow ok and so let's look at this collective all reduce strategy because that's something different and what we talked about previously with a mirrored strategy so what is that thing well it is specifically designed for multiple worker nodes and it's essentially based on mirrored strategy but it adds functionality in order to deal with multi host or multi workers in my cluster and the good thing about this is that it automatically selects the best algorithm for doing reduce the reduce are the alergies function across this cluster so what what does that mean what what kind of algorithms do we have for doing all reduce in a multi node configuration well one of them is very simple very similar to what we have for a single node which is the ring all reduce in which case the GPUs they just travel across the nodes and they perform a an overall ring reduce across multiple hosts and GPUs okay it's essentially the same as for single node it's just that they are traversing hosts with all of the penalties associated of course of doing that depending on the interconnect between these hosts another algorithm is hierarchical or reduce I think that is a really complicated English word and what happens here is that we essentially pass all of the variables up to a single GPU card on the respective host see that we also end up missing an error two errors over here with one arrow here never mind that they're supposed to all send this stuff to GPU zero GPU one and then we do you know we do an all reduce across the nodes there and the GPUs performing that operation then propagates back to the individual GPUs within its own node okay so depending on network and other characteristics of your setup and hardware one of these solutions are would work very well and the thing with collective already strategy is that will automatically detect the best algorithm to use in your distributed cluster all right so that was multi node multi accelerator cards within the nodes there are also other ways to scale to multiple nodes with tensorflow and one of them how many of you are familiar with parameter server strategy the parameter servers now this is the classical way of how you do tensor flow distributed training and eventually this actually this way the classical way you should not continue to do that you should actually once we roll out distribution strategies that's the way to go so what I'm describing here is essentially the parameter service strategy but instead of describing it in the old classical way of the intensive law I'm going to describe how to do it with distribution strategies does that make sense yeah if you didn't understand that you haven't used your one just don't worry about it just listen to what I have to say here to get a recap of what the parameter service strategy is it's essentially a strategy where we have shared storage we have a number of worker nodes and they're working on batches of this shared storage they're working completely independent well not complete it will see shortly but they are working independently calculating radius based on batches and then we have a number of parameter servers so these workers when they are finished with a batch they send it up to the parameter servers the parameter servers they have the updates from the other workers so they kind of calculate the average of the gradients and then pass all of those variables down to the workers so it's not synchronous right these workers they will get updates on the variables in a synchronous fashion which has good sides and bad sides the good side is one worker can go out and the other worker can still execute as normal okay that's the way this works so how can we set this up in a in a distributed strategy cluster well it's really easy instead of just specifying the worker replicas in their Jinja file we also specify the PS underscore replicas so that's the number of per servers that we have in our kubernetes cluster all right so that is the kubernetes setup now what about the code so that's also really easy you saw the run config the confi parameter previously instead of using the collective all reduce strategy I got that right this time collective all reduced strategy you use the parameter server strategy see that so it's just another type there you still specify the number of GPUs per worker you specify the config object-- to the s the chaos model to estimator function call and you're all done right very very few lines of codes needs changing even though we're talking about massively different way of doing distributed tensorflow tentacle training all right there is one more configuration that we are working on I think we will have a release of this in 1.11 at least so you can try out and that is a really really cool setup where you actually run distributed training from your laptop and in this particular case you have all of your model training code here and the only thing you so forget about parameter server now we're back to our back to multiple workers and all reduce here the only thing you fire up on these workers is the TF underscore STD underscore server dot PI or whatever variant of that you want to use because this code is available also in the tens of your ecosystem repository so you can go check it out how we did it for this normal setup and you can change you to one everywhere you want the thing is that this script and the installation on the workers they don't have the model program at all so when we fire up the model training from our laptop or workstation here it will distribute that model over to those so if you have any changes to your model code you can just make it locally and it will automatically distribute that out to all the workers now you may say oh that's a hassle because now I got to install this script on all the workers and you do not have to do that because the only thing you do is to specify the script parameter in the ginger file that you've seen a couple of times now and we have the same number of workers here and that means that this script will actually start on all of these nodes so what we're talking about here is the capability to fire up a kubernetes cluster with an arbitrary number of nodes without any installation of code you can use a local laptop and it will automatically distribute the model and train into all of these worker nodes just by having these two lines here what about the code so again we have in the run config here and this time we're going to set a parameter called experimental distribute to the distribute config and as part of this should be config we are going to embed a collective all reduce strategy with as we saw before the number of GPUs we have per worker but the distribute config requires one more parameter and that is the remote cluster okay because the the master node here needs to know the cluster to which it should send all the model code for these demos that are waiting there for the model code to be shed make sense so you got to specify that parameter then your finish you got your config object in model to estimate just specify the config and as you've seen before it's just a couple of lines of difference between these different configurations all right that's really it for tensorflow multi node training so let's summarize what we've talked about here today first of all we went through the single node distribution strategy setup we talked about the mirrored strategy for multiple GPUs within a single node and we talked about the TPU strategy to distribute work to the TP use okay we also went through the old reduce algorithm which is used by distribution strategy to be able to do this single load distribution then we talked about multi no distribution talked about using kubernetes to distribute tensor flow training using these Jinja files that compiles or translates over to the Jama file for deployment we talked about the reduce using collective all reduced strategy we talked about the parameter server setup with distribution strategy and then finally we talked about distributed training from a standalone client distributing the model code over to the workers all right working progress so most of the stuff that we talked about today you'll find you'll find tensorflow that contrib dot distribution strategy you'll find that in the tensorflow repository but as part of 1.11 many of these things that we talked about you will be able to start to use okay if you if you really want trial you can also check out nightly and see how far we go but 1.11 should be out shortly and we are working on a performance still this is always going to be something that we're going to work on to match state of the art as you saw with single node multi-gpu we've achieved 90 to 95 5% for scaling a performance on on the GPU card we're continuously working on trying to improve this for all of the different configurations we've talked about TPU strategy will be available as part of 1.12 with chaos right now it's only available in estimator but remember we have the estimator trick right model to estimator within chaos you can actually take a chaos model convert to an estimator and still use GPU strategy that will be part of 1.11 multi worker GPU support is something we're also working on as i said so that means that in Kara's native Kara's code we can actually specify multi worker GPU support and also Igor execution how many of you are familiar with Igor execution gotta check that out that's really important a feature of tensorflow so if you're not using Iger you should definitely stop using anything else and start using Iger okay the entire getting started experience with tensorflow is based on ego mode and we will have great performance bridges between eager execution and graph mode execution and all of this distribution so the entire architecture kind of builds on on this so check it out eager execution is also something we're working on so you can directly knee your execution mode utilize multiple GPU cards and multiple nodes in the same way that we discussed in this setup all right and then when we have multi worker GPUs obviously if one fails and we talked about this already synchronous synchronous gradient updates we do have a discussion on fault tolerance so that's something we're looking into to build into this so we have more resilience with respective faults so another summary what did we talk about today we talked about the distribution API which is very easy to use it's the new way of doing distributed tensorflow training okay forget about anything that you did before start to learn about how to do this we we talked about distribution strategies having great performance right out of the box we saw the scaling between 1 and 8 GPUs on a kubernetes cluster and then we looked at how it can scale across GPUs different accelerators GPUs as well as TP use a single node as well as multi node and TPU pod right and that's it you should definitely take a picture of this slide because this slide summarizes all of the resources that we had and with that we are done thank you very much for listening to this if you have any questions as you 