 boy I'm delighted to be here today and have a chance to talk to you about what is one of the biggest challenges we faced in computing in 40 years but also a tremendous opportunity to rethink how we build computers and how we move forward you know there's been a lot of discussion about the ending of Moore's Law the first thing to remember about the enemy of Moore's law of something Gordon Moore said to me he said all Exponential's come to an end it's just a question of when and that's what's happening with Moore's law um if we look at what does it really mean to say Moore's law is ending what does it really mean well look at what's happening in DRAM that's probably a good place to start because we all depend on the incredible growth in memory capacity and if you look at what's happened in D Rams for many years we were achieving increases of about 50 percent a year in other words going up slightly faster even than Moore's law then we began a period of slowdown and if you look at what's happened in the last seven years this technology we were used to seeing boom the number of mega bits per chip more than doubling every two years is now going up at about 10% a year and it's gonna take about seven years to double now D Rams are particularly odd technology because they use deep trench capacitors so they require a very particular kind of fabrication technology what's happening in processors though and if you look at the data in processors you'll see a similar slowdown moore's law is that red line going up there and nice on a nice logarithmic plot notice the blue line that's the number of transistors on a typical Intel microprocessor at that date it begins diverging slowly at first but look what's happened since in the last ten years roughly the gap has grown in fact if you look at where we are in 2015 2016 we're more than a factor of 10 off had we stayed on that Moore's long curve now the thing to remember is that it also there's also a cost factor in here fabs are getting a lot more expensive and the cost of chips is actually not going down as fast so a result of that is that the cost per transistor is actually increasing at a worse rate so we're spinning to see the effects of that as we think about architecture but if the slowdown of Moore's law which is what you see all the press about is one thing the big issue is the end of what we call Dennard scaling so bob Dennard is was an IBM employee was the guy who invented the one transistor dram and he made a prediction many years ago that the energy the power per square millimeter of silicon would stay constant would stay constant because voltage levels would come down capacitance would come down what does that mean if the energy if the power stays constant and the number of transistors increases exponentially then the energy per transistor is actually going down and in terms of energy consumption it's cheaper and cheaper and cheaper to compute well what happened with Dennard scaling well look at that blue line there the red line shows you the technology improving on a standard Moore's Law curve the blue line shows you what's happening to power and you all know I mean you've seen microprocessors now but they slow their clock down they turn off cores they do all kinds of things because otherwise they're gonna burn up they're gonna burn up I mean I never thought we'd see the day where a processor would actually slow itself down to prevent itself overheating but we're there and so what happens with Dennard scaling is the last it began to slow down that's starting about 97 and then since 2007 it's essentially halted the result is a big change all of a sudden energy power becomes the key limiter not the number of transistors available to designers but their power consumption becomes the key limiter that requires you to think completely differently about architecture about how you design machines it means inefficiency in the use of transistors in computing inefficiency in how an architecture computes is penalized much more heavily than it was in this earlier time and of course guess what all the devices we carry around all the devices we use are running off batteries so all of a sudden energy is a critical resource right what's the worst the worst thing that happens your cell phone runs out of power your smartphone runs out of power that's a disaster right but you think about all the devices we walk around with they're hooked up to battery think about the the era of the coming era of IOT where we're gonna have devices that are always on and permanently on which are expected to last 10 years on a single battery by using energy harvesting techniques energy becomes the key resource in making those things work efficiently and as we move more and more to always on devices with things like Google assistant you're gonna want your device on all the time or at least you're gonna want the CPU on all the time if not the screen so we're gonna have to worry more and more about power but the surprising thing that many people are surprised by is that energy efficiency is a giant issue in large cloud configurations this shows you what the typical capital cost would be like for a Google Data Center you'll notice that green slice there those are the servers but look at the size of that red slice that red slice is the cost of the power plus cooling infrastructure spending as much on power and cooling as you're spending on processors so energy efficiency becomes a really critical issue as we go forward and the end of Dennard scaling has meant that there's no more free lunch for a lot of years we had a free lunch it was pretty easy to figure out how to make computation more energy efficient now it's a lot harder and you can see the impact of this what this just shows you 40 years of processor performance what's happened to unit processor single processor performance and then multi processor performance so there were the early years of computing the beginning of the microprocessor era we were seeing about 22 percent improvement per year the creation of risk in the mid-1980s a dramatic use of instruction level parallelism pipe multiple issue we saw this incredible period of about 20 years where we got roughly 50% performance improvement per year 50% that was amazing then the beginning of the end of Dennard scaling what did that that caused everybody to move to multi-core what did multi-core do multi-core shoved the efficiency problem from the hardware designer to the software people now the software people had to figure out how to make use those multi-core processors efficiently but Amdahl's law came along reared its ugly head I'll show you some data on that and now we're in this late stage period where it looks like we're getting about 3 percent performance improvement per year doubling could take 20 years that's the end of general purpose processor performance as we know it as we're used to for so many years why did this happen why did it grind to a halt so fast well think about what was happening during that risk era where we're building these deeply pipelined machines 15 16 17 stages deep pipelines for issues per clock that machine needs to have sixty instructions that it's working on it once sixty instructions how does it possibly get sixty instructions it uses speculation it guesses about branches that Yanks it but instructions and tries to execute them but guess what happens nobody can predict branches perfectly every time you predict the branch incorrectly you have to undo all the work associated with that misprediction you've got to back it out you've got to restore the state of the machine and if you look inside a typical intel core i7 today on integer code roughly 25% of the instructions that get executed end up being thrown away guess what the energy still got burnt to execute all those instructions and then I threw the results away and I had to restore the state of the machine a lot of wasted energy that's why the single processor performance curve ended basically but we see similar challenges when you begin to look at multi-core things and those law Genie on toll road anvils law more than 40 years ago it's still true today even if you take large data centers with heavily parallel workloads it's very hard to write a big complicated piece of software and not have small sections of it be sequential whether it's synchronization or coordination or something else so think about what happens you get a 64 processor multi-core in the future suppose one percent just one percent of the code is sequential then that 64 processor multi-core only runs at the speed of a 40 processor core but guess what you paid all the energy for a 64 processor core executing all the time and you only got 40 percent 40 processors out of that slightly more than half that's the problem we've got to break through this efficiency barrier we've got a rethink how we design machines so what's left well software centric approaches can we make our machine can we make our systems more efficient it's great that we have these modern scripting languages they're interpreted dynamically typed they encourage reuse they've really liberated programmers to get a lot more code written and create incredible functionality they're efficient for programmers they're very inefficient for execution and I'll show you that in a second and then there are hardware centric approaches what we call what Dave Patterson I call domain-specific architectures namely designing architecture which isn't fully general-purpose but which does a set of domains a set of applications really well much more efficiently so let's take a look at what the opportunity is this is a chart that comes out of a paper by charles licensing and a group of colleagues at MIT called there's plenty of room at the top it they take a very simple example admittedly matrix multiply they write it in Python they run it on an team core Intel processor and then they proceeded to optimize it first rewrite it and see that speeds it up 47 times now any compiler in the world that can get a speed-up of 47 would be really remarkable even a speed-up of 20 then they rewrite it with parallel loops they get it almost a factor of 9 out of that then they rewrite it by doing memory optimization that gives them a factor of 20 they block the matrix they allocated to the caches properly that gives them a factor of 20 and then finally they rewrite it using Intel AVX instructions using the vector instructions Indian in the Intel Core right domain-specific instructions that do vector operations efficiently that gives them another factor of 10 the end result is that final version runs 62,000 times faster than the initial version now admittedly matrix multiply is an easy case small piece of code but it shows the potential of rethinking how we write this software and making it better so what about these domain-specific architectures really what we're gonna try to do is make a breakthrough in how efficient we build the hardware and buy domain-specific we're referring to a class of processors which do a range of applications they're not like for example the modem inside your cell phone right that's programmed once it runs modem code it never does anything else but I think of a set of processors which do a range of applications that are related to a particular application domain they're programmable there they're useful in that domain they take advantage of specific knowledge about that domain when they run so they can run much more efficiently obvious examples doing things for neural network processors doing things that focus on machine learning one example GPUs are another example of this kind of thinking right they're programmable in the context of doing graphics processing so for any of you have ever seen the any of the books that Dave Patterson and I wrote you know that we like quantitative approaches to understand things and we like to analyze why things work so the key about domain-specific architectures is there is no black magic here going to a more limited range of architectures doesn't automatically make things faster we have to make some specific architectural changes that win and there are three big ones the first is we make more effective use of parallelism we go from multiple instruction multiple data world that you'd see on a multi-core today to a single instruction multiple data so instead of fetching having each one of my cores fetch separate instruction streams have that separate caches I've got one set of instructions and they're going to a whole set of functional units it's much more efficient it's what do I give up I give up some flexibility when I do that I absolutely give up flexibility but the efficiency gain is dramatic I go from speculative out of order machines what a typical high end processor from our more Intel processor looks like today to something that's more like a VL iw that uses a set of operations where the compiler has decided that a set of operations can occur in parallel so I shift work from runtime to compile time again it's less flexible but for applications when it works it's much more efficient I move away from caches so caches are one of the great inventions of computer science one of the truly great inventions the problem is when there's low spatial and low temporal locality caches not only don't work they actually slow programs down they slow them down so we move away from that to user control local memories what's the trade-off now somebody has to figure out how to map their application into a user controlled memory structure cache does cache bugs they're automatically for you it's very general purpose but for certain applications I can do a lot better by mapping those things myself and then finally I focus on only the amount of accuracy I need I move from I Triple E to the lower precision floating point or from 32 and 64-bit integers - 8-bit and 16-bit integers if that's all the accuracy I need I can do eight integer operations eight 8-bit operations in the same amount of time that I can do one 64-bit operation so considerably faster but to go along with that I also need a domain-specific language I need a language that will match up to that hardware configuration we're not going to be able to take code written in Python or C for example and extract the kind of information we need to map to a domain-specific architecture we've got to rethink how we program these machines and that's gonna be high level operations it's gonna be a vector vector multiply or a vector matrix multiply or a sparse matrix organization so that I get that high level information that I need and I can compile that down into the architecture the key in doing these domain-specific languages will be to retain enough machine independence that I don't have to recode things that a compiler can come along take a domain-specific language map it to maybe a 1 architecture that's running in the cloud maybe another architecture that's running on my smart phone that's gonna be the challenge things ideas like tensorflow and OpenGL or a step in this direction but it's really a new space we're just beginning to understand it and understand how to design in this space you know I've been I built my first computer almost 50 years ago believe it or not I've seen a lot of revolutions in in this incredible IT industry since then the creation of the Internet the creation of the world wide web the magic of the microprocessor of smartphones personal computers but the one I think that is really going to change our lives is the breakthrough in machine learning and artificial intelligence this is a technology which people have worked on for 50 years and finally finally we made the breakthrough and the basis of that breakthrough we needed about a million times more computational power than we thought we needed to make the technology work but we finally got to the point where we could apply that kind of computer power and the one thing this is a this is some data that Jeff Dean and Dave Patterson and cliff Young collected that shows there's one thing growing just as fast as Moore's law the number of papers being published in machine learning it is a revolution it's going to change our world and I'm sure some of you saw the duplex demo the other day I mean it in the domain of making appointments it passes the Turing test in that domain which is an extraordinary breakthrough it doesn't pass it in the general terms but it passes it in a limited domain and that's really an indication of what's coming so how do you think about building a domain-specific architecture to do to do deep neural networks well this is a picture of what's inside a tensor processing unit the point I want to make about this is if you look at this what uses up the silicon area notice that it's not used for a lot of control it's not used for a lot of caching it's used to do things that are directly relevant to the computation so this processor can do 256 by 256 that is 64,000 multiply accumulates 8-bit multiply communits every single clock every single clock so it can really crunch through for inference things enormous amounts of computational capability you're not gonna run general-purpose C code on this you're gonna run something that's a neural network inference problem and if you look at the performance and you look at here I we've shown performance per watt again energy being the key limitation whether it's for your cell phone and you're doing some kind of machine learning on your cell phone or it's in the cloud energy is the key limitation so what we plotted here is the performance per watt and you see that the first generation tensor processing unit gets roughly more than 30 times the performance per watt compared to a general-purpose processor it even does considerably better than a GPU largely by switching from floating-point to lower density integer which is much faster so again this notion of tailoring the architecture to the specific domain becomes really crucial so this is a new era in some sense it's a return to the past in the early days of computing as computers were just being developed we often had teams of people working together we have people who are early applications experts working with people who are doing the beginning of the software environment building the first compilers in the first software environment and people doing the architecture and they're working as a vertical team that kind of integration where we get a design team that understands how to go from application to representation in some domain specific language to architecture and can think about how to rebuild machines and new ways to get this it's an enormous opportunity and it's a new kind of challenge for the industry to go forward but I think there are enough interesting application domains like this where we can get incredible performance advantages by tailoring our machines in new way and I think if we can do that maybe we'll free up some time to worry about another small subproblem namely cyber security and whether or not the hardware designers can finally help the software designers to improve the security of our system and that would be a great problem to focus on thank you for your attention and I'm happy to answer any questions you might have Thanks can you talk about some of the advances in quantum and neuromorphic computing yeah so quantum that's a really good question so my view of this is that we've got to build a bridge from where we are today to post silicon the possibilities for post silicon the too big there are a couple I mean there's organic there's quantum there's carbon nanofiber there's a few different possibilities out there I characterize them as technologies of the future the reason is the people working on them are still physicists they're not computer scientists yet or electrical engineers they're physicists so they're still in the lab on the other hand quantum you know if it works the computational power from a reasonably modest sized qubit let's say 128 corrected qubits 128 corrected qubits meaning they're accurate that maybe that might take you a thousand cubits to get to that level of accuracy but the computational power for things that make sense protein folding cryptography of 128-bit qubit is phenomenal so we could get an enormous jump forward there we need something post silicon we need something post silicon we've got maybe you know as Moore's law slows down maybe another decade or so before it comes to a real halt and we've got to get an alternative technology out there because I think there's lots of creative software to be written that wants to run on faster machines I just at the end of your presentation you briefly mentioned how we could start using hardware to increase security would you mind elaborating on that sure sure okay so here's my view of security everybody knows about meltdown and Spectre first thing about meltdown inspector is to understand what happened is an attack that basically undermined architecture in a way that we never anticipated I worked on out of order machines in the mid-1990s how long that bug has been in those machines since the 1990s and we didn't even realize it we didn't even realize it and the reason is that basically what happens is our definition of architecture was there's an instruction set programs run I don't tell you how fast they run all I tell you is what the right answer is side-channel attacks that use performance to leak information basically go around our definition of architecture so we need to rethink about architecture you know in the 1960s and 1970s there was a lot of thought about how to do a better job of protection rings and domains and capabilities they all got dropped and they got dropped because two things first of all we became convinced that people were gonna verify their software and it was always gonna be perfect well the problem is that the amount of software right is far bigger than the amount of software we ever verify so that's not gonna help I think it's time for architects to begin to think about how can they help software people build systems which are more secure what's the right architecture support to make more secure systems how do we build those how do we make sure they get used effectively and how do we together architects and software people working together create a more secure environment and I think it's gonna mean thinking back about some of those old ideas and bring them back in some cases after I took my processor architecture class which used your book I hope it didn't hurt you hopefully not I had a real appreciation for the simplicity of a RISC system it seems like we've gone towards more complexity with domain-specific languages and things is that just because of performance or has your philosophy changed what do you think no I actually think they're not necessarily more complicated they have a narrower range of applicability but they're not more complicated in the sense that they are a better match for what the application is and the key thing to understand about risk the key insight was we weren't targeting people writing assembly language anymore that was the old way of doing things right in the 1980s the move was on UNIX was the first operating system ever written in a high-level language the first ever the move was on from assembly language to high-level languages and what you needed a target was the compiler output so it's the same thing here you're targeting the output of the domain-specific language that works well for a range of domains and you design the architecture to match that environment make it as simple as possible but no simpler with the domain-specific architectures do you have examples of what might be the most promising areas for future domain-specific architectures so I think the most obvious one is are things related to machine learning I mean they're computationally extremely intensive both training as well as inference so that's one big field virtual reality and virtual reality augmented reality environments if we really want to construct a high quality environment that's augmented reality we're going to need enormous amounts of computational power but again it's well-structured kinds of computations that could match to those kinds of applications we're not going to do everything with domain-specific architectures they're gonna give us a lift on some of the more computationally intensive problems we're still gonna have to advance and think about how to push forward general-purpose because the general purpose machines are going to drive these domain-specific machines the domain-specific machine will not do everything for us so we're gonna have to figure out ways to go forward on that front as well what do you think about some emerging memory technologies how do they impact the future computer architecture thank you yeah that's a really great question so as we get to the end of d Ram's i think some of the more innovative memory technologies are beginning to appear so called phase change technologies which have the advantage that they can probably scale better than DRAM and probably even better than flash technologies they have the advantage that lifetimes are bad - then flash probably flashes it wears out some of these phase change or memristor technologies have the ability to scale longer and what you'll get is probably not a replacement for DRAM you'll probably get a replacement for flash and a replacement for disks and I think that technology is coming very fast and it'll it'll change the way we think about memory hierarchies and i/o hierarchy because you'll have a device that not quite as fast as the RAM but a lot faster than all the other alternatives and that will change the way we want to build machines as a person you think about education quite often we all saw Zuckerberg getting [Music] having a conversation with Congress and I'm excited to see children getting general education around computing and coding which is something that a lot of us didn't have the opportunity to have where do you see education not only for k12 grad post-grad etc but also existing people in policy making decisions etc yeah well I think first of all education has become a lifelong endeavor nobody has one job for a lifetime them anymore they change what they're doing and education becomes constant I mean you think about the stuff you learned as an undergrad and you think how far how much technology has already changed right so we have to do more there I think we also have to make more people technology society needs to be more technology savvy computing is changing every single part of the world we live in to not have some understanding into that technology I think limits your ability to lead an organization to make important decisions so we're gonna have to educate our young people at the beginning and we're gonna have to make an investment in education so that as people's careers change over their lifetime they can go back and engage in education not necessarily going back to college it's gonna have to be online in some way but it's gonna have to be engaging it's gonna have to be something that really works well for people hi only freak BBC just wondered what your view was on the amount of energy being used on Bitcoin mining and other cryptocurrencies and that sort of thing yeah sure I could build a special-purpose architecture to mine bitcoins that's another obvious example of a domain-specific architecture for sure so I'm a long-term believer in cryptocurrency as as an important part of our space and what we're gonna have to do is figure out how to make it work how to make it work efficiently how to make it work seamlessly how to make it work inexpensively I think those are all problems that can be conquered and I think you'll see a bunch of people that have both the algorithmic heft and the ability to rethink how we do that and really make crypto currencies go quite quick and then we can also build machines which accelerate that even further so that we can make Trant the cryptocurrency transaction should be faster than a cash transaction and certainly no slower than a credit card transaction we're not there yet but we could get there we can get there with enough work and I think that's where we ought to be moving to what do you think about the future operating system has to have - yeah the future of operating system you said yes yeah so I think operating systems are really crucial you know way back when in the in the 1980s we thought we were gonna solve all our operating system problems by going to kernel-based operating systems and the kernel would be this really small little thing that just did the core of functions of protection and memory management and then everything else around it would be protected basically and what happened was kernels started out really small and then they got bigger and then they got bigger and make a bigger and all of a sudden almost the entire operating system was in the kernel primarily to make performance to make it perform in a performance efficient and the same thing happened with hypervisors they started really small in the very beginning and then they got we're gonna have to figure out how we structure complex operating systems so that they can deal with the protection issues they can deal with efficiency issues they can work well we should be building operating systems which from the beginning realize that they're going to run on large numbers of processors and organize them in such a way that they can do that efficiently because that's the future we're gonna have to rely on that in your intro video you mentioned this chasm between concept and practice and also in your talk you've mentioned that hardware is vital to the future of computing given that most investors are very Hardware averse especially this day na where do you expect that money to come from is that something that will come from governments or private investing how are we going to fund the future of computing is really what my question is yeah it's a good question I mean I think the answer is both you know certainly Google's making large investments in a lot of these technologies from quantum to other things I think government remains a player so government you look at how many of the innovations were used to the Internet risk the rise of VLSI modern computer aided design tools were all had funding basically coming from the government at some point so I think the government should still remain a player in thinking about what's the one area the government is probably funded longer than anybody else artificial intelligence they funded it for 50 years before we really saw the breakthrough that came right so they're big believers they should be funding things long term they should fund things that are out over the horizon that we don't yet really understand what their practical implication may be so I think we're gonna have to have that we're gonna have to have industry playing a big role and we're gonna have to make universities work well with industry because they complement one another right they do two different kinds of things but they're complementary and if we can get them to work well then we can have the best of both worlds you talked a little bit the difference between the memory hierarchy and storage that is coming up with these new memory technologies have you seen any applications where the compute and the storage get combined kind of more like the brain yeah I think I think increasingly we'll see things move towards that direction where you the the software takes care of the difference between what is in storage and storage quote-unquote right because it may actually be flash or some kind of next-generation memory technology and what's in DRAM what you need to tell me is what's volatile and when do I have to ensure that a particular operation is committed to non-volatile storage but if you know that you know we've got log base file systems you got other ideas which move in the direction of trying to take advantage of a much greatly different memory hierarchy greatly different storage hierarchy than we're used to and we may want to continue to move in that direction particularly when you begin to think about if you think about things like networking or i/o and they become major bottlenecks in applications which they often do then rethinking how could we could do those efficiently and optimize the hardware but also the software because the minute you stick an operating system transaction in there you've added a lot of weight to what it costs to get to that storage facility so if we can make that work better and make it more transparent without giving up protection without giving up a guarantee that once something is written to a certain storage unit it's permanently recorded then I think we can make much faster systems so do you see the implementation of a domain-specific architecture being implemented as hetero type or do you see it off die off chip type implementations or both I think both I mean I think it's a time of great change the the rise of FPGAs for example give you the opportunity to implement these machine try them out implement the men FPGA before you're committed to design a custom silicon chip put it in an FPGA unleash it on the world try it out see how works see how the applications map to it and then perhaps the site whether or not you want to freeze the architecture or you may just want to build another next-generation FPGA so I think we'll see lots of different implementation approaches the one thing we have to do you know there was a big breakthrough and how hard it was to design chips that occurred from about the mid 80s to about 1995 or 2000 things have kind of ground to a halt since then we haven't had another big we need a big breakthrough because we're gonna need many more people designing processors targeting particular application domains and that's gonna mean we need to make it much easier much cheaper to design a processor I'm wondering as a deep learning engineer for private enterprise what is my role in pushing forward DSA yeah well I think I roll is vital because we need people who really understand the application space and that's really critical and this is a change I mean if you think about if you think about how much architects and computer designer hardware designers have had to think about the applications they haven't had to think about them all of a sudden they're gonna have to develop a bunch of new friends that they can interact with and talk to and colleagues they can work with to really get the insights they need in order to push forward the technology and that's gonna have that's gonna be a big change for us but I think if something is absolutely crucial it's great for the industry too because all of a sudden we get people who are application experts beginning to talk to people who are software domain experts or talk to Hardware people that's a terrific thing you mentioned the performance enhancements of domain-specific languages over like Python for instance but they're also much harder to use so do you think software engineering talent can keep up in the future yeah I think the challenge will be the game we've gotten in software productivity in the last 20 or 30 years is absolutely stunning it is absolutely stunning I mean a programmer now can probably write ten to a hundred times more code than they could 30 years ago in terms of functionality that's phenomenal we cannot give that up because that's what's created all these incredible applications we have what we need to do is figure out all of a sudden we need that we need a new generation of compiler people to think about how do we make those run efficiently and by the way if the gap is a factor of 25 between C and Python for example if you get only half that that's a factor of 12 times faster any compiler writer that can produce code that runs 12 times faster is a hero in my book so we have to just think about new ways to approach the problem and the opportunity is tremendous are there any opportunities still left in x86 as far as like lifting the complexity of the ISA into software and exposing more microarchitecture to the compiler it's tough I mean I think the Intel people have spent more time implementing x86 s than anybody's ever spent implementing one is a 1 instruction set ever they've mined out almost all the performance and in fact if you look at the tweaks that occur for example they do aggressive prefetching in the i7 but you look at what happens with prefetching some programs actually slow down now on balance they get a little bit of speed up from it but they actually slow down other programs and the problem right now is it's very hard to turn that dial in such a way that we don't get overwhelmed with negative things and I see my producer telling me it's the end of the session thank you for the great questions and for your attention [Applause] [Music] 