 All right. Let's get started. My name is Jay Liu, and a long history in MSR, and now, I'm in the Visual Intelligence Team in AI Perceptions. So today, it's my great pleasure to introduce Professor Yiran Chen from Duke University. Yiran graduated from Purdue University in 2005, I believe.  Yes.  Was assistant and associate Professor at University of Pittsburgh until 2007, before he moved to Duke. Now, in addition to the academic duties, he's also the Director of the NSF Industrial University Collaborative Research Center, for Alternative Sustainable and Intelligent Computing. Big effort he is putting together. So without further ado, welcome, Yiran.  Thank you so much, Mr. Jay. Okay. So it's my great honor to be here to introduce our research work. So before I came here, I talked to Jay about what I should give because, we're certainly going to talk about many, many things, okay. In my group, we- actually my wife is also the faculty member in the same department at Duke. So, we actually have a fairly large group, where we have about 30 PhD students [inaudible] staff. We are doing from the chip they design until the some application. Eventually, we decide to give a topic regarding how we're able to obtain some Light and Efficient Deep Learning Network, and how we're able to run those network within some hours. Okay, so that maybe is something of interest to MSR. This is the outline. I will give virtual introduction about why we want to choose this topic, and why this couldn't be our major efforts in the resource. We're sure everybody know this. Then, we'll talk about three spotlights about how we're able to do quantization, and little pruning, and do some compression for different levels of the computing platform. One is on the chip, one is actually running either single machine, and the one is on the distributive apply forms. Okay. That give you some idea. Then we share our ideas and a perspective about those things. Okay, people actually know those and very well we actually experience up and down in the Neural Network Research. Actually, everything we're doing here, is not that different from what we invented in certain years ago, when at that time, people invented a Convolutional Neural Network, okay? But there are three reasons for us, what they are able to apply those in the real scenario, and now we know those ratings includes vanishing gradients because due to this vanishing gradients we're not be able to efficiently retrain our neural network. Now we know we need to do them on your double flowing point who's on first so double precipitation to enable it. At that time, we didn't observe any benefits by adding more layers but now we know that because need to add up enough layers. So, by that time, we talked about certain four layers, it will be that deep neural network. Now we talk about Southern in the networks and, of course, we don't have very high performance computing devices. Now it's a permanent power of the single chip is actually one million to over 10 million times higher than the one we're using 20 years ago, so that's will be the rhythm. Of course after people inventing all of those technologies which can support those applications of the neural network, there is a no wonder way now have the renaissance of this neural network. Starting from 2006 when [inaudible] hops to paper about the GPU with [inaudible] everything. So, machine learning is a hot topic in academia I'm actually a grab some very interesting figures from some news, on their side show the number of the publication and login to that deeper learning in academia from different countries, you see the number of those who have propagation increases exponentially in the last couple of years, so that's only but 2015, but I believe there are more and more and actually China and US is something that leads this trend. Also you talked about restoration of the nymphs and the actual restoration number is increased use financial [inaudible] A20. Alright so, they used to only register a couple of 100 back to only a few years ago, but now you're only able to find the ticket right because this year, the reds are full registration is gone in about 12 minutes I believe, so a lot of people come for learning about this, but luckily we have some type paper I get in, so I got one and you know restoration because I'm the author but, that becomes something created. Actually there there'll be another figure I didn't show here, is like if you follow that trend, if you don't count show the registration number, I think about 2030, 2040, the number of registration on the nib so we'll actually see that the total population on the worse well this quite enough about this. I was often asked by many people about how they think about the future of machine learning certainly where you're experiencing the peck after the research? So to answer this question, there's another charts which show it's a prediction, by basically about technology adoption by the market and I also saw in the first. So this is a chart will show the stock price of this new technology industry in the past, in about 20 years. You will see the PC, the future phone, the smartphone, AI first, so you will see the up and down in the array slowly grow and then reach the peak and then quickly drop, okay. But a cycle they can be about, as sure as three years or as long as the seven or eight years. So if we look at the last couple of years, we're actually all just took off and now, if we look at a chart and notice it's about two years ago, we're not at the peak, so there are maybe one more year, we're going to reach the peak or maybe two years so then we'll get into see the drop okay. So, that's the prediction in terms of the investment, and also the prediction of the market value. Anyway, so my point is, after that we will see the matureness of this technology, so we're able to do something before risks are taken after that, you'll start to use this technology to different industries. So why the target of deep learning and so hot now? So there's certainly some driving horses, so we had a big data, we have algorithm, we have the computational power and I don't need to list out all those numbers averaging about this, and in ideal Kino we're actually doing some relevant research for doubling up applications sidewalk with some companies it on something like a image segmentation, of course from the computing side, and also some privacy, the robustness on the first and the operand thigh we are primarily working on acceleration technology for the neural network. So don't get me wrong I'm not really a die hard, the AI guy, I'm more like doing those risks or from the computing side. Which means I am trying to improve the computing efficiency for those technology. Also the Honda Howard side we're doing almost Irish thing we can touch, for the computing psi. But primarily from the computing how platform, so where did in the under chip? Okay so what kind of GPU or CPU optimization? We are working on the distributed computing, I will show later on, especially on the hydrogen yes contest. Okay. Okay. So, now let's move to the fourth topic. So, I was thinking, what could be the good sequence for me to deliver my talk, either from the top-down or from the bottom-up. Then I decided from the bottom-up. So, then give them maybe closer and closer to the main focus of the IMSR, but I try to avoid the hour as much as I can because I know I'm giving a talk in Microsoft, right? But then I realized anyways a student gives you some background about what is our concern when we are running those things in the hardware platform. So, that's why we decided to start with something we're optimizing on the chip. So, it gave us some basic idea about, how we're able to do the quantization of the neural network or if we have some constraints from the Howard. So, the one example is IBM TrueNorth chip. I picked that one because that's a huge dream case for the quantization. But we also have a lot of other works, regarding the conventional a basic design or GPO so on first. But I'm not going to touch this in today's talk. So, let's talk about the IBM TrueNorth chip. So, this project was launched in 2008, that's about 10 years ago. That's actually one of the oldest hardware dapper projects to address to modern neural network and I'm not talking about a deep neural network, but neural network application. So, the one chip after TrueNorth it enclose the 4,000 they call the neurosynaptical course and the one core is composed of 256 by 256 nodes. We'll call that one a synapse. So, one core has about 256, the input neural and above neurons, so totally on one single chip, we have about a one million neurons with a 256 million synapses, that's the key. Also array data is represented as a spikes across a whole chip, and a spike will transfer between the found course in that way with a very low frequency, about a 1K hertz. We have an hour to connect all those cores on the our single neuron is actually have very low resolution or presentation, which means, they can only represent the three levels, negative one, zero, and a positive one, no others. So, that's why I called it that can be used dream case, with a kind of Theta, it's INTAs supported by GPO, so on the first. So, the whole power of this TrueNorth chip is very low, it's about a sixth to five milliwatts, during the real time computation. If you consider GPU can either let 200, GPO 300 watts as they go viral. So, how we're able to use this chip? So, we have a new database, another with the genre of the raw pixels, and then as you can put into a cafe or any other framework to train the neural network, that would be like a water we are doing in the conventional task, and after that, we will go through with what we call the CPE, they'll collide the programming environment to deploy the training the neuron are now are down to the TrueNorth chip, and when we're doing this, we basically need to adopt a low prestation representation of the synaptical weights, and also the input, will show later on. Then we will suffer from some accuracy degradation. After that, we'll have absorbed pixels for the task and we will use the Spike Encoder to generate the binary spikes at the input of all the TrueNorth chip, and we're sending to a cell to do the computation, and then we receive the output, which was the accuracy or classifications on first. So, now let us also look at the detail about how we are going to map our neural network down to this hour. So, we have the neural network that we show here, so let me say either they can be represented either y equals w times x plus b, b with the bias, x will be the input, w will be the weights and the z equals h the function of y. So, the h will be the activation function, and that they will be the output. When we are mapping those things into the TrueNorth chip, it's a input x will be representing the spikes, and the West w, will be mapped to the crossbar will show here, when there was always a w prime remember it's the resolution of the double-prime, is only three lie levels. Nurture one, zero, and a positive one, and then we generate the output which is a y-prime, and then we go through some cmos circuits, we generate the operational at z prime, and z prime is basically a very synapse simple, Mecca colored paste neural model, is a base case shows during the one at the output okay. So, how we're able to map the inputs to this, so the input can be adopted by the TrueNorth chip. Let's get one example as MNIST application, we have a 28 by 28 pixel pictures, and then, we know that the maximum number of the input will be the 256. Which means, we have two yields of four 16 by 16 our block, to cover the whole area of one MNIST picture, let me show here. Okay, there may be some overlap, which is fine. Then, the 16 by 16 pixels will be mapped down to there's a 256 at the input, and the central one core. That's why we need a four cores to compute the whole things. That is the open working proposed by IBM. So, why we need to tolerate into the deploy day sedation into accuracy, because in such mapping, we need to map the floating point of value down to the integer or even the three level representation on the TrueNorth chip. For example, if we have some inputs, which is the 0.75. So, we need to have a fourth sparks there within one frame, and we only injects three often at least neighbor another one, so then we can root to represent 0.75. You can imagine that if we have 0.2, we'll be in trouble because we can only represent 0.25 or a zero. So, that is a quantization lost that we will have. Also, if we look at, later doesn't work very well, if I have the floating-point weights, which is 0.8, we may have five copies of this crossbar, and then we'd turn on four of them to be one, and a one as a zero, so it's amortized value will be 0.8. Yes, please.  We have trained the neural network, do you take that constraint into-  I don't.  So, this is trained as a floating point or a neural network and you're trying to convert it?  That's the key, they actually suffer from this loss, is something we're trying to solve. We'll show later. So, you can imagine that if we do such a my pin, we either suffer from the longer computation time because we need to have a spikes to represent a floating point value or we need to suffer from a larger hardware costs because we need to have a multiple copies and I will turn on and turn off the weights. After this, we can prove mathematically, if we have enough number of sparks and also the copies, then it's a mathematical use factorization of the output, which were the y prime, will be called to y. If I have sufficient number of the copies under the spikes, and it's easily additive because we cannot really afford a infinite, it's a number of spikes and it's a Carl copies, will suffer from the loss. For example, originally for a trend the MNIST accuracy to be 95 percent in the cafe for maps are seen down to the TrueNorth with the one copy of the crossbar, which is a neuron the called a synaptical course, and other one spike for a friend, which means you have a binary input, the accuracy will drop down to 90 percent. If I increase the number of the sparks for a friend or increase the number of hardware copies, you can basically restore the accuracy up to 92 percent or 94 percent by paying the longer competition ton or the larger hardware cost, and this is something you're going to trade off, okay. When we received the check because we were the force of several- Yes?  [inaudible]?  I'm sorry?  Is it cheaper to add 16 neural networks or one is supposed to bring precision?  So, if I have multiple copies, then the runtime will be the same, whether it's running simultaneously, like in parallel. But if you have a multiple of spikes, then you need to input the multiple spikes which are going to be proportionally increase their completion time. Yes.  Ninety percent accuracy loss, I mean five percent accuracy loss, does it require retraining at all or just purely in matter-  Purely in mind. That actually is a rate rhythm we suffer from this loss. So, we better do something in the training process. I know you start to develop the idea about why we're going to do. I'll show you in the next slides. So, when we received this Chevy in 2014, that's back to four years ago, so we were the fourth, several groups receive the Chevy in the world and it's a software and not the PDK, that DK, so and so at first, but the chip is so small because if we want to run something meaningful, you'll need to have a larger number of [inaudible]. So it makes that's very hard to run anything beyond and miss. So we don't like it, but every work costs me 25,000 bucks. It's certainly big amount of money for world-state, may not be a good money for you guys. So, we're trying to see if we can do some optimization to minimize the hardware costs, and also runtime because I know that it's a trade off. So, what do we do? Either we're trying to figure out the Delta Y, which is the difference between the Y prime and the Y in this computation. Y prime will be the one we receive on the chip, and the Y will be the fluid point of value with on the training. We found out the expectation of the Delta Y will be zero. We have already prove this because if a number so big and will be the same. But the variance of the Delta Y is not zero, is basically the Sigma of the variance of a W_i prime times X_i prime. The X_i prime will be the quantized input, and W_i prime will be the quantize of weights that'll be not a zero. However, we cannot really control the input because the input depends on the applications on first. So now, our goal is trying to minimize the variance of a [inaudible] prime. That is the idea. So, we do very simple mathematics, right? So, this is a very simple one to show the variance of W_i prime it cost, the expectation of W_i prime square minus the square of the expectation of W_i prime. So, W_i prime can be 0, 1, and negative 1. So this can be normalized between zero and one basically because so we have the probability to turn on and turn off the 0 and 1 weights at the P_i and then the value of this will become P_i times 1 minus P_i. So P_i with the probability, we turn off the one bits, that- if you look at this. So what do we all be it's a condition to achieve the minimum product between P_i and 1 minus P_i. One P_i, it cost a zero. What's the maximal value of this product when P_i is 0.5 and the value will be 0.25. This actually we learned that the front of the elementary school. I don't need to have any mathematics on this. So, what does that mean? That means that if you look at the variance of regenerates the mild lighting skin, the eighth the normalized or training of width it's between 0 and 1 and it's around 0.5 then my pink variants will be the largest one which is 0.25. If the 20 the value is actually non normalized value is actually 0 and 1, then the variance will be 0. This is very simple right because if we normalize a width, if the width we know we can only represent a 0 and a 1 in our hardware. So if it's a trainee than width will be zone one will perfectly mapped this width to 0 and 1, but if we're trained something which is our 0.5, which one we shall choose, 0.01 or 1? Either one will give us the largest of variance, very simple idea here. Okay. Now the question becomes how we're able to train our neural network to make sure you know the width or majority of the width will be 0 and 1 in our new now what would you and not- in other words, close to the binary neural network, okay? If you still remember how it creates a loss function, we're basically trying to reduce the difference between the target output and also the one we received. So if we don't do an insane then the training whereas these be ocean will become like this. We'll still see the [inaudible] and a 0 and 1, but anything else if you will be evenly distributed across the normalizer rent versus own one. So if a map this diffusion of the West down to the tuners there will be like this, so the brighter color means in a lot of variants you will see actually we see a lot of violence when we're mapping that [inaudible] to the tuners. But if I asked a panel div, so basically which generates a penalty function will give the largest penalty when we train the neural network, know whose width is about 0.5 then the large penalty will be 0.5 and the lowest one be 0. We're basically trying to push the training of width to 0 to 1. Okay so try to avoid drops to 0.5. It's a very simple penalty way out on the loss function and the result will be like this. So the distribution of the trend of width will be closer to 0 and 1 are very minimal number of wells are still around 0.5. Okay. So what are your trend it will be what you'll got [inaudible] I think this one, the totally blind which means Where clouds for the rage and or triennial bad. It's actually we only change one line of code that IBM has sent to us. So just one line. Let's look at the results. So red one is the result directly derived from the IBM IST PDK, and the yellow one is our mattered, okay? So in adding cases, the number of spikes per frame at number of copies will outperform baseline of the IBM products like this one or, remember, one issue one line. And if I look at the speedup achieve the same accuracy we can't choose a 6.5 times the speed of [inaudible] because we're trying to dramatically reduce the number of spikes no [inaudible] or we can reduce the core occupation which is a hardware costs by two third, we only need a one third of this [inaudible]. And we see very consistent to relate- relation between the spike per frame on the core reduction and so on, so on, and so on, the first. Okay. Due to the contract where we're not able to publish this thing until the 2016 or '15. They hold our public on for more than one year. They can file other things there. They actually incurred this things in our latest SDK is give to not only the academia but also to their garment costumers. But the downside of this, they think then they don't allow us to touch their code anymore. They just gave a ban on any codes, a done deal, okay. The UN, don't touch our code. So anymore so that's the first thing I want to talk. So a second one that'll give us Y let's move our focus down from the chip level to the single machine level. Let's say, you know, how we are able to simplify the neural network and then promote, you know, it's a computational efficiency for the neural network where core the structural spar [inaudible] of our deep neural network. So if you look at, you know, it's a trend of the neural network and we have more and more parameters running and every parameters that we need to count compiled which means it's a competition cost. So it's almost a proportional to the number of the parameters that we have in the neural network, right? So how to reduce the number of the parameter in deep neural network is becoming the full focus in the celebration of the neural network and people try and and many, many different methodology to reduce those things in a way that no sacrificing the classification accuracy [inaudible]. So in 2015, some people propose some methodology. Trying to weight regularization of L1-norm. The basic idea is very similar as a way yields in the True North, the chip. They basically gave the penalty to the west which is are very large, okay. And they try to push the distribution of the west close to the 0 and because they can't safely remove those West by assuming it's a small West won't generate a huge impact on the output. That's usually going to be a reasonable assumption if we consider the input is going to be very rough and- So their shoulders out saying okay so they can achieve very highest bar to the by removing more than 90 percent west but still achieving good accuracy, and the theoretical speedup will be very large in no more than ten times. Remember, I'm talking about semi-radical speedup. Okay. And also the Song Han, they actually they proposed all those neighbors to 25 during a [inaudible] whether you tend to front a convolutional layer to the fully connected layer. I was asked to serve on the way and they achieved a very similar result I'll show here. But if I look at this, there is a [inaudible] here. The useful is if we were really taken their code running on the GPU- You will now go to practical speedup for many layers. So, this is the result we show when we're running their costs, okay? We're running on the GPU, so on different GPUs. You will find out, when the speedup is larger than one, that basis showed a positive acceleration. If the speedup is smaller than one, that will be the negative, which means a slow down the computation. If you look at different colors on layers, you will find out not every layers will give us a positive speedup, and many of them will be the negative, which means that you'll even slow down the computation in the different GPU or platform. Why? It's very simple because the yearly appearance of those width is smaller than a threshold will show up in the random position of a neural network. If I pick up those running this and remove those words, they will generate random sparsity where we will create a lot of a whole when we are storing those words in the memory. That will generate irregular memory access where it reads one width we're respecting the nicest data will be the one-way word where professional staff will log this one in while we got cache miss. Then, went to need to have a poor cache locality when you go through the min memory to the disk or got another one, and we keep doing the stuff, yes?  Does this also apply to push messages that applied fully-connected layers or other convolutional layers?  Also both.  Both?  The second one, what if I don't just remove the visual parameters from the foci, but I remove foci as a whole?  You're talking about the one-way wave proposed. Yeah, you're talking about the one-way proposed.  Okay, that's it.  That's what's called a structural one yes. Well, I'm joking. But, I don't like to give a talking I may suck. You guys [inaudible] you know the answer. So, what's the point for me to [inaudible] , no kidding. Wherever, so people actually trying to hard code there's a nonzero weights in the source code. Okay, that's a very inefficient idea or they can customize their hardware. They do some lower installed to do the work, that's be very for me very dumb idea, okay? But, the Michelle's just as even as you just mentioned that. There is a simpler way to solve the problem, which is, we don't have to remove those weights in a random way. We should remove those things in the structural way. We can remove the whole lines, the whole column, the whole block, and we're still maintain the locality of the cache, accesses on first, right? Then, we'll switch to the good speedup, so then actually convert this theoretical spit out to the particle speedup, that is the key. Now, the question becomes how we are able to do it, and in other words, in which levels of granularity we can read, we can do this. So, we proposed runway called the group law, so is not new. Let me put it this way, Group Lasso is not on your weight. We didn't [inaudible] event. But, the basic idea is, besides the difference between the target output will generate another term we call a Group Lasso, that we saw here as a Lambda g times the RG is on the first. This item will partition the weight in the different groups and within the group of this to maintain the special correlation or locality, okay? So, we can safely remove the whole block, but still maintain the locality of all the cache. One example like this. Let's say you have three words, okay? So, W0, W1, W2, and we know W0 and W1 is actually stay in the same block. So, we basically partition the three wasting the two groups and you can either minimize this group or minimize that group. We can safely remove one of them or both of them, okay? So, that's big case. But, we don't individually remove one out of W0 and W1. Mathematically, we can prove that is basically it means we are imposing the constraints like this a blue space show here and in this will be the optimization source surface. We are trying to move those two eventually find out across. That'll be the optimization points, okay? So, move down to the visualization of the neural network. So, we basic can penalize I important to filter then the channels, we can remove all of them we. We can learn actually the corresponding position on each filters. We're actually remove those positions because they also storing the whole column and I'll call row in the main memory. We can actually remove the whole layers, but we need to bypass the input to the output. I've showed here. We'll call that one the filter wise, the channel wise, the shape wise or depths wise. There are many maddening correlation between different structure, which we come way complete, okay? I don't need to group where a group of them raised respectively. So, let us look at the radar. Unless they get wanting double alone and I'm nist, okay? So, the our baseline we will have the 0.9 nine percent error. So, we have the number of the filter. For the different layer will be the 20-50. The channel number 1-20. This will be the original baseline. If we want to maintain a similar error rates up either 0.8. The number of the filters will reduce down to a five to 9919, and a channel number we're down to the one to four and a way we reduce the number of flops and down to 25 to 7.6 percent. That is one. The speedup is now like a four to 10 times. That would be if we're running on the GPU, that would be either 1.6 or five times, and that's a real speedup. If we can tolerate relaxing 0.1 percent of accuracy, we will have number of the filter will be down to the 3-12. The channel number 1-3, and we'll even generates a smaller number of the flops. So, the loggers speedup. If we'll look at how we visualize the field of filters, you ever have a soma and different things. But, actually you've already instructed the major features, now that a Manning was still maintained the majority of the important features here. So, that's the key. So, we have a lot of redundancy, which we can really safely, and if regarding the learning field or share, we have to kind of remove the original Phi to Phi, two down to the 21 and even to saven. By maintaining the similar accuracy, there upon five percent or one per person and lobby or similar array radar. If we combine them to gather or they can remove the whole row and the whole column, that are still going to see the good retaining of those features, and the error rate is about one percent for martial law or the neural network, and you have the low sparsity and a colon sparsity we're pulling here. So, and it can receive the speedup, okay? So, that is the combination of those knowledges. So, it is more like this, okay? There's basically, we remove the whole column [inaudible]. We've learn on the CPU and the CPU or we'll show here, okay? So, let me show you what the Haganah present this one, okay? So, this is a higher requirement on the lie accuracy or the error rates. So, you will see that. You will see that for the L1 norm that people use for Rwandan sparcity, a lot of layers you actually got an active will spit out by the way it's a show and can buffer us all with a got that positive one. So, that's the key, and if you have lost to tolerate about two percent accuracy loss because some people like to trial the trainings on a first, we can even push the speed of fall, fall off father, and the L1 we should grade still gave us an active speedup in some cases. Actually, you feel compare with a GPU and the CPU raise odd, you will find out that GPO is a more sensitive to the structural data storage. So, which means the Rwanda's bars and is actually not favored by the GPU platform because the CPU route come is a more sensitive to societal such stuff, and if we can achieve a higher improvement compare with our sign. For the CPU because anyway there'll be bad and around the baseline, so no one won't be that sensitive. The second one way away to learn. As this outer knowledge that had been adopted by many, company which surely I don't know, and why you should in part is how we are able to remove the layers, okay? People actually, it's not new. People had been doing this for quite a while. So, very original and last one with 20 or 32, we can constantly reduce the number of layer down to 14 and 18 without sacrificing the IRR, and if you look at trend of removing those layers, they actually another monotonically reduce the error rate. So, the best I see, as for the 18 will be the optimal number and after that, if you keeps reducing the number of layers, the area will increase. But, before that, you are not going to see the monolical trend because there's up and down there. The detail analysis you will find out you know which layer we are removing is actually the intermediate layers, okay? Because in the beginning, you are basically process that data and after that your instructor the features first and then wind the method doesn't really give you much help compare with adding other layers. Yes?  Sir, does this sort of observation or incentives change across different data science or different tasks? Reasons, I see this is resonant for several 10, right? What if the [inaudible]?  We see the similar trend, but there are some variations across the DMV find your network. Yeah.  Here's my question, I guess. So, for this pre-trained network as if we're deminding that. Where as where's this separate and [inaudible]. The amount of tasks that you're doing is very different. One is a smaller constant down is much flatter causes. Further saying that book's structure, right? How generic that needs to be and how much room you have for Cooney? Does it depending on the actual house that you are targeting?  I understand. Unfortunately, there is a radical concurrent about, if you have a different neural network with different structure. How well the pruning will be able to apply? It was just a try.  Okay.  We're doing some research now, trying to understand how the different layer process the features, and how this information will be transferred to the different task and the different output. So, it's relevant to the interpretable neural network, right? So, we thought some result about this. Because if we look at the trend, we see the impact of the neural network on the upper, basically a curve like this. We start from beginning and where there is some peg and after [inaudible] there is the drifting down. Well, I show some data maybe off line. But, we still don't have a mathematical proof why that is a case. But our wish is, there were some layers which actually in the middle of this layer, for one neuronal, even they don't really impact the output them much in this pruning. But this layer become very critical if we design the transfer learning neural network. This is actually very interesting. We don't understand why because we're pretty sure that's this, okay? But the transfer learning show us something else. I can show you the result later. Yes. You want something?  I am Dr. Trent Stevens.  Yes. Where your work so.  Our work is more aggressive. So, just publish it [inaudible].  So, that's a very good question. But we don't have a mathematical proof. We only have some arbiter re aeration there. Yeah. So, last one topic, I have about 10 minutes. We'll have one about, TernGrad. So, this actually the one talk we've given in the last year nips. Luckily, this work was one of the oral talking in the last year nips. This were pictures from the latter nips. I'm not sure. Can you see this? That are about 5,000 people there. So masculine give it hush about this. Your vertices, basically, you need to look at the screen. Okay. So, essentially about the distributed deep learning because our light levels up. So, I'm sure you're very familiar with the parameter source on first, right? So, you're in a better position and you're neural network and the new position of the data for each node. Your copays radio keep everyone replicates of the model and the after some training and you basically send this information to the parameter server to a synchronization in a sudden bag so on first. So, if we're looking at those things you will find out the bottleneck will be the communication between the Parameter Server and the nodes. So and the latter, say some years, this scenario will become more severe. So when we're looking at this, we are very familiar with compression and quantization acceleration will try to see if we can do something to minimize the communication between the node and also the parameters to our server and we find out of course. In case, we'll be- you don't really send that data, you're just under three levels: negative one, positive one and a zero. You can send two: negative one, positive one, but that won't converge. So, I'll show you later on why. So, we call that ternarized gradients, if I can send this three values to tabular you can either increase the value or reduced a value or keep the same. Right? Those are the things. And then, by doing so, you can dramatically reduce the communication because they don't have to send a floating point value or signs first. Now, the question becomes how you can represent your gradients basically between those no signs of primary source server. Without sacrificing the accuracy that much, right? So, before we do this, we actually have to do some math. But, I'm not going to all the details don't worry. I'm not going to do this. But, the basic idea is we found out that many years ago or actually 20 years ago. Someone proved that if we can constrain this evaluative within some range and then we can guarantee the convergence of such training process. Even we send out such a comprised information. But, there are no guarantee about accuracy. It's just about convergence but, without being enough. Okay. Then we'll talk about, how we're able to find out a good representation of gradients. Okay. Look at what we have done. We started with range and no gradients, the HB ocean. Let's think about the communication theory. The best way to transfer the assembling of the signal is not to transfer the stumbling. It is going to transfer the parameters describing the distribution of this signal. And this distribution of this gradients, follow some types of the HB ocean. Okay? So based on this, We just gave some parameter that describes this HB ocean. The front went to another end. It can restore these values from another end. It can send them like a mean or sigma or sauna first and also the one, negative one, and zero to adjust all those stuff. But before you do this, you'll have to do the clipping to constraint the value to some range to guarantee the convergence. That's something we learn from the math. Okay. We do have an original one, we do the clipping, we ternarize the values. Upon assuming, they are following some distribution that was sent on another end and we'd restore everything. And, let's see the results. It's a very common track we're using in communicating research. They're pretty good. TernGrad slot. For a top one accuracy and iteration. You know, falling the increase of the iteration. You'll see the baseline and it's a TernGrad line. That's accuracy, very close to our baseline. And if I look at the training loss versus iteration, it's also very good. So it matches faster than the baseline. For AlexNet, you know, if we have a number of worker on two, four, and eight. Though we miss it basically, you know, increments by mini-batch size and the way reduce number of the iterations and if we look at all the results for the floating point that's the original baseline for TernGrad. So, the accuracy is very similar. Okay. It's just some very minor degradation. If we don't do the collate leaping, there is a two percent accuracy. Which means that the converter and so you saw sort of becomes a problem. So that's why when you do the clipping here. Okay. So, just to let you know. For the latter, neural network, we have the similar sayings. But of course, the accuracy loss will be larger. That's a year's spectrum. But still, you know, in a good unique range. So, we even created a performance model and that model has been used by Manning following pay papers and a tool similar like if I have a different Bandwidth between the Parameter Server and the nodes for them both Ethernet and PCI Switch or Infiniband and NVlink. However, our methodology will affect the performance and you will find out the larger bandwidth so make our methodology less effective. You understand this because you don't really need the binaries so large. But still we can achieve about two to three times performance improvement. Actually running those into which HP Lab and collaborate with them when we run- we actually verified these things in their environment. Okay, in another end. Okay. We'll talk about the papers, the past paper, yes, the NIPS and wishes. But I'm more care about the impact that is in the real industry. So, here are the results. Our one level quantization method which, I just talked about is encoding the latest PDK, ISTK of IBM TrueNorth Chip. Okay. Our structural pruning technology is supported by the library of the Intel Nervana Neural Network processor. You can actually download this one in their libraries on Zoom. And it is also adopted by Intel's newest NLP because we're using this technology to not only see and basically convolutional and fully [inaudible] are in RSTM. So, they will support this, in their timing my Micro processors and adopted by the ISAT technology in China's [inaudible] actually it is the largest one. To a two time performance improvement without changing their hardware, the infrastructure in their data center. That we're doing nothing we're just rewrite of their framework for training. So that our TernGrad technology is supported by the Facebook Caffe2 and I can download that one. And the HP parameters server products has already encoded in their products. So it's a real product. You can either go and buy it and they are going to support it. Okay. To share our perspective, which I'm close to end. So, AI is going to be the mainstream. No wonder a lot more people jump into it. Show the great potential in the bulk Cloud and Edge, but the limitation and the infrastructure, it always be easier. Because we are always ready on the company computing power so on the first, okay. So no matter how much you have, we're going to roll them up. And the future, yeah. will be more friendly, more automatic and more cost efficient. That's something we're trying to do. And the accuracy isn't always a primary goal and there exist many trade-offs on matrix of an AI system. Not only it's about efficiency companies efficiency, accuracy but also the privacy, accuracy. Our privacy is saved in, and all the kind of things. Okay. So I think that's the paid off our center. Where today, so I'm the Director and I'm open for any questions you may have. Yes. Thank you. Yes.  So, part of the optimization. I wonder if you can provide any special optimization code in CPU or GPU and if you do, will you be still able to accumulate a similar data?  Yeah. That's a very good question. Simple H in one line will give you some speed up but not enough. Okay. So the better way is, if you really can control the lowering process basically means that you can manipulate the data mapping procedure down to the computing infrastructure, you can get a better result. We have another paper actually probably, I think 2017 or 16. I forgot. You know talk about this, where basically, we rewrite the library of Intel CPU or lowering process to achieve additional speed up on top of the copular. So, yes. If you can do that, yes. If you know more information about how data is stored in the memory, how that is loaded, then you will achieve more improvement.  Because people are more interested in mobile platform. Especially of high efficiency input. So, I wonder, for example like the model optimization works, which one can give better performance improvement on a mobile platform without Google part or approach?  You will try all the things actually, Jim. I mean, you know. Actually, I'm organizing the attribute the low-power pattern recognition challenge. Okay. So, we run our competition every year and it will have workshop in CAPRI every year. So basically, our participants they optimize everything. They even contact this adding, every single layer, optimize the library or even the protocol to fetch the data from the servers so on the first you know to speed up this. On the same platform, like TS2 in the last three years, the peak performance in our computation improved by 12 times for only for the under the same hardware platform. There is dramatical role you can import it but, you'll need very carefully customize every single piece of you. Yes.  So, I also have question about structural sparsity and select from the theoretical. Maybe you could go back to one of the slides is that- I did see something like speed up numbers like on my right- Yes, so right here. Keep going back. Right, yeah. On the speedup right here, were these theoretical numbers or server numbers?  No. It's a real number.  Real numbers?  Yeah. But, it's a little bit tricky about this. So, we don't really consider the data ratio lowering, basically only the computation part.  Okay.  Yeah. So, it's a little bit tricky to see this number. If you really consider all the head and tail of the computation. There'll be null and vast [inaudible]. Okay. That makes sense. Yes.  Yeah, I guess I was kind of going to answer my question because like the difference between like two x is-  No. That's why you see if I report like ten times, if I really work with like an [inaudible] and technology, two times is the baseline we can get. Theoretically, you can get like eight or 16 times but that's, you know-  And when like SF technology kind of like implemented this method? Were there like any lessons that you kind of learned from-  There are many lessons. I think that's true rather than an article about- Yeah. One student actually went there. Worked with them for three months to optimize their whole infrastructure. There are many things. Regional goal, that'd be eight times the speed up. Up to three months we can let you up to two times but because their main concern that they are in the real scenario. Yeah.  Are there any examples that like that stuck out to you, that you feel comfortable sharing or-  I think I will write a paper about this. So, yeah. To be very honest, so many things. But I think, I will write something about this. Yeah.  All right. Thanks. 