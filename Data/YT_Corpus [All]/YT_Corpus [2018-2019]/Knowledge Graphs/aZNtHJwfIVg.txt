 my name is Steve skeena I'm a at Stony Brook University and I'm gonna talk to you today about some work we're doing on turning features into graphs and again I'm an academic some people may be from with some of my books we have any algorithm designed for manual fans here a few people here ok good so again I am again I'm I'm interested I've been interested in classical graph algorithms for many many years over the last you know 10 15 years we've been interested in data science in various dimensions and the work I'm going to talk about today and which is I think it would be what's interesting to you guys is kind of a combination of the idea of graphs and how they apply in data science and in particular let you see if this works ok yes so what I believe is if I understand who you guys are you get you're a different crowd that I usually speak to but I am gonna say that a large fraction of the people in here have graphs that they work on ok as data in some way or another and I'm imagining a lot of you guys want to build models to use this graph to get more out of it you want to build some probably some kind of a machine learning model or predictive model to learn something about the vertices of your graph perhaps you want to do something like linear regression or SVM's or deep learning or anything you you you have a task in mind maybe you want to predict tooth nodes that are gonna be connected to each other maybe you want to classify oh here's a node is it gonna be interested in this product or not maybe you want to you know do some some you know functional prediction on it or a regression okay and typically what I believe you guys have is you have a graph you have some attributes of each node so some features you could build a model on but somehow there's this there's this graph still and you'd like to get the topology of that graph into your model and what I'm gonna be talking about is something we developed called deep walk which is an amazingly simple ok nice approach to taking your graph crunching on it for a while - so that we build features okay such that these features are gonna be useful for any kind of a machine learning model you're gonna be building and that that's kind of what I'm gonna be talking about here so as far as the the outline here okay I'm gonna our technique for deep walks depends on something called word embeddings which many of you I think have heard of I think I mentioned it earlier li ER in one of the early talks I'm gonna show how how how our ideas based on word embeddings and for to get at the heart of why word embeddings are interesting okay I'd like to see suppose you wanted to represent the concept of cat in a computer okay what is the most useful representation of cat okay maybe it's a picture you might say or maybe it's that definition over there or I guess being graph people you probably want some knowledge graph about graphs and stuff like that and you say yes a cat is a node in a knowledge graph for our purposes do a click of a pointer here know for our purposes what a cat is is going to be this vector and I want you guys to look at that what we do have it okay very good on top okay very very good okay so what I want to convince you of it here is that what a cat really is is this if my talk is successful you're gonna be able to look at that and say sure that's a cat it looks just like a dog okay what is the point now okay let's keep moving on so what is the idea the idea behind word embeddings and by word embeddings i mean word to vac which is I'm sure a buzz word people have heard of is that you'd like to be able to take a large amount of text that you know nothing about or no no annotations just being text and try to figure out what the meaning of words are what the roles of words are and the idea but that lurks behind word Tyvek is that if you know what words mean you should be able to tell the difference between a real phrase and a corrupted phrase okay so if we take a look at some texts a perfectly reasonable thing to have in your text is when I visited New York okay that might have been in a text corpora that you had supposedly not want to corrupt it I could take the middle word and randomly change it pick a random word from the vocabulary and replace it and unless you were a really terrible writer taking your text and replacing one of the words that random will make it worse okay so the great thing about this idea is you can take a large amount of just positive examples automatically construct negative examples and if you can build a system that is good at telling given the choice of a original phrase and a corrupted phrase if you could figure out how to tell which it tell them apart you must know something about what the words mean okay and this is basically what weren't effect does look sorry about that this is what word effect does okay there is a neural network that they use but exactly how that works or so is not really important what is important is that it that basically it treats each word as a vector okay of 64 dimensions or a hundred dimensions or 128 dimensions okay um the neural network is exposed to pairs of real and corrupted things okay and it adjusts its weights and adjust these representation vectors so it gets good at separating real phrases from corrupted phrases and once you do that you've got a sense of what these words mean okay and again I told you that you you know you were gonna be saying why is it dot what why does this look a cat look like a dog well if you look at build these things this is kind of like a visualization of work some word embeddings of popular words and if you look at them if sure enough if you look at the vector of values for a cat and dog they are look very very similar certainly you could tell an animal from a color does everybody see the pattern for colors looks different and numbers and countries okay and so you can view that we have done is we have framed and built models where there are in this case 64 dimensions we don't know what the dimensions mean explicitly but it's clear that they capture the similarities of what words mean okay and that is the underlying technology forward the back and that is going to drive our graph feature representation and again these word the wreck things are really kind of amazing when you start to play with them so if you take a look at some of these things and you say treat each word now is it is it the point in a hundred dimensions or so you can ask yourself what are the nearest words to it okay and these things are often really uncanny for a country the nearest words are there oops I'm sorry about this this is difficult for me country's nearest words are gonna be other countries in other countries in the same part of the world colors are gonna have their nearest neighbors as colors okay dentists are gonna have their nearest neighbors is orthodontists okay and it's smart enough to pick up on the difference being capitalized in lowercase lowercase apples or fruits uppercase apples or companies okay any questions about that oh and it may be I'm gonna say that maybe for the end but the key point here is that you can capture all the similarity learned in an unambiguous way from text and this is actually an example you guys like graphs this is we took all of the English words we developed what they are representations and points were connected words to their nearest neighbors and now you've got a network and now you can project it and cluster it and things like that and there's amazing clusterings that you see in here you know like these are less names clustered differently than first names and cities custard differently than states and and all this kind of stuff so this kind of unsupervised thing there's an amazingly good job of capturing what is interesting what is going on just given a a large body of text okay and again we've in our lab we work a lot on language processing we've done all kinds of things that we find interesting with uh with these kind of embeddings one of them we've built multilingual and recognition systems polyglot is a system we built that that can do basic NLP in a hundred different languages and the reason we can do this is we trained on Wikipedia editions from all of the different languages and from that we could leverage it and again this the similarities that the subtleties that it picks up I find amazingly interesting like apparently in Arabic the word for there's a word for two boys and when you look at what the nearest neighbors are it's two sons two children two daughters okay so there's really a lot that you can pick up in this unsupervised way about what words mean and it gives it a lot of power okay but you guys are graphs people you're not were what you call it are not NLP people particularly how can we adapt this idea of using word embeddings okay to graphs and we developed a technique called deep walk that does that okay so what would you guys people like to do you have a graph it's got you know all kinds of nodes and edges it's got at the topology associated with it and you know you can represent what your graph in different ways I mean obviously the people here want you to represent it with neo4j somewhere and perhaps conceptually could represent that as a a Jason C matrix of what node is connected to what other node you would like to be able to take this connectivity information and turn it into features for all kinds of different applications that you would care about so how can we do that we developed a method called deep walk that is going to learn a well is a latent representation and by latent representation I mean one of these higher dimensional representations where the dimensions don't really have names associated with them or features to just kind of think about representing the vertices of a graph in some higher dimension typically a hundred dimensions or so okay so that we can use these dimensions for features okay and what is great about we have a deep walk out of them which I'll tell you exactly how it works in a minute but what about it what's great about it is it tends to be very scalable there are other techniques that some of you may know from linear algebra like SVD singular value decomposition or something like that for taking matrices and turning them into features this turns out to be a very very you know a complicated and expensive operation on very very large graphs okay deepak has the property that we don't need the grow entire graph at once what you need to do in these kind of matrix methods we work online with it as chunks of the graph come in we can do something with it and and it has become very popular it develops very very good features and the connection with what I said before is that it it works on an analogy with natural language okay in particular if you think about what it is if you have a vocabulary in a language what are sentences sentences are sequences of words okay that's what a sentence is okay and what is a walk on a graph if the if the vertices of a graph can be thought of as being the words of a language then walks on a graph can be thought of as being sentences in the language okay and if you know about training set if if word Tyvek is this magic for taking sentences and turning sentences into features then why can't we use random walks and turn them into you know treat them as sentences for the same purpose and that's the basically the entire idea behind deep walk okay we are going to take a graph we're gonna generate random walks from each vertex in the graph okay use these as training data then our random walks are gonna be short some length so that they are analogous to sentences well maybe maybe of length 40 or 20 or 40 it's a parameter okay and being a random walk we are just going to take the next step from all of our neighbors uniformly is the first try maybe you might want to try to sweeten that in some way but for our bread-and-butter algorithm we're just gonna take a random walks visiting each neighbor you know basically uniformly and we're then going to feed this thing to word Tyvek which is going to now try to build vectors out of this thing okay so that I'm will come back there so that you know what you so that again word de Becque is going to try to predict what vertex given I corrupt the vertex on the I randomly substitute one of the words one of the vertices with a nut on the wok with another vertex can I tell the difference between that and the real wok okay if I get better and better at that I know what the words the the graphs what the nodes of the graph mean and that's kind of the secret to what we're doing here okay and again we you know being academics we we compare ourselves against other algorithms things spectral clustering things like that those are sort of like the SVD methods that that we've been talking about they're a bunch of algorithm methods for turning graphs into features here we were looking at trying to classify this is a multi-label classification problem where we were given a graph and we wanted to we had a small percentage of the nodes were actually labeled and we wanted to figure out the labels of the rest of them and even given a smaller fraction of the training labels we were able to do a pretty good job of reconstructing what the what you call what the the labels were and better than the other methods that's what we like and when you run it on very very large graphs some of the methods like spectral clustering which tend to do very well okay you can't run them on very large graphs but they require the whole thing in memory they they they're they are super linear algorithms things like that so so deep walk is able to do these kinds of things so basically deep walk has become amazingly popular everybody is doing the deep walk okay you can see who somebody else trying to do the deep walk you know again as an academic you know I'm a reasonably well sighted academic I have never had a paper get us anywhere near as popular as fast as this deep walk paper you know there-there's all you know by the citation statistics are extremely good it's it's quite popular if you measure there's that perhaps the biggest conference in data data mining or data science is something called kdd and out of the 4,000 ktd papers that have been published in over the last 20 years ours is the 28th most popular over the last year so it's something that epically a lot of people are reading and again you too can do the deep walk you may say how can I do the deep walk okay well we'll be talking a little bit later about how you get deep walking neo4j but let me just give you an example of why I think this is a great method for getting data from graphs I've worked a lot over the years in doing analysis of Wikipedia and I'm interested in social science kind of things we wrote a book on ranking the importance of historical figures okay based on analyzing Wikipedia and one kind of question that you would like to be able to ask are which people are similar to each other so you can imagine that the population of Wikipedia the depending upon when you do it is half a million a million people okay and certain people are similar based on some kind of roles some kind of when they were in history what were they doing and we were interested in trying to build systems that were a program that given you know given the context of Wikipedia could tell which people were similar to it you would like to say who's similar to Einstein where you'd like someone like Fineman and Max Planck and Newton okay and maybe you might think well why don't we do it by analyzing the text and given that my group works in text processing we spent a fair amount of time using our word embeddings and our text analysis to try to read the articles and figure it out but we ended up getting much better results just using graph embeddings okay how do you tell how whether you understand what wikipedia are well we did did some fests where we took triples of historical figures and measured something about what categories and you know if Wikipedia or have a Wikipedia article at the bottom of the Wikipedia page there's categories that people are put into and you might say to people are probably more similar if they share more Wikipedia categories in common and so that was one way we could evaluate whether or not our our method was was doing a good job and when we did it again we looked at a lot of different methods we tried looking at the text of the articles and we tried doing a topic analysis you know about natural language processing Lda and stuff like that and we worked pretty hard at trying to analyze the articles but we found that when we just took the network of who's connected to who in Wikipedia if my Wikipedia article cites your you know quotes your Wikipedia article then presumably there's a connection and we found that if we did just a deep walk of the what you call it the the the connectivity graph and then asked ourselves is the deep walk representation of these two nodes closer than the third one that would be the more similar one and this did much better than all the ones we worked pretty hard on and in some sense it said that if you wanted to analyze Wikipedia and figure out who's similar okay who are the people that you know capture that idea you didn't should throw out all the text and just read the links okay and that was kind of an amazing thing for us and again they were introducing anecdotal examples you know when you looked like who were the nearest neighbor of Beethoven you got Schubert and Brahms and Mozart who was the nearest neighbor in deep walk space of Mick Jagger you got Beatles you got Rolling Stones right who's a near neighbor Obama you tend to get presidents you got Democrats okay who was a nearest neighbor of Scarlett Johansson well you got reasonably young actors and actresses okay who was the nearest neighbor to me in Wikipedia well okay it was a you know okay yes sir and and the amazing thing is that two out of three of us are billionaires okay but the rest the rest are computer scientists also so so I think I count that as a way and and and if you also notice the thing that the number in the parentheses is the distance and as you can see I'm further from Larry Page for example then Beethoven is from Schubert okay which captures the idea that that may be thought it's not it quite as close a match okay fair enough okay and again we've been interested in these network embeddings and these notions of historical similarity we have an app a way of kind of browsing similarity orders once you can tell that two things are similar it opens up all kinds of different in some there scold sense even if you don't know what it actually is you can kind of use this kind of thing for browsing you find something you're interested in you want to look at other things that are similar and this is a way of turning your graph into some kind of a notion of similarity that that I found very very powerful okay so that's basically what I wanted to say but there's enough interest in in in deep walk and a lot of applications that there in implementing it at many o4j and Michael is going to take the rest of the talk and explain what that you guys have been doing thank you thanks a lot Steve that was insightful yeah I'm standing in for my colleague magneto who couldn't come to a New York and talk about how we used Deepak implementations to make them available near for Jane so principle in principle as Steve said Betty you've want to take the data on your graph and convert it into something that can can be used by machine learning methods right whatever the method is but you want to have a kind of feature vector and also we want to enable all of you to make this as part of his life over flow so you don't want to pull the whole graph into an another library duty analysis there and then pull the data back and like lots of additional steps but we want to make it as easy as possible and it's also something you don't want to do as a manual step you don't want to identify all the features that are irrelevant manually because that is bs error-prone as letting the IBM steward right and so what he did is he built and Neveu J user defied procedure that takes your graph data and runs runs the book onion graph data and Nanoka turns or stores embeddings or feature vectors on each of the nodes of your graph so you can get this as part of curly and experimental library but if as I said kind of finishes and yeah stability as as where we wanted to be and as we get enough feedback we will also move it at spot and into the graph algorithms library as well so that it's then available out of the box basically in near future so clearly yes in a models repository I have also releases where I can download library and then use that how how does it implement it we didn't do the whole new network that Steve showed on the slide but we basically used the implementation from deep learning for Java DL for J which has already a Deepak implementation there you can just plug in other graph sources for to run random walks on and so that's what we did we basically took the data on here for J made it available to the deep for complementation deep learning for J and then took the results out of that Deepak implementation and made it available again to the user or stored it in a 2d graph so it looks like this you call just procedure the two nulls on top is basically your labels or relationship types that you want to run on or you can even specify once I have created a generates a note list and there are second Sipho created generates an H list so you can also very custom projection so if you have data on your graph in a certain shape but you want to run the deep work on another shape of the data like a projected or an aggregated view on user to use or citation through cetacean view then you can project it to any kind of shape and that's what you pass in on top there then you just pass in the default parameters that you want to use and the property that you want to use for writing back to data to your notes and run it and then after a few seconds or minutes it completes and then all you know it's get feature vectors as properties stored that you then can later use for building and KN and network or whatever right you can take them and feed them to a deploying network or out of things so you can combine them with other properties like actual node properties or graph I Burum properties like sensualities or clusters or things like that and then have an even more interesting vector as well okay and I think that's it that's it you don't have to do more right so it's really just calling this procedure on your data and then it magically computes the feature vectors for you and then you can do all the fancy stuff that Steve showed you and that you can do and so in the in the def zone which is only on the fourth floor Amy and Jake are running the graph algorithms in ml booth and if you want to know more what to try it out just come come down and talk to us also if you have feedback if you try it out and run into questions or things that you want to talk about then we'd love to hear from you and get feedback on that so that we stabilize all this stuff and get it into a graph algorithms library as quickly as possible okay and yeah that's basically my thing what we also are working on besides deep Fork is yes and another approach it's called tip GL that we have working on with a company called Braintree in London which uses then multi-level and multi-generational feature reduction and things like that which is also really interesting and that we currently are also exploring and other things that we're also adding to the graph algorithms library is providing different similarity metrics so you can do Jaccard similarity Euclidean similarity cosine similarities on top of these feature vectors to then generate DK and network out of that and it would be actually question for me to Steve what kind of similarity metric did you use to compute the distance on the feature vectors C yeah so I come up here we also take a look at that no I think I like this one better the truth but notice so I mean the question was about um I guess similarity metrics I don't remember exactly was probably excused of Euclidean met I mean the you know again I personally find these these these features very surprisingly robust and so you know so it's it's not that sensitive to what wrote what what metric it gets some sensible thing of what is similar to something else we usually use a Euclidean metric sometimes we use cosine similarity that's also something we use a lot but but in general I don't think that there's there's it's particularly sensitive to that any other questions from the audience right okay so so what do you do when the question is let me try the question has to do with what do you do okay the that with a limitation let's say of word embeddings in English is what happens when a new word comes along like neo4j no one has maybe trained that in the corpus that people had used what does this mean okay and you know there are techniques that people have developed in natural language processing to try to get around these things that's basically what was being asked what would I do if you now said oh I need to I'm gonna have a graph where I guess the heart of the problem here is that there is a batch training that is going on okay and you know you'd given your graph you're gonna spend a certain amount of time chugging through it to build an embedding over time things are gonna cheat you want to keep that embedding around for a while you know you could first of all if you really have your act together you could take your new thing and start doing soon as you insert it in the graph you could start to do you know walks from it and you know maybe keep that online but that's not not a mainstream the mainstream thing to do you know typically what you would try to do is impute what the embedding would be for your new thing without you know in a couple of methods one is you could say well look if this note in my graph is connected to new node is connected to five other nodes and I have deep walk embeddings for four of them maybe taking the centroid of those four is a reasonable estimation for what this new node that I otherwise know nothing else on so that might be maybe one way to do it and you know another possibility is to do some kind of a regression thing where you might want to be able to given if all you have is property that you don't even have this thing in your graph okay you don't have any connections to anything else but maybe you've got some metadata about what this note is if it's a company you know how big is the company where is it located stuff like that maybe you could use a regression to try to take the features and predict the deep walk in that vector from the from the features and that could give you a you know some kind of a representation for it okay so if you've got dynamic graphs this is a problem you have to worry about if your program isn't that dynamic then of course you shouldn't have extra vertices that you're you suddenly have to worry about okay so um okay so um the couple of questions that may be in there if I'm not gonna get this question right then then so one question was about I think the complexity of finding nearest neighbors okay so that you know if you want to find who is the most similar person to Skeena in the wikipedia deep space in bet deep walking betting ostensibly you've got to go and now compare me to all the other people in Wikipedia my my imbedding which is now a 100 dimensional point a point 900 dimensional space if you compare it explicitly to every other element then that is now an order end computation where n is or of Wikipedia and that is large well the fact that this is a geometric thing means that there are some algorithms for high points for similar nearest neighbor search or near neighbor search in high dimensions it becomes a geometric problem and so there's certain geometric data structures you can use to try to do that so it is it is finding nearest you know exact nearest neighbors in a modest number of dimensions is not trivial but on the other hand there are fast data structures to get you fairly close so if the question was about how do you find nearest neighbors I would say realize you're now living in a geometric world and the algorithms that come from geometry are now at your service to do that ok and if that is that completely answers your question or good enough ok any other questions yep yes yep right okay let me say again okay the question was about the reason we care about these kind of features why do you care about this graph feature at all the idea is somehow that there is some subtle stuff lurking in the connectivity that that you may not have explicitly in your features okay so so one question is okay so the question is why do you mess with this this deep walk stuff at all okay you're saying if I want to label whether or not what I want to try to identify if you know this person in my network is a terrorist okay and I'm gonna say well I could look at my nearest neighbors might my direct neighbors ask if they are terrorists if they are terrorists are more likely to be okay the advantages of doing something like deep walk are a couple one is that that you are coming up with okay so first of all recognize that a lot of your your neighbors are not always going to be labeled so let us take the terrorists example I think I showed you some results where I was doing a label classification problem okay that would be kind of I guess along the lines that we were talking about and again when you've got a very small number of labels okay the you know the you you you have relatively little information to get doing if if there's a very say only 1% of the people in the network are terrorists or hopefully only about one out of a thousand or one in 10,000 okay it's very very hard to say your neighbors are probably not going to be one maybe the neighbors of neighbor by the time you get a small number of hops you to everybody in the universe and so there are limits to something like that one of the advantages of the the feature of this vector feature of representation is that it is just better for many machine learning techniques if you wanted to do a linear regression what does linear regression take as a feature it takes a number ok ideally a real number and ideally a real number that's kind of normally distributed okay and saying how many terrorists were there among your neighbors is not as kind of let's say robust to feature at capturing this kind of thing so there's so I guess what I would say is it produces features yes it's based on connectivity but it's not just based on nearest on your neighbors these are these random walks can get you fairly far along in the graph how often do random walks take you to a particular neighborhood that's kind of what's implicitly encoded in here that might not be exploded in a simpler more explicit feature yeah basically it's also you'll capture de pattern that identifies a node right and then you try to apply this pattern in other places of the graph which might be completely disconnected even could be even two different clusters right so that I have actually no connection but you have the same kind of shape around this node and if feature actually captures this topological shape and that's what you can and he use it's like if you look at frequent sub graph mining or graph summarization these are some techniques from graph theory that kind of have similar focus and identify and repeating patterns in a graph emotive finding is another example that just because you know that's a good question but think back to the similarity of between me who is similar to me and Wikipedia now I am not that big a fish in Wikipedia okay and so they're not that mean I'm a good person but but but but they're not that they're no explicit links between me and Sergey and Larry okay or the you know the people found at Google but somehow in fact if actually there's Stephanie links from my Wikipedia page to any other person's Wikipedia page okay this probably I haven't looked at it lately there's but there's not that many links there somehow you know if you say you know somehow it's capturing something stronger because they are these neighborhood effects for me you will get to other computer geeks and from computer geeks you're gonna get to the people who founded Google or other academic computer scientists and things like that and that's kind of what is implicitly encoded here and that's why I think it's a powerful powerful method okay so the what you go so the question has to do with what about when there are different these different types of graphs but I think what you're really saying in my speak is that there are different types of edges okay and these ideas are labeled and have different kinds of meaning what do you do to deep walk it okay how do you do to turn that into features and you know so they are a couple of if you're determined to use you know our our method we don't make use of edge labels so you would have to be judicious in what you are choosing to include as an edge my recommendation would first of all I'd say you use everything and see what happens then you might find that there are some depending upon your name some very common labels that or edge types that really may not mean very much okay there's probably a certain level of judgement of which are the most important relations in capturing it maybe you would build a bunch of different representations one for each edge type or you know group your ad you still want the graph to be connected so you need enough density for the graph to be connected but this is one where you know to use our method I would say you wouldn't you what you would have to make some kind of a judgement of what you're dealing with there there's there's been a lot of academic work on these methods that since our deep walk paper and many of them try you know different kinds of things where they say Oh what if what if it's directed what if it's uh you know you know what what if we have labels can we exploit the labels and stuff like that and so there are variants of this again the key issue is really gonna be when what kind of a walk do you want to take through the graph and it might be that some of these edge types are less preferable to walk through than others okay and so you might lower the probability or something like that yeah and in India for implementation you can actually project your graph also to any other shape again so if you have a multi type graph or multi-layer graph you can also project it to a single neighbor graph if you want to but I agree just changing the runner at the probability of the walk makes more sense and it's and actually I think actually a multi-layered graph has probably better outcomes and in deep walked and a single I because because it captures more topological information basically because it's richer in its semantics as well one of the things again when you use these vectors of saying I say it's a hundred dimensions or something like that you get this is a parameter you can fix but that gives you a way to capture similarity in Malta that gives you enough space to capture similarity in multiple ways and so that's why if you've got different edge types hopefully the the embeddings that you learn will capture these different senses of what that thing is if some good old way yeah it could be actually an interesting thing to run deep work on different of these graph projection so for each edge type and then actually compare the similarity of the graphs by comparing the similarity of the features across all nodes so it would be so pairwise comparisons and then see if you can actually say something about how similar are the graphs at at the hole with each other that could be an interesting thing and to say how important are these labels on the only edges as well actually something you should try out um I would say that that okay so has to do a little bit with Interpol part of it is a question of interpretability okay how do you interpret these things and on one level I would say we are bad for interpretability because you know in some sense we're giving you this point in latent space and I can't tell you what any of the dimensions mean on the other hand one thing that this the method is very good at is finding nearest you know is it's naturally lends itself four nearest neighbors so if you want to ask yourself oh my god you look that you rat you have an algorithm for detecting vampires and you come back and you look and said who's asking as a vampire well why do I think Skene is a vampire and you can look at what other points are near skeena and start to see well you know if you know it see if there was something there that you can learn so I do think that the fact that you can look at nearest neighbors and example I mean nearest points neighbors in this in this virtual space okay I learn a lot about what the thing is thinking just by looking at those examples so I don't get explanations of I made the decision because of this but I can say my god what's why does it think this that this guy looks like that and when you say well as similarities are he's hanging around with this you know to the New York neighbor of me is Al Capone and and and somebody like that you start to understand why you got a view yeah and for the foj itself what you can do the idea behind the explain ability was as for instance for deep work you could mark the paths that have been taken and went into a future vector for a node and then for the actual neural network that computes the like work to Veck frequencies the idea is there because it's a neural network and neural net focus in that focus well so you can actually materialize this as this as a graph and then you can actually analyze neural network with graph methodologies or you can run graph algorithms on top of it and try to identify which neurons of the neural network are is possible for which decisions basically so we can basically try to reverse engineer a decision flow after renewal in the network and that's the idea about this kind of explain ability of machine learning methods using graph technology all right so these two aspects there and that's something we are looking really forward to explore more and and dive into more as well cool any other questions otherwise thanks so much 50 for coming and [Applause] 