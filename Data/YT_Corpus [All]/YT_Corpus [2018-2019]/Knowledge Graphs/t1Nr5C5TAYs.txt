 hello in today's recording of the epic video serious you want to look at epic periodic iterate a very powerful procedure that allows you to match up to the operations on your graph I see mad no neeraj is a transactional database which means each operation is bounded by a by a transaction and these transactions make sure that your updates are independent of each other they are consistently durable and isolated so when one thread updates the database the other threads don't see these updates until this threads transaction is complete which also means that the data that's being updated has to be kept in memory until the commit happens when it actually sinks to disk to the right a head lock to the database so each of these operations especially if they get large can consume a considerable amount of memory so to accommodate for that we can use a properity iterate to take a large operation that updates like millions of nodes and chunk it into small and small operations which are then each of them committed individually and can even or on the run in parallel so let's have a look at how this looks like um we can see that a properly iterate is one of the many operations in the periodic package the other ones are more concerned with scheduled operation but you look at a later stage and a periodic iterate is here and basically what it does it takes a two statements to several statements and a bit of configuration so if you look at this it basically looks like this so our first statement provides the data to be operated on so these can be either literal scalar values like in this case this is just an range from 1 to 1 million basically returned as individual rows but it could can also be notes or relationships or properties or Maps or rows of a CSV file or rows of a relational database or other database so whatever you choose at or turned from the first statement this is kind of the input that's being worked on in the second one and the second statement is variously doing the work so it contains the update operations and then update the graph change the graph or move things and so on so in all case we create 1 million IDs here and for each of these IDs they are passed transparently into the second statement we actually take this thing as a parameter party automatically prefix the same query so it can access these things without dollar signs and then we can do something with them so for instance in this case we want to create one million people and we just do create person ID for ID and we say our batch size is $10 so each transaction at what has one and I wasn't elements and it wordlist true means this batch is actually executed as a single cipher transaction staff operation so it again uses unwind internally to do that which is more efficient than running 10,000 surface statements each part creating one twisted basically and palette rule they can run this even in parallel because these people are independent which is also true for instance property updates or deletion of nodes without relationships as soon as relationships are involved you have to be a bit careful basically you can run you in your updates in parallel and the each canals item that comes out of here or each batch that comes out of here it's in a diff different sub graph so you would for instance group something by clusters that you identified before with clustering mechanism for instance ok cool let's let's run this and see what's happening so we create one million nodes which feedin can also see so it takes 12 seconds so this varies a little bit unload so now the recording takes one CPU I think it used to be five seconds before so if you try it on your own machine it depends on the number of CPUs you have available on a large machine this goes down to two two seconds to some contact so if you check if you have like 10 million notes about database okay cool um what can we do with this so of course we would want to refactor our database reference if you want to find all our notes and what we actually are to do we want to add an edge property that's based on the ID so basically we just take this person and set the edge property to ID modulo 100 so it's kind of people between 0 and 99 years old and we do this for 1 million oh it's yeses slight thing to watch out for so the first statement here in this case would actually use a compiled runtime any foj but this materializes its results when you access them from the java api in memory as a list so what we do is we just tell size what we used in runtime this lot to run tab which doesn't do that it's cluttered so it is something I need to fix an epoch that to do it this automatically all handled if he starts this work and I'm Receptus ages and you can run into parallel again because this is again node-based ok so you see it constantly quickly and now all our people have an edge property Oh No and actually I wrote so you see yeah it got cut 100 a message is here so because I forgot to put in PS so and as Elementor we also see that you get some error information if it shields with something else that's interesting is that you're getting passed in a batch variable and a count variable which is helpful too if you want to do something with this batch of information which bet you could be run into okay let's - again so takes a little bit more time and you can also wanna cry over the database to see so it did take 16 seconds to update these 1 million properties and then we can see what our age property it's from 0 to 99 and 49.5 is to the average cool and of course you can use this also with other data so for instance if you had an CSV file you could change this statement for instance to load CSB with headers from ul so you can also pass a pair of ear parens something return o as Rho Rho and then you can batch this update from the CSV file so you can take each row and create data update data and so on the nice thing here is that it basically replaces parody commit and allows you to do operations even if you usually would get into this ego problem where it loads all the data first and then kind of consumes too much memory so you can it really it's kind of hard split into $10 in size elements here and will not exceed that so even if it goes ego india in the second statement so for instance if you merge on person twice or something like that then it will still it still run in in reasonable time so much so so if you do it something like this so I don't have this easy father I know but if you do something like that then we can actually launch one and then we can actually call this operation without running into memory issues and the same is true for low JDBC Lord JSON or XML and so on um yes one more thing that I'm wanted to mention which is when you delete data so let's delete all 1 million notes so that's one time so in our case it's only delete usually you would do detached elite so something to have to watch out for is that basically T number of updates that a transaction can needs to hold it's not just the leading denote but also deleting all the relationships so depending on your degree of the nodes you want to it use the batch size because potentially if you know ten sirdar's relationships it's not just one update but basically 1000 add one updates that it has to do because it has to do it all the doors to relationships and dolls one node so if you say batch says 100 then it's actually 100 doesn't as much to us in this case right and so in this case it's better to let use the batch size if you do detach delete justice' that's a hint so and if you run this now it should delete our million nodes basically from the database so this took 34 seconds which is also longer to to your recording ok so now our database should be empty again and with that I want to leave you if you want to learn more please visit the epic repository which also contains the help which also contains examples for this loading CSV and then doing periodic iterate with the data if you have question come to a slack channel and subscribe to our YouTube channel for more updates on this thanks for watching 