 [Music] I'm Chris Ramsdale lead product manager for the Google assistant SDK for devices and I'm Glenn shires technical lead for Google Assistant SDK so today we're going to talk to you about a couple of things at the core we're gonna talk to him about our software development kit that allows you to embed the Google assistant into hardware devices and how to use that SDK to extend the assistant to work with the features and functionality of your devices so just to kind of calibrate a little bit last year at Google i/o we launched the initial version of our Google assistant SDK for devices that does just what I said allows you to embed the assistant experiences into the devices you're building whether you're a prototype or a maker or commercial Illium this is the technology used to actually get those features and functionality into your hardware and to kind of frame the conversation a little bit so you can kind of get like because it's an ecosystem and there's a platform and there's things involved so let's talk up a little bit before we go deeper into how this relates to the other pieces of technology that we're building so at the core in the middle here is the assistant service this is the artificial intelligence and the machine learning the AI and the ml that's actually powering the virtual assistant that is inside your home hopefully if you have a Google home or Google mini or a JBL speaker whatever it might be that's at the core and that we've been building for years and years and years that's basically the evolution of search and other functionality that we've been building at Google then we talked about ways of extending it right so one way of extending is bringing third-party cloud services to the assistant so that's actions on Google and there's several sessions about that going on throughout I oh and that's how you bring those services to the assistants so you can do things like order a pizza book a ride you know and how you can extend the things that we're not necessarily doing so like what you might provide calendar and geo and things like that but other people are providing like those services again ordering a pizza ordering a ride then the other way of extending the assistant is bringing that assistant experience to hardware devices right and that's where we come in and that's we're gonna talk about today that's using the Google assistan SDK four devices okay so to give a kind of again in framing let's talk a little bit about what people are using this SDK for so we'll talk first more about commercial OEMs so recently uh at the Consumer Electronics Show in January we announced integration with the LG TV so we brought the assistant to the experience inside the TV literally you're talking to the the push button on the remote invokes the assistant on the TV and then helps you out with everything from like what's my day look like to finding media inside the TV the nice thing about this is Google as we a little bit deeper into what the SDK is the TV was actually using our cloud based API and then what LG did was wrote a very thin client on top of webOS webOS is their operating system that's running on those TVs and they were able to do an over-the-air OTA update to get onto the TVs to reach millions of subscribers and millions of end-users of their TVs so it was nice because it gave them the ability to take new technology the Google assistant but actually work with in market devices that they already had then we also announced back in February integration with an S camera the nest cam IQ what's nice about this is that two things one the nest cams we're finding is that households that have bought a camera actually end up buying multiple cameras and what that does is that starts to give you kind of this ubiquitous experience in your house where you're just walking through you ask for some help and your virtual assistant actually helps you out wherever you are when you when you need that help so it starts to push us into the direction of jike Oh going from my living room has like a Google home or a JBL speaker or something like that to I actually have the assistant kind of streaming throughout my house so those were commercial OEMs we've also had some fun experiments with our friends over at Deep local so deep local was a creative agency out of Pittsburgh that we've worked with several times over the past year and a half to build some really cool experiments so if you were out at Google i/o last year maybe you saw the mocktail mixer that was out at the front of i/o or maybe you saw it on YouTube then in our October Hardware event they built a pop-up donut shop which is really cool that what the you could walk up to it and engage with the assistant and give you doughnuts but then at random it would give you actually a google mini which kind of looks like a doughnut without a hole in the middle and then they brought that throughout many states in America at the Consumer Electronics Show in January they did a giant gumball machine which was a huge hit I think at one point in time there's about a two-hour long line of people coming up and actually interacting with this thing and then over in the developer sandbox the assistant developers networks we have a poster maker that we can go and interact with the assistant and generate a unique poster that you can then take home with you so not commercial devices but still a lot of fun and it's really fun to innovate with with that team and it's also a really fun to innovate with the larger maker community which has been awesome like this has been a very long tail of developers just taking our software development kit and using it in ways that we would have never thought of so I just highlighted a few here and I want to insert the the youtube link so actually if you want to check them out but we've got people building retro Google home devices that are kind of beautiful we got a candy dispenser we've had a ton of robots actually which Glenn's gonna talk a little bit about as well and then we have had one maker actually embed the assistant inside of a Mac OS to bring it to the actual laptop experience so a lot of fun there okay so that's a bit of an overview and kind of framing but I also want to just jump right into a demo to let you see what actually is happening and let Glen take over and give you the demo and then yeah how it works yeah so as Chris was saying you can build the Google assistant into all sorts of types of devices and there's also starter kits that you can get for example the imx7 is for Android things and we also have one called the aiy voice kit that's available at several retailers and you'll see the URL that you can see the retailers and get that at what that is is this cardboard box if you look closely you can see two microphones as well as of course a speaker so we've got the microphones up here on top and we've got the speaker here I've got a big battery here you can just plug it into the wall or whatever you'd like to do so inside this cardboard and the whole thing comes in the kit inside the cardboard is a rather small little computer called a Raspberry Pi and let me show you how it works how long is how long is the golden Saint bridge Golden Gate Bridge has a length of eight thousand nine hundred eighty one feet so it's doing better than I am in terms of speaking I presume how long is that in meters one foot equals zero point three zero five meter so you've got the Google assistant inside an embedded device turnout hot word accepting hot word so now I don't have to trigger it and I can simply say hey Google pick a random number from 1 to 100 okay seventy-three hey Google how tall is Mount Kilimanjaro Mount Kilimanjaro is 19 thousand 341 feet tall hey Google turn off hot word sorry I'm not sure so let me show you how that works so it's this cardboard box as I said is comes from a kit it's an a my do-it-yourself kit with everything you need and the the kit itself connects to the Google service via Wi-Fi there's two different types of software you can run on this SDK actually supports two ways that you can run it one is a way to run it on almost any platform any operating system any programming language so it's just you run all your code directly on here and we have sample code that does exactly that in this case I'm actually writing running some Python sample code on the box that implements the entire client in that Python code so you've got the entire sample the other way which is when you say hey Google turn off hot word and I'm pressing my luck I guess is the other way when it's running that way is using a library that runs directly on the assistant I'm sorry it runs directly on the client so we provide the entire client library which includes things like the Hat word detection okay Google as well as echo cancellation and another number of other nice things like timers and alarms and that runs on either Linux or Android things okay Kris why did you add some code yeah so let me show you a little bit about when you're running the library it's actually quite simple to use you can see there's simple functions you can call to start it to turn the microphone on mute you can also rather than starting it with a hot word you can start the interaction by programmatically and then that slide here shows you some of the events that come out on the following slide we see a lot of different events that your code can handle if you'd like to handle it or there's no need to yeah another thing I mentioned there are two microphones and you may notice that also on Google home there's only two microphones I could go home and yet could go home does a wonderful job in background noise with people speaking to it from a good distance away and the way that we do that is a technique we called neuro beamforming what it does is it's very similar to the way people have two ears and they're very good at picking out speech out of noise what we've done is we've used machine learning and run this on the on the server to get a very robust noise robust fire field experience and what that means is the client-side has minimal processing power so we could really keep the clients low cost yep back to you Chris cool so kind of building on what Glenn was talking about let's just go and a little bit deeper into what the SDK actually provides so at the highest level there's a cloud-based API that's built on a G RPC protocol so G RPC is actually uses HTTP to to go back and forth to give you streaming support which is important when you're actually doing audio because you want it to be fast low latency the benefit to that API is that it's available available from just about any platform so again just like the LG TV example that I gave they had webOS right and we've got a number of partners that are actually coming with their own platforms that are already out in the market and they want to know how to bring those platforms to the assistant and so we can create a very thin client that communicates with that cloud API and out of the box we provide what are called G RPC bindings those are those thin clients that are built on Python no Jaya C++ and they run on either Linux or Android things or any platform that you you actually have those that API is really good for push button push to talk support so when Glen was with the Box and he pushed the button that's actually invoking a thin client that's talking to our API if you want to have an experience more like that nest cam where it's hands off because those cameras are typically mounted above above you like in your ceilings and whatnot we call them far field or hands-free experiences and they use technologies like wakeboard or hot word is what you may have may have heard the whole ok Google bit to get that experience we have client libraries that are built for Linux and specifically Linux 3.18 and above that give you that hot word support and echo cancellation and Glen is mentioned and then because it's a software development kit obviously there's many Docs and samples and tools that allow you to embed these and debug the assistant and test it as well and then beyond that we have hardware kits so Glennon mentioned the the aiy kit over here so aiy is a twist on DIY so artificial intelligence done by you and then there's also the what is that the IMX 5 minute 7 kit yes yep so there's these software kits that allow you to start going we're gonna expand over time to actually bring more and more software on ships Solms to to the market for developers to get up and going and running with the so with all that kind of framing this again is our goal is really to bring that ubiquitous experience to to everybody - and we're not gonna build all the hardware out there nor are we gonna build all the experiences so we've done a fairly good job with speakers and whatnot but there are appliances there's auto there's things that are in your bathroom and so we are really trying like why Glen and I get up in the morning is actually to come and figure out like what are those user experiences we want to bring to market with partners right the prototypers or makers or commercial yems and then what technologies do we have to actually have to build to make that happen and when we think about it one way to to kind of categorize it is to think about your day and this is a little bit trivial but it kind of gives you an idea that we're trying to give the holistic experience from streamlining your morning when you want to wake up and have your coffee made or you know you want to scream NPR news to figure out what's going on or maybe you don't want to stream the news then when you've actually moved from your house to on-the-go oh I forgot to actually set the security camera I left the garage open or if you're coming home from the say the grocery store and you want to preheat the oven to 350 because you have lasagna that you need to put in there when you get there and then finally helping you relax in the evening so everything from hey you know I have kids and so no more screen time for kids to turn off the Wi-Fi in the kids room or turn off the Wi-Fi in the house or is you know dim the lights because we actually want to watch a movie or watch TV but you know just trying to figure out what those user experiences are that actually add value to you and then then figuring out the technology behind it so when it comes to actually integration paths for doing this so if you're actually building hardware there's two paths that we have coming in - the assistant and it's a little bit of marketing speak we call works with assistant and then assistant built-in works with assistant is if if anybody has like a Philips hue light or nest thermostat those are works with devices so they can be controlled by any other device that has assistant embedded inside of it that is if you want more information on that tomorrow at 11:30 on Stage five they're gonna be talking a bit more about works with can how you can integrate with works with Glenn and I are talking about built into the second part where you're actually taking the assistant and embedding and hardware so it's kind of a controller versus control II you're building a controller a device that can actually control other devices as well and interact with the assistant service for knowledge queries and things like that so let's talk a little bit about developer benefits of the assistant and the assistant SDK first of all minimum hardware requirements so as I had mentioned if you're doing like push-to-talk scenarios and you want to integrate with our cloud API there's very little that's that's needed on the on the client side the fact it's all up to you whatever you're running on your on your client you can keep running it it's the effect of making a simple like rest call to our service to integrate beyond that though if you actually want to integrate and have hot word detection and echo cancellation and things like that so you can have that hands-free experience then we still have minimum hardware requirements as planet mentioned we don't require a massive microwave to mics and you're good to go and we can actually use neural beamforming to figure out what you're saying and then do proper hot wear detection and grammar detection from a ram perspective it's only 206 Meg's around required on the device to get up and running and then one core arm d7 processor to get up and running so we're truly trying to shrink that down and then over time we'll start looking at things like our toss and microcontrollers as we move into the appliance space we have built in hot word support so you don't have to provide your own hot word model or anything like that you simply download our library put it on an embedded Linux based device and you're off to the races you've got okay Google and everything will pick up and a planet had mentioned and showed you in code will take care of the rest the library will take care of bringing in the audio transmitting it to us in real time and then streaming it back down the response and then as as global as a Google is a global company we know that we need to continue to flesh out our languages and locale story and we've done a great job since last year moving into fourteen different languages and locales so you can see up on the slide right here but we want to see over time to expand this map to actually get into other countries because we know that people that are building with again whether you're a prototype or or maker or commercial um do you need to meet your customers where they actually are where your end-users actually are and so we're gonna continue to put momentum behind this in terms of actually when you're a commercial OEM I wanted to like as we've learned over the past twelve months working with LG and working with nest how to go from prototype to commercialization so if you're in that space and you are trying to build a commercial device I wanted to give you kind of an insight into how it's working right now we're still early stages working with a few commercial OEMs our goal is actually to be more immersive and go deep with them to figure out what are those right experiences for their end users so that we can build the foundation on which we can start building more voice technology on top of and so the path here is I'm hand waving or a few details but you start prototyping using the assistant SDK to build an idea to build a concept you submit that to us for a review and we'll iterate on the device itself how it fits in the larger ecosystem what are the end-user experiences that you're trying to bring to market if it's all good we end up assigning an actual account manager and a Technical Account Manager to you to help you facilitate that and move forward and then beyond that you go into certification both in terms of like okay is the voice recognition actually working on the device does the marketing guide doesn't meet our marketing guidelines is the branding correct are we all in good shape and then step 5 launch have a party and be good to go so that's kind of the path that we're taking right now and over the course of the next few months that's what we're gonna we're gonna focus on and then we're looking towards really scaling that up in 2019 okay somebody goes through a few new features and we're gonna go back and forth a little bit on demos I think so since last year we've been hard at work and we've added a couple of things first of all we've added visualizations support to the SDK so now you can actually enable your device to say it's capable of handling it's a it's a display enabled device you can handle visualizations you get things back like knowledge queries sports and scores whether personal photos Glen yeah thank you so first off I want to show that we have a new developer tool what you're seeing on screen right there is is my Chromebook and with this rather than using an embedded device you can actually use your laptop with Chrome to test out the SDK and to get your application running and test out different parameters for example we support as Chris was mentioning several different languages and you can set the parameters and then just test it out for example I can say what's the weather in San Francisco currently in San Francisco at 62 degrees Fahrenheit and partly cloudy today it'll be cloudy with a forecasted high of 62 and a low of 53 so this is not just visual output but it's also input in that you can see some suggestions here I could say those if I wanted to or if I just want to click on it I can find out the weather for this weekend in San Francisco Friday it'll be mostly sunny with a high of 70 and a low of 59 degrees Fahrenheit Saturday and Sunday it will be cloudy with lows in the mid 50's highs will be in the low 70s Saturday then be in the mid-60s on Sunday there we go of course this can do things that Google home can do such as search I can say who is Larry Page according to Wikipedia Lawrence Edward page is an American computer scientist an internet entrepreneur who co-founded Google with Sergey Brin now because this is a developer tool I can also for example look at the requests I made this is actually a JSON request that shows the different parameters that I sent up including in addition to the audio and I can see the responses that I got back from the server you can see the transcript as it was forming as I was speaking it it's showing the transcript later on it's showing the HTML coming out and you see the audio is actually being streamed back as well but beyond search results I could also do personal results for example show me my photos this is what I found in your google photos sorry it wasn't on the right screen when I did that here's what I found in your google photos so there we go and of course I can scroll through see the different photos of a whitewater rafting trip we did recently and zoom in full-screen and so that shows what we can do with this developer tool as well as the the visual output so let me show you how that works what we've done here let me bring up the slide to show you how this is working what this is doing is is using the service API as we said we can run on any code any platform so this is running in JavaScript and it's using the Chrome browser as a client the service is generating the audio response in addition to html5 and of course the browser is displaying the html5 so in terms of also new features one of the things that's very important for us especially when thinking about third parties that are building hardware devices for the system built-in is to get parity so that they can build devices that actually work just as well if not better than some of the devices that our Google Google's building so one of the things we lacked for a long time in the SDK was the ability to notifications so to really like have the assistant service push out updates to devices so in this case I have a trivial example of hagel will ring the dinner bell which will ring a dinner bell to all of your devices this also works and helps us out with things like OTA updates so over-the-air updates when we're actually want to update a language package for example on a device it's very important that we have that push notification for them and so now we're happy to add this to the SDK we're also making an endeavor in forays into into music so we're starting with news and podcast support so now you can actually access those news feed so NPR news for example or your favorite podcasts radio lab I happen to be This American Life a fan and so now you can actually build third-party devices that have news and podcast support built into them and I think gent Glenn's going to show that to us right yeah so let me demo that for you again I'll be using the hey why cardboard box and I will simply say play the news and it brings down the file and live from NPR news in Washington I'm Windsor Johnston president Trump's not stop news and the other thing we wanted to show is notifications one thing that you can do with Google home and you can also do with other embedded devices is have one device talk to other devices so for example you could broadcast things or you could say something like ring the dinner bell okay broadcasting now so it's broadcasting from one device to the other device it's dinnertime and so if you want to call your kids for dinner all the devices in your house can say it's time to come down exactly kids exactly so let's show you how that works so again that that notifications doesn't necessarily have to be between two cardboard boxes or two embedded devices it's actually be between two google assistants that are logged into the same account so I could actually use my phone to ring the dinner bell on an embedded device and so that's how notifications work and that's how this works all right okay so to round out things here one of the features that I've been really excited about is what we're calling device actions so when we initially launched the Google assistant SDK the feedback from the community was like this is great this is awesome I can build a Google home clone now how do I make it do custom things and it was nice because like that part of the community just got it like they understood what we were doing and then they understood where we should take it as well and so this was our one of our answers to that request which is it like okay cool let's let you embed the assistant and then let's you let's let you extend it to control that device and so this breaks down into two ways of possibly doing this we call them built-in device actions and custom device actions and so just bear with me for a second and I'll go a little bit deeper into these so built-in actions are built on top of grammars things you can say to a device where Google curates those so a lot of the home automation right if you have an s device or a Philips U or a Wemo or whatever it may be turn on turn off turn down the temperature make it hotter these are all grammars again things you can say to a device that we curate and they're not static they're dynamic we actually can change them over time we can internationalize them on your behalf so if you're building a device and you can leverage our built-in actions and know that the grammars that we have there will continue to grow an anecdote that I like to tell folks is we were doing good with home automation then we rolled out to the UK we didn't see nearly the traction we saw in the US when it comes to lighting we didn't know why and it turns out that a lot of people in the UK would say pop on and pop off the lights not everybody but there was a there's a segment of the the population that would and pop on and pop off was not something that we had known about and so we were able to do our due diligence research and then change it and we change it on the back end and none of our lighting partners had to do anything it just magically started working for those UK customers should we have figure that out first hand maybe but you know that's debatable so that's some of the benefits of actually going with the built-in route now all that said then and while those devices and grammars and traits will evolve over time again we're not going to build every device and we're not going to understand everything that you want to do on that device and so for that we offer custom actions where you as the developer you as the device manufacturer can provide the grammars and the commands mapping to us and so what you say is like these are the ten things that people can say to this device model and these are the structured intents like actual commands that should come back down to the device and then you build the device drivers on there to actually you know do a dance do the Macarena whatever it may be on your device but it's kind of an escape hatch right now so that you can have the flexibility and customization that you need to when building out your hardware so again keeping with our theme I'm gonna keep it going on yeah thank you Chris so yeah I'd love to give a demo what we're going to demo here is actually a toy robot that's specifically built for prototyping and it operates by bluetooth and what let me turn on this robot and I'll ask my favorite favorite a iy device connect to robot connect to robot okay let me try this one okay let me let me show you how this works and then I'll give that a shot in just a second what this is doing is we can switch to the slide thank you this is using again the Google service but we are implementing a custom device action so I can say things like connect to robot and what happens is the Google service will understand my speech and send back a command to this AI Y box and the AI Y box will at that point send a Bluetooth command well it receives Jason that it can parse and if it wants to connect to robot it sends a Bluetooth command to connect to the robot and again we're using the same library that we've used in the past we've added a little bit of code on top of this to implement the custom actions connect to robot I can just keep talking I'll try it one more time okay connect to robot okay oh it gets better trust me the light did turn green set color red so I'll get it sent a Bluetooth command to do that robot get up so it's a self-balancing robot go forward turn left don't fall off the table turn right and there we go so you can see that we can control a robot or we can the point of custom actions is you can build an appliance or anything where you can actually set your own grammar and then parse the commands and have them do whatever you'd like them to do awesome hey John [Applause] so yeah so let me show you very quickly with what we're sending up in terms of custom device actions first of all you can define these using a lot of the tools that you may have used for regular actions regular assistant actions said she has the actions and Google tools it also dialogue slow what those will generate are in this case a JSON file that you can install into your device and when your device is talking directly to the assistant it's not like you have to say open my robot app and tell it to turn right you can simply say turn right so the first thing you would defined is the intent the grammar what you would say to make something happen in this case for example when I said set color red here's the intent that would allow me to say set color red set color to red robot set color red so there's a variety of different ways that you can say say things and then the next slide you'll see what the response is the fulfillment first of all I can define the text-to-speech so in this case it's saying setting robot LED to red and then the execution where I actually can parse these parameters and I can see that I'm setting a color to red so that is the way that we define custom device actions we simply parse that and then translate that into the commands to control the robot cool thanks all right so the the slide of I'm gonna tell you what I just told you the recap but seriously we're striving again for that ubiquitous assistant experience in your life to help you out robot and we know that it's gonna be fueled by it to be successful to be fueled by a healthy ecosystem of developers so that's all of you in this room and everybody that's watching on youtube right now and our goal Glen and I in a team that's helping us out is to to provide that software development kit those two robots technologies to help you build and embed the assistant in the hardware devices that you're building so with that I think if this works you might have one more doubt trick up your sleeve yes we do do a dance all right well done all right thank you very much thanks [Applause] [Music] you 