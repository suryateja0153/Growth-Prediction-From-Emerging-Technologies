 [Music] so I'm gonna talk about a few things today first I'll talk about what eBay Shop out is I'll explain why we don't integrate with conversational platforms in the typical way I'll explain our approach and then I'll show how we use Google cloud to make this architecture possible I'm gonna focus mostly on kind of overall you know the breadth of what we built I'm not going to go super deep on machine learning or AI I'll cover some of that but I'd also recommend checking out Tod morals talk from yesterday it was a pretty good coverage of kind of the basics of a bot and the theory behind that so just give you a sense of the scale that we operate at these are eBay's quarterly our second quarter numbers we have a hundred and seventy-five million active eBay bars worldwide we had 1.1 billion live listings on our website we did twenty three point six billion dollars in transaction volume and sixty percent of our revenue was international this is powered by thousands of services making billions of requests per day my group at eBay develops entirely new products using the public cloud we do AI research and development which includes natural language processing computer vision machine translation and search we built a buying experience for the Chinese market and what I'm talking about today is conversational commerce with eBay shop bot on Google assistant and facebook Messenger so we use Google cloud for almost everything don't worry about trying to digest this slide we essentially started on compute engine we played with cloud functions but then we decided we wanted to kind of formalize our deployment in execution and we moved to kubernetes and when you do that it brings along a lot of other things we use PPC for our networking cloud load balancer and firewall rules VPN to connect into our corporate infrastructure DNS we use the big data products and then we also use a management and security products and we're a big consumer of the utility compute products including cloud storage BigTable and pub sub so I figured it'd be easiest to show you a video of what eBay Shop on is so you can see kind of the breadth of what we're what we built with this product [Music] find me a canon camera with a zoom lens [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Music] [Music] [Applause] [Music] all right so the vision for eBay shop bought was an intelligent and trusted personal shopping assistant that understands your intent and shops for and with you this video showed a multi turn conversational AI that was simple fast and conversational our AI demonstrates natural language processing computer vision and speech recognition it curates and highlights the best deals for you and shows the unique finds we have a size profile that allows us to remember your preferences and as you continue to shop with us will remember what you entered and personalize your shopping experience for you and then of course the end result is an item card where we take you to an eBay page where you can purchase the item that you shop for a major aspect of what we've done here is ubiquity instead of expecting users to come to ebay calm or our mobile app we go to the channels where they already are if they're in Google assistant or if they're in facebook Messenger we want them to be able to shop from that interface we adjust conversations based on the capabilities of the channel and device the user is currently in so if you're in your kitchen and you have a Google home you should be able to shop using your voice from your kitchen we allow users to interact seamlessly across devices resuming where they left off so going back to the example of using your Google home at some point you might want to actually see the results that we provided for you so we can transfer the conversation from your Google home to Google assistant on your phone via visualize so while ubiquity was our goal we decided to start with two markets first one is Google assistant which has over 500 million devices today and facebook Messenger which has over 1.3 billion monthly users on Google assistant you create an action to personalize an interface as a developer they have over 1 million actions today on facebook Messenger there are over 300 thousand BOTS so now I'll jump into the technical details I'll explain what a conversational platform normally looks like and how you would interact with it and then I talk about why we didn't do it in the typical way so for any conversation platform you integrate what this is kind of the diagram that you would you would want to build towards on the Left you have and the users input device like their phone it could be Google assistant where they might input speech it could be it could be a facebook messenger where you upload an image and then that's received by the conversational platform that's converted into a request that's sent to the custom web book that you create as a developer you would of course want to validate that that request came from the platform then you had formulate a response and then send back the response synchronously or asynchronously to the platform in the case of facebook Messenger they have a model that's completely asynchronous with Google assistant you have five seconds to respond to the incoming request and then finally when the conversational platform receives the response from your web book its routes the output to the user so this is Google suggested variant of this approach for Google assistant similar to I had in the last diagram you have the input device on the left on the right you have your web book which they use cloud functions for and all their samples actions on Google is the name of their platform that you would develop against and what's different here is in the middle they have dialogue flow which is a natural language processing tool that they in this example the user might say ask eBay what my iPhone is worth this will be sent from actions on Google to dialogue flow it will do the natural language processing and determine that the intent of this request was a what it's worth request and the parameters included a product parameter with the value of iPhone X so we actually did start with dialogue flow for Google home we found it to be pretty good for rapid prototyping and it helps you get going in just a few minutes with a working integration we built what I just described in the last page of what it's worth flow in about five days and we actually were able to demo it at the keynote at this conference last year but as we went further on that integration we felt like we kind of outgrew dialogue flow at some point the first issue was it was challenging to scale out to the moment for our team their tool is a kind of a point-and-click website and it didn't at the time have environment support or revision control all the kind of tools that you as developer or nobody used to having and so we had kind of a medium-sized team and beyond a few developers it became kind of challenging to work with this there's a limited set of conversational platform integrations so going back to our original goal being ubiquitous what you see there on the right is the the logos from their website of all the platforms of support and there are actually a lot more conversational platforms in the world than that our original integration was based on a few key categories where we imported our catalog into dialogue flow so they can do the natural language processing based on our our items in reality our entire catalog has one point 1 billion listings it was just way too much to actually import into their systems and then we had a requirement for a dynamic multi turn dialogue dialogue flow is kind of built for more static kind of closed retrieval base systems and then finally we are a big company we actually do have in-house expertise and so we didn't feel like we needed kind of like a more basic tool we wanted to be able to do something more advanced with our AI and NLU so not to pick on dialogue flow but finding the right off-the-shelf platform in this space is an ongoing struggle but framework starting a constant state of flux so similar dialog flow there's a product called with the AI that was bought by Facebook and now if you go to their website it's almost all geared towards facebook Messenger it's not really considered to be a general purpose tool anymore even though you probably could use it that way vendor lock-in and upselling are common so if you were to look at Skype a few years ago they had a bot API and you integrated with with it within a little bit of time you had to upgrade to the Microsoft bot framework that went through a few revisions now it's called the azure bot service and that's being used to also upsell people on Microsoft cognitive services so you might do a simple free integration and within a couple years you might actually have to pay real money to keep it a lot so this is kind of driven us over time to build abstractions over these conversational platforms abstract away the platform specific details into just a kind of a closed area so that we're not impacted by these by this flux from these third parties and then the other part of the original diagram I showed that Google recommended was a use of cloud functions so when we originally look at this we ran into a few issues that made a comment on option for us one were language limitations our Web book was written in Scala and it required no js' there was an issue with not having wall requested access for facebook messenger entire security story is based on getting the wrong requests computing the hash on it to a header that they send you without that support they were parsing the JSON stripping out whitespace and you could never get back to the original request and actually validate that it was secure there was no SLA so we saw issues with like uneven response times when we we were trying to use cloud functions and then there was no virtual private cloud support which was an issue for us because we're using kubernetes with VPC and in order to access our back-end services we need to be able to get through the DPC or otherwise we'd have to expose our services publicly so this was my slide I had up until last night when I read the news that claw punches now GA and they perished addressed all these issues so we might actually need to reevaluate it one they had a Python support for languages they added container support so I I don't that means but I think it means that you can write an or more can have a normal docker image and run that anything tagcloud functions they have raw request access now they have an SLA they have EPC support and they even allow you to run cloud functions inside your gke environment so definitely worth revaluing so let me dig into our approach this is our architecture at a high level and I'll briefly go over each the area as we build it into them later on the left we have you know a user's input device they might interact with actions on google or facebook messenger the request is sent to our web hook through a Google global load balancer will immediately try and identify the user by looking up in our own internal identity service from there we'll send the request to our AI subsystem which will process the request to generate a response along the way we'll use reliable messaging that we built to make sure that the request is handled and the response is delivered we record the request in response in our chat history and we also do tracking through cloud pub/sub but the user uploaded any media like an image will write that media to cloud storage and we also have a service for doing notification so that we can send requests or send messages to the user proactively so this is our AI subsystem which is the brains behind a base shopbot we have a custom NLU for understanding language we have a knowledge graph which helps us understand the relationships between the things the user is asking about we have a dialog manager which takes the output of those two services and combines it with the content that we provide to formulate the response for the user and then this dialog manager will use an AI memory to remember whether you just said between conversational turns so going into our natural language understanding the area a little more it transforms natural language text into a formal machine readable representation so it handles parsing it'll take a string break it up into words that identify nouns and verbs and add other structure to the original input text it doesn't tent detection to determine what was the key thing the user is trying to do and then it extracts any entities that can find in the text so basically parameters to the original intent so the example I have here is my husband needs some new black leather dress shoes but I want to spend less than 20 less than $80 what do you have so our Enel you will go through this and it does a bunch of things that I won't go into completely but the key thing is it identify the intense so needs and what you have tells it that this is a shopping intent the identify is also entities including gender item condition color product and price range and then what you see there on the top is it did spelling correction so husband was spelled incorrectly so it corrected that before try to do the entity extraction this is something we've had for a while dialogue flow I think just announced that they added support for this yesterday and then on the right we some like basic sentiment analysis that we do so that emoji tells us tells us something about the users state of mind and we can possibly tailor the response based on that this runs in kubernetes we have a variety of machine learn more in GCS and when the machine starts up it loads them into persistent SSD backed volumes we have a knowledge graph that is a probabilistic inference graph of eBay user behavioral data it allows us to infer a shopping funnel so if the user said shoes we can narrow down the aspects to just things that are relevant to shoes it runs with Apache airflow to pull data out of eBay wiki data our wiki data Wikipedia and that loads it that's handled by cloud data proc which is Google's spark implementation and loads this into a neo4j graph database at this point we have over 500 million nodes and 20 billion edges so it requires a large amount of CPU and memory to run this but we also use persistent as its defect volumes as well and we use stateful sets so that as we deploy it doesn't have to reload all that data on to the machine so this is an example of what a what our knowledge graph looks like just so you can understand more about it on the Left let's say the user said shoes we know usually what their gender is and especially Facebook messenger where they have a profile where you can load information some basic information about the user so we can skip right to the next level question which in the case of a male this graph is saying that there is a higher chance that he cares about color as his next question than brand so we can ask him which color do you prefer red or blue if it's a female we would go over to the brand question and ask deeper fur coach or although our AI memory maintains conversation maintains memory across conversational turns it is our dialogue state and also keeps track of expected future responses if we are expecting more input from the user so going back to the flux that you see sometimes when you're dealing with with conversational platforms this is one area where you might want to build your own AI memory Google system provides a dialogue state object that you can use on their system something like Microsoft bot framework had this they took it away so if you're gonna integrate with something like Cortana through Microsoft Bob framework you'd have to build this sort of system anyway so you might as well use it everywhere it uses a big table and it's keyed on a conversation ID which is the ID that the conversational platform provides to us through the for each turn of the conversation and then what we store is the serialized version of our dialogue managers in memory context so as we go through each turn of the conversation we will load this context out of our a I memory will update it and then we'll write it back out the big table our download manager manages the dialogue flows it's maintained the content behind it is maintained by editors and designers and this is the voice of eBay shopbot it leverages and updates our AI memory and it looks at the NLU output ai memory and the knowledge graph to generate for the responses so this can lead to another question or they can produce results so I'm going to jump into our web book and AIC subsystem interaction so our conversational web book is built in no js' Express typescript currently lives in kubernetes as I said but we could possibly port this to cloud functions in the future we I built it as a single service with one router essentially per conversational platform that we integrate with we could split that into separate micro services later on if we wanted to and the philosophy is for this to basically just be a translation mechanism between the third-party conversational platform and our own internal request response format that we call fabric API so this is what it looks like it's our internal model of a conversational event and we call it fabric because we want to picture our experience being on an ecosystem of channels we want the bot to weave through we originally built this as a rest JSON based interaction but we wanted to have a concrete interface to build against we also wanted to have a client generation for the various languages that we integrate with so we defined this interface using protocol buffers and I'll go into the each of the areas there which is basically a subset of the things that we built for this for the fabric interface event is a top-level type and it's and caps lights a lot of things you would see across conversational platforms so sender recipient conversation for maintaining context across conversational turns we encode capabilities that might come from the request and then we break down the event type into either a message a post back which is essentially a button click and then we could differentiate between new users and returning users who are starting a new conversation with us our message type allows us to handle both speech and text it allows the user to upload media including images audio video and files it has a scheduled attribute that allows us to basically delay messages so we can kind of provide pauses in the user interaction we also have a typing indicator in there as well that makes it look like the bot is thinking we have a card type that's kind of generic card that we can use to fill and with any any items but usually we use it for an eBay product so we put in there the title a price a description and image and buttons that you used interact with it going back to the message we have a carousel which is a horizontally scrolling set of cards and we have a list template which is a vertical version of the same thing so one of the key things here is modeling channel and device capabilities so we want to be able to adapt the responses to the channel and device so we have static config based on the channel so for example what you see here on the right is Google assistant showing a vertical list on Google assistant this can handle up to 30 items on facebook Messenger there vertical list can handle 4 items so when we're integrating with this we want to be able to factor in the different capabilities of the different channels we also have dynamic config based on the incoming request and this is especially important on Google assistant where you can have a Google home that's purely voiced based where you have a cool assistant on your phone that has a screen interaction so we need to be able to factor in the two types of you know voice and screen bass interactions Nvidia shield is one product that has cool assistant running on it but they didn't add a web browser yet so we can't have the user link to ebay.com for example so we need to factor in all these different capabilities when we format our response and when we do that though we want to avoid having channel specific logic because we're trying to create they bought that has kind of a consistent voice across these different channels we want to be able to use that content across those channels without having logic that says if messenger or if actions on Google we want to be able to say like basically in terms of capabilities this is what you can show that would optimize experience for this device the user has so just to kind of drive that home this is an example of what that looks like on actions on Google and patient messenger these are the basic request payloads that you would get in both cases the user said hello world and we could translate that into a single fabric request and it looks almost identical between the two we encapsulate the or we include the user ID which our internal user ID we record the channel ID and we take the conversation ID that the conversational platform gave us and add that here as well we have capabilities and then finally in our message list we include the original hello world text message the response is pretty much the same thing in this case Rai Rai responded with you said hello world and we have channel specific code in our web book that can translate the fabric response into either actions on Google or Facebook Messenger and what you see there is what that looks like in each of those platforms so at this point supporting additional channels has become more of a business decision than an engineering one we have a common back-end shared by all channels and we just have to create a new Express router in our app and set the end point for that in our conversational platform the biggest challenge is then mapping the external request response format into our fabric API and accounting for any unique capabilities that that channel might have we've already validated this approach so beyond Google assistant and messenger we've done this integration with Cortana and Skype SMS slack and we've got something basic working that we know we could easily scale out if we chose to take our business in that direction so I'll go into some of the supporting systems now these are not major systems or any major components of what we built but I think is interesting to look at them because they were built I have Emily leveraging GCP and it's the sort of thing it would take a lot more work to build off tcp like on a traditional platform so we built our chat history and BigTable and we want to archive incoming and outgoing messages so some use cases include internal dashboards to visualize conversations analytics retraining our AI models and handover to a human operator we basically just rying to record a JSON sterilized version of the message we include some metadata we have a version attribute that allows us to know how to parse that message we include the channel and then we include ascent from user value basically know it was a request or response that we're recording here the key thing here if you spend any time at BigTable is on that key design and this kind of aligns with big tables best practice for how how to build the key but we want to be able to retrieve based on a user ID so that's the first component of our key then we also want to be able to make sure that the messages are in order according to the time of the user acted with with our BOTS so we use a timestamp we wanted to be able to get those most recent first so we reverse the timestamp and then we use a random number in case there are multiple concurrent writers we don't want them to overwrite each other and then if we're recording multiple messages at once we have a batch ID that's sequential for that batch so it all kind of comes together and gives us one kind of easy interact with lightweight chat history system we actually built a reliable busting system and big table so the thing here is we want to make sure that every input and the user gets a response if something goes wrong in our back-end and we drop a request it looks really bad essentially looks like the bot is just not functioning very well it's low quality and it also aligns with Facebook messengers interaction with Web books which is they'll send your requests and they expect you to return a 200 response and if you don't they'll retry to get in 20 seconds you return too many failures they'll disable your bought altogether so no matter what happens we want to be able return a 200 so the system we built takes a request from messenger and we immediately run into BigTable each user has their own queue which is represented by a row and BigTable and the message is written to a column with a sequential monotonically increasing ID we'll try and set the message if we can right away but if something goes wrong a worker will pick up a little scan the table it will pick up the row that still has all standing messages it will atomically try at least the row and then it'll send the messages one by one after all the columns are deleted it will release the lease and then all the messages should be sent at that point so this ensures ordered at least once delivery we do have some edge cases where could possibly result in duplicates but we figured that was better than having no responses at all and this is something we built over a year ago we haven't had any problems that really you have big table issues that are kind of within their SLA so we have blips here and there it hasn't really impacted users and every time we've had one of these blips is recovered on its own so conversational platforms will deliver media in different ways on the right you see an image being uploaded from facebook Messenger and when that's sent to our service it's in the form of a short-lived URL and other platforms might deliver these this media in different ways so we want to extract those details from our back-end services so at the edge in our web book we may record any uploaded media into GCS and then the GCS key is what we pass around to our back-end and once again is something just works for us it was very fast for us to implement this and we haven't had to think about it since we launched we built our notification system with cloud datastore so this is what proactively sends messages to users we handled package tracking events deals editors picks and marketing campaigns use descriptions are stored in datastore so the user will subscribe or unsubscribe to a notification through their conversational interface it will be sent to our notification service which records that in the notifications dB we have quartz cron jobs that run periodically to check to see if any notifications need to be sent it will receive events from eBay to know when package tracking updates happened and we also call in to our deals API to look up any new deals for the users when this notification system runs is invoked by courts we get a transaction to essentially lock the processing of the notifications to a single worker and this is one area we actually might want to revisit our decision to use datastore we used it at the time because it was new and we wanted to see how performed and we've run into great limiting issues with a time and again so probably what we do instead is use a traditional relational database or cloud SQL or something else like that we built our analytic system on DCP and what you see here is actually a pretty standard our service will track requests and responses in cloud pub/sub we have cloud dataflow that does the stream process of any events and it persists the results into cloud storage what you see on the right is a data studio dashboard it's not ours is just a sample just to give an idea what it looks like and when the user loads the dashboard it goes through bigquery to load the external data from cloud storage summarise it and then present it to the user so tying this all together i'm going to show a video of google assistant in action and I didn't go into localization or multi region support but just a heads up this is done from the from the viewpoint of an Australian user to find me a Kate Spade bag okay got it looking for a color that black brown or something else what great - you want a style that shoulder bag messenger in crossbody or something else shoulder bone I looked through seven options on eBay and the best tool on this item is $95 I have more info on this still for you is it okay if I send that to your home yes [Music] okay so just to summarize what you saw there was a few things that I talked about in action we showed our AI so when the user says asking me for a Kate Spade bag our Lu will try and parse that input it will go to our knowledge graph and try and find additional attributes about Kate Spade bags so including color and style it uses our dialogue manager to process the output of the MOU in the knowledge graph and then takes the content that it has internally and formats responses it uses the AI memory so that remembers across these three turns what the user inputted and then finally it returns results we use the fabric API so that we can translate from the conversational platform Google assistance requests a response format into the format that our AI understands internally and then we factor N channel device capabilities so the initial interaction was through Google home and it's a completely voice based interface and we had to tailor the response with the expectation that the user didn't have a screen to look at so at the very end when we had results to present to them we asked them do they want to transfer this conversation over to their phone on Google assistant and that's what happened there we loaded the final results out of our ai memory and when the user came in from their phone we presented them with an ID card showing them the results so in summary eBay shopbot is an intelligent and trusted personal shopping assistant available today for Google assistant and facebook Messenger we ran into issues that cause us to not use dialogue flow or the traditional way you would integrate with conversational platform we built our own AI stack and we have a request response format we translate external formats into so we can handle them in a platform neutral way we built this all on Google cloud and to help make this entire architecture possible all right thank you [Music] 