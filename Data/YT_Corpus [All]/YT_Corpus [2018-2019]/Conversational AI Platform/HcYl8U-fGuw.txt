 hello and good morning everyone thank you for coming on this final day of a conference at 8:30 in the morning it's uh it's quite the trek to get here this time my name is Chris Mullins I am on the bot framework team I am one of the engineers on the team I've been on this team for about two years now I write code every day for those of you who look at some of the new things we have coming out like the C sharp before SDK a lot of that code is mine a lot of the tools that we demoed last year and again this year part of those are I've written I've been building BOTS going back to the days of IRC XMPP all of the messenger apps that came in between and most recently conversational AI so I'm co-presenting today with my friendly co-worker Darin Jeffords Darin so I could do this yourself Mike good morning everyone so my name is Darin Jeff had recently joined the team but actually working really closely with the team over the past couple of years actually helping some of our customers actually build the first wave of these conversational assistants and providing some great feedback and scenarios and actually some of the things we'll show today but kind of a scalar listening to some of that feedback and actually can I'm making things easier thank you so for those of you who here last year we showed a bunch of numbers those numbers have gone up in every dimension so we're seeing more companies more developers more bots more messages across the board bots the hockey stick graph robot growth has it continues so before I get too much further how many people here have actually built a bot either with our stuff for when the Varden excellent cool how many people here have built bots in c-sharp how about node how about Python Java suite so for those of you who don't know we're gonna be talking a lot about all four of those languages today and a lot of the services and tooling that comes with that so we have customers with the bot framework all over the world we're in Australia Asia Europe Africa North and South America we have the compliance bits that you need to be able to use this to build your boss all around the world we have the language support San Luis we have local data centers the odds are really good your company as it sits today is able to build using our stuff and check all the boxes that you need for your enterprises so today we're gonna start pretty basic we're even start off with what's a bot because it turns out many people are still confused as to what they are they're going to go through kind of how to build a bot and what some of the tooling is like around that Darren's gonna spend a lot of time talking about how to make BOTS more intelligent and then we're going to talk about some of the things coming in the future where Jason Williams is going to come up and present conversation learner and some of the more future looking AI texts that we're playing with Darren so kind of one of the great places to start when talking to customers and kind of developers around they're breaking down here the whole kind of bot to architecture it's really focusing on the end user so actually I want a text-based input is the kind of typical input that we all kind of default into but it can also be speech and we're now starting to see the first wave of speech enabled experiences as well as being able to you know sort upload images actually as part of a conversation to maybe could help with pre filling a form but also it's really key to make sure that these conversational assistants show up in the channels where your users are actually spending time so actually writing that bot once and maybe making it show up in Skype or Facebook Messenger or kind of web chat but you also have then how kind of the bot intelligence kind of box so this is very where we bring in these kind of AI capabilities which will kind of touch a touch on gonna throughout the talk where we're bringing in language understanding and translation and Q&A maker so these are kind of standalone components we're kind of bolting in to it could increase the intelligence of your of your bot now a key thing to bear in mind is when you're writing a bot using our cloud platform today under the covers you're effectively clear creating a web service so actually integrating with api's and kind of data sources are just as they would be with any web app or can a web service you're building today and how there's your platform under the covers providing all the good security in color logging things kind of throw out some terms of breaking breaking things down we have the bot builder SDK which helps you actually as a developer author these bots help you kind of manage dialogue into this conversational kind of logic and we spoke about these AI services we have the cognitive services platform so you have seen a number of these kind of demos in the keynote and so on so key ones being language understanding taking a particular phrase and understanding the intent of that but also things like text analytics for sentiment and Q&A maker which will kind of cover we've also got this new concept of accorded the dispatcher which will kind of talk on a bit later and this is kind of helping us actually better take a question and actually figure out which bot or which language model should actually process that utterance so we one of the scenarios we've seen with more complex BOTS is actually trying to figure out where to send that kind of question to so The Dispatch is a great way of trying to find the right place to go and process that it's worth bearing in mind that whilst the various canvases that we kind of spoke about a key question we've had in the booth a lot this week is being great I can do this in web chat and put it my mobile app but I want to go and embed this in my own experience I have a a WPF app or I have a sort of a mobile app whatever that is can I actually embed indirectly so yes you can use something called direct line which allows you to kind of send and receive messages directly they're separate to the various canvases that we support so we kind of break things down as I said the bot builder SDK it's kind of open sourced on kind of github you can go up and kind of get started quickly there's a bunch of samples in there and actually Chris will take you through building a bot from scratch by taking one of the templates and getting up to speed pretty quickly we then have the is your bot service which is again helping you plug in these various kind of intelligence services and as I said before these channels are the really key aspect another part that's kind of quite overlooked is you can write your bot once and you're insulated from all the differences of the various channels so if you want to expose on every single channel or one or two or three it's a configuration setting and then we as India's your bot service actually will then manage how do we had you talk to facebook Messenger how do you get messages and convert them from their schema into another schema so we kind of protect you from that complexity so in terms of intelligence we've spoken this kind of a few about this a few times from a lowest point of view the language understanding service you don't need to be a data scientist actually for you know any of these capabilities so as a developer in the room you can take the language understanding service you can go ahead and create a number of intense so an intent would be things your bot can do like booking a flight or finding a restaurant and you type in a few examples to actually help train the model with different ways humans may express that question you can also bring in some of your data so if you've got two brands or restaurant names or city names or whatever those things are you can actually add those entities in to help improve detection so one of it one of the examples always like to use I could create an intent around booking a restaurant and actually bring in one of the date/time entities we have and I could then say hey book me a restaurant a week on Wednesday at 4:00 p.m. now as a developer passing that string and figuring out the intent and also figuring out a date time from that fairly complex constrain it's quite hard Lewis would just do that for you it would tell you the intent and actually give you a date time or some candidate date times if it's a bit fuzzy and you'll see more about Lewis as your search is being kind of the core of some of our best bot experiences where you've got document and unstructured content being able to get a reason over there and try and find some information alongside Q&A maker but actually one of the retail bot scenario that will give if you have a catalog for example and you want to bind your conversation to this catalog so you can find products that are of a certain brand or a certain category and less than a thousand dollars or whatever we can actually buying that conversation directly - as your search and it's greatly kind of simplified some of the build of those experiences now it's absolutely key that a bot has can do things so tasks or intents but also have knowledge so better answer more general questions like what are your store opening times what's the telephone number what's a return policy you know how do I contact you whatever those kind of that general knowledge a good bot experience can do things but can also answer more general information so QA maker as you can see as you'll see later on is a great tool for being able to harvest some of that information out of documents for example we can also get into speech so the ability to wire up speech into these experiences as well so actually if you use the web chat control there's kind of native support for this you give it up a search key it's a speech key and then you can go ahead and color talk to it a key feature that again is overlooked to something called speech priming so that's where you can take the words you know the words about your products for example from the language model and bring across into the speech engine so that dramatically improves recognition because we've all probably had speech experience where you you mention a product name and it doesn't quite get it right and comes up with something in the dictionary the speech priming is a really easy way and we've had some really good results of doing that you're understanding who is speaking is another one translator will show so actually being able to allow anyone to speak any language without you having to do any extra work and we'll show how that works and custom speech you said you saw some elements of that kind of in the keynote so that's the ability to train a custom model to remove background noise if it's in an airport or a kiosk and I gain that dramatically improves kind of recognition so another key aspect to a really good experience it's actually integration into api's and that either you have or the microsoft has across our various properties so actually the most common kind of bot scenario with many customers today is actually around internal bots so employee self-service finding information HR policies whatever those things are but actually wiring in the Microsoft graph is where it really gets interesting if you can start to actually help people find people with certain skills for example so you can actually broaden each of your of your BOTS in this case by bringing in the Microsoft graph so again you can bring in office 365 and dynamics in LinkedIn and bringing that information to end up with a much better experience now this has been slightly complicated to do until now so any of you that in the room actually hands up who has had to add authentication into their BOTS experience ok a good number of you which is good so was it easy probably not so I wrote to some of the first at first engagements we did we had to write libraries to go and do this there's been various kind of packages out there to try and simplify it with magic numbers and so on actually one of the features the team is actually announcing this week is natively if you go into the is your portal with your bot you've actually got the kind of Roth configuration setting if you want to off to get hub or so is your active directory it's a drop-down list you put in your keys and you add in you know a couple of lines of code and all of a sudden now when you try and actually access something that's going to require calling to an API you'll get a a challenge so you'll see this video which is probably already played so let me just go back so where the experience works what you're gonna see is someone oops you try that again what's its not let me pause it for some reason here we go so actually you can see on the right hand side that actually the users asked the question yeah sure might get hub repos so actually Louis in this case he's going to understand that the user wants to access some information in github so it's actually actively need to authenticate now to get that token to actually then go and pass to that API so with any luck you're then going to see automatically with the new authentication support it's actually issuing a card saying actually you need to sign in so the user is going to go ahead and get hit that button and sign in and do the various authentication codes and all those good things and what you're next see when that signs in all you would see if I just skip ahead in the video you can actually see they said thanks Jeff it actually it now knows who that used a reason he's actually gone ahead couleur called the API passing the token for authorization and actually pulled that information back so with a couple of lines of code a bit of configuration you can now go ahead and add authentication to your box so anyone doing it any bot development should find this a big kind of step forward so it's great to see ivt so one of the things that we've been doubling down on this cycle is really improving the overall life cycle of a bot the finding that working with that in your life cycle in your SDLC and this party that we've created tooling we've got command-line tools we've got integrations with the azure command-line tooling and ibiza to get all of that tooling working now there's a session later today put on by my friends Elena and Fishbach that's gonna delve a lot deeper into this they're gonna show a lot of these tools very with a lot of depth behind it today for right now this is just a quick overview but what we're going to talk about here is as you start planning things when you when you walk up and start on the bot you've got to design your dialogues this is the life cycle that we've seen with the web once upon a time with mobile apps you can't just walk up to a bot and expect it to work because people don't have the mental model yet for how the conversational UI's work so we've got this transcript infrastructure now that works in the emulator you can design cards you can different design dialogues and that and you can visualize that you can load that transcript in the emulator you can show it in presentation mode to give the people you're working with a real feel for how your bot looks we have lots of tools for building BOTS of course we have the SDK the old the C sharp b3 SDK and the node would be three SDK those aren't going anywhere those have been out now for a year those are still maintained those are still getting new features added the OAuth bits that you just saw were added last week we have all kinds of tools for adding intelligence we have Louis we have Q&A makers so the building bits we've had for quite some time now and those just continue to improve and iterate testing we have a new emulator and Vishwas going to show this in a session today I believe it's 2:30 it might be at 3:30 he's going to go through and show we have a plug-in model for the emulator now so you can show visualizers in there you can actually improve your Louis models from within the emulator as you're looking at flows back and forth we're gonna show so many integrations with app insights we're gonna go ahead and show some of the DevOps tools to get building and publishing your bot to work so that a lot of your SDLC now can be more more what you're used to having you can script a lot of this you can store for example lewis files and with that resource code control system you like and pull it out and start setting up things like continuous deployment we of course have all the information to connect your bot - everything from skype to teams to cortana and lastly with the AI aspects of this we have a lot of the active learning and evaluation phases where you come into Louis and you look and see what are people typing how does that work with the intents i defined where Darren's going to be showing the dispatcher in just a moment where were you look at if you have all these different conversational models what overlap is there that you may not have anticipated so to build the bot at the most simple layer the first thing you do is you choose a platform and today it's c-sharp and node if you're on the v3 stack as part of this kind of preview today we're showing the world what we call the SDK v4 this is a preview this is we're trying to do this very transparently this is on github it's been on github now for about six months we have C sharp no Java and Python of course and we're trying to align them very closely with each other so there's there's one set of documentation one set of patterns but first step one is to pick what platform you want once you have a platform picked you go ahead and get started we have yeoman templates we have physics templates we have all the things that you would expect to kind of jumpstart that and make that go very quickly we also have scenarios that you can go download code for those just sitting out on my website and lastly you deploy your bot using the a Shabbat service you can deploy this locally to your machine if you want you can do this to your own private clouds and as part of all of that we have this command line tooling we have what we call MS bot which I often call mrs. bot because it looks like mrs. pacman to me we have the infrastructure there to take your bot connect it to as you're connected to Luis to edit your model to do all the things that you would expect to be able to do from a command line we also have this thing called Lu down which obviously is a play on markdown where you would define what your lowest models would look like so in something that looks a lot like markdown you can edit this in vs code you can edit this a notepad you can check this into github you can do whatever you like with the file and then we have a tool that will take that file and turn that into a Lois model for you so it becomes very maintainable from a command line tool and file format perspective Lois of course now we have a nice command-line tool to import these things export these things and train and publish them and the course we're integrated with is your much more deeply now so you can do a Zbot create or AC bot publish or AZ bought you edit all the different features and settings there to really integrate with the SDLC that your company is using does anybody been waiting for these tools I mean I I'm surprised that they haven't been there as long as they have because it's made my life difficult so having these makes everything I do a lot easier it integrates with all the best practices integrates with all the azure infrastructure everything just kind of starts to come together now and I say there'll be much deeper demos of those later today Fishbach will stand up here for probably about an hour and walk through every one of those tools in great detail if you'd like to learn more as well feel free to come by the booth and we can demo them there so once you built your bot you've designed a chat experience and you've started connecting into things one of the things that we see is you need unique cards a lot of people are expecting cards some bots obviously are great with natural language some bots are spoken but the combination of touch talk and type really requires the ability to adapt to the canvas that you're on right so we've got these things they're called adaptive cards we've had them for about a year and now although they've come a long way in the last few months this is an open source project this is on github these are open standards the renderers for these are also open you can render these natively on iOS on android windows and the dozen other platforms and what this does is defines a JSON document that says this is what my card looks like and then it's up to a renderer to say what does this card look like on teams or in Cortana or an Android or in Windows Timeline and it lets you the bot developer not worry as much about the pixel four of ik bits but when your bot is viewed for example through Cortana it should look like the native Cortana experience when it's viewed through slack it should look like a slag experience when it's viewed through SMS will down render that to a bitmap for your users so getting those cards out there helps a lot they also support SS ml for those of you who do any work with speech so you can start adding intonation and all of the things that come with SS ml so the last bit here you hand it back to Darren who's going to talk a lot more about Lewis and language understanding great thank you and so London Stanwyck Oh touched on this already and I keep heart there's cut again overlooked from from a Louis point of view is absolutely it goes hand in hand with kind of bot creation but it is a separate cognitive service so actually why wouldn't you or why couldn't you actually take a desktop app that you're creating or a website and actually have your a text box at the top where instead of a user going through and clicking through tabs and forms and things they could actually just express what they would like to do and actually have the app then go and actually go and perform that so one of my favourite examples at the moment is around travel booking so that in this kind of example where recently internally we had a new kind of travel tool rolled out kind of internally and the user interface is exactly as it would have looked like ten or 15 years ago it's got a where are you going from where are you going to a date picker control and all the other kind of complexity so actually imagine a scenario where you could write a bot for travel booking or you can have a box on your website where the user could just say hey I need to I need to book a flight from London to Seattle I'm going to stay in Bellevue and I'm going to leave a week on Tuesday and return on Friday and actually in one kind of fairly simple kind of utterance you've actually provided all the input data for that application to go and do that work so again Lois can be kind of plugged in kind of wherever I kind of make sense again you don't need to be a data scientist you can just pick it up and build a language model you can bring in your data and kind of train that and you get a web service endpoint that you can pass the question to actually get a response back it's also got that kind of that ongoing kind of supervisor learning kind of aspect where if I'm asking questions and maybe the language model doesn't quite understand in the early days of your testing so it can actually capture those questions and as an administrator you can go into the portal and actually look at some of the questions that that may be the language model hasn't understood in the correct way and actually retrain the model publish it version it all these other good things that you would expect so in terms of color there's a bunch of new features but kind of touching on the enterprise kind of aspect so it's now you know GA switch reach they can are generally available kind of milestone December last year it had s delays that has kind of multiple pricing tiers is one of the the big questions in build can the last year of actual when's this gonna go GA and when can I get your higher we're out of reduced throttling and things another key aspect is around compliance so actually there's been a number of changes around the cognitive services around gdpr and so on which is actually really changed the conversation with many of our kind of customers who are maybe slightly concerned that questions that are being sent to the Louis endpoint and Microsoft was using that to retrain our our and so on so actually there's been a number of terms and conditions that have changed across the cognitive services which means that is no longer the case we are a data kind of processor not like not a controller so actually that's unblocked a number of customers who are nervous around using the cloud and calling these various cognitive services that's moved a lot of things kind of further forward twelve languages are supported thirteen different as your region so you should find it in the region that you that your solution needs to operate in so talk a number of kind of new features you know from a speech point of view before you had to go in and kind of manually do that speech priming that's where you're improving the speech recognition by extracting the utterances and entities from your language model so that's now kind of automatic and there's been some unification around the keys there another one is sentiment I think every box I think that we've created we've wired in a number of cognitive services in addition to Louis so we used to wire in the spell checker so every question that came in we call the spell checker to make sure that we are you're correcting any spelling mistakes so that's now integrated into Louis and that was done last year another one is sentiment so we would call tech the text analytics cognitive service to actually understand the sentiment of the question is it negative is it positive maybe the bottle will respond slightly differently so that's now just a checkbox and you'll see that in the demo today and the same thing around keyphrase extraction again I'll show that that's all just integrated so we've got an we've got a number of languages that we support from these twelve languages but actually we've now got the machine translation aspect where with just a couple of lines of code you can actually target any language and I'll show that kind of interactively again these were things you could do before but they required you to write the code and get extra keys and do all of that kind of jumping through hoops and so on from a core language understanding there's been a number a number of improvements that kind of a now available patterns is one where you can actually teach the language model some patterns of utterances to improve recognition it's the example that you will see a lot speaking about is if you're dam trying to understand information about your organization and they want to say you know who does Chris Mullins report into or who does who does Daren work for for example and actually you can train all these kind of various utterances but you can sometimes get confusion so you can actually provide a pattern and say actually if I'm saying I want to fly from London to Seattle you can provide some examples around the origin and the destination which improves recognition so there's some great documentation explaining some of those examples so it can reduce confusion also bringing in regular expressions natively so again until having to train it with all of the entities as part of your training you can actually put a red X in and get that to extract kind of product numbers for example from utterances and and do that more reliably and key phrases I'll show in a minute so again this has been quite key for as your search scenarios where if I say I want a new Microsoft laptop now if that's going to go off to a search tool then I want a is going to appear in a lot of things those individual words will appear all over the place and you'll end up with slightly odd search results so keyphrase extraction is a technique to take that question and pull out covert key phrases which improves search so translation you can see a great example here of a bot that's been written in English the language model is in English the data is in English and actually by bringing in translation as you can I can't see on the top right and Chinese and kind of French in this case we're doing translation of all the cards and all that information coming back it's also a little bit cleverer than that you know I have I have had the pain of writing manually translation support and actually you try and be too clever and automatically switch languages and it that doesn't quite work out but there are also some things you don't want translated you know don't try and translate my name or a product a name from one language to another so again you can provide some sort of markup on utterances and that would tell the translation implementation not to translate this portion of a of a phrase so let's actually bring this to life so number seven we'll see if the demos are gonna work cool so a couple of things the kind of show straightaway so hopefully you're generally familiar with lewis so i have a model here that is a kind of a generic kind of white label demo we have around a retail kind of bot experience so I can say I want a Microsoft laptop for under thousand pounds for example this is uses Microsoft Store data so we should see if the demo is gonna work is it found one laptop that's actually under a thousand pounds and it's brought up will brought back an adaptive card and so on so this is the Lewis model that's kind of under the covers here and if we go ahead and look at entities one of the entities that I added ahead of the demo is the new key phrase entity here so again you're going to add that pre-built entity in and it will automatically identify the key phrases in any utterance and kind of show that to you the other thing when you publish your language model so you can all go and do this today so you can see here there's a checkbox for sentiment analysis so if you want Lois to take every utterance and give you an idea of sentiment is it positive negative give you a score it's now going to do that automatically for you you've also got the speech priming support there right there again this has been there for a while you can also bring in kind of a spell checker kind of aspect in here so I go ahead and just show that working quickly so if I go ahead and say I want Microsoft laptop before and thousand pounds this will just show you the the raw response from Lois in this case so you can go ahead and see this is the query here it thinks I'm finding and looking to find a product and this is the score which is pretty high which is good if we scroll down you're gonna see we've got some entities here list entities that I've got obviously I typed in want incorrectly but anyway as you can see here the key phrase is actually picked out is Microsoft laptop so it's removed the I want a out and giving you kind of the key phrases you've got to go off do a search it dramatically improves accuracy it was when a kind of a key improvement in one of our projects that we did and down here you've actually got the sentiment analysis here it's giving you a label is it positive sentiment and actually giving you an idea of a school score even and if I say this is terrible and you can see negative sentiment is actually kind of being returned here so again not rocket science again you could go and do this before and plug in components and cool keys and do all that kind of manual management it's now just a checkbox as part of difinitely lewis kind of processing which is quite cool so one of the other aspects I wanted to talk about is one of the learnings from a number of these BOTS that we've built is as they get more complex is you can sometimes end up with slightly strange results that you give it to a user and they ask a question and it goes off to the Q&A maker source or it goes down and an intent that it shouldn't have gone down and I spend quite a bit of time with other customers and they look in their language one ago why is that if I look at the training data for these intents it looks quite reasonable that it should go down here now why is it why is it getting confused and doing something else but you can also get situations where people maybe you've got a business person adding questions into Q&A maker and they're actually causing overlap so they're putting questions over here they're in the language model and cording and a slight kind of confusion so in order to Andrus if that leads to a situation today where end users don't get the outcome that they want so actually one of the dispatcher tools that at them that I'll show now actually allows you to evaluate your your your Louis model and make some best practice guidance actually to say actually this intent doesn't have enough utterances or you've got the same type of question here in this intent and actually over here in this intent so it can help guide you and how you can make some pretty straightforward changes to your your language model but it will also bring in Q&A maker so I'll actually bring in the questions from there and the language model and do a set of cut-across validation and testing and actually identify where there's overlap and ambiguity and there you can go ahead and improve your model it's actually one of the things when I found out about this kind of a couple of months back I went back across all the projects we delivered harvested all the kind of Lewes models and actually ran it through the tool and overall they were pretty good which was a good thing but there are a number of really straightforward changes that noticeably improved kind of accuracy which actually unless you have uses testing at scale you'll find it really hard to actually kind of test so the demo that I'm gonna show it's a kind of a bunch of command-line tools so now in this case I'm going to initialize use the dispatch command command line tool so I'm going to create a new dispatcher instance and could provide my lowest key my authoring key in this case so that will do that so the next thing that I'm going to do is add my Lewes model so I've said the typist Lois here's the idea of my model and the version to use and it's gone ahead and added that if I then also go to QA so I'm going to add a Q&A instance I'm adding the knowledge base ID and the key so go ahead and do that and I'll go ahead and create the dispatch model so this will gain create a model from all the various utterances and questions they'll go through and do that work and it does take a little while so here is one I created earlier so you can actually see a summary here so this is just a summary that's created and there's some top-level guidance around you know there's duplicate utterances between the language model and the QA so actually there's some duplicates that you should go and remove because that's gonna cause confusion especially as the language models evolve over time as you do more and more training than the scores are gonna adjust over time and suddenly you could get something going down one in ten verses to the other in ten so it's kind of giving you a heads up around that it's also saying there's a number of intense where the utterance is the training data isn't isn't high enough and yes some some other bits and pieces but I can go ahead and actually look at duplicate so in this whoops it's in this case we've got a uh pterence called hai which is actually trained in the language model but it's also trained in the QA model so actually as training evolves over time it could start to go down QA or actually go down the language model and have different results so it's highlighted straightaway an easy change to make also ambiguous so there's a question I would like to install new software how can I do this so the expected intent is the language model however there is a Q&A in them score which is pretty high so again you're likely to get you know an ambiguous response there's also misclassified so this is actually highlighting questions or utterances that are going to go down a route that was not designed so I've trained this question to go into the Luas model but because of confusion with the Q&A maker instance it's actually now going to go down Q&A maker versus actually the Luas model so actually just making it really easy to understand that the expected intent was the language model but actually it's going to go down the nun intent and in this case it's a it's expected to go to Q&A maker but it's actually going to go to like a fine product intent or something so this is a great tool that I encourage any of you that using Luis today and kind of Q&A maker it's a couple of command-line kind of entries you can get a bit of a health report of your language model and again we've had good results straight away and I'll talk more about the dispatcher later on so I think it was everything from a Louis point of view yes translation good one so the other aspect and Chris will show the new bot framework v4 and middleware components but I've got a simple bot here if I go ahead and start this this is just an echo bot sample that I just pulled down from the the visual studio templates so I can go ahead and just say hi to this bot with any luck and it's just going to echo back and say you sent hi so I can say good morning we will say you sent good morning so and this is the new emulator by the way so what you're seeing here so actually I go ahead into visual studio so in order to add translation into my kind of bot experience before again you could do it you have to write a fair bit of code to go and do it but I'm going to add a new translation Nuka package and actually i'm gonna either using here for the new translation support and there is one line of code here that i'm gonna add so this line the codes adding a new component into the middleware so any messages coming in i got to go through this kind of component pipeline and we've got a translation middleware component we're going to say the language is my bot supports which is english in this case here's my translation key and the last parameter is whether the translator should translate messages going back out so translating messages into the bot should also translate responses so I'm going to go ahead and say false for now I'm going to go ahead and build that and let's run that again so I should now be able to say is good morning again and it should just work as we would expect but what I should be able to say wait for it to spin up so you think good morning so I'm gonna go say good morning in German that's pretty much the only language other language that I know so I can say good and Morgan and you can actually see the translation middleware is translated that into good morning kind of automatically to two lines of code we've added translation so if I go back and change to translate responses and recompile what I should know better say is good Morgan and it should should now translate the response thank you so it's actually done the translation into German so actually this is very simple implementation we were just seamlessly switching languages the documentation is got the steps to allow you to maybe train a set of questions like I'd like to speak in German and it will just seamlessly switch across and it's also got the support to stop you translating people's names and product names as well so it's a really nice implementation and kind of shows off kind of the real power over before kind of framework so gives you a sense of how you can broaden the reach of your bot and your data without you having to do any translation work and we're sort of the new neural network kind of improvements with a translator you know translation across a number of languages has got significantly better liking it in the last so six to nine months so it's definitely worth checking out thank you Darren so one of the things that we are showing today is what we call the bot builder v4 SDK so for the last year we've had v3 that is a ga product it is released it's open-source there's hundreds of thousands of developers using this every day we get support we get we have a complete ecosystem around that however in the last year we have also seen a lot of feedback around that that was this needs to be more modularized the c-sharp and node needs to be more aligned we really need a python version we really need the java version so we went back to the drawing board and we created what we're now calling the bach builder v4 sdk and that is in preview today it's been available on github if you knew where the look for the last couple months or so and that is really designed from the ground up for extensibility Darin just showed an example of that he dropped the middleware plug-in that walked through the pipeline and did translation for you both in and out that's one example of how this new system is extraordinarily extensible it's also modular we looked at what makes really successful frameworks and as part of that analysis we really decided packaging and the ability they just pulling the dependencies I need and really clean lines of dependencies between them was a key component of this so we've gone ahead and built this completely modular so we have strong alignment between packages across all the platforms we have strong alignment of concepts we have the same middleware pipelines across everything and lastly we have Java and Python now now we're leaving the design work in c-sharp and JavaScript at the same time and what's called we're fast following that in Python and Java so the bits released today that you can go play with our richer in c-sharp and JavaScript now and in the next few weeks you'll see Python and and Java come up to speed much faster now that we know we've solidified a lot of designs there's not as much churn going on there that is a genuinely open project you can file issues you can submit pull requests we're trying to do this as transparently as possible and we would really love feedback there's areas of that SDK that we think are right and there's areas like for example maybe some state management that isn't quite as right and we would love community feedback on the best ways to do this so please take a look the stuff it's a github bot builder net or bot builder - Jas bot builder - Python bot builder - Java and you can go hit those repos now and take a look like I said I would love feedback on there we're triaging that every day and community support is the best way we will end up with an amazing product here so one of the things to drive on that extensibility for a bit is middleware you know we looked at asp.net core with its middleware solutions we've looked at Express we've looked at rest afire we looked at Redux and seeing the ecosystems and the integration that middleware it gives you and we took the same approach here there was no need to invent there was no need to do something new so we took the same approach to middleware for all these protocol handlers so now adding a new feature be it logging exception handling analytics translation is as simple as adding in the new middleware component into the pipeline and this is an example of the JavaScript one having pretty much the same thing Darin just showed in c-sharp one line of code some keys you're good to go now you can pass in far more you can pass in options and defaults and everything else but that's just one that many we're components righty means is very easy in fact most of them frankly are only a few lines long which is great and the power that involves because they're so stackable and modular is really it's been transformational we've been doing haps with this around the world we're kind of an organization we called CSEE would bring companies and customers in to hack on this to get the initial feedback and the middleware bits have been where integration with other systems has really been powerful this has let them integrate with existing analytics tooling with with everything that they have at their company so to go with that we have a new emulator the emulator is still an electron app it still runs everywhere at its core nothing has changed for that but now we have a lot more features in there we have a full plug-in model here so that you can start building visualizers for these things so that when you click on for example some data that comes back from Louis as part of your bond you can actually edit and manipulate your Louis model there we have all kinds of activity tracing here so if you're in your bot you're doing the equivalent of printf statements or debug write lines or what have you those will now be sent to the emulator over an out-of-band Channel and the emulator will surface that to you so the old printf style debugging is actually practical again which is always a nice thing to have the emulator supports it supports chat transcripts it supports looking at things it supports multiple bots for those of you who are trying to launch multiple emulators that want to have multiple conversations with a single bot this is now a tabbed approach so you can have as many conversations going with your bot as you like so after Darren's demo mine they're a bit anti-climatic but I think that's actually a good thing so I was going to show you really quickly here if I can bring my laptop up see if that comes up which one of my on it so here we are in Visual Studio she let me cancel this let me start completely fresh get started so this is Visual Studio 2017 with nothing unique installed so I'm just gonna go file new project this is the easiest way to get started building a bought and c-sharp for those of you in node we have yeoman templates for those of you in Python and Java those are for the most part coming very soon so visual studio supports physics templates so from getting started all I did is I came over here and saw it online and I usually scope this to just be bought related templates and you can see the bought builder before template that we just released and say okay you can wait a moment so that's going to create me the world's simplest spot right this is just an echo bot with a little bit of state management so that I can count turns but that pulls in all the right nougat packages and we can see here we come up here over dependencies we can see it pulled in our asp net core this by the way is asp net core - this is not in that standard so all of this bits will run so at that point we have an echo bot let's go ahead and just run this lets do nothing else i haven't changed a line of code yet so as soon as this comes up clearly this is a reason I have to tell my boss I need the bigger newest surface book so now they come over here and copy this URL this can actually be improved because the emulator now does support kind of you are n so we can actually build the trigger in here the click and do it we just haven't had a chance to do that yeah I'm gonna create a new pod configuration I'm gonna call this my demo echo bot I'll put my URL right here not worry about anything else I'm just gonna save and connect to it we can save it right there so at this point I should be able to say hi and have that that go back to me so the emulator is now connected we can click on things this inspector as I said is fully pluggable we have plugins now for Q&A maker for Louis or a bunch of other things it's also possible to write your own the emulator itself is open sourced on github it's bought builder emulator if memory serves so take a look and feel free to submit comments issues and even PRS against that I wanted to take a quick look at the code so this is my full echo bot so this is the template for for building a bomb this is deeply integrated with asp net core so all we've had to do here is have a class that implements I bought that in turn just handles turns right the turn the conversation is is the other ends back and forth so all we've done here is said hey if the user sent me a message echo it back and the only lines here that are kind of tricky is get conversation state and turn count so what that implies all by itself is that we have a way to manage state you can either deal with that yourself if you choose or we have middleware components under the hood that they're running that you can turn on or off to do that for you so in this case we have a turn count variable and all we do is echo it back and to give you an idea of what that state looks like that is my state management class right there now there's just a plain old C sharp object no methods no nothing just managing state we have state managers today for in memory because it's the quickest to get started with it but obviously it's not very durable we have file based ones for very simple getting started scenarios we have Azure storage we have blobs tables we have a cosmos DB one it's also very easy to write your own the interfaces are pretty well factored so you can go ahead and build your own storage infrastructure you have you have something you need to use yourself so that is my most anti-climactic coding demo ever so now let me come back here to five and Q and a maker is one of the ways we are enriching these BOTS that's making them smarter because echoing as much fun as it is it's not probably not going to impress your customers as much as it impresses me so our hands up who's used Q&A maker previously all most of you so hopefully this will be good news for you so it's announcing general availability of QA maker color this week so it's been a monastic uestion of when's he gonna be GA when we gonna have higher kind of limits and so on it says note that no longer has kind of the the throttling or the quota limits that it had before a key change which is driven by kind of customer feedback is before it was kind of like a software-as-a-service thing you're going create QA maker you add your questions in and we store it somewhere and we use kind of search technology it's actually now when you go ahead and create a queue in a maker instance and I'll show you this in just a moment you actually go and do this in the is your portal now and actually it creates it as your search instance so it's now using as your search so actually nursing helped improve some of the search capabilities when it deployed QA and as your search and a few other storage components into your as your subscription so you have control over it it's not too hidden away in some kind of Microsoft element so from a compliance point of view and they're really important color change there that you own that have come aspect and also native integration as you would expect a bit like the bot framework of application insights you get good insight into what's going on so cool so just to show you where Q&A maker can now all shows up so if I go ahead into the portal I can go ahead and search for Q&A maker and you hit create and just like any other incidents in as your you go ahead choose your subscription the pricing tier and things and the app name and there goes ahead and creates that in actually in your in your tenant so if I go ahead to community maker so the portal still exists and once you've created that initial incidents you can go ahead into this portal and I say I want to create a new knowledge base and actually takes you here to these your portal so I've already done this so now I'm going to select the my is your subscription and you'll see I created a Q&A instance before here you give it a name so I'm just going to call it build demo - or something and again just like before you can add URLs and actually we'll go and grab you know fa Q's out of it and part of the new some of the new capabilities are around there better PDF support so a lot of you that have used community makeup before and had this great promise to better take a document and better harvest out those questions and one click and away you go in it it worked great for some documents if they were really well structured but if you gave it something that was a bit more real-world than kind of human readable then actually it wouldn't do as good a job around out actually extracting that so actually one of the things I want to show you live so you know that this isn't because I made up I'm gonna go ahead and I've selected some manuals from you know just downloaded kind of generally so this is a manual so I'm going to show you the manual just so you know that it's not a if I get a Samsung TV so this will be a manual like for any of the products that you would expect it has all the usual images and tables and stuff in there so a normal manual that a human would use so I go back to here I've added this PDF in I'm going to go ahead and create my knowledge base this will take a few minutes where it goes and does all of its magic it can extract kind of information out what we'll do just in the interest of time is we won't wait for it because it will probably take a minute or so if I go ahead look at my knowledge bases you'll see one that I created this morning if I look at Samsung manual and actually that's the same PDF in this case and this was and it's extracted you know pages of kind of information around you know securing the TV to the wall and so on so one of things you can interactively do in the portal so I've gone ahead and done this so I can ask a question like you know how do I put my TV on the wall or something put my put my TV on the wall so this is just a PDF it's in just brought this information in and again I haven't said secured my TV to the wall I've said put my TV to the wall and it's got a head actually done a really good job at extracting out that kind of section actually in that PDF manual now it's not always gonna get it right every single time so actually one of the really cool things if you hit inspect it will it's showing you the most appropriate answer that he thinks is relevant here but it will also show you a confidence score but it's also giving you other ideas so actually interactively you can go actually you've got that wrong this is the answer you can see it's interactively changing you're just giving a little bit of a nudge it's brought data in it may not get it 100% right so again you can go ahead and kind of nudge that publish it and then away you go so this is call it a nice kind of tall there so with manuals I would definitely encourage you for where you've used it before and it hasn't quite worked properly is definitely give it become another look one things I want to give you a line-of-sight offs and this is not something you have to go and use right now today but to give you perspective of what what the team have been working on it's kind of great we can extract out too intense and entities and so on but could be one of the common requests is how do we pull the images out of the document so in the bots conversation if you're guiding through changing the bulb on your car for example how can you pull out the text and the image and give that to the conversation to actually go and show so this is a little kind of internal tools quite cool so it's taken in this case a car manual but it could be the TV manual as well we're just find where my mouse has gone there it is do you think you can actually see in here it's actually extracted a lot of structural information in this document you know climate control hints manual air distribution so you can see this is the structure that Q&A maker under the covers is actually extracting you can also see this pulling out entities like we all footwell windows are perv you know or a bunch of these entities which all helps with a bot being able to go and find the information here so entities as you would expect qat pairs which is obviously what Q&A maker is showing to you today but also give you a bit of a sense of some of the things that are kind of coming through is that there is a section here that's got a you know a fairly broad set of information so it's actually which is starting to explore the ability to automatically have a dialog and say well I don't quite understand I've got like four bits of information which one do you want so you can see here it's actually showing you about general information switching between letters and numbers and without the navigation system and so on you can start see we understand the structure of the document so we're reading it in a bit more kind of common sense another cake a case here is table extraction so actually you see here we're actually identifying tables in the document and that metadata and returning it out so you don't have access to this kind of quite yet today but you can see where we're going with Q&A maker of tables and images and so on so you should see kind of further kind of improvements in this space so from a Q&A maker kind of point of view any good bot experience to go back on the best practice it should better do things complete tasks for me but it should have some more journal knowledge to help to satisfy that users question and Q&A maker is a great way of quite quickly and adding that extra knowledge to your assistant she's just very quickly talk about the dispatcher there were some good kind of documentation kind of yeah on this today so you saw me create a dispatch model before so I took my multiple Lois models I took my Q&A maker data and I'm able to evaluate and understand where there's ambiguity or kind of your duplication so I can improve the quality but you can also take that resulting model and actually push a question coming into your bot into the dispatcher model and it will evaluate which of your end Louis models you should actually route that to so if you're going to break your bot up into lots of different components and things then this is a great way of routing it to the right component and knowing then it will work reliably and and it will then help you route it to QA maker or to colluding or to the Lewis model so actually on some of our more complex kind of bot engagements and that we've done where a customer had a lot to different Louis models for different products and so on actually figuring out with the bot framework today you're calling each Louis model and getting the score and trying to compare scores and go is not 0.29 better than not point three nine in this model is very very brittle because as you retrain the model then the scores go up and down over time so you end up with this kind of not great experience around things going down the wrong route so the dispatcher addresses this so this is kind of in in preview the bits are up there and we're continuing to look at this but I think from from our perspective initially react or architectural component kind of moving forward with that pass over to Jason cool super thanks Darren thanks for having me Matt my name is Jason Williams I'm with Microsoft Research AI I've been working on machine learning approaches to conversational systems for almost 20 years or so and I'm really happy to tell you today about project conversation learner a conversation learner is an AI first way of building a task oriented bot or a task oriented conversational interface at the core of the approach the heart of the approach is a recurrent neural network that we train on your behalf for your domain it takes all of the dialogue history of this conversation seen so far as input and then directly predicts which action to take next that allows it to learn directly from example conversations in your domain and that in turn allows designers business owners domain experts to directly express what they would like the bot to do in a very accessible way at the same time this approach is fully situated in code so it's possible to have API calls that invoke functionality in your domain so for example to print a train ticket it's also possible to add constraints that limit which actions are available so for example if you're building a banking system and you want to have a constraint that says only users that are logged in are allowed to transfer funds you can express that as a hard constraint that's not appropriate to learn as a machine learning relationship the overall approach here is to code when it's easy to code and to learn what it makes sense to learn and I'll give you an example of how we can put this together in just a second but before I do that I'll explain the workflow because when you get started there are no example conversations to train from you have to create those you have a cold start problem so to get started the developer starts off by playing the role of the user so they type in examples of what they think the user might say then using our UI they interactively make corrections to what the bot is doing and after they do this for some small number of dialogues they get the system working sort of a good-enough level of performance that they can put it in front of some early users those users interact with the bots and then again using the UI the developer can then go and make corrections in the space of the example so they can look at places where the bot took action a and say no you should have taken action B instead and those immediately become training examples and we constantly are retraining this neural network so this workflow lets you get from nothing to something to good all the while improving the same machine learning model so I'm going to spend most of my time here on giving you a quick demo of this so this is an example in the password reset domain and here I have three training examples three training dialogues that I've populated here so far here's what these look like so here's an example where the user started by saying I forgot my password then this was the system's response then the user said local account and so on and just to emphasize when the system is predicting this last action here how to reset password on Windows 10 it's it has sight of this machine learning model has site of all of the preceding contexts so if there's something that happened early on in the dialogue that's important for that decision it's the machine learning model is able to see that it's able to use that information the actions by the way are drawn from templates that you design we're not generating word by word there's not a risk that will go off brand or will say something completely unexpected you have control over what what the bot says let me show you how the interactive teaching process works so let's say that now I'm the developer I'm going to also play the part of the user here because I'm just getting started so I'm gonna say I lost my password and now what we'll see are the predictions of the neural network so this is just on these three training examples it seems so far this is it's they're sorted by score this is its best prediction that's right that's what I want it to do so that's good I'll have it go ahead and take that and I'll say this is for my local account and here you can see it says which version of Windows do you have and again that's that's what it that's what I want it to do and now I'll explore a path that hasn't seen before so I'll let's imagine if the user says Windows 7 so here if I look through here there is no good action at this point that's fine at any point I can I can add more actions so now I'll add a new response that the bot can say this could also be rich content actions can also include cards links and so on so I'll go ahead and say done teaching and every time that I add another example in the background we go off we retrain the neural network and there you can see that now that network is updated with that most recent training example so now all the future predictions will have the benefit of that so I could continue this process and add add some more instead I'm going to switch over and show you what an end-user would see so this is the web chat control we just make this easy to access in our in our UI this is the same web chat control that you could use to embed in your web page and now let's imagine that the user says I forgot my password so this is now the end user interface let's say that's for my Microsoft account so so far so good now if you've ever deployed a bot you know that users are gonna say things that you hadn't expected and they're gonna say things in ways you hadn't expected so let's say the user says something here like thanks you can see ah okay our bot didn't do the right thing okay we'll hold that thought for a second because that's that's one sort of modest case there then the more extreme cases maybe users come along and they don't even know what this bot can do and they type in something like restaurants well I don't I don't have a good response to that so that's fine this is good data the the key is how do we learn from that data quickly so here now I'm going to go back to being the developer here are logs of those conversations that just occurred so I can look at this conversation as the developer and I can say okay my user said restaurants I don't know why but that's not the right response let me click on that and let me choose what the bot should have done you know none of these are right I actually have to add another response let's say sorry I can't help with that so that will add that to the catalogue of responses I can then choose that and say that's what you should have done that creates a new training example I could continue that training example here if I wanted to I'm just going to go ahead and save that here a couple of other logs that I did before I came up on stage here's a user who said driving directions well that's not right I want to use that sorry I can't help with that response there let's go ahead and save that and now these two let's have a look at these so here's one the user at the end here says thank you everything before this was right but this thank yous not right want to add one more response here that's how the system say you're welcome there there we go and then this last one this is the one I just did here my user said thanks yeah let's end that with your welcome as well great so the point of showing you this is to illustrate how quickly you can iterate and improve the bot by looking at these examples and by choosing the response that the bot should have taken and again in the background we're constantly retraining the model and so here now I'm going to explore through and see if we've learned some of those new behaviors there's a very small number of training examples in here so your expectations should be pretty low let's say if the user came along and said web search now hasn't seen this language before but I have a hope of getting this right and it does and I'll explain why so in here there's also a word embedding model that gives this it's trained on hundreds of millions of webpages that gives the model and inbuilt understanding of synonymous so it can begin to learn boundaries between classes so here it has a class of everything that's outside of password reset everything that's inside doesn't know that boundary perfectly yet it's not super confident but you can see that it's beginning to get the right idea so now let me continue here let's say that I say hmm I lost my password so now I'm I'm still in this same conversation is that for your local account or Microsoft accounts so here I'm getting back on track even though I've never seen this particular flow before let's say that's for my local account which version of Windows do you have say Windows 8 so again this is a flow that we haven't seen in its entirety before and at the end after this Windows 8 we've never seen a user say thanks before and see if I press my luck here we can get that right as well so notice I didn't have to go in and code all the different paths I didn't have to create a whole flowchart and specify exhaustively what happens everywhere because machine learning is able to look at fragments of conversations that seen before and infer paths that ought to make sense and if it ever gets the wrong path then you can go in and you can add another label okay so let me switch over just a sec and very briefly show you a little bit more of the depth of the tool this has just Illustrated the ability to ingest text and to choose responses there's there's a fair amount more here so I'll switch to a different a different task as well which is ordering pizzas and here I can add entities so entities are passages short mentions in text that have a type that you define so for example toppings on a pizza and in our tool entities can also be text that gets inserted into actions that the system outputs so entities are also can also be programmatic there's a collection of actions here and what's interesting is we can express we can embed entities in actions so for example you have dollar sign toppings on your pizza and we can also express relationships between actions and entities and finally we can have API calls and we can also have cards so let me just quickly show you just a kind of an example of one of these training examples and how some of those concepts work so here's a place where the user said peppers and sausage as toppings they want on their pizza we can label those here and when we label them they come into the bots memory and you have programmatic access to this memory too so in addition to recognizing things from users you can be manipulating this however you want to there's a series of callbacks let's see we also have API calls so this your order is on its way as example of an API that exists in my bot and finally here you can see here's an example of a cart so this is an action where I wanted to kind of guide the user to our choosing yes or no so I'm glad you asked us so this is available today as a labs cognitive service by invitation you can go to the page here and request an invitation and we're gonna be taking people on progressively we're really looking forward to your feedback we're delighted to see what you can build and we'd love to hear from you Thanks thank you Jason all right thank you alright let's look at one more service that's it's hard to top Jason's but one more service that you might really like for those of you building BOTS is called personality chat and this is another labs cognitive services that you can start using with today and what this is is a means of adding kind of basic thank you you're welcome with personality to your bot so this works the v4 SDK here's an example of middleware being added and you can see right there you just when you're creating your bot you add in the personality chat middleware you can set some options if you have custom responses you can drive these from a file and the way this works is there are three personalities out of the box and you can type something in hello how are you thank you and it will give you the basic kind of conversational responses around that these are editable there are three different personalities to pick from I'll show those in just a second and it's just a way of adding a little bit of the basic things to your bot to make things feel a little bit more real so let's do a live demo of that so here we are this is live this is on the cognitive services lab page they've embedded this little web chat control there and if I just walk up to this thing you can see here this is just a normal website and type in hello and there's a whole set of things that they do document every single thing that they respond with so hello how can I help you that's a nice professional response if we drop that down to humorous and type that again maybe I can just say yo it's gonna come back with something else yeah so you can drive all this from a file this is all documented this is it's all gleaned from these responses request response pairs you can play with them you can you can do all kinds of different things with them and you just add them as I said very quickly through that middleware infrastructure so the team members who wrote that are around today stop by our booths and you can talk with them and see how you can add that to your own BOTS and other than that I wanted to talk about support for a moment one of the things that we offer as part of the Microsoft bought frame because we have free support through Stack Overflow and github so as you're encountering problems as things are happening as a particular adaptive car there's not rendering right on teams or an action that you requested isn't working right and Skype you can go the Stack Overflow and post and tag your stuff with by framework we have a team of people who will work with that every day and their SLA s are quite good around getting responses we don't charge for that at all if you do want the more professional SLA based approach we are we do also work with of course the as your professional support infrastructure so we have an ecosystem to help support you building your BOTS so a lot of the little nitty-gritty details of my adaptive card works here it didn't work there I couldn't register a channel I didn't get the callbacks we can help with that so if you get stuck please reach out Stack Overflow is a great place for that so is github with that you can go ahead and go to github.com Microsoft bot builder that's where our SDKs and our open-source footprint really starts we have this nice AKMs site you can go there I see everyone there phones taking a picture I'll leave that up for a moment and lastly I will highly recommend Elaine Cheng and vish whack at 2:45 today are giving you best practices for building BOTS they're gonna dive very deeply into a lot of the tooling that we showed they're going to go through for example vish whack and the SDK v4 will take you from the echo bot that I showed you he'll add prompts he'll add single turn dialogues who had multi turn dialogues he's going to build ed cards to that he's gonna show you how to use all of our date/time infrastructure which is amazing in terms of daytime resolutions and constraints and natural language around that both in and out so I would strongly recommend that talk later today otherwise if you have questions I think myself Darren and Jason I can hang around up here come up because I know we're out of time we'll be here for a bit yeah we'll be in the booth as well please feel free to stop by and say hello happy to answer questions without thank you everyone for coming we really appreciate your time and have a wonderful day 