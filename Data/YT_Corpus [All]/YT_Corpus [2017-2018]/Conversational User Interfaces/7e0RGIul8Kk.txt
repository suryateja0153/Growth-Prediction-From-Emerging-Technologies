 [Music] hello everybody thanks a lot for coming to our talk so I'm Gila Falls and the developer advocate for Google cloud platform and I'm Brad Abrams I'm a product manager on the Google assistant team and we're going to talk a little bit about how to extend the assistance with actions on Google so when sundar announced the assistant at I alast year it was for one purpose and that was to build a conversational interface to Google so when we think about so we want that conversational interface to be available everywhere that that users are so we want that conversational interface to be there in your messaging application so when you're talking to your friends about where to go out to eat the Google assistant is there to help you pick a restaurant we want it to be available in your pocket when you're trying to figure out how to get there full access to everything you can do on your phone and of course we want the Google assistant to be there in your home when you're kicking back listening to some jazz or if you need to read a recipe or set a timer we want the assistant to be there too and of course this is just the beginning we actually want the assistant to be available in all kinds of devices whether that's dishwashers or tractors so how do we think about the assistant the way we think about it is that it's a conversational interface and it's a conversational interface between you and Google that helps you get things done in your world and so the thing I'm really excited about with the assistant is it builds on decade or so of research and development that Google has done in a wide range of computer science topics certainly in search not but also natural language understanding and tint processing speech recognition text-to-speech of these core advances that Google has made but what's really interesting is the we brought it together in a really magical experience but that's not all what's important to me as a developer is that we've also built on Google's history of platforms if you think about it Google search was our first platform it was a platform that connected publishers with readers then we have a platform with YouTube today that connects creatives with their viewers and a platform with play that connects application developers with their users and so building on that rich history of platforms with Google we have actions on Google and actions on Google is the way you as developers can extend the Google assistance it allows you to connect with your users wherever the Google assistant is and it allows you to help users get things done with whatever service that you can offer and it allows you to innovate really in a brand new space a new paradigm with the way that users are interacting with computers this gives you a nice framework a way to experiment and innovate and so we launched the platform late last year and we couldn't be more happy with the actions that we've seen developed for it there's hundreds of actions out there doing all kinds of really interesting things and I hope that by the end of this talk you'll be ready to build your own action for the Google assistants so let's talk about that action so Guillaume and I were working on this knowing this conference was coming up we thought it'd be great to help users find their way around the conference so we build an action that helps users understand what what sessions are going on in the conference and how to get things done and so this is this is just our demo application and so the way it works with the assistant on Google home is that's where we've launched actions on Google so FAR's on Google home is users give some sort of query like ok Google let me talk to cloud next 17 and then the assistant will respond sure here's here's cloud next and then you hear an ear con ear countenance to the ear when an icon is to the eye that's a signification a demarcation that something is changing in this dialogue and then our action actually kicks in and takes over the conversation and has a two-way dialogue with the user and that happens in a different voice to further differentiate the experience from what's going on with the user so there are four voices to choose from and you can go and hear audio clips two male and two female voices so you can pick the one that that best matches what you're trying to accomplish anyway then there's a two-way dialogue and then when either party is ready to end that conversation the conversation goes back to the assistant after the exit earcon is played okay so that's the high-level framework what we're going to do in this talk is we're going to build that action for you we're going to walk through the entire process from the beginning we're going to talk about designing for this new mode modality for this eyes-free modality it's very different experience than if you built a website or an application before then we're going to talk about the development of such an application both in terms of the machine learning natural language understanding aspect as well as connecting to your back-end systems and hosting of your business logic then we're going to talk about how to deploy deploy that action both in terms of the server deployments that need to happen as well as how you help Google understand about that action with actions on Google ok so I'm going to jump in and talk about the design section so first we need to have a conversation about conversations will we ever be able taught to talk to machines the way we talk to people you know science fiction certainly says that we can but let's be honest talking to computers is really difficult today if you've ever been through one of the voice trees before you know that conversing to computers today really sucks but that's only because human to human communication is only deceptively easy it's nothing short of miraculous that two people can enter a conversation and leave it knowing something neither of them knew going in so it seeks Oh conversations are easy right well they are easy for the user in fact they're probably the easiest way that users have to interact you don't have to guess what will happen when I push a button I just use my natural language but for for us as developers and software professionals it's hard these rules are hard to codify it it's difficult to make a computer really understand what a user is saying but it's only it's only easy for us as humans because we've been doing it our whole lives conversations are governed by language and language has rules and language has syntax and context and as humans we've gotten amazingly good at understanding and dealing with context but take heart we whoops we don't have to get there whoops we don't have to get there overnight humans have been speaking in language for hundreds of thousands of years and we've only just started to have computers try it trying to talk to humans but this does mean because humans have been speaking this way so long it's probably better to teach computers to talk like humans rather than the other way around so let's talk more concretely how do we get computer how do we design an interface that helped humans talk to computers well I'm going to offer a couple of tips for you the first one is really to create a persona the way you interact with your wife or husband is very different than the way you interact with your boss that's different the way you interact with a person selling tickets because there's a different persona different roles involved there so it's no wonder that the first thing you need to do when you build your action for the Google assistant is to think about what persona you're trying to create so these this is some rules that we've a process that we've laid down and we've used it with many partners it seems to work first we try to identify our core brand attributes what is fundamental about our company or bot that we're trying to trying to do and so when gingham and I started working on this action we thought you know we wanted it to be knowledgeable helpful encouraging this kind of experience we wanted out of it and then we were next to try to tease out what are those design principles what are the guide rails that puts on our development one of the partners we work with told me this process really helped them trim features from their experience by by deciding what the design principles are so for us we really wanted to be data rich we wanted it to consistently recommend and be proactive and so actually I am spent a bunch of time getting all the data we could about what sessions were available when they were available who was speaking what room so that we would be a very data rich experience next think about the actual dialogue writing what is the voice what's the style and persona that we want and for us we thought maybe a little geeky I mean this is still a Google conference but eager and motivating as well and finally we recommend you create a style guide or bio sketch that you can use to inform the writing of dialogue a concrete artifact and so here's a template of one this is the one that we used and notice it says cut sample things that your action might say what might your action say what are things that you would just never say I mean they're they're outside of their persona you shouldn't say that and then what words should you use you know what are some substitute words what you finally especially if you're doing this at scale is you may have several people writing dialogues for your action maybe the person changes over time as happens in development processes and having an artifact like this that really makes concrete who you're trying to be as your action can really help that leads us to the next one which is around writing dialogue so here's an actual sample dialogue that we started writing and then you could see the mark-up that we did as we went through and started applying some of the design principles that we have and that just gives you a sense of how this process can go so and that is really my second tip is really to go and invest the time to write some sample dialogue and that can be as simple as opening up a Google Docs file and writing dialogue using the style guide as a way to help yourself do that but was this a few samples we suggest you write so first write down the happy path what's your dream path the way people are supposed to go through your action and make sure that's good and you thought through what the interactions without are next think about that first Hung user experience somebody who's never used it before what do you do to help them understand the interaction so that they know what you can do you frame the problem for them so that you they ask questions that you're more likely to be able to answer next is the opposite which is think about the returning user once you've earned them back how can you trim down the experience or taper the experience so that they can get to the functionality they want quicker the next one is there are no errors in dialogue there's never an error there's just clarification of meaning right when we have a conversation sometimes we misunderstand each other and we repeat back we clarify we ask did you mean we get more information in context before we take some major action maybe we confirm it if it's not a major action maybe we are confident enough to just do it so thinking about some repair air scenarios and finally you need to think about those personality questions what we've seen from the Google assistant generally is especially when users are first starting to use your actions they prove it and they ask it some personality questions hi how are you thank you good evening do you want to go on a date can I marry you those are our questions you got to think about within your persona what are appropriate answers so regret I don't want to marry you yeah you don't think it we'll talk we'll talk okay so that's a bit on design let's turn it over and talk about development right I'm a developer advocate so I'm also developing a B or C so I'm happy to speak about development so what's happening we are users we're talking to our Google home device the Google home device is going to interact with the Google assistants we speak to the device right so there's speech turn that's going to be transferred to the assistant and we're going to do speech to text analysis to be able to think and react to what's being said but with words which are easier well written words say which are easier to understand them than voice we do the natural language processing to extract things like the entities all the parameters that make the conversation as well as the intent of the user the user wants to learn about a particular session etc and there's lots of intelligence in there the ranking picking up who the the user actually ease etc and as well as text-to-speech back when we go back to - to the user so we ask to speak to our assistants so sure okay here's Google next 2017 and in the back it's going to call API AI which contains the our assistance the structure or the intents all the entities hi welcome to club mix 17 and then the conversation really starts so I want to hear more about machine learning so again speech recognition etc going on speech to text and it's going to invoke API and so there are some things which can be done automatically from within API AI let's say canned responses and things like that but sometimes you actually need to fetch some data from somewhere else and that's where we're going to use the Google cloud platform and in particular cloud functions which as you discovered this morning was announced as beta and that's within cloud functions that I'm going to write the business logic of my action and what's nice is that on the next website there's actually a REST API that I that I can invoke to get the list of speakers the list of sessions the list of topics etc so my function is going to invoke that REST API to find the all the talks with the label machine learning basically and then okay the next version running session is blah blah blah and then with text-to-speech I get the the response the the session I was interested in and I let Brad oh yeah no uh let me just say a few words about that one and I'll let you two do the demo afterwards so ATI what is it so it's cross platform 2 for building advanced conversational interfaces we define engine so the difference of questions or interaction you can have with your assistance but then those interactions also need sometimes some parameters so one intent would be okay I like to learn about machine learning likely the example we had before so the engine is learning about something but that's something that's our parameter here so it's exactly like mapping utterance what the user says to actually a function called the function name would be the intent that we try to call and the parameters are the parameters like the topic motion learning that we pass as a parameter to this engine and I let this time alright can we have a du by one okay so what you're seeing is the API de AI tool and it's a tool that's freely available everybody can go use it and it's available right now so it it is the tool that helps understand this huge vagaries of human language and turns it into intents with arguments so here's the set of instance I have will talk about a second and then this is a little test experience here so for example we can say talk to cloud next 17 and that's an example user utterance and then it gives us this the same result we saw on the slides but what's more interesting is it's actually taking that text and it said that goes with this particular intent and in this case is the default welcome intent and you can see in this default welcome intent there's we've given several user says expressions we get in several examples of what a user might say that should come into this intent and then in this case we've given some static text that you should respond so that looks pretty easy like you could maybe do that with a if statement but it also works we do some basic machine learning so you can add add words like please and it still gets it or can I get cloud next 2017 it's not a French keyboard yes not even so even if there's some optionality and it learns time you can do some training on this more advanced topic so that's pretty basic actually an aside here is actually showing us one of the cloud marketing guys and the prompt is what would you like to know and I said well clearly the thing I want to know is where can I get a cocktail okay market guy on it there's nobody from cloud marketing here okay good that's what they wanted to know is where can I get a cocktail and you can see we give an OK response here it's basically the default help intent that basically we didn't know what the what to match the two so we matched it to the health intent so that's kind of not a great answer so let's actually see what it would be you know since the marketing guys are paying for this if we can help them find their cocktail so let's call the intent fine cocktail and then what a user might say if they were looking for a cocktail well they might say where are the cocktails can I get a drink where's the booze yeah exactly are there any cocktails so we recommend between five and Kennedy's expressions to get started to have enough to have it makes sense then worked with marketing to get some approved language for this action because this is this is the most important one clearly and now when I give that same query where can I get a cocktail notice this time it max to define cocktails right and in fact is there a cocktail please still matches so it's same kind of optionality whatever gets gets done and if you do some more training it can get even better so that's pretty simple let's look at something a little bit more complex in this find topics so the way this one works is we've given several samples but we've highlighted some things as entities or slots we're saying anytime they say breakout sessions whether they say big data or another topic then it should match this intent or are there any talks or I want to know about or whatnot you can see this actually matches to a topic so and the topic is actually an entity so we flip over to this entity you can see that we've actually pulled in a lot of data about what are all the entities what are all the things that users might say so for example I believe we are now in the Big Data machine learning topic and so we've said look other ways people might refer to that topic are these are these aliases so now for example if we say I need to learn about Big Data then it will match that hopefully it'll match that to the fine topic intent that's great and it actually matched it the topic to big data and machine learning so that's pretty cool right it actually matched it and extracts that entity to make things more interesting there's commented in the UX section about context so if I reset the context and I say talk to GCP next and I say more here it says sorry I don't know what you're talking about because more doesn't make sense for that prompt but if I ask about Big Data then in math hopefully it matches to that intent and it does then I then here I can say I want to know more about that and now more actually has a meaning we're within a context so the way I'm sure this is going to work in just a second yes yes can we just roll over time yeah there it is the way it works this time and knew what it was and it got the data and notice the context is fine buy more next and that's because we actually set up that this context when it leaves this intent it will set that context and then the more next intent actually request is guarded by it requires this find I find my context next okay so that's the basics there one more time here's big data and let's see what the Jason actually looks like so you can see this is actually what API they I is going to send to your web hook so all API they I does is take the natural language and turn it into a function with a method call the function is this it says the action is find by topic and the parameter here is big data and machine learning so it's taking this enormous variety of human language and turned it into this big data machine learning fine by topic so now GM is going to take over and show you how to process that jason exactly so let's go back to the slide I'm going to take a few words about claude function so here find by topics it's actually going to call claude functions the cocktails it was already inside api i didn't have to get outside of api i but here we're going to use odd functions for you know doing the function call let's say the thing is here I'm speaking about cross functions but what you need is just somewhere to have an application deployed that's able to parse and produce JSON content basically so it doesn't have to be on Google cloud platform it doesn't have to be class function so you can use anything anywhere as long as it's a URL which is available on the on the internet but white-clad function so it's been a bit uh announced as because this morning it's our new server list platform it's even base or even torii nted there are three kind of events which are supported right now cloud storage update so for example imagine you have an image which is uploaded and then you want to react to do them nails and resizing or applying some filters if you're using cloud pub/sub it's going to react to a new message which appears on some topic but the one we're interested in it's the HTTP trigger so you can trigger a cloud function via a call and HTTP call in our case that's a post request to our platform so what's interesting with that card function [Music] especially when you're writing something like like an assistant you are not necessarily a super wizard with tons of knowledge of how to deal with with your you know servers and things like that that's what's nice with a server let's approach and with god functions being a fully managed service you really don't have to care and worry a worry about the ops side of things also the other nice aspect of it is that there's automatic scaling happening so even if your assistant is very you know successful you won't have a problem with scaling because the scaling is done automatically it's a fully managed service and it's also cost effective because it's not like long-running VMs or containers it's built by basically the function core I don't remember the billing details for cloud functions that it's pretty cost effective if you don't get anyone using your agent it won't cost you anything and the last aspect which is important as well is that class functions when you offer pod functions you devote cloud functions each using javascript and no GS so you know pretty much any developer these days know enough of JavaScript to be able to write cloud functions so even if you're you know a front-end developer you're obviously using JavaScript at some point so you don't have to be a back-end developers but if you're a back-end developer using no GS well fine and also as that would be also the announcement of the GA of App Engine flex by the way and there's no GS support in App Engine sector you could also use that alright uh so let's get back to our little architecture diagram so we have the user asking the home device contacting the assistant then calling API AI then for some of the interns it's calling our cloud function which called the REST API exposed on the next website but yeah before saying that cloud function is really pretty nifty when you deploy a new version of function you know you're giving okay let's answer somewhat differently or let's see there's a special case in the dialog that I need to handle like an error because be like the next REST API is not available etc so you you're developing and then you're you have to redeploy your function deploying a function is pretty fast it takes 20 or 30 seconds but when you are developing you'd like to have somewhat of a tighter feedback loop and be able to develop more quickly without having to you know wait let's say search seconds every five minutes so you could for five minutes then you wait to get 30 seconds while you deploy your changes so it can be a little bit irritating so I'm going to tell you about two interesting tools which allow you to be more productive and see your changes live so the two tools I'm going to talk about are and Rock and the odd functions emulator the client friction emulator is a way to actually rent clyde tensions locally on your machine on localhost basically so it's a node module it's open source it's available on github you can use pretty much the same comments as when you use g-cloud functions deploy etc it's pretty much the same set of functions which are available of commands which are available but there are two nice aspects live reloading so as soon as you make changes to your code the changes are taken into account so you don't have to redeploy anything the changes are live right away and the other thing but that I will not cover in my demo it's the fact that you can also debug in Chrome what you are doing your your your code the code of the body of your function which can be pretty handy because you know it's not like putting println statements when you interact or the println statements is like talking through the google home device basically and the other tool so the emulator is working locally on your machine so it's as I said on localhost but API I when you say okay use my web hook pointing at my cloud function it needs a URL which is accessible from the web well localhost is usually not accessible from the web otherwise you know your machine might have been you know hacked or something but there's this nifty little tool called an grok which is both our service and come online tool which creates a tunnel a secure channel over the Internet to your machine so let's see that in action please let's move to the second laptop alright so in our will we'll have a look at the fine by topic intent or perhaps the simpler no let's look at this one fine by topic so we have over all the possible occurrences and at the very bottom there's there are two things which are interesting advanced I could also mention that what the action that's how we will refer to this current instance in our code later on but the two things I want to show you is so these are the parameters I want to find talks about a particular topic so that's my parameter here and here I'm using a web hook so this is how I tell I PII to ping somewhere else to fetch the data and in the fulfillment section here this is where you can define the URL where the calls will be made so usually normally when I use cloud function so here I have a few functions my function here which is the one I'm using there's a trigger tab where I can find the URL so usually that's the one I would have had here but here I'm using n rock and end rock and I'm going to do that live let's let's see that in the channel I'm gonna okay so when I develop my my function and I'm going to show you the the kareena in a moment I have the the emulator available here so I've already started the emulator but usually you start the emulator with doing function start and then functions deploy agent to well it's already deployed but I'm doing it again just for the sake of this demo function is deploy agent 2 and the I'm using the HTTP trigger and this is how I function I deploy sorry the first time my function so it's available and look at the accessible URL of course it's on local host on port 80 10 that's going to be important because we are going to use that information for running n drop and this is the local related path related to the the domain and here I'm going to launch and Rock say okay it's in a HTTP call on port 80 am the end Rock service is giving me the web accessible URL here and this is the one I'm gonna copy and paste in my fulfillment webhook and I save that okay and n work is running in here but there's also if you look at this there's a web interface and while I'm going to clear that there's a web interface which allows you to see all the calls that came in so you can inspect the request the response so when I go back here and I do so I want to hear about App Engine talks for example it's going to call here you can see from the command line that there was a call that was made and from the interface you can also see the request here and the restaurant that my function replied and then let me show you the code all right so this is JavaScript not GS I'm using two node modules there's the this one node fetch is because I want to interact with a web api the the one on the next conference website and this one is more interesting this is the actions on Google node module so although you can just consume and produce JSON there's they are several cloud SDKs actions SDKs which are available for different languages different platforms so here I'm using the the node.js version I designed some constants for my different intents and I'm going to go directly to the end and here I'm exporting the function here that's agent to that's the second version of my function which takes the request and a response so I'm initializing the API assistant API I assistance module then I'm doing the mapping between the actions that use so here the actions which were named so find by topic you remember me saying something about action here fine by topic this is the constant so this is this one fine by topic and it maps to local function JavaScript functions that I have so for example the one I'm interested in that this one fine by topic so I can get the argument because my function by my intent here takes the topic as argument then I'm doing some string magic to to make it work well because like when you have what was the problem I remember having something with Android and the let say be the real end connector which was a little bit weird so what I'm doing so if there's no topic that was passed okay sorry I'm going to reply sorry I don't about that topic could you tell me more about that otherwise it's fine I can fetch the content of the rest API here so I'm calling the sessions endpoint and then with that I'm parsing the JSON and then I'm looking through all the existing sessions to see which sessions actually correspond to the topic I'm interested in so if there is no session found on that topic sorry I couldn't find any such session is there another topic you're interested in and otherwise what I'm going to do I'm going to set the context look at this so this is the context that Brad explained earlier we found some session so we're going to look at the first one but I'm going to add inside the context the the IDS of all the sessions on that topic because I want to be able to iterate over the various sessions which are available so that I'm able to fetch one after another and then okay the next session about the topic is called blah blah blah would you like to hear more or do you want to move to the next session so either you kill more you want the detail the room it or you want to hear more and so let's see that in action so you already so the first goal but when I do so let's say tell me more that Chrome again so here this is the the full thing the time the room number the building and then the abstract with the details right did you notice something bread you know yes thank you I noticed at that time you know I'm gonna clear in America now like we don't do this way for our clock okay okay so I'm French and I usually write the time 13:20 or it was actually the military builder yeah yeah yeah alright alright let me see what I can do for you Americanize it work yes so this was this is the fine by topic more intent that's the one which gives more detail but as you can see the time I just blindly took the start are callin stop minute and I just you know appended the two so I can be a little bit more clever clever so I'm gonna comment this one and use this one instead so if this cut our so I'm done doing a modulo 12 stop minute and then if the the the actual hour was before twelve that's a.m. after twelve twelve included that's p.m. alright so I did my chain I'm saving the the changes I did the changes live okay and when I go back here and I call more again it's going to write 120 p.m. I didn't laugh didn't have to redeploy again or I don't know recompile or whatever redeploy and lose like the 30 seconds that I was mentioning before all right and then yeah so something I wanted to mention as well is the the you can also tale oh I forgot to launch it you can tell the logs locally locally so you can see the logs as this as the the requests coming but when it's deployed live that's this icon when it's deployed live I can also see in cloud functions of course you can also see the logs thanks to stackdriver logging alright let's move back to the slides please so you had the various modules that you needed local JavaScript functions and then you export the actual function use the assistance tool kit or SDK and there are many more available in different wages javascript go Ruby Python and I don't remember the other ones so that's basically the structure so it's pretty you know straightforward kind of JavaScript so you should be a really Addison it should be fairly easy to develop assistance using cloud functions all right yeah something we can also show it ultimately you want to use the real device obviously but we also have a web simulator for actions on Google so the we I'm going to show that let's go back to the second demo laptop so oh yeah I have to so let me close that so you you'll come back to that as well in the integration parts when about the deployment but you can integrate your assistance with different things like facebook Messenger etc but there's also an integration with the assistant and Google home so here I'm gonna you're gonna go into details and see that so I'm just going to launch the simulator here so authentication and then there's a preview button and when I use the preview button it's gonna let me launch the simulator again I need to click start there's one more sonication going on and then I can do talk to cloud next seventeen sorry this action is not available in simulation now sometimes I think can take some some settings you talk it go Tok oh yeah sorry talk to cloud next seventeen sure here's cloud next seventeen then we can do my I'll be your guide to Google cloud neck coming up March 8th through the 10th in San Francisco I can help you explore topics or pick a session to attend what would you like to know and then the usual flow as we so can continue better you'll do the real them on the real device later on yeah back to the slide earlier on something I wanted to mention before going back to the slides that you might notice that the request and response the the J'son is not exactly the same as api i that's the one from the assistance and then the format's are not necessarily the same but it's also interesting to debug and see what what's going on as well all right back to the slide so I guess it's time to deploy so when you deploy locally its regis function deploy agents and then the trigger HTTP flag and when you deploy on the real cloud function you do g-cloud betas functions deploy agent trigger HTTP that's pretty much the same thing with the G cloud SDK the sole thing that's different is the stage buckets because you'll need a cloud storage bucket to actually well deploy your code so that it's being actually deployed in the in class functions but it's fairly easy and also something since we're using nodejs you can I mean the this line might be might seem a bit long to write but you can also in your package JSON JSON file you can also have some shortcuts so that you do node ran NPM not NPM run deploy and then it does open that all the magic behind the scenes so yeah we deployed the cloud functions and then good I'll get that okay so that was the server deployment that does bring it back to actions on Google how do you deploy it so that it's available on everybody's Google home wherever the assistant is so there's a web-based portal where you go and put in basic triggering information and it's just part of the same cloud console developer console that you've been using and you list basic merchandising information icon description sampling vacations and then there's an approval process there's a set of both automatic and manual checks that get run so right now for your very first iteration we're looking about a one week turnaround just so you can you can plan on that for some of the manual checks that get done but then subsequently it's much quicker than that for updates to go log and now to speak a little bit about discovery within the assistant how do people find out about your action well one is the Google home app the directory listing is there so you can see users can browse through and find things and then there's also both guaranteed invocations and discovery pattern so we think of guaranteed invocation is much like your URL you have a URL to your website it's guaranteed people type in the URL go to your website so that's true the guaranteed invitations there's the set of phrases for example talk to and then your invitation name will guarantee users get to you and then there's a discovery patterns and that's other triggers that you can register for and other people can register for them too and so then there's a ranking problem which Google's good at figuring out about who is going to win that query for that user at that time based on a whole host of signals so you can register for those and they can be reasonably broad and will still let you have those and occasionally trigger for them okay let's take a look at a demo so back to demo one so here we are in the API manager part of the Google Cloud console we've just search for the Google actions API you get here you give it a what we call a display name that's what shows up in the directory but then notice you also have to give an invocation name so here we're calling it Google cloud next and notice I spelled out 20 2017 because there's a variety of ways you might want your name read is it 2017 2017 like there's a lot of different ways to say that and so by you spelling it out like that way it makes it clear to us how you want it said and then based other basic information then what you do is you go back to API AI and into integrations and as Ghia mentioned we have a whole host of integrations but the best one is actions on Google of course this and then now a game showed you the preview button I'll show you the deployed but just a highlight here's that project ID so that's the connection between what you put in the console all that merchandising information with this project and you'll just click deploy here and we'll just confirm that and I believe I have one under review so that won't work but then you can go it'll it'll submit it for review and you can see I've done this a few times and even I occasionally don't get it past the review but you do get some detailed explanation as to kind of why it didn't pass the review and I put another one in for review so they're there looking at it right now but this gives you a sense for where your action is so we now have our action deployed so let's take a look and actually try it on the real go home which is very risky to do in a large audience with these speakers and whatnot so we're going to do the best we can and then we have a video and perhaps we could also could you switch to the demo machine the second one because I can also since I let the local development instead of putting the right engine right deployment functions you'll see the the actual requests coming in okay let's just try a basic thing that actually this is the most common query done hey Google what time is it the time is 611 p.m. right so that it's an expensive clock hey Google talk to cloud next 17 sure here's cloud next 17 hi I'll be your guide to Google cloud next coming up March 8th through the 10th in San Francisco I can help you explore topics or pick a session to attend what would you like to know I need to learn about Big Data the next session about Big Data and machine learning is called introduction to Big Data tools that deliver deep insights would you like to hear more about it or hear about the next session Metro sure sure the next session is optimizing the value of data processing would you like to hear more about it or about the next session but sure the next session is infuse your business with machine learning would you like to hear more about it or about the next session or more about work for you sure here is more information the presentation starts at 3:00 10:00 p.m. in room 2002 at Moscone Center West the abstract says with machine learning instead of programming a computer you teach a computer to learn something and it does what you direct it to do in this discussion we'll share an overview of machine learning and how your business can benefit from it we'll cover the spectrum of ways Google uses machine learning and how businesses can leverage the technology that we've open-sourced the kuruma fuel also learn how you can embed machine learning in your applications today and create new applications powered with machine learning would you like to hear about the next session no I'm good thanks goodbye and have a good time at GCP next okay so that's our action lies thank you okay that's right yeah can we switch back to the slides please okay we don't have to show the video amazing okay so that could be hopefully the key takeaways you have from this is that you can and should even extend the assistant with your own custom action I hope you have like a hundred ideas in your head right now of things that you could go build I look forward to seeing you in the review process hopefully you understand now that API to a I can make talking with humans approachable and something that you can actually go accomplish and that the cloud platform here offers a really powerful way to do your business logic and just some other resources there's a few other talks that are in this general domain if you're more interested in kind of the box I'd especially box an enterprise space there was a talk you've already missed that one but you can watch it on the on the stream later if you want to know more about API todayI including other scenarios outside of the assistant and how it can be used for say Facebook or embedded in websites and apps there was another talk that you also missed on that and but that's also available online later we were the last one then of course ours ours and I saw some people taking pictures of the UX design resources we've actually put a good set of white papers up a whole checklist a bunch of resources because we really think this is the key to having really high quality user experiences wherever you're building your conversational interfaces so please take advantage of that oh and with that we've got time for a couple questions thank you [Applause] [Music] [Music] 