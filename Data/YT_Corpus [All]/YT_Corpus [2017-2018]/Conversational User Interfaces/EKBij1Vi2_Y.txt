 thanks very much 3 and thanks for having me come here I haven't been to Brighton before it's a beautiful town got to wander around a little bit last night so and I'm looking forward to the conference I wish I haven't been to here before and learning what you guys are doing and what your interests are I by the it so this sound okay do I need to sound stand close to the microphone okay I'll try and do that so so I'm gonna be talking about haptics I'm primarily a haptics person and I want to take perhaps a little different approach to it than a lot of what I've seen going on here although it's not completely divergent MyLab we call it the sensory perception and interaction lab I've been at UBC in Canada for 17 years and before that I came through a convoluted path that began with planning to be a veterinarian and then went to mechanical engineering and being a hardcore robotics designer controls designer and eventually got more interested in the people side of things and perception and eventually interaction design before I actually knew that that was a thing and so what we work on now is a combination that there's a theme of haptics to it especially in the robotics I work on robotics that that we touch but a lot of other haptic things as well but from early on I became quite interested in how people were perceiving the sensation not just in a perceptual sense but what what the meaning of it might be and how you could use that as a communication channel and and and how you would design for that I think that my roots in engineering really made me a designer at heart so what I'm going to cover today is that theme on design and how we design tactics for people to use and how we understand what people are how people are making sense of it so I feel that we're but a bit of a tipping point in haptics over the last few years I've seen it growing maybe in the last three or four and then very sharply recently where I started I got into this field I think I had to see I built one of the very very first half tick displays in the early 90s we didn't call it haptic soon we call it is that it feedback and it was not vibrotactile his forces and it really came out of the robotics community and since then it's been along that was a while ago I'm dating myself but we're at a point where actually we're seeing a lot of this in industry and not just inexpensive high end applications but in consumer technology and so what I really noticed more and more over the last five years especially is that designers and industry are struggling to actually use it as a design medium and that the tools do not exist to do it and there's a lot of actual challenges to designing with haptic feedback that's different than designing in other modalities many challenges that are similar but there's also challenges that are different and so one of the things that I've tried to do is to understand what those differences are and then over the years we've tried to see how you would support those particular challenges that are different so hacked ish ins need tools and we're at a point where tool design I feel is as important as a technology design because it's a primary obstacle to uptake of the technology to use to generate good tools you have to first look at what the people using those tools need to do and what their processes are and what particular kinds of challenges they they go through so I've been a researcher for quite a while and I say I'm a designer and my students are designers so what does a haptic design look like for me so I'm just gonna flash through some pictures and I'm sure that each one of you could show a similar bunch of pictures from your own work I'm just trying to make a point so I'm not going to go into it too much we do physical we do a lot of conceptual and interaction design you guys especially in this room are familiar with that okay we do a lot of prototyping low fidelity prototyping high fidelity prototyping we build stuff okay we do experiments and the experiments might be really elaborate and complicated and take a long time to do we have to get them peer-reviewed and subject to very high standards and these can be quite challenging experiments that take a long time to do and we analyze the data and we have lots of kinds of that we analyzed okay so we this is research right we call I call it design this is how I design it's pretty intensive stuff the point is that if you're actually working in industry all that stuff you don't have time to do right you can't you can't make it maybe the prototyping but aside from that you're not be able to do those kinds of experiments you can't do that kind of data you have to move a lot faster than that this is an obstacle if that's what it takes to actually progress in bringing the the haptics into your application it's not going to happen and then working with haptics brings other challenge as well so let's look a little bit at process both for us a research designer but also looking at professional designers and so we did that we actually went out and a few years ago it was actually quite hard to find professional haptics designers there weren't a lot of them most of them turned out to be in the automotive industry and a little bit in the surgical surgical support industry there's a lot more now there's challenges because a lot of them couldn't really talk about what they did it was proprietary but we could talk about process to some extent in some of the challenges and so we did a qualitative study this was with my former student Oliver Schneider who's now one of those mini German researchers that were seeing listed up there so there's lots of types of things that we do so we tried to actually look at the stages of design these stages are not new they've been out there and listed and other kinds of designs as well but we wanted to actually look at them through the filter of what you're doing when you're doing haptics so browsing first you look for examples you see what's out there you get inspiration okay you create at some point you do have to make something new and you might want to brainstorm you might want to do this broad lis quickly come up with lots of different designs kind of a sketching low fidelity prototyping stage and then at some point you're going to have to refine and you're going to have to share you're gonna have to get feedback from people on what you're doing you're going to want to show it to your boss to your client and eventually you are going to actually have two people try it out who might want to buy it so let's just zoom in on one of those phases so let's think about browsing what problem with browsing is that if it's haptics we don't have a common tactile design language so far less the problem that I'm sure you can already see forward to that it's you have to be physically present to actually feel a haptic sensation we can't even talk about we don't even know we don't we don't we don't know what the words mirror there how we talk about the particular sensations and if you're coming from an engineering standpoint we often have our our language is based on machine parameters because we're focused on actually building these things what you tell the motor to do in order to create a particular sensation so we have the designer who's thinking about just using example of vibrotactile which is a quite restricted channel compared to force feedback even so you're thinking of frequency duration amplitude waveform and yet we have the end user who's feeling it and the india might be using very different language to talk about or to perceive what what that person is feeling like urgent or bú musical sounds like a motorcycle or even just you know sounds brings to mind some piece of music that i've heard before so there's a huge disconnect between these two ways of thinking about it and so one of my former PhD students awesome also now a german research this one in stuttgart as hast dr. husky psyche i started to look at it from a visual information visualization perspective and saying what we really need is better ways of browsing haptic feedback wasn't trying with this work to attack the problem that you still have to be co-located but at least can we organize can we acknowledge that people might look at or perceive these haptic sensations through different cognitive frameworks the same set from different directions at different times so this is known as a faceted view in information visualization so she made a tool called vibe is that actually gave you access to a set from different perspectives and actually what those perspectives were and how they were associated with each other was the quite a lot of research a whole PhD as a matter of fact and so let's see if this works I don't think there's is there sound on this one maybe there's not so the idea is that you have different views that you would actually look to this is an online web tool that we could that you could interact with but the point is that if you select something on one side its reflected in the other so we have a physical view a sensory emotional view metaphor filters or usage examples and that you could click on these different things and highlight different different items in a corpus and you can always click on the single item and actually feel that particular sensation so the idea is that you take a single sensation embedded instead of you know 100 or so sensations and you can see where it shows up in different maps if you're looking at it in different ways and that can actually help you find other things but this was simply a browsing tool it had individual discrete sensations and what hasti went on to do in quite a bit of other work that I can't talk about now was to say what it what when you want to actually modify them maybe you have two things and you want to move between them or you want to take one and adjust it because it isn't quite what you want how do you actually take those starting points this examples you find in your browser and modify them to be closer to what you want and that was an effort for end user customization and we have a whole line of tools of design tools going in that direction so what might a detail of design tool look like so let's shift gears and go to this another set and I'll talk a little bit about sharing at this point because usually throughout this both in the early creative design and also in the detailed design you're probably going to need to do a lot of sharing there to get feedback or to show your colleagues or just talk about an idea and trying to understand it better so this is something that again back to Oliver Schneider and some other students in our lab worked on this and so Oliver created a tool called McCarron that some of you may have run into at different conferences we built it it's a keyframe editor that lets you do detail it design it I'll show you a very brief look at it but the point is we actually build as a platform to study designers so this was a tool that we took to industry designers and actually asked them to use it in different ways to design things and then we plotted and looked at the different kinds of things they were doing and and characterized their design process using it it's not a creative tool it's not an easy freeform sketching tool we got those two this one was really if you want to get precise control and do something very carefully and we also developed a tool called haptic we got very frustrated by this think of we couldn't ever crowdsource our studies even when we wanted very high volumes of feedback on them because it's it's it's difficult to crowdsource haptic feedback what we did was we actually made visual representations and auditory representations of haptic sensations and then tested to see if we get the same kind of aesthetic responses and meaningful responses in from both local and remote participants to those proxies as to the full-on sensation so that you could actually do some kinds of studies crowdsource and that was haptic that was at Kai last year so briefly McCarron just to give you an idea of what it looks like this is Oliver's wood select using the source of an example enables much more powerful tools to study this haptic design we built a by protecta editor and added example features we've logged user actions to capture and illustrate their design process finding users followed an archetypal process with examples facilitating design and helping users learn we also report several lessons for future design tools so it would take a little more time to explain all the things that that tool could do it's actually a very powerful tool but the key organizing principle of it was example use being able to source examples and then put them together to to modify and and borrow them and one of the observations that Oliver had made is that in many other design mediums you can actually like if you're redesigning a website or even an image you can always go into the source and see how someone else did that and gives you a great deal of power and being able to go and then make something different and make it be your own way you don't have to just deal with a superficial image and so this way of being able to open up the inside of something and expose that take take an example open it up and then change it was really one of the things that he was looking at and so that tool allows you to do that as well as a lot of other things so going back to this this view we've in in no way suggesting that I've been exhausted and and finished figuring out for example how to browse I'll just say that we have examined the problem and have some idea of what's hard about it and what's different about it for haptics and and made some tools that let us examine that problem a little in a little bit more depth and perhaps raise some challenges and say this is what you actually need to do we have found this this framework these four aspects of things that people do to be quite useful as an organizing principle so we've looked at browsing we looked at sharing we looked at detailed editing and studied them so I want to talk about creating now which is actually one of the hardest things I think to do in haptic medium because it's it's taking that first step in this in this field when you don't in this medium when you don't have tools can be really super challenging so a lot of our work we've done in vibrotactile because it's very accessible it's handy to do we can do a little bit of crowdsourcing because it's starting to be ubiquitous in certain platforms and it is actually a lot more expressive than you might think if it's well designed it's generally not well designed in what you see in your in your smart phone buzzers because people don't know how to design with it very well but and and one of our big discoveries there was that effect is actually a very useful way to look at if you look at designing even vibrotactile feedback from an effective filter because that's how people look at it if you can actually design in that facet rather than in engineering facet it gives you the power to do a lot more much more directly so that was great but vibrotactile does have some limits it is not my favorite medium it is not what I want to be my life work is buzzing cell phones so let's go more deeply into effective touch as we look at creating and the platform that that we have been able to go much more deeply into for this is actually human robot interaction in particular effective touch so the starting point was quite a while ago actually I had a student come to me who had this thing about cats that he had he had had a cat actually his former girlfriend had had a cat and he was quite attached to the cat and when the girlfriend left and the cat left I think he missed the cat more than the girlfriend and around that time he came and started working in my lab and he just said you know something about that cat eye it's like there's there's touch and animals and I think we should try and understand it more and at the time this was this was quite a while ago and it was it sounded like a pretty weird thing to me but you always have to have one of those weird things going on in your lab and little did I know how much it would end up taking over my own work so so there is this thing where people touch animals and something goes on and it's rather magical and mysterious and it's very hard to study we don't really know what it is and there's been a lot of studies in the medical disciplines that look at how important touching animals or are touching other people how much social touch is to our health and wellness particularly people who are vulnerable very young very old very ill studies that show that if you've had heart surgery and you trace people's outcomes that more important than how serious your condition was or what kind of medication you had or a number of other situation conditions what made the most difference for your recovery prospects was whether you had a at home or similar things with other animals and other conditions cancer recovery and so what so it's it's very high level like you have a cat what what are you doing with a cat it is so important is it that you have to care for the cat and you're responsible for it is that it looks at you in a wonderful way is that when you walk your dog that you're socializing with other people because that's also known to have happen or how much is it that you're touching it and it's touching you and that this is somehow activating something very primal in us that would be good to understand better it's very hard to study we can't control the animals it's a it's a difficult area to study so we set off to try and study it with a robot and built a robot that would actually start to break this down and look at it and see if we could see what was going on when people touch animals in a more controlled and since tway so it really started as a platform for studying emotional touch and this was a very simple model that Steve yohannan.the student of the cat came up with long ago and it almost seems so simple that it's hard to do I even bother writing it down okay so the idea here is that you have your person on this side robot on that side so the person does something touches perhaps but usually you touch with some kind of an emotional attend I don't touch a cat the same way I would touch this object necessarily I would touch it with so I'm going to pet it I'm going to tell it something I'm going to somehow have some social or emotional intent and touching it it's an emotionally evocative thing that I'm gonna interact with okay and then the the robot then has sensors on it and perceives the touch in some way and there's a step of perceiving which is just oh I was touched in a particular way you know what the sensors tell you and then there's interpreting what does that touch mean in terms of what the person meant so there's now some user modeling going on and then how is the robot going to react to that so am I going to respond in a pledge pleasing way or like to get all revved up and playful you know what kind of fuzzy robot creature am I and how am I going to respond to that and there's an aspect of personality or even agenda in there in terms of setting the personality okay and then somehow I have to render that I'm the robot I need to show my response in particular way and this of course depends partly on what I can do what my degrees of freedom are and what what I'm able to move and what I'm able to show but partly on again my personality and how I Suz as a robot to behave or how my program tells me to do that okay and then we're back to person and person is perceiving this perhaps through their hand or if they're hugging it their body in some way and somehow and then you do exactly the same thing on the human side the human perceives it interprets it and then decides how they're going to respond and you could substitute a real animal in here robot but there's interactive loop is going on now if we're looking at as a robot then you can quickly see we have some work to do we're gonna have to do some sensing we're gonna do some recognition we're gonna have to do some user modeling and vice-versa we're gonna have to design a robot that can move in interesting expressive ways and we're going to have to be able to run a program that can do all of this and so we Steve spent a lot of time working on these two kind of engineering challenges but also we have this idea that eventually if you close this real loop what's going on in the human can you actually change the human's emotional state and this would be very interesting if you could do that because then of course you're having something that looks a lot like what happens with animals but you're doing it with a robot and then you know that maybe there's something going on with real animals that is rather mechanical you know because you can replace it with a robot which is not a real thing so if you put these things back together is there a synergy and that was really these were really the defining research questions that we've had initially for this project that we set on so there was about five years of work which roughly coincides with Steve Yohanan PhD thesis this is the robot that he built it was not the first version he made a few passes at it before we got even this far and you can see it was using technology that we could put together around 2011 and so forth and the plus abilities for surface touch sensing were not at all what we have today we had to make do with I say that this is our turtle with a fur coat it's fiberglass with force sensitive resistors on it which worked better than you might think it looks pretty awful it doesn't feel right and there's all these gaps in between the sensors where you can't sense anything but we actually could get some touch sensing off of this we needed pressure we knew right away that we needed pressure not just surface touching and it could move in some particular way so that we did a great deal of pilot work to figure out you know using various kinds of puppets to try and understand what people were considering as emotionally expressive motions and so you come up with something that we figured that the breathing seemed to be fairly important but it could do things with its ears they could purr we tried heat for well the problem with heat is this thing is full of motors and it we are just desperately trying to dump heat out of it because you have all these big strong motors inside a fur coat and and we really didn't want to add more heat to that system so it was burning out motors pretty badly as it was already so the it's warm trust me so so so we use this platform and we did a few things with it so called this the creature 1.0 so there was three primary questions that align quite well with that model that I showed you so you start by saying how do people interpret the creatures emotion rendering in other words when it moves what do people think it's expressing how do people display particular motions to it so that we were hoping maybe that if people made very characteristic ways of interacting to show emotions you could figure out what their emotional state is by looking at their gestures if you could actually sense that then you would have that would like be really nice and clean okay and then do people's emotions move when the loop is closed and I'll just give you the short answer that yes they do I'm not going to tell you about this whole long study we did that was actually another student past past Steve who was able to document that quite strongly and not even with kids we thought that this would be a platform more effectively on kids but it works on adults very very directly as well so there's a bunch of papers here where we talk about these things and I'm just going to give some high-level overview of the kinds of things that came out of it because this is again work we did a while ago first of all just to get me situated who is familiar with a two-dimensional model of emotion or effects so most of you okay so I don't have to go and you see a lot of arousal and valence in here but you guys know about that so here's here's first of all something we called the touch tinted dictionary which Steve generated by having a whole lot of people talk about and demonstrate how they would express particular emotions to this fuzzy robot okay and what we came up with is a lot of a lot it says this is qualitative study where it came up with a lot of particular emotions and definitions that how of how people would do it and we confirmed these by going back to people again and asking people to confirm them and then demonstrate them again so we felt a pretty strong validated thing that probably applies quite well to animals as well as a fuzzy fuzzy robot so so well I'll come back and refer to that touch dictionary again one thing that I just won't go into too much detail on is that we also did some other studies where we looked at the frequency with which people use these gestures to display particular emotions hoping that we would see some kind of rough correspondence so that we could use gesture recognition to infer emotional state and I'll tell you that that works occasionally there's a few things that people use very correct characteristically indefinitely for example distressed we can say show that you're distressed people usually do the same thing which is they'll pick the robot up and hug it okay but if you ask them to show almost any other emotion they will do a whole bunch of different things and so this idea of being able to say oh if I see a particular kind of stroke or something that that means a particular emotion there's some one-to-one mapping was clearly not gonna happen but what we did see was that when people were expressing different emotions they would make particular gestures but in different ways and we could see this in video studies because remember our touch sensors were really not up to snuff right now we couldn't detect what people were doing with a resolution that we really wanted but what we could do is we watched them and we could see and we had a very strong suspicion that there was there was actually a lot of differences in the way people would modify the touch as they were making depending on on their emotional state and so if we had better touch sensors we thought we could actually do better now looking at which gesture they were making but how they were making the gesture so so we came back to this thing later my computer is frantically telling me to reconnect to wireless network here so hopefully that's not interfering so anyway and life moved on during this period a lot of people came through our lab and would see this robot and as inadequate as we felt it was this this funny platform that we had intentionally I didn't really talk much about the design of the robot itself but we intentionally designed it to not look real it's not representation of any particular animal it's very just a fuzzy mammal that may be vaguely brings to mind a few different kinds of animals but no one in particular but mammals so people have some expectations for how it should behave it doesn't have a face it doesn't have eyes all those things were very intentional I won't go into those decisions right now but it was it was designed to to be a platform to study emotion in the context of an effect grid okay like can we can we hit can we be recognizable on these four places those people came through seeing the robot and we're intrigued by it and said oh like I have an autistic kid and I think that this would really be great for their therapy or someone who studies chronic pain and felt list would have potential there there was many many people coming through who were saying this has therapeutic potential and you really need to look at that particular application so I just listed go and record it was not my idea but so when we looked at that concept we really needed interaction design we needed to not just have this this object which could render in one particular static emotional thing it had to have a story it had to have a narrative in to the extent we started working with kids the kids always needed a story about the robot they needed to know what its origin story was what it meant when you turn the power off was it dead or was asleep we got very very careful about turning power off this version had a tether it was its tail actually had data and power so it had it had to go into the wall and that's generated a lot of questions from the kids kids knew it was a robot that was there was not confusion about that but it was they needed to know more about it and they were really ready to put their own story on it but they even either we gave a story or they had to come up with a story there had to be a story one way or another so this idea of narrative frame is something that we've studied quite a bit more recently trying to get deeper into that but people need to have a story about the robot in order to interpret what it is and what I'm doing with it so when we decided to start going into these therapeutic applications we needed to design a robot that could support a larger interaction and that could move dynamically I don't mean move around physically but could actually progress the the story and not just do one thing and then switch to another thing and so we needed a different robot we needed a robot for example that could do an orienting response it could be and now now we're more concerned about visual gesture as well as what you feel because when the person first encountered the robot they won't be holding it a scenario that was very helpful for us to work this out was one that we were working in at the time which was I can tell stories about studies until I'm blue in the face but one thing I will tell you we we found that the robot had a very strong effect in calming people down we first tried to study this in kids but we found an experimental problem right away which was that to study climbing kids down you have to start with him being upset and that meant we had to get them upset and this turned out to be prophetically problematic and and so we tried a lot of different things like there is a school near us that has kids with learning disorders and as part of their daily routine they actually put the kids in somewhat stressful situations and we tried to piggyback on that but all we succeeded in doing was putting them into a flow state rather than calming them down it wasn't it was just and it was actually very hard to experiment with the kids because they were they were too intrigued by the robot and wouldn't settle down and just let it be in the background so we ended up working with adults a little bit more but we continue to look for contexts where we could study upset kids and it came to me without any effort when an anesthesiologist asked if they could actually put these robots into the waiting room of kids who are about to go under major surgery we had kids that were already stressed out and needed to be calm when they went into the operating room so that the anesthesiologist didn't have to basically struggle with them because that generates really bad outcomes all the way down so and they wasn't something that was non-pharmaceutical and so they thought a robot could be great and so we started actually trying to figure out how that would work in a preoperative setting where you have kids and a kid is coming in the family is stressed out as well and and you want to actually design an encounter with the robot where they come in and they see the robot we need a story okay what's going on with the robot the robot is sitting on the table or it's sitting in a basket the robot is scared you need to pick it up and you need to take the pill which is you need to hold the robot and get the robot close to you so that we can deliver what it does to you so so we had to construct a story and support that so what do you need an orienting response you need to say that see that the robot is paying attention to you you need to be able the robot needs to show that it's frightened and scared and huddle versus I'm great I'm open and good for business like I'm feeling playful so you need to be able to do that yeah lean in you need to be able to lean its head against you and cuddle still need to do all the effect grid stuff the arousal and valence and it needs to be clinically robust than that one was not easy I thinking about kids throwing up all over it like we need to be able to handle that so I thought uh-oh this is not this is not another five year computer science student trying to build a robot we need to go to a professional design firm for this so we did and this is what resulted and I can say that it did not work out and the reason was that the design firm felt that our piddly $200,000 was not enough to do the kind of design that I wanted which would be iterative and doing a lot of wide-ranging brainstorming about how you could actually implement these different functionalities they wanted engineering specifications and then to go and build them and it's kind of hard to right off the bat give an engineering specification for lean in and feel cuddly you know it's like it takes a little while to translate that or this whole thing about shrinking away versus feeling being relaxed you know this is not something that you get right on the first pass so we figured out some useful things in terms of how to do something modular 3d printing had happened so we could make a platform that we could keep modifying and changing the outside but there was many things that just didn't work and I have to say that this was a very expensive lesson and we haven't used this robot for a single study yet but we did learn that we needed better physical editing systems and so its student Paul Buchi for his master's degree actually starting for his undergrad he's now my PhD student started developing and he actually came from a visual arts background and started took them very seriously this haptic sketching thing and he developed these design systems where we decided to focus on one degree of freedom breathing initially to say let's really understand breathing and let's make a whole bunch of very simple quick to iterate robots in just just desiring the hell out of that so we went the opposite direction of the very expensive one-off robot and develop these modified modifiable template systems we came up eventually out of after a lot of work came up with two that we then did a lot more work on just to give you an idea of what that looked like this was presented at Kai just less earlier this year I think that there's no sound on this one so so this is just showing design process and I think from the look at the demos and things that you guys are not going to be too shocked by this particular approach to design I think that the engineers who built our cuddle bit our cuddle bought we're not thinking along these lines so we start very simple retrying to say well what what how could you make breathing work in a way and we were very rested in the robust thing we wanted to be able to throw it at the wall or step on it and have it not break it should never burn out we want kids to be able to look at this and not feel like it's super delicate we looked at two different eventually two different families we were interested in contrasting whether if you have something kind of ribbing hard how people would react to that compared to something soft and fluffy so we came up with two very different design ethics we called them flexi bit and ribbet as the two different approaches both of them have a very similar basic idea which is there's internal client compliance in the structure and then you have a motor inside that pulls on it that way you can squash it and it doesn't break the motor because there's nothing to back drive it's a tendon ribbon and so and then you have to cover this with fur for it to be very nice to touch but then you can get this remarkably expressive motion just from this breathing thing where you can actually control member it's not just up and down but you can change the rate the the roughness of it the symmetry and all of this is quite controllable so we went on from there and say how expressive were these robots actually and so this is a study one we did we've done lots of studies with these things but I'll just try and try and run through one so this this is showing 14 subjects who went through and rated the the robots how well they achieve particular emotional things so the top row so these are not laid out on an effect grid but each one individually is an effector grid so we have valence across the top and arousal on the bottom and the circle is kind of just think of the centroid of where we're subjects found it to be and the big green circle is is standard deviation kind of stuff and so what we're basically saying is if you're right in the middle of what you're saying is that this would be neutral neutral neutral valence neutral about select not super low of either one so what we're trying to do you look at this row it's sluggish droopy bored drowsy what we're trying to achieve here is low arousal and valence okay you see that those are low-energy but not particularly okay so we would expect to see something on this side valence wise and in the bottom arousal wise so you want to see something in this quadrant not too good okay we didn't do that one too well but if you go on you see here we are trying to achieve calm relaxed arena DS which this would again be low arousal but positive valence and you can see that that worked a lot better we got our quadrant we got the low arousal and positive valence pretty well so that so okay we did that better with these cuddle bits and then we tried to do attentive proud enthusiastic determined so there you have high arousal of positive valence you say well it's a little bit mixed here we generally that this is the one that's closest to being in the right quadrant okay and then finally we have guilty hostile nervous and scared once again we got arousal pretty well but valence not so much okay so there's there's two that we actually nailed pretty well for that but if you look a little bit closer what I've highlighted here is whether we got half of it right okay so in this case we can see that we actually got arousal correct that so when I've highlighted it the users actually perceived arousal the way we intended them to by the way the way we generated these behaviors was was my ass and working with other subjects to generate them we went back to new subjects and saw how they perceived them so here we got for example arousal was always correct always correct always correct and almost always correct so basically you say we did well at arousal we could generally get arousal in either direction what we could not do was valence nearly as well this is not we're not the only ones who have this problem generally valence is much harder to predict you can tell if a dog is wagging its tail is a classic example is the dog wagging its tail because it's furious and excited or is happy to see you and excited you might actually even if you're a dog person you know it might be able to tell you might have to know that specific dog to be able to interpret that particular thing and I will point out that the kettle bits do not have things so it's actually kind of hard to access that that particular dimension with these robots so that could tell you Oh what do we have to do in order to be able to go there but I want to move on from there that gave us feedback about what we needed as we continue to design this and we're going on to design other degrees of freedom for example that hunch thing and try and integrate it with breathing but even when the body is adequate it's very difficult to source the movement and so we've done a lot on that and now we're getting back into that create thing so so we've we've done create in terms of being able to generate lots of body templates it was hard to do that that was not a quick prototyping but now we have a system within which you can rapidly prototype so it's become a prototyping design tool that you can modify things now we have to source the behavior what should they do and we actually observed that when people we asked people to talk about you know how should it show something people would be at a loss for words and they would start vocalizing nonverbal vocalizations they would start making sounds like and and and and making things like that so we finally said well why don't we just like take the sounds they're making and see if we can do that a little bit more directly and so this actually came out with a much earlier idea that Oliver had had this is a sketch from way earlier on where he thought wouldn't it be cool if you could actually he was thinking about examples if you could just source examples from different places like holding up your microphone so just some sound that you thought was kind of interesting and then source another sound and then be able to move a slider between them to kind of get the qualities that you wanted that was an early inspiration that we have since built that for a number of things but that sourcing part we thought this is Moodle which we presented at this earlier this year and I understand that someone has in the audience has already seen it and then a Mac mix tool it was that macaroni mix that I showed you earlier we adapted that to be able to do this mixing thing with examples I won't be talking about that but here is Moodle local doodling for effective robot interaction when describing something abstract like motion many people may find themselves at a loss for words I may opt to use non word vocalizations to express themselves we will translate some properties of your voice into motion on a one degree of freedom robot yeah yeah both the text features like pitch and amplitude and interprets them according to a set of user definable parameters with Moodle users can quickly naturally sketch effective behaviors in a unique conversational design space so I had the pleasure of being the one to demonstrate vidole at a haptics conference about a year ago and I know that I can change I can sort everyone in the world into categories people who are willing to bark at a robot in a public setting and those who are not willing to do such a thing so how expressive was beetle and so that thing about self-consciousness we actually did a study we decided he's professional voice actors because that inhibition thing was not everyone was willing to do this and so this is some data that we actually found where people took three voice actors over three sessions and asked them to sketch on an effect grid how expressive they thought they could how explicitly they thought they could design with this platform and so this was session these are participants you see in the first session they kind of weren't listen this is what this is where they could achieve something and then progressively they would draw more and more now in between these sessions they asked for improvements they asked for different BOTS different cuddle bits say so we would go back and would make do new robots and they would ask for different things on the Moodle tool different so that we could actually I want different things I want more reverb I want to be able to control these different things on the on the tool remember it's not directly going from your voice to the motion we actually run all these filters on your voice to actually pull out different things and then have a control panel where we can adjust the filters so so people would ask for that so the tool is actually fairly sophisticated in that way and so you can see that by the end people were able to do a lot more and we have to chalk this up partly to experience in working with a medium but also to improvements in the tool as we went along so anyway I'm getting close to the end here but I've been talking about the robot display but remember that it was very important to be able to sense the effect of input and this is actually some really cool work that we've done a lot of in our lab that I just wanted to talk on especially given this conference sensing and recognising effective touch so this is our very first not our very first but one of our most fun early examples satisfying ones [Music] so so this is a work of on a flag that we've done as a live demo a haptics conference in 2012 and what actually consists is of it's purely a fur sensor so we actually had threads interlaced into the fur and the way it works is that when you squiggle the conductive threads together and they touch each other there's a stochastic signal it changes in ways that you can do really cool things with machine learning on it and actually be able to accurately classify a lot of those touch gestures from Stevie Oh hands touch dictionary from way back so is a kind of a really surprisingly effective thing now the devil was in the details it actually was not that easy to make these things you have to get lots of lots of parameters adjusted quite right and it's a little bit washable but we weren't so sure about the kids throwing up on it okay so so we ended up also looking at other sensors that were more and remember now this is moving past that this was now still I think this was done in 2011 and 2012 and we were still in the shape of having to invent our own touch sensors there's not nearly as much work as there is now and we just I just wanted something I could saran wrap an object with and have relatively low-resolution touch sensing but completely flexible like fabric I wanted to tailor something and I didn't want to have it be on a hard surface it should be a squishy thing it should feel right no more turtles and and we just needed to find a way to do that and I wasn't willing to put up with fiberglass in FSR anymore what's going on there ok so we did a more extensive study with a sensor that I'll show a better picture of in a moment where we were able to classify these 9 gestures pretty accurately and this is how accurate they were so essentially a hundred percent classification of those gestures I showed you where the most frequently used gestures in our very large touch dictionary so if you could do those you're doing pretty well again this is just telling how people are touching something and what was a surprise that we completely did not expect was it we could tell apart person like you could tell what's gesture they could make you could also tell which person was making that gesture and so I said wow so you have a little robot and it's in the family or in the classroom you could tell if it's Johnny or Jane who's playing with it right now and it could actually respond in a personalized way to them that's very interesting so it tells us that people are touching idiosyncratically but also differently but but also these gestures are very easy to determine even with a very very simple low-cost sensor so this is the the sensor this is not the first sensor this is a simple one it's made out of electrically striped fabric with a separating layer so basically it makes these pixels or tacks holes that are about a centimeter in diameter the current version we're using so that you it's about the size of your finger and this is all the resolution we need for the kinds of gestures we're making if you have more than that it's just extra computational load and more wires you have to deal with and so you could take this and you can tailor it and you can put it on our little fuzzy cuddle bits or under the fur and it works through the fur and it works without it doesn't have to be against a rigid surface so we could actually do pretty good touch sensing with pressure on this particular platform and so what did we do with that so and we've done we've done a lot and I'm not going to have time to talk about it all but we are now at the point of actually being able to infer emotion as well as what gesture you're making from the way people touch to some degree and there's a study here that I will just briefly touch on that we recently finished where we actually comparing what we could do with just touch you with biometric input which is currently probably the gold standard for effective sensing and we're now moving on to using EEG as well as a gold standard comparison set and we're finding that touch is not as good as biometrics but it's not bad either you can actually tell a lot but the most important output of this study was that people that this whole idea of the effect grid we're starting to kind of say it's not very useful for us anymore things emotions are static nose emotion our dynamic you're having multiple emotions at once and it's too high standard to say we're going to label your emotion right now instead we're gonna look at emotions change over time and it's a dialogue and we're going to make guesses and then respond and see if that's consistent and narrow down our understanding of what emotion you are in right now and this is really a more realistic way and more naturalistic way of how it works because this is how people determine each other's of emotions if I walk into the kitchen in the morning and my husband makes a joke how I'm going to respond to that joke might depend on you know what mood I think he's in is is he making a joke or is he serious when he said that thing and I might have to probe and kind of follow up and check and see how he responds to my follow-up and so that's that's that's the way we work as social animals and still we're looking more at that but there's another problem here which is all the studies that I've shown you so far or most of them we had to ask people to pretend to simulate emotions and we really wanted a situation where accessing genuine emotions and we knew what they were so we actually divided because obviously we need more certainty that we were after the right thing so we developed a whole new experimental protocol now working with emotion psychologists where we act we ask people to come in and think about strong emotion experiences that they had had so memories and then to actually tell tell these stories to the robot okay in a private room with a camera on their face but no sound so that they could they could speak or not speak but but they would not be recorded audio recorded for privacy sake so this is speaking of twisted setups I I would never be able saying this as my students Laura and Paul did this you can see the evil grin right there so so it looked a little bit contorted a comfortable chair but the person is holding this we're also looking at days as another biometric another another modality they had the biometric sensors on them and we're really looking at how this compared with their biometrics and so each participant recalled a strong emotional experience and they could only go through this twice it was pretty involved they had to tell a story and we gave them two conditions and it worked a little too well there were tears we we actually stopped the study because we were afraid that we needed we shouldn't be doing this without a psychologist president it was seemed a little bit off to put someone to tears into an experiment and then say oh thank you very much and here's your ten dollars and go away so but because some great data and and so we asked the individuals to report the genuine of their movement on the affected so basically where did they start they started from a relatively neutral point and we saw that for each of these they had a lot of movement from neutral to the outer edges of the corner so that this is just confirmation that yes they self-reported that they experienced a change in emotion that was genuine and quite substantial and we visually confirmed this it was it was very evident and so I I have a data view but it's not very easy to explain so I won't I won't show it basically what we saw is that biometrics got this 100% touch got it pretty well gaze not so much but like I said earlier the really interesting part was that we're now looking at because people were telling stories there was this journey that their touches went so we have this rich data set where people are touching it and things are changing as we go and we're now mining that data set in collecting more data set to really look at this dynamic model of emotional touch and upgrading our sensors and all of that so we can actually do do to see more of what we're actually looking at a lot of this is computational and just how fast we have to be sampling and how you deal with the huge volumes of data that you're coming up with so and and then moving on to these interactive scenarios who are actually deliberately trying to modify people's emotional state so I'll stop there we're going on and continuing in these therapeutic directions we've deconstructed the design process to understand what's different about design requirements for tools we've made a few tools we've posted all our tools online they're all available and mechanisms to evaluate haptic does closer to scale and trying to figure out how to do that but there's lots more to do I just wanted to give a really quick plug as three mentioned that not my personal lab but a large initiative across UBC that I'm leading called the designing for people research initiative we are recruiting for graduate students for a big graduate training program that we're launching we just launched this fall so you apply now for the next fall so if you're interested in something like that it's a really cool program and I can tell you all about it and this is what beaches look like in BC so I just have to say that okay so and I will stop there [Applause] hi that was amazing thank you so much I'm really particular interested in what you said about how the cuddle bot doesn't have fangs and so if you think about like the real cat but cats are haughty and they kind of let go off halfway through and they leave you but people form very strong emotional attachments to them and I wonder what your thoughts are on how much it's necessary to bill if you're building like a robotic approximation of a living thing like that how much is necessary to build in the fans or the rejection or the kind of that the more negative emotions as well to make it a really kind of rich emotion integration that people have with it but actually sustainable incredibly cute very much my belief is that the it's going to going after the goal of making something that's as enticing as a dog or a cat and that richness is people can work on that this can take a long time I love animals when I'm going for is more something that's in the background but it has to have some of those qualities if you take a kid with them what we are trying to do is add just enough so that people put their own story on it and bring most of that interest to themselves it's not going to be exciting but it needs to be something so there needs to be some expressive palate and I think of some of the things put a lot of work into figuring out so what what does it need to do and I think that I'm feeling good and feeling bad and also acceptance rejection go away I don't like it versus oh yeah no you need you need to be able to do that much and so I think that's important the hostility and the anger but there has to be something our robots can't run away and that's really importantly different from a cat or a dog because its displeasure by [Music] 