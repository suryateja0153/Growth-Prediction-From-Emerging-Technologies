 welcome today on AI adventures we're joined in the studio by Justin Zhou a Google research engineer hey Justin all right thanks for joining in the studio today yeah it's great to be here we're gonna be talking today about natural language interfaces and how computers and humans can talk to each other in ways that are natural and not awkward yep sounds good awesome so I want to start by talking a little bit about your team's area of research and kind of the general natural language processing field and then we'll delve into your area of research and see where our conversation takes us yeah that sounds great so broadly speaking the area of my research is natural language processing or NLP and what that is NLP is all about trying to understand how humans communicate with each other and how to get a computer to kind of replicate that behavior so that we can interact with computers in a more natural manner wow you guys really picked a small field to target yeah I know if he sounds super broad yeah it's like everything yeah it's pretty broad um so in fact like I have some slides that we can pull yeah that'd be great yeah yeah so first I think it's important to talk about the conversational user interface and for something like the Google assistant there's two big domains of NLP problems that come into play on one side you have the problem of understanding which is what did the users say what was the user's intent and on the other side you have the problem of generation which was what should we say to the user and how do we respond in a way that's intelligent and conversational right so I work on the generation side and the ultimate goal of natural language generation is to teach computers to turn some kind of structured data into natural language which we can use to respond to the user in a conversation wonderful and this is definitely something that I feel like conventionally NLP has really been broadly thought about as a field where it's all about processing the words and understanding what text means but you are working on the generation side which in a lot of ways often gets overlooked and so it's really great that you're able to kind of tell us more about this side of things yeah mmm that's what I'm here for so how do you then teach a computer to generate natural language rather than just understand it right so for now let's set aside the structured data you know part of natural language generation and we can focus on the natural part of the natural language generation so what makes a conversation like the one we're having no human speaking of the one we're having it's a little meta that we're having a conversation about what makes something conversational so that's a common remark on our team yeah we have to not be too robotic in our conversation yeah so I think this breaks down into two kinds of requirements first of all the content of what we have to say it has to make sense in the context of the conversation so is what I'm saying an appropriate response to what you're saying does or is it out of the blue hey what are you having for dinner so that's kind of out of the loop that's kind of out of the blue yeah definitely so yeah exactly and then I have to think about if what I'm gonna say is actually gonna answer your question so if you were gonna ask me where we want to go for dinner it would be weird to just to suggest like a coffee shop or a clothing store right yeah yeah unless you really wanted to get some coffee stains on your clothes for dinner yeah I guess the second requirement is that you accept to use the language correctly so this is like you know how's my grandma do my verbs agree or you know if I'm using a pronoun is it ambiguous that makes sense so it's basically what do you say and then how do you say it exactly okay yeah and you also mentioned earlier there's this structured data that we kind of put aside where does that come into play that's a great question so structured data primarily helps us figure out the first requirement which is what we want to say for example if let's say a user asked us about the weather next week in Santa Clara in Google search results we see a box filled with all this information about the weather for the next week okay and some are within this data hopefully answers the users question and we just have to figure out how to all this data into a response to the user that's kind of the problem that we're focusing on in natural language generation and that's because we're talking about a situation where we're going to say our answers not just show them the box to look at that's correct it's like an audio interface in that case I guess I can imagine a sort of naive solution for this sort of problem we already have the data right yeah so but I don't know if it would be sufficient well you know that's depends by all means go for it right so let's say we make some kind of a template right and we can say you know on blank day it'll be blank temperature and then some blank weather condition like on Tuesday it'll be 72 degrees and partly cloudy and then you could build a full forecast by just iterating through all the days of the week like that so I will say that that is a very straightforward approach and some assistants do use that implementation however in practice it's a lot less conversational than you might think so how about you try asking me what's the weather like this week and then I'll use your algorithm to generate a response all right sounds good we'll call this the Adjustment Assistance set perfect okay Justin what's the weather like next week hi you Fang Sunday it'll be 66 degrees and partly cloudy Monday it'll be 63 degrees and cloudy Tuesday it will be 66 degrees and partly cloudy Wednesday it'll be 68 degrees in cloudy oh boy okay yeah that's too long and just too robotic yeah let's let's call it let's call it that yeah you've been saying it for me you felt a little strange yeah so clearly generating natural language from structured data is non-trivial how would you actually go about using a computer system to answer the users question then well first you know I would want to think about how it answered as a human so you know as a human I would hope that I'd be a little more contextually aware and I would realize that there's actually a lot of repetitive information in the data so I'd probably try to summarize it something like it'll be cloudy until Thursday with showers the rest of the week temperatures range from the high 40s to the mid 60s hey you might want to consider a career as a weather forecaster if you know the research thing doesn't work out yeah maybe all right so we've done a little bit of an overview of natural language generation about what makes conversation natural and we even gave kind of a admittedly silly example of leveraging this structured data to select content for a natural language response yeah and we've also included some links with more info in the video that's right now we have all right so then getting back to the topic at hand how how does machine learning then get involved well that's the ultimate question that our team is trying to answer without machine learning everything that we've talked about so far from parsing the data to figure out what to say to actually figure out how to say it it had you have to do this with writing lots of rules and rules are great they're very stable they're very predictable but they're usually very specific and they require a lot of engineering and because of that it's not really scalable to new inputs and outputs for example if we wanted to talk about finance instead of weather or if we wanted to support an entire new language altogether sure it would require writing a whole new set of rules yeah and it sounds like that would be way harder to maintain as well keeping all those rules lined up as things change and it would also be hard to replicate that creativity and spontaneity that comes with human conversation right so that's exactly one of the motivations of our research our hope is that by giving the model examples of data and the language it needs to generate we can let the model form its own rules about what to do and not only does this save us from having to write these rules ourselves by hand but it also gives the computer more free rein to be creative in its own way so showing many examples to answer questions you might say so that you can write fewer rules I mean that's kind of the crux of machine learning as a whole that's wonderful exactly yeah and and so what kind of machine learning architectures then are you guys exploring to try to protect this problem well so far we've seen really promising results with recurrent neural networks but that's just one kind of neural architecture that we're exploring okay recurrent neural networks so on our previous episode we looked at deep neural networks on the show and that had neurons connected in layers resulting in something kind of lattice structure right and for our viewers can you explain what it means to have a recurrent neural network yeah so you can kind of think of a recurrent neural network as a deep neural network but just wrapped in a for loop and the network is recurrent because the outputs of the network feed back into itself and instead of this kind of one-shot you know input/output the model can kind of make decisions over several time steps okay awesome that's a really great way that I kind of conceptualize it really love that and we've also included some links about recurrent neural networks down below and if you have more questions about this kind of network structure feel free to leave them below in the comments and we'll try to get to them for now we'll talk about why recurrent neural networks will be useful for doing natural language generation right so it's important to keep in mind that language just in general is extremely sequential sure yeah for example the cat sat on the mat is a very different sentence from cats at than that on yeah order matters definitely so are nuns are especially good at remembering what it kind of saw earlier because it enforces a sequential policy over the data the inputs are decided in a very ordered manner and instead of in these kind of large conglomerates okay so I guess it's both amazing and not entirely surprising that recurrent neural Nets would be useful for natural language problems it sounds like so as humans we rely a lot on what we previously said to figure out what we'll say next exactly well let's talk a bit more than on how you're using these recurrent neural nets to generate this language so one kind of fun variation when it comes to recurrent neural nets is that since the output is generated one step at a time you can kind of choose the granularity of your output so some models can choose coarser outputs like entire word phrases just words in general and then this goes all the way down to models that outputs like bytes a single bytes at a time by the time and for us the we've been using outputs at the character level okay so be like spelling out the words really right okay and this kind of model is called ik is a character based RNN and you can find out more information in the links below so when we first talked about having you on the show you showed me this interesting graph here right I would love to understand a little better what is it showing us exactly so this is a small visualization of our recent research each row here represents different pieces of our structured data the shading of the squares indicates how much the model actually cares about that piece of structured data and lastly each column represents a single step in our model so as we travel across the columns you can see how the model has learned to pay attention to the structure data at different time steps okay so we're kind of traveling left to right character by character for each column and so the lit portions the lighter parts are parts that the model is paying attention to right exactly okay and then on this model over here for example it means the model is paying attention to this bit to decide what character it out but it's not saying that that's the character we'll say it'll just that's just the data it's looking at right exactly so it's gonna look at that particular piece of data to try to think about what - what character town exactly and then one really cool result is this diagonal line in headed oh about that it's kind of formulaic and it almost looks like you guys added that in afterwards - yeah it'd make it make for something interesting it's like hard coded yeah so those particular pieces of data are basically the characters for a specific location and what that diagonal line is showing us is that when the model has reached the part of the sentence where it wants to spell out the specific location is learn to read that from the data character by character wow that is that is awesome and no one taught the model to do that they were just able to learn how to do that just by looking at examples exactly that's the magic ncredible that's that's super outstanding yeah so um the diagonal line is pretty cool but if you dive into our data there's actually a lot of other intriguing ways that the model kind of learns by itself how to reference the data to decide what character to output so you know that said there's still a ton to explore but I'm super excited to see what you know where we come up with in the future and how far we can push our research this looks super cool Justin and I am really excited to hear about what your team comes up with next maybe maybe you'll write a research paper using one of these networks interview yeah that sounds pretty fun yeah Justin I want to thank you so much for coming into the studio today and teaching our viewers about natural language generation looking forward to catching up again in a minute I'm gonna wrap up here yeah okay sounds good it was my pleasure all right well I hope you enjoyed this episode of AI adventures I certainly did in our conversation we talked about using machine learning for natural language generation and its role in conversational user interfaces I had a blast chatting with Justin and if you like this format please let us know in the comments below and for more information and details about everything that we talked about we've included tons of links in the description and be sure to subscribe to the channel to catch future episodes and maybe some more interviews as they come out 