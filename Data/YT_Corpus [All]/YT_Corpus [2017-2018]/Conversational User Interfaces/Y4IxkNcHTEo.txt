 Hey, everyone, my name is  Andrew Gasparovic. I am an engineer at Google research  working on machine intelligence . And today, I'm excited to  give you an introduction in machine learning using  TensorFlow. I'll show you some examples of how we've been  using it at Google. And I'll share with you a few recent and  up coming developments and then talk about how you can get  started solving real problems with machine learning. So  first, let's talk about exactly what it is that TensorFlow does  and why you might want to use it. TensorFlow lets you get  straight to work solving all kinds of machine learning tasks.  The goal is that in general, no matter what your problem  looks like, TensorFlow should be able to support it at some  level of the API. And in general, it's designed to be  fast, so it's optimized for the hardware and the platforms that  you actually want to use it with. One of the things that I  think really makes it unique in terms of machine learning  frameworks is that you can actually build  a model in let's say five or ten lines of code and then take  that model and scale it all the way up to production. So you  can train that model on the cluster of tens or even  hundreds of machines, and then take that model and serve it  using super low latentcy predictions. So let's talk about what it is when I say model and how  machine learning relates to that . Here's a simple problem. Predict whether an image  contains a cat or a dog. This is something that would be  difficult or even impossible to do with traditional programming  because how do you make a set of rules of what is a cat  versus what is a dog? And then on top of that capture all the  variations like breeds, poses, the brightness and scaling of  the image, all of that kind of thing. So what we can do  instead is make a neuronetwork neuronetwork, which is an  extremely simplified version of how neurons in the brain work.   Each one of the dots in this image is a neuron, and they're  all connected together layer by layer from the input of what we  see to the output of what we understand. And then what we do  is we go through lots and lots of examples of cat and dog  images. They're all labeled with the correct category, and  we try to make a prediction. Initially, all of those neurons  are just randomly initializeed, so it's a complete guess.  What we do is we calculate how  far we are from the guess to  the correct answer, the error, and then we use that error to  adjust the strength of the connections between the neurons.  And basically, we want to slowly move towards the right  answer. After we repeat that, you know,  a million or so times, let's say say, then what you end up  with is a nice cat and dog prediction model. But what you  actually want to do is, you know, build a cat and dog  prediction website; right? A user gives you a photo, and you  have to tell them whether there's a cat or a dog in it.  And the connection strengths that you learn during training  are what allows your model now to generalize. So if you give  it this photo, even though it has never seen it before, and  there's no label attached to it, it can get a dog prediction  out, based on what the model learned via those weights  attached to the neuron about, you know know, the nature of  cats versus dogs. At least in terms of the images that it is  seeing. But how much it can actually  learn is a function of the model, size, and complexity.  And we just didn't have computer power and the tools to  experiment with really big and complicated models, until very  recently. This picture is basically what neural networks  used to look like maybe five or ten years ago. And at that  point, they had a small number of neurons, they were just  fully connected between the layers, if there were that many  layers, and then the end result is they weren't super powerful . In fact, for a problem like computer vision, they were  almost written off compared to a specializeed hand-tuned model  that was built by experts for that exact task. And you can compare that to a neural network model that we  use today for image classification, this is called  inception, and the idea is that you feed in an image, and you  get a prediction of what's in the image, among thousands of  categories. I think it's, like, 17,000 potential classes. And with a framework like TensorFlow, you can train a  model like this that has, you know, tons of layers and is  much more complicated than the early networks. That's what we  mean when we say deep learning learning. The deep in this case  refers to a deeper arrangement of layers and the more  complicated connections that comes with that. The end result is that you have millions or even billions of  neurons in your model. And that's what allows a deep  neural network to get results that can actually vastly out perform the earlier hand built, hand tuned hand- hand-tuned  models. But the specific reason why  TensorFlow is so efficient in working with those huge networks  is because it turns the code that you write into a graph of  operations. And it's the graph that it actually runs. The  data, by the way, that flows between those operations are  called tensors, which is where the name comes from. And  because your model's represented as a graph, you can do things  like delaying or removing un unnecessary operations, or even  reusing partial results. The other thing that you can do  really easily is the process called back propagation. So if  you remember when we updated the strength of connections in  our model based on the examples that we saw and the error that  we calculated, that's the process of back propagation.  Because the model's represented as a graph of operations  instead of code, you don't have to write additional code for  that, you can just compute and apply those updates  automatically. And another nice side effect of  having a wrap around is that you can in your code using a  one-line decoration, I want this part of the graph to run  over here, I want this part of the graph to be distributed to  a different set of machines. Or you can even say I want this  part of the graph that's very math intensive to run on a GPU  while the data input code runs back on the CPU. And TensorFlow runs on CPUs and GPUs out of the box. It also  can load models and run inference tasks like doing a  prediction or a classification on iOS and Android devices.  And now even raspberry pi device . And inside of our data  centers, we've been serving TensorFlow graphs using the  specially hardware called TP TPU. And  then going forward in the network, the strains of  connections between the neurons for each layer. That is  basically very large matrix math operations, and that's  something that TPUs do a lot of very quickly very well. Version two of the TPU hardware we're calling the cloud TPU,  and I will talk a little bit more about that in a bit. So once upon a time, Python was basically the only choice if  you wanted to build a TensorFlow graph. And it's still a  perfectly great choice. It's very simple. There's a lot of  example code out there. It supports everything out of the  box. But there's also support for a whole variety of other  languages. And since TensorFlow is open source, there  are a lot more additional language choices with community  support that are being added all the time. And so the end result is that if you're interested  interested, you can try out TensorFlow in probably your  favorite language right now, and it will work out of the box. And I just wanted to mention, my coworkers on the TensorFlow  serving team just announced their 1.0 release last month,  which is a really huge milestone for them because TensorFlow  serving is a very, very high performance piece of  infrastructure that you can use to load your models and then  serve inference requests, low latentcy on your servers. Internally, we use it for about 800 products, but we really  wanted to release it as part of the open source distribution  because it's such an important aspect of, you know, a real  world deployment. That's one of the things that when we say  TensorFlow is production ready, it's things like this that  makes the difference between code that you write for doing  research and being able to actually run it in production  solving real problems. Another one is a tool called Tensor Board. So this is one  of the visualizeers included in the package. This particular  one is showing a clustering of hand- hand-written digits that a  model learned for that particular task. In general, visualizing what's happening in a particular model  and then trying to debug predictions that you get out of  it has traditionally been a very difficult aspect of  machine learning. It's kind of, you know, one of the achilles  heels of a lot of machine learning frameworks. So that's something that we  really wanted to include because that -- you wouldn't be  able to serve something in production unless you were  actually able to understand what's going on inside of the  model and figure out if the prediction that doesn't match  what you expected, why that's happening. So that production readyiness in  general has been one of the keys to the success of the  framework and one of the things that makes it different.  TensorFlow since its release, has been the number one machine  learning repository on GitHub. And it's really been incredible  to see the adoption since it was released. This chart shows  the number of stars on GitHub since it was launched. And  last time I checked, it was I think over, like, 68,000 at  this point. And I think one of the other  reasons is because we actually take our place in the open  source community really seriously, it's never been for  us a matter of, you know, throwing code over the wall or  taking a thump from our source code repository and just open  sourcing data, and that's the end of it. Open source and open  source contributors have been totally a first-class a first- class part of the process since it was released. At this point , we've had more than 1,000 external contributors on  TensorFlow, and some of those external commits have added  huge new features like additional language that was  mentioned earlier. Additional hardware support, and even  whole new platforms that TensorFlow runs on. And the other aspect of our open source work is making sure  that users are productive and informed about how best to use  TensorFlow, so to do that, we've answered thousands of  questions on Stack Overflow, and we are also very serious about  looking into fixing issues on our GitHub issues page. Because  in general, we want to have a really seamless experience from  the time that you download the framework to the time that you  actually launch a model in production. But just to be clear, you know, we use TensorFlow a lot within  Google. This graph shows the number of directories in our  source control tree with models in them over time. And the  orange bar is when we internally release TensorFlow  for all projects to use. So you can see before that, there  was interest to machine learning and some people white  listed or using our precursor framework. And then after it  was released released, it just exploded, and there are more  than 6,000 products at Google using TensorFlow today. And,  in fact, it's pretty much every major Google product is using  TensorFlow and doing machine learning in some form or  another. And that has given us a ton of feedback and  opportunities to make TensorFlow better by doing things like  streamlining the APIs that we provide, adding new high level  APIs over time to make it easier to use, and, you know, also,  just providing some of the production- production-ready  tools that I mentioned. So let me show you some of the  things that we've been specifically using TensorFlow  for because there's so much variety in the types of problems , it's I think a good demonstration of how flexible it  is as a framework. Google translate used to use a model  that basically translated word by word. Maybe a few phrases  here and there, but that was basically the extent of it. And  then on top of that, it had hundreds and thousands of lines  of hand-tuned code written with the input of linguists and  language experts. Even so, it had a lot of difficulty  accommodating all the nuances and differences in languages. So here's an example on the right of it translating a  particular Chinese phrase into where will the restroom, which,  you know, leaves a lot of room for improvement. We've  replaced that entire previous system with a new deep neural  network-based system called neural machine translation, and  that's running on TensorFlow. And the end result is that many  of the language pairs have had huge gains in translation  quality, up to 85% in some cases . And the reason why is  because the model works by considering a whole sequence of  words. A sequence input and a sequence output. So the end  result is that you get a more natural-sounding output, much  more like a human translator would do. For instance, here,  excuse me, where is the toilet? A much better result. And sticking with that translation theme, we added the  word lens feature to the app, and this is actually running on  the mobile device. It works in airplane mode mode, which is  pretty incredible, considering that it's doing basically a  combination of computer vision and translation all in the same  model. We had to add features to TensorFlow specifically to  make things like that possible. And now, you can train a model  on a cluster of servers on one machine, however you would do  it normally, but then take that model and reduce the size of it  to fit on the device while keeping the quality high. And then Google photos is an example of an already great  product that was enhanced by adding machine learning  functionality to it. So in about six months time, the team  took that image-based classification system and got  it working live in Google photos . And the idea is that you can  take a term and search for it in your photos for pretty much  anything. Like, you can type in beach and get photos of  beaches, you can type in photo and get an image that contains  umbrella in it or even an abstract term like sunny without  previously having added those tags to your photos. A more difficult image-based task using another deep neural  network is the show and tell model from Google Research. It takes an image input, and it outputs a human- human-sounding  caption. It also starts with that inception model, but in  this case, it's not just classifying the objects that  appear in the image, it's actually writing a caption that  sounds natural and captures the relationship between objects  that appear in the image. To do that, the model was fine- tuned on examples of images that had human-generated  captions. And from that, it learned about relationships. As  a side effect of that process, the model actually got better  at describing details in the images like colors because it  found that those are things that humans want to hear in a  caption that they like. So Google Research, by the way,  open sourced the entire model, and there's an in-depth post  about it on the research blog, and you can go there and follow  the links and try it out for yourself. One of the other things that we've been doing at Google  research is working on diagnose ing a condition called diabetic  retinopathy using computer vision. Ideally photo and  ophthalmologists, they take an image like this and analyze it  for early signs of diabetic neuropathy. The problem is  there aren't enough open that ophthalmologists going around,  and the end result it is the leading cause of blindness in  the world. So we published an article in  the Journal of the American Medical Association, which shows  that a computer vision model can actually be as good or even   slightly better than the average ophthalmologists at  diagnoseing the condition. So that's something we're really  excited about. Because if we can get the model out there,  then it will have a real impact to help find more of these  cases Brits before it's too late . And one last thing from the research team is this problem  using a deep neural network to actually learn what kind of  architectures are good for solving different types of  problems. So what we're able to do is do a  search from a poor-performing machine-learning model to one  that's much more accurate without any human intervention  in between. The model actually builds a machine-learning  model that solves the task. And those sort of problems are  called learnings learn, which is a really exciting area in the  field of research, and there's going to be a lot more  happening in the next couple of years in that area. But just before you get the idea that, you know, TensorFlow is  meant for long-term research or meant for big budget  blockbuster apps, I wanted to show you a Japanese cucumber  farmer. His son in the back of the photo there built an  automatic cucumber sorter using TensorFlow, along with an  Arduino controller and a raspberry raspberry pi. He  trained the model by showing it 7,000 examples of cucumbers in  nine different categories. And so this is a job that his mom  was doing for ten hours at a time after every cucumber  harvest. And in his words, he said I wanted to leave sorting  to AI so that we can focus more on growing good cucumbers. So  after the model was trained, he hooked it up -- he hooked up a   conveyor belt to the controller , and an array of webcams to  the raspberry pi. So as each cucumber comes by the belt,  it's imaged by the webcam, it's classified, and it's sorted  automatically, which I think is just a fantastic example of  something really outside the box that  you can do practically with machine learning. And so TensorFlow was I think really powerful for solving all  of those sort of problems as of the 1.0 release. But there  have been quite a few new developments since then. So let  me just go over a couple of those with you now. First of all, it has gotten a lot easier to use. So I  mention TensorFlow has always been extremely flexible with  the goal of being able to solve any sort of problem that you  throw at it. But it wasn't always the easiest to use,  necessarily. It has the distributed execution engine at  the bottom there, which is what actually executes the graph of  operations and distributes them among the processors and  machines. That has pretty much stayed the same. As of 1.0,  then we added a layers API with the idea that you could  construct a model without having to actually mess around with  the graph and operations directly. But you still had to  build the actual network architecture and all of the  layers and that sort of thing. Still had to do that part by  hand. And then we added on top of that the estimator API,  which is a high level way to take a model and combine that  with input and do the training and evaluation process. Now as  of 1.3, we added another layer on top of that  for what we call the canned estimator. You can create a  deep neural network classifyier in literally one line of code.   And then when you use the high level estimator APIs, you get a  bunch of things for free like distributed training, automatic   snapshots, and the ability to run on a mix of hardware that  you have like the CPUs Us and GP Us. You also get all of the  best guarantees that the performance improvements that  we've been making will actually apply to your model model.  We've started publishing benchmarks of different tasks  running on different combinations of hardware. And  that is important because it's going to show how we're going  to continue to improve performance over time time. But  it's also important because it can tell you how you should  expect TensorFlow to behave on the combination of, you know,  hardware that you have and the problem that you're trying to  solve. And as far as un uncommon configurations, here's  the cloud TPU that I was talking about earlier.  It's the second generation tensor processing unit. The  first generation we only used for accelerating the inference  part of machine learning. The second generation also  accelerates training. And it's a big improvement in general  because each cloud TPU does 180 teraflops individually. But  they're meant to be connected, 64 of them at a time into these  TPU pods. One pod is 11.5 petaflops, which is an enormous  amount of operations. And the neural translation that I  talked about earlier used to take a full day to train on 32  of the best GPUs that we could get our hands on. And now we  train it to the same accuracy in about half a day using  one-eighth of one of these TPU pods. And the -- oh,. [Applause] The cloud TPU, by the way, we're  making available on the cloud platform to external users later  on in the year, and I'll have a little bit more information  about that at the end of the talk. We also need to make sure that we're fully taking advantage of  whatever hardware you give to the machine learning task,  whether it is a TPU or the GPUs that you have, or even just the  instructions that your CPU supports. So we've been working  on a compiler that converts those graphs that I talked about  directly into assembly code. It's called XLA, which is for  accelerated linear algebra. It's a requirement for running  on the TPUs, but it also runs in Gitmo to compile a graph for  CPUs and GPUs so that it can choose exactly the right  colonels for the hardware that you have available. And then  there's a third mode that's meant for mobile where you can  compile a model ahead of time and run predictions on the  mobile device. The advantages that the compiled  model is much smaller, but it still has the possibility of  running more efficiently as well on the specific device.  And one last note about mobile is we are working on TensorFlow  Lite, which is a whole new run time that's specifically built  for Android mobile devices. The idea is that you put a very  slim engine in a mobile app, which supports exactly the  hardware that you have on the device. And then it leaves out  all of the unnecessary purpose code. When you combine that  with an XLA compiled model, you get the efficient use of  hardware with a small memory footprint. And so you can do  this on-device inference tasks like I showed earlier, as well  as something new that we're calling learning where you can  take advantage of a model that has been trained and running in  the cloud somewhere, while having your own individual  training data on the device. So it's never sent to the cloud,  but you can combine those together on the device. Even if you have a lot of programming experience, I will  readyily admit that getting into machine learning can be  daunting. Very daunting. And one of the benefits of trying  out TensorFlow is that the stuff you play around with, you can  eventually actually use in production. But still, I would  like to give you some tips on how you can get started, no  matter what level you're at. First recommendation is to start  at TensorFlow.org because there's a getting started  section there that has a nice hands on direction to TensorFlow , as well as some machine- learning tasks that you would  want to do with it it. And they assume some knowledge of Python  for those particular intros, but that's about it. And then if you start with those , you can progress all the way  through the tutorials to building things like a  convolutional network, which is good for image classification,  and a recurrent network, which is good for those language and  translation tasks. And then there's also a really  interesting demo of different neural network architectures and  parameters at play ground. tensorflow.org. You can try the  different layers through the neurons and the learning rate  and get an intuition about how neural networks work by seeing  the effect on a very simple classification problem. And then once you start -- once you're ready to start building  real models to use in production , I recommend using that high- level estimator and a canned estimator APIs because you get  all of those automatic benefits like the save checkpoints,  exporting the tensor board, and the ability to do additional  training with pretty much no additional work. So nine times  out of ten or even more than that at this point, the  estimators and the canned estimators are the way to go. And then, of course, by using estimators, you could if you  wanted to, move your model onto cloud TPUs down the road pretty  much automatically. And at G. co/GP G.co/GPU sign up, there's  a sign up form if you're interested in finding out more  information about those. That link is also the place to go to  find out about the TensorFlow research cloud. So we're making  1,000 of those cloud TPUs available for free to machine- learning researchers because there are so many people that  have great ideas but limited access to the kind of hardware  to do the really state-of-the-art research. So check out that link as well if you would like to apply for  that. And finally, there are a number  of good online courses for machine learning. I recommend  checking out the Google developers YouTube channel. And  also, we would ask to use deep learning TensorFlow, which  really gets into the deep math background of machine learning.   And if you like that course, it's a step closer to getting  the machine learning nanodegree. I hope that you want to continue  with machine learning because it is an incrediblely-exciting  field. It's more accessible than ever, and there is so much   happening. I want to thank you for your time, and I hope you  enjoy the rest of your conference. If you are interested in giving feedback, you'll have to look elsewhere.  But thanks. [Applause]  Hello, everybody,. It's  great to see you here. My name is Alexey Kokin, and I'm on the  Google development team. We work with developers across the   globe to help them grow on Android platform. If you have  been following our blogs announcement this year, you  probably notice two important messages that we gave. One in  Q1, and second a few months ago. But we are going to move away  from just installs and mute mutateers, and we will  start focusing on more complex like engagement or box. Today we're going to talk about in detail, first of all, why  it's important to have a quality app and we'll talk about new  tools we have to help you troubleshoot applications and in  the end, we will discuss how you can benefit from having  quality app and what can Google Play give investing out of time  on building applications. First of all, let's start with  the basics. Why it is important to have quality  application. There are a lot of components  for the quality of applications inside Google Play, and we'll  talk in more detail about each and every one of the components  today. One of the examples of  applications in Play and performance, we have seen the  application moves on average quality, they display an amazing  and some of the most important methods for application  developers. For example, they have seen up to seven times  increased retention and up to six times lead in spend  applications applications. Those methods alone can make or  break your entire application business. So this is phenomenal  phenomenal, and this is one of the most crucial things we can  focus on in building your mobile application. Our partners work on data devices, they generally confirm oire ideas about the applications. Some examples  from them are examples of significant user ratings after  performance problems of the applications. Have a high rate  of 4.1, and they manage to do it  to 4.5, and those are great results. Those who work on  applications, you know how difficult to grow when it's  already high. Another partner, Lambda, they  have focused on performance. And faster launch makes a lot of  impact on their business critical. As long as they gain  a lot of graphic, in fact, on the monitoring on the LTV. A lot of great applications can use and more to use important  goods through such applications. And finally, there's the most  important factor for your application in Google Play.  Every user knows every generating helps you more five  star generating helps. And the statistics likes it. 50% of  negative reviews as I mentioned stability and bugs. And 60% of  positive reviews mention stuff like speed, design, and  performance. You know Android Vitals is our new step  to help you build applications, which we announced this May. This is by Google to  improve stability and performance. And you as a  application developer are the important part of this effort.   We are starting by focusing on three main performance. The  stability and rendering. By collecting data from devices all  over the world,. To give you a little bit more  inside of how different from other and what areas you should  focus on to improve your application quality. Again, support us in this activity, and if you are  speaking about application stability or on the performance  on device here, even the biggest display of market can  benefit to doing actual good work or decreasing this. For  example, Kabam growing retention of improving  application performance. And everybody in this room knows  how important this metric is for your application. So our three main areas. First area is stability. Where we  focus on crashes and the application is not responding  when the application is frozen for five seconds or so. Second , major focal point is battery because the main concern here is  not to overly use the battery of your device. And the third focused area is rendering because of the way  everything is rendered faster moved at 60 frames per second.   And a lot of things if your UI is slow, from using the device  application. So this three major areas, we  have added a new sections play vitals. And so so if there is  one thing after the presentation, the developer  console and check this section for some new insights. I will  consider my mission a success. And if you will analyze some of  the data, this area and performance improvements based  on the statistics available. And the best thing about Vitals  is that they already work. You don't have to implement new,  you don't have to do anything because those statistics are  available for you right now for your -- for each and every  application. And from this users who can focus in on the  decorations. So if you want to help, you should also this  option on your device too. It will help you. So let's go to the stability areas, and I probably will not  be focusing a lot because each and every review review, you  definitely, you try to fix them. But which we call the  application not responding and happen when the application is  not responding. So definitely users experience  embarked user behaviors. And the issue cannot use. But we  can because the response and this information in Android  Vitals. You can head over to the section of your console,  and you will see a lot of information of the performance  and number in different views of different devices and  application and versions. There are a lot, one in particular  we will definitely and exclamation point. And  generally, a lot means more than 5%. So if something is above  5%, there's something definitely to focus on and analyze about. There are a lot of additional statistics in this particular.   Here you can see , for example, you can see how  the user is impacted and the effect. There is one questions. We have  found out that the matter actually a lot, and they have  analyzed applications on Google Play and application rates,  again again, above five, they have around 30% more one-day un  uninstallations. So if you want to avoid -- even big developers, like,  for example, Twitter here has benefited from the statistics  and help them improve experience for actually hundreds  of millions of users all over the globe. Next chart is battery. Our Android team is working  constantly on making some good improvements to increase the  battery life of each and every device. The best way to keep  the battery alive is to shut down most of the features  and hardware that is not used right now. You as the  developer needs more power for the applications, keep the  device alive and behavior speech can be difficult. Wake lock. You can definitely  keep the device awake when you need some processing power in  the applications. There is a -- which is not that bad. The  full lock is performed from device, the screen is off and  the user is usually actually using the device, like, playing  a game or driving the car with the screen or watching some  videos, and knows that he is doing something with the device  and probably wasting the battery. So it's not a a high  priority. The priority concern is when the screen is off but  the application is still doing something in the background.  And sometimes, this activity can stuck and this wake lock can  work for a long time, sometimes more than an hour.  And when the wake lock is stuck for more than an hour, the  behavior inside the application actually Android Vitals. Again, if you head to the specific section of the  development console, you can see a lot of statistics where we  show you how many stuck wake locks you've had in  your application and the particular Android version.  The goal here is to have a wake lock. It's optimal from our  perspective. And the wake lock more than an hour, then  something definitely going wrong . Why is this important? We have our own internal study for  devices inside Google, and we have found out that in 30% of  cases when the users battery charge and lifetime value,  lifetime of the device, -- so keep an eye on them and try to  help users. Other thing when you can wake up  the device in a specific period of time to perform some  type of correction. Again, if you do this more often, it will   drain battery more, and we look at the battery behavior four or more wake-ups per hour . If you do it ten times or more, it's something definitely  to look after and probably increase the amount of wake-ups  in this case. Again, the statistics that show  you which specific API the wake up and how many times per hour.  And the worst-case scenario, of course, is you have a wake-up , which then becomes stuck, then it really means that the  battery will be drained. So this is the main point to avoid  in terms of battery life. And the last thing but probably  not the least is rendering. And as I said before, it's generally fitting fitting. We  are tracking two major points here. There's a slow rendering  and a slow rendering is when 50 50% of frames of the time  greater than 60 milliseconds and a frozen frame when a  specific frame is 700 milliseconds or more. And if  you have this, again, you go in the console, and you will be  able to see how fast your UI is rendered and. In addition to the frozen flames , we will also note the slow UI . When the UI spread, which has a lot of for processing  activity goes, presume it should increase the speed. Again,  this is something to look for and probably move this activity. So using troubleshoot the app issues and  make it more stable is one thing particularly important now  to measure the app in the Play store. But  there are other components for application quality, which are  equally important, and I'm going to talk about them in the  next session. So what's a quality app? First  thing's first, performance. We already model this discussion.   And then there are features that the users love. There will  be beautiful design which fill guidelines and the application  should really focus on supporting the latest features  and technologies. This will create a wonderful experience  for the users, and this will make the users love your  applications more. First thing's first, the  material design. We have been speaking about material design  for two or three years already, and in general, material design  gets a lot of quality not only from Google itself but from  independent designers all over the world. The materials on  how to implement material design and how to applications according to our  guideline and you can define them. Remember, that just using  some of the preview components , which are provided to you by  material design is not enough fully compliant. Design  property should design your application from the ground up  using things. And having some universal patterns the  applications Android platform hearing familiar patterns and  elements in the application design really helps our users  to be more engaged with the application Android, and it  really helps them to start using applications faster. Next thing, next big thing is platform support. First of all , quality app. You should always check your application's  stability and performance against the latest professional  system, and you should definitely use the latest  available to build applications. In addition to that, a lot of  other Android platforms out there and the more of them  support for your application. Remember that there are plenty  of devices for Android. There are wearables, tablets, TVs. Not all of them  of course is for your application use case. The more  considerable platform -- For example, smart TVs. When  user is using your application not on a mobile device but on a  smart TV, the user tends to spend a lot of time before a big  screen and better engagement for your entire application  application. Wearables. Wearables are mostly , as in 75% of the cases and if the user is buying and using  wearables is definitely more engaged and focused because  already has several devices, which he uses frequently. On  the wearable means the application will stay not only  on the wearable device but on the smartphone too, and the  users will return your applications each time they go. VR. There are a lot of buzzwords against VR, and there  are definitely a lot of use cases right now. Has its own  specific platforms, for example, definitely have longer but  less frequent VR application. There are a lot of new ways to  interact with the users within virtual reality environment. Next thing is monetizeation, and we are definitely believe that  the user uses your application for the  content. So when you pass certain  thresholds in premium quality generating. And  supporting of Android play for scenarios in the context where  Android Pay is available and one of the first companies in  Europe which launched in support of Android Pay is also  great, and you should focus on new. Next thing as to think about the easiness for users. Like user create accounts or  sign into your profile faster and easier and focus on  communicating with the users who leave feedback. Keep your  generating above four. So when your application generating is  four or more, it basically opens each and every door for  featuring activities for a collection inclusion. So this  is important and users on their own language in the reviews  sections will help you provide great feedback and communicate  with them. The application itself some  tools in Google Play and data. First of all, there are a lot  of things Google Play listing. Then you  can use testing to see what works and what doesn't, and you  can better your apps in the channel. A lot of things to work on and when you start to think about  your Appstore optimization, this is your number one destination . Title, icon, feature graphic, videos and stuff like that,  they also, they all can be presented to a particular  country. Of course, those wonders in terms of store. And you see what works and what doesn't, you can use testing,  run experiments in a specific language and understand what  specific graphic screen or title works better for specific  users. Why it's important? Because  it's one of the best things to improve. 17% is not the top  number you can get. There are some developers who get up to 30 % more conversions of fully optimizing Google Play. And  remember successful experiment have a clear hypothesis of  testing, and you should have some applications design your  testing. They shouldn't be very similar or results won't be  very -- And the last session of our  presentation is to talk about what you get from creating athe  best quality app. There are main things and main benefits to  a healthy app is search. You can get featured by our Google  Play team, and you could get recognized and receive certain  awards from our colleagues. So first thing's first is  visibility, promotability, and discoverability discoverability.  The better your generating is and the better your video, the more graphics from  the Google Play store. Result in application engagement and  Android, they impart in, first of all, in the search. So when  the user such as applications and having everything great in  terms of applications will help you boost your application to  the top and get more traffic and get more visible. In  addition to this, there are a lot of different collections  inside Google Play store. App for work, app for leisure, and  stuff like that. And around 90% of those collections, they're  now automated. So they are graded by machine learning,  which picks up the application and group them for the similar  level theme or similar. And those collections more  usually have a better quality. High quality apps before  average quality apps and collections. And, again, each  and every Google Play user see a different Google Play home  page, each time they open the Google Play store, and we try to  show them the contents that they might be more interested in . So having apps and more meta collections, and those  collections to the users more for the use case. And this  will get not just regular installation and not just  regular organic, but focused and relevant graphic with users  who are more likely applications. And next thing is you can get featured and be included in the  collections, which are curated and handpicked by our team,  Google Play business development and merchandising. Each app  is for featuring. Goes through an extensive review from our  team and analyzes the quality and analyzes its design and  features and provides them feedback and provides them  requirements for improvement. When those  requirements are met, included in some of our featured  collection, for example example, new and updated collections. A  lot of traffic viewing. Again , quality is always a factor for  our featuring requirements. Now we have a lot of formal  requirements, for example, your ratings should be more than  four, you should have a good generating in terms of Android  Vitals, so getting your app updated is number one. Before getting -- Again, featuring is usually really important for developers  and objects perspective. So you should get featured and you  can feel appreciated. But the traffic you can get from a  featuring is usually, the traffic you get organically  from applications over the course of several months or  several years of being advertised in search results  including automated collections is generally better, bigger,  and more than this one-time featuring boost. So working on  a quality for featuring is great because it gets you  visibility, but long-term benefits of quality apps deliver  better results over time. And the last thing currently is  you can get specific awards in Google Play. Currently, we have  two types of awards and recognitions . First of all, done by our  colleagues in Google Play merchandising, and one app gets buzz from social  media which is new and hot, it can get featured as an  application. And there is another which we have launched just a  few months ago, which requires -- showcases best Android apps  on the platform, which are the best in terms of publication,  design feature, relyiability, and user feedback. So getting  the Android award is quite hard in terms of effort and what you have to do. But the applications, usually the best Android apps there are . And if you are looking somewhere for inspiration or if  you are looking for best practices, just head out to the  Android excellence portion of the Google Play store and check  it out because definitely applications, you can check, you  can download, and you can probably reuse some of the -- in  your own applications. That's about all. We'll have  some office hours later today. And if you have some additional  questions about how featuring how Google Play works, how to  get featured, and how to improve your application quality , please come in, and we can discuss it. Thank you. [Applause] for realtime captions... test.  testing testing. This is a  captioning test. Please standby for realtime captions. The  panel Please standby... Nandini, and I enable  conversation between humans, among businesses, and any  combination in between, some of which include some machines. series, developers? We just  launched this as a teaser on YouTube, but it's coming and you  will hear all about that today. But I want you to stop and  take a picture in your mind right now of why voice is wonder what you're thinking.  We're living in a time when innovation is shifting our  outlook so rapidly of our future. It's worth taking a moment and  figure out how different it can be in a week's time, a month's  time, and next year it will be a different landscape than it is  here. machines to speak on our  behalf, to speak to us out loud is so deep to our context as a  culture and as human beings, it makes us question where we are.   It makes us question where we are with technology. It really  is putting us at the intersection of technology and  the human condition. More than ever before, we're actually  confronting a have a lot more to say about  that, but today's not the forum for that. Today we're just  starting a conversation. And the hardest part about any  conversation is listening. So we want you to know, developers,  designers, businesses, we've heard you. We've seen the  feedback from I/O and conferences like this is the  most valuable part of the experience is interacting with  other Googlers and those creating to create a different feedback  loop for helping you understand just how  to get your arms around this interface, and how you  supplement that with other elements like visuals. have to work with, how to  create your landscape, how to build your world for your users  in So in the first season, we just  wrapped filming. We have six episodes. We're going to have  things like greetings and good-byes. How do you start and  end a conversation. How do you identify users and authenticate  them. And, how do you make money at this? Transactions are  a big part. We will show you how to create custom voices to  brand your experiences. What does it take to create character  and persona? And also, what are sounds involved landscaping you can create in  all of this that's an entirely new world that will create  gateways. We will explore deeper concepts and feature and  highlight examples from the eco-system. We want to showcase  what good looks out there and what kind of  people it takes to make this work. And then on each episode,  we'll always feature guests that will help us dig a little  deeper, and they will be people from the community. Sometimes they'll be Googlers  and a lot of I'm going to dance. So we're  going to do a little live show today to talk a little bit about  what this means and hopefully get a little bit of insight.  And we'll actually not answer all your questions. But we have  a few questions that I've gathered from social media and  from our advocacy work that we've done in that space. To  start off, I want to introduce our panel. We have, first,  thrilled to have this great set of panelists here that I  personally selected because I feel like they're most  representative of where people who have come at this and  pivoted to this field from other areas to give us some insight  into their journey of how they got here. I think there are a  lot of experts in the field, but very few that are considered  experts. But there is just this tapped in muscle that you don't  realize we have. That's what I'm hoping you can get out of  this today. We'll start by first we have Marc Paulina, who together with Peter Hobson have  created a new methodology and interactive design. To give you  an idea on how we're really taking the audience and building  with the audience and viewers in mind, we built this panel  also directly from feedback from you guys. For example, Marc  just ran a sprint with peter with a small group across the  street with this new methodology, but it has to be  done in a small scale. We want to scale that big, so we're  going to show it on camera so you can Kimberly Harvey, who has partnered panel is a direct question from  the GDD community. Research expert asked for more representatives  from UX And if you don't already know  him, Sachit has found the line between design and engineering.   I credit him and his team for pulling me out of my shell where  I was working and doing more advocacy work. example is the G plus  community, we reached out to David Wang, who's going to be on  one of our very first episodes talking about that from the  product management team. These are all  advocates for you, and we're excited to get into some  questions. so, Sachit, let's start with  you. Voice. What's all the noise for having me on the an introduction, I'm an  engineer working mostly on developer tooling. I was  working on the living room team before on Android TV and cast,  and Google Home became part of the living room, and that's how  I got sucked into this. About a year ago, Nandini and I were  working on some of the first samples. If you ever have the  chance to help with app rebuilding, she has an  extraordinary talent for the get back to your question about  what's all this noise about? I think that this noise that we're  hearing is the low, low hum of the Sci-fi promise about voice  recognition. We are at very, very, very early stages of the  next motive interaction between us and machines. We've dreamt  about it for days and it's finally happening. But it's  still a little rough. The interactions are frustrating  sometimes. But listen through the noise for the signal of the  magical moments happening with machines. We will be talking  about these days years from now when it's working as they What's all the noise about? what is exciting about it is I  feel like we're getting closer and closer to a direct thought  download. I studied communication and cross cultural  communication lot of different methods of  communicating. Some may be watching on video, using  gestures, words, systems of writing are all around the  world. I feel like having humans interact in a way that's  been natural, because that's one of the first skills that we  learn as babies can kind of get rid of some of the challenges we  have, can get rid of some of the challenges we have Marc, you've been a designer for  a long time. You started in voice, actually way before we  had the Google Assistant. What an interesting time at the  moment. Talking about the journey that we've had, I wasn't  a voice designer before I started working on voice actions  at Google. I was working on interactions in mobile,  automotive and TVs. Voice, for me, was quite a stunning  journey. It was so different than what I was used to. For  me, it's been really enlightening thinking about  voice and conversation and natural user interfaces. And  how there's such an emergence of AI, internet of things,  robotics and so on. So we've got all of these natural usage  bases that set people's expectations so much higher than  we've had before. So, as a designer, it's been exciting to  try and create the new design methodologies where we can  understand people a lot more and try to meet the expectations.  It should just work. But right now with the technology how it  is, there's a lot of really interesting design you know, that there is some  kind of forgiveness there. People are at this point where  they're like, okay, we get that it doesn't work perfectly all  the time. And it's great to see that people are working on  that, but, it's like, people get the possibility, finally, which  is so exciting. Let's start. I have a few questions. I kind  of pulled some from online and others from asking around at the  conference. We do user-centered research, very not  last minute at all. This one comes from Twitter. Is the  Google Assistant Well, the first thing I would  ask, Bob, is why do you think it is a Girl Scout? What lends  itself to that stereotype that you research. But to answer the  question, I would say that the Assistant is not a girl scout,  but it can facilitate relationships like girl scouts. itself with personality, makes  sure that the interactions we're designing are on par with that  personality, make sure that the wording is right, and shows  different parts of the And the head of the doodle and  personality team has a great analogy. You ask personality  questions to poke at things and ask it things that you can know  what to expect an answer that. It's a way of  establishing trust. So when you establish trust with your  neighbor, for example, you might borrow a cup of sugar. So  grant says borrow a cup of sugar before you ask for the lawn  mower. So how do you write personality questions that are  just about the character that you're interacting with as  opposed to the deeper back-and-forth interaction about  whatever it is the task that you want to a great question. zinger via LinkedIn, probably  for Sachit and maybe the others the pipe in. There's a common  sentiment among developers who are leery about starting project  development in newer tools as they have been burned in the  past by abandonment the community-based nature of  design, can developers find assurances that Google  recognizes this pillar and won't be abandoning support for the  assistant SDK, or other related projects? This is amazing. To  allow innovators create new products and found businesses  upon. Well, I will say one thing. If we are doing that,  none of us have any jobs. So we're here. from a developer perspective,  you guys work with tools and APIs, so what would you First, I would empathize with  the It was Brandon Hunter from  LinkedIn who asked the that before, and I certainly  wouldn't want to inflict that on anyone else. I would say on  this space that I think a little bit extra of adventurous  attitude is actually called for here from the developers' point terms of picking up knowledge  around APIs and terminology and figuring out what the design  looks like for these types of things will pay off  exponentially in the future. To directly answer the question in  terms of Google's investment in the space, I think it's clear  using the list of services and APIs that the person asks the  question, it's very clear that we're trying to push an entier  suite of products on developers here. It gives the idea that  Google is trying to push heavily into the space. Internally, we  are seeing this as sort of the next step. And this also is  shown through the consumer side. Assistant has a company  initiative that reflects the developer APIs and services that  we're releasing. What's clear is if you look at  the scope of what Google is actually putting out there that  we see this as the next step for  users and developer s and we will be supporting this going  forward. platforms going away. Clearly  the platform as a whole, the conversational push as a whole  isn't going anywhere, and we're sticking to Thank you. Okay. Let's pivot  to design. Marc, can you talk about some of the design methods  you have used So, as I said, learning the  design process, being new to the field is quite a challenge. that we used before for other  experiences, for example, designing apps, websites, and so  on, for learning new methodologies. So  I, personally, take a lot from service design theory. One of  the principles of service design is that we democratize the design process  to come up with powerful experiences and user-centered  designs. The way to do that as a designer or researcher or  anyone on the product team just to be aware of who the user is  and what the user needs. And keep asking the question, what's their goal? What are their you understand about the user  the more interesting your design will entire expectations of the  natural user interfaces than other types of interface. And  also, best practices for conversation design. So one  principle we have over conversation design is about  failure. With technology the way it currently is is there can  be a lot language, there's a lot of  ambiguity. We focus on this a lot. The happy part is fairly straightforward,  but the unhappy part is quite complex. There's also  prototyping, so that's another part is being able to validate There's a reference in there  somewhere. Happy little trees, the Bob Ross method. So we can  all talk, speak, gesture. It's the interface we learn first and  know best. We don't need help understanding that. So why do  we need UX UX research? Just because we  know how to speak doesn't mean we know the intention of what's  going on. Backstage, I was saying let's say Marc was  outside in the center of the hall, and I was talking to  someone and I said what does Marc look like. They would  describe him in a certain way, he's wearing white shoes. But  if Marc has been out for the flu for the week and I turned to my  boss and asked what Marc looked like, he would say he  looks a little better than last completely different answer  based on context. If we're designing an experience, we  don't know until we actually see it in motion,  whether we iron out all of the kinks and whether there are  surprising behaviors that are emerging make sure you have everything  covered. other disciplines as you get  into this, but just from a developer perspective, getting  the ramp up into this, like what can a developer bring into the the great thing about our  platform in particular is it's mostly driven through the cloud.  What that means is for developers who have already been  developing in the cloud, web apps or APIs they have been  building, they will bring all of that experience in. And the  tools they use to provide to abstract some of the harder  problems like the natural language understanding, those  tools are fairly easy to understand for anybody. So  first, learning that is fairly simple. There's a low barrier  to entry. And with your existing cloud web knowledge and  using basically efficient coding practices around things  like string manipulation, those kind of things, you can build  that pretty effectively on our platform. One of the things I'm  actually really looking forward to, hopefully that I want to  see from the community is more tooling and frameworks similar  to frameworks on other platforms like web apps if you just want  to build a UI. So I'm curious to see how people will build  with reference to voice you're early in the process,  you want to start prototyping. So Marc, you have suggestions on how to I think about prototype,  looking at something that I can use early in the design process  that I can learn quickly, and it's really quickly. So I think  it's really important that we invest in prototyping,  especially if we don't have years of experience working on  voice user interfaces, and that we're really learning on methodology that we've used  with some success designing conversational UIs at Google,  such as Wizard Of Oz prototyping. The idea is you  remotely control the device that the participant is using in a  study environment. But they think they're speaking to the  assistant, but really they're speaking to me  as the puppet what they believe is the end-user experience, but there's  no AI or cloud, it's me on the other gets in terms of can go anywhere. You never  really know. There's no set path. To feel natural, you need  to be able to pivot in for that, the only person who  can do that is a real person doing that in realtime. Wizard  of Oz is powerful. It can be as simple as having a blue tooth  speaker, audio files that you're playing as they respond. It's  pretty good. early on in the design process  is saying out loud, role playing. And Google design  sprints, where I've got the whole product time designing the  conversations. We actually do investigative rehearsal, which  is a methodology created by Adam Lawrence. And it's basically  the idea is that you rehearse the conversation in realtime,  and then you investigate that conversation, and you ask  questions about it. most scrappiest, most low  stability prototype you can imagine, but incredibly powerful  for the first draft of the conversation, before you invest  in any coded mention that you, for those  sprints that you just talked about, that you have the whole  product team there. What you're facilitating is a conversation,  to build a conversation, and it creates better conversations in  terms of the actual whole product team there. It democratizes the user requirements, everyone  understands their goal and maybe business goals as well. And  everyone's thinking through that at the same time and trying the  stuff out. It really is a platform approach fact, that's probably a good  place to close and wrap up and to get everyone thinking about,  you know -- I've been doing this for a really, really long time.  So hopefully, if you feel like we raised more questions in  this panel than answers, then good. Because there's a lot to  talk about. As I said at the beginning, this is just the  start of the conversation, there are worldwide one of them, and it's a much  larger one. So a conversation about the future of technology  and, you know, where we are today and where we're going from  here is going to take a new kind of eco-system. This is  called a conversation help, we need to build this new  eco-system. We can actually, at this point, take a quantum  leap past this AI first world. Words really matter, right? It's  quite unfortunate that we call it artificial at all when we  need things like authentic and advanced intelligence more than  ever. We are all starving to amplify our own intelligence, so  maybe amplified intelligence. #fixtheglitch. It doesn't  matter, users come first. If we build a world using a  people-first creative approach to building solutions that  people can use in their daily lives, it's more than just  talking and getting answers anymore. People want insight,  and they actually want to be able to do things in their  world. We wand to find those micromoments that are there's another A-word. We  just need the whole alphabet.  disciplines at the table.  Diverse voices, diverse approaches, diverse thinking.  This is really a time when, you know, it's like I sit at the  intersection of science and culture, human interaction,  human solution to a global problem. Like, we built these  machines. Let's make them work on our behalf to make them  connect more to each other and the world and the things we want  to get done. I have been doing this a long time, working with  this technology. But really, it's my personal mission to give  voice to others, and to help create a culture that we can  create experiences that illustrate a shared vision of  our future. We need to collectively recognize that good  means going beyond don't be evil or do what's right, but to  create experiences to show how it's done. And so hopefully  with this series, you want to show what that looks like. You  want partner experiences to showcase what you're creating  for your users. And you know, for the common good of users  everywhere. So, you know, we want to give insight to the  kinds of people inside Google who are building this, and also  changing the lives and future and technology, and also, you  know, this is the power of spoken language. What kinds of  people does it take to change the landscape, you know? It's a  disruptive environment. It's a living our legacies in  realtime. What are you going to be known for? What kind of  world are you creating for your kids? We're going to create some  amazing things, and I can't wait to see what you create. Thank  you so much to our panel and thank you to the audience for  being part of this exciting, creative moment for us. Thank  you. Good afternoon, all, and for  those following along on YouTube at home, good morning,  good afternoon, good evening. I'm Brett Morgan, I work on the  developer team for Google Maps based in Sydney, Australia.  That means I do things like write sample apps, write  tutorials, Codelabs, and all of that sort of things. As I said , I look after Google Maps APIs. Google Maps APIs is the  oldest Google Maps service has. Over ten years old. We have a  lot of APIs. We have a lot of interesting functionality that  you can use in your applications. One of the things  that we hear from developers, like yourself, is that that's  great. You've got all of these APIs, you've got this wonderful  documentation, it tells you what the arguments are, it  tells you what the result is. But how do I as a developer  take all of these APIs, glue them together to create  something that solves a problem? So the JA developers relation  team in Sydney came up with an idea of making solutions to  show you end to end how to solve a  problem. So I just wanted to give you a  little bit of history here. Every year we have an annual  developer conference, and a couple of years ago, we decided  to move it from downtown San Francisco where it had been for  quite a few years into Google's backyard at the amphitheater. Now, this was great. Bigger venue means we could do bigger  things, have more people come along. But how do you take  7,000 people from a bunch of hotels all from South Bay and  get them into this one event? Well, there's a pretty easy  answer to that. You get people in buses and move people around  with them. But this is an interesting opportunity for our  relations team. How can we best utilize Google  Maps APIs to show people where their buses are, and how long  it's going to take them to get between different points? So Steve down in the front row here and myself and a bunch of  others put together a solution that actually did this. Showed  attendees at Google IO the realtime location of their  buses and also showed them timetables of how long the bus  was going to take to get them back to their hotel. So what we're going to do this afternoon is walk you through  that solution. A quick little note here what we're building  is actually an asset tracker, and that one is specifically  called out in our terms. So before you go ahead and  implement an asset tracker, please have a chat to our sales  about a premium plan, because you're going to need it. Okay. Let's have an overview of the transport tracker solution . So here's the architecture. For those of you who attended  the Flutter and Firebase talk yesterday, this probably looks a  little bit familiar. It turns out Firebase encourages you to  actually building this way. It does a very good job. So what do we have here? Well, first part of the problem is we  have 30 odd buses running around South Bay South Bay, and  we want to know where they are. So what we did is we put an  Android phone on each bus with a little suction cup holder to  keep the phone out of the way of the driver and a USB Power  Adapter. And that had a custom application on it, which was  then turning around and publishing to Firebase the  location of the bus. Firebase is wonderful. It turns  around and when you push information into it, it turns  around and notifies anything that's listening on that path. So following on from that, we have a Node.js process running  in a Google compute engine server in the cloud actually  being notified that this bus has moved from here here to here  that process then cleans up that data and writes the cleaned up  location of the bus along with timetable information, various  other things back into Firebase in a different location, which  then turns around and changes the display. So at Google IO, we had big 60- inch LCD monitors that had --  as you saw previously -- the realtime information of the bus  and the timetable information.  So this is the overview  solution that I'm presenting this information afternoon. So let's dive in. Let's start with the first component.  These are the Android phones. How do we get the accurate  location of the phone using Android APIs? Well, thanks to  Steve's code lab, which some of you probably had a go at  yesterday afternoon. But if you're following on YouTube or  you didn't, I'll have links at the end of the talk that  actually I'll link you to the realtime code lab. This, I just want to show you the core component that drives  the ability to track the location. So we're doing to start with is configuring a location request.  But then reasonably aggressive here , we're asking for high fidelity, high accuracy of the  phone, and we're asking you every ten seconds. This means  the phone's going to be really active. The circuit bug, the  radio circuitry is always going to be on, so this is going to  have battery implications, which is why I mentioned before that  when we installed the phones in the buses, we connected them  up to USB phone Chargers. So this is an explicit design  consideration when I turn around and do this sort of thing , you need to be aware of is this phone permanently connected  to power or not? Another thing that we  implemented to deal with this was monitoring software. So we  had the phones actually writing their battery level into  another part of Firebase that we had an admin user so that we  could see all about the phone, the current location, and most  importantly their power status. Because occasionally, a bus  driver would get into his bus and go what's this phone doing  here, unplug it, and we would go and find the bus and get them  to plug it back in. Okay. So we configured how  we're going to get locations. The next point is then use the  location provider API. This gives us -- fuses together  cellular tower data, GPS data, Wi-Fi data to give us a more  accurate depiction of where this phone actually is. We then configure whereabouts in Firebase we're going to write  this location information. But finally, we create a call back  to hand to the location provider API that we can actually then  turn around and write the results into Firebase. And with  this ten lines of code, we are now tracking this in realtime. Okay. So we dealt with the  first component. Now we move to the heart of the system. The  Firebase realtime database. The Firebase realtime database  is a very powerful realtime communication tool. For me,  while I was developing this system, it was absolutely  insanely powerful because traditionally when you're  developing a distributed system, the main pain point you have  is understanding what the whole system is doing. Now, because  I've got 30 phones wandering around South South Bay, I would  like to know where they are, what they're doing and in this  console, I can see which each umph is doing at all times. I can sit here and through this console turn around and make  changes to the data structures and see what the downstream  components do when faced with certain edge cases. This allows  you to effectively have realtime development in a  distributed system. You can see what it's doing, change what  it's doing, all while looking at a nice Web browser-based  console. Okay. So told you about the  Android phones, they put their location information into  Firebase, Firebase then notifies our Node.js process  running on Google compute engine. So this component has a  couple of things that it does. It has a list of  responsibilities, but I'm going to go through this. The first one is while running simulations, we like to move  things around the map. To do that, we need to generate the  paths the buses are going to take. They start at a hotel,  go to another hotel, and then wind up at Google IO. We need  to work out the path of all the roads, the roundabouts that  that bus is going to take so that the simulation looks real istic. The next thing I'm going to show you is what the  server does with the phone location to tidy up the GPS data . Tends to be a bit noisy. Okay. So the first code snippet . Here's where we're generating a path. Trip points  is where we're going to take all the points of the path and  take them in and do things with them later. So this is  effective effectively just in the could you tell me later  accumulateor. For every stop, I have a  database of all the trips trips. So what we're going to do is  get the stops that this trip is going to make out of that  database. Here's where we create a directions API request.  At its simplest, a directions request has a starting point  and ending point. But in the case we have multiple hotels  that this bus was going to, we have way points, which is  between the origin and the destination. This is the line where we use client library to turn around  and make a request to the directions API. I just want to  stop here for a second. We actually have four client  libraries. One, as you can see here in Node.js, one for Go,  one for Java, one for Python. And these client libraries take  the Web service APIs like we have, like directions API, road  API, so on and so fourth and give you language-appropriate  help. So for Java and go, we expose to you what the result  types are going to be, so we get -- if your browser or editor  gives you type completion or what sort of thing. Okay. Now that we've made that request using the power to turn  what's basically an asynchronous operation into  something that looks synchronous , now we look at the response.   Was it -- did it actually work? And if it did, now we  actually interrogate the response structure. So a  directions response consists of a set of routes, and I'm just  going to take the first one here because I don't really care --  well, I do care. The first route is going to be the best  route; right? For each route, we have a set of  legs. This traditionally shows up as different models.  In each leg leg, there's a set of steps. And then finally in  each step, we have a polyline. So polyline is encoded format  for a list of lat longs, location points. So to code  this, I've got the points for the step, and then I turn  around and glue them all together, put them in trip  points, and then I use it later for actually rendering the  buses going around in simulation . So the second code snippet. This is where I'm going to deal  with the bus. So the bus is walking around -- walking?  Driving. Rolling around. So we have a bus moving around South  Bay, it's checking in every two seconds through Firebase  telling us where it is. So on the server side, I collect a  history of the location of the bus. A couple of minutes, in  this case, and I take that history, and I send it off to  our snap to road API. So this is part of our roads to API  service. So roads API knows what all the roads are, and I  can turn around and give it a couple of minutes of history of  the bus moving around. And roads API will then take those  points, do some mathematics, and make a best guess as to which  road you're actually driving down down. So it effectively  eliminates the noise from the GPS track. So then we -- because we're actually interested in the  current location of the bus, we take a list of returned points  and take the very last one. It's worth noting here that if  you're plotting out a path to something on a map, that you can  also ask roads API to insert additional points because as I  said, it knows where roundabouts are and curved roads , so you can add it additional points to snapped road such that  when you plot it, it's a nice line that actually follows the  roads as opposed to just the points that you sampled using  locations provider. In this case, we're not plotting  anything, so I didn't do that. I've just taken the most  recent point.  And of course there's error  recovery. So now we're onto the final part  of the system. This is where we put the buses  and a timetable on the map. So you might have seen the map  I showed you. It doesn't look like a standard Google map.  Well, what does the coated look like? It looks like this. We  have a description language that allows you to define the  set of rules that tells our mapping engine how to render a  map. I don't know about you, but I don't really want to turn  around and figure out how a map renderer is using this. I  would rather use a what you see is what you get wizard. So  what I would like to show you now is show you how to use a  wizard to come up with a map style. Pray that the demo gods are actually working. Okay. This  is our wizard. I would like to create a style. Sydney,  Australia . Can't tell where this was  written; right? Let's go to Krakow. Okay. This  is the stock standard Google Maps styling. But I'm creating  an application, and I don't want this style. I want  something that speaks more to Krakow. Should we go with  something that's completely saturated? Or respecting  Krakow's history? Should we go with something retro? I think  I like this. But it has a bunch of visual noise on it that's  not talking to me with the application I'm designing, so I  can then turn around and -- yeah, that's better. I've  calmed down the map. Maybe I want to take off the landmarks.  Let's zoom in a bit. How is that? I'm happy with  that. Okay. If you want to dive in deeper  and actually make more modifications, we have a deeper  level here that you can turn around and play with the  individual settings and see the results in realtime. But I'm  happy with this map as it is. So we give you two ways of using  this map that you just configured. The first one here is the JSON language that I was showing you  before. This map styling language is useful in,  obviously, Google Maps for Web. So using the JavaScript API.  But it also works the exact same language on Android and iOS.  So this way, you configure your map once and use it across all  of your applications applications. Question also give you static  maps, so if you want to configure it and style it in  some way, you can do this, and this saves you from having to  load a JavaScript maps API. A much lighter response if you  know you don't want your users interacting with the map. Okay. Now I've styled the map.   We're onto the final part of the journey where we've had the   information, start on the Android phone, we cleaned it up  in compute engine, now we're going to display it on a map.  we must be back in JavaScript land, got a Firebase database  we're taking reference on, now we're listening to a different  path in Firebase. So this is where the compute engine hosted  Node.js process and writing the cleaned up location of the  buses. So what I've done here, I'm  writing an entry for each bus. So thus I can look up all the  buses all at once. I look at each bus, and I have a routine  here for what color marker to put for that bus depending on  what bus route that bus is serviceing. Then we create a marker. Real istically, you need a bit more  complexity here to deal with re using markers markers because  if you kept creating new markers , you would have the screen  full of markers. We move the marker around, and then you  delay it when the bus goes out of service. But I hear people out in the  audience screaming the Web's great, but I want to put it on  mobile as well. How can we do this? Well, just so happens, we  have the same code for Java. We've got types here, we're back  in Java land, we have a database reference, we turn  around and create a child listener to add to the Firebase  to notify us when things change . This time around, I'm just  tracking a single bus, I'm not turning around and mapping all  of them. I've got out of the Firebase, I've got the lat, so  that's the latitude of the bus, longitude, and the bus route.  So now I have enough information to turn around and  configure a marker on the map. Same logic applies here. You  probably want to hold onto the marker once you create it and  move it around. Simple enough. Okay. So just to go over a high  level of what we've just covered, we started out in  Android using location provider to get the location of an asset , we published that location every ten seconds into Firebase,  Firebase notified a Google compute engine hosted Node.js  process to cleanup that data and deal with the GPS location  data. That updated information was written back into Firebase,  which controlled a front-end Google map-hosted dashboard  that was shown to all the attendees at Google AI. I've  also shown you how to do this with mobile. So where next? For starters, if you would like  to see the documentation for the Google map solutions that  we're publishing, it's all up on developers.Google.com. If  you would like to have a look at our client libraries and our  open source code like the samples -- sorry, like the  solutions, it's all up on GitHub . And finally, the code that  you saw in this talk is actually lifted out of three Codelabs.   The first code lab was the one that Steve and I hosted  yesterday, which was the asset tracking code lab. The code for  the Node.js process was lifted out of the transport tracker  back-end code lab. And finally , the code that I showed at the  very -- almost at the very end of putting markers on a big  Google map was taken out of the transport tracker map. And  with that, I thank you for your attention, and I invite you to  give us feedback. Thank you very much. [Applause]  My name is Thomas Steiner,  I'm based out of the Hamburg, Germany office. And I work on  mostly Web stock stock. And with me today is my guest  speaker Steven. Steven, want to come up and introduce  yourself?  Hi, so I'm Steven. I'm  product manager at Google in California. I'm just visiting,  guest speaking today on the identity on the mobile Web.  Thank you. So today, we want to show you what's next for  the Web or as an alternative title, exploring. But before  we get started, here's a fair warning. There's a lot of  content coming, and I'm going to speak fast. Probably even  too fast, so hold onto your seats and especially watch out  for other links. So many of the slides have links, and, of  course, you can just snap a photo of the slides so you can  look up some of the content after the presentation. So let's get going. Let's get started. Our first big topic  area is proving Web apps or short, PWA. Let me show you  how you can with PWA create immerseive screen  experiences. Because your apps take up a lot of real estate.  Some of you may remember iOS 2016, the paper paperplanes app  is still an app. Pu launch this application,  you might notice something. This is a Web. But if you  notice on the screen at the top, there is no network signal, no  clock, no battery status notification notification. It's  a true full screen experience. And you can do this by setting  the display property in your Web app to the value of full  screen. Very exciting for game  developers and, yeah, hopefully you can try it out for your  application. For PWA, we also add to a home  screen experience. Install the PWA, they will not be able to  find PWAs in the Android app drawer, with just all the other  native applications. So they look and behave and finally  react as true native applications, so you can just  search for your app. And something that a lot of  developers have really been looking forward to, you can also  now update the icon and change the application name after the   application has been installed. And something that's really  cool as well is with the improved flow, you can now  based on the scope attribute there Android in the Web app  manifest, create an intent that allows you to react whenever  your pattern within the scope, so your application can just  open directly whenever it is. And I'm going to check, for  example, and go into the PWA experience. The next thing is the Web share API. Because we all know that  sharing is caring. And if you want to do that on pretty much  any site in 2017, you end up with an experience like the one  that you can see on the side.  So a lot of screen real estate  is in a sense wasted for all of these sharing buttons. And, of  course, looking at the sharing buttons, always there's one  social network that you would actually want to share on is not  on the list. And now as we have seen on the slide with a  full screen experience, sometimes the URL bar is hidden , so you need a different way to share your application. And  this brings me to the Web share API. It's integrated with the  native Android sharing dialogue dialogue, which is really cool  because there, you have a truly native experiences --  experience of sharing your content. So if you look at this  code snippet, it's very short. Navigator.share. You can give  a title, you can give a Texas, and you can give a URL, and  this allows you to change the URL that's  displaced on the screen. So, for example, something that is  hosted somewhere or embedded in the app cache, for example,. The next topic is Web notifications. Directly in  the native system notification pattern, so they no longer  feel like strangers, they're now just native notifications.  Which also means they respect your devices or your systems do  not disturb setting, just powerful because you don't want  to be disturbed by notifications when you give a  presentation, for example,.  So the next topic is an  interesting topic. Some companies ship down three  megabytes of image data and then ship it down on the site  and other companies have worries about performance of the  service.  Navigation preload. What this  does is it kind of makes the time or the service record boots  up not waiting time but parallel time that your app can  already use for navigation request. So if you look at this  little diagram here at the bottom of the slides, you can  see the service record boot time before and the service record  boot time after this trial is activated. So you can see it's  running in parallel, so the request can already  happen. If you look at this code, you can see there's two  events. The activate event with this highlighted yellow line,  the wait, navigation preload enable, you enable navigation  preload. And then in your fetch list now after going  through the chain of do I have the response. Maybe no or yes.  You can check if there's a preloaded response that you can  then ship to the users. So this allows you to get content  to the user faster without waiting time or the service  worker to boot. On mobile, you now have a pretty  exciting new way of signing up . And this brings Steven on  stage.  Okay. Thanks, Thomas. So  what we'll talk about here is one part of the experience of  building a mobile Web app that is often frustrating. It's only  a small part, but it's often a big part developers encounter.  And that's how to get people signed in and signed up on a  small screen. You've probably all had experiences like this  before where you have to remember user name, password,  type it in on a mobile keyboard and perhaps you've added  something like Facebook sign in or Google log in on your app.  But there's not people using it . There's always a concern of  what information gets shared and how you end up back on the  site. So what my team has been working on in Google is how to  improve the mobile experience for the mobile browsers out  there. And what we're working on out there is a new and un  unreleased API that adds an I frame to your mobile site where  you can offer one tap sign up or sign in experience. This  gets someone into your site or signed up through  checkout or a conversion flow with as little friction as  possible. And as a new thing, we would love to hear feedback  from developers like you on how this would work on your site.  So check out the link or send me an e-mail. Send us feedback  on this part of the mobile app experience. Thank you.  So I'm going to use that.  Essentially, there's two API calls that you can make. But  auto sign in you can use smart log.retrieve or smart log, you  can do smart lock.hint. You're not spooned this code, it's  just to give you an idea of what will be possible soon. So  let's get back to PWAs in general. The question I get a  lot when people develop a PWA A and develop caches, what am I  even allowed to use? How much storage do I have? Or if they  already have a cache full of content or somewhat full of  content, they're wondering how much of my cache am I using?  So with a new storage Web API, you can now find out. You just  very recently call navigator. storage.estimate, then you get  how much storage you are already using and how much  storage is available in total. A long time thing by developers of PWA. Let's  move on to the next topic area, which is media-related APIs.  Media on the Web is very exciting and the next thing I  want to talk about is image capture API. Because you  should be zooming in on what matters. So on the screenshot,  you can see what matters is my face. I'm zooming in on my  face and the animation that I created for the slide. So this  API allows you to actually natively zoom in. So this is  not large. This is native zoom of the camera. And you can use  this directly from your browser, and this is how you do  it. So for this little demo here,  you can see the slider and the slider, that's of course how  much can it slide? So what is the maximum zoom level? And you  can see this short snippet code how from the media stream  you first get the track, check its capabilities, and from the  capabilities -- sorry from the track, I check the available  settings, and I can use that to -- yeah, update and create my  user interface to zoom in with the capabilities of the device  directly being used and being applied. So that's pretty cool, pretty exciting. Something even  cooler and even more exciting is the shape detection API that  allows you to detect faces, read bar codes, and even to run  optimal character recognition right in the browser. I was just holding my phone and pointing to different browser  tabs where I had opened first a QR code and then on a second  tab, a face and some text here here, you can see how in  realtime the guided detects different objects on  the page. So this is directly integrated in the browser. So let me quickly show you how it works. You first need to  initialize the different detectors, so the face detector , the text detector, and the bar code detector. And then you  can just listen for events. So everything you can cue up all  the different detectors and so on, and then you will get the  detected objects in the results . So really cool, super easy to use. If you check out this  demo, you will see its 100 lines of code and everything working  directly in the browser. So super exciting, super cool API.  I think this unlocks a lot of cool applications. The next thing is the media session API. Many play media  content on the Web. You want to know, of course, well, what is  playing, when is it playing, and on a mobile device, you of  course want it controlled. You want to skip over, go to the  next track, pause, this is all possible now with the session  API. You can see meta information of the currently- playing media, and you can, of course, directly control it.  Let me show you as well how this works and what kind of  metadata you can control. So in the screenshot here, you can  see there's a title, there's an artist, there's an album,  there's artwork, and then you have different action that you  have. So a lot of things possible.  And this block of code might first look pretty intimidating,  but actually, it's just too big things and the first half are  sitting here with the media. And then on the second half, I'm  just connecting the action handleers and connecting  functions with the different events. So super easy to use.   Again, very powerful, great integration , to a native operating system. What we are. Big topic. Super cool thing because sometimes,  the real reality is actually not real enough. So what we are  allows you to create compelling VR experiences right in the  browser. It's an open standard that just allows you to  directly program these kinds of experiences so that everyone  can get VR experiences very easily. You can see how this  could change the lives for interior designers, for example,  who would just pass on a link to work that they've created to  their clients. You don't have animated 3D or anything. They  can just take out their VR logos, cardboards, whatever and  then take a look at how their future home might look. Super  cool directly in the browser. Performance. This is super  important, of course. We preach performance a lot.  Performance APIs no longer allow you to have an excuse for  having a bad performing site. Network information API is  something that you can use to know how fast it can go. Let me  show you what you can do here. So based on the connection  type of the user, you can now adopt your experience. So, for   example, you can show someone low resolution image assets  when they are on a slow connection. And of course on  mobile in a true mobile way, so sometimes you're in a tunnel,  sometimes you are on the country side, and then sometimes you  are in the city. So what I am getting at is connections change . So of course you don't want to have someone with a low  experience for the entire session. But sometimes when  they have a good connection, you of course want to keep them. So you can see how the code reads out the information but  then notifies when the connection changes. Why is  this important? Well, you of course have media queries. So  you can say, well, this user is on a retina display super high  resolution Apple book but actually combined with that  information, it might after all not make sense to show them  super high resolution images. So this is just a short example  of how you can programmatically from JavaScript check for the  results, then combine it with the connection API to see if the  connection actually right now is good enough for supporting  high res images. So you can directly work with media  queries and with the connection API together. And there's the core performance API because you do care or  should be caring what's happening when a page loads.  For this, I was very quickly auditing the actual event page  for the GP. So there's a lot of different things that you can  get and calculate and an take the. So I can see here I'm  getting the data from the window . window.performance.timing  object, and then you can calculate a number of things.  Like, for example, the page load time or the connect time or  the render time, and these are core metrics that it could then  just simply report back to your analytics back-end, and you  can see how the real users in the real world out there  experience my page. And you then see alarming numbers, it's  probably a good idea to get going and improve your site and  get it to load faster or render faster. So that's really cool . But then, this is only one part  of the story. Something more interesting and more important,  actually, is making sure that content in a meaningful way gets  to the user. So what it can do now is with a timing API,  information about key painting events. So when did what  content arrive at the user's device? And there's even a  performance observer, so I can get that notified and react  whenever a certain page happens. I was operating this page on a  slow 3G connection, and you can see here on the console it's  relatively small I think, but you can still maybe see it, the  first kind. So when was the first moment something no matter  what was painted and when was the first moment where  something meaningful was painted ? So, for example, text text,  for example, an image so that people can actually consume  content, not just stair at a gray background or a white  background. So for super easy for API, of  course core metrics, these are core metrics that can, again,  record back to analytics and see if everything is okay. Client hints. Also very exciting topic. That allows  you to adopt and see what kind of device you have to do with.   We have in Chrome telemetry, so we have a feeling of what's  going on with the browser, and we've noticed that a lot of  foreground crashes are actually due to out of memory exceptions . So when the device runs out of memory, the Chrome tab might  crash. So you can see here an Android phone more on the  cheapened of the spectrum. For these kinds of devices, it  might be a good idea to not serve super, super high res,  super RAM- RAM-heavy experiences , but maybe adopt it. So, first  of all, servers can advertise that they support. For this  there's a new code accept-CH,. CH for client hint. And then  it can provide different line hints that it supports, for  instance, device memory. And then it can send the device  memory, for example, I'm a poor ish device, I have .5 gigs of  RAM only, so then the server can adapt the experience  accordingly. And also, you can read  out this information on the client side with JavaScript.  So you can just have a look at navigator. navigator.device  memory and see what is the actual hardware device memory  right now. And you can also see how many corresponds does the  CPU have? So when you program a game, for example, you can  decide to use heavy architectures or textures or the heavy textures ? And also, how many corresponds does the CPU have? Super exciting; right? New features. Which then brings us  to hardware-related APIs. The first API that I want to  mention is the Web Bluetooth API , which allows you to  communicate with nearby Bluetooth devices. In this  demo here, you can see how with his phone, he's controlling  Bluetooth canals. I wasn't sure that this kind of thing exists , but it does exist. Bluetooth canals. You can see why this  would be fun. You can just allow anyone with a device that  is in range to control the experience here. Using the app, so generic profile, and uses Bluetooth  core. So very energy efficient. Implemented in Android, Linux , iOS, and in Chrome wear. Very exciting API. The next thing is Web USB. Which allows you, as the name  suggests, to expose USB devices directly to websites. This is  of course not for any and all US B devices but for USB devices  that are specifically designed for that. And this demo here  created by Kenneth Rhodes, you can see  how he is using an Arduino board to display text on a  screen. So these devices need to be  specifically designed for being used on the Web, and it's  obviously HTTPS only only, so this is a very powerful API, so  you want to be sure that this is HTTPS only and, of course, it  has to be initiated, so you can't just secretly connect to a  US USB device. It's always after user suggestion. Again, implemented on Chrome, Android, and Chromecast. Today, there's one more thing. Today we're launching Web SCSI,  which allows you to have your old SC SCSI hard disk and bring  the '80s back to the Web. SCSI is a small computer system  interface for transferring data between devices. Has a controller right in the browser and defines interface  for access. It is landed in Chrome 64 preview preview. Very  excited about that. This was, of course, a joke. But what I  was getting at is this is not the end. This is just the  beginning. The Web is so great.  The Web is ever evolving.  And if you want to stay up to date, we very much invite you to  subscribe to developers developers.Google.com/Web/ updates for all the great, new things that are coming. With  that, thank you. [Applause]  Hi, everyone,. Thank you so  much for coming. My name is Je n Person. I'm a developer at  Google for the Firebase team, and I am so thrilled for you to  be part of this, my first talk as a Googleer. [Applause] Thank you. Thank you so much.  Yeah,. I really can't think of a better place to do it. I  hope you had as much fun as I have. I've seen some really  great talks and some really awesome training. And met some  of you, and I hope to meet more of you, so thanks so much for  being here. So what I want to talk to you about today is  incorporating Firebase into your existing app, so what I found  is talking to other developers at conferences, you know, you  may like the idea of incorporating the Firebase  database or storage or authentication, but a lot of  developers already have an existing app that has all of  those things in it, so they think Firebase isn't for them.  But I'm here to tell you that there are some features that are  still very useful for incorporating your app,  especially to grow your user base. So I'm going to talk about two  main topics. One is app quality and another is app  growth. I find when designing what to talk about, these things  really went hand and hand to me because a lot of what makes  an app grow has to include a quality app. People aren't  going to be sharing the app with friends and telling them  about it if it's crashing. So I'll briefly spend a little bit  of time talking about the Firebase features that can help  with your app's performance and your app quality, and then  I'll talk about some of our app growth options. So according to a study of one in two star reviews reviews,  nearly half of the complaints are related to prompts  developers can address. So crashes and performance.  Exactly what I was talking about . And since we all tend to  want to be informed consumers, we tend to look at our reviews  first. I don't know about you, but I have not bought anything  for the past, let's say, five years without looking at the  reviews first. So definitely poor reviews can curve your app 's growth. So with that in mind, let's take a look at the  app I'm going to use as a sample to incorporate Firebase.   And it's called friendly translate. And if you've done  any of our code so far, you know that I love things that's  friendly. It uses the cloud API to translate from a couple of  different languages, and it doesn't use Firebase. At least , not yet. So I'm going to show you  friendly translate. And here it is. So simple interface.  Looks a little bit like another translate app that you may know . Has some languages to choose from, and it -- I am glad you are here.  Now you can see how bad I am at texting on a small screen. But, yeah, that's all there is to my app. Back to the slide.   Okay. So even though I have a totally  awesome translate app, it is certainly possible it can have  some not so great reviews. So what do I do if I'm getting  reviews like these in the Appstore or the Google Play  store? These really aren't un like reviews you have seen for  other apps before. I even did a very quick search of the  Appstore and found one that could actually pertain to  something like this. Reading other reviews, I realize that  it's not just me. It has been at least a month or so since I  tried using the app on my iPad, but I couldn't because it  crashed for me instantly. That was a while ago. Again,  definitely something you would not want potential users to be  reading. So sometimes reviews will tell  you exactly what's going on. But you may not know why users  dislike your app. So Firebase has a bunch of features to help.  I'm going to take a look at each one of those reviews I just  showed you, and then show you how a Firebase feature can help  you address that issue and thus , help you grow your user base. Let's look at this one. Always crashing. We see these ones a  lot, and they can have a huge effect on how your app is  received. And so even though I know what the problem is, this  doesn't really help narrow it down. If I had some crash that  I didn't have in my initial testing, it can be tough to do  so if I haven't already found it. So Firebase for a couple of  different resources for reporting and finding crashes  happening in your app. Firebase crash reporting has  been around for a while and Firebase developers like it.  And I imagine that you heard that Google recently acquired  fabric, which provides crash analytics, which is another  report for crash reporting. In fact, if you go to the  Firebase console right now, and you haven't registered any  crashes yet, this is the screen you'll see. And it's pretty  clear from what's written that Firebase is using crashlytics if  you haven't already. It's even a slightly different color. But there are still reasons you may want to use one or the  other. No matter which option you choose, just know that  Firebase will help developers migrate easily to a final  solution that combines the best of both of these. If you're  developing an Android app, in fact, you could just incorporate  both. On iOS, you do have to choose. So while there are good reasons for both of them, I wanted to  try out crashlytics. It can help you and find resolve  problems quickly. And it can minimize the crashes for your  users by focusing on the ones that are impacting most of them.  And it's available on iOS and Android. Now, Firebase -- other pardon me . It will give me deep insights  into what's crashing in my app, and it will send me  notifications. Here you can see there's been a crash in  friendly translate. So let's take a look at just how fast it  reports on crashes. I happen to know there is a crash in my  app, and I'm going to make it crash for you right now. I'll  take a look at crashlytics shortly after. So here I have my app. After a  moment, we'll be able to see that in the console if you want  to pull up the computer. Thank you. And we have our crash reported. So it really is that fast.  You can really find out what's going on in your app in  realtime. And because it aggregates this different  crashes, you can see what this trends are. Now, what about a review like  this one? The app worked quickly for me on my new device  and my fast office Wi-Fi. This is something that you may have  heard before. But depending on where your users are located  or what kind of devices they're using, they may not have that  same experience. And certainly, try to test in several  different scenarios, but you will not necessarily patch very  specific issues that way. So I can monitor and address  performance issues using Firebase performance monitoring,  and it gives you actual insights into your app's  performance from a user user's point of view. And it was  released in beta at IO this year. Performance monitoring can tell us how the app performs from  your user's perspective, and it runs in the production version  of your app with minimal over head overhead. So you can rest  assured that it's not bogging down your app in the process of  trying to improve it. So performance monitoring offers  a couple of features right out of the box. Automatic tracing,  and you can create custom traces. And also, automatic  HTTP and HTTPS metrics. So a trace is a report for a  period of time that is going to have a defined beginning and  end. And there are some that are done automatically like a  cold app start, how long your app is in the foreground, and  how long it's in the background . HTTP and HTTPS traffic metrics include response time, which is  going to be from the moment of indication to the final fight  that's received. Payload size and bytes, and success rate.  So it's going to take a look at your status codes like 200  types being successful and 500 type errors, and it's going to  help you narrow down if there's a specific server or network  issue that's causing that. It also includes URL pattern glot ting. So, again, you're aggregating the different URLs  so that all of these will be considered the same structure so  that it gives you an aggregated view of how your APIs  are performing. And the transaction metrics it  includes are app version, device, OS level, carrier, and  radio, which I'll show you what that looks like in the console . Now, this is a sample of an app that is live and uses Firebase  performance monitoring. What I want to briefly show you,  besides the traces, including the defaults and the custom  traces is that deep insight you can get is network request. So  here, if I want to find out more about what kind of response  time I'm getting, I can have it, again, broken down by how  long each carrier is taking, and the app version, and what  device it's on. Really narrow it down if I'm having some issue , sometimes it can be just a single carrier that's causing  it, and this really helps you narrow it down. All right. Thank you. Okay. So sometimes your  feedback is not particularly helpful. And not to mention  that very few of your users are going to be giving that  feedback. They're not reporting the issues, they're  just unin uninstalling the app. So how do we find out what  users like and don't like about your app? That's where Google   Analytics comes in. And what I love about Firebase, it's un limited. So you have this option when you incorporate  Firebase into your app that gives you reporting on up to  500 distinct events, and you can have custom events with custom  parameters, and you can view all of that data, including how  it connects up to other Firebase features, for instance,  if you use crash reporting, you can incorporate that into  your analytics. Take a look at your dynamic performance,  notification, in app purchase data, and see how your campaigns  are performing. Oh,. Right. Next feature I  want to talk about is remote config. This is another way  that -- this is another feature that works well with analytics,  really narrows down what people like about your app, what  they don't like about it, and helps you give a custom  experience to your users. So remote config lets you change  the behavior and appearance of your app without pushing an  update. So you don't have to depend on your users to do so.   So if you're anything like me, there's at least 12 updates  that need to be done. Especially if you have  something important lake a crash that you want to resolve by  removing the feature, you don't want to have to wait for them  to them to update it. You can use the Firebase console to  override default values, so you can have default values for  different scenarios, including user properties. App versions,  operating systems, audiences that you define in Firebase  analytics analytics, it's really up to you how you want custom  that configuration.  your app's color theme to  match a seasonal promotion or for a sale. Or I can use  remote config to change what the default language is going to  be. On one hand it's possible that in a certain country, they  may want to start with whatever language is native to  that area. Or maybe a lot of people using the app in that  area are visiting, and they want to translate something into  that language. So depending on where somebody lives, you may  want to have a different default language. So here I can use remote config to change what the default  language is and use the region where the device is located to  decide which language is shown as being the original one. And  I can show you that here. Bring up the phone? Is it possible to bring up the  phone? I'll just hold it real high,  so you can see. Well, come up and check it out later because  I'm pretty excited about that one, and I'll show it to you. Okay. So according to a study by local analytics, 23% of  users abandoned an app after just one use. So those few  first moments that a user interacts with an app are  critical. You don't want to become part of those statistics . So Firebase provides some great tools to make the  experience as easy and as enjoyable as possible. And the  ones I wanted to talk to you about in a little bit are app  invites and dynamic links. Also , to go with app insights and  dynamic links, 82% of Americans asked for recommendations from  friends and family. And that's even higher at 92% for 18 to 24  24-year-olds, which is why these features are so powerful. So app invites allow you to share referral codes or content  via SMS or e-mail. And it really is an out of the box  solution that enables you to send invites from your actual  users to their friends. And who can give better  recommendations than people who already enjoy your app? And  with a few lines of code, you can send out those invites. And this can include dynamic links. So rather than just  going to let's say the default page in your app, if there's  some specific contents that that user would be most interested  in, you can have a link that sends them there and even  survives the install process. So let's say you've fixed your  performance issues, you solved your crashes, and you have made  it a much better experience. How do you reengage those users  that haven't touched your app in a while? Firebase cloud  messaging makes it super easy. So Google Cloud messaging sends  out so many messages every day, and they are nearly instant and  very reliable. Firebase cloud messaging -- let me -- Firebase cloud messaging  makes it really easy to send out messages that are directed  to specific groups, rather than just spamming messages to all  of your users. Oh,. Yeah,. [Applause] So let's say that this  particular user in this case, me, is not very interested in  using the app anymore because every time I try to pick my  favorite language, it crashes. But now that crash has been  resolved, I can send a message to a specific group. It can be  based on properties or events, and I can just say back to translate. Because we can switch back and  forth too fast, you may not even see the notification  because it can come through very quickly. And there we are. Come back to  friendly translate translate. Invites me to come back, and it  doesn't have to be a simple message like that it can include  links that you want them to be in your app. This is a simple   example. So, again, you can use Firebase  cloud messaging to invite users to drive user engagement and  invite them back to use your app. You can send messages all  the way down to a single device , groups of devices, have topics  that they're subscribed to. There are endless possibilities  to reengage your users and, again, without spamming them by  making it simple for you to choose specifically who that  notification pertains to. There we go. And there's one  last thing I wanted to show you that incorporates analytics  using Cloud Functions for Firebase. Because I'm a huge  fan of Cloud Functions for Firebase. If any of you have  seen my video, you know I love to talk about them. So I  couldn't help but include an example. So I wanted to  briefly show you some code with how you can incorporate  analytics events to reengage users with your app. So what I've done here is create an event that is triggered  when the app is removed. And, again, this occurs in the  server side and not in your user 's app. So in order to have  this event trigger, just make sure that if you're using  analytics events trigger for Cloud Functions for Firebase,  it has to be conversion event.  So you may have to go to the  console and make sure it's triggered as a conversion event.  So if an Android user removes the app, you can log that event  -- or it is logged automatically, and you can send  them an e-mail, and you can invite them back to use your app . Now, in this case, I wrote out  verbatim. I wrote out specifically for each language.   You could even incorporate the translate API here as well if  you wanted to make sure that you're translating to whatever  location the user is at, what language they would be using.  But in this case, I figured it may make sense to hard code  something because if some sort of faulty translation is the  reason that they stop using your app, you wouldn't want to  incorporate here as well. You would want to make sure that  everything was translated in a way that looked right to them.  In this case, I don't speak all of these languages, so they may  not actually be the best translations. So, again, using the -- sending them an e-mail, logging the  event, and that way any time a user decides to leave my app, I  can invite them back, and I could even give them some sort  of discount or motivation to come back. And the slides again, please,. All right. So after making some  great changes to my app, seems like people like a lot more now . Five stars with translate again, and it translates so fast  and so easy to use and fun. So these are all features that  you can incorporate into your app, even if it's already up  and running, and that's actually the best time to use them, to  really get your idea out there. I mean, you've already spent  the time to build something and come up with a great idea. Why  not really make sure that the users who would enjoy it are  actually getting to see it? So here's a summary of the  features that I added.  Crashlytics, performance  monitoring, remote config, Firebase cloud messaging, app  invites with dynamic links links, and because I can't help  myself but talk about Cloud Functions, I also added a brief  example for using Cloud Functions with Firebase with  node Mailer to send out a message whenever a user removes  an app from their phone. So thank you, all, so much for  coming. And I hope you're having a great time here at GDD  Europe, and I hope to see you next year. I'm Jen Person. If  anyone else has any questions 