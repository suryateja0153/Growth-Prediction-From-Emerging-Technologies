 - Computing ethics has long been a subject of strong interest to ACM. Every day, it becomes more evident that the reach of computing technology spawns new ethical issues. So our next panel will take a penetrating look at that subject. Please welcome director of the Berkeley Center for Law and Technology, UC Berkeley associate professor, Deirdre Mulligan and her panelists. Are you ready? - [Woman] Time, or is it already time? - [Deirdre] Thank you. And am I gonna invite them up? - [Wendy] Everybody up, everybody up. - [Noel] Oh yeah, sure. - Okay. - [Wendy] Have we lost somebody? - I don't see Helen. - Hey. Noel. - Who are we missing? - Lovely to see you. - Helen. - [Noel] Yeah, yeah. Yeah, Helen. - Woo, those lights are strong. - Oh God, the lights are bright. - Where's Helen? - We are just waiting for one more panelist. No, sorry, I don't know if you can appreciate how bright the lights are up here. Okay, well, I think I will introduce the panelists who've arrived already. All of them, their list of accomplishments and awards are so long that we could literally occupy the entire panel if I gave full due to their bios, so I'm just gonna introduce them very briefly. To my immediate left, Jennifer Tour Chayes is an interdisciplinary scientist and expert in network science and the inventor of the field of graphons, which are now being used in machine learning networks at scale, and she both founded and leads Microsoft Research New England and Microsoft Research New York City. Next, we have Noel Sharkey. He's a professor of AI and robotics and of public engagement at the University of Sheffield, and his research interests include biologically inspired robots, cognitive processes, many things, but his current research passion, as I think many of you know, is the ethics of robot applications. To his left, I have Dr. Raj Reddy, a Turing recipient in 1994, and he's the Moza Bint Nasser professor of Computer Science and Robotics in the School of Computer Science at Carnegie Mellon University. And his research interests also are enormously expansive in AI and cognition, computational political science, digital democracy, and today, most importantly thinking about technology in service of society, including its values. And Helen Nissenbaum, I'm going to introduce, despite the fact she's not on stage. She is our ethicist today, and she's a professor of information science at Cornell Tech, currently on leave from New York University. And Professor Nissenbaum's work spans societal, ethical and political dimensions of information technologies and digital media, and Helen is one of the few ethicists that actually has participated in building privacy-enhancing software. And so you can find Helen's software, such as TrackMeNot and Ad Nauseum that try to actually bring privacy protection into the internet ecosystem. So it is an enormous pleasure to be here with this incredibly impressive group of individuals to talk about some of the ethical issues that are facing us today. It's always a pleasure to come to an ACM conference, and I've had an honor of working with people like Barbara Simons, who's been a longstanding member of the Public Policy Committee of ACM, but I also bumped into Jeremy Epstein and to David Jefferson, and other people who I got to know really well, because they all were so active and continue to be active in questions about how to use electronic systems in the context of voting. And for many of you, this is back in the news today, but seeing the ACM community and technologists lean in very seriously as state and local governments and the federal government, many years ago, were looking at the transition between paper based voting systems and fully electronic systems and optical scan systems and mixed method systems, and in very many ways, completely unaware of the limits of the technology and watching them engage. Here in California, I got to participate on a top-to-bottom review committee with people who were interested in looking at the hardware and the software, and we were reading the manuals and really leaning in, and flash forward to today, and not only are we looking again at the ways in which technology was used in voting in ways that were not necessarily attentive to the way in which it changed the risk profile, right, and the social and political consequences of not attending to those in not just the development but in the deployment. And we're looking at things such as machine learning systems and other sorts of decision-support systems becoming a part of the fabric of government and the private sector in every part of life. And so my opening question is where you are developing all of these very powerful technologies, and often the communities that are using them, the local government, the state government, the courts, the justice system, generally, don't appreciate their strengths, their limitations and the context of development. And so having watched this community deal with voting issues and testify and participate at the local level and the state level, what do you think the next steps are for that level of engagement with those who are gonna be using your systems? - So I'm coming more from the technical side. And I think it's really important to understand that in AI systems and in machine learning systems, there are various ways in which the systems can be biased. There are ways that involve the data itself. So you can have a fair algorithm, whatever that means, and I'll get to that in a moment, because nothing is truly fair in all senses. And yet, it learns on data that is, it's supervised in its learning, on data that is generated by human beings with biases. One of the things I'm heartened by is that there have been few cases, and I think there will be many more cases, in which we can learn to go in and de-bias some of that. Adam Kalai, who's at my Boston lab and a few other people, went in and looked at word2vec, which is a way of, it's a 300-dimensional space, which is used by search engines and many other pieces of software, in which if you put in a word, it can broaden it for you in some way in this 300-dimensional space. Adam looked at word analogies, okay, and so, you know, he is to brilliant as she is to, and it came up with lovely. He is to computer programmer as she is to homemaker, I mean, just horrendous things you would not want to have to come up if you were looking for a computer programmer for a job, if you're thinking of what to say about somebody and when you're writing a letter of recommendation. I'd much rather be called brilliant than lovely. In fact, very few people would call me lovely, to tell you the truth, which I'm proud of. Those who know me know this. And so Adam found a way of going in there and looking, there are cases in which he is to, you know, to father as she is to mother. You want that to remain. But you don't want he is brilliant as she is to lovely. And he found ways to de-bias this. Now it doesn't mean that he has the ethical training to figure out in what way to de-bias it, but you, with your notions of ethics, can go in there. So that's de-biasing some of the data problems. And then there are also problems with algorithms themselves, and I think we'll probably be talking more about this, but there are ways in which when you're trying to make a simple binary kind of decision, should I give this personal loan or not? Should I set a high bail or not? You want to cluster people in various ways. And let's say it's a really simple clustering, give them a loan, don't give them a loan. If you look at a majority population and don't look at something like waste, then what happens is that you may have a certain clustering that's chosen, you know, some hyperplane that separates yes from no, which makes sense in terms of the majority population, but maybe the minority population, because of other things that are going on, is a little bit skewed, and the judgment should be different, but it's minority, so when you end up setting the hyperplane, the impact is completely different on the minority population. And so there are people who are now thinking about how to take care of these things. So what I'm hoping to see as we go on is that we will develop tools which will allow the people who need to come up with these policies and who need to execute on these policies to take into account these disparate impacts. - So Raj, I think this is, oh, I'm not sure I'm on. Ah, there we go. So Raj, this seems like an interesting place where you might have some stuff to add, and I want to give, to kind of stretch out the question a little bit. So I was recently reading the brief by the US government, and in a case that's right now on, there's been a cert petition, so a request to the Supreme Court that they take the case. And it's a case called Loomis v. Wisconsin, and some of you may be following the case. It's about the COMPAS system that's used in making decisions. It's supposed to provide some recidivism risk score. And the case is super interesting in several different ways, but one of the things that's most fascinating to me is that the Wisconsin Supreme Court basically required the system that when it's being used as part of a pre-sentencing risk report, that it include a statement that explains to the people who are relying on it, that it's a proprietary system. So we haven't been able to examine it, that it groups people into categories. So it's not actually making an assessment about a specific individual, that researchers have raised concerns about the way in which it might re-encode bias and particularly have a negative effect on certain parts of the population, that it was trained on a national, not local, sample, right, that it was trained over everything and is being used in a particular locale. And to me this is super interesting, right, that the way in which the court is looking at the limitations of the software is not to actually ask for the software to be something that the government can look at, or to suggest that maybe we shouldn't train software on one population and then use it to make important decisions about people in another population but rather to provide a whole series of disclosures. And so I'm wondering, Raj, if you were going to talk to the court, what would you advise them to do? - Before I answer that question, the big question you wanted us to address is what is the role of technologies in informing the government? There are two topics I'd like to kind of raise, and then we can come back to the court question. The first one is we need to identify technological solutions to societal problems. I believe we can. I gave a testimony in front of Congress, around '95 or something, '96. They were concerned about the hacking that was going on at that time. I said if only we can build self-healing systems. Every system we build must have a stamp saying no matter how it gets hacked, it will kind of wake up and keep running again, no matter what. I believe we know how to build those hack systems, but that's never been a high priority for system designers. So designing self-healing systems in every system that you design might be one of those. The second one is kind of a futuristic one. As you might know, there are more slaves in the world now than there were slaves in the US during the Civil War, prior to the Civil War. These are kind of different kinds of slavery, modern slavery, where you are actually being held against your will for various kinds of problems. All you have to do is go Google modern slavery, you'll find it. And I've been thinking about what is a technological solution that you could find for slavery. And I believe we have a solution. It requires the  courage and determination of the governments to do it. Already half of the population of the word, given appropriate privacy anonymizations, if you can give me time and location, that's all I need, for everybody on the planet. Right now, I can't do it, but I can do it for half the population. But if the governments were to give everyone a smartphone, and everybody's time and location was known all the time, there's a privacy issue. I'll be happy to come back. I'll be happy to come back and address that. I believe there are solutions to that problem. Then you can actually determine who is maybe being held against their will, because their time and location would be more or less the same throughout. Whereas the rest of us will be moving. This is not the only societal problem, but this is a good example of it. I believe, what I'm kind of trying to say is, I believe there are technological solutions for every societal problem. It only requires us to identify and think about it. Now coming to the court, if there are technological ways they could have come up with a better decision, they need to know that. Then they at least need to know to ask. I think the current government is asking our technologists to say, "How can we make a better government?" You could actually take a special case of that for the courts, and say, "How should we organize "so that courts can provide better decisions "along the whole spectrum." - So Helen, do you think every problem is susceptible to a technological solution? - Firstly, I want to say I think it's amazing where we've come to, today, because when I first started out in this field, it was then called computer ethics, and now you know we've gone through various evolutions of the field. It was, ask the philosophers, I believe you made the introductions already, okay, who were trying to say to the computer scientists and engineers, you know, we cannot do this alone. We bring a certain way of thinking. We bring a skill set, but we actually need partnership with computer scientists and engineers in order to do the job well. So to hear computer scientists say, "Oh, we can solve social problems with technology," that feels fantastic. Now, but, here's the but You gave a solutions that then said well, you know, phone and yes, of course, there are gonna be these privacy issues, and maybe unbeknownst to you, that is the crux of a lot of ethical thinking, which is we don't actually know the solutions. We don't know the solutions to the social problems exactly, and there's this analogy in my head, which is if you're an amateur, and you're trying to fit a rug in a room, and you kind of think, "Oh, that's very nice. "It goes to the corners. "It fits very nicely. "Oh, there's just this spot. "Well, let me just tug at this spot over here." And then you find that the rug comes up in the other corner, so this is the nature of ethical thinking and ethical problems. So you say, "I'm gonna solve it in this way," but oops, there is this privacy problem and so on. And so a lot of the deliberation is of a practical nature. So how can we overcome the privacy challenge? But then it becomes a different kind of challenge, which is how do we compare these values, one against the other? And I think that in this morning's panel, we were trying to talk about security and privacy. Part of the work that needs to be done is to actually figure out what the solution is to the problem, and then we can see whether technology can take us to the next step. And so I think it's the whole, I mean, I don't know if this is the moment to get into the values and design approach, or maybe you want to pause and-- - I think it's a nice hand off, actually, to Noel, because you've been writing recently about robots that are actually being designed for compelling social purposes, right, things such as robots for care-taking, in the context of the elderly, or to help children as they're developing, and in the educational context. And yet, as I think Helen just said, you've found that when you pull the rug a little bit, right, it raises some other set of issues. And I think part of it comes to that question of how you define the problem, right. Is the problem defined as we need somebody to make sure you take your medicine every day? Or is the problem that we're trying to solve when we're introducing a robotic healthcare provider, should it be contemplated on a slightly larger way? And could you talk a little bit about what you're concerns are, and what you think the kind of limitations of our current conceptions of the problem might be contributing to some of those underlying ethical unease that you feel? - Well, I think, first of all, let me say that I really don't think there are gonna be technological solutions. I'm completely against that, because how are you get technological solutions to bad technological solutions? That's one of my problems. So as I see it, there are so many problems now, because we've got industrialization of AI as the big change, industrialization of robotics on a very large scale. And really what technologists need to do, I would say, is to really work very hard and think very hard of the kind of problems that are gonna be created by the technology and not always say, "Humans are bad at this. "Therefore, machines are going to do it." Because that doesn't follow at all. It's illogical. Because humans are bad at something, getting machines to do it might make it even worse. I mean, I look a lot at, well, everything, essentially, but we mentioned elder care, for instance. Now in elder care, technology could be absolutely amazing. I mean, I'm looking forward to my exoskeleton suit when I'm in my 90s. It's gonna be really good. Interestingly, when just mentioning age and bias, one of the things I really love about coming to the Turing things is that almost every other event I go, I'm the old guy. And I don't feel that here. Now that's the kind of bias we want to eliminate. So in elder care, so you get your exoskeleton suit, but then there's people just talking about, you know, there's problems in care homes, so let's put robots in charge of care of the elderly, and that seems awful to me. I don't want to be in the hands of machines. I don't want some stupid chat bot being my companion. And you might think they're going to get a lot better, but I'm not sure. I mean, I've been looking at them for a very long time. I don't want to be sitting there, being driven crazy by these. I mean, the latest thing I've been writing about is the plans to bring sex robots into elder care homes. You know, that's really a wonderful idea, you know. I mean, I'm not sure it's a bad thing. You know, I don't know. I mean, it's hard to know. I'm not old enough yet. Maybe if I'm completely Alzheimer's, I'll think it's great. But there's all these problems. We've really got to try to fix it. We've got to work with social scientists and lawyers. And I set up the Foundation of Responsible Robotics, along with Aimee van Wynsberghe. We're at the Hague Institute for Global Justice. And we're working at taking all this stuff that's been going on for years in computer ethics, really in robot ethics and trying to turn it into action, because otherwise, it's just all talk. It has to turn into policy somewhere. And I think it has to turn into international policy, not local policy. We need our governments to act and get them to do some joined up thinking about how all these things are going to fit together. I mean, if you look at robotics, you've got drones overhead, and there's so many applications for them, including Amazon deliveries. Now I'm worried that I'm never going to see the sun again. And now we've got them on the ground, deliveries on the ground. All over London now, they're starting. I'm going to be tripping over these things. I'm gonna be getting pizza in my hair. What you'll end up thinking, I'm gonna walk in somewhere, and it's gonna be something like the Pepper robot is gonna be the receptionist or gonna be the salesperson who's monitoring my heart rate, looking at what I'm interested in, looking at my pupil size and doing all this stuff, and you taking advantage of me. A business model now, a lot of the time, is you produce these incredible apps, and the business model is to sell the data. And the big thing that you were talking about, and that's the one that worries me is things like gender bias, age bias, racial bias, and it's coming up very strongly. And I don't know if you're gonna find the solution. You've really got to work at it. But what seems to be happening, if you use big data all the time, what you're doing is you're ossifying society's values, in fact, stepping back a bit. And really, in my lifetime, I've seen that we have evolved our values. And we don't want to be at a standstill again. We don't want to take your votes away. That's what these machines will do. And so, in summary, I think we need to look at the international view. Now I'm working very hard to try, at the UN quite a lot of the time, but I'm really trying to get the Human Rights Council to look at this idea of having a bill of human technological rights to try to protect this and make sure that humans are in meaningful control of the technology. And the bill of human technological rights is so that you can future-proof, because at the minute, law can't keep up with technology. So we make it about us. What can technology do to us? And that's what I'd like to see. - Can I come back to the COMPAS case? 'Cause I think it's really interesting in bringing some mathematics, some logic to it, points out that you can't get out of it with mathematics and logic, and that you do need to rethink it from an ethical perspective. What goes on in this system is there are 10 different bins, and somebody comes up, and they're gonna get sentenced, and you say either they're very unlikely to recommit a crime. They're down at number one, all the way up through number 10. They're very like to recommit. And so you want to set bail high. You maybe don't even want to let them out on bail. And if you look at that, what happens, and so this is what the people who came up with this COMPAS algorithm said is, okay, so race is not one of the 100 factors that go into this. But of course, it's highly correlated with other factors that do go into it. But they said, "Look, this is perfectly fair. "If you get a number seven, "if you're black, you have a 60 or a 61% chance "of recommitting, and if you're white, "you have a 60 or 61," one is 60, and the other is 61. So Northpointe says, "That's fair, isn't it?" And then ProPublica went in, and they looked, and they said, "Well, what happens is that "if you get a seven, "and you are black, "you have actually half the chance of recommitting the crime "as if you're white." I mean, how could that be? That sounds like it's logically impossible. No, it's actually logically possible. In fact, it's logically required by the fact that blacks recommit at twice the rate of whites, for all kinds of other reasons. And so, you look at these two outcomes and Northpointe says it's fair, and ProPublica says it's unfair. And it's very clear that nothing is completely fair there. And so we need to go back. We need to have ethicists like Helen, and we need to have people like Deirdre asking which kind of fairness do we want, and how important is it that we have fairness rather than accuracy, and maybe the whole idea of setting bail is unfair, and we should be doing something else. Sharad Goel and a few other people wrote a very nice Washington Post article on this, in which they reanalyzed the data, but I think the important point is that when you actually go in with a computer science lens, with an algorithmic lens, you realize there isn't a simple solution of this problem. In fact, you really need to return to the ethics and to the question of regulation and see what the right thing is. So I think that's a great example of how the more we look at it with a mathematical lens, it's the more we see. It has to come down to an ethical question. - So this would, I think, be a softball down to the other end of the-- - [Helen] Thanks, Jen. - The stage, and so Helen, I think you came in a little late. I was talking about the way in which the Wisconsin Supreme Court, in dealing with the use of the COMPAS risk assessment scores, that were relied on, not for bail determination, but they were part of a sentencing conversation, but were supposedly only used as confirmation of other things that are allowed to be used, rather than an independent basis. But they set out this whole set of things that have to be disclosed with the report, including things such as it's a proprietary system, so we haven't been able to look at it, and it was trained on a national population but used locally, so a whole set of things. And the question, I think, for you being what would it mean, regardless of which definition of fairness we wanted to choose, what does this kind of translation by Northpointe, right, that there's some judgment of theirs that's going on and the way in which that might relate to, for example, the court and their idea and commitment to fairness, as an ethicist, what are the new issues that you see emerging here? - I have to be like all the politicians you see, which is like slightly tweak the question in order to be able to answer it, if you'll just bear with me a little bit. - Except the politician doesn't tell you their tweaking it. - [Noel] That's exactly what I think. - So you're not, you're an academic, not a politician, 'cause you're honest. - Because it's so, you know, Raj's, kind of ambition, for technologists, for computer scientists, to take responsibility for social issues, I think is... I remember this Alvin Weinberg article, which talks about, you know, he's a physicist, Nobel laureate, saying we can solve social problems technically. And I have concerns about that, but I think there's, lurking in the concern, is maybe a way forward. And it does eventually get to the question you put to us, which is that, at the very least, can technologists help us in society, resolve some of the societal problems that technology creates, or not that it creates worse than humans create, but some of the questions that technology puts in front of us, can a technologist help us with those problems. And so if you take, I mean, the COMPAS system is not, at first, I used to think COMPAS, the system, was some fancy machine learning. It isn't quite that, but we could imagine we have this intelligent system that's helping judges make certain decisions, and then we find that they do this strange thing. What lies in my view at the heart of some of these problems is that, and actually, I loved what Barbara Liskov said this morning, and I'm going to kind of tweak it and use it here, where she said that what Newell and Simon, the contribution they made was to underscore that you needed a way of thinking about human cognition that was meaningful computationally, and then you could make that connection. So they performed this bridge between the study of human cognition on the one hand, and then AI, let's say, or modeling human thinking computationally. And that move was crucial to moving forward. And I think that we're in a similar potential space at the moment, and there seems to be a lot of will on the part of at least the technology community to try and find that bridging, to say we, well, we actually, we don't, even concepts like privacy and accountability and fairness and justice and so on, these are not easy concepts for anybody. But we're trying to say is there some bridge we can build to take these concepts that are values concepts, that philosophers and ethicists and legal scholars have been thinking about centuries long, and see where there's a way to bridge them, to make them computationally meaningful and the other way around. So to take what the COMPASes and the various delegation of certain tasks to robots and intelligent systems and say, "Oh, this is what it means "to give care to someone." And we're saying, "Does this map on "to our concept of caregiving?" It seems there's something missing here. So there's so much work to be done in this bridging space, and we need, I think, we need both sides to be part of that effort, and we'll make progress on these sorts of questions. - Yeah, so I think-- - Just let me say-- - Please, yeah. - So I just wanted to say one sentence. Robots are very good to help you with the tasks of care but not with the practice of care. That's human and human meaningful. I just wanted to slip that in. - Yeah, we'll pick up on that. So Helen raises this very important point, right. The idea that we can work with values questions, right, requires a lot of conceptual work. We actually have to be able to provide some kinds of formalizations of these concepts and then hopefully be able to take those formalizations and identify properties of systems that might support them. And I, like Helen, have been super excited about the willingness of the technical community and the real, kind of, rolling up of the sleeves on issues around particularly fairness and privacy. And whether it's differential privacy and all of the work that continues to swirl around that particular issue or the growing community around the fairness and transparency in machine learning or NIPS workshops around fairness issues, we see an enormous amount of energy. Yet, yet, there's the question about once we have techniques, right, we know this from security. Many people who work in computer security like to say, "We understand how to make systems more secure, "just nobody has the will to do it," right, that they don't want to invest the money. They don't want to invest the political capital. Our election didn't have to be so easy to hack, right. And so there's this question about how we ensure that we see the same sort of tech transfer on things that are kind of the social and ethical pieces of computing. And doing some work with Cynthia Dwork, looking at implementations of differential privacy in the field, and one of the things that's been really heartening is the extent to which the privacy push has come from the engineers themselves, right. It wasn't the lawyers or the privacy people or some external. It was actually the engineers who were like, "Oh, this is a concept of privacy we can work with. "This is part of our work." But I'm wondering what sort of scaffolding do we need in the world of practice to make sure that the ideas that are happening in the lab, that would develop privacy and develop fairness and address a definition of care that was robust about care-taking, not just caregiving. What are the things that we need to do in the external environment so that there's a pull of all of this research so that it actually ends up deployed and not just as toy systems? So I was wondering if the three of you could talk to that. - So first of all, I find it interesting for the theorists who are out there, differential privacy and the early notions of fairness, of Dwork, Roth, et al, are worst-case. And so, in a sense then, they're best-case for the individual, because what it means is that if it's differentially private, it's very safe for you as an individual. Those kinds of things though are very hard to do. It's hard to get differential privacy. It's hard to get the analog of that for fairness. There are other notions that some other people like Sorelle Friedler and others have come up with that are more on average and based on some legal concepts about disparate impact, and they're more on average. So they do map well to concepts that computer scientists are rather familiar with. Now it's interesting, 'cause you say that oh, the computer scientists have been willing to do this. I think there were always cryptographers and security people who are willing to do the theory, and the problem was that people didn't want to implement the theory, because it was too expensive, and it was too difficult. One of the fears that a lot of people have is oh, if something happens, and there's a big privacy violation, then there's gonna be regulation, which isn't very well thought out. And so we hope that there's research here that they can pull on, but what's happening is that, driven by the European Union's stand on privacy and what I believe are gonna be other stands on liability of companies if the output of what they do is unfair, and I'm not just talking about the Microsofts and the Googles. I'm talking about Microsoft's Enterprise customers who might be making decisions on whether to give car loans to people or something like that. I think there's gonna be similar liability issues that are going to make us all a lot more open to taking these solutions that people like Cynthia and others have come up with, in collaboration with the social scientists and implement them, because there are going to be huge costs to enterprise if we don't do these things. - I have a much bleaker view of privacy, I think. But before I say that, I just want to say that this idea of selecting, what you mentioned before, selecting a racial characteristic and saying there are twice as many of that risk that are gonna do this event than another is to me essentially a racist thing to say. - It's the, yeah? - Because, it doesn't matter, because the law should be about the individual, not about the race they are, regardless. Those statistics are crazy. You know, I'm an individual, so you find out that guys with white beards are much more likely to shoplift, but I want them to look at my specifically, at my case, at my relationship to my family at this time. You know, I've got small children, I'm less likely to be a recidivist, so that's what I think. I mean, but that's my kind of proof. - You understand they're not putting race in, at the beginning. It's coming out as a correlate-- - Yes, I know, I know. - Of other characteristics. So I mean, that's what's so difficult about this, because they are doing the fair thing and not putting it in, and yet, the impact is disparate. So it's coming out, and so, you know, when human judges have made these errors or these unfair judgments for-- - So let's not compound it with machines doing it. The thing is that you're saying you don't put in that the person's black, but there's a lot of things you can put in, their zip code-- - That are proxies for that, absolutely, absolutely. - Which side track they're on. So it's quite clear. But your question was about, sorry, I lost it. We were talking about.. - How to make it happen. - [Deirdre] Yes, how to make it happen. - How to make it happen. How do you take everything that there is? - Oh, privacy. There's a bleaker view to privacy, 'cause I don't know. I don't get this thing about engineers driving privacy. When did that happen? The big problem with privacy, I think, is not the engineer's fault or the cryptography or whatever you're using. I think it's about our governments, really. I mean, you've gotten, in this country, and now you've got the point where the government can look at all of your internet data. In the UK, we have the Snooper's charter, so my service provider has to keep all of my data for a whole year and then can look at this. And it's all very well now when our governments, well, I'm not sure about the US government now, but the governments seem to be a little bit benevolent. But look at the power that that gives you when you can look at everyone's data. Look at the possibilities of control that gives you. When do you become a dissident? Well, whenever somebody says you're a dissident. But if you have a look at human rights law, everything's very vague, very, very vague. And you could track us all down for our, what do you call it? Our political views. There were 200 million data points stolen. Where was it? They were on an Amazon cloud. They were being kept on an Amazon cloud. And it told you for 200 people in this country, you now know were they pro-abortion or not, what were their political affiliations, who are they most likely to vote for. Now all that information is power. And it's not about engineers keeping things private, because you can keep it private for your government, but your government can read the data, and that's a real problem for me. - So there is a-- - It's a bigger social problem than just, you know, keeping our data secure in computers. There's a lot of stupidity with that. - I think people do sometimes like to scapegoat the tech, right. Like, it was the technology's fault. It was the engineer's fault. They were acting unethically, or they weren't paying attention, or the VW scandal, oh some engineer, no, it turned out it was not just some engineer. - Yeah, of course. - That it started a little bit higher. And certainly sometimes technology is going to replicate the way in which we behave in the world, which perhaps it doesn't reflect our values, or as I think you said, at least doesn't reflect today's values, right. And the question about is it really the engineer's fault that that's the way in which the math works? But the question of how we negotiate the hand-offs between law and technical systems, and I think the secrecy and privacy issues that have arisen with government spying is a great example, that for many years, I think, many people thought that yes, okay, we'll encrypt things when we really think that there's a highly-motivated adversary, but for most of us, we have nothing to hide, and the government, they have a bunch of rules to follow, and they know they shouldn't spy on all of us. And then it turned out that perhaps the government was not necessarily doing what we thought they ought to be doing. And so we've seen the technical community decide that we should encrypt the back hall, and we should encrypt things more frequently, and it should be rolled out, and data and storage should be encrypted. And in many ways, there are governments all across the world that are not so happy about the fact that the technologists actually have taken on greater responsibility for doing what they think is ethical, right. And I think there was an earlier conversation about that. Of course, it begs the question about whether or not that new configuration with technology carrying more of the water for keeping our secrets is consistent with our ethics, right, in that there are always circumstances under which the government can request access to information and obtain it, and by encrypting it, we may make some of those things more difficult, and sometimes, impossible. So in that hand-off between reliance on law to provide a protection, or a reliance on technology, some of the nuance and some of the flexibility might have gotten lost. But we do see technologists kind of stepping into the fray, I think in a very forceful way. And I think the question is does it require, kind of, government acting lawlessly to roll out privacy-preserving technology? Do we need a similar sort of really stupendous event to get different approaches to fairness rolled out? Or are there other sorts of structural things we can do, not just pass a new law, and you may be more optimistic about new laws being passed than I, the lawyer, but are there other things that we can do? Are there educational approaches? Are there structural approaches within companies? We've seen great attention to building out infrastructure to get people to think about privacy. We've seen approaches to get people to think about security, whether it's about security requirements in IETF, RFCs, right, that you have to consider the security issues. What other things can this community do for itself or help us do as a society to make sure that our ethical commitments are not afterthoughts but are actually things that are moving at the speed of technology? What can we do? - We could be responsible, for a start, about what we create. But you know the privacy thing is getting really bad. We could sign our privacy away, every day. Every time I buy something, I'm giving data away myself. You get things like law can be very, very strong on this. Germany has been very good recently, because I don't know, you've probably all got your Internet Barbie and play every evening or something, but you have these Internet Barbies, and you buy one for your child, and essentially, you look at the privacy agreement. For the Barbie to work, you sign away your child's privacy completely. It will talk to your child, collect the conversations, put them on the internet, and that has been hacked already, by the way. Put them on the internet, and then it says that it can ask the child a series of questions about their preferences and then selectively advertise to your child. Germany just said no, and they're banned. And I think that's a good thing to do. So the law can be very strong about that and do the right thing. I would close things down. I mean, if you've got an algorithm now in Germany, or in Europe generally now, there's a new law saying that if your algorithm makes a decision that impacts on my life, and I ask how did that algorithm work, what's the causal structures, not the statistical structures, how did that make that decision about me? And if you can't tell me, then you get closed down and fined very heavily, and that's good. And unfortunately, it means that if you're using big data and deep learning, which we all love, of course, it doesn't work there. You won't be using it in Europe much longer. - So there are lots of open questions about the way in which the general data protection, regulation, will come into force and how it will be interpreted with respect to requirements to disclose the logical processing, but there are a set of questions about what does it mean, and this isn't just from a legal perspective, but from a how do we make AI systems, robotics, compatible with humans, right? The systems are learning a lot about us, and yet in order for us to work with them, we also have to be able to interpret and understand and predict their behaviors, right. We have lots of examples of models behaving badly, to coin somebody else's phrase, but if we think about the GDPR requirements, in that broader perspective, right, there's this concern about how do individuals understand the limits of a model, how do they understand the assumptions, right, which may include biases that are both intentional or perhaps a result of the data on which the system is running. And this growing question about as systems become more opaque, at the same time, they're becoming so ubiquitous. How do we as a society rely upon them and use them when we're driving our car, when we're making decisions about what stock to buy, when we're relying on them for decisions about medical care? How do we deal with this increasing complexity in a way that allows humans to interact safely? - [Noel] Weep. -We can't weep. - I think that there is, at this point, a real problem with deep learning, because we, as computer scientists, don't understand how deep learning is working, so it makes it very difficult to satisfy some of these regulations. It's not a matter of intentional non-transparency It's a matter of computer scientists do not understand why it's working. So I think that's yet another reason, besides curiosity, which is motivating a lot of us, to try to understand why deep learning works. I don't think the countries in which this is being rolled out are necessarily gonna want to do without the benefits that they will get from deep learning. I'm not sure how this is gonna be resolved, quite frankly. I wonder whether there might not be a series of steps that you go through, at the end of which, you have basically signed away most of your rights. I'm not sure how that's gonna play out with the law. There are cases in which there is more transparency. If there's not a lot of circuit depth in the machine learning algorithms, then you really can explain where some of it came from. So I think there should be a move towards using lower circuit complexity when possible in making these decisions, and in other cases, with image processing and that kind of thing, I think people have to decide whether or not they want the benefits. They have to do cost-benefit analyses, for which we need ethicists , and philosophers to help us and then hopefully be able to make those as individuals as well as societies. So I think there's a, you know, use transparent algorithms where possible, and there should be more of a move towards that. - But not for everything. I mean, it's just appropriateness. I worked in learning for about 20 years. I really like it. I think it's incredible. So you can use it in very many tasks, but it's not always appropriate for everything. - Sure, but there are elements of it. I mean, if I can do something with a circuit depth of two, I can actually tell you how I came to this conclusion. I can unwind it. If I'm doing an DNN, I can't unwind it. - Well, as we heard this morning from the deep learning panel, we're heading towards what we used to all in the old days, and they don't seem to call it anymore, hybridization. So we've got a mixture between transparent AI algorithms using deep learning at the roots, and in that kind of way, you can actually get it transparency, because the learning bit is doing something that you don't care about the transparency, and then the symbolic bit is doing something where you do care and gives you the explanation. So that's possible to do it that way. - So Helen, this conversation about transparency, people are often talking about transparency or understanding for reason, and often, it's because they want some sort of accountability. And historically, we've had concerns, and you've written quite extensively about the ways in which computing can complicate accountability. And I'm wondering if there are particular new wrinkles that you think are emerging that require new ways to think about this issue of accountability. - You know, I'm curious to hear also how, because this work on accountability that you mentioned, thank you for mentioning, was accountability in a computerized society at a time where, you know, I'd read Fagenbaum's work on expert systems. You know, that was what AI was at that time, and we were still considering the delegation of human tasks to machines of one kind or another, hardware of software. So I'm wondering, in terms of transparency or explainability as people are very concerned about explainability, and this also came up, I think, in the deep learning panel, because we want to know why. If something happens, we want to know why. And sometimes, lightning strikes, and we look up to the heavens, why, why me? And we know that the lightning strikes thing, we say why, and we don't really expect an answer, but certain things like why did they put me away for 10 years and that person away for five years, we believe to expect it. The question that I'm confronting now, when I look back at that work for accountability for a computerized society versus the questions that we're asking today about accountability in light of delegation of tasks that were performed by humans, and accountability, by the way, again, it's not an easy problem. We struggle often when bad things happen, and I quote Arthur Ripstein, who's a political philosopher in Toronto, and he said well, the question you ask is something bad happens. You're in an accident. A bucket of paint falls on your head when you, the question is who's bad luck is it going to be? And it's actually a societal decision. So are we confronting different kinds of accountability questions in the current environment, where we believe that there's an inscrutability of deep learning algorithms that we're confronting, or is it actually different from decades ago when we were delegating these responsibilities to AI, slash, expert systems? - [Deirdre] What do you think, Raj? - So accountability is one of the attributes we all look for in systems. And often, when something happens, like you have a self-driving car, and it goes and does something, you want to be able to assign the blame to appropriate thing. Recently, we had the Tesla verdict where they said the system was saying hang on to the wheel or whatever it was saying, and the person did not do it. But that requires a whole set of systems, right. And my own feeling is the role of ethics and philosophers is to convince the government just like the Founding Fathers used the results from John Stuart Mill on liberty and freedom of speech, all the other things, and then they wrote it into the Constitution. Therefore now, everybody has to follow it. Now if there are issues that we should all be following, I request my friends, philosophers and ethicists, to convince the government, because unless it is written into the law, it will not be obeyed. We can sit here and wring our hands and do all kinds of things. Nothing will happen. Privacy is the best example of it. There is not privacy in my life. Every time I speak on the phone, every email I send, every time I step on the street, every time I do any of these kinds of things, everything is known. The only hope for me is I'm not doing anything illegal. They might know what I'm doing. There's no privacy. They know my bank accounts. They know who I'm paying. But there's no privacy in my case. And therefore I don't bother about privacy. We can all sit here and wring our hands and say, "We need privacy, we need privacy," but the government is already violating every aspect of privacy, and nobody seems to know what to do, because we don't know. Because if the question of security and safety versus privacy, security and safety will always win, unless it's written into the Constitution, and it's not. - But one of the problems, I fully agree with you. We've got to get some rule-making in place that will bind all those folks who are violating our privacy. But one of the problems is that-- - [Raj] The biggest violator of privacy is the government. - See, I think that we have private companies out there who are actually-- - That's small action. 1% of violation of privacy is the companies. 99% is the government. And until you solve the problem of what to do about the government continuously monitoring everything you, and they have a reason to want to do it, because they are worried about safeguarding the nation. All the other excuses are gone. And that's what Dick Cheney said. "If I have to use torture, I'm going to use it," even though it's written into the law that it cannot be done, and they did it. And nobody has gone to jail. So now here we kind of sit around and wring our hands and say privacy this, that and the other. Unless we find mechanisms to get it into the legal systems, the law, punishable by appropriate things, we can have all kinds of ethics discussions on panels, nothing will happen. I'm sorry I'm being cynical here, and that's life. That's life. There are other countries where there's no freedom of speech. If you go to China, Deng Xiaoping said, "I don't want freedom of speech. "I want stability, and I want economic welfare. "I couldn't care less about freedom of speech." If you said that in this country, there'll be a big revolution. Fortunately, it's written into our Constitution, and the Supreme Court has faithfully upheld that, right. - So we're going to stipulate that the government has been spying in this country and in other places, and that perhaps we don't have the political will to rein them in, and I think we're gonna come back to what the computer science and engineering community can do, and one way to think about that, and I think this dovetails with Helen's question, that much of the data that is relied upon by the government to monitor and track is either captured by the private sector and then demanded from them or is generated and collected through technology that's being developed. The government isn't making it themselves. It's generally not DIY, right. And so I think this raises a kind of question, if we want to level up a little bit, and think about how did the grand challenges and the shared research agendas of this community relate to our ideas of what it means to be a good and fair and just society? Are we aiming at the right things? Are we asking the right questions? Are there ways in which we are actually maybe more a part of the problem than we might want to think? - Here is an example. The government recently said private sector can sell the data that they're collecting from me. There was a regulation. They removed it. And there's nobody complaining, saying, "My God, "how can you do that?" - [Deirdre] Many people are complaining. - Nothing is happening, though. You're not in power. - But many people are complaining. - If you're not in power, it is not gonna happen. And for the next five years, four years, they will sell out your data. We can sit here wringing our hands. Nothing is gonna happen. Until you get it into the law, that the President of the United States cannot issue a regulation removing it, it will not happen. - One thing, you brought up the idea that technology could solve all social problems. - Yes, I could solve this problem, too. - Now that we've got all the data of everybody in the world-- - Every problem but privacy. - Yeah, well, now we've got everybody's data, we could run machine learning on it and work out who's fair and just and who's likely to be fair and just, and then arrest them if they're not. That would be quite a good idea. - First, we'd need a definition of fair, and of course, we know everybody would have a different definition. - Firstly, I agree that we need to engage government in this enterprise, but getting to Deirdre's point about what this community could help us with, and picking up on, it's to say that how do we decide what to do with these things that we're creating. We're creating AI, machine learning. We're creating, I mean, a lot more. It's just that it happens to be front and center of public discussion. But who gets to say? Why are we just expending so much activity on self-driving vehicles? Why is that the thing, that we're going to use this amazing technology to solve? Why is all this effort going into behavioral advertising? We have remarkable technology. Isn't part of what this community of computer scientists and engineers should be doing, is to think about, and I do see this as an ethical challenge, is to think about the priorities, to say hey, we should be supporting the use of these technologies to protect. I mean, there are people doing that. We should try and figure out how to not necessarily, so I think the elephant in the room that we have not addressed is the marketplace, the big companies, because to a large extent, these are the folks who are driving the technological agenda, and somehow we need to shift that. - Jennifer. - So first of all, I am very optimistic about this. I mean, as you know, Helen, nine or 10 years ago, I hired anthropologists, sociologists, communications people, Kate who has a PhD in philosophy. People asked me what I was doing. Now I have people who have PhDs in computer science with specialties in machine learning who are working hand-in-hand with the social scientists and really trying to develop this. So I think this is something that is attracting some of our best minds. I think it is something that we should be making sure that we teach people who, you know, if you have a machine learning class, I think there should be some part of that class that addresses questions of ethics, of fairness, of privacy. I really do, and I think that, I mean, it's not like everybody who uses machine learning is going to be part of the group that develops machine learning to help de-bias and to help make things fairer, but certainly everybody who uses it should learn about these tools so that they can employ these tools when necessary. Also something that I have seen from the highest levels of Microsoft is that people who I think at first, wondered why I was hiring all these folks are now thrilled that we have these folks, and they think that it is going to actually be a market advantage for Microsoft to put out software that allows you to attempt to be fair and attempt to de-bias. So my hope is that there will be, you know, not out of the goodness of people's heart, but because there will be an economic imperative to deliver these things. I know that, you know, you say I'm a little bit optimistic, that liability and GDPR and everything are good things, that laws are good, but I think the laws are good in that they give corporations and economic incentive to consider these problems and to take responsibility in solving them. So in that sense, I'm very optimistic. - But also, I think you have to go beyond machine learning. You're talking with doing that for machine learning, looking at ethics, but really we need to change this education in computer science. - Mm-hmm. - When you look at computer ethics, which my wife teaches the courses in that in our department, well, in engineering faculty, and you look at that, it's all about professional ethics. So it's all about your client, making sure that you specify correctly, making sure that your client is satisfied with what you produce. So for instance, if the client wants a big gun, you make sure you make a big gun that maximizes killing. Whereas really, we need to shift this towards, and I don't really want to use the word ethics anymore. I think we should be shifting it towards the notions of social responsibility. What is it like to be a socially responsible engineer? And I think in that way, if we give a good education, but who's going to deliver it? - I think it has to be delivered not in separate classes. I think it's very important that in every class, we should think about what is the ethical component of this, because otherwise if it's just separated off, it's too easy to forget it. - So I find I can teach my, I teach in a program that is not lawyers, right. It's information scientists, data scientists. And it's important to provide them with an education that includes the ethical, legal and social issues. But when they got out into the workplace, the question about whether or not there are structures that support them in using those tools, in using that knowledge, is often lacking, right, and that there are ones that find niche roles in different companies that, for different reasons. Sometimes it's clearly driven by law, but other times, it's driven by the market needs in a particular sector. But for the most part, I think that it's still, as much as I'm excited about the research that's happening, a niche, right, that it has not become the grand prize. There is no Netflix prize for the ethics of computing that I'm aware of. - [Noel] It's early days yet. - What? - It's early days yet. - [Deirdre] Is it early days? - Yeah, because you're sending your students out, if you make them have a very strong feeling of social responsibility, the structures aren't there. 40 years later, they're in charge. - Yeah, and I don't, but there's a difference between whether or not you're gonna go into socially responsible computing, right, that you're gonna devote your life to the use of data or computation or building technical systems to support the UN's mission and figuring out how we build out structures within companies that are working with data and using systems to make decisions about people, with people, for people on a daily basis in ways that respect social and ethical norms. And I think those are two different things, and I would hope that we don't have to wait 40 years to get the latter. So Helen, you've been working on this issue. And are you seeing any things that you think are particularly hopeful in this direction? - I mean, we've made, the values in design concept, and you know, you've said it, the teaching of ethics, and I want to use ethics in a broad way. I don't want to just make it about professional responsibility. I know there's a move in data science to talk about data ethics. And there's a fairly, maybe too narrow focus on the professional responsibility of data scientists. I think that is important. I like the idea of an effort, and we've been saying this for years. Once people started thinking about values in design and the fact that technologies are not neutral, and I think, like Brian, was it Brian Ford, this morning, talked a lot about the fact that technology is not neutral. That idea immediately leads you to saying we need to integrate ethical thinking everywhere. We can't separate out and say this is computer science, and this is ethics, and good luck making the connections between them. But it takes significant effort, and as we know, the rewards are not there necessarily, in terms of the career for the student who goes out with that kind of enlightened way of thinking. I think, Jennifer, your hiring of people who think about that is an important step in that direction. We should keep trying. We should keep trying to make that happen. I do think ACM, the funding agencies, the big companies could play a role in providing the incentives to see how to kind of integrate ethics all the way. And what I've seen about the computer science community, even back in the day, with Terry Winograd and Computer Professionals for Social Responsibility, is that this field has always seen itself as a caretaker for social issues, and I think we should continue to have that be the case. - I'm gonna give our Turing Award recipient the final word and ask one question. People often ask are we asking more of our technical systems than we ask of ourselves, right? Humans have all sorts of biases. We're the ultimate black box. What is it with all these demands for fairness and transparency of our computational systems? And for you, as someone who's spent your life thinking about how we see and work with them, are the demands fair? - No, my own expectation is, in the future, there won't be a separation between a human and a machine. It'll be a single system. It's not necessarily implants in your brain, but whatever it is, and it'll be acting as a single object or system, and it will be superhuman in ways that we can't currently estimate. And it will be able to do things that no human being using current DNA can do. And no computer by itself can do, not thinking about the single computer, maybe thousands of agents working with me. And that kind of a system needs to have this concept of ethics. You know, they understand what is right, what is wrong to do, but unfortunately, it is trumped by the governmental laws and regulations. And if they say, "You have to give me all the data," there's nothing any company can do, because it's a criminal problem. And so I think we need to both change the government, and the only way to do that is to kind of get all the red states to see why they need to change And I don't either. Until we do that, basically what they're saying is "I like the way I used to be, and I don't want to change. "I don't like these LGBTs. "I don't like this," you know, whatever. - I think we also have to be more compassionate in terms of the red states. Just I want to put that in there. - I'm completely compassionate. I am accepting what the result, and I'm saying that's what they wanted. That's what we're getting. - So our ethical challenges are grand. Hopefully, we can include ethics in our grand challenges. And with that, I'd like you to join me in thanking our panelists. - Well, I said we'd left the least controversial to last. Thank you very much, panel. 