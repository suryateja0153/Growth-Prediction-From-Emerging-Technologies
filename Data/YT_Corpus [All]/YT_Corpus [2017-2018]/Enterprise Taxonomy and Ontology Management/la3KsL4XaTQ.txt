 Welcome to the first lecture in this course. It is titled Healthcare Data: An Introduction to a Big Topic. To set the stage, let’s start with just looking at the topic of data very broadly. Data simply refers to facts and statistics collected together for reference or analysis. Data is fundamentally RAW. At its most basic, data is binary code (represented as 0’s and 1’s). Even put together these binary units only create characters, numbers, words, phrases, sentences, codes, tables, images, videos, scanned documents, sound files, etc. And by themselves all this is MEANINGLESS. But, together with the help of computers and the skilled human analyst it is POWERFUL! Capturing understanding and meaning from data is extremely complicated. As human beings, we have many complex ways to express a concept. Language is the most obvious but we also use other forms such as non-verbal communication, emotions, creativity and the like. These expressions can be kept BY computers and even organized to some extent but that does not bring them meaning. Humans easily learn to interpret these different words, symbols, and representations and to create meaning from them. Computers can be a great help to humans by organizing, presenting, and tabulating information…even to the extent of finding patterns that humans might not easily be able to (based on the sheer amounts of data to process.) But computers can not truly find or apply meaning in those expressions in the same way that humans do. We won’t go any deeper philosophically but if you are interested in the nuances between what computers can help us do (thinking) and what humans alone do (creativity being THE huge divider), check out this link: http://bigthink.com/going-mental/can-computers-be-conscious One example of the challenges working with data in healthcare relates to synonyms. For example, in health care, we can talk about a “heart attack”, which is also known as a “myocardial infarction”, acute coronary syndrome, MI, AMI, and heart attacks can can also be expressed in symbols. With a little education, we can adapt to different terms that are used that often mean the same thing. Computers – they are not so good at this understanding the nuances between synonyms. So what’s the problem? Computers need unambiguous (defined) ways to refer to synonyms or often what are abstract concepts or ideas. Even though they can tabulate more and faster than the human mind, they cannot fully interpret bits of information or creatively apply concepts. They are more limited in their ability to apply meaning and interpretation. Organizing these differences into groups or classes helps us help computers to transform data into information from which we can derive meaning. At it’s core, this process is analytics. This process is really the essence of Coding (data) and Knowledge Representation (information and knowledge) Knowledge Representation and reasoning (KR) is the underpinning of all computing including the advanced field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition. There are many terms used in coding, knowledge representation, and Analytics, including: Code Controlled Vocabulary Terminology Taxonomy Mapping Ontology These terms represent things we create to help computers associate different synonyms and types of meaning from data. They speak to information about information…sometimes referred to as “metadata.” FOR EXAMPLE: A computer might have access to a word/data stored as DOG. The terms above speak to ways we can help give the computer more context on that data (which of course at a basic level it has none and is ambiguous.) So perhaps we map an associated table that adds that a DOG is an “animal” and then a dictionary linked to that which defines an animal as “a living thing.” By having those now available reference points, the computer may then also find the word CAT in it’s data and be able to relate the two and draw a conclusion that both are “animals” and “living things” by way of this additional data it can draw upon. It is now (more) unambiguous to the computer. So, why is it that we need to worry about this when thinking about healthcare quality? Clear meaning leads to better representations. Better representations lead to better data recording. Better data recording allows us to draw more and better inferences from the data as the computer now can better organize, present, and tabulate the data. If we have better data that can lead to better inferences about our patients, we have the ability to improve healthcare outcomes. What is clear is that healthcare data is complex. Healthcare data is used to capture complex concepts (Even something like a broken finger is complicated…which finger on which hand? What kind of fracture? Which sub-bone(s) in the finger are affected? Is there inflammation? Is there an open wound as well? How much pain is the patient in? What age are they? And on and on…) The data may involve varying data structures (It may have different tables storing the same data that are named differently and/or have different rules such as one being limited to only numeric values and another allowing numeric and alpha, etc.) It may involve multiple different types of data (We might have coded numeric data associated with a diagnosis while also having free-text from the clinicians with more detail. In addition, we might have some scanned documents and a radiology image as well.) Data often requires aggregation (To make meaning of the data we may need to look at and compare 24 different labs taken every hour on the hour for 24 hours before we see a trend that could lead to a diagnosis. But the lab data is individual by lab so we need a way of seeing them in a linear or comparative fashion etc.) There are different types of data. One type of data is called unstructured. Unstructured data an be any type – numeric, text, image files, etc. Unstructured data does not follow rules (there is not a schema) (For example, think “freetext.” Can use any words you want to describe/label something.) Unstructured data has no real format or sequence (there are no rules on having to enter only text, only numbers, use only certain abbreviations, etc.) Unstructured data can be very unpredictable (Based on the above you can see how this could happen…think just a freetext free-for-all. One clinician calling a heart attack an “MI,” another calling it a “Heart Attack” etc.) In healthcare, we have a lot of unstructured clinical data – largely in the form of physician notes.There’s a lot of information and data in within these notes, and while there may be some organization it can also follow a “stream of consciousness” or “brain dump” of information. This is not easy for a computer to pull meaning or categorize it as discrete data. Clearly this is unstructured data! We will discuss unstructured data in more detail when we cover natural language processing later in the course. Another type of data is called structured data. Structured data has a formal organization of data into a schema (Think about it as being organized into bundles dedicated for that specific type of data.) Structured data can be typically organized into rows and columns (For example, think of an Excel spreadsheet or an Access database) Structured data are often organized into relational tables with a clear structure of records, attributes, keys, indices (Moving toward relational databases, such as one in an Access format) Structured data may be queried using structured query languages, such as SQL (Again, think of querying a specific combination of data from different tables within Access or other advanced softwares such as MySQL) Here is an example of structured data. -It can be considered structured because it’s organized into rows and columns! -The data conforms to a schema -- last name is in one field, first name in another, and age also in another. -This data could easily be queried in an effort to summarize it systematically. -Finally, note that some of this data comes from the unstructured note used in the previous example. In sum, structured data has a high degree of organization and can be thought of discrete data collected within a relational database. Unstructured data is information that does not have a high degree of organization and is difficult to organize using traditional mechanisms. A dictated physician note is an example of unstructured data. There is also semi (or Quasi) structured data. This type of data does not conform to the formal structure of tables and data models. It has some presence of data markers or tags that retain semantic meaning. It has some enforcement of hierarchies and fields, and some standardization. Finally, it is useful in organizing documents possibly but likely not content. A physician hospital discharge summary is a good example of semi-structured data. Certain elements of the document could include elements that could be hard coded into the computer for organization and sorting. These elements would be found in the “structured” sections such as: The elements in the header row, such as provider or patient name. Or major headings in the report, such as discharge diagnoses or procedures POSSIBLY the bulleted and numbered codes (likely numeric only.) The Freetext or unstructured data resides below. Because this type of document contains BOTH unstructured and structured data, it’s deemed Semi or Quasi Structured. So, how does this all come together? The more structured data is (higher on the pyramid), the more a computer can do with it because it’s been organized and defined in ways the computer understands and can process. Also, note that this shows a delineation between Semi and Quasi structured data. That is a fine line with no definitive markers of when it crosses from slightly less to slightly more structure. So, typically Quasi and Semi structured data are considered the same. Correct knowledge representations are critical as we expand the use of health data for multiple purposes beyond providing bedside clinical care to encompass administrative decision-making and to support population-based health efforts. Without a common terminology we will struggle to create the interoperable health system envisioned. There are many different types of data to support these efforts. One type of health data is epidemiologic (For this type of data, we can think population health and the Center for Disease Control and Prevention. Crunching big data and numbers to look for health trends. For example: after analysis of 300,000 records from the State of Colorado, we see that 30% of people with a certain diagnosis come from two specific zip codes.) Another type of data is Clinical (Clinical data is captured in both structured and unstructured formats. Clinical data includes blood pressure readings, lab results, radiology images, other facility information on scanned documents, free text notes by clinicians, etc.) Health data is also Administrative in nature (With this type of data we can think operational & financial information. For example, we can use administrative data to ask questions such as: How many of these types of surgeries did we perform last year? How many of our patients are self-pay? On average how many days does the average patient spend in our facility? AND…(perhaps our ultimate goal) health data also includes data that is Multiuse! Multiuse data is data that combines multiple types of data. (For example: using multiple types of data, an organization could rank conditions at a facility by the most diagnosed to the least. To perform this analysis, administrative, clinical and possibly epidemiologic data would be needed.) The scope of data reporting varies widely in healthcare. Data is kept and reported for different purposes and at different levels. So SCOPE varies but often and in the best designs, the underlying DATA feeding these needs IS THE SAME! For example, data is sent to governing bodies. It must be reported as aggregate numbers for a period of time as opposed to sending every single data point. Clinicians also use data comparatively. For example, a clinician may be interested in all the specific clinical details of a patient’s history going back several years or longer. Or they may only want to compare radiology images from three years ago to today. Administrators rely on data for all kinds of decisions. An administrator may need a high-level view of what kinds of procedures are most prevalent in the organization and how much each costs on average. Other business units rely on all types of data regularly for financial and operational decisions. The Health Information Management team, for example, keeps track of how many charts they analyze to track their employees productivity by quarter. Or, an insurance wants a copy of the physician notes from a certain visit to validate a code that they were charged. Similarly, materials management may want to know how many of a certain type of syringe were used over a six month period and whether those syringes are interchangeable with another model. Data is also used heavily for research purposes. For example, a researcher may want to know how many people between the ages of 18-36 had this type of procedure with a certain diagnosis. Data is also used for patient engagement purposes. A patient may need information from their hospital stay record for their specialist but really only needs the pertinent information consisting of their History & Physical, Discharge Summary, and labs. While data is used for producing a variety of reports, it is clear that data quality is highly variable in healthcare. These two cartoons illustrate the point well. If the data is questionable that is stored within healthcare databases, any analysis that is conducted using said data will also be questionable. Integrity of analysis begins with quality data. Regardless of TYPE, any and all data that is of poor quality will cause problems for computers and analytics. Because computers process data, you’ve likely heard adage: “Junk in, junk out.” Poor quality data can lead to: Diminished quality in patient care and/or safety Poor communication among providers and patients Problems with documentation Reduced revenue generation due to reimbursement problems No capacity to evaluate outcomes Can’t participate in research activities Incorrect conclusions and/or missed conclusions among others. Documentation is a major forum of data capture in healthcare. Poor-quality documentation by providers can have a major impact on: Patient safety – inadequate information, misinterpretations Public safety – if data is incorrect or missing, statistics that are relevant to the public will be inaccurate Continuity of care – if the data is missing or incorrect, sharing this information with a care team will have a negative impact clinically, result in inefficiencies for patients and clinicians, and may lead to distrust in using shared information Health care economics – conducting analysis on healthcare data is an expensive endeavor -- one estimate is that information and report generation in healthcare costs are well over $50B annually. Poor data quality undermines this investment. Clinical research and outcomes analysis – Because of the relationship between data quality and legitimacy of findings, lack of uniform information capture to facilitate the acquisition of data can negatively impact clinical research and outcome analysis. There are two components of documentation to consider here. The first is information capture: Information captures relates to the process of recording representations of human thought, perceptions, or actions in documenting patient care (such as writing, filling out blank fields, selecting drop down fields, etc.), as well as device-generated information that is gathered and/or computed about a patient (for example…an EKG machine feeding heart tracing information directly into the patient record.) The second component for us to consider is Report generation based upon data captured: Report generation refers to formatting and/or structuring captured information. It involves the process of analyzing, organizing, and presenting recorded patient information for authentication and inclusion in the patient record. (For example…EKG data may only be presented in 12-hour periods in the Legal Medical Record as opposed to showing every single data point for every minute the machine is monitoring the patient.) Because of the reliance on high quality data for creating useful information in healthcare, there are multiple ways to achieve quality information. One of those ways is through establishing standards For years in all areas of healthcare there have been no true agreed upon or legislated standards. Some have emerged but it is still widely “unstandardized” with some organizations following one and others following another. (This also has implications then for easily sharing information…more on that later.) Professional Organizations and Legislation Creating Standards is another way to improve data quality. An example of where organizations are setting standards relates to the Legal Medical Record: The Medical Records Institute (MRI) has created the “Essential principles of healthcare documentation:” The American Health Information Management Association (AHIMA) has also created principles related to information governance. Regarding the Medical Records Institute’s principles of documentation, they include: Unique patient identification must be assured Information must be confidential and secure Healthcare documentation must be: Accurate and consistent Complete Timely Interoperable across types of documentation systems Accessible at any time and any place a patient seeks care And documentation must be auditable The American Health Information Management Association has created principles related to information governance. These eight key principles provide the foundation of data and information governance. The principles include the following: Accountability: Designation or identification of a senior member of leadership responsible for the development and oversight of the IG program. Transparency: Documentation of processes and activities related to IG are visible and readily available for review by stakeholders. Integrity: Systems evidence trustworthiness in the authentication, timeliness, accuracy, and completion of information. Protection: Program protects private and confidential information from loss, breach, and corruption. Compliance: Program ensures compliance with local, state, and federal regulations, accrediting agencies’ standards and healthcare organizations’ policies and procedures and ethical practices. Availability: Structure and accessibility of data allows for timely and efficient retrieval by authorized personnel. Retention: Lifespan of information is defined and regulated by a schedule in compliance with legal requirements and ethical considerations. Disposition: Process ensures the legal and ethical disposition of information including, but not limited to, record destruction and transfer. AHIMA has also created the data quality management model, which also functions to help standardization includes the following key elements: Application: The purpose for the data collection Collection: The processes by which data elements are accumulated Warehousing: Processes and systems used to archive data Analysis: The process of translating data into meaningful information The DQM model was originally developed to illustrate the different data quality challenges that healthcare professionals face. Similar to AHIMA’s IGPHC, this model is generic and adaptable to any care setting and for any application. The tool expands beyond the EHR to include data quality across the healthcare continuum. It is a tool or a model for all healthcare professionals to assist in the transition to enterprise-wide DQM roles. The tool can be applied to all data in the organization, and expands to encompass both clinical and non-clinical areas. AHIMA defines characteristics of data quality. These include: Data Accuracy Data Accessibility Data Comprehensiveness Data Consistency Data Currency Data Definition Data Granularity Data Precision Data Relevancy Data Timeliness All of these features of the data quality management model will be presented later in this course. There are three key areas related to improving data quality. 1. Prevention 2. Detection 3. Actions This slide provides a list of methods used to improve data quality in each area. For example, to prevent poor data quality, organizations can compose a minimum data set of necessary data items or define data and data characteristics. These preventive strategies are proactive in nature and should limit the opportunity of collecting data with low integrity. Organizations can also detect data with poor quality. For example. They can perform automatic data checks or quality audits. These automatic or routine checks work to assure that data is being captured correctly. Finally, organizations can take action and intervene to improve data quality. For example, data quality reports can be constructed and the results can be presented to users in an effort to illustrate problems and create a foundation for change. Obviously organizations can correct inaccurate information once it is detected. However, a critical task is to realize that data quality is an ongoing process and it is imperative that issues surrounding it are communicated well. So there are obviously many issues with data. The question is -- can we still use the data? The obvious answer is Yes. But you have to understand what you are working with and make the data as useable as possible! To accomplish this, the analyst has to often spend time “cleaning” and “preparing” the data. We call this DATA NORMALIZATION. Watch the following video to learn more about “Data Wrangling”…otherwise known as Data Normalization. There are many common data quality issues that can be addressed through data normalization or data cleansing. One major issue related to data quality is accuracy, which include: Misspellings, incorrect values included. Synonyms (MI vs Heart Attack) Another issue is Redundancy For example, data may duplicated in a database table, or relation. In this case, “Database normalization” can help prevent redundancies Completeness of data is a major issue. The dataset may have missing values (it is the job of the analyst to determine if the data are missing or are there zero values? How may this impact the results? Currency is another issue analysts face. This relates to the timeliness of the data, or how current the data is. A good example is ICD-9 vs ICD-10 (Should you be using the most recent code set available, or is it important to use a mapped version for the particular analysis) Finally, consistency is a major issue that relates to how the data is collected over time . For example: If a field labeled number of clinic visits reports zero for a patient, yet the patient has two rows of data with different dates for the visit. It is the job of the analyze to determine if there is a mistake with the dates or dd was there multiple visits? Is the field not counting visits or counting them consistently? This slide presents a figure that displays two rows of data before and after normalization. You can see that the manner in which the street field is stored is normalized to improve accuracy. Accuracy is also improved by ensuring the spelling is accurate. The data completeness is improved by including the zip code for the data. Conformity is improved by standardizing the way the dates are stored in the Date of Birth fields. Finally, consistency is improved by standardizing how the State is stored in the state field. A real world scenario relates to National Drug Codes. NDC codes relate to a unique product identifier used in the United States for drugs The code is a unique and has 4 to 10-digits; each code is broken into 3-sections Segment 1 of the code relates to the labeler code of the company that manufactures, distributes, or packages the drug Segment 2 relates to the product code that establishes the strength, dose, and formulation And Segment 3 relates to the package code that identifies the package form and size This slide presents a dataset of NDC codes before normalization. Can you find the normalization opportunities related to the areas of: Accuracy Redundancy Completeness Conformity Consistency Uniqueness Where are there opportunities for dividing data into more discrete fields? Do you see the normalization changes in the dataset presented on this slide? For example, we have broken the NDC codes apart based upon the three segments and we have also broken the information related to the product apart. These changes represent only a FEW of the opportunities for dividing this data into more discrete fields. THINK about how just this amount of data being separated into new, discrete fields would help aid querying and reporting. Different users have different needs and standards of quality, which can affect the design of a database. All users should be aware of what the data mean and how it should be used. EXAMPLE: A department keeps track of the number of charts they analyze for productivity standards BUT, they decide based on time not to include the type of charts. This data is then limited in it’s usage as we wouldn’t be able to tell how many of what type of charts they analyzed…just a raw total. EXAMPLE: One user has been putting “MI” in a field that tracks Heart Attacks. She doesn’t know that others are inputting both MI for more chronic heart conditions and “AMI” for the acute vairety of the condition. Because they are using different definitions, this is where a Data Dictionary can greatly help to normalize data and ensure all users mean the same thing when they input data. In summary, data has many types and comes in many formats. Including structured, unstructured, and quasi structured. Data quality is a major issue and has multiple consequences. And there are many methods to address data quality including standards and guidelines. 