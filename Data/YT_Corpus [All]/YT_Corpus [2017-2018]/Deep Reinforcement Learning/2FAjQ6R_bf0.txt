 [Music] it's almost a full room at 2:30 in the afternoon on Friday so that's that's awesome my name is Doug I'm going to talk to you about magenta a project that we're doing in Google brain that's focused on music and art with machine learning so I hope there are some questions I'll try to blaze through this and leave a little bit of time for questions at the end I'm going to talk about two different projects both of them under the umbrella of magenta one has to do with drawing and one has to do with sound but I want to give you a little idea first what magenta is and and why we're doing what we're doing fundamentally we're asking the question can we use deep learning and reinforcement learning to do something creative and by this I really mean furthering our own creativity as people I don't mean pushing a button and standing back and watching the computer make art I think that's less interesting than actually having a cool new piece of technology to work with there are two things that we're doing concretely one of them is an open-source project on github it's part of tensorflow tensorflow magenta and we're trying to engage a number of people to work with us all of you in the room including creative coders including musicians including artists and including developers and at the same time it's a research effort so part of our mission in Google brain on the brain team is to publish papers and work with academics and we're pushing all of this stuff out not only an open source but but in in the research world we have a blog if you want to remember one URL it's magenta tensorflow org it's also geo magenta but it's fine to do it this way and that will link you to our code and to sit to other things so please check it out now there are a number of questions you can ask about creativity and about art and about music and how they all work together I thought about this problem a lot and I've come to the conclusion that there's a role that technology plays and there's a role that artists play and that we need to really carefully understand that interaction also I want to point out that this has been true since we've been doing art a cave painting requires a piece of charcoal musical instruments are high you know highly evolved pieces of technology and so I think of us at magenta more like the person on the left whose Les Paul one of the people credited with creating the electric guitar and less like the people on the right that st. Vincent but you could think of Jimi Hendrix you could think of any artist who's pushing the boundaries of the electric guitar and I bring this up because I think it's important to understand what people do with technology and art they push it they drive it they break it Jimi Hendrix came along and made the guitar distort maybe some of you in the crowd are aware but the goal of the electric guitar was to create a loud acoustic guitar and distortion of the amplifier was a failure case in the engineering world it was a fall over yet people come along this is true for film cameras as well to overexposing film any piece of technology you think of you're going to find people breaking it as part of the artistic process and I love that so for me success for magenta would be if someone does something in five years with what we built that we had no idea was coming none whatsoever so I want you to have that in mind that what we're thinking about is furthering the creative process like all technology that came before this used artistically very quick - or given the time we have of deep learning deep learning is deep see how deep the stack is weighted connections in a neural network are being trained in order to predict something it's also worth noting that these models naturally extract nice features nice filters from data making it possible for us to focus on the use of machine learning for creative processes so we don't have to build these things in by the way this this slide is great it's from from Zeiler and furgus a great research paper from about five years ago I also want to point out for those of you that are paying attention to deep learning deep learning in some sense is not new we've had neural networks since at least the 1980s arguably the 1960s and but they haven't always shown themselves to be the most you know the best models for the job one explanation for this is that neural networks a really good at scale they're really good when you have a lot of data or when you have large models and so as we move with more compute power what we find is that neural networks end up winning out over other other technologies which i think is is in its own right interesting and worthy of discussion but not here because we have to move on all right let's jump into our first project this project was led by David ha a Google brain resident extremely creative guy and this project is about teaching a machine learning model to learn to draw and these are some of the pictures that this machine learning model drew just a show of hands how many people have seen the sketch R and n stuff are familiar with the quick draw data all right that's great ok so we're going to see a little bit more of that some of the quick draw guys are sitting right over here if you want to give them a shout out for the game and for the data awesome work on our part in magenta we train models on quick draw and we just released a bunch of source code and you can train your own models we even have a nice jupiter notebook the call to action there is to go to magenta dot tensorflow org jump into our blog you'll find the blog posting it's easy enough to find our github and you can play around you can decide like we did here you can draw some flamingos and like three lines of code and it's kind of fun to do so please check that out and give us feedback I want to talk a little bit about the machine learning in this project fundamentally what we're working with our kind of machine learning technology called an autoencoder and there's a couple of basic ideas I want everybody to understand what we're doing is we're taking some input in this case it's strokes as drawn by someone on a screen and we're building some sort of neural networks that can encode them into some other representations and in general that representation is not larger but smaller than the source representation so the model is forced to try to pull out the important regularities in the data because it's not big enough to memorize the data and so that's Z in the middle think of that as your reduced representation that has pulled out all the important bits that it needs to try to recreate these and it's going to drive a decoder to recreate the output in our case the input our stroke based drawings done by people when they play the the game quick draw and we encode them using a recurrent neural network that is actually moving through the sequence of strokes trying to predict the next stroke and actually moving backwards from the end sequence trying to predict the sequences in reverse order it's called a bi-directional recurrent neural network or a bi-directional lsdm and the whole job of that net work is to create this vector the Z I was talking about or if you're Canadian Zed and this um this vector is going to be used to condition the decoding so we have this embedding this number the string of numbers in latent space that we can sample from that we can add some noise to and generate new instances of data that will then be driven through the decoder which is in this case another recurrent neural network the only going in one direction from left to right and it's going to drive a mixture of gaussians so a mixture of possible places where the pen would land next and for some of you that's word salad that's great it's fun word salad and if some of you understand it better jump off and read our paper you can grab the paper from chance of flow of magenta that Center flow org so the most important thing in machine learning is having data and the quick draw team a designed a brilliant game thanks for us who's sitting over there and the rest of the the people in Creative Lab for drawing a really fun game to play along with and also for releasing the data for machine learning researchers to work with and also for you to play with and enjoy so everything that we did in this project relies on having that data and running that data through generative models I also want to give a shout out to a related project from the handwriting team Auto draw which is doing in some sense the reverse of what we're trying to do we're trying to generate new drawings this one is trying to take your drawing and find the nearest matching icon or some nearest matching drawing done by a professional so it's another really cool thing that you can try just hit Auto draw calm now I want to do a demo so can we switch over to this demo screen this demo was built by David and what I'm going to do is I'm going to draw something on the left and then we're going to sample from the model nine times and remember the model has some noise in it it's not completely deterministic so we're gonna get nine different drawings this is a model that's trained on rain and so let's take it to draw a raindrop all right you'll see my raindrop appearing nine times and I did a nice big round raindrop and now I'm going to let the model go and it's going to make rain happen and you can see some of the varieties a kind of natural variety in the model that comes from sampling from a multiple times you could also just say hey let's draw rain like this because some people draw a rain like this and notice the model kind of follows my lead and it draws rain like I did right or and this is this is this is my favorite when David did this for me I thought it was pretty cool if you draw a cloud right in your mind's eye what's going to happen when I let that cloud go it's going to rain alright I just think that's so cool and I also like some of the more complicated drawings like I actually don't know how to draw a cruise ship I've never actually been on a cruise ship I don't think and I don't want food poisoning and there's there's so I'll just do that and then quick draw will fill it in with different kinds of cruise ships or sketch RNN based on click draw so that's kind of fun too and finally let's do one more it's fun to use that time on these demos we can't leave without a cat and so I'm going to show you what this temperature dial does down here we're sampling from this model so we have this Zed and we're going to use a little bit of math to draw a new version of Zed and then generate from it and we can actually change the parameter that makes the drawings follow lower probabilities instead of higher probabilities so we can kind of flatten out the probabilities and that makes it seem like the temperatures been turned up like it's a little hotter it's a little bit less predictable so if I draw at low temperature if I draw my best cat I'll just give myself to cat cat ears the model should draw some nice round cat faces and maybe even draw some whiskers if we're lucky yeah you're kind of getting our typical cats there at low temperature now I can control this with this temperature and turn it up a little bit and we'll start to see the cat looking a little bit less round so that looks like the cat that I draw the one in the middle I was always given you know like art time they were sending me off to play with the piano or something like that this is not my area okay can we switch back to the slides please that's the demo for a sketch RNN it's also fun just to see samples from this model here we sample unconditionally just drew samples from the model at different temperatures so it's the color of the icons get a color of the art gets more red that indicates that it's sampled at a higher kind of crazier temperature and even in these fast talks it's fun to just kind of look at this art the low temperature ones I think are really really nice simple views of how people draw yoga you can actually try to recreate some of these if you want and probably get away with it but don't try the high-temperature once at home Hot Yoga is dangerous I mean it's it there really look at me they're just really funny to look at so it's fun to sample from the model unconditionally and it's also fun to do conditional generation it's very different now what we're doing is we're taking a drawing those four drawings on the Left we're done by David and we ran them through the model and decoded them so you see on the right the reconstructions but we remember the Z in the middle disease our latent space and now we can just interpolate through our latent space and generate drawings for all of the spaces that we don't know about and I think it's quite good the way the model moves from these different shapes so each of those each of those reconstructions are in the corners and the color is following the color mapping between those colors and you get an idea that the model has a nice smooth representation of how faces are drawn also I want to point out that the model is not memorizing data we purposely restrict the the size of the representation to force it to generalize and we add a little bit of noise and I think this is cool it's a benefit so for example if you look on the left hand side the human drawings that are basically good examples of the class cat are more or less reproduced by the model on the right we drew some you know an eight-legged pig which is reconstructed as a four-legged Pig because that's what the model knows about pigs it knows that pigs have four legs before you call me out for some version of nose for some version of pigs and for some version a four leg also if you draw a truck and pass it through the pig classifier you get kind of a pig truck which i think is really cool like like if you were asked to draw a pig truck could you do a better job than that it's really good right three eyed cats they don't exist there's no chakra for our cats and if you do something like run a toothbrush through the cat classifier you don't get a cat you don't get a toothbrush you kind of get gibberish so the model really is is honing in on something about drawing so so that's a lot of fun I foresee a lot of possible creative applications of this comes going forward lovely if people want to hack our code oh this is the last thing if you're in a math it's like you're a math geek just look at this formula do the algebra these are this is math done on the on the embeddings I'm not even going to explain it just look at it yeah wait long enough they applauded pretty cool right so I love the idea that this space is well enough condition that you can actually move around numerically in the space and get meaningful results you can build pig heads from pig bodies and subtractions of cat heads and cat bodies etc no pigs were harmed okay I'm going to move on to project number two project number two is called n sense we also release this this week or some some some part of it and what we wanted to do was learn a music synthesizer we wanted to actually make new sounds and sounds that have captured some of the underlying richness and variance of musical sounds that we work with all the time as musicians people have been doing this forever we're certainly not the only use of software to make sound here's an analog synthesizer lots of knobs fun to play with what we're trying to do is use deep learning and see if we can get a particular feeling and a particular interpret ability to the sounds that we're making this work also I want to point out is a really I thought a great collaboration between the deep mind team in London and and the brain team in Mountain View the Sandra Dee Lehman was one of the authors on the original wavenet paper which I'll talk about in a second he was a fantastic collaborator with us and this project would not have happened without it and I love that kind of cross cross collaboration so in terms of what you can learn outside of this talk we have two postings on our blog at magenta tensorflow org that point to this really wonderful AI experiment that came from Creative Lab that I'm going to show you in a few minutes and we also for those of you our show of hands who knows what Ableton Live is all right all of you might consider going to our open source and downloading the plugin for Ableton Live and you can drive your own music with these samples it's it's fun cool so give that a look all right so first I understand what we're trying to do let's let's look back at the paper from last year called wavenet it's a paper that's trying to learn to generate audio from audio it's actually learning on the raw PCM post called modulation the raw speaker cone position sampled sixteen thousand times a second and it's trying to predict the next sample conditioned on about the last two seconds of samples and what it uses is something called dilated convolution so you see the arrows they get spread further and further apart it's almost like they're being violated in time so that the the the next prediction is conditioned not only on the sample that came last but some samples are some representations of samples that happen further and further in the past and this allows wavenet to be able to make make predictions that have a little bit of coherence however there are some limitations to this coherence because the model doesn't have anything to condition it to allow it to be more stable it really has a hard time doing something that is coherent over more than about maybe a fourth to a half of a second so let's play let's play dizzy please [Music] we're not alerting to play dizzy gillespie trained on dizzy gillespie only and get something out of it let's play Metallica [Music] and then on the bottom we trained on individual musical notes from a data set that we released an open source called ensign the whole project called enzymes and so is the data set because we wanted to confuse everybody and so the model sees only individual musical notes and if it's trained if a wavenet is trained on this data it's not quite capable of producing a single note it wanders around so please play the second one [Music] definitely learned about harmonics let's go to play the first one - it's fun I like that bass so what we see is that that the instance data trained on wavenet alone does some cool things but it doesn't give us what our desired goal which is to have kind of coherent musical notes so what we decided to do was add a auto-encoder to wavenet so that we can constrain and help it understand how how sound is unfolding in time so this basic diagram should look familiar you have some input now it's not a cat or a pig it's an input waveform we're going to encode that in time using a kind of convolutional model it's not a wave net but is also using deep deep dilated convolutions that's going to give us some sort of embedding and in this case the embedding actually unfolds in time so it's 16 values that change every few milliseconds and then we're going to have a wave net decoder the same wave note that we just saw and the wave that is actually going to have the input audio available when it's training but it's also going to see this conditioning information from our Zed and if it wants to take advantage of it it can and in fact it does to great effect so now what we can do is in code an entire note and we can then decode from it so let's listen to the original bass now if we run that bass through our model and decode it in the same way that we ran the cat through our model and looked at the cat it sounds like the bass on the bottom so it's hard to hear with the fan noise it's a little bit distorted sometimes we get clicks but more or less it captures the sound of the bass now let's hear the original flute and now let's hear the wavenet flute so it's closed so now you're asking why would you want to reconstruct a noisy versions of these samples because now because we're leaving in this embedding space we have this reduced representation we can do exactly what we did with the images of the cats or the images of the people we can move between sounds we know and listen to what the model does in spaces that we don't know so let's listen to what bass and flute sounds like original it sounds like a bass and a flute right that's what the average is you just average the signals together now let's listen to bass and flute from n cents let's play that again I invite you to go to our blog and listen to the examples there with headphones but what if what it does in my mind's eye is makes a really big bass flute right like physically that's what it sounds like in the interest of time let's just go ahead and play flute Plus organ I think even without hearing the organ you kind of get it and let's hear flute plus organ from from incense again it kind of grabbed the flute sound and modulated it with an organ like sound I'm going to move on from these now in the interest of time these are what the embeddings look like so the I told you these are temporal and beddings they actually unfold in time and there are 16 of them so we gave each one a different color and then now we're looking at bass glockenspiel and smoogle horn column-wise and you're seeing the original waveform shown in a kind of special spectral representation where time unfolds left to right and then you're seeing the embeddings that were used to generate the reconstructions I wanted to give you this basic idea that what we're really providing are those embeddings those 16 values changing over time and then the wavenet is learning how to take advantage of them now let's go to i'm gonna go to please switch to the demo again demo computer please okay so the creative lab team also just released this n sense music instrument you can play with it online we have a link to it we have a link to it from from our blog you can also get at it oh I have I'm sorry what I see up on the screen is our internal link you'll find it from our blog it's easy to find look for instance a sound sound maker and here you can play with this yourself so that's our base reconstruction compare that to the original upright bass by clicking here and here's a clarinet and then we can move through this space and [Music] we can sit sit in areas that are blending the clarinet and the bass in and I think interesting ways they're not all beautiful but they're all creative interesting try different samples I love this interface again excuse to the creative lab female we can do a cow I didn't mean the move bass pass cow who would want to do that so we have a cow let's do a vibraphone cow so here's our cow right there's our cow yeah you're clapping aren't you because you know this is this is why this is they pay us to do this work I just want to point that out okay we get paid and now we bring the vibraphone in we've actually got a really interesting sound like it's getting a kitchen astir the fibre foam but it's really like modulated by cows you have this cow modulator so I invite you to like lose if you're if if you have an auditory system that works at all I promise you you can lose easily an hour by just throwing on a set of headphones and playing around with this and left your unless your hacker news and then you're negative about everything but that's okay let's go back to the to the slides again please we also released a data set for those of you that want to do machine learning yourself we have about three hundred thousand individual instrument sounds that we generated from different sample packs everywhere and we also release them in a format that has a bunch of the metadata attached to it so you can know what the genre was some of the note light qualities like bright and dark and multi sonic and so this opens the door for doing things like training conditional models models that know about these values so you can use them to to condition your generation you can use these for anything you want to and it's out there in open source and we hope that it fuels more research and music related deep learning I want to close now and have plenty of time for questions I'm in love with this quote who's heard of Brian Eno if you didn't raise your hand homework go learn about Brian Eno fantastic musician and I just think this captures what we're trying to do with magenta I like that so much I'm actually going to read it whatever you know find weird ugly uncomfortable and nasty about a new medium will surely become its signature CD distortion the jitteriness of digital video the crap sound of 8-bit the distorted guitar sound is the sound of something too loud for the medium supposed to carry it I love that thought I love the idea of trying to build a new medium with the understanding that it's a medium that's meant to be broken like we want artists to musicians to try to break these things to try to do new things with them and I think that's a really wonderful interaction between technology and art and it's one that I think is just a it's fun and B it's it's cool right something I don't know what to say so there's your quote to close with here's my call to action what can you do have a look at tensorflow magenta that tensorflow org there's our blog there's our data discussion list and github you'll also find links off to the great work by Creative Lab including links to the data sets or you just look on Reddit and you'll find them there and now I managed to actually say some time for questions I hope you have questions I'm going to stop here and I thank you very much for your attention on a hot Friday afternoon you [Music] 