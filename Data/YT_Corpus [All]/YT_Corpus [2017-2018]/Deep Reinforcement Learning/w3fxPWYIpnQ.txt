 Hi my name is Marcos Campos, I'm the head of artificial intelligence at Bonsai and today we're going to be talking about challenges and strategies in reinforcement learning. This video is about what kind of strategies people can use to make reinforcement learning problems easier to solve. Reinforcement learning is a very broad subject matter. Instead of talking about algorithms and mathematical equations, I would like instead to focus on challenges and strategies to solve reinforcement learning problems. What are some of the many challenges in reinforcement learning? Let's talk about dynamic programming nature of the problem reward functions, and scalability. Reinforcement learning provides an approximate solution to dynamic programming. Dynamic programming is probably the hardest of the main optimization problems things such as linear programming, quadratic programming, mixed-integer programming, and then we have dynamic programming. Some of the key challenges of dynamic programming is that what will optimize not the immediate action or choice that the system makes but the future discounted reward considering many steps ahead. So for example imagine that you're playing a game like chess and you make a move. The move itself cannot tell you if you're going to win or not the game. The game actually will, victory will, depend on many of the steps that move only contributes to the final outcome so basically what you're trying to do when you choose which action or move to make at a given configuration of the board will depend on how the kind of choices you make later on in the final outcome. And that's what we call the reward and the nature of the dynamic program with a strategy that you need to do because of this sequence of steps and this temporal dimension the problem is much harder than to just solve or select an action to solve provide an optional solution for a given point in time. Also we have very limited information about the effect of each action right so the action in itself is only at one component to the final solution is really a sequence of actions that put together leads to the final result. That makes the problem extremely hard to solve. Another dimension of reinforcement learning is the notion of reward functions that's what the system is trying to optimize. It provides a feedback, a signal, of how well the system is performing the action that the system chose to perform how much value they add to the final solution or goal. In supervised learning we actually get information about what's the correct action that we should have taken. In reinforcement learning we only get a signal if that action was good or bad towards the goal. But you don't tell, you don't get the information this the environment doesn't tell the system what's the correct action. That's a much harder problem. Reward functions come in two flavors, sparse functions which are easy to specify by hard to solve. Let's say for instance, imagine that you do many many actions and choices we could do like playing a game like chess and you can have so many movements and the only thing you can tell in the end of the game is actually if you won or lost and you have to evaluate a move that you did like maybe 50 60 steps beforehand at the end of the game if that move was good or bad after looking at all the other steps that took place afterwards. So that's a sparse function you make moves and don't get any signal for if the movement is a bad or good movement or action. Only much later on or at the - usually what we call terminal stage you get some information but they're very easy to specify. You just say yes you won or you lost. That was now you achieve your goal or you didn't. You were able to land your plane or crash. The other class of reward function, non-sparse functions, they are hard to specify but easier to solve. Basically we construct or we have provided to the system a reward function that can provide a notion of how useful a given action is at any point in time. So every time the system takes, you know, may the control the system the AI system makes a decision to generate an action it gets information back that was good or not oh how good it was, right. So that makes it much easier to drive the system toward the final goal. Something else that we can do is sometimes you don't have a single reward function you could or goal you can have multiple goals the question is how do you weight different goals and let's say you may have a one goal is to save fuel. Then another goal is to take a safe route. So it may be this route that it now saves you the fastest route it might not be safe. How does it trade between the two goals to try to achieve some optimal behavior? So how to weight different goal so of course if you have a weighting scheme say "oh this is better than that by this much" we can combine them but in many situations we don't have that the only thing we know is that well there are two goals that we want to achieve and the system needs to negotiate between the two of them so actually combine multi goals into a single way that the system can learn for reinforcement learning is in itself a hard task. So this notion of combining different goals we basically trying to solve multi objective problems there is no overall reward function that actually can combine multiple competing rewards and goals. We need to learn how to trade between goals based on environmental context so basically there are two strategies when you have multiple goals one there is no - you cannot really define weights the other one is actually somebody has to provide how to combine that which is actually come from expert domain expert or coming from you know a practitioner expertise. In summary reward functions are essential for the success of learning with reinforcement learning one of the things is that we have this trade-off between sparse functions which are easy to specify hard to solve and non-sparse function which are hard to specify but easier to solve. We're going to come back to that as we talk about strategies later on. Scalability is another dimension of reinforcement learning that's quite challenging so reinforcement learning problems can take a long time. This is also called - you know - sample intensive like reinforcement learning is a sample intensive task. This system has to interact with the environment many many times to collect information back with samples in order to actually learn the problem because as we mentioned before instead of having information about what's the correct action for a given situation the only thing we get is - was the action good or bad. Not what you should have been and in fact some of them in the case of sparse reward functions can happen much later in which case requires even more samples to be able to make an assessment of how good an action is or not over time. One way you deal with this is about using exploration and exploring the environment by trying different actions in different contexts is a way of gathering information also taking that into account over time to learn strategies to actually be able to optimize the problem. Traditionally reinforcement is essentially -  was essentially a sequential method well because in order to do the next step required to know what were the previous actions that you took each action that you take leads the system to a new stage and then you need to evaluate again okay what's the stage that will follow after take another action and because what you try to define is how in a sequence of steps a given action produce good behavior or not in the future there is this kind of temporal dependencies we mentioned now early on with the nature of dynamic programming. Now what are the general strategies to make reinforcement learning problems easier to solve? A couple of them are shaping, curriculum learning, apprenticeship learning, and building blocks. Shaping addresses the difficulty of learning sparse reward functions. Shaping transforms a sparse reward problem to a non-sparse reward problem while converging to the same solution as the original problem that was specified with a sparse reward function. That is quite an achievement because as we have discussed sparse rewards are hard to solve for although easy to specify and non-sparse reward functions are hard to specify but easy to solve for. If we can actually, using shaping, transform a hard problem into one easy to solve it doesn't need to be perfect because that's the interesting aspect of the technique. Now, in the process of doing that there is an approach, a methodology, to do so which includes something called potential functions which are functions that you add to the original reward function in a certain specific way with a decaying factor which allows it as time goes by that its influence disappears. As influence disappears we in the limit we are back to the original problem but we actually have guided the system of how to explore the space in a way that is productive and easier to guide it to the final solution. There are two types of shaping the first one is shaping reward functions we add another function to the reward function we call a potential function which provides an evaluation of for every state of the value of the state right so basically when we do make an action we lead to a new state we can evaluate how positive or negative was the move. You know, that the action generated. So that's very useful because we transform a sparse reward only when I give a signal that's very rare just with the specific part of the state space and into something that always gets information from any part of the state space and because we add that in a very specific way with a decaying that over time that part that we will call the shaping potential function it vanishes and then we are left with a relatively explore the space toward the solution. And now another type of shaping is called value function shaping in that case we provide an evaluation not only of the state but also state inaction so it can be used to bootstrap from previous learned solutions and then get on top of that with like for instance you might have a Q-function use much more information than in the previous case and lead to solution more quickly. Now what are the pros from shaping? Faster solution, conversion to the solution of the original reward function problem. What are the cons? Potential functions are easier to define than non-sparse reward functions but still not easy. So requires expertise still and combined potential function in reward function is an art; there is no theory. So that usually requires some weighting scheme and again the partition is really depends on the partition knowledge but still is much easier than trying to solve the problem just with a sparse to reward function. Curriculum learning is another very useful strategy curriculum learning structures the learning process by breaking it down to lessons ranging from easier to harder tasks. That's basically how we learn, right, we go to school and usually we have a curriculum we start first learning and lessons that will provide information that we can build on top to become more proficient and solve harder and learn harder and harder concepts so the same way when you learn a game, for instance soccer, you start by, you know, learning to control the ball and know and then you move into dribbling and passing know to anticipate passes is becoming better and better at doing that. But originally you're just learn how to maybe hitting the ball against the wall so that you learn how to kick. In general later lessons mix material from earlier lessons with more advanced tasks. Another example you can think of is like a robotic grasp task. If you just have a robotic arm trying to reach for an object and grasp it and say well let's try to learn and the only signal that you have is that when it grasp it successfully you know it gets a reward is a very hard problem for the robot robot who is gonna have to be exploring the space randomly quite a long time instead if we constructed that as a curriculum you can start with the hand very close to the object then you can move it a little bit further way and then he learned that as it's approaching he arrived at the position that state space that it has already learned he knows what to do and then you connect keep moving it away and make it to harder and harder but every time that actually hit one of the states that he has learned about he knows how to do after that. So that by structuring the problem this way you're guiding through the curriculum how to achieve the solution in a much faster way. So pros: faster learning, intuitive to specify because we humans have actually learned quite a bit ourselves using this type of strategies. Cons can be time-consuming for large curriculums if we have to build a large curriculum it can take a long time to train because you have to train many many many lessons and also may need many lessons to solve complex problems and that can be time also time consuming to create so required for the designer or the architect of the system or the architect that is training the system to devise same ways when we're doing you know teaching a complex course and we have to design a whole curriculum is a time consuming strategy. Another strategy that has been quite useful especially in motor control tasks is apprenticeship learning. Apprenticeship learning uses sample performances of a task as guides for learning. For example, imagine that we want to teach a robot to reach, grasp, something move it to another place, and place the object there. One way to do that is by demonstrating the task to the robot and it can imitate or you can actually hold you know the the robot arm and perform manually the task in a way that they can record that performance many times so this is the type of learning by imitation. Demonstration usually by humans of a good performance and then the system can explore around that performance to find even better solutions. So solutions that are found by these strategies are a refinement of observed demonstrations. Pros: constrain the search space, faster solutions that emulate expected known behavior so the system generates behaviors which are variation on performance that humans or have this thought there are good ways to solve the problem but the decision can further optimize that and automate of course the performance. Cons: not every problem can be demonstrated effectively, also it does not find solutions that are innovative or very different of the provided performances. So the final strategy that we like to cover is let's call it building blocks. Using building blocks to solve a problem. So the idea is to solve harder problems by combining building blocks that can perform useful tasks or compute useful features. Building block behavior guide bias the exploration for new solutions. Again we have a set of skills that know how to solve or execute specific tasks now the idea is that we think on the right situation basically at a certain area of the states and because ultimately what we're doing we're trying to decompose a hard problem as a combination of simpler primitives. It allows the reuse and specification of prior knowledge. So for example playing tennis we can train the system to do backhand and forehand strokes now during the game you need to make a choice when to use one versus the other as the ball comes towards now let's say to the robot or the player. So combining two skills backhand and forehand at the appropriate time allows you to solve a much harder problem which is how to play tennis effectively. There are a couple different approaches that can be used and how for combining blocks for solving a more complex problem. One of them is the options framework which you want to learn how to combine them you know at the appropriate time. Another one is to decide when to use things that you already have but you learn new skills for situations that the previous skills do not actually enough to solve the problem. Finally we can also in for both cases we not only use predefined blocks that you don't train but you can continue optimize the blocks themselves so that they become better suited for this specific task at hand. Pros: faster learning because it decompose the problem into simpler problems that can be then combined and the combination is usually much easier than to learn the complex problem and each one of the blocks can be learned much faster as well because they only define simpler tasks. Reuse of previous modules. So you can basically train once a module and use a block many many times amortizing the total cost of learning. Cons: bad building blocks may harm learning so if the designer provides lots of blocks which are not useful the system doesn't know that so it will try know by exploration to not only use good blocks but bad blocks until it learns that those blocks are not useful and that may increase the time of learning. 