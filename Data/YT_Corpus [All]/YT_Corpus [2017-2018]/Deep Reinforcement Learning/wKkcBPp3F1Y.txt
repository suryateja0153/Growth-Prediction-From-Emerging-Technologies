 you've not heard a lot about how to search for good hyper parameters before wrapping up our discussion on hyper parameter search I want to share with you just a couple final tips and tricks for how to organize your hyper Frances search process deep learning today is applied to many different application areas and that intuitions about hyper parameter settings from one application area may or may not transfer to a different one there is a lot of cross-fertilization a lot of different applications domain so for example I've seen ideas 2000 the computer vision community such as confidence or res Nets which we'll talk about in a later course successfully apply to speech and I've seen ideas that were first developed in speech successfully applied in NLP and so on so one nice development in deep learning is that people from different application domains do read increasingly research papers from other application domains to look for inspiration for cross fertilization in terms of your setting for the hyper parameters though I've seen that intuitions do get fail so even if you work on just one problem say logistics you might have found a good setting for the hyper parameters and kept on developing your algorithm or maybe seen your data gradually change over the course of several months or maybe just upgraded the servers in your data center and because of those changes the best setting of your hyper parameters can get stale so I recommend maybe just re testing or reevaluating your hyper parameters at least once every several months to make sure that you're still happy with the values you have finally in terms of how people go about searching for hyper parameters I see maybe two major schools of thought or maybe two major different ways in which people go about it one way is if you babysit one model and usually you do this if you have maybe a huge data set but not a lot of computational resources not allow CPUs and GPUs so you can basically afford to Train only one model or a very small number of bottles at a time in that case you might gradually babysit that model even as its trained so for example on day zero you might initialize the parameters random and then start training and you gradually watch you know your learning curve may be the cost function J or your death set error or something else gradually decrease over the first day then at the end of day one you might say gee looks looks learn quite well I'm going to try increasing the learning rate robits and see how it does and then maybe does better and then bless your day to performance and after 2 days you say ok still doing quite well maybe after the momentum term a bit or decrease of learning regular bit now and then you know in two days three and every day you kind of look at it and you know try edging up and down your parameters and maybe on one day you found your learning rate was too big so you might go back to the previous days model and so on you're kind of babysitting the model one day at a time even as a training over a course of many days or over the course of several different weeks so that's one approach and people that babysit one model that is watching a performance and you know patiently nudging the learning rate up or down that's usually what happens if you don't have enough computational capacity to train a lot of models at the same time the other approach would be if you train many models in parallel so you might have some setting of the hyper browsers and just let it run by itself either for a day or even for multiple days and then give some learning curve like that and this could be a plot of the cost function J or cost of your training error or cause you're just an error but some measures in your tracking and then at the same time you might start up a different model with a different setting on the high preferences and so your second model might generate a different learning curve maybe one that looks like that looks like that one looks better and at the same time you might train a third model which my genuine learning curve does all that and another one that maybe this one diverges that looks like that and so on but you might train many different models in parallel where these orange lines are different models right then so this way you can try a lot of different hyper parameter settings and then just maybe quickly at the end pick the one that works best look like in this example it was maybe describe their little pest so to make an analogy I'm going to call the approach on the left the pander approach you know when pandas have children they have very few children usually one child at a time and then they really put a lot of effort into making sure that the baby pandas device so that's really babysitting you know one model or one baby panda words the personal rate is more like what fish do and commonly called it the caviar strategy there's some fish that lay over a hundred million eggs in one season in one mating season but the way fish reproduces they lay a lot of eggs and don't pay too much attention to any one of them but you know just see that hopefully one of them or maybe a bunch of them will do well so I guess this is really the difference between how mammals reproduce versus how fish and a lot of reptiles reproduce but I'm going to call it the pander approach versus the caviar approach since there's more fun and memorable so the way to choose between these two approaches is really a function of how much computational resources you have if you have enough computers they train a lot of models in parallel then by all means take the caviar approach and try a lot of different Hydra parameters and see what worlds but in some application domains I see this in some online advertising settings as well as in some computer vision applications where there's just so much data and the models you want to train are so big that's difficult to train a lot of models at the same time it's really application dependent of course that are but I've seen those communities use the Pandora approach a little bit more where you are kind of babying a single model long and nudging the parameters up and down and trying to make this one model you know work although of course you know the Pandora approach having trained one model and seeing it work or not work maybe in the second week of the third week maybe you actually initialize a different model and then you know baby that went along just like even pandas I guess can have multiple children in their lifetime even if they have only one or a very small number of children at any one time so hopefully this gives you a good sense of how to go about the hyper parameter search process now it turns out that there's one other technique that can make your neural network much more robust to the choice of hyper parameters doesn't work for all neural networks but when it does it can make the hyper parent to search much easier and also make training go much faster let's talk about this technique in the next video 