 The human side of AI, how do we turn this camera back in on the human, we are talking about perception, how to detect cats and dogs, pedestrians lanes, how to steer a vehicle based on the external environment, the thing that's really fascinating and severely understudied, is the human side, we talked about the Tesla, we have cameras in 17 Tesla's driving around Cambridge because Tesla is one of the only vehicles allowing you to experience in a real way, on the road, the interaction between the human and the Machine, the thing that we don't have, that deep learning needs on the human side of semi-autonomous vehicles and fully-autonomous vehicles is video of drivers, that's what we're collecting, that's what my work is in, is looking at billions of video frames, of human beings driving 60 miles an hour plus on the highway in their semi-autonomous Tesla, what are the things that we want to know about the human? If we were a deep learning therapist, we’d try to break apart the different things we can detect from this raw set of pixels, we can look here, from the green to red is a different detection problem, a different computer vision detection problem green means it's less challenging, it's feasible, even under poor lighting conditions, variable pose, noisy environment, poor resolution, red means it's really hard no matter what you do, that's starting on the left with face detection body pose, one of the best studied and one of the easier computer vision problems, we have huge datasets for these, then there is micro saccades, the slight tremors of the eye that happen at a rate of a thousand times a second. All right let's look at— First, why do we even care about the human in the car? One is trust, this trust part is a— If you think about it, to build trust the car needs to have some awareness of the biological thing it's carrying inside, the human inside, you assume the car knows about you, because you're sitting there controlling it, but if you think about it, almost every single car on the road today, has no sensors with which it's perceiving you, it knows, some cars have a pressure sensor on the steering wheel and a pressure sensor or some kind of sensor detecting that you're sitting in the seat, that's the only thing it knows about you, that's it, so how is the car supposed to— this same car is driving 70 miles an hour, on the highway, autonomously, how is it supposed to build trust with you if it doesn't perceive you? That's one of the critical things here, so if I'm constantly advocating something, is that we should have a driver facing camera in every car, despite the privacy concerns, you have a camera on your phone and you don't have as much of a privacy concern there, but despite the privacy concerns, the safety benefits are huge, the trust benefits are huge. Let's start with the easy one, detecting body pose, why do we care? There is a seatbelt design, there are these dummies, crash-test dummies, which we can use to design the passive safety systems of our cars, and they make certain assumptions about body shapes, male, female, child, body shapes, but they also make assumptions about the position of your body in the seat, they have the optimal position, the position they assume you take, the reality is, in a Tesla, when the car is driving itself, the variability, if you remember the deformable [unintelligible 00:04:44] you start doing a little bit more of that, you start to reach back in the back seat, in your purse, your bag, for your cell phone, these kinds of things, that's when the crashes happen, we to know how often that happens, the car needs to know that you're in that position, that's critical for that very serious moment when the actual crash happens, how do you do? This is deep learning class, this is deep learning to the rescue, whenever you have these kinds of tasks, of detecting for example body poses, you're detecting points of the shoulders, points of the head, five-ten points along the arms, the skeleton. How do you do that? You have a CNN, convolutional neural network, that takes its input image and takes an output, it's a regressor, it gives an XY position of whatever you're looking for, the left shoulder, right shoulder, then you have a cascade of regressors they give you all of these points, they give you the shoulders, the arms and so on, then you have— through time on every single frame you make that prediction and then you optimize, you can make certain assumptions about physics, your arm can't be in this place in one frame and then the next frame be over here, it moves smoothly through space so under those constraints you can then minimize the error-- the temporal error from frame to frame or you can just dump all the frames, as if there are different channels like RGB is three channels, you can think of those channels as in time, you can dump all those frames together, and that's what I call 3D convolutional neural networks, you've dumped them all together and then you estimate the body pose in all the frames at once. There are some data sets for sports and we're building our own— I don't know who that guy is— Let's fly through this a little bit, so what's called gaze classification, gaze is another word for glance, it's a classification problem, here's one of the TAs for this class, Not here because he's married, he had to be home, I know were his priorities are at, this is on camera, he should be here, [chuckles] There's five cameras, this is why we're recording in the Tesla. This is a Tesla vehicle, in the bottom right, there's a blue icon that lights up automatically detected if it's operating under autopilot, that means the car is currently driving itself, there's five cameras one on the forward roadway, one on the instrument cluster, one on the center stack, steering wheel, his face, then it's a classification problem, you dump the raw pixels into a convolutional neural network, have six classes forward roadway, you're predicting where the person is looking, forward roadway, left, right, center stack, instrument cluster, rearview mirror, and you give millions of frames for every class, simple. And It does incredibly well at predicting where the driver is looking, the process is the same for majority of the driver state problems that have to do with the face, the face has so much information, where are you looking, emotion, drowsiness, different degrees of frustration, I'll fly through those as well, but the process is the same, there's some pre-processing, this is in the wild data, there's a lot of crazy light going on, there's noises, vibration from the vehicle, so first you have to— video stabilization you have to remove all that vibration, all that noise, as best as you can, there's a lot of algorithms, non-neural network algorithms, boring but they work for removing the noise, removing the effects of sudden light variations and vibrations of the vehicle, there's the automated calibration, so you have to estimate the frame of the camera, the position of the camera, and estimate the identity of the person you're looking at. The more you can specialize the network to the identity of the person and the identity of the car the person is riding in, the better the performance for the different driver state classification. So you personalize the network, you have a background model that works on everyorne and you specialize each individual, this is transfer learning, you specialize each individual network to that one individual. There is a face frontalization, fancy name for the fact that no matter where they're looking, you want to transfer that face so the eyes, nose are the exact same position in the image, that way if you want to look at the eyes and you want to study the subtle movement of the eyes the subtle blinking, the dynamics of the eyelid, the velocity of the eyelid, it's always in the same place so you can really focus in remove all effects of any other motion of the head, and then you just— it's the beauty of deep learning, there is some pre-processing, because this is real-world data, but you just dump the raw pixels in, you dump the raw pixels in and predict whatever you need. What do you need? One is emotion, You can have— I had a study where people used a crappy and a good voice based navigation system, so the crappy one got them really frustrated, and they self-reported it as the frustrating experience or not on scale one to 10, that gives us ground truth, a bunch of people to used this system, they put themselves as frustrated or not, so then we can predict, we can train a Convolutional neural network to predict is this person frustrated or not, I think we've seen a video of that, turns out smiling is a strong indication of frustration, you can also predict drowsiness in this way, gaze estimation in this way, cognitive load, I'll briefly look at that, the process is all the same, you detect the face, you find the landmark points in the face, for the face alignment, face frontalization, and then you dump the raw pixels in for classification, step five. You can use SVM's there or you can use what everyone uses now, convolutional neural networks. This is the one part where CNN's still struggle to compete, is the alignment problem, this is why I talked about the Cascade regressors, is finding the landmarks on the eyebrows, the nose, the jawline, the mouth, there are certain constraints there, so algorithms that can utilize those constraints effectively can often perform better than  end-to-end regressors that just don't have any concept of what a face is shaped like. There are huge data sets and we're a part of the awesome community that's building those data sets for face alignment. This is the TA in its younger form, this is live in the car, the real time system predicting where they're looking, this is taking slow steps towards the exciting direction that machine learning is headed, which is unsupervised learning, the less you have to have humans look to the data and annotate that data, the more power these machine learning algorithms get, currently supervised learning is what's needed, you need human beings to label a cat and label a dog, if you can only have a human being label 1%, one tenth of a percent of a data set, only the hard cases, so the machine can come to the human and be like, I don't know what I'm looking at in these pictures, because of the partial light occlusions, we're not good at dealing with occlusions, whether it's your own arm or because of light conditions, we're not good with crazy light drowning out the image, this is what Google self-driving cars struggle with when they're trying to use their vision sensors, moving out of frame, all kinds of occlusion They are really hard for computer vision algorithms, and in those cases we want a machine to step in and say-- and pass that image on to the human, be like "help me out with this" and the other corner case is, in driving for example 90 plus percent of the time all you're doing is staring forward at the roadway the same way, that's where the Machine shines, that's where machine automated annotation shines, because it's seen that face for hundreds of millions of frames already, in that exact position, so it can do all the hard work of annotation for you, it's in the transition away from those positions that it needs a little bit of help, just to make sure that this person just started looking away from the road to the rear view, and you bring those points up, so you're-- there's a— using optical flow, putting the optical flow in the convolutional neural network, you use that to predict when something has changed when something has changed you bring that to the machine for annotation all of this is to build a giant— Billions of frames annotated data set, our ground truth, on which you train your driver state algorithms, in this way you can control, on the x-axis is the fraction of frames the human has to annotate, zero percent on the Left, ten percent on the right, and then the accuracy trade-off, the more the human annotates, the higher the accuracy, you approach 100% accuracy, but you can still do pretty good, this is for the gaze classification task, With an 84-- 84 fold to almost towards the magnitude reduction in human annotation, this is the future of machine learning, and hopefully one day no human annotation, and the result is millions of images like these video frames, same thing, driver frustration, this is what I was talking about, the frustrated driver is the one that's on the bottom, so a lot of movement of the eyebrows and a lot of smiling, and that's true subject after the subject, And they're Happy, the satisfied, I don't want to say happy, the satisfied driver is cold and stoic, and that's true for subject after subject, because driving is a boring experience and you want it to stay that way Yes, question. Great, great question, they're not-- Absolutely, that's a great question So these cars owned by MIT, there is somebody in the back— The comment was— my emotions then have nothing to do with the driving experience. Yes, let me continue that comment, your emotions are often— You're an actor on the stage for others with your emotion, when you're alone, you might not express emotion, you're really expressing emotion oftentimes for others, your frustration is like "What the heck" that's for the passenger, and that's absolutely right, so one of the cool things we're doing— As I said, we now have over a billion video frames in the Tesla, We're starting to collected huge amounts of data in the Tesla, emotion is a complex thing, in this case, we know the ground truth, how frustrated they were, in naturalistic data, when it's just people driving around, we don't know how they're really feeling at the moment, we're not asking to enter an app "how are you feeling right now?" but we do know certain things, we know that people sing a lot, that has to be on paper at some point, it's awesome, people love singing, so that doesn't happen in this kind of data, because there's somebody singing in the car, and I think the expression of frustration is also the same. Yes. The question is— or the comment is that the solo data set is probably going to be very different from a data set that's not solo, with a passenger, that's very true, the tricky thing about driving this is why it's a huge challenge for self-driving cars for the external facing sensors and for the internal facing sensors analyzing human behavior, is 99.9% of driving is the same thing, it's really boring. So finding the interesting bits is actually pretty complicated, so that has to do with emotion, that has to do with— so singing is easy to find, we can track the mouth pretty well, so when you're talking of singing we can find that, but how do you find the subtle expressions of emotion? It's hard, when you're solo. Cognitive load, that's a fascinating thing, I mean, similar emotion it's a little more concrete in a sense that there's good science and ways to measure cognitive load, cognitive workload, how occupied your mind is, mental workload is another term used, the window to the soul, the cognitive workload soul is the eyes, so pupil— first of all the eyes move in two different ways they move in a lot of ways but  two major ways is saccades, these are these ballistic movements, they jump around whenever you look around the room, they're actually just jumping around, when you read the eyes are jumping around, Like if all of you just follow this bottle with your eyes, your eyes are actually going  to move smoothly, a smooth pursuit. Somebody actually told me today, that probably has to do with our  hunting background as animals, I don't know how that helps, like frogs track flies really well, so you have to like— Anyway, the point is there are smooth pursuit movements where the eyes move smoothly, and those are all indications of certain aspects of cognitive load, and then there are these very subtle movements, which are almost imperceptible for computer vision and these are micro saccades, these are tremors of the eye, a work from here, from Bill Freeman, magnifying those subtle movements, these are taken at 500 frames a second. So cognitive load— when the pupil, that black dot in the middle, in case you don't know what a pupil is, in the middle of the eye, when it gets larger that's an indicative of high cognitive load, but it also gets larger  when the light is dim. So there's this complex interplay, so we can't rely in the wild outside, in the car, or just in general outdoors, using the pupil size, even though pupil size has been used effectively in a lab to measure cognitive load, it can't be reliably used in the car, the same with blinks, when there's a high cognitive load, your blink rate decreases and your blink duration shortens, I think I'm just repeating the same thing over and over, but you can imagine how we can predict cognitive load, We extract a video of the eye. Here is the primary eye of the person the system is observing, happens to be the same TA once again. We take the sequence of 100-- it's 90 images, that's six seconds, 16 frames a second, 15 frames a second, we dump that into a 3D convolutional neural network, that means it's 90 channels, it's 90 frames, grayscale, and then the prediction is one of three classes of cognitive load, low cognitive load, medium cognitive load and high cognitive load, there's ground truth for that, because we have people-- over 500 different people do different tasks of various cognitive load, and after some frontalization again, where you see the eyes are traced no matter where the person looking, the image of the face is transposed in such a way that the corner of the eyes remain always in the same position, after the frontalization, we find the eye, active appearance models, find 39 points of the eyelids, the iris, and four points on the pupil. Putting all of that into a 3D CNN model, they're positioned,eye sequence on the left, 3D CNN model in the middle, cognitive load prediction on the right. This code by the way is freely available online. All you have to do, dump a web-cam from the video stream, CNN runs faster than real-time, predicts cognitive load. Same process as detecting the identity of the face, same process as detecting where the driver is looking, same process as detecting emotion and all of those require very little hyper parameter tuning on the convolutional neural networks, they only require huge amounts of data. Why do we care about detecting what the drivers doing? I think Eric has mentioned this is-- On the-- Oh man, this is the comeback of the slide, [laughter] I was criticized for this being a very cheesy slide, in the past towards full automation, we're likely to take gradual steps towards that. I can't, it's enough of that, this is better— Especially given that— This is given today, our new president, this is a pickup truck country, this is a manually controlled vehicle country, for quite a little while, we like control and control being given to somebody else, to the machine, will be a gradual process, it's a gradual process of that  machine earning trust, and through that process, the machine, like the Tesla, like the BMW, like the Mercedes, the Volvo, that's now playing with these ideas, it's going to need to see what the human is doing, and for that, to see what the human is doing, we have billions of miles of forward-facing data, what we need, is billions of miles of driver facing data as well. We're in the process of collecting that, this is a pitch for automakers and everybody to buy cars that have a driver facing camera. And let me close-- I said we need a lot of data but I think this class has been— through your own research you'll find that we're in the very early stages of discovering the power of deep learning, for example, recently,  Jean [?] said that it seems that the deeper the network, the better the results in a lot of really important cases, even though the data is not increasing, why does the deeper network give better results? This is a mysterious thing we don't understand, there's these hundreds of millions of parameters, from them is emerging  some kind of structure, some kind of representation of the knowledge that we're giving it. One of my favorite examples of this emergent concept is the Conway's Game of Life. For those of you who knows what this is, will probably criticize me for being as cheesy as the stairway slide, but I think it's such a simple and brilliant example of how-- Like a neuron in a neural network is a really simple computational unit, and then incredible power emerges when you combine a lot of them in a network, in the same way, this is called  the cellular automata, that's a weird pronunciation, every single cells is operating under a simple rule, you can think of it as  a cell living and dying, it's filled in black when it's alive and white when it's dead,  if it's alive and it has two or three neighbors, it survives to the next time, otherwise it dies, and if it has exactly three neighbors, and it's dead, it comes back to life, if it has exactly three neighbors, that's a simple rule, whatever, you can just imagine, it's just simple— All is doing, is operating under this very local process, same as a neuron. It's a— or in the way we're currently training neural networks and there's this local gradient, we're optimizing over a local gradient, the same local rules, and what happens if you run this system, operating under really local rules, what you get on the right, it's not— Again, you have to go home, hopefully no drugs involved, but you have to open up your mind [chuckles] and see how amazing that is, because what happens is, it's a local computational unit, that knows very little about the world, but somehow really complex patterns emerge and we don't understand why, in fact under different rules, incredible patterns emerge, and it feels like it's living creatures communicating, when you just watch it, not these examples, this is the original, they get complex and interesting, but even in these examples, these complex geometric patterns that emerge, it's incredible, we don't understand why, same with neural networks, we don't understand why, and we need to in order to see how these networks will be able to reason. What's next? I encourage you to read the deep learning book, it's available online, deeplearningbook.org. As I mentioned to a few people, you should-- Well, first there's a ton  of amazing papers every day coming out on archive, I'll put these links up, but there's a lot of good  collections of strong papers, lists of papers, there is  the literally awesome list, the awesome deep learning  papers on GitHub, it's calling itself awesome, but it happens to be awesome, there is a lot of blogs, it's just amazing, that's how I recommend you learn machine learning, on blogs, and if you're interested in the application of deep learning in the automotive space, you can come and do  research in our group, just email me. Anyway, we have three winners, Jeffrey Hu, Michael Gump how do you-- Are you here? How do you say your name? No, that's not my name [laughter] My name is Purna [?] Oh, I see. [?] Well, anyway here-- [applause] He achieved the stunning speed of-- So this is kind of incredible, I didn't know what kind of speed we were going to be able to achieve, I thought 73 was unbeatable, because we played with it for a while and we couldn't achieve 73, we design a deterministic algorithm that was able to achieve 74 I believe, meaning like it's cheating, with the cheating algorithm that got 74, folks have come up with algorithms that have done— that had beaten 73 and then 74, so this is really incredible, and the other two guys— all three of you get a free term at the Udacity self-driving car engineering degree, Thanks to those guys for giving that award and bringing their army of brilliant— So they have people who are obsessed about self-driving cars, and we've received over 2,000 submissions for this competition, a lot of them from those guys, they're just brilliant, it's really exciting to have such a big community of deep learning folks working in this field, this is for the rest of eternity, we're going to change this up a little bit, but this is actually the three neural networks, the three winning neural networks running side by side, you can see the number of cars passed there, the first place is on the left, second place, and third place, and in fact, the third place it's almost-- right now, second place  is winning currently, but that just tells you the random nature of competition, sometimes you win, sometimes loose. The actual evaluation process runs through a lot of iterations and takes the medium evaluation. With that, let me thank you  guys so much for— Wait, we have a question— are the winning networks online? Yes. All three guys wrote me a note about how their networks work, I did not read that note, [chuckles] I'll post—This tells you how crazy this has been, I'll post the winning networks online, and I encourage you to continue competing and continue submitting networks. This will run for a while we're working on a journal paper for this game. We're trying to find the optimal solutions. Okay. This is the first time I've ever taught a class, and the first time obviously teaching this class, so thank you so much for being a part of it. [Applause] Thank you to Eric, if you didn't get a shirt please come back, please come down and get a shirt, just write your email on the note, on the on the index note. Thank you. 