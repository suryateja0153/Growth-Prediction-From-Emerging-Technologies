 [Music] very happy to have a professorship removal from the Columbia she's also affiliated with our data science institute there and should press also visiting faculty at Google this fall she's been really interesting and influential work in several areas of privation and machine learning and today she's there to talk about her work on posture sampling the series of works on competently ok thanks father thank you for inviting me and showing us here to listen to me so this is a overview of sequence of birth that I've done since 2011 it started in Microsoft Research of India and has continued during that duration Columbia and before I give the exact problem combination and the star algorithm that I'm going to talk about today I want to talk about general philosophy of studying these functions so the idea here is to understand the interdependence between data and decisions so the decisions we are talking about could come from number of different applications you could be deciding what movies to recommend to a customer what products to recommend what ads to show even in a clinical trial what treatment to administer to a patient or an exciting application which has caught a lot of attention lately is about how to decide how to play again like whether to move a pedal left or right when playing the games like like pong and in all of these applications the general idea is that if you have some data you use the data to improve your decisions if you have better data you are able to make better decision but what I want to emphasize is the reverse relation in almost all these applications when you make a decision you get a feedback in response to that decision if you present a movie to the customer you see whether the customer watch the movie or not if you show an hour you see whether the customer clicked on the ad or not and that goes back to your database so your data current data decides how good your current decisions are but what decisions you make actually decide how good your data will be for future so for example as an extreme case if you always show certain kinds of movie to customers from certain profile then you would never know whether the customers would like another kind of movie or not if you always look at certain system symptoms of a patient and decide a certain set of treatments you would not know from your data later whether another alternate treatment could be good so these are extreme cases of course the reality is more subtle but what it points to is that we need to take into account the information that a leader that the decision gives will he decide making our decisions so that we have better data for teacher and this requires exploration we cannot just rely on our current knowledge and make the best decision that seems like the best decision right now but explore other possibilities and of course there's a trade-off involved here between the short-term revenue that would seem like to be maximized by the current data versus long-term revenue and this trade-off is often referred to as the exploration exploitation trade-off and that sport brings us to a cute mathematically formulated problem called a diamagnetic problem it's a basic formulation to capture this trade-off and I'll spend most of the talk on this before moving on to more advanced versions of this problem so the multi-armed bandit problem many of you might have seen this before it comes from the problem in thin slot machines in a casino let's say let's say you have some hundred dollars to spend on the slot machines and each of these slot machines are raised in different ways and you want to figure out which machine to put your money on the only way you can find out about a machine is if you put money put a coin in the slot and pull the lever now how much money you should spend to learn about machines and when should you start actually investing your money in the machine that you think is good so that's where the exploration exploitation trade-off come in how much men should you stop trying and then should we start playing and the the formal problem that we are going to look at is you have some online decisions to make which so here it is which lever to pull so you have some decisions to make it could be which movie to recommend which treatment to administer which I had to show so every time you have to pull one out of some n possibilities n R and when you pull an arm the important is the most important property of this of this problem definition you only get to learn about that so the only way to learn about the slot-machine is to actually put money and stage or administer a treatment on the patient and see what happens so that's we will refer often - as band-aid feedback and here is the assumption that we make in stochastic multi-armed bandit problem which is that the rewards are generated from a pig's distribution so this is the special assumption for stochastic problem this the distribution is fixed but unknown so you don't know anything about this region except that this is a fixed distribution which generates the reward whenever I pull let that arm and we also assume it's a bounded distribution will relax the solution later it has some fixed mania my so given these these assumptions now the goal in this simply stated problem is to maximize the total reward so you have horizon is fixed you have t time steps to play you have tape t turns that you get and you can divide these turns into any any way into your arms and your goal is to maximize total reward so now one observation is that if you knew for every arm the in the underlying distribution then the problem would be very simple if your goal is to maximize total expected reward you would just pull the arm with maximum mean all the time so the optimal out is simply the one that maximizes me now then for this problem the accordance should simply try to find this to try to explore in a way there is this less time on other arms and quickly converges this Quran and the measurement of performance of this algorithm is the loss suffered by not pulling this on the one arm that is the best and that is what we defined as regret and I'll state it they regret in a different place so that that will help us understand how how one would go about finding algorithm that has small regret so the regret is is the is the loss for not earning the optimal arm we know it's the one which has the highest mean so the regret for not playing arm or tomorrow but instead playing sub sub optimal arm is just the gap between the mean so that's the support in every time you don't play so if you have some number of times you played a suboptimal arm I then regret is simply the Delta I times the number of times you failed e of suboptimal not I and some overall suboptimal arms so the key is that we want in algorithm that that can figure out how to play different down so that we minimize the number of times of you play a suboptimal so what can we expect in this problem this problem is is very well understood theoretically in the sense that we we know exactly what can what we cannot see so the best you can do and this is a very very strong lower bound there are very few such lower bounds in in theory actually the bear this is not a worst case learn this is an instance size of so you give me an instance and for every instance I can say that any reasonable algorithm so I will define the reasonable algorithm you cannot achieve a regret that grows better than log t and we even know the exact constants that it grows with which has to do with that instance so so the more complicated expression for Bernoulli case is this but it's somewhere close to the some overall suboptimal arms of 1 over Delta so you given an instance that is at fixed rate so the gap between optimal and suboptimal or dips and we know that the regret has to grow log ethnic in time and with this this multiplies for every instance we also know worst case log on which is that exist some instance where the degrade goes square root with time in square root with number of hours so this is the this defines the limits of what we can do and the question is can we do that so to highlight the difficulty in the problem or the trade-off that we are dealing with let us look at a simple approach that doesn't work right so what is the most one of the straightforward approach you you just you make a decision based on your data right so you look at the current empirical mean from your data so far and you pick the arm with the best mean so whichever slot-machine gave you the highest average money pick that machine next so that approach doesn't converge it doesn't even converge forget about regret so it can be stuck on a suboptimal arm forever with a very high cognitive so it's simple example suppose you have to Bernoulli arms if one Bernoulli young gives you free board with 3/4 probability when you see one for Authority if you knew this you would know that you would have to pick the first term all the time if you pick the second term you suffer regret now I'll show that you if you follow this strategy you could be stuck on the second arm forever with a constant force so now with the constant probability whatever you do initially so initially you play the play each on sometimes it says in whatever fraction you want to play there is also probability that the second arm appears to be better it can turn out that way be the constant profit and it evenly it even happens that the second arm is actually very very close to its means second now observations are not very from its me but the first time is me no it's me so first time is like save on my 10th we need right now second arm is closed to muffle it can happen be constant probability now based on your strategy you will look at the data and say I want to place economics so you will improve with observations perfect now that will again make you play the second now because first time is stuck as 1/10 and you'll keep improving your data but you'll only improve it for the second down which means you will keep stuck at second up since you are not playing the first term you'll never improve your observations about first time so that is exactly the problem that this algorithm is not exploring it's not even improving its observations about the first time since it can improve only by playing the first time now the diselle condom can be fixed and in fact the popular upper confidence bound algorithm which some of you might have heard of fixes this algorithm and achieves optimal bounds so I wanted to to put this slide here to emphasize that the focus of the work that I'll present mainly is not to achieve or improve state of the identity when we started this work the state of the art was almost closed in the sense that we had there is a theoretically optimal algorithm that exists for it then existed even at that time for multi-armed bandit problem the the fact that it matches exactly the instance files nor mouse was proved almost the same year that we started this work but it does exist right so this is this talk is not about improving the state of the art in 2001 bandit algorithms bar it is about studying and algorithm which is which is very nice and elegant and it was very very popular and what our aim was to understand why this algorithm works and end and I know and in future try to use such a technique other problems and I'll in the end I'll show you a recent result in which we actually improve the state of for reinforcement learning using this algorithm so let's come to the algorithm that we are going to talk about today just Thompson sampling so this algorithm was in fact introduced in the paper where the multi-armed open it all problem was cause formulated in 1933 if I'd have appeared earlier in other forms but that's the best source we know and it's a simple heuristic based on page in philosophy of learning which I'll explain later around 2008 there was a lot provide technique and commercial interest in this algorithm a number of papers showed that this works empirically much better than the other techniques this was also reported to be used in in some industrial application then and more recently so these these are published sources where this was used at some other places by personal communication I work I became known that this was being used but it was understood very little theoretically so only it is not known in 2012 was that it converges to 40 mph regret what want or not so the first result we had was about showing that comes in something not only converges it actually achieves instance wise and verse is optimal regards with the optimal constants so there is no gap between appearances so I will first talk about this result so let me describe you this this is Bayesian heuristic so let's recall that what are we trying to do we are trying to find the we are trying to explore and exploit so that we play the optimal our most of the time and to find the optimal now we need to figure out which on has highest me so ultimately we need to learn the mean of each arm let's assume for now that arms are Bernoulli arms so they just produce either 0 or 1 just to keep it simple so we need to just learn every arm screen and if we find out which are man is mean here man so here is the heuristic so since we don't know the mean let's maintain a belief about where the mean is it can be anywhere between 0 & 1 since initially we don't know anything we'll assume that the mean is uniformly distributed between 0 & 1 it can be anywhere between 0 & 1 with equal probability then whenever we see an observation if we see a 1 we shift the distribution to right and will shift of course using page tool and that will come next if you see a 0 you see if the distribution to left and so you will have your current belief according to these shiftings and any given time step you take these distributions and you plan the arm every arm with the probability of being best see you apply every arm you give it a chance equal to its priority of being the best and that's what ones in sampling is so more precisely how do you update your belief so you start with some prior belief which is here for every every arm you have a uniform prior on on its mean every time you see an observation are from that arm so if you pull an obviously an observation otherwise you don't do anything for that that means so if you see an observation for an arm you calculate its posterior and you change the posterior to that updated posterior now I need at any point you will have a belief for every R now you know you don't choose the arm which is most likely to be the best arm you choose everyone with its probability of being the best so if the first town has 70 percent chance of being the big health of having the highest mean you don't pull the first time you pull it with 70 percent probability so what are you are doing is you are you are giving benefit of doubt to those who are next explore you are giving a 10 percent chance arm a 10 percent benefit of doubt and you are exploring it and what we can show the theoretical results will show is that this is the optimal benefit of doubt you should not give any more so I'll describe this in algorithm in more detail using a specific posterior and prior update so if you have Bernoulli rewards natural choice is to use theta distributions as patient priors so you start with a uniform distribution on the condom on the belief which is you start with beta 1 1 beta 1 1 is same as from distribution and it has 2 parameters alpha and beta now what you can show by just doing the using the Bayes rule is that if you start with the beta and pamita prior so if the parameters are alpha beta right now and you make a observation which is 1 then the posterior is simply beta distribution with alpha increase fiber and if you make an observation 0 then posterior is simply better distribution with beta increase fiber so that's the nice thing about this this posterior is that it works very nicely with Bernoulli likelihood you can simply update it by increasing one parameter rather in fact if you look at the these are the curves for different alpha and beta larger alpha means that the peak is on the right so if you look at alpha equals 5 which is the blue curve the peak is on the right and smaller alpha means the peak is on the left so it's really like shifting that further at the distribution based on seeing 0 or 1 now given this this is that Gotham so you start with the beta 1 1 belief every time you have your current belief so because you increase alpha every time you see a 1 or a success your current belief will have alpha parameters number of successes or 1 plus 1 and beta parameter would be number of failures or number of zeros so this will be our current posterior for every I based on how many times you tried it and how many successes you saw and how many failures you saw so you take these posteriors and from every posterior generate one sample and it's it's max so why are we doing this so this will exactly give you an armed with this probability of being the highest this is an implementation of you don't have to really cast the probability if you play the armed with higher sample then that same as saying the armed with this probability of being the highest so that's all and you just simply play this arm and if you see a success or a failure based on that you obtain the posterior of this are any questions so for this algorithm for Bernoulli rewards you can show the exact we can show the exact Bernoulli instance wise optimality so the instance wise optimality the exact expression only works for if you are given the distribution so if you know it's controlling so then you can show exact contents but we can also show the worst-case optimality within log T factor within a square root lock detector so the field the only assumption that we made was that the like the distributions are Bernoulli they produce 0 or 1 we also study another version of thumbs in sampling using Gaussian priors and there you be don't to make any assumptions about what the distribution is you only assume the distribution is bounded or has sub curse in mice so I'll explain this so the derivation so here is I want to make a subtle distinction here the derivation of the algorithm is based on assuming our sin distribution but when we do the analysis we don't make any such assumption so so I will describe the derivation of the algorithm which works only for the cause in description but you can take that algorithm and just apply it to whatever observations you are seeing and the analysis will work for that so that so to derive we'll go ahead then let us say that suppose the reward is Gaussian with mean mu I and standard deviation 1 so Bo Peep on 1 parameter distribution now to derive their point oh let's say we start with standard normal then you can show that every time you observe I make an observation reward observational Gaussian likelihood with a Gaussian prior then the posterior is simply going to visas it's going to be the empirical mean as mean and 1 over the number of trials of that arm as radians so it has a simple update every time you pull an arm you'll see everybody update its mean and you update the variance like this so then if the boredom becomes really simple you simply take these four steers sample from each posterior pull the arm with maximum sample and then update the empirical mean in bummer of trials so so you can think of like terms of sampling like as a randomized version of UCD yeah but as I said the derivation we that we you do is using this assumption that the likelihood the rewards in three different girls in but you can use that the given are called for any any Devon distribution you don't even if you don't know what the distribution is you just take the current observation you take the empirical mean and update the post feeding is there any confusion on that point so you like you need some smoothness like what any problems I will tell you what the property will need so the the bounds that we prove were not either bounded reward distributions or sub Gaussian noise so so the Bauman's that we prove since since we don't make any assumption what distribution is so we get is what we don't get the exact constant but they love flamenco so what the bound we get is the best available bound for any algorithm in terms of instance by some humanity which is logarithmic in T and the constants are sum over I 1 over Delta it's very very close to the lower bound this is the best you can get if you don't make any assumptions on what 34 distributions now we also and we also get the worst cases there's not within the law against being part and the assumption as I said we do not assume even even though we are using Gauss in interpretation we do not assume that the actual reward institution is causing nearly as you founded rewards or sub curves in maths so what I will do next is is explain why this kind of algorithm is working what's going on here and to do that I will use like I will show a proof for a very simple case of two are so first given to give an intuition of what is happening here so the idea is that your current posterior shape depends on when what your observations are and to how many times you have played at all so if you have played an arm large number of times so this is the beta distribution part so if alpha plus beta is large selfi was number of successes and beta was number of failures in the posterior so alpha plus beta is total number of claims here either get success or failure so if this is large in fact the variance of the posterior is going to be small so the position is going to look very paint if this is small if the number of places for the posterior is going to be spread out so when the posterior speak you are basically saying I am very very certain where the mean is if there is no the probability of it being outside this is really really small I'm almost certain that mean is somewhere here Here I am saying I am really not sure I have not seen enough data these can be anywhere so what is happening is that as you play an arm more and more your uncertainity is getting reduced when you embed an arm less your uncertainties more which means that you are giving it some benefit of doubt you are saying that sometimes I might sample from here and pretend that my mean is very high sometimes I might sample from here and pretend my mean is very low so when I sample from here I would get some exploration for this on so we will see this precisely how it is working using two arms so let's say you have two urns one with mean mu1 and you too and less as you know one is higher you don't know that of course that's an algorithm but first off is actually the optimal norm the difference between two is your regretfully suboptimal oh that's Delta it is only one Delta here so to get a lock T over Delta a great bond that's of a target you have to basically prove that the second arm which is the bad arm you don't pull it more than log T over delta squared that's what you need to show so let's first consider an easy case so suppose that both the arms on 2 and on 1 have been played log T over Delta squared times that's when then then the things are easy why because I can show you that after if both arms have been played that many times then their means are well separated the posteriors are also even separated so the empirical means would be when separated you can show that by just doing some kind of a turn-off bound so if you have log T or Delta square moves there means their error in the empirical mean cannot be more than Delta by 4 so their means are well separated their posteriors are also well separated because because the standard deviation is actually proportional here to 1 over m square root number of planes beta posterior standard deviation so if you have that many plays then again your means are separated by at least Delta your posting is the standard deviation is smaller than Delta by 4 right so this spread is smaller than Delta by 4 so they clear well separated from each other so which means so what does it mean the picture looks something like this so the two arms there is no confusion even with the spread of their posteriors there is no way that you would sample from this annual sample from here so the samples will be always higher for the first time and lower for the second so you'll never make a mistake after this happens so what we have to show is that the regret is low before this happens another easy case is that if arm two has been pulled less if total number of poles of arm is less than this then of course the regret is small because baton has not been take so the only difficult case that left is that arm one has not been played whereas arm two has been played a lot and this case why is this tricky so in this case if r1 has not been played its posterior looks something like this it's very spread out whereas arm two has a peak posterior in these there are two situations here at least the empirical mean of arm 1 is higher than on 2 as we want the other possibility is empirical mean upon one could also be lower but both the case are problematic because what can happen is Faraone you could be something from here there is 4 on 2 you might sample from here and so there's a very good chance that you will think that down to is better so how do we get out of this situation so in fact the uncertainty in arm one is what helps us so I I said that you could be getting a bad sample from one because of this uncertainty but also because of this uncertainty you could be getting a good sample from harmful so we can show that if the if the one is so spread out there is a constant probability that you will get a good sample it is the constant part you are getting a bad sample but there is also constant you're getting food sample so roughly and I'm being vague here roughly every constant number of steps you will play on one just by because it's it's so uncertain that you will just by chance we had a good sample for a bun and plate so every constant number of steps you will play this arm so constant times log t / delta squared after that many phases of playing on one you will reach that nice situation that we had in the field where both of the arms have been played so how much regret you suffered in this time is just locked R square times Delta now I am being loose here with constants and to get exact constant matching the lower bound you have to be more careful about because this constant property actually is not acid it decreases you you get smaller and smaller steps between two place of gamma so that's our view of the proof and these somebody that the takeaway point is that the variance of the posterior here is implicitly enabling the explanation and avoiding you to make mistakes so I'm the rest of the talk I will give overview of the other extensions that we have developed for linear contextual bandits and reinforcement learning I won't have time to go over the results dessert assortment optimization so linear contextual bandits again we will play a catch-up in the sense that we prove the same results that are known for UCP but here is where you see an provement and reinforcement any questions yeah so I get the intuition right when you're comparing just a means your bad example was you have to recorder one quarter and something really bad happened and you had 1/10 estimate so if that happens here don't you just get like a 1/10 estimate with actually pretty little variance yeah taxons up when you think you just get in a state where it takes a long time so that content can happen with a with a high gravity or constant fog the only gods are number of steps right so if you have played the first time 100 times then what's the chance of getting 1/10 very low so only if you have played a few steps of first term then you'll get this bad bad instance but if you have played the first console few times then it's uncertainty is large so the variance is so large that will enable you to to go back to the first all right so so the the most so deep what I describe as 1t unbounded problem he's I have literally seen it used that way because in most puzzles the number of harms is very now so any regret or form square root n three or n nog T is not really useful because n is really really large it could be all possible products that you have it could be even a combination of product and customer because the response to a to a recommendation actually depends not just on the recommendation it depends on who is seeing it so it depends on the context so the most useful version of manners is actually contextual bandits where you assume that the response to pulling an on depends not just on the arms but the context on which it is shown and also where the number of arms is very very large so you how do you handle the scale and context and the standard mail will go back to supervised learning for that we say that we will describe the context and the product by their features so we so what we are doing is we are getting rid of the basic assumption that we made so basic assumption we made is when we build an arm you only learn about that time and nothing else but that's not true in practice when you recommend a movie you also learned about similar movies for similar customers so we want to take the singularity into account and the wave is takes in radiant account is two features we will say the similar feature means similar preferences so so so to get a contact description of the problem we go to parametric models that map the customer and product feature or the context in product features two different consoles and that's the contextual bandit come when it's coming now we will be doing exploration exploitation still but to learn the parameters of this model of this professor so let me describe a concrete setting which is the linear contextual bandits so now you have n arms but possibly very large it could even be in finite n right so it could be different combinations that you can create for different customers of ours and so on so it could be very very large in but inviting that every possibility is still described by some feature vector they see and that which I render could change with time so it could be the feature vector XIT forearm ie at time T so if the customer is changing with time individual feature vector could change in time but there is a compact description of references that's the main assumption that there is a linear parametric model that map's the features to the remote so in the linear medium valid case you assume that the expected reward for arm I is given by a linear transformation of the feature through a ridden parameter theta and the goal of the algorithm would be to learn the hidden by a parameter theta by observing only the noise realizations of the word compasses to one main point that I want to emphasize here is that now the optimal now if you remember in multi I minded case I just said that octave alarm is this fixed found at the highest me you have you should pull that arm all the time now the optimal arm is no longer fixed it really depends on the customer who comes at time T and what are the choices available at time so it depends on the context so the optimal arm is the arm that you should be pulling in that on that time step depending on the features of the customer at that time steps think that remain constant here is the parameter theta that's what you are trying to learn from all these observations and the goal again is to max is to maximize minimize regret which is to say that it for every time and every context I pull the best arm I should be Minnie should be comparing to that my choices so XT is my choice and XT times what I should have done if I knew that ax I thought X with your features yeah xx IDs the feature I'm saying okay so maybe the over node in the symbol so XT that says the choice I the XIT of the are mine I chose their ability their because I can imagine sort of constructing an input where the general arm is given by the arguments but baby you ever deviates for example we pick the second-best arm that's really horrible so you could construct such a path in the hood you're good this assumes the martysb so in fact the source yes so so we do assume the model is yes so it is assumed that the expected reward is linear it's not agnostic so so nice and model a more practical models which we to say that I will find the best linear predictor irrespective of whether the actual model is linear or not but this is not that that's it we are assuming the model and that's quite critical to on the recurrent otherwise we cannot say how about how bad your second choice is that's good question remotely yes just want to clarify again on this X star T I mean normally when I think about that feature vector some are going to be kind of context features and some are actually going to be action features but the context features at time T you don't get to control those and so yes the only does it affect some of those features and I just want to make sure that I've got the right understanding of your notation yes so X I did X I substituted them by both int so it's think of this as appended vector of all context and action features or even combinations of those okay all the possibilities you have so they might be decided by the wrought by the given customer so let's say the customer comes figure out in Kerrville features that's it that's the option you have so from there vailable options you pick the best one so now the hadith of the sampling version for this problem is a simple extension of what you would do in offline kids so as we saw in the mighty unbanded case you took the empirical mean and built a posterior around it in the same way in fact the concepting approach for this case also works out so if you had just some data if you hadn't seen oisi observations of rewards from a linear model one of the possibilities do to is to do a least square estimator right so you build a least square estimator it has some standard solutions I'm not detail and B T inverse is it takes this form which is like your it is also often called like covariance matrix of your estimator linear least square estimator and the calculations work out so that if you start with a standard normal prior multivariate normal prior then the posterior in fact is simply the one which has mean as your current least square estimator and with covariance as your covariance matrix of the estimator from the least square estimate so it has a same kind of form as we saw in the one dimensional case so now the algorithm is simply you take this multivariate normal distribution and I use this new squared because we have to do a small scaling of the covariance the scaling is much like some law gets midterm here and you take this posterior and you sample and theta from it you see sample and theta and then you you pretend that that data is your real theta and you pick the arm that maximizes x transpose theta so if that theta was your real theta which one would you pick just pick that our sample from your current posterior belief about their vodka dies and you pick the arm in the West first prediction according to that so again you are doing the same thing instead of using a least square estimator you are using if uncertain version you're putting a steer around it and you're using that version of that sample from that posterior instead of T is quite a stool and for this algorithm again be shown that the regret can be bounded square root T of in terms of time and D is the dimension of theta so D raised to 3 by 2 so some important points here is that first of all again even though there Carlton was derived based on Gaussian likelihood you don't have to assume cause in my finger anywhere you can use it for rewards from any distribution as long as the distribution of bounded or subconscious minds there's no dependence on number of arms here so the things I'm hiding in tilde are logarithmic in time there's no dependence on number so you you could really as long as you can pick an arm in polynomial time from your cell you can even have infinite number of arms or a continuous space of arms the only thing is you have to see how will you pick an arm and even a theta from your scepter so there's no dependence on the more arms the dependence on square root T is optimal the dependence on D in fact the lower bound is d naught T raised to 3 by 2 but interestingly in fact there is no polynomial time algorithm that can achieve the UT that you see be version can achieve if it's not on this competent even table depending on the set of constant lab so there is a relaxation of UCB which is polynomial time which also achieves these two three by two so you can say that in fact that is the best known for for a polynomial time algorithm for this problem and in fact this is an interesting open problem in some sense that kind of has slipped through cracks that I don't know it's maybe better working on this but this gap will forever be the polynomial time algorithm has not been achieved where root T vs T raised to 3 right yeah ok so now it brings to my final problem here which is the reinforcement learning so this is very very recent result now imagine that again you are clicking actions or arms but now the response doesn't just depend on the on the arm but it also depends on the state of the system so for example if you are if you are selling something or or if your customer is coming in your setting price whether you want revenue will you achieve will depends on your inventory state how much how much product you have otherwise you might lose the demand or if you are playing the game more like a chess then your your certain set of moves available you can move upon here or there but what will it do really what the effect would be depends on what state of the board is right now you can move the pawn left or right but what it will do depends on the state of the game so your response critically depends on the state of the game and now the the critical thing is that now you don't have to just learn the reward response model you also need to learn the state transition model how do the states change if I if I take it in action versus another so formally now you are operating with an MDP so your you have a Markov decision process in which your reward depends of the current state in action and your state transition also depends on current state in action and mainly what I want to emphasize that is that you don't know the transition dynamics so your job is to find the optimal way to make decisions while learning the transition state transition function you also have to learn reward response model in general but that problem is very similar to what we did it might be unbounded so I want to focus on learning the reward it's the state of the Shahadah again you have to learn from your observations only so if you have your current state EST and you take an action eighty you see a state transition from st to st person that's all you get to see so if you if you never played a certain action in certain state you don't know what will happen if you if you did that so you need to explore different actions in different states but the problem now is much more complicated because it might you might go to a state from the it might even be impossible to reach another state so you have to plan ahead now again you can measure the performance in terms of regret meaning that you think of your total reward that you achieve depending what state you went to you compare it to reward that you would achieve if you play the optimal policy of the time so now you are thinking about optimal policy well this is the mapping from state to action so if you had a fixed policy to decide every time you had a given state and you used it all the time this is the reward that you would have achieved versus whatever you got depending on whether whichever states you went and so the optimal policy might go to a totally different states there is some technicality here like such a stationary solution may not exist all this that's why we make an assumption about communicating everything face me I will not go there it basically means that you cannot have this impossible situation where given a state there is no way to go to another state so you cannot get stuck at this is always a finite time policy which will take you out from one state and make you go to that state you may not know what that policy is but there exists all this way to move from one place to another in finite time and I won't go a lot of details but the nice thing about about about thousands I think is that you get a very directly a very nice algorithm for this problem also so now you have there your observations out every time you have a state you take an action you see the transition from one state to another and what you are looking for is to learn the transition function meaning you want to find out if I take this action in this state this state will I transition - with what probability so you're really trying to learn the probability vectors PSK for every state in action so it's a transition probability vector that sounds to one so you have kind of the multinomial output right now you will see a reward 0 and 1 you see one of the ends and states that you would so instead of beta is a very similar distribution the multinomial version of it we use we use that and it again has the same nice property if you see a condition to a state it has instead of two parameters it has n parameters and if you see a transition to one state you just increase that parameter that's all you have to do to update the first teacher so really easy to maintain the posterior and now the basic algorithm is the same you you take your current posterior beliefs about every transition probability vector that you have you sample from it and use the sample instead of the two that you don't know and you solve that ending the instead and every time you saw that you see you get optimal policy for that sample matrix and you use that to decide your action that's the basic algorithm unfortunately we could not have analyzed that so with the main change we had to do is that we take multiple samples from that posterior long so we take s samples where s is the number of states and we form an extended MDP using that so we call it optimistic posterior sampling and here is the main result so this is a recent result very sure that that for any communicating MD p.m. the diameter D like you can reach any place of D you get a regret which is of order d square root SAT so in fact the lower bound is square root es ET so this is the optimal dependence on esta gente and finally we were able to be at UCP let me which could only get a square root 80 and this is actually very interesting reason for why this does better basically in this case what happens is that it's not so you see we if you know if some of you know what they'll call them is is based on using an optimistic version of your estimate or something that will be better than the actual now since in transition probability kiss it's not clear which is better it's a you it's a probability transition probability vector what is better really depends on what states it will take you and what's the reward will be in those states and so on so it's not so straightforward to compute for is better but if you think about tons of sampling it says samples from around age it doesn't really have to basically know what is better and so you all all the complexities in proving proving that it works the Gotham itself doesn't have to know where both parties on optimistic samples so the maximum thing I can take that offline but the matching thing is actually not for that reason it's because I strongly believe it can be removed it's because you have like multiple equations to satisfy okay so so we had to like take multiple sample so that all those equations could be satisfied all right so to consult yeah you had to make multiple samples and so on you didn't have like a lower bound for that simpler algorithm for there no using so I think I strongly believe that you don't need order s samples you can definitely do with log number of samples that's what my intuition is right now can you do it constant number of ones about one sample that I don't know mainly I think the the problem that we had was deliciously distributions are very very hard to it to prove anything about like their concentration or anti cancellation is special because we have to show that it has a distribution enough uncertainity that is the current so excuse me do you have any assumption on T here T can be anything or you have some restriction on it yeah so so I yeah I eliminated that so so there is a there is a term here which is like of lower order then this term which which will become significant if T was very small so yet so there is a lower bound on T so so this is true for large T so it was like s larger than s square a I think thank you yeah I might be missing it some some function of s and a I think it's at squared a to my best of my knowledge yeah thank you right so some of the immediate things to work on is in reinforcement learning having a simpler version I think we are just limited by then analytical tools I think the who should work with few samples one thing I'm really interested is in continuous continuous state and action spaces like here basically we were assuming discrete s number of states in a number of actions and one very interesting application for me is the inventory management here because that has a lot of structure but it still so states are the level of inventories in for different products but what the states are continuous and the action space is also Guardians obviously there is a lot of work to be done in terms of like that so the main we have multi unbounded and contextual version of that you can think of parametric Policy learning is as the contextual version of this kind of reinforcement learning so so it's not about to be done in that direction like basically to hide and scale of states and actions and with something that I didn't discuss we also have recent results on assortment optimization it's also something where there a lot of interesting problems in in terms of what multinomial choice forest well yeah so when you present the assortment the choice depends not on just individual products and the resort right but the entire said because there could be externalities negative externalities and so so so we we work on one model like the multinomial choice went along a logic model but there is obviously a lot of work to be done on more advanced models right that's yeah it seems simple but you're solving the MDP after every step you do some sort of one starting at when you start all the parameters you just have to do partial new sample transition matrix yeah so so in fact that's one thing on my to-do list like to basically if we can do just like a value for value value function update or something instead of resolving it yeah it is the power scale ratio same thing right yeah so it should work it's a little bit a little bit of issues in analyzing it yeah that's that's that's an important point here also I want to add that in fact we don't we don't resample in every step so we we can do like we can do resulting in a very doubling time step that works like you don't have to resample the whole I'm dipping every step so you would believe something only logarithmic the number of times I see yeah so one thing I missed you were requiring too soon you knew the size of the MVP or could w PB country in 10 size meaning the number of states are states yeah so this is all finite states in finite action so that's a big limitation I agree I I think like the useful version would be something of the sort near for context showcase where you have some kind of a supervised learning approach on top of of these things I have some potential formulations I don't know which one of them is like none of them are exactly what people do in fact is yes so for example the interactive the most intense the question is can you like do this with deep learning I have don't know I don't know where to start analyzing that so but I would start with like a linear parametric formulation of the value equations like she probably will be visiting r-spec for one day a week maybe in the next couple of months so she have other questions or discussions [Music] you 