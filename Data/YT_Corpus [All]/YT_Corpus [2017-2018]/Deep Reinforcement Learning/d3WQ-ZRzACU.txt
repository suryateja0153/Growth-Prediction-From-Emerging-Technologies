 (bright electronic music) - Qian Yang completed his PhD here in 1989. I was his PhD advisor. He was a professor in Canada for about 10 years and then he moved to the Hong Kong University of Science and Technology. He currently has an endowed chair and in addition he's the head of their Nuclear Science and Engineering Department. He has a hugely influential set of academic publications. His h-index currently is 89. In addition he has founded two major journals. The ACM Transactions on Intelligent Systems and the IEEE Transactions on Big Data. He's the fellow of four major professional organizations, the AAAI, the IEEE, the IAPR, and most impressively the AAAS. Finally, he's a member of our department's Alumni Hall of Fame. Qian, we're very proud of you. - Thank you, Dana. Thank you everybody for coming on this Monday afternoon right after Thanksgiving. So my talk title is right here. When Transfer Learning Meets Deep Learning. You must be curious abut this type of knowledge. Everyone has heard about Deep Learning, but not so much about Transfer Learning. So let me tell you from the beginning. Now everyone has heard of alpha mode, right? This is the reason why AI interests and there's a surge of interest in AI. Do you know what AI cannot yet do? So if you take a certain size board and had it perfectly trained perfectly for this board. But if you change the size of the board then alpha mode to be completely retrained for that board, okay. Now if you take that even further. If you train alpha mode for the game of golf and if you take it to a different game, say chess, then alpha mode cannot adapt itself. At least not yet. At least not without transfer learning. So what is transfer learning? Transfer learning is our ability to adapt from one skill to another. So, for example, as humans we learn how to ride bicycles. So we find it very easy to adapt and go riding a scooter, or motorcycle. By the way, this is transfer learning in Chinese. (audience laughs) So enjoy that, I bet you can figure that out. Okay, transfer learning is not yet as hot as deep learning. But it will not take long. Okay so recently some leaders, like Andrew Yayman has openly spoken about transfer learning as being maybe the next big thing after deep learning. Okay so this is currently is working. We are along the timeline, the next curve to rise is mark transfer, and there is reason you're going to, so how I will explain that is if you really want to find about transfer learning, you can check Google Scholar. So this is a recent chapter on Google Scholar run by the number of citations. The first in the list is a paper written by my student and I, my study on transfer learning. Currently has more than 3000 citations. The second one was written by Andrew Yayman, and students on self-taught learning, and the third one was again written by a student of mine and myself, whom has boosting transfer learning, and this one has over 1000 citations today and rising quickly. Okay so the reason why I'm showing this is because I want to share the excitement with you. This transfer medium is catching a lot of attention. Particularly due to apparent limitations of the learning. So what are some of the limitations of deep learning? First of all, let me look around, especially in every day life. Not very often do we have big data. We often have small data, small amounts of data. They are sparse, sparsely labeled, annotation, deal with a lot of noise, a lot of unlabeled data. So when we take a big data and train a model, we wish to expand, to generalize in fact that model to (mumbles) or related to where there is only small data, and we wish that model to still work. Okay so this is a practical concern, we need this training when you look at so, for example medical imaging. So we did a recent survey in order to find a student of mine a feasible research topic for his PhD. And we collected all sorts of collected data, we found only donates maybe four or 500, at most 600 labeled images, and anything beyond that is beyond public reach. Of course there are hospitals, they are 10s of thousands, but they are not in this number. In education, it is another area where the interaction so a recent instructor said she had 20,000 students in his class, and he was very excited, though when we looked at the interaction between him and his students, the data is extremely sparse, can not be used to train an automated chat bot. So these are examples we are frustration came from. Another reason for motivation for transfer learning is because unlike suffer generic practice, many learning researchers do not pay attention to reliability in their analysis. When you have a mission critical software such as a statistical model and when you tweak the other level data a little bit, when you tweak the model at the end of it, it often fails, there is a dramatic drop in performance, and this is not acceptable, in the real world. So we wish to extend the model to other situations, neighboring situations by changing the environment by data. We don't want the environment, the model to change, a exponent of delta. And finally, we are in the era of personalization. Everybody carries a mobile phone, we look at news on the mobile phone, we talk to people, so we wish the phone to know us. Now how do you do that? How do you ensure that a recomender system operating on the phone to know about you? Well, our current way is to upload our data to the cloud. And like the machine train the system with our preferences, with our bias, and then download the resulting for our phone. So this process actually leaks our data to the cloud. So when you are very concerned about privacy, this could be a problem. Now on the alternative is to train a universal model in the cloud, and download that model to your phone, and having adaptive right on your phone. Imagine your phone we can't ever more horrible in the future. So between the expectation can be done on your phone. And in the whole process, there is no bidirectional movement of the personal data. That's protected in the environment. So we are working on a model of test to really solve this privacy issue on its route. But when you work on transfer learning, you know it's very hard. So I took this slide actually from, from the web, somebody has already made this slide, not from learning, not from mission learning, this slide came from education psychology. In the field of education. This term called the learning transfer has been a (mumbles) over the years. Where people have, there's a branch of this education theory that believes that the metric to measure whether a teacher is good is when the student of that teacher can easily adapt. From one area to another. From one class to another. From planning information also, data must be a good teacher. Learning transfer is such a measure, but okay it's the same idea, transfer learning, but it's hard. And the primary reason for why it's so hard is because when you compare two domains, and when you want to transfer knowledge from domain A to domain B, what we are doing is looking for your varied. We are looking with domains numbers, but those improvement number is looking for invariant in the programming property is a way to prove correctness, and here looking for invariant is a way to transfer knowledge, so maybe use this example which I personally experienced. If you drive in the US, it's like this, the driver is on the left hand side of the car. If you drive in mainland China, it is the same. Whereas, if you drive in UK, or if you drive in Hong Kong, the driver is on the other side, and I don't know how many of you have experienced of renting a car and start driving, and drive into trouble on the wrong side. I have, okay, it was very dangerous. But knowing transfer learning can save you. Thinking back, and the key is this. You look for the invariant. Now who can tell me what the invariant is from these two pictures? Okay. - [Audience Member] The rule that I was told was you always keep the passenger side near the curb. - That's right. That's why you're still alive. (group laughs) I say that because Dave goes to, goes to Hong Kong all the time. Yes. Now so the invariant, the way we saying it is the driver is closest to the center of the road. Or the passenger side is always closest to the curb. If you do that, you switch immediately. And if a system can find that, this is the key to transfer learning. This is the invariant. So our job is to enable a system to do that. And so how is that done? And this picture on the right illustrates that. Let's say the green part is this one machine learning domain, and the red is another. This domain is known as the source domain. This is where you have a lot of data. This is new, the red is new. And now you know then how you can transfer much knowledge to the red part. And the key is to the find the knowledge that's invariant, that's common to both the red and the green, and then transfer it this way. And move the knowledge that way. So there are various ways to do this. And here is the simple summary for this small word. So the first, the green part says instance based approach. This is one class of algorithms that can look for this invariant. Another one is the less feature based approach. The third one is known as model based. And the last one is relational based approach. So these four approaches pretty much cover all methodologies in which transfer can occur. For example, for instance based approach, let's say you have lots of food halls from say the image net in the source domain with labels, and if you're only interested in a particular kind of pictures the instance based approach is to look for similar images to the target image and give them higher weight, and as a result, you boost the twinning data by making multiple copies of similar source images. So this is the idea behind the so-called instances based transfer. Whereas the feature based transfer is to compare the two domains and match both into a sub space, in that sub space, the two look exactly the same. Identical. You remove all the different features, and the result is the same. You can even extend this to an area where these two domains look like completely different. But they have similar meanings, so this is a Chinese poem, this is a Chinese painting, then express the same thing. Okay so far I think (mumbles) academic, and these are just maybe live on the research papers. But I should tell that transfer learning is already taking place in industry and making huge impact. So here is for example. Before we go into further detail, so this a company created by my student, it's in Beijing, it's called Four Paradigm. What it does is it does all sorts of question learning projects with tanks and financial institutions, and they have this particular problem of modeling credits, loan credits for buyers of luxury cars. Now as we know, luxury cars are sold to people with lots of money, right? And those people are very few, and so as a result, the samples are very scarce, very rare. And so we just need some host, build a credible model of luxury car purchases. So what they do is they go through small ones, and so called micro models where the loan is in the order of maybe a folder in 5,000 US dollars, X amount. And those loans can be in the order of 100 a year. Okay, so with that data you can build a model, but that model is for micro loans, so there is a difference between micro loans and large loans. Now transfer learning allows you to move that difference and look for what's common, and so by using this transfer learning they were able to increase the effect by 20%. So it was very effective and is being used by the bank. Now this is the first part of the talk, which is introducing what is transfer learning. The time of the talk is from transfer learning meets deep learning, so now we have to think of a word of deep learning. So first slide we give you the analogy of transfer learning used to be done like this, like building a one-story house in the Chinese architecture. But when you go deeper, you have multiple levels that brings extra elements, extra, so in other words, what you can achieve here you can achieve more, by being more qualitative. So here is the qualitative one. So let me give an illustration of that. So this a deep learning model turning sideways. So the input is here, the output is here, so this multiple layer is here. So roughly deep learning is doing not only transformation between layers, and using a museum or using Raspnet or whatever transformation, but the architecture looks like this. So let's say the blue is one domain where we have built a very credible, and reliable model. Now suppose we have a new domain, where we don't have so much data, but we built a rough model. So we are interested in how much knowledge in the blue domain have you transferred to the red domain. Now deep learning allows us to answer this question layer by layer, so we can ask how much is the first layer transferable. How much is the second layer transferable. Under what condition. In other words, we can for example, draw a relation between these corresponding layers, and we can ask what the differences are between these top layers. So these are known as domain distance, we can now finally ask about this distance layer by layer. So there is this recent paper by Benjamin Appo, they are asking exactly this question. So my student looked at this and thought "Hmm, what an interesting idea". And so what about other work that mixes transfer learning and deep learning together? First of all, do they give us an extra advantage in terms of performance? And so this is the result of her investigation. So around the horizontal axis, we have the year in which this work appears, and along the vertical axis we have the performance, so you can consider this as 70% accurate, and without looking in details in the domain, we realize that those red ones correspond to transfer learning plus deep learning, and they generally give you higher performance than those that use a single layer deep learning, transfer learning, so if you use transfer learning alone, so this for example, TCA, what's the result of another student of mine in 2011 also highly cited near 1000 citations, but gradually the citation rate is slowing down. Why? Partly because he didn't use deep learning, and this work that appears, so for example this work is the result of Xing Mar researcher working in Berkeley. And the result is like this. Okay, the graph is very ugly and sophisticated. But you can try to understand it layer by layer. So first of all, the first layer is marked frozen. Frozen means from the source to the target, you transfer the whole thing as whole. You don't make any changes. The next several layers are more about fine-tuning. Fine-tuning is you allow the transfer to happen first as a whole, and then adjust it by later twists. And the last several layers are marked with this called MK and MB, which is the distance measure based on multiple distributions, from unity distributions, and formulated is rated here. You're using kernels to measure the distribution and judge how much difference should we allow these two domains to have. And these correspond to their final tasks. So they are treated differently according to different layers. And so this exactly the idea when we go a step further in terms of studying how transferable these domains are. These are three domains, this one is from Amazon, you can see Amazon has taken out of the background in order not to let people copy from their images, and these are from webcam and other means. Then people have studied, so this work titled all transferable features in the depper networks which appear in number 14, quantitative study, layer by layer, so this one is the first later, second layer, this is the last layer. And there are several conclusions you can draw. The first one is that as you go deeper and deeper, the transfer of data actually goes down. So that means on these vision based domains, lower levels represent more general and more transferable, and more invariant features. The second conclusion is that if you allow things to change a little bit, so instead of frozen, you allow some fine tuning to happen, then this performance can come back. So the last one is when you really pull these two domains that are far from each other, that the performance can quickly go down, so this idea of a deep learning and transfer together have to really watch out for the original distance between them. So based on this quantitative knowledge we can then design a field engineering solutions. The first engineering solution is what we have unsupervised data. So we have unsupervised data in the source domain we have unsupervised data, in the target domain, can we do a transfer? And so the idea is you can partition the layers into two parts, the first part is known as frozen or protected, and the second is called adaptation, and here you can write the loss in terms of two losses, one is the loss on the source, and another one is on the target, and the laws can be described using any number of these loses, one is called discrepancy loss, and serial loss, or reconstruction loss. And not going into details of this, but these are engineering tools you can use. Now once you have an additional label data, say in the source domain then you can introduce another loss which represents the loss in the source domain, this one source. And so this additional domain can be an extra guidance. So the final picture that looks this. A domain, a source domain that can reconstruct itself, a critical domain that can reconstruct itself, and some paired loss if you have a mixture of the source and the target, say you have a picture with some words in it, these are pictures, these are words, and this pair loss represents how much of that you can reconstruct. So this corresponds to the traditional, so called multi-model, in other words you have images and words mixed together. And you can handle all of this. Okay, so far we've talked about how to measure the differences between two domains, how when we have deep learning architecture, how to transfer from a source to target, while giving different losses to be represented in organization to make use of this connotative knowledge. Now we go into some more interesting topics once we can build (mumbles) together. The first one is in traditional transfer learning, people have only considered two domains. A source domain and a target domain, and wonder how to transfer from this source to the target. Now if we are in this model, we can think of not just one domain, so what if these two domains are from each other? If they are far then people in transfer learning said you can not transfer. But what if you have some stepping stones? You have some intermediate domains like this? So you have a source domain that's far from the target domain, but you can find that the field intermediate domains that take you from the original target. And this is exactly what is still in the series of publications, so let me show you the architecture for that. So the idea is you have a source domain, you have a target domain, say that as a source code it may be a bunch of text documents, the target domain is a bunch of images, so there's no way for the text to affect the image. But you have a number of intermediate domains, some mixture of text and images appear in this intermediate domains that can pull together in a chain of collection. So like building a bridge. So for this we can deal with the deep learning architecture, and we can feed it source they have intermediate data, and some target data, some target data with label. And in the process, we want to make sure that in this training we reconstruct source, intermediate and target at the same time, we'll pull together source and target by the means of intermediate. So whatever intermediate can do to pull us together will show up in a classifier to have the correct performance. And when that happens, we add a weight to an actual weight to the source data. And so eventually when this system is trained, we will have a well performed classifier based on the (mumbles) in the immediate data. Okay, so by means of just feeding this together, we can have this stepping stone effect. Now how well does this do? There are various tests that my student has come through, but recently we found this other work which is highly related to being very interesting, and the work actually came from a joint work between staffer and United Nations. And what they tried to do is they take satellite images of the African continent. And from this image they try to infer the poverty level. So knowing the poverty level of region allows them to decide on support policy in order to give different area a different type of aid. So before this was done by sending investigators to the (mumbles), and investigating which region in turn and can be both costly and lengthy and dangerous. So now the question is can we just take a satellite picture and immediately come up with an answer? Okay, well, you can't. Because these are unlabeled. But you can be smarter. You can take the same region, you can take a picture at night. And the night picture are illuminated with lights, and usually we understand the lights represent, the amount of light represents the amount of economic development. So this gives you a first hand rough idea, but that how do you segment is, this can come from image pact. If we learn a good image segmentation so far, we can use that, we can use that model to segment the images, the light images, and then we can in turn use the light images to help label the date images, and if you keep doing this and train a little bit of label data, then eventually we get a pretty good result. And the result shows that using the transfer learning, they get 71.6% accuracy, as opposed to 75.4 using the human labor, which is already getting close. So we are quite happy that somebody else did this work shows the transitive method works. The second occupation which is pretty interesting is in sentiment practical application. So the type of it is quite long. And you state the title and just, I can tell you what it tries to do. So those ecommerce to allow users to comment on different products, so these comments can be "one of the best "novels written by this author", "excellent", "I was disappointed". So things like that, right? So we have for different vertical domains say on Amazon or Chinese Baba, we can have a huge collection of these comments, and each one is about a particular product in that particular domain. So the question is, suppose we have built a credible model for say like products that's how much of that is applicable to an entirely different product. So if you take that model as is and apply it to comments of say emails and dramatically, we can see a downturn in terms of performance, right? That's readable because they use different terminology to describe the sentiment. So this is illustrated more clearly here, so this column here corresponds to the electronics domain where as this one, the video game domain. Here we have labels, we have done up and down, and here we don't have any labels and our task is to predict the labels on this side. But if you just read this carefully we can find that they are all using different terminologies, like this one says, unit, this one says picture quality. Whereas here it says, hoot, this says "Part". So they don't correspond to each other. But if we look even more carefully we can find some words that are domain invariant. Remember at the beginning of the talk I was talking about looking for the invariant as they key of transfer learning, so this red marked words import are invariant between domains, like "good" and "Good" here means positive sentiment. "Excited", and "excitement" means positive sentiment. So these words that are indifferent to the domains and yet reveal sentiment, known as pivot words, because they connect different domains. So okay, this gives us a hint, if you can find enough pivot words, then we can easily transfer the sentiment model from one domain to the next. So finding the keyword is the key. But how are people making use of the pivot? About 10 years ago there has been two pieces of important work. One was by this guy called John Blizzard. I will explain, he was a researcher at Microsoft Research, and he actually spoke fluent Chinese. He worked in Beijing at Microsoft Research for a while, so we talk a lot. And his method was considered state of the art at that time. But there was on drawback, which is he relied on people to pick out the pivot word, so it's totally manual. And because of this, it's very hard to scale. Then a student of mine turned at home, they started using automatic method to find what pivot, but it's very rough, you loose class treatment, spectral class treatment at each level. Now recently a group of deep learning researchers started looking at this and building the so-called black box model. They can make very accurate predictions of sentiment in one domain, but to transfer is still very hard. To transfer between domains is still very hard. Now when they can do the transfer, they can not pick out the pivot words, plus the models are not very explainable, so earlier I was talking to students and I was saying that one of today's trends in deep learning is to come up with explainable models, explainable terminology. So then what do we do? Well we have two goals in mind. One is in each domain you want to find the pivot so that the models are very accurate. So usually with the sentiment has very high accuracy, so that's our number gone goal. Number two goal is we want to find the word that are invariant and different to the domains. So those words which we use to build the model have to be accurate in its home domain and have to be inaccurate when it becomes to distinguish between the two domains. Okay so have to be, these words have to be blind to the domain difference and have to be very sharp as to telling the positive from the negative. Okay so two things: now while we have these two things, it's almost like playing a game, playing a chess game. Somebody wants to maximize the score while another person tries to minimize the score. So the question is can we use this mechanism to build a game? And the answer is yes. And yet we have a third constraint, which is we want the system to be a white box, we want it to tell us which words they are, the people's words are. So in research learning there is a new technique known as memory networks, and it's able to pick out the so-called attention. So to put a long story short, attention is a way to give ways to important key words when you perform machine reading, when the computer reads through a long text, and can leave some memory behind, and the memory can answer certain questions. Now you use multi-layer in order to keep track of those attention words which are (mumbles) by the first layer. So here three layer architecture is used, so this is one. Another one is, the number we want to also we want the words that are picked to be pivot words, pivot words by definition are indifferent to the domains. And so they must be very bad at classifying the source and the target. So we want the computation performance to be as low as possible. Between finding the source and the target. And then we want these two domains to work together. Side by side. So this is the adversarial problem, and we want them to work side by side to share these attention words in the two domains, so eventually this is the mechanism to make this work. So how does it work? Well, we took multiple Amazon domains and let them mutually transfer each other in order to build the experiment. So the first one is comparing with the hand-picked keywords and this one is the automatic one. The one with the deep learning method, and serving the deep learning method. And you can see it's dramatically higher in performance. Okay, and number two is comparing with the black box method such as this one, this one DANN, which appeared last year cannot pick out the important words. The performance is roughly the same, but the actual performance actually comes from the ability of our system to pick out these important words, just to read them loudly, these words are "great", "fantastic", "excellent", "terrible", and "uncomfortable", and so on. So they make a lot of sense, right? And these are yet another collection of these positive sentiments, and negative sentiment words. So this the seventh word of how we combine deep learning with transfer learning. All right. Now let me end the talk by mentioning two also very interesting work on combining these two techniques. We also have a group of students working with chat bot, we know chat bot, we know for example Siri is a chat bot, right? Echo is another chat bot. Idea Wasssan is another. But there is actually because of chat bot, why is the kind of chat bot that can carry on the conversation without much of a purpose. Just to be purpose is to have as long as conversation as possible. Another chat bot is like a student and a teacher, all right, you wish the conversation to be as short as possible. You want the sub task to be accomplished. Let's say if you're talking on the phone and buy the plane ticket, you want that ticket to be bought instead of having this day long conversation with a service person. And so this is known as task-orientated bot system. And here we are interested in building a personalized task repeating dialog system. Okay so the system lives on your mobile phone and you will get out both the general chat knowledge, general knowledge of the domain, as well as your own chat history, and can cater for your preferences. So that's our goal. Now wouldn't that be wonderful? But this is a chatting group problem because if you transfer, if you build this by transfer learning, we build a general model, how do we transfer to an individual's data? When that individual's data is very, very limited. Well, so here I want to speed up a little bit and just directly tell you some of the assumptions we made. The first assumption is dialog systems can be built using reinforcement learning, so for those who don't know what reinforcement learning is, it is a form of planning. Of moving ahead to meet a goal. Except it makes use of two concepts: one is a state, another is an action. So if you are in this state, you take an action, and it takes you to another state and hopefully that state is closer to the goal. So this is it. So in reinforcement learning people try to use a function, the west Q function to represent this overall progress or moral state. In which there are two privateers, one is a state, another is an action, so in conversation, you can consider that the conversation so far as the state, how much you have achieved, and what you will say next as the action. This is known as the speech act. So to a personalize a general model, we split the model, the conversation model into two parts: the green part corresponds to the general model, QJ, QG. The personalized part corresponds to the QP, which is the personalized model. So everybody has the same QT, different QP. So the question is how do you learn this part? We ignore this part because there are various other methods to do that. So let me just quickly and tell you that in fact we invented a dating mechanism, this is a switch, it can be zero or one. When you carry on a conversation there is a probability at any moment you are talking about something in general. You are asking a general question about, about purchasing a plane ticket, or if it's switched on, we spoke the personal situation. The personal preference, you still prefer the model. Yes, okay, so this one can learn to be switched on and off depending on the state of the conversation, and so eventually they will be able to do a lot of experiments and show the performance is really high. So just before I came to this trip I heard that this paper was accepted by AAAI this year, so people will go to AAAI in New Orleans where you will get to know this paper and the comment in more detail. So here is the baseline conversation in the user and the agent, here is the personalized, so you can see the length is a little shorter. And generally, this is the case. So this one I just quickly want to go to show you that using transfer learning and quick learning in general is beneficial to building a personalized chat bot. Actually have a demo, but it's in Chinese. (group laughs) So I'll save that for later. Finally, I want to show you something about how to use transfer learning for crowd-sourcing. As we know, crowd-sourcing is to mobilize a large crowd who are number experts who perform a task. And statistically using the aggregate information, in order to generally to find a result, such as labeling a picture. So for example if you have lots of pictures, you ask what species is in this picture, and the answer is dog. And of course there are many people who don't give correct answer. Mainly because they don't consider it as a dog, or maybe because they're just out there to (mumbles). So they're not careful with their task. So here we can use transfer learning to help with this. So let's say we have a domain which is represented by this row. We have another domain which is represented by this row. Now let's say we build some really good data. Somehow, by this one we have a lot of very reliable results of whether something is improved or something does not belong to our class. Let's say we have done something really well there. Now in fact, if for a new domain like this bottom one, if you are not sure whether the reader, the user is giving the correct label, what we can do is we can leverage the relationship between these two domains and learn about reliability of each user based on the task. So this is how you can use transfer learning for crowd-sourcing is you can use, the thing you'll transfer is the invariant knowledge about users' ability. So we use existing tasks to rank the users, and then you use the rows that you learned to come back and to write additional new results. So we won't go into too much details of this exact same that you have built a, you can build a latent model, or you can use a deep learning model for this, and the result can be multi-faceted. So in the interests of time I will stop here. Let me summarize by saying that pressure learning is to learn from lots and lots of data, deep learning especially, but transfer learning is trying to bring these bigness, the requirement for big data may not be necessary in the future for reliable learning. If you can adapt from other learning models, and in fact transfer learning and deep learning can work together side by side by quantifying which layer is more transferable, and making use of that by different levels, when you level regularity in terms of the data, and using this technique we have demonstrated at least three different application domains that transfer learning and deep learning can work together to do really well. So thank you. (audience applauds) Yes? - [Audience Member] In order to pick out the pivot words, one of the things that you needed to do, as least you intend you needed to do was to ignore the effects of negations, great versus not so great, and in principle I can imagine that causing some problems in practice I dont know whether it matters that it's immediately gonna extend or not. (mumbles) - Right, so actually pivot is only a phrase. So "not so great" itself is a pivot. So in fact, when you apply transfer and deep learning together, you can consider angry freeze to have a certain degree of pivotness, and un-pivotness. So to be specific and (mumbles), and in that wheel, then you don't care so much about these preceding ones. - [Audience Member] Can you go over again how you measure how was the, any specific thing (mumbles) or not, those are the only two domains that you have? - Sure, Yeah. So you probably meant quantification, right? - [Audience Member] Yeah. - Okay. For example in this, yeah. - [Audience Member] Right, so the way say it is is accuracy? - Yeah, accuracy. This is accuracy, yeah. - [Audience Member] And what is that exactly? - X axis is a layer. So here each layer you can use that layer as a feature vector. And then you can connect that directly with the (mumbles) of classifications. - [Audience Member] Oh so you do you have sketches for example as (mumbles). - Exactly. Yeah, and this is the performance. - [Audience Member] If you have very few samples, like very few (mumbles) that already you make ... - This is assuming you have enough data and what you can do with it. - [Audience Member] So in general you mentioned that the source and domain should be close enough that transferable, so is there a way that we can measure that beforehand, before just going to try and see what will happen? - Yeah, this is the question asked. We need to learn out. I think this is the holy grail of transfer learning. People have devised, so first we have tried things like care of emergence, and because it seems to be accurate enough, and then so this thing using the so-called MND, which a measure of the mean of the general distribution, turns out to be more accurate, but not quite enough. So theoretically, there's a line of research that's potentially to be explored, which is how do you come up with a universal measure of any two data sets how far they are, and how transferable they are. So this distance is yet to be discovered. - [Audience Member] But yet there's nothing strong enough. - Practically, Yeah. Practically, what people do today is (mumbles). It's not very principled. - [Audience Member] Would (mumbles) help? Like the variance and scanners. - That's right, for sure, and they will help, in fact, when you go deeper in using higher level, multi level features similar to what I've described. They have. - [Audience Member] I have another question. Can you comment, realizing the boss structure which is the summation of the loss and the source and the loss of discrepancy? - Yeah, for sure. - [Audience Member] What is the challenge there? Is it computationally cost? Or is it ... - Right, the goal is to have accurate and a good model, and a high performance model here. But you are trying to make use of this source the way as we recommend, so the three things you can do , there are three things you can do to make that happen. One is in the source, because you have lots of data, you can measure the loss, the source, you can try to minimize that. But that has to be related to what you do here. So you try to, if you have some label data that you try to minimize the performance, the error, but if you don't then you can try to minimize the reconstruction error. But if you have some data that have built in features on both, then you can have an additional co-parent loss, as a third term. So whatever you add here, the goal is you want this process to happen in such a way that what you do here can influence what you do here. - [Audience Member] I see. My next question is if before we do observe normal difficulties in terms of minimizing these kinds of loss functions, 'cause I've never done experiments like this, I just wanted to know whether it is easy to train? - Right, so depending on the convergence rate. Convergence rate, sometimes slow. Especially when you have (mumbles) you're trying to deal with. So typically what you do is you review this domain and when you train from this you're trying to move a portion of this over. So it's not done simultaneously. - [Audience Member] I see. Do you think there is potential for more research in this area? - Yeah, definitely. Yeah, I think this kind of thing, this kind of architecture is very interesting, and people, (mumbles) generally necessarily to try to simultaneously minimize the domain difference, to maximize the performance solution for this kind of done that way. So there some architecture wise, and organization wise, you're done. - [Audience Member] It sounds like you have two architectures, one is like a why, shared and it splits, the other one was an exploitation. It starts separate, there's a common submission. - That's right. - [Audience Member] So the first one I think of it as kind of understanding a shared perception, have to deal with input, and then a specialized (mumbles) task. The second one is kind of learning what the common core concepts are that would be applicable for both practices. - Exactly. - [Audience Member] So for the end for end curve, which one of those is the one that gives us that? Is it the ability to transfer easily from the same input task, or actually learning some really neat conceptual things that are (mumbles) that's knowledge that you can transfer from? - It depends on what (mumbles) are first to move is (mumbles). So what's the (mumbles). - [Audience Member] So it's a conditional perception. Shared perception. - For X, it's more vicious. Because you cannot target the different sequence and share the same semantic core. People have tried that, for example in multi-model one coming from X and another coming from pages. That can be done in some special cases such as, you have (mumbles) blank pages, and multiple (mumbles) images. Yes. - [Audience Member] When you talk about compilations in terms of like faster RDN and (mumbles). - I have to convert (mumbles) learning. You can say. (audience member asks question) - [Audience Member] Nurse wants primary student B, and then they can transfer to the task. - Okay so I guess there are some commonalities, so I would hear a mention another, (mumbles) is more related to life long learning. Life long learning is something which repeats as for the reading on the indefinite course. So the system can accumulate knowledge in the long term, and it's never gonna stop. I guess meta learning is trying to learn some commonality among tasks, and so when you deal with a new task, it's trying to use its memory and memories to save some extra work. So the spirit sounds (mumbles) transfer learning, but maybe there are some differences. Yeah, I'd like to learn more about that. Yes. - [Audience Member] This might be a little judgy but does deep transfer learning have any practical applications in a sensor flow organization? - It does. - [Audience Member] With Google Zoom, things are (mumbles). - It does, it does apply to that. But because, you really have two dimensions that you come to answer for the type of optimization, one is you will build better architecture, with this degree. Another one is across different degrees, in trying to reuse as much as what you have learned as possible. So if you can recognize true domains where the differences are small, then you don't have to start from scratch. You can usually in the individual domain, you can jus train the top two layers and that's enough. In the best case. So this effort in optimizing depends I think we can take on this two different directions. There's also a lot of work on building on the bottom layer and network layer. There are several efforts that go from the system side, also try to make the system train faster. So the answer is yes. - [Dana] Thank you very much. - Thank you. (audience applauds) 