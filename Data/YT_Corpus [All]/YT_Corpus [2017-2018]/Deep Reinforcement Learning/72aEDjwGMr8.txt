 thank you for inviting me to to give this webinar and thanks everybody for coming so before you know I get into the nitty gritties of machine learning let me tell you a little bit about how I come to this field how I learn about machine learning as brinkley said after I graduated I had the chance to join IBM's human language technologies group and I stayed there for three years as a research associate and now this group has produced a few Distinguished Alumnus alumni one of them is the co-ceo of Venice on technologies Port Mercer you might have heard of him because he was recently profiled on the New Yorker magazine in the most recent issue and the title is the reclusive hedge fund tycoon behind the cotrim presidency so he was actually quite famous recently but not because of machine learning the other distinguished alumnus is the other co-ceo of nanos on technologies his name is Peter Brown and well for those of you who may not have heard of venison technologies it is one of the well he sees widely regarded as the most successful Quan fund in the history of the industry and it had one of the most profitable vehicle the medallion fund reportedly had been achieving annualized return of of be for fees of 80 percent year after year for the last 20 years and never had a down year and that the third Distinguished Alumnus of my group was David magaman who also has joined rentec in the early days and built out the infrastructure to trading in the structure of Renison technologies and now enough about my you know colleagues who gone on to become billionaires my own system when I was at the IBM is a language is a knack natural language processing system that system was submitted to a competition and ranked number seven globally and the competition was was judged by the Defense Advanced Research Project agency and the Department of Commerce so after the you know so I I had a very very exciting time at that group but I thought that this since everybody in my group seems to be moving into the finance industry I thought I might join that to sew ins but instead of going to rent AG I went more in sandy where they had a group called artificial intelligence and data mining group where we develop among other things trading strategies to support the different various trading groups in the firm and then after that some of the people in my group decided to start up our own trading group at Credit Suisse called horizon trading group and we mainly does equities probably trading so now after all these experience both in the high tech as well as in the finance industry you might have thought that I would have been extremely successful in deploying AI algorithms in our machine learning algorithms and apply them to trading and would have been making millions after all this experience and how did they turn out well actually it didn't work out I did not succeed at that time in after all this work in applying machine learning in trading that's a surprise so but interestingly I didn't know why at that time because I was quite shocked that despite being in the best in the field and in the most august of firms it didn't work how could that be so but now I know the reason and that is the goal of this talk is to party explain the pitfalls machine learning why it doesn't work in trading and how it can work in trading so let's talk about the pitfalls first when I first started using machine learning in trading I had the misfortune of thinking that it would work on applying it to daily bars and I further had the misfortune of thinking that my input should be technical indicators and finally I had applied it to futures and index trading like S&P 500 futures and so forth and all these are the reasons why did fail and the reason is that they are in you know under these circumstances the data are simply insufficient for machine learning algorithm and machine learning will suffer severe data snooping bias when you are applying under these conditions under these three conditions in contrast what do you what do what I believe has potential where I believe machine learning has potential of making a profitable trading strategy is if you apply it to tech data in particular if you apply it to order book data or to fundamental data or non-traditional data that is for example news or other non price data and finally if you apply to stocks individual stocks rather than to futures and indices where there are many many choices but you know you you can you know multiply it by three thousand or five thousand each a teacher data bar you can have a cross sectional dimension of three to five thousand stocks that you can apply your algorithm to and that's much more likely you will succeed if you were to apply machine machine learning to trading so now I mentioned that data snooping bias is one of the main impediment impediment to having a successful in machine learning algorithm or applying it to trading and so how do you in general overcome it now as I mentioned you can of course get more data but you can never have enough data in some sense right more of data the merrier even if you are using tech data you could have more of it so how you know given a certain map data you know and sometimes may not have tech dated as at your disposal how would you overcome this limitation how would you create more data so that your machine learning algorithm has a better chance of working and one of the most common way of doing so is called resampling it's a standard statistical technique and sometimes it's called over sampling it's actually a better name is over sampling other names would be sports trapping or bagging and the idea is is actually quite simple is what you do is that you would create random data but with the same statistical distribution as the actual historical data set by sampling these data with replacement so you would create multiple rows of certain data and mix them in so that apparently you have expanded the data set by a certain factor but actually a lot of the data rows are repeated now that songs simple and it may sound like cheating to some of you but it actually work with the caveat that if you are running your machine learning algorithm on a time series data it's like for example bar data which has serial correlation you have to take care to preserve the serial autocorrelation in time since data you cannot just say okay let me just recreate sample five times of two days they and seven times yesterday's later well if your trading algorithm needs the return between yesterday and today to predict tomorrow's data you know just inserting a few lines of data that is you know yesterday's data would completely destroy your trading strategy because that could destroy the serial autocorrelation in time series data so there you have to be more careful than just blindly replicating roles in the data set now how we do that is a rather detailed question which you know probably should be discussed in another context but just to highlight the fact that resampling is typically one way that we can overcome data to snooping bias in the face of a limited data set now in a natural language processing parlance you would be making prediction I use you know to preserve this serial autocorrelation is like in a natural language processing using diagrams or trigrams instead of using single words to predict what the next word is that's something that people in the in the machine learning community would be very very familiar with you simply just need to in you know use multiple days as input instead of just a single day at a time in order to preserve this kind of see where autocorrelation okay so now there's another way a kind of orthogonal way to reduce data sampling bias besides creating more random data and that is reducing the number of features that might simply surprising with features by the way there's another word for it and that's predictors right or the input to a machine learning algorithm these are the inputs many people would have thought that the more data the more features the merrier right for example wooden if you can create 10 technical indicators wouldn't be better than having one lead technical indicator having 1000 or 100 fundamental input data due to that measure a company's financial status wouldn't that be better than having 10 fundamental indicators right so that you know the bias the common perception misconception pump common misconception is that the more futures and the more predictors are the better but actually in trading feature-rich data set is a curse and that's not just my own bias it's not just my own subjective opinion Nassim Taleb whom you might have also known famous writer the inventor of the notion of Black Swan he already written a article in the magazine in 93 I'm sorry in 90 in 2013 called beware the big errors of big data and the same the same point he's making as I do now is that the more features you have the more likely you will find spurious auto correlation between those feature end of and end of and the the target so although we want more data we do not want more features the more you have data the less likely you have data sampling bias but the more features you have the more likely you are going to suffer from data slipping buyers so feature selection is critical we want to reduce the feature set to a small enough set so that you won't suffer from data sampling bias and fortunately a lot of machine learning algorithms are designed for doing that with one exception very critical reception neural network and deep learning algorithms isn't one of them I know that and many people are greatly excited by neural net and deep learning it's all the rage out there you know Google's been using it to feed it into the self-driving car going into Google Translate Apple using it for their Siri auto assistant and you know things like everybody else is is using newer Network and batting it into your cell phone in your website but one thing I have come to observe is that it has never worked in trading for me now I some somebody else might have been able to make it work but it never worked for me and the reason it never worked for me easily is because new net and keep learning does not select future it basically takes everything you got and then mix and match them and squeeze them into a sausage such that it will force an output out of it and that is not ideal for trading okay so what are the however what are the good machine learning algorithm that does feature selection they are quite a quite a lot of well quite quite a few of these I will highlight three of them one is stepwise regression stepwise regression you know can hardly be called machine learning because it's been well known in statistical circles quite quite simple and the idea is simply that instead of using all the you would pick one feature at a time to see if adding that particular feature will give you a better prediction based on some statistical criteria if you want to understand a bit more of it you can read my book machine learn a machine trading or you read my latest blog article I I'm sorry I will tell you my blog at the end of the lecture but um I have a little bit more explanation of that but essentially is simply it's like the ordinary multiple regression except that you don't flow in order feature at a time you add one feature at a time until the album tells you that it can improve your predictability further and then you start pulling out one feature the goal is to find the minimal set of projectors in your linear regression okay so so yes so the other one is is called classical classification and the regression trees cut so I will show you a little a diagram of what card is but it is somewhat similar to Cypress regression except that it is there is a lot of conditionals in you know in terms of classification it doesn't up apply all the regressors on the equal footing instead it applied in a hierarchical manner with conditions imposed at every iteration now it is easier to see what it is than to explain what it is so I will show you a picture later on the third feature selection algorithm is called random forest random forest is not a particular classification algorithm but it is a technique that you can apply to a multiple different classification algorithm the idea is that it will combine bagging which as I mentioned is the resampling or over sampling of data with another technique called random subspace which is the resampling of predictors but it is in this case under sample okay so as I said you want to have a lot of data but a very few predictors so random for us achieve that by we over sampling data but under sampling protectors and again I I think it would be probably best explained if we can have a table which I will show you later on so the table is here when I talk about data and features I'm thinking of a two dimensional table okay we arranged the data in a two-dimensional array where the rows are individual samples that you input into the learning algorithm and the columns are the protectors so I the group for example for random forests you would replicate rose by over sampling them with with basement but for the columns you would under sampled them see with replacement but you won't sample all of them you understand for them so here is it for example a sample table in the next slide here is for example a bunch of data that purports to predict moles return right so the first column is our target variable it is 2 moles return that's what we want to predict and we may have five features one of them is simply today's weather the second feature might be today's volatility the third feature might be the today's return ok and the fourth feature might be the sentiment that is perhaps captured by some and some other algorithm to determine you know where is the sentiment today's bullish or bearish by reading a news let's say and then finally ah let's say the fifth feature is whether the US Dollar Index is up or down okay so these are perfectly hypothetical five possible features and all these five features are used to predict the tomorrow's return which is the first column so in in a random forest algorithm you want to over sample the rows so you might suddenly say okay let us create two of these roles create five of that row and create another row add ten of this row and then mix them up okay so instead of one two three four five six seven eight nine car rolls you might have 30 rows where some of the roles might be replicated duplicated or triplicated and so forth this is what we mean by over sampling our data creating more rows at the same time we would say that okay there are too many features we don't want a five feature some of these might be users because I have no idea if the weather is a good feature I have no idea whether US dollar index is a good feature how would I know I only know that these are the input I have but I have no idea if they're useful or not so you would apply one of these classification algorithm whether whether it's my progression or whether it is classification or regression tree or whatnot and under sample them so you might randomly pick okay let's try this feature one and feature three or let's say we just pick feature two and feature four and see if by under sampling the feature you get as good a predictability at that way you can reduce the feature set that's another way that that is part of the job of a random force a technique in reducing feature set and reducing data snooping bias okay so there's a question what about using mutual information certainly there are many more techniques and mutual information is you know another good one I will I haven't used it too much myself I used to have done some work with it but certainly not recently but yes I'm by no means I am I suggesting that the technique guys described are exhaustive they are there many more as some of you I pointed out beauty information and there's if you go to my blog there's a guy who commented and he suggested and not technique which I have fun forgotten the name right now but if you read the comment there's another technique for reducing data slipping bias that was mentioned and I'm going to look I will I promise him that I will look into it but I haven't done that yet so there are many indeed techniques for doing that these are however the most I think simple and well-known okay so going back to us one of the classification classification algorithm the feature selection algorithm that I talked about which is a classification classification tree in particular so let's say we have taken done the work of resampling the data over sampling in the data and and perhaps we have narrowed down the set of features we can apply to classify and one of the classifier that we can apply to is the classification regression tree so an example of this technique classification technique would be trying to find out what values of a technical indicators well first of all find out which technical indicators would be useful and secondly for those technical indicators that we find are useful what values of them will generate positive and neck or negative sby future when they return so again I have described it in a book but let me give a picture of how the classification of the cut algorithm works so this X 1 and X 2 and so forth in the picture are simply representative or the predictors they might be again fall agility might be sentiment going from 0 to 1 it could be one day return you know in this case probably x2 is probably two day return and x1 is the past one day return so when the algorithm first run it decided that the two day return is the most protective that's why the top node is a pick x2 is the past two day we turn is the most useful in predicting the future one day return of s py and it decided that if the two day return is less than this number and this number is picked by the algorithm itself we can further investigate it and if the two day return is greater than this number it will give you generate a negative return so immediately give you a conclusion it believes that most of the time if the two day we turn is greater than 1.5 percent the future one day return will be negative but if it is less than 1.5 well we will have to apply more so it created a subset of the data so we classify some data based on this first variable and then we do everything you're trying to see again what other variable can give us a better classification and at this time it takes the one day return you decided that if one day return is negative it has a statistically significant subset and then we will look into it further by again reiterating algorithm so again I'm not you know going to have time to describe the detail of this algorithm but this is a you know typical way how machine learning algorithm would work in terms of automatically finding the predictive variables and then automatically finding the parameters that it would be useful for the classifier and then iterate until it finds no statistical significance so machine learning algorithm often time is essentially a glorified statistical regression system it's just more trees and turns more both more details than a simple regression or simple data time series analysis it has a lot of conditions that it impose on the data to cut the data into smaller and smaller subsets where predictive that that your variable can be more and more predictive so there's a question already and it's a Christoph asked if you use price data form the lower TF you have kind of unlimited data how does that apply to unlimited price data I actually don't quite understand what this question is maybe you can rephrase it anyway I'll let you you know rephrase it because I don't quite understand what what the what the question is anyway so another classification algorithm so this is a classification tree another very common classical algorithm is the support vector machines so again the problem is to predict what the future one day svy return is whether it's positive or negative and again you can use a technical indicators as as as the input the in this case for support vector machines classification ISM is based on finding a hyperplane to cut through the data so that data that for example belong to the future one day positive return will sit on one side of the hyperplane and data and data that represent negative one day return will be sitting on the other side so we are looking for this hyperplane so that we can cut the data into half and if we can do that we will know very well under what condition do we most likely find a positive one day we turn tomorrow and one what condition we would find a negative one day return tomorrow now that's the simplification that's a simplified picture of the data but of course most often times there is no such hyperplane that is so easy to cut the data into half and all the classes the positive return colored samples is on one side the negative returns data is on the other side life is never so perfect so oftentimes you have to actually apply a nonlinear transformation to the data of transform it through what is called kernel function so that you can apply such a hyperplane I this height this kernel function can be a Gaussian function can be a sigmoid function to be polynomial function but the idea is that after you transform it then this data which originally cannot be separated will suddenly be able to separate okay it has a topological transformation of this data point so that they they were able to separate by a hyperplane or another way to look at it is that instead of using a hyperplane to cut through the data you will have to use a rubber sheet of some in some high dimension and you will be able to separate the data so there are two ways to look at it but this nonlinear transformational called kernel transformation is critical to making a support vector machine well but before you know without going to the details this is the simple picture of how a SVM supposed to work supposing that all the positive return days is on one side or the negative return days on the other side if you can find a plane that can separate them and the X and the y axis by the way represent the predictor so in this case we only have two predictors but if you have you know n projectors you will be dealing with n dimensional space right so that's why we are talking about hyperplane but in the two dimensional space with two predictor x and y you can see that if you can find this hyperplane that means that for a certain x and y inequality you know X greater than a times y plus B you know that will typically if you negative return and if X is less than a times y plus B you will typically get positive return so that's a simplistic way of understanding a support vector machine but that's yet another way of classifying data so before I go on let me look at some some questions yes so so yes so crystal said we need to resample because we don't have enough data that is correct from price data from a time frame like wind in one minute you can have lots of data now maybe well it depends on again how many predictors you have you know one minute might seem like there's a lot of data but a lot of that data might be correlated so you know it might not actually offer a lot of enough data for the machine learning algorithm to learn and particularly if you have a lot of protectors you would need more data in order to increase this in Sisco significance so whether or not one has enough data is a relative judgment it is really relative to how many protectors you have right so you might think that if you have only a couple of protectors yes what if you have one minute data you may not need to resample that is true you might not need to you can just use the raw data as an input now support vector machine I would consider of a less of a classification algorithm if I'm sorry less of a feature selection algorithm then stepwise regression or classification tree and the only reason is that the input R or use up right so you are trying to find a hyperplane that cut into any dimension and the dimension is not reduced right the dimension of the hyper space is equal to the number of predictors that you are entering so you know the algorithm doesn't automatically get rid of some dimensions for you you are trying to do the best you can in transforming the data and finding a hyper hyper plane that can cut through the data in that fixed dimension so once you fix the dimension it is less of a feature selection algorithm so you know of course we can talk a little bit more about it if we have time in in the end but in my view SVM is is less of a feature selection algorithm than the other two namely the stepwise regression and the classification and regression tree so okay let me move on a little bit okay so now mentioned that neural network I have never been a big fan of in trading but let me mention what it is because it's been all the rage Nero is essentially a nonlinear equation we talk a lot about regression where they step right regression or regression tree or even SVM you can think of it as a kind of regression although is transformed but neural network is definitely a kind of nonlinear equation now what is the difference between your network and deep learning where deep learning is simply a neural network with many layers but very few nodes per layer it has been found that by adding more layers but having fewer nodes actually is easier to capture for the for the for the network to capture features in stages okay I'm no expert in that so that's all I can tell you what the hell deep learning is but the central idea of the nonlinear equation is that you fit a monster long linear function instead of using a linear function as in regression model but you fit in nonlinear function to data and the nonlinear function is why I call it monster is because it's a iterated function you start it with exit moita function which is already nonlinear sigmoid function is 1 over 1 plus e to minus X it looks like an S that's why it's constant model and you iterate them by by iterating I mean functional composition you feed some of these model function into another sick model function and then you do that again and again did that become a network so here's an except it's a picture of it so you might have for input X 1 2 X 4 okay and you feed them into the sick your some of them and then you take the sum as an input to the sigmoidal function and you do that again on the next node so you can have multiple nodes each node will have a different weighting method of weighting these inputs and you might have a different parameter to this ik motor function and you do that with as many node as you like and then for each node you have an output and this output is feed into another layer of these of the neural network and which is again summing with different weights and different parameters of the sigmoidal function and so on so you can see that this function is what I call a monster nonlinear regression function because it's so complicated it has so many sum but you can it can be proven that if you build it complicated enough network you can approximate any nonlinear function and so in the sense that's the power of new network whatever is the causal is the mathematical relationship between the input and the output right so input might be yesterday's return two days return volatility and whatnot and output might be tomorrow's return so whatever unknown mathematical relationship is between these paths these protectors and the target variable the neural network an approximated and supposedly you know if you feed in a new input like today's Sunday the 403 goes to two percent and however the return is minus three you can just as well predict and do return for tomorrow sounds great right this is this is sounds like powerful you know it's like magic you train the network on path data and it can precisely tell you what exactly you should expect is to most return given today's variable you can have 100 variable that you finish but that is sangs great in theory and it works very well in those situations where the data is stationary for example it works very well in self-driving car it works very well in computer vision it works very well in natural language understanding in speech recognition in machine translation that's why people are using it but it doesn't work in trading why doesn't it work in trading it doesn't work in trading is because there are the the financial market is not stationary you cannot learn everything by just feeding in as complicated data as as you like for example you can fit in the position of Mars relative to the earth the position of the moon relative to the earth the temperature of New York temperature of London and you know the weather is sunny or cloudy and you know you can have as many variable as you like and the new level is going to fit exactly to that it find that for example you could find that in the Black Monday in 1987 the Dave could be 27 you know degree Fahrenheit or it could be sunny and it thinks that whenever these temperature and this weather prevails and when the 40s is low or high it will produce a crash well that doesn't work because you can never you know have the insight to capture the relevant features rather than capturing this treeview accidental feature and that's why it is very difficult to make a neural net in particular or even even deep Learning Network to work in finances because the data is non-stationary there is no particular causal effect with a lot of variables with the market there may be some variables that are very causal but it is very hard to find them using this approach of an neural network approach because as you can see the problem of neural network is that it uses all the input it never skip anything it doesn't select the features ok so let me see that any questions so far before I move on ah okay all right there are some people who have raised their hands I don't know if they have questions because if you have questions you can just type into the Q&A box if you don't actually have to raise hand to to ask the question I assume that the coab holy some has a question so I will pause for a minute please backtrack one time oh yes okay uh well I maybe answer some of the questions later because I don't see any any questions showing up on the Q&A box so anyways so let me wrap up anyway this is almost the end so when will machine learning be useful to traders that is useful if you don't have intuition about your data or you don't have intuition about the market ok and or if you don't have a mathematical model of your data when my same mathematical model it could be very simple such as a mini-version model or a trending model or a in the case of let's say you believe that the as the the stock market is is driven by three factors so that would be a simple mathematical model right or you might um you might think that M let's say you are talking about trying to protect stocks return again you might have a simple three factor model so these are simple math memory but in some cases you might not have this model you don't know how to find them or you don't have a good intuition you don't know if the this price series is new voting retraining or if how one can trade it well that's when machine learning can help you to develop some of the intuition if not to be the final trading model at least it can give you some intuition right I might find out that certain protectors are useful and then you can drill down on them by other methods sometimes also you have too many features right that there you are you have a suffering the curse of dimensionality like for example you are presented with a financial statement of a company you might have 50 fields is the revenue important is the earning is important or is it the dividend that's important we don't know why it's your you know you may not be a charter financial analyst and you just want to use this to trade so that if you have too many features and don't know which ones are important you can certainly apply machine learning algorithm to help you to narrow down the features if on the other hand you have if these are not true if you have good intuition if you have a good simple mathematical model of your part of your market you're better off building simple models rather than using machine learning okay so now when we machine learning P users as I pointed out if you have too little data let's say you have only daily data okay data data on futures okay that's not sufficient and the second situation is when I mentioned that when you have regime changes very often let's say you know if you are feeding data in since 2009 it will all be bull market the machine learning algorithm thought that the stock market always goes up right that's hardly a particularly smart application of machine learning if you think that all you need to do is to long SP y-you don't have to do anything just long ok PI on tip that's it similarly if your market has been all the surface is placid again if you train it on the market since 2009 most of the time is very Placid you think that shorting for a fee will be good such as every day you short vx6 that's about that's the most powerful algorithm right similarly trending versus inverting growth versus value low versus high interest rate or inflation one has to make sure that your algorithm can is actually learning from different kind of regimes rather than just learning from one Jim and if because otherwise whenever there is a regime change your machine learning algorithm will completely fall apart because if it's a poor market model or it will do is to long svy and you know when the market suddenly turned parrish it doesn't work anymore so again that relates to our notion of non-stationary statistics and that is when just I'm going to elaborate on that in addressing some of the questions when the statistics are non stationary no statistical algorithm can learn from it because you know it's actually no one can learn from it unless you were able to extract features that are stationary so if you feed in non stationary data to a learner obviously garbage in and garbage out you could not possibly learn anything that is going to be of value out-of-sample okay so that's the x then machine learning are not useful anyway so I have explained some of these points and others details further in my book machine trading and I will explain some of these again further in my may workshop or not visual intelligence techniques for traders you can visit my website VP Chen calm ah and I also have elaborated on some of this example on my blog EB Chen box for calm and I oftentimes treat articles links that are of interest to trading and machine learning and so forth at my at my tree to handle Chen DP thank you very much for coming let me go through some of these questions and see if I can answer them so so crystal pop as suggested to avoid non stationarity we can protect you know to apply stationary tests well that is a very narrow sense of stationarity right and so the stationary I think you are thinking of is whether the Taylor is doing a random walk or whether is being averting that's the sense of stationarity I think that you're thinking but I'm when I mentioned stationarity here it is in a is a more general sense of statistical stationarity it doesn't necessarily mean that it is mean reverting it doesn't mean that it is integrated of order 0 in technical terms it actually means that the statistical characteristics of this time series remain unchanged and well yes there are tactical tests that you can test to see if the statistical could stakes is unchanged but oftentimes the result is ambiguous it's it's actually quite difficult to to be absolutely certain that the there is no regime change you know it's not a simple technical matter oftentimes is there's a lot of ambiguity there is a really hit upon the the problem of the the limitation of limited data statistics okay so there's a question about go home from call policing there's work being done on reinforcement learning okay so I think that missing part of it that is is that effective as opposed to traditional deep learning now I I have I know first of all let me say I'm no expert in reinforcement learning deep learning deep cue networks and whatnot and the reason is that whenever I look at a article okay I used to be quite diligent in replicating this method whenever I read an article that suggested some of these machine learning method work and I try to replicate the result and it never work out of sample so I've stopped doing that I rewrite I want to save my time so when you ask me okay is this effective or not I can tell you I don't know but I'm deeply skeptical because when I was still tillage 'only replicating the results of these people I have never found anyone that has any relationship to new whenever to work out-of-sample so that's why I stopped doing it so subspace do you know say yes linear regression is better than new net in some sense that is true but you know I would say that perhaps a little bit off from more clever use of linear equation as in set wise regression or s in classification regression tree Toto's might still be okay you know because this is you know and that it does see a selection for you at the very least because just because we are using linear regression does not avoid data slipping vials because if you have a hundred inputs okay and you run a multiple regression on them you will still get data sampling bias the question is how do you reduce that 100 input to a small number that's really predictive and that um oh fashioned in your equation won't do it for you okay so boost said I have not discussed what the target function may be indeed you can the target function is to be customized to your particular trading strategy sometimes you want to predict next day return of svy other times you want to predict the one-month return of a particular stock so it is really up to you to be you can choose the particular target function now of course in the Europe you know we are limited by data as I said if you want to predict one month return most likely you are we are stuck with daily data is unlikely that the minute data will have any signal useful for predicting one month return so if you're going to have a target function that is one month we turn well that means that we are stuck with daily return as the input and therefore we are limited by the amount of data we can get if however you have a target function that is one minute return now maybe we can apply machine learning better using order Pok data and whatnot so so the so the target function selection is tied with you know how much data we can get in the predictor because you know you don't want to use a tip you don't want to have a big mismatch between the time scale of the target function at the target variable and the predictor variable so one except what are the underlying difference between adam mao methods and deep learning so one will be added but the other dozen but the underlying difference is mainly in my view that the other methods use feature selection and deep learning in your work does less good a job in feature selection and I view feature selection as one of the most critical function that the machine learning algorithm can do okay it's not really how we're we'd fit the protective the relationship between protector and and the target variable that's mule network that's very well it fit a perfect function the relationship between the predictor and a future it can fit perfectly but that's not what we want we don't want it to fit perfectly we wanted to select those variables that has high statistical significance instead of fitting it to the past perfectly what some of the when you say measure algorithm you I think you mean context statistical significance typically one can use AI seeker interior P I see criteria those things that measure the likelihood but penalized by the number of protectors so you know if your data is noisy your maximum likelihood will be your likelihood function will be lower and if you have a lot of predictors you're likely function with your your bi C or AIC criterion will be actually larger right so so there's a minus sign there but anyway likelihood and the penalty for protectors would be what we judge as to how good an algorithm is is in predicting the future so Christoph said can I give an example of a target well that's that as I said it's a you could be as simple as two two moles we turn off the s py right so let's say you want to protect how the market index move so that would be an example one day return of the SP y variable contacts us all so how do you define regime over the course of long period no there are many ways to define regime right so you know everybody have their own favourite dream I give some examples it could be a volatile versus a placid regime and help you cut off well maybe you can say fix greater than 20 is fall 620 is not qualify Placid or you can say poor pair market well they are probably definition out there which is a you know if you are have a certain number of days in the drawdown you will be a bear market and so forth so you team definition by itself is also arbitrary right so the question you know we we kind of intuitively understand what it is but but um but you know it's not no true person can exactly agree on what that regime is and whatever they can agree on is arbitrary anyway so but the idea is intuitively obvious that if you are going to run your arrow from only on pool boorish regime it's not likely to very good job in the bear market right okay so I think that we are almost out of time so let me just quickly go through the rest of it how many trays in your opinion it works um it's not a question of how many traits it is a question of well let me think about this question effectively you are talking about you know you have a expected return and you want to find error in the expectation and so the error typically is proportional to the square root of the number of trades so you know you effectively if you want to have a high accuracy in protecting return you have to reduce your n especially we reducing the square root of n to a manageable number and fortunately Sharpe ratio is one example of how we can measure so if you have a strategy that has a high Sharpe ratio we can say that it works right a Sharpe ratio of two or greater than two it is a strategy that works you can say and Sharpe ratio is actually a very good measure of statistical significance because it incorporates a concept of the square root of N in the in the construction so okay I think that there's some you know I think the last question that Christophe heads is it's a bit too technical and frankly I've I have not studied the particular with them that you you you mentioned so I won't be able to offer much insight to that and I think it's a rather absolute technical for this very 