 Great pleasure to welcome Yann LeCun, who will be our first invited speaker. [APPLAUSE] So while Yann is setting up. I cannot resist to say a few words. Of course, Yann does not need any introduction. Everybody has heard about Yann, and it suffices to say that he is the head of AI research at Facebook at the moment. But I've known Yann since we were PhD students, and it's an incredible travel that he has made since then. Already in the 80s, he was passionate about neural networks, and this was the subject of his PhD thesis. And he's one of the inventors of an early version of the backpropagation algorithm, with which you couldn't do anything with neural networks today. And Yann went on after that to do a postdoc with Geoff Hinton, who everybody knows also in this field. And then went to work at AT&T Bell Laboratories with a great number of researchers, that have now founded research at NIPS to a large extent. And I just wanted to mention an anecdote before Yann puts up his slides. When we were at AT&T, once we were sweating on some projects. And somebody turns to Yann and says, and what if it doesn't work? What is Plan B? And Yann says, there is no plan B, it will work [LAUGH]. And so, I think this is the story of Yann's life. Yann does not have plan B, it has to work. And this is what has been his success throughout the years, even when there were downturns in the neural network field, he never lost faith. He knew he had to work, and in the end it did work. Yann LeCun.  [APPLAUSE]  Thank you, Isabel. I actually don't remember this episode, but it could very well have happened. It probably did. So this is my plan A for the next 10, 20 years or so. So I'm gonna talk about predictive learning. This may not be perhaps a phrase that you have heard in the past. And really, it's kind of a renaming of relatively old things. But before I talk about this, let me show you a picture. I see Terry here, down there. He knows that picture. This is a picture of the participants of the first Connectionist Summer School that took place at Carnegie Mellon in July 1986. So a little over 30 years ago. And this was kind of the start of the neural net community, if you want. And it was co-organized by Geoff Hinton and Terry. And there's a lot of people in this picture that you may or may not recognize. And so I'm gonna help you a bit. Geoff and Terry are down at the bottom as well as Jim. I'm sort of hidden in the pack here at the top left. There's Michael Jordan, still a PhD student at the time. Rich Sutton, sorry, Andrew Barto, not Rich Sutton. And a bunch of people that went on to do great things. Some of them in machine learning, some of them outside machine learning. Okay, better microphone. All right. Apparently, that's a better microphone. Okay, so we all know about supervised learning, of course, right. You take a bunch of samples, you run them through the machine, and when the machine doesn't produce the answer you want, you adjust the parameters on it, the knobs, of which you may have hundreds or millions in modern systems. And if you do this enough times, the knobs eventually settle on a configuration that you hope will recognize all the things you've trained it on, but hopefully also will recognize the things it hasn't been trained on, that are similar. And over the last several years as you know, we've seen a kind of a change from traditional methods in pattern recognition and machine learning, towards deep learning where all the layers are trainable. And that has led to great success in image recognition using convolutional net that allow us to build hierarchical representations of the world. An interesting thing is that there's been an inflation in the number of layers in those networks in the last few years, that you have now practical network that are used everyday by companies like Facebook and Google, etc, Microsoft, that have maybe 100 layers. And that's the kind of inflation that I wasn't predicting back 20 years ago, certainly. So, in particular, one thing that has become very popular in recent years is this idea of ResNet, which is a neural net that's essentially by default an identity function. And where the network part of it learns the kind of nonlinear part deviation from the identity function. And that allows us to train very, very deep network. This is an idea from Kaiming He, who came up with this when he was at Microsoft Research Asia, who is now at Facebook AI Research in Menlo Park. And what convolutional nets and deep learning have allowed us to do over the last decade or so is do things like drive robots around by kind of learning the traversability, applying convolutional net convolutionally on the image so that it can label every patch in the image and eventually drive a robot properly. We've used convolutional nets to do semantic segmentation. There's been a lot of progress over the last few years. This is a five year old paper. But there's been a lot of progress over the last few years on doing this. And so much progress in fact that some systems that are for self-driving cars actually use the basic idea of semantic segmentation. So systems that are produced by companies like Mobileye or Nvidia, etc, use convolutional nets to recognize obstacles and detect them, to locate them, to label the entire image as to whether it's traversable or not, to produce proposals for steering angle for the car. And there's been really impressive demonstrations, even deployed products, that use those things. So a lot of impact. More recently, ConvNets have been used for things like, not just recognizing objects but also outlining their contour. Again, it's just supervised learning, a convolutional net net which is trained to produce a category as well as a mask for the object. And it produces object proposals. Some version of it that was produced at Facebook AI Research in Menlo Park was recently open sourced. So you can just download the code and you can use it in your research. It's called SharpMask. There's two version, DeepMask and SharpMask. And the result of it are pretty astounding, at least for some computer vision researcher who'd be transported from five years ago to today. Looking at results like this, I think would be surprising that it happened so fast. So these systems can identify cellphones, people, laptop, kinda outline them. Can pick out people who are behind chicken wire, and fuzzy people in the background, and tell that there's a frisbee. Pick out broccolis in Chinese dish. And do amazing things like count sheep and tell one sheep from another. So those tasks are really complicated, because there's nothing that looks like a sheep than another sheep that stands behind it. And so it's kind of amazing that this works so well. And it's pure supervised learning. But the problem with supervised learning is that you need human supplied labels. You need two things for supervised learning to work. The first thing is, you need people to basically label images or whatever it is you want to recognize. If you want to do language translation, you have to have parallel text and things of that type. And the other thing you need as well is you need those labels to be somewhat reliable. To be, if they are categorical, that means the image has to be more or less unambiguous. And you don't want too much of a error rate in the labels, the provided labels. And that's a limitation because that means the amount of information that the machines can be trained on is relatively limited. It's limited by how much reliable data people can input in the system. So in my opinion, that's one major obstacle towards AI. And there are others. So let me speak to this. What are the obstacles to AI? What I'm interested in at Facebook and at NYU, and that's been kinda the project of my life if you want, is to understand the principles behind intelligence. Whether it's natural or not natural. And of course as an engineer, the best way to understand something is to build it. So kind of building intelligent machines, verify whether the hypotheses about the nature of intelligence are correct by building a machine that actually reproduces the The functions of intelligence. And very early on I thought that learning was an intrinsic part of intelligence. You don't really have any intelligent entity that we know around, at least not generally intelligent entities. They don't have the ability to run. Okay, so what are the real obstacles to AI? If we think about the architecture of an AI system, we need machines to be able to understand how the world works. I'll come back to this at length. So understand the physical world. Understand the digital world, of course. Understand people. Understanding people is probably one of the hardest part there. They need to acquire some level of common sense. They need to introduce, so they need to learn a very, very large amount of background knowledge, mostly through observation and actions. So, babies for example, in the first few hours, weeks, or months of life, learn a huge amount of knowledge, background knowledge about the world. We're not born with the idea that the world is three dimensional. We're not born with the idea that there are objects in the world. We're not born with the idea that objects don't disappear spontaneously. We're not born with the idea either that an object that's not supported will fall. So a lot of those concepts we learn in the first six months of life. And some concepts, like the idea that an object will fall if it's not supported, we actually learn between the age of six months and eight months. That's relatively well measured. So those are really very basic things that we learn by observation, by experimentation. And it's a much, much larger amount of information that the current information our machines are learning through supervised learning. So we need machines to be able to perceive the world. We need them to be able to estimate the state of the world. We need them to be able to plan in such a way that the world will reach a satisfactory state to satisfy goal. And so perception plus predictive models which essentially are ways to predict where the world is gonna go. Plus memory that's necessary for that. Plus reasoning and planning that's what makes an intelligent agent. Okay, so I mentioned the phrase common sense. What do I mean by common sense? What is common sense? It's a very old classic problem in AI of the fact that machines don't have common sense. So even though they might have some knowledge on a very narrow area, as soon as you get out of this area for things that we take for granted their response is usually stupid. So, for example, there is a very well known set of sentences called winograd schemas. Of the type, the trophy doesn't fit in the suitcase because it's too large. Or the trophy doesn't fit in the suitcase because it's too small. So in the first case, the pronoun it refers to the trophy. And in the second case, the pronoun it refers to the suitcase. And to be able to lift this ambiguity, you have to kinda know how the world works, more or less. This is where my colleague at NYU Rodney Davis actually has a list of a couple 100 of those sentences that people have collected over the years. There is an annual competition where computers sort of try to figure out what the pronoun refers to and the best performing machines are below 65% correct, humans are about 95. Similarly if I say, Terry picked up his bag and left the room. There is a lot of information you can fill in. There is a lot of missing information in this sentence that has a few words. But there is a lot of information you can fill in by just knowing how the world works. You know that he has to stand up, extend his arm, grab his bag cruising his hand most likely. He's probably going to walk. He's probably not gonna fly or dematerialized like this gentleman here at the bottom. Although he does have superpowers. And he's probably gonna walk towards the door, open the door he's not gonna walk right through the wall. There is a lot of things like this that we can infer about the scenario of the sequence of events that have to occur. Because we know how the world works. And the knowledge of the world, the constraints of the world allows us to fill in the blanks from this kind of short sentence of a few words. That's I think what a lot of people call common sense, the ability to fill in the blanks. So how do we get machines to run this? So common sense perhaps is the ability to fill in the blanks. That means inferring the state of the world from partial information. So we don't have perfect information about the state of the world through our perception. Whatever perception we have is very local in the world anyway. So, inferring the state of the world from partial information seems it's something that's very important if you want to have an impact on the world. Even if this impact is just moving an object or something like that. Common sense is inferring the future from the past and the present. It's also inferring past events from the present state. Particularly useful if you are a policeman or police inspector, all right? You get to a scene and you have to figure out what happened. It means very basic things like filling in to visual fields of the regional blind spot. We have a blind spot, that's where all vertebrates have a blind spot. That's where our optical nerve punches through our retina. Invertebrate are better designed anyway. They are not in the same local minimum of the fitness function of evolution. The wires come out the back of their retina. So we have this blind spot and we're not conscious of it because kind of low level kind of visual cortex in our brain kind of fills in the blank, if you want. We can do things like complete occluded images and infer what's behind an object if we have some idea of what the general shape of the object is. Perhaps if you are on the side, you only see my right profile but you can pretty much infer what my left profile looks like. Predicting the consequences of our actions. That's very important, of course, to be able to plan. Predicting a sequence of actions that will lead to a particular result is also, of course, very important. So really, filling in the blanks mean predicting any part of the past, present or future percepts from whatever information we have available. That's what I can predictive learning. But, in other terms,a lot of people would call this unsupervised learning. Now, unsupervised learning also means a lot of other things. And sometimes is taken as a very narrow meaning. And so, that's why I prefer to use the word predictive learning. So it doesn't necessarily mean predictive in the future, but it means filling in the blanks. But really is unsupervised running. So the necessity of unsupervised running follows from this argument that I've heard from Geoff Hinton since I've known him, which was in 1985 I believe. And it related to the fact that the number of samples that are required to train a learning machine for any task depends on the amount of information that we ask it to predict. So if you train a neural net to do binary classification, you can't make the neural net very large, unless you have tons of data. Because the amount of information you ask it to predict is very weak. So for example, some of the first experiments that we did with conventional nets on real images was face detection or pedestrian detection. And it didn't work that well. It worked fine, pretty much for state of the art but not spectacular. Partly because the dataset was small, but also partly because the task is really weak and provides very little information about the world. If you do something else, if you train your conventional net on image net which has 1000 categories and then you fine tune it on a task of this type that is very weak in terms of label, it works much much better. And it's because there is a lot of things that are about vision that are common to almost any task. And so if you train the system on the generic task in which you give it a lot of information to predict, you can then specialize it to any task you want. Again, this is called transfer learning. So here is Geoff Hinton's argument. And this is kind of a copy paste from a answer to a question that, ask me anything on Reddit, a couple of years ago. But he's been saying this since the 70s, or at least the early 80s. The brain has about 10 the 14 synapses, which means parameters, if you want. And we only live for about 10 to the 9 seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsupervised learning since the perceptual input, including proprioception is the only place we can get 10 to the 5 dimensions of constraint per second. So this stems from the idea that you can need roughly as much data as you have training samples. Here we're thinking about input data but really what it really means is, how much did you ask the machine to predict? That's kind of how you train a machine. A very powerful machine, you have to ask it to predict a lot of things. Otherwise, it's gonna be degenerate. So allow me to draw a slightly offensive slide. And I apologize in advance. And I've taken some heat for this. But I promise I'll make it up, okay? How much does a machine need to predict? So depending on the mode of training of that machine, it's gonna get different amounts of information that it's gonna be asked to predict. In pure reinforcement learning, by which I mean you train the machine to predict a value function, as a function of an input and an action, for example. The only thing you ask the machine to predict is a value function. It's a scalar. Moreover, you only give it the scalar once in a while. But let's assume you give it at every time, say, for every trial, okay? So there is no temporal dimension to this. The amount of information you give to the machine is extremely weak, extremely poor. And there's basically no way a large machine will learn anything useful, unless you provide it with millions, and millions, and millions of examples. So in its purest form, in which you ask, purely, the machine to predict a scalar, and that's the only information you kind of provide it, if you want, this type of pure reinforcement learning just can't go anywhere. Supervised learning provides a bit more information. So this is cuz you provide a label, which may contain a few hundreds bits, or something of that type, maybe 10,000 bits or so. But it's still very weak, and the amount of data you can train on this, whatever, has been labeled by people. So predictive learning is where the bulk of the information is. And you have to ask the machine to basically predict the world. And that way, you can provide it with millions of bits per sample that it needs to predict, and perhaps train a very complex machine to learn complex dependencies about the world. So it led me to this kind of half-joke analogy of if intelligence is a cake, the bulk of the cake, the if you want, is unsupervised learning. The icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning. And I must admit, this is slightly offensive to people who spend their days working on reinforcement learning. But I'll make it up, I promise. I'll make it up because there is a form of reinforcement learning that actually uses unsupervised learning as kind of a submodule, if you want. So first of all, reinforcement learning is really important. You need to be able to kinda train a machine to maximize some value function or whatever by figuring out a sequence of actions. And in fact, we're working on this at Facebook Research. In fact, this is some work led by Yuandong Tian, who is at Facebook Research in Menlo Park, of training a machine to play Doom. And he actually won the last competition of VizDoom, the so-called VizDoom competition. And so this is using, basically, a competition on that, looking at a picture and being trained, with details that I won't go into, by reinforcement learning to win. So it works in this case because we have tons of examples. We can basically have the machine play millions and millions of games. We can simulate the game really quickly. Here is another example. This is work by Gabriel Synnaeve, Nicola Zenya, and a few others at Facebook research. And this is using reinforcement learning to train in the sort of battles in the StarCraft Brood War game. So this is the first version of StarCraft. And in fact, it's invisible, but we just released a piece of code that interfaces StarCraft Brood War to Torch, which is the deep learning environment that we use at Facebook. And that allows you to kinda plug deep learning into StarCraft. Actually, classical reinforcement learning doesn't quite work there. So Gabriel and Nicola had to come up with slightly different techniques there to get this to work. But you can train those things to kinda figure out really what's the best strategy to deal with an opponent. Okay, so here is a nice quote. I don't know if Rich Sutton is in the room. I know he is in the city.  [INAUDIBLE]  Right here, hey, Rich. Hey, this is copied from your paper about Dyna. [LAUGH] So Rich had a paper about 25 years ago, called Dyna: an Integrated Architecture for Learning, Planning and Reacting. And there's this little quote in his paper that I like very much. It says, the main idea of Dyna is the old, commonsense idea that planning is trying things in your head, using an internal model of the world. And there's a bunch of references for people who, of course, formulated this idea, including himself ten years before with Andy Barto. This suggests the existence of a more primitive process for trying things, not in your head, in the world, but through direct interactions with the world. Reinforcement learning is the name we use for this more primitive, direct kind of training. And Dyna is the extension of reinforcement learning to include world model. I think this should be the philosophy of most working reinforcement learning nowadays, because although what people now call model-free reinforcement learning has a very, very bad sample complexity, what we now call model-based reinforcement learning, which is really what Rich is proposing here, is considered to be better because it takes advantage of unsupervised learning. So I'm gonna go into this a little bit. But before that, I'm gonna talk about classical control theory. So in classical control, optimal control, you want to control what control theories call a plant. It's always called a plant, regardless of what it is. So you have a plant simulator. It's basically a model of the real system you wanna control. You might have identified the parameters of this model by just observing the plant in various conditions. But now you have this plant simulator, and the advantage of this simulator is that it's differentiable. So you can plug a command, a time t, run the simulator for one time step, and then you get the next time step. And then you can produce another command, run the simulator again. And this is like a recurrent neural net, except it's not a neural net, it's a hand-built model. And the way optimal control works, sort of classical methods for, say, computing the trajectories of rockets, or something like this, this is the technique NASA used in the 60s to compute rocket trajectories, is that you figure out the sequence of commands that will minimize a particular objective function, while perhaps taking the state of the plant at a particular state, at the end, or minimize the time it takes or the energy it takes to get there. You sort of encode this in the objective function. Now, because all of this is differentiable, you can use gradient descent to do this planning. This is a very traditional, classical technique in control theory, which, in my opinion, a lot of machine learning people have forgotten and should probably exploit a bit more. So having a differentiable model of the world brings you this ability to do planning through gradient descent. And that's a very, very powerful thing, because we know gradient descent is much more efficient than combinatorial search. Okay, so now I'm ready to talk about what I think should be the architecture of an intelligent system. And this is a diagram that you've probably seen in papers going back to the 80s, where an intelligent agent produces actions that have an effect on the world. And the world responds by producing percepts, which the agent uses to estimate the state of the world, essentially. Now, in classical reinforcement learning, or in a lot of classical settings, for this kind of thing, the reinforcement comes from the world to the agent. In this thing here that I mentioned, the objective is a function of the state of the agent. Basically, the objective is some sort of hard-wired, immutable function, if you want, that tells the agent whether it's happy or not. And we have that in our brains. There is things at the bottom of the brain that basically tell you if you're happy or not. And you cannot try to keep that thing in a particular state. So that's called intrinsic motivation, in the context of reinforcement learning. And so it's a bit different from the sort of classical model where the reinforcement comes from the world, okay? It's sort of intrinsically generated in this case. Okay, so what structure does the agent have to have if you want it to act intelligently? And basically, that's where this idea of a world simulator comes in. Of course, the agent could just learn to be reactive. And you could kind of specialize this diagram for supervised learning, for deep reinforcement learning, DP learning, for example. But really, if you want the agent to be intelligent, you would like the agent to be able to reason and plan. And to be able to reason and plan, there is a huge advantage to having at your disposal a world simulator, okay? So it's this green box here, inside of the agent, which takes a percept from the outside world that allows it to kinda get an estimate of the initial state of the world. And then it receives action proposals from an actor, and the actor sends its state to a critic. The role of the critic is to predict the future expected value of the real objective. Okay, so it's kind of a predictor for the objective in the long run. So again, getting inspiration from a dynamic architecture and classical control theory, when the system is meant to produce an action, what it does is that it kinda thinks about a sequence of actions that could possibly take the world into a particular state, or itself into a particular state that would satisfy its critic. And through gradient descent, it can actually figure out what the best sequence of actions would kind of optimize whatever objective it wants to optimize. And then once you have this sequence of action, you can do two things. You can take the action, first of all. And the second thing you can do is you can train the actor to predict the action directly so that next time you're in the same situation, you don't need to run the simulator anymore. You can just run the actor, and just produce the action. So I think this idea, actually, of producing targets for neural net by using some optimization algorithm, and then training the neural net to directly predict the result of this optimization, I think, is a very powerful concept that I've presently been using in a lot of different situations. But in this particular case, I think it's really interesting. This kind of idea has been used by people who are kind of at the boundary between machine learning and robotics. That use model base reinforcement learning for controlling robots and things like this. When I'm thinking of people like Peter Ibelli, Sergey Levine, Igor Mordatch, [INAUDIBLE] and other people. Okay, so how do we learn predictive forward models of the world. So one example I'm gonna show you is some running experiment that was done a couple of years ago. I mean last year really, although the paper is only one year old by some of my colleagues at Facebok. Adam Lerer, Sam Gross, and Rob Fergus, where they tried to train a ConvNet to predict the future from a 3D game engine. There is quite a bit of work in this area on sort of learning qualitative physics by Josh Tenenbaum in particular at MIT and various people at Deep Mind and other places. And so there, it's basically just a ConvNet. You give it an image coming out of kind of a 3D game engine with a physics engine. And you train it to predict where the objects are gonna go, fall in this case just because of gravity or whatever. And of course, you can just run the simulation and so you can train the system supervised. Okay, so a way to do systems identification on this virtual world if you want. And these are a few results. So what you see here is kind of a starting point on the left of every block and then the top row is the ground truth and the bottom row is what the system predicts. And so what you see is sometimes, sort of somewhat fuzzy predictions. You see fuzzy predictions here a little bit, because it's not entirely clear where this cube is gonna fall. Same here, it's sort of ambiguous. If you make the towers taller, you get even more fussiness in the prediction, because it's really not possible to tell where the blocks are going to fall. It's an interesting experiment, because although it's entirely trained on artificial data, it sort of works on natural data. The real towers of wooden blocks that are filmed. So that's an attempt of learning qualitative physics in a very simple environment. There is more and more interest in this kind of work now. In fact, both Deep Mind, Open AI, and Facebook research have proposed sort of environments. That allow people to kind of play with physics-based simulations to train intelligent agents to do the control and do causal inference and all kinds of things. Okay, so let me show you one example of a system that is able to infer the state of the world from text. So this is not from vision, you have a machine read a text and the machine needs to keep an updated estimate of the State of the World, so that it can then answer questions. And this system is not trained by reinforcement learning, it's trained supervised to answer questions. So it's been given a text and then at the end of the text, it's been given a question and it needs to answer the question. And it's given the answer to the question and then we train it supervised. Through that propagation, it's a recurrent net of some kind, to train itself to answer the question. But what it needs to do is that it needs to, because it reads the text, before being given the question. It needs to kind of figure out how to store the state of the world and update it every time it sees a sentence that describes an event. So before I talk about this model, I'm gonna describe a few work in this area that preceded it. Which are basically recurrent neural nets that are augmented by a piece of memory. And in my opinion, this is one of the most interesting developments of machine learning over the last few years or deep learning over the last few years. I'm actually missing a reference here which is the dynamic neural computer from Deep Mind that just appeared in nature a few weeks ago. So that started with [INAUDIBLE] and [INAUDIBLE] LSTM in the late 90s, had the idea of sort of augmenting neural nets with registers. Originally to release all the long term memory problem and the vanishing gradient problem. And then more recently, there were two pieces of works from physical research. One called the Memory Networks by Weston and the other one the Stacked-Augumented Recurrent Neural Net by Joulin and Mikolov. And both are basically recurrent neural nets where you add the piece of memory to the neural net. Because there is kind of very curious thing that recurrent nets are very bad at actually remembering things. So if you use a recurrent net and you run it for more that 20 durations or so, it will have forgotten all information about its initial state unless you build specific architectures into it like LSTM or explicit memories. And in fact, I learned from another scientist that the cortex in the brain is similar. The cortex by itself cannot remember things for more than about 20 seconds. And we know that because there are patients who have lost this separate piece of memory, called the hippocampus. The hippocampus kind of sits in the middle of the brain and sort of integrates a big chunk of the cortex and it's there, it's going to be working memory, hippozotic memory, etc. And if you don't have a hippocampus, you basically can't remember anything for more than 20 seconds. So nature has figured out that you need a separate entity for storing things, as a scratch pad memory or a short term memory. So what these guys have been doing is sort of various forms of memory. A particular one in the memory network is basically a differentiable memory. You need a differentiable memory.because you want to propagate gradient through it. So that a recurrent net that uses it can sort of decide what to write and how to read from it in the process of answering answers. So I'm not going to go into details of how this is built because I want to talk about the entity RNN. But basically those systems were used in the past to answer questions, to build a question answering system. So the Stack-Augmented RNN was for a different purpose. But the memory network was or particularly the form of it called end-to-end memory network or weakly supervised memory network was built for answering question. So you look at the diagram at the bottom right, a question comes in. So we assume that the memory stored in a form of a list of vectors, the story that it used to answer questions about. Okay, so the system reads a story and it stores every clause in the story, every sentence, if you want, in the piece of memory, and that process is gonna hand craft it if you want. The way the sentence is encoded into the vector is learned, but the fact that each sentence occupies the different memory slot, that's handcrafted. And then what happens is you encode the question in a form of a vector. Then you compare this vector with all the vectors in the memory. And with a kind of soft max type competition, the memory produces an output which is sort of weighted sum of the memory entries. Weighted by the those coefficients coming out of the soft max determined by whether the input vector kind of matches the items in the memory. And then that goes back to the recurrent net that kind of crunches on it and generates another vector to the memory, etc. So that allows the system to learn to access particular relevant facts or sentences in the memory, so as to kind of produce an answer. And at the end, you give it the correct answer, you back propagate the gradient. So you just train it with [INAUDIBLE] time essentially. And you fold it three or five times or so. So this system which came out about 2 years ago was able to do things like you can have it read the 15 version. It's 15 sentence version of Lord of the Rings and then ask questions about where every object is. More importantly, Jason Weston and his colleagues at Facebook came up with this list of 20 different tasks that this kind of neural net could be asked to solve. So question answering type tasks. Whether the questions can require a single fact, or multiple facts, or some inference, or require counting things, etc. And those memory networks, as well as other models that people have come up with more recently, can solve 18 or 19 of those tasks but not all 20. In fact, no existing system until now could solve all 20 of those tasks, all 20 types of questions. So here comes the entity RNN. So what the entity RNN is is basically sort of a distributed memory, RNN, if you want. So it's a bank of recurrent nets augmented by a memory. So each of those module is kind of a vector state if you want. Which you can think of as containing two things, kind of a key for the memory, as well as content for the memory. And whenever a sentence is given to the system, each item, each entity, each memory cell can choose to edit itself or not using a getting mechanism depending on the nature of the information that comes in. So here is a good example. If I say something like, John went to the kitchen, then what the entity network should do is perhaps have a cell that stores the properties of John. And change the location of John to the kitchen and you should also probably have the memory cell that stores the content of the kitchen and did that with John. In fact, I can say John picks up the milk. And then John gives the mail to Alice and Alice goes to the living room. So there is kind of a sequence of events like this and if too many events occur, then the previous system, the memory network can't really keep track of too many events. If there's 20 people coming into the room and then 19 leaving, if you count, you know that there's only one person left. But If you don't maintain a state as well, this is very difficult to do, you'll have to read your memory 39 times to be able to tell. So this system actually trains itself to keep a dated state of the world, if you want. And this is the first one that actually you can solve all 20 of the baby task, and my computer doesn't wanna switch slides. This is paper on archive that, so the first author is Mikael Henaff, H-E-N-A-F-F. And this is submitted to a so you can also see it on OpenReview.net. Okay, this machine is really sick, so I'm gonna have to completely reboot it, but it's gonna take a while. So maybe we can take a few questions before I go to the rest of the talk. Here we go, it came back. All right, so now let me talk about unsupervised learning really, because that's, if we want to build predictive world model in the real world, we need machines to be able to predict not discrete things, but continuous things in high dimension. That's really where the complexity of unsupervised learning is. Here is a way to formulate and supervised learning. There is a very natural way to formulate it in terms of density estimation. But I want to stay away from it for reasons that may or may not be clear at the end of my talk. So let's say our entire universe is composed of two variables,Y1 and Y2. And what we observe in the world are those points along this curve. So there is obvious a dependency between Y1 and Y2. If I give you Y1 you can probably predict Y2, more or less. If I give you Y2 it might be two values that are possible. So there is a dependency between them. And my view of unsupervised learning is that we should have the machine learn a contrast function, which I'm gonna call from now on an energy function. In such a way that this energy function, this is not an energy function we minimize by learning, this is not what I mean. It's an energy function that is kinda like a negative likelihood if want of the data. So it's something that tells you if you are on the near data, real data or far away from data. It's a contrast function that will take low values on the training samples and higher values everywhere else. And of course, there is lots of different functions that will satisfy this condition. Okay. So training the function of this type, shaping your function of this type by training a learning machine to compute this surface is called energy-based learning. And I've used this for supervised learning in the past. But one of the things you might wanna do here is kind of a energy function being trained here. One process to learn this is that you have a neural net or whatever parent transfunction you want being fed a Y vector. And it produces a single scalar value, which is the value, the height of this curve at this particular point. So if you feed it a point from the data set, you tune the parameter of this function in such a way that the output goes down, right? You want low energies for points that actually come from your data set. And then the big question, the complicated question is how do you make sure the energy is higher everywhere else? And so a technique for example is to pick points randomly or semi-randomly outside of the kind of manifold of data, if you want, and push them up, push the energy up. So tune the parameters of the Machine so that the energy goes up. But in fact there are lots and lots of ways to do this to kind of make sure the energy outside of the manifold of data is higher than everywhere else. And in fact, you can sort of formulate a lot of classical unsupervised learning algorithms in those terms. You can find the equivalent energy function that those algorithms compute like PCA, K-means, gas mixture models, square ICA, etc. And kinda view their learning algorithm in the context of this kinda learning this contrast function. So for the probabilistic models, of course, it's obvious the contrast function is negative log likelihood, negative density that the model learns. But that's just a special case. So maximum likelihood works in that case you make the probability of the data points high, automatically because of normalization the probability of the other points goes down. The problem is all the interesting models of probability densities that we wanna use are essentially intractable. We can't normalize them easily and so we had to resort to all kinds of tricks to make sure that, to approximate the fact that whenever we push up from the probability of something we have to push up on the probability of other things. In terms of energy, that means when you push down on the energy of the data points we had to push up on everything else around. And so, I made a list of seven different methods to push up the energy of things outside of the many full of data. One is to build the machines so that the volume of low energy stuff is constant like PCA, K-means, et cetera. Push down the energy of data point, push up everywhere else, that's really what maximum likelihood is doing. Push down the energy of data point, push up on chosen locations. This is what contrastive divergence does, ratio matching, noise contrastive estimation, minimum probability flow. There's all kinds of methods of rational inference. Minimize the gradient and maximize the curvature around data points to score matching from. Train a dynamical system, so that the dynamic goes to the manifold of data. Thus the idea of denoising auto-encoder. There is no energy function there, it's more like a vector field. Use a regularizer that limits the volume of space that has low energy. That's the idea behind regularized auto-encoders like sparse auto-encoders and things of that type, which were very popular in the mid 2000s when among people working on deep learning. And then there is other techniques. And PCA gives you energy functions like this. So if you train with data set that come from this little spiral here, PCA will give you an energy function of this type. Basically low energy on the principle axis and quadratically increasing energy as you move away from it. K mean gives you something like what you see in the right where each of the prototypes are put around the surface. It seems to work really well except it doesn't work in high dimension really well. Sparse coding does a piece wise approximation of your manifold of data. But all of this I think, it's not so useful anymore because a new idea popped up two years ago called adversarial training. Sometimes known as generative adversarial networks. This is a very, very, very cool idea by Ian Goodfellow. Who at the time was a student in Yoshua Bengio's lab in Montreal. And since then moved to Google, and since then moved to Open AI. And I think this is the best idea machine learning in the last 10 years. Occasionally I would say the last 20 years. I am really a big fan of this idea. And the reason why I'm a big fan, I think is because it's the ticket to solving the main problem unsupervised learning, which is that the problem of unsupervised learning is predicting under a certainty. So here is a prediction problem here. Let's say we want to predict the future. So I give you a snippet of video that shows me putting a pen on the table and letting it go. And I ask the system what is the world gonna look like a quarter second from now? So, obviously the pen is gonna fall but it's very hard to predict exactly in which direction the pen is gonna fall, right? So, here's this G function here, takes the past video frames. It also takes as source of random vectors that's the z variable, think of it as it kind of written variable. And, it predicts the future and it's gonna make a prediction, which is this point y bar, which is symbolized by the video of the pen falling to the left. But in fact, when we observe the future, the future is telling us that the pen doesn't fall to the left, it falls to the back and slightly to the right. And so should we punish the machine for making the wrong prediction? We shouldn't because there is really no way the machine could have predicted the correct answer. It sort of, it predicted something that's kinda conceptually correct but even if it's not exactly correct. So perhaps what we could say is the machine, what we want the machine to predict is one point along this red ribbon of plausible features. And if it predicts the point on that ribbon, we don't want to punish it for it. If it predict something outside the ribbon, we do want to punish it for it, we want to predict something on the ribbon. Okay, now the thing is, we don't know what this is gonna look like in advance. And so what we need to do is train another neural net to learn the ribbon. And that's an energy based model. It's a model that basically learns on the ribbon, produce low energy, outside the ribbon, produce high energy, okay? So what I'm going to talk about now is a slightly different formulation of adversarial training than the original one by Ian Goodfellow. So this is kind of energy based adversarial training if you want. And really it works like this. You have the discriminator. So the discriminator is really this energy function, this contrast function. It's output is scalar. It's supposed to be low on the real data point, the blue point. And it's supposed to be higher on every other point. And you train it in two phases. You show it a point from the data set. And you trained the parameters of the discriminator to lower its output. Cuz you want low energies for real data point. And then you pick the green point. I'll tell you how in a minute. And you train the discriminator to now increase its output because the green point doesn't come from the data. It comes from some other process. So now the list of seven methods I described was basically how to kind of come up with these green points. What we're gonna do now is train a neural net to generate those green points. So instead of using Markov Chain Monte Carlo, grading descent, contrastive divergence, whatever, we're gonna use a neural net. And we're gonna train it to produce the green points. So what this neural net, the generator is gonna do is produce those waypoints, which are the green points. Every time it produces a green point, the discriminator knows it's fake, so it increases its output. It tunes its parameters so that its output increases. But then the generator is going to cheat. Because what it wants to do is produce points that are as close as possible to the data. And so it gets the gradient of the output of the discriminator in respect to its input. And with this gradient, it's going to adjust its parameter so that the green point gets closer to the manifold of data. And so eventually what's gonna happen is the green points are gonna get closer. And there's going to be an equilibrium when the two things kind of balance each other. The green points are on the dataset and the discriminator can't make the difference. And the generator, basically, doesn't get any gradient because the energy of those points is already low. So there's been a lot of really amazing success by an increasingly large number of groups. On generating images using this adversarial training. Bedroom images, this the deep computational generative adversarial network of Radford, Metz, and Chintala. So Radford now is at OpenAI, Chintala is at Facebook. You can interpolate in the latent space, you can do arithmetics on faces. There's this new form of energy based adversarial network. Where we can basically choose whatever architecture we put in our discriminator. In particular, we can put a encoder. That's one of the things that we have in our paper. And we have two loss functions. And what we can show in this new paper about energy based GANs. Is that the optimization performed by the system reaches a Nash equilibrium. And at the Nash equilibrium the probability distribution of the data produced by the generator. If it's infinitely powerful are kinda similar to the one of the data. So, there are sort of interesting things here. It sort of breaks the usual paradigm that we have in machine learning. We're not minimizing or maximizing a function anymore. We're finding a center point, or really a Nash equilibrium because it's not a single function. We're kind of minimizing two functions. But because one of their functions has kind of the negative of the first. They kind of compete with each other and there is a Nash equilibrium. Those things work pretty well. So if you use the discriminator as a feature extractor now. So you want to use it to recognize digits, for example, you train it on MNIST, it actually works pretty well. It gets 0.9% error using a fully connected network of a particular type of a Ladder Network. Trained unsupervised using that. And then supervised with just 1,000 samples per category, which is kind of pretty amazing. You can use it to generate images. So this is one of those Energy-Based GANs trained on the ImageNet dataset. 1.3 million samples without any information about the category. And so then you just feed it a random vector and you ask it to produce an image and it produces things like this. So if you are far in the back, there is repeating screen so I'm not sure. But if you are far from the screen, you may think those are kinda cool looking images. And if you are in the first row, you realize, what the hell is this, there is no object you can identify in there, right. It's all a mess. It looks kind of statistically correct from far away, but they actually are not any objects you can identify. If you train the same model purely on dogs, you get sort of weird, soft dogs. We are in Salvador Dali country. I think his house was a few hundred kilometers away from here. So maybe those are Salvador Dali dogs. So another thing you can use adversarial training for is video prediction. You feed that to a computational net. And of course it's just another form of supervised learning in a way. Where you feed the system with four frames and you ask it to predict the next two frames. And if you do this with least square you get those very blurry predictions that you see at the top right. And that's because what the system predicts is an average of all the possible futures that could possibly happen. And it doesn't know which one happens. So it produces an average and that's a blurry picture. Here is what you get if you use adversarial training. So again on the top right, this is l2 d squared loss. And the other images are with adversarial training. And so the first four frames are observed, and the last two frames are predicted. And they're not blurry. Here's another version of this where we predict five frames. And those are trained on segments of video shot in apartments in New York where the camera pans. And so what the system has to do to be able to predict the red frames. Is to kind of invent what the apartment looks like in places where it hasn't seen it. And it's a little hard to see on this one, but let me look at this. So the right panel on the bottom left, there is kind of a bookcase type library. And the system kinda dreams up what this bookcase is supposed to look like, as the camera moves. And it kinda makes a decent job at sort of inventing books and shelves. However, if you let it run for 50 frames, it doesn't quite work. So one thing to understand, so since then, since we produced this work, other people have done video prediction. And they've done video prediction by making assumptions that are relevant to video. There are objects in video and it's basically motion of things that don't change, and etc. You can do a much better job if you make those [INAUDIBLE]. But we're not interested in video prediction, we're interested in just prediction. And so we don't want to put anything in it thats specific to video. So we want to use this for text or for other types of modalities, speech, etc., just a model of the world, physics, etc. Okay, so I'm going to stop here, and thank you for your attention basically. I'm gonna show the dogs because it's just too funny. So the conclusion of this is that what we need to do, what we need to work on I think to make machines. Is what people would call model based reinforcement learning. And so one of the things that we are the most interested in at Facebook to work on is model based reinforcement learning. At some point, we have the reputation of not liking reinforcement learning, that's not true as you saw. But what we're working on is the whole cake. You need a cherry on the cake, but you need the whole cake. The cherry without the rest of the cake is kind of pointless. So learning predictive models of the world, I think is the ticket. And then learning actions and then optimizing intrinsic objective functions is really I think the path forward for making progress in AI. Thank you very much.  [APPLAUSE] you 