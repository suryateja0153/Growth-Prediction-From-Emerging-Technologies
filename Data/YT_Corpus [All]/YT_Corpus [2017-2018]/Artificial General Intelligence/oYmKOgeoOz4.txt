 [Music] hello my name is John Burke Ali and I work at verily which is Google's life sciences company I also lead a group called singularity Network which is an internal organization composed of more than 3,000 Googlers focused on topics about the future of artificial intelligence for which we are here today it's my pleasure to be here today with dr. max tegmark as a brief introduction max tegmark is a renowned scientific communicator and cosmologists and is accepted donations from Elon Musk to investigate the existential risk from an advanced artificial intelligence his research interests include consciousness the multiverse advanced risk from AI and formulating an ultimate ensemble theory of everything Max was elected fellow of the American Physical Society in 2012 one scientist science magazines breakthrough of the year in 2003 and it was written over 200 publications nine of which have incited more than 500 times max tegmark ever thank you so much it's a really great honor to be back here old friends and so much intelligence now there are some important lessons I think we can learn from this as well so I want to devote the rest of this talk to another journey powered by something much more powerful than rocket engines where the passengers are not just three astronauts but all of humanity so let's talk about our collective journey into the future with AI my friend Yan Talon likes to emphasize that just as with rocketry it's not enough to just make our technology powerful we also have to focus on figuring out how to control it and on figuring out where we want to go with it and that's what we're gonna talk about I think the opportunities are just so awesome if we get this right during the past 13.8 billion years our universe has transformed from dead and boring to complex and interesting and it has the opportunity to get dramatically more interesting in the future if we don't screw up no for about four billion years ago life first appeared here on earth but it was pretty dumb stuff like bacteria that couldn't really learn anything in their lifetime I call that life 1.0 we are my I call life 2.0 because we can learn things which of course in geek speek means we can upload new so four modules if I want to learn Spanish I can study Spanish and now I have all these new skills uploaded and then I mind and it's precisely this ability of us humans to design our own software rather than be stuck with whatever the software evolution gave us which has enabled us to dominate this earth and give us what we call cultural evolution where seem to be gradually heading towards life 3.0 where we can wear which is life that can design not just its software but also its hardware maybe we're a 2.1 right now because we can get cochlear implants and artificial knees and a few minor things like this but if you were robots there were able to think is cleverly is right now of course there we no limits whatsoever how you could upgrade yourselves so let's first talked about the power of the technology obviously the power of AI has improved dramatically recently I'm gonna define intelligence itself just very broadly as the ability to accomplish complex goals I'm giving such a broad definition because I want to be really inclusive and include both all forms of biological intelligence and all forms of artificial intelligence and as you guys here at Google all know obviously a subset of artificial intelligence is machine learning where systems can improve themselves by using data from their environment much like biological organisms can and another subset of that is the full course deep learning which we use neural net architectures and if you look at older breakthroughs in AI like when Gary Kasparov got his posterior kicked by IBM's deep blue the intelligence here was of course mainly just put in by human programmers and deep blue beat Kasparov just because it could think faster and remember better whereas in contrast the recent stuff that you've done here at Google like this work by Ilya sutskever group there's almost no intelligence at all put in by the humans and they they just trained the simple neuron lab with with a bunch of data and you put in the numbers that represent the pixel colors and it puts out this caption a group of young people playing game of Frisbee even though the software was never told any told taught anything about what if frisbee is or what a human is or what a picture is and the same stuff you put in other images gives other captions which are often quite impressive I find it even more striking how cool things can be done with videos so this is google deepmind of course learning to play Atari games and for those of you those few of you who haven't seen this before you need to remember that this neural network here with simple reinforcement learning built in had no idea what a game was what a paddle was what a ball was or anything like that and just by practicing gradually starts to miss the ball less often and drop the point where it hardly missed it all and plays much better than Ike I could play this and and there the real kicker is it of course the people at deep mind they actually didn't know that there was a clever trick you can do it when you play breakout which is always aimed for the corners and try to build a little do a little tunnel there so when so once this little deep learning software figure that out it's just every single time the ball comes back look how just uncannily precise it is just putting it from right back there and in the corner and playing I can only dream the flip to play this well now this is of course this very very simple environment a little two-dimensional game world but if you're a robot why you can think of life as a game just a more complex one and you could ask you ask yourself to what extent these sort of techniques might enable you to learn more interesting things and and so deep mind more recently have three-dimensional robots in a simulated world and just ask to see if they could learn to do things like walk and this is what happened this software had never ever seen any videos of walking he knew nothing about the context the concept of walking right all that the software was doing was sending random commands to how to bend the different joints and it got rewarded every time this creature managed to move a little bit forward and it looks a bit funky maybe a little bit awkward but hey actually learns learns interesting stuff so this raises this very interesting question okay how far can a I go how much of what we humans can do will machines ultimately be able to do if we use not just the techniques that we know of so far but factoring also the additional progress that you people in the room and elsewhere are gonna do I like to think about this in terms of this landscape I drew this you've made this picture inspired by a paragraph and the one of my favorite books by Hans Moravec for many years ago where the height here represents how difficult it is for a computer to do a certain task and the sea level represents how good computers are doing them right now so what we see here is that certain tasks like chess playing in arithmetic some of course long been submerged by the slowly rising tide of machine intelligence and there there's some people who so who think that there are certain tasks like art and book writing or whatever that machines will never be able to do and there are others who think that the old goal of AI to really solve intelligence and do everything that we do will mean that sea levels will eventually submerge everything so what so what's gonna happen there have been a lot of interesting polls of sign of AI researchers and the conclusion is very clear we don't know little bit more specifically though what you find is there are some people in the techno skeptic camps you think that the AI researchers ultimately doomed we're never gonna get there or maybe we're only gonna get there hundreds of years from now but actually most thing I researchers think it's gonna happen and more in a matter of decades and some people think that we don't have to worry so much about steering this Rockets metaphorically speaking because it's not gonna happen there we'll ever get powerful after we have to worry about this but but that's a minority and and then there are people who think we don't have to worry about steering because we're it's guaranteed that the outcome is going to be awesome I call such people digital utopians and I respect that point of view and they're also people who think it's guaranteed that things are gonna suck so there's no point in worrying about steering is very screwed anyway but most of the people in surveys tend to land or here in the middle and what I've called beneficial a eye movement where you really motivated actually to ask what can we do right now to steer things in a good direction because it could be awesome or it could be not so great and it depends on what we do now I put this webpage up age of eight org and we did a survey there for people from the general public could answer these same questions you can go there and do it too and I was actually very interested that the general public responds almost exactly the same as AI researchers have done in recent polls this is from something I analyzed this weekend with fourteen thousand eight hundred sixty-six respondents and you see most people think maybe we're decades away from human level AI maybe it'll be good maybe there'll be problems so this is maximally motivating to think about how it can steer this technology in a good direction so let's talk about steering how can we control how can we learn to control a eye to do what we want it to do to help with this my wife Mayer who's sitting there and I and some other folks founded the future of life Institute and you can see we actually have the word steer up here in a mission statement our goal is simply to do what we can to help make sure that technology is beneficial for Humanity and I'm quite optimistic that we can create a really inspiring future with technology as long as they win this race between the growing power of the technology and the growing wisdom with which we manage it but I think if we're going to win this race we actually have to shift strategies because technology is gradually getting more powerful and when we invented less powerful tech like fire we very successfully used the strategy of learning from mistakes and meant that fire oopsie and then invented the fire extinguisher invented the car oopsie and then we invented the seatbelt the airbag the traffic light and things were more or less fine but when you get beyond a certain point at the power of the technology this idea of learning from mistakes it's just really really lousy right you don't want to make mistakes if one mistake is unacceptably many and when we get mean talked about nuclear weapons synthetic biology and certainly superhuman a high I feel we're at the point where we really don't want a mistakes make mistakes we're gonna shift strategy from being reactive to being proactive which is exactly the slogan you said you're also using for your work here at Google earlier and then I'm optimistic that we can do this if you really focus on and work for it some people say you know don't don't talk about this because it's just Luddite scaremongering when you're talking about things that could go wrong I don't think it's Luddite scaremongering I think it's safety engineering we started by talking about the Apollo moon mission you know when NASA thought through very carefully everything that could possibly go wrong when you put three astronauts on top of this 100 meter tall rocket full of highly explosive fuel that wasn't blood I'd scaremongering what they were doing was precisely what ultimately led to the success of the mission and this is where I think we want to be doing where they I as well I think so far what we've learned from other technologies here is that we need to up our game a little bit because we haven't really absorbed this idea that we have to switch to being being being proactive today is a very special day in terms of nuclear weapons because we came pretty close to September 26 being the 34th anniversary of World War 3 in fact it might have ended up this way if this guy stanislav petrov hadn't just done got instinct this ignored the fact that his early warning system said that there were five incoming Minuteman US missiles those should be retaliated against so how can we do better how can we win this wisdom race I'm very very happy that the AI community has really started engaged with these issues a lot in recent years and thanks to a lot of people who are in this room here included Peter Norvig and with the future life is do you organize the couple of conferences in Puerto Rico and then in this year earlier this year in Asilomar California where there was really quite remarkable consensus around a number of very constructive things we can do differently try to develop this wisdom and steer things in the right direction and I want to spend just a little bit of time time hitting some highlights of things here from this list of 23 Asilomar principles which has now been signed by over a thousand AI researchers around the world first of all people first of all there was it says here on item one that we should just define the goal of it of AI research not to be just making undirected intelligence but to make beneficial intelligence so in other words the steering of the rocket is part of the design specs budget and and then there was also very strong consensus that hey if we have a bunch of unanswered questions that we need to answer we shouldn't just say oh yeah we should answer them well we should answer them the way we scientifically know is the best way to answer hard questions namely to research them to work on them right and we should fund this kind of research is just an integral part of computer science funding but both in companies and in industry and I'm actually very very proud of Google for being one of the founding members of the partnership on AI which enable a marry much to support this kind of AI research and eye safety research another another principle here that the was very broad agreement was a shared prosperity principle that the economic prosperity created by AI should be shared broadly to benefit all of humanity what do I mean by that obviously technology has kept growing the economic pie it's been growing our GDP a lot in recent decades as you can see if you look at the top line and this is plot here but as you're also generally aware of this pie hasn't been divvied up quite equally and in fact if you look at the bottom ninety percent of income earners their income has stayed flat almost since I was born actually maybe it's my fault and the 30 percent poorest in the US have actually gotten significantly poorer in real terms in recent decades which has created a great deal of anger which has given us the election of the whole Trump it's given us bricks it and it's given us a more polarized society in general and there's there was a very strong consensus among AI researchers that you know if we can create so much more wealth and prosperity and have machines help produce all these wonderful goods and services then if we can't make sure everybody gets better off from this now shame on us some people say well this is just nonsense because something magical is going to change in the statistics soon and the jobs that get automated away are gonna be replaced by much better new jobs that don't exist yet but actually if you look at this data it doesn't support that we could have made that same argument 200 years ago when much more people worked in farming but all those jobs that were lost we're gonna replace replace by new jobs that didn't exist yet and this is what actually happened this is made this little pie chart here of all the jobs in the u.s. by sides and you can start going down list managers drivers retail sales persons cashiers etc only when you get down to twenty first place then you get to a job category that didn't exist 100 years ago namely software developers hi guys so clearly what happened is not that most farmers became software developers what instead happened was people who lost generally from the Industrial Revolution and onward jobs where they were using their muscles to do work went into other jobs where they could use their brains to do work and these jobs tended to be better paid so this was a net win but they were jobs that already existed before now what's happening today which is driving the growth in income inequality is similarly that people are getting switched into other jobs that have existed before it's just at this time since the jobs that are being automated away are largely jobs where they use their brains they often switch new jobs that existed before but pay less rather and pay more and I think it's it's a really interesting challenge for all of us to think about how we can best make sure that this growing pie just makes everybody better off another item here on on this list is principle number 18 - am AI arms-race this was the one that had the highest agreement of all among the Sylmar participants an arms race and lethal autonomous weapons should be avoided why is that well first of all we're not talking about drones we're which are remote-controlled vehicles where human is still deciding who to kill we're talking here about about systems where the machine itself using machine learning and what whatever decides exactly who is going to be killed and then does does the killing and first whatever you think about them the fact is there's although there's been of course a huge amount of investment in civilian uses of AI recently it's actually dwarfed by Tom talk about military spending here recently so if you look in the pie we there's a real risk that the status quo will just mean that most of the slewed sucking noise trying to recruit AI graduates from MIT and Stanford an elsewhere will be to go to military places rather than to places like Google and most AI researchers felt that that would be a great shame here's my wife think about it if you look at any science right you can always use it to develop new ways of helping people or new ways of harming people and biologists fought really really hard to make sure that their science is now known as new ways of curing people rather than for biological weapons they fought very hard and they got an international ban on biological weapons passed similarly chemists managed to get the chemical weapons banned by really speaking up as a community and persuading politicians around the world that this was good and that's why your associate chemistry now mainly with new material it's very stigmatized to have bioweapons so even if some countries cheat on them it's so stigmatized that Assad even gave up his chemical weapons not get invaded and if you want to buy some chemical weapons to do something silly you're gonna find it really hard to find anyone who's going to sell them to you because it's so stigmatized and what what there's very widespread support for in the AI community is exactly the same thing here to try to negotiate an international treaty where the superpowers get together and say hey you know the main winners of having an out of control arms race in any weapons is not gonna be the superpowers it's gonna be Isis and everybody else who can't afford expensive weapons but would love to have little cheap things that they can use to assassinate anybody with anonymously and basically drive the cost of anonymous assassination down to zero and this is something that if you want to get involved in the United Nations is going to discuss this in November actually and I think the more vocal the air community is on this issue the more likely it is that the AI rocket here is gonna veer in the same direction as the biology and chemistry rocket went finally let me say a little bit about the final similar principles here I find it really really remarkable that even though a few years ago if you started talking about super intelligence or existential risks or whatever many people would dismiss you as some sort of clueless person who didn't know anything about AI these words are in here and yet this is signed by you know that mr. Saha based the CEO of deepmind it's it's signed by Peter Norvig who's sitting over there by your very own Jeff Dean and by really who's who of AI researchers over a thousand of them so there's been a much greater acceptance of the fact that hey this is part of stealing if maybe AI is actually gonna succeed and maybe we need to take these sort of things into account let me just unpack a little bit what the deal is with all this so so first of all why should we take seriously at all this idea of recursive self-improvement and super intelligence we saw them a lot of people expect we can get the human level AI in a few decades but why would that mean that maybe we can get hey I'm much smarter than us not just a little the basic argument for this is very eloquently summarized in just this paragraph by or I J good from from 1965 and a mathematician who worked to Alan Turing to crack codes during World War two and you mainly I think you're mostly heard this all before basically says that if we have a computer that a machine that can do everything as well as as we can well one of the things we can do is design AI systems so then it can - and then you can hire instead of hiring 20,000 Google employees that work for you it's 20 million I think is working for you and they can work much faster and that the speed of AI development will no longer be set by the typical or nd timescale of humans or years but by how fast machines can help you do this which could be way way faster and if it turns out that that we have a hardware overhang where where we've compensated for the fact that we really are kind of clueless about how to do the software of human level III by having massive amounts of extra hardware then it might be that you can get a lot of true improvements first even we're just by changing the software which is something that the that should be done very very quickly right without even having build new stuff and then from there on things could get them you might be able to get machines that are just dramatically larger or dramatically smarter than us we don't know if this will happen but people but basically what we see here is that for linear researchers reviewing this is at least a possibility the way we should that we should take seriously another thing what you see here is existential risk it's a more specifically it says here risks posed by AI systems especially exponential risks must be subject to the planning and mitigation efforts commensurate with their expected impact an existential risk is the risk which basically include humanity just did he wiped out altogether why would you possibly worry about that there are so many absolutely ridiculous Hollywood movies with Terminator robots or whatever you can't even watch like cringing so what are the serious reasons that people like this sign onto something that talks about that well the common criticism you hear is that well you know machines there's no reason to think that intelligent machines would have human goals if we built them and after all this why should they have sort of weird alpha male goals of trying to get power or even self-preservation you know my laptop doesn't protest when I try to switch it off right but there's a very interesting argument here I just want to share with you in the form of the silly little fake computer game I drew for you here just imagine that you are this little blue friendly robot whose only goal is to save as many sheep as possible from the big bad wolf okay you don't care you have not put into this route this robot does not have the goal of surviving or getting resources or any stuff like that just sheep saving it's all about these cute little sheepies okay it's gonna very quickly if it's smart or figure out that if it walks into the bomb here and blows up then it's not gonna save any sheep at all so a sub goal that it will derive is well actually let's not get blown up it's gonna get a self-preservation instinct this is a very generic conclusion if you program if you have a robot then you program it to walk to the supermarket and buy your food and cook you a nice dinner you know it's gonna again develop the sub goal of self-preservation because if it gets mugged and murdered on the way back with your food it's gonna not give you dinner so it's gonna want to somehow avoid that right self-preservation is an emergent goal of almost any goal that the machine might have goals are hard to accomplish when you're broken and also if if the robot the robot might develop finally have an incentive to get a better model of the world that's in here and the discover that there's actually a shortcut it can take the to the where the sheep are faster then it can save more so trying to understand more about how the world works is a natural sub goal you can get no matter whatever fundamental goal you program the machine to have and then resource acquisition to can emerge because if when this little robe bought here discovers it when it drinks the potion it can run twice as fast then it can save more sheep so it's gonna want the potion it'll discover them when it takes the gun it can just shoot the wolf and save all the sheep great it's just gonna want to have resources and so I've summarized in this pyramid here this idea which has been very eloquently was mentioned first by Stevo mohandro who lives here in the area and just talked a lot about in Nick Bostrom's book the idea is just that whatever fundamental goal you give a very intelligent machine if it's pretty open-ended it's pretty natural to expect that it might develop sub goals of not wanting to be switched off and try to get resources and that can be fine there's no not necessarily a problem being in the presence of more intelligent entities we all did that as kids right with our parents the reason was fine was because their goals were aligned with our goals so therein lies the rub we want to make sure that if we ever give a lot of power to machines of intelligence comparable or greater to ours that their goals are aligned with ours otherwise we can be in trouble so to summarize these are all questions that we need to answer technical research questions how can you make how can you how machines learn not retain our goals for example and let me just show you a very short video up talking about these issues and super intelligence and then some and see if we have better luck with video at this time will artificial intelligence ever replace humans is a hotly debated question these days some people claim computers will eventually gain super intelligence be able to outperform humans on any task and destroy humanity other people say don't worry AI will just be another tool we can use in control like our current computers so we've got physicists and AI researcher max tegmark back again to share with us the collective takeaways from the recent Asilomar conference on the future of AI that he helped organize and he's going to help separate AI myths from AI facts hello first off max machines including computers have long been better than us at many tasks like arithmetic or weaving but those are often repetitive and mechanical operations so why shouldn't I believe that there are some things that are simply impossible for machines to do as well as people say making mini physics videos or consoling a friend well we've traditionally thought of intelligence is something mysterious that can only exist in biological organisms especially humans but from the perspective of modern physical science intelligence is simply a particular kind of information processing and reacting performed by a particular arrangement of elementary particles moving around and there's no law on physics that says it's impossible to do that kind of information processing better than humans already do it's not a stretch to saying that earthworms process information better than rocks and humans better than earthworms and in many areas machines are already better than humans this suggests that we've likely only seen the tip of the intelligence iceberg and they were on track to unlock the full intelligence that's latent in nature and use it to help humanity flourish or flounder so how do we keep ourselves on the right side of the flourish or flounder balance what if anything should we really be concerned about with super intelligent AI he knows what has many pop AI researchers concerned not machines or computers turning evil but something more subtle super intelligence that simply doesn't share our goals if heat seeking missile is homing in on you you probably wouldn't think no need to worry it's not evil it's just follow me it's programming know what matters to you is what the heat-seeking missile does and how well it does it not what it's feeling or whether has feelings at all the real worry isn't malevolence but competence super intelligent AI is by definition very good at attaining it's gold so the most important thing for us to do is to ensure that its goals are aligned with ours as an analogy humans are more intelligent and competent than us and if we want to build a hydroelectric dam where there happens to be an anthill there may be normal level ones involved but well too bad for the ants cats and dogs on the other hand have done a great job of aligning their goals with the goals of humans I mean even though I'm a physicist I can't help think kittens are the cutest particle Arrangements in our universe if people super intelligence we'd be better off in the position of cats and dogs than ants we're better yet we'll figure out how to ensure that AI adopts our goals rather than the other way around and when exactly is super intelligence going to arrive when do we need to start panicking first of all Henry's super intelligence doesn't have to be something negative in fact if we get it right hey I might become the best thing ever to happen to humanity everything I love about civilisation is the product of intelligence so if AI amplifies our collective intelligence enough to solve today's and tomorrow's greatest problems humanity might flourish like never before second most AI researchers think super intelligence is at least decades away but the research needed to ensure that it remains beneficial to humanity rather than harmful might also take decades so we need to start right away for example we'll need to figure out how to ensure machine is learn the collective goals of humanity adopt these goals for themselves and retain the goals as they get ever smarter and what about when our goals disagree should we vote on what the machines goals should be should we do whatever the president wants whatever the creator of the super intelligence wants let the AI decide in a very real way and then the question of how to live with super intelligence it's a question of what sort of future we want to create for Humanity which obviously shouldn't just be left to AI researchers as caring and socially skilled as we are so at least the very final point that I want to make here today the to win this wisdom race creating an awesome future with a high in addition to doing these these various things I've talked about we really need to think about what kind of future we want what sort of goal we want to have we want to steer our technology so just for fun this survey I mentioned that we did we asked people also to say what they wanted for the few and I'll just share with you here these are from the analysis I did last weekend most people out of the 14,000 866 here say they actually want oh hey I have to go all the way to superintelligence so that some are saying no here most people a lot of people want humans to be in control most people actually want both humans and machines to be in control together and the small fractions you see prefer the machines and then when asked about consciousness a lot of people said yeah they would if they have machines are behaving as if there is intelligent zoom as they would like to have them have a subjective experience also so the machines can feel good but some people said nah but they prefer having exam be robots don't feel conscious that people don't have to feel guilty about switching them off or giving them boring things to do in terms of what a future civilization should strive for there was a large majority you felt we should either try to maximize positive experiences or minimize suffering or something like that then more people who said that if you'd let the future civilization pick whatever goals they want as long as it's reasonable some people said they didn't even care about if they thought they called the future wanted was reasonable even if it was pointlessly banal like maybe turning our universe into paper clips they were fine with just delegating into humans but most people actually felt that since we're creating this technology we have the right to have some say as to where things should go the broadest agreement of all was on this question that actually maybe we shouldn't just limit the future life to forever be stuck on this little planet but give it the potential to spread and flourish throughout the cosmos and to get people thinking of more about different futures my wife mayor likes to point out that even though it's a good idea to visualize positive outcomes when you plan your own career and then try to figure out how to get there we kind of do the exact opposite as a society we just tend to think about everything that could possibly go wrong and then we like freak out about it when you watch Hollywood movies it's almost always this topic depictions of the future right so to get away from this a little bit in in my book the whole chapter five is a theories of thought experiments with difference of future scenarios trying to span the whole range of what people have talked about another so you yourselves can ask what you would actually prefer and the most striking thing from the survey was that people disagree very strongly and in what sort of society they would like and this is a fascinating discussion that I would really encourage you all to to join into I'm just gonna end by saying that the I think when we look to the future there's really a lot to be excited about people sometimes ask me max you know are you for AI or against AI and I respond by asking them what about fire are you for it or against it and of course they'll concede that there for fire to heat their homes in the winter and against fire for arson but it's the same with all technology it's always a double-edged sword the difference with AI is just it's much more powerful so we need to put even more effort into into how how we steer it if you want life to exist for beyond the next election cycle and maybe hopefully for billions of years on earth and maybe beyond then it's just pressing pause on technology forever there's actually just a really sucky idea because if we do that the question isn't whether humanity is going to go extinct the question is just what's gonna wipe us out whether it's gonna be the next massive asteroid strike like the one that took the Dinos out or the next supervolcano or another one on a list of long things that we know are gonna happen to earth the technology can create later sorry the technology can prevent but technology we don't have yet that's gonna require further development of our text so I for one think that it would be really foolish if we just run away from technology I wouldn't I would feel I'm much more excited about in the Google spirit and I love your old slogan don't be evil asking what can we what can we do to steer the development there are technology in the direction so that life can really flourish not just for the next election cycle but for a very very long time on earth and maybe even throughout our cosmos thank you [Applause] thanks so much Max and anyway we have time for questions from the audience we have a mic over here which we can use for questions and I so I can pass this one around and while we're doing that I'll pull up the Dory great and since you mentioned there were a lot of questions make sure to keep the questions brief and make sure that they actually are questions AI risk seems to become a much more mainstream worry in the last few years what changed to make that happen and why didn't we do it earlier I I'm actually very very happy that it's changed in this way and trying to help make it change this way it was the key reason we founded the future life Institute and organized the Puerto Rico conference and they still amore conference and so on because we felt that that up until a few years ago that the debate was kind of this dysfunctional and we what I think has really really changed things for the better is that the AI research community itself has really engaged joined this debate and started to own it I think that's why it's become more mainstream and also much more more sensible okay so you're the boss we alternate with online offline questions do you want to read the questions so as I said I think Google already has the solo to do exactly what's needed this don't be evil slogan Larry and Sergey I interpreted it as though we shouldn't just build technology because it's cool but we should think about its uses you know for those of you who know the tall Tom Lehrer song about very often brown that's not once the Rockets go up who cares where they come down that's not my department says very fond Brown I view Google don't be evil slogan it's exactly the opposite of that thinking mindfully about how to steer the technology to be good I'm also really excited again that Google is one of the founding partners in the partnership for AI trying to make sure that we that this happens not just in Google the Google in what Google does but but throughout the community and I also think it's great if Google can pull all of its strings to persuade politicians all around the world seriously fund AI safety research because the sad fact is even though there's a great will for AI researchers to do this stuff now there's almost no funding for it still that what Elin must helped us give out 22:37 grants for is just a drop in the bucket of what what's needed and it makes sense but Google and other private companies want to own the IP on things that make AI more powerful and build products of it but these same private companies it's better for them all if nobody patents the way to make it safe and keeps others from using it's right that's something that's great if it's developed openly by companies to share it or in universities so that everybody can use the same best practices and raise the quality of safety everywhere all right so max actually talked to you last night about a lot like futures like really maybe a hundred years ish what's gonna happen but if you look at AI know there's not a lot people folks around today oh just image risks so if you think about how the traffic our deal acted and how those things went wrong his last few years you can't really denying that AI has contributed a lot especially in the fake deals at a is like a suggested contents so instead of like focusing on overall energy into the future so I felt there are really few people that's like looking into you today so do you think that's a problem or do you think we need to do better on that yeah I think there's a really great opportunity for us nerds in the tech community to educate the broader public and politicians about the need to really engage with this this is one of the reasons I to write this book I think when we read watch the when I watched the presidential debates for the last election for example completely aside from from the issues they talked about I thought it was just absolutely astonishing what they didn't talk about no none of them talked about at all hello they're talking about jobs they're not mentioning AI they're talking about international security and not talking about AI like the biggest technology on out there and I think in addition to just telling politicians to pay attention I think it's incredibly valuable also if a bunch of people from the tech community can actually go into government positions to add more human level intelligence in government so so the to prevent the world governments from from being asleep at the wheel I mean actually maybe we should just you can talk more afterwards but give everybody a chance to ask first hello hi when you introduce the concept when you introduce the Asilomar treaty you mentioned the difference between undirected intelligence and benevolent intelligence don't you think that if humans succeeded in creating controllable benevolent intelligence that they really have failed in creating intelligence this question you want to just repeat the punchline I'll rephrase do you think that benevolent intelligence would be the intelligence that we should strive towards or should it be general intelligence that perhaps cannot be controlled so that's a great question you asked what I what I think I am trying to be very open-minded about what we actually wanted and I wrote the book not really really avoiding saying well I think the future should be because I think this is such an important question we just need everybody's wisdom on it and and then yeah again I talk about all these different scenarios some of the which correspond to some of the different options or even listed there and I'm interesting interested to hear what other people think would actually be good with these things one one thing that may I and I found very striking when we discussed this was when I was writing the book was even though I tried quite hard to emphasize the upsides of each scenario there wasn't a single one there that I didn't have at least some the major misgivings about do you think deep neural networks will be the way to get to artificial general intelligence if not do you see fundamental reasons why these do not have the potential for recursive self-improvement that can speed up the development of AGI or super indulgence that's a great question so I think that although let me say two things about this first of all our brain seems to be of course some kind of recurrent neural network that's very very complicated and it has human level intelligence but I think it would be a mistake to think that that's the only root there I think it would also be a mistake to think assume that that's the fastest route there may I like to point out that even though finally a few years ago there was a beautiful TED talk demonstrating the first ever successful mechanical bird no that came a hundred years after the white the Wright brothers built airplanes and when I flew here yesterday you'll be very surprised to hear this but I didn't come in a mechanical bird and turn that there was a much easier a simpler way to build flying machines and I think we're gonna find exactly the same thing with human level intelligent machines the brain is just optimized for very different things than what what your machines that you build are the brain is if a Darwinian evolution is obsessed about only building things that can self-assemble who cares if your laptop can self-assemble evolution is obsessed about creating things that can self repair it would be nice if your laptop could self repair but it can't and you're still using it so and also evolution is doesn't care about simplicity for humans to understand how it works but you care a lot about that so maybe this is much more complicated and needs to be just so it can self-assemble and blah blah whatever my guess is that the first human level yeah I will not be because I working exactly like the brain that it'll be something much much simpler and maybe we'll use that to curate later to figure out how human human brains work that said the deep neural networks aren't so of course inspired by the brain and are you and are using some effort sever some very clever computational techniques that evolution came up with my guess is that the fastest route to human level I will actually use a combination of deep neural networks with ngo5 various good old-fashioned AI techniques more logic based things which have a lot of their own strength for building like for building a world model and and things like this maybe I should just add one more thing about this also this poses the increasing successes of neural networks Paul suppose is a really interesting challenge because when we put AI in charge of a more and more infrastructure in our world right it's really important that it be reliable and then robust raise your hand if your computer has ever crashed on you right that wouldn't have been so fun if it was the the machine that was controlling your self-driving car or your local nuclear power plant or your nation's nuclear power nuclear weapons system right and what so we we need to transform today's buggy and hackable computers into robust AI systems we can really trust what is trust where does trust come from it comes from understanding how things work and neural networks I think you're a double-edged sword they are very powerful but we understand them much less than the traditional software so in my group at MIT actually we're working very hard right now on a project that I call intelligible intelligence where we're trying to come up with algorithms where where you can transform neuronal networks into things which we really understand better how they work I think this is a this is a challenge that I would encourage you to all think about - how can you combine the power of neural nets with stuff that you can really understand better and therefore trust so should we be afraid that hey I will use its super intelligence to figure out that it's treatment by the humans is essentially slavery you were just extra straps question I haven't talked at all about consciousness here but the whole chapter eighth in the book is about that now a lot of people say things like well machines can never have a subjective experience and feel anything at all because to feel something you have to be made of excels or carbon atoms or whatever as a scientist I really hate this kind of carbon chauvinism I'm made of the same kind of up quarks down quarks and electrons as all the computers are I'm just minor just arranged in a slightly different way and it's obviously something about the information processing that's all that matters right so we've and people moreover this kind of self self justifying arguments have been used by people throughout history to say oh it's okay to torture slaves because they don't have souls they don't feel anything oh it's okay to torture chickens and today and giant factories because they don't feel anything you know of course we're gonna say that about our future computers too because it's convenient for us but that doesn't mean it's true and I think it's actually a really really interesting question the first to figure out what is it exactly that makes an information processing system have have the subjective experience a lot of my my colleagues whom I really respect think this is just BS this whole question this is what Daniel Dennett says I looked up at the Macmillan dictionary of psychology and it said worth reading has ever been written on but I really disagree with this and and actually let me just take one minute and explain why I think this is actually scientifically interesting question so just look at this okay and ask yourself why is it that when I show you 450 nanometer light on the left and 650 nanometer light on the right why do you subjectively experience it like this and not like this well I like this and not like this I put to you that this is a really fair game science question that we simply don't have an answer to right now there's nothing to do with wavelengths of light their neurons or anything that explains this but it's an observational fact and I would like to understand and why does it feel like anything why do we have this experience you know you might say well we know that there are three kinds of light sensors in our retina the cones and when I with a with a 450 nanometer light to activate one kind when I have that longer wavelength activate the other kind and then you can see how they're connected to various different in the back of your brain but that just sharpens the question the mystery of consciousness because this proves that have nothing to do with light at all because you can experience colors even when you're dreaming when different neurons in your brain are activated when there is no light involved right so my guess is that consciousness by which I mean subjective experience is simply the way information feels when it's being processed in certain complex ways and I think there are some equations that we will one day discover that specify what those complex ways are and once we can figure that out it'll both be very useful because we can put a consciousness detector in the emergency room and when an unresponsive patient comes in you can figure out if if they have locked-in syndrome or not and it will also enable us to answer these this really good question you asked about whether machines should also be viewed as moral entities that can look if they cannot feelings and above all and I don't see red quartz while here today but you know if he can one day upload himself into the reycarts world robot and live on for four thousands of years and he talks like gray and he looks like gray and act like well you'll feel oh that's great for Ray you know now he's immortal but but suppose suppose it turns out that that machine is just a zombie and doesn't feel like anything to be it he would be pretty bummed wouldn't he right and and if in the future life spreads throughout our cosmos in some post biological form and we're like this is so excited our descendants are doing all these great things and we can die happy if it turns out that they're all just a bunch of zombies and all that cool stuff is just a play four empty benches wouldn't that suck I'll do another question from the dory what do you think is the most effective way for individuals to embrace a promote as a security engineering mentality ie were not even one glitch is tolerable when working on AI related projects well first of all I think we have a a lot to learn from existing successes and safety engineering that's why I started by showing the moon mission this it's not like that this is anything new to engineers I think it's just that we're so used to the idea that AI didn't work that we didn't need to worry about the impacts of things you know and now it is beginning to have an impact so we should think it through and then there are also a few challenges which are unique in specific to AI some of the Asilomar principles talk about them and and this research agenda for AI safety research is a really long list of specific safety engineering challenges that we need smart people like you to work on and I hope we can support that so also on the topic of security engineering a lot of rockets blew up on the way to the moon yeah and you know given the intelligence explosion it's likely we're only have one chance field would it get the alignment problem correct and you know I think we couldn't even align on a set of values in this room let alone a system that would govern the world effectively because you know there's certainly some tropics of capitalism so I'm hopeful I'm glad that Elon is hedging our bets by making a magic hat but it seems like you know you and your group are focusing on the alignment problem and I'm just you know kind of just curious like you know what makes you optimistic that we're gonna be able to get it right on the first time first of all yeah but you will note that most of the rockets that blew up in fact all or not that's the rockets that blew up in the moon mission had no people in them right so that was safety engineering the high-risk stuff they did it in a controlled environment where the failure didn't matter so much so if you've made some really advanced AI to understand and really well maybe don't connect it to the internet the first time right so the down so the downsides are small there's things like this that you can do and I'm not saying that there's one thing that we should particularly focus on either I think the community has brainstormed up a really nice long list of things and we should really try to work on them all and we'll figure out some more challenges along the way but that's the main thing we need to do is just you know I just yeah this is valuable let's let's work on it then you asked also why I'm optimistic let me just clarify there are two kinds of optimism there's naive optimism like my optimism that the Sun is gonna rise over Mountain View tomorrow morning regardless of what we do that's not the kind of optimism I feel about the future of technology then there's the kind of optimism but you're optimistic that this can go well if we really really plan and work for it that's the kind of optimism I feel here we have in our hands to create an awesome future but let's so let's roll up our sleeves and do it hey max in the paper entitled why does cheap and deep learning work so well with Lynn and now Roenick as well you asked a key question and you you draw a lot of connections between you know a deep learning and then core parts of what we know about physics low polynomial order my uncle processes things like that and just curious what are the reactions you'll receive both from the physics community and then from the AI community to that attempt to kind of draw some deep parallels generally quite positive feedback and then also people who have pointed out a lot of additional and research questions related to that which are really worth doing and just to bring everybody up to speed as the what we're talking about so we don't devolve into this disgusting in there the research paper here we were very intrigued by the question of why deep learning works so well because if you think about it naively even if I just want to classify I take all the Google Images and I want to have cats and dogs and I want to write it and I want to take your neural network that will take in say 1 million soles and output the probability that it's a cat right if you think about it just a little bit you might convince yourself that it's impossible because how many such images are there even if they're just black and white images so each pixel can be black or white there's two to the power of 1 million possible images which is much more images than there are atoms in our universe there's only 10 to the 78 right and for each image you have to output a probability so to specify an arbitrary function of images how many parameters do you need for that goal to to the 1000 which you can't even fit if you store one parameter on each atom and in our cosmos so like how can it work so well it will be the basics for inclusion we found there was was that of course the class of all functions that you can do well with a neural network that you can actually run is it almost infinitesimally tiny fraction of all functions but then physics tells us that the fraction of all functions that we actually care about because they're relevant to our world is also almost infinitely small fraction and conveniently they're almost the same I don't think this was luck I think Darwinian evolution gave us this particular kind of neural network based computer precisely because it's really well tuned for tapping into the kind of computational needs that our cosmos has dished out to us and I'm I'll be delighted to chat more with you later about about loose ends to this because I think there's a lot more interesting stuff to be done on that take a Dory question being humans are they in the age of AI seems like an egocentric effort that gives an undeserved special status to our species why should we even bother to remain humans when we could get to push our boundaries and see where we get all right I'm egocentric effort gives us undeserved special status to our species but first of all you know I'm totally fine with pushing our boundaries and I've been advocating first doing this I mean I find that very annoying human hubris when we go in a stop box and we're like well you're the pinnacle of creation and then nothing can ever be smarter than us and we try and also build our whole self-worth on somehow human exceptionalism I think that's kind of lame on the on the other hand and we should probably make this the last questions and on the other hand the eccentric efforts well we are the only ones it's only us humans are in this conversation right now so and somebody needs to have it so it's really nice to us up to us to talk about it right we can't use this kind of thinking as an excuse to just not talk about it and just bumble into some completely uncontrolled future I didn't think we should take the firm grip on the rudder and steer in whatever direction we decide to steer in so let me thank you again so much for coming out it's a wonderful pleasure to be here and if you had any more questions you didn't get in I'll be here signing books and I'm happy to chat more thank you all for coming and thanks to max for talking at Google and thank you for having me you 