 hi I'm Joey Ito I'm the director of the MIT Media Lab and I'm Jonathan Citroen and director of the berkland Kline Center for Internet in society at Harvard University and so we're gonna talk for about an hour now I'm just gonna but we we were in the program but III really wanted to sort of introduce this because we have been working in this space of AI and ethics and last year we actually tried to do a conference but then we looked around and we saw well you know I'm not afraid of the curt's Wiley and singularity as much as the singularity where there's an AI conference every day you know and so so one of the things we said well let's refactor and we thought the a now conference look like the most interesting conference that happened last year unfortunately we weren't invited so we decided if we hosted it maybe we could get invited so that was sort of my idea which turned out not to be true so we'll be leaving as soon as our introduction is over and we are part of a new venture called the ethics and governance in artificial intelligence fund and I thought we'd say a little bit about that as well I don't want to say any about the origins Joey yeah we will have a session about that later but basically it was Reed Hoffman and the knight Foundation the Hewlett Foundation and Pierre Omidyar and Jim pelota got together and we were we and Kidd Meredith have been really helping us think about this was that the funding of this space was just sort of non-existent and the idea was that the philanthropists would try to get together to fund work that crossed over between the disciplines and increase the diversity and convening was one of the key things that we thought was important so Kate and Meredith have been super helpful with that and there are three zones that are opening zones of focus for the fund which include autonomous vehicles criminal justice risk scores for bail probation parole and information quality / fake news but we can't call it fake news so those are sort of the three areas and when we think about and I think about the title AI now it calls to mind I think as we're thinking about the space for AI we mean it writ large we mean a kalokhe really not as a term of art because if you get to AI people in a room to talk about what is AI there will be three views one of which they will have programmed and that is not what we're wondering what's on what boundary of what I think at least for me it's more of a mouthful but we're thinking about tightly coupled autonomous systems that evolve over time and therefore can surprise even their makers and for the now part to me now means now is stuff happening now that's going to require some thoughtful interventions including as with the founding and development of the internet itself thinking about what belongs public what belongs private at what point is the public interest implicated enough that what two people are doing in a garage should have maybe a third person along for the ride as soon as it starts to take off and that also implicates the question of how easy is it to do AI to build it is it something that requires a bunch of PhDs and a lot of resources kind of a Microsoft Google thing or is it something Allah the the mythology of the Internet that anybody in a room can do as they please and that's different from some of the AI later questions that involve screenplays for the Terminator and stuff like that there's plenty on our plates right now and when I say our the brain trust gathered in this room is incredible and I know I'm really excited I know you are too Joey at what we're gonna learn with this kind of gathering both in the formal sessions and just having chats outside I hope there's a designated survivor if anything should goes wrong because this is the group that could help figure out what to do and when I think of the apex of that group or the the firsts among equals reminded actually of Ralph Nader who became something bigger than any one company or movement or institution he's just Ralph Nader and was the guy who while the title of his path-breaking paper was unsafe at any speed he wasn't anti car he was just anti death car and I think that spirit is a lot of what infuses a lot of the work here and I think of Kate and Meredith as kind of the Ralph Nader's of this bigger than any one institution even then Google or Microsoft respectively and looking so much forward to the leadership they're gonna continue to bring as we sort through what are genuinely incredibly difficult problems if they weren't that difficult we wouldn't need the lanyards that we're looking to make progress on and the fact that I suspect none of us has the answer yet is a wonderful exciting challenge rather than a depressing let's just go home yes so with that pre thanks thank you and off with the show with Kate Meredith thank you guys when we have a microphone thank you so much Jonathan and Joey for that incredibly generous and I think somewhat illuminating introduction I never thought of us as the Ralph Nader's as researchers we tend to go way nerdier than that but but we'll take it first of all a giant welcome to you my name is Kate Crawford this is Meredith Whittaker we will be your co-chairs for today and I want to know who here is feeling very excited about this discussion put up your hand because I know a lot of you have flown a long way to be here and we're incredibly excited to have you if you look around you'll actually notice that it is an incredibly diverse room today that is by design it's something that we think that the AI research field needs a lot more of and you're here specifically our thank you you are here because your work has inspired us that's why you've been invited you've been doing this work for years there are a lot of people who have been thinking about the social impacts of artificial intelligence and it is our pleasure to bring you together for these second annual AI now experts workshop so when we think about artificial intelligence and you've absolutely heard what we would agree with which is that there is no fixed definition we have to talk about a multitude of things we need to talk about technical modalities like machine learning neural nets natural language processing we also need to talk about the industry itself which is highly concentrated with around seven players in the world who are really doing AI at scale depending how you count we also need to think about the diverse communities who are being affected very differently by these technologies already and that is happening now not in some science fiction of future and we also need to think about the way that AI is already proliferating through criminal justice health care education access to credit and hiring and so these are some of the sorts of topics we're going to be dealing with today and in order to understand the full picture we're going to be calling on all of you we're also going to need people with very strong technical domain knowledge and if you look you'll see a lot of those in the room today but we also urgently need other perspectives we need historians to let us know how we got here we need legal thinkers to help us understand the potential impacts on the justice system and we're also going to need ethnographers and economists to map the changes to labor workplaces and well-being and policymakers and members of civil society to help turn that research into action we also need to expand the definition of AI research to include all of these types of practices and disciplines but let's be clear AI isn't everything and the marketing hype will try to tell you that and we're really going to push back on that today we want to keep a very critical lens around exactly where AI should be and it's not going to be everywhere we have to think about where it's going to be best applied and where it shouldn't be applied so that's about enough for me I'm gonna throw now to my co-chair Meredith Whitaker for you Thank You Kate MS nadir please so a year ago many of you were here for the first a I know experts workshop and back then we didn't know this would be an annual event but the discussion in that room was so vibrant and it continued on after the event and of course we realized there was so much work to be done that we decided hey let's make it yearly so we're here again for the second annual AI now experts workshop because we want to nurture this work because we want to build and strengthen the community today we are going to focus on four core domains and these are those that were mapped out in the AI now 2017 draft report that you all received before the event to refresh these rai in relation to rights and liberties issues of bias and inclusion labor and automation and ethics and governance this was report but this report was intended to ground you all in these topics and you'll notice many of your work was cited in there but now that we're in the room we want to kind of build on that discussion we want to hear from you you know how are these issues touching your domain what are what are the next steps that this field needs to do to address these tough questions and how do we get there together so before we start in on that just a quick grounding in our schedule you have an agenda in front of you or in the bag you were given and here's a rough outline of the day so the day is divided into roughly two parts there is the the in the morning we have lightning talks from a number of experts across these four domains and that's going to be kind of the input section if you will we will then break for lunch and that's when the people on the livestream are going to be saying goodbye and we will return for a closed-door candid group discussion where we really want to talk about what are the next steps from your perspective you know concretely to begin to address these problems these challenges will wrap up at around 4:00 and you will have time outside with some refreshments to meet people to collaborate on projects to scheme for the next steps and then we ask you to be back here at 5:30 for the expert for these for the symposium the public symposium where you all have VIP seats following which we really want you to join us for the VIP cocktail reception which will happen right outside where we're having lunch so a quick note on guidelines and logistics if you walk out the door go down the hall on your right that's where the bathrooms are and if you have any questions on you know how things are working if you need any help a phone charger what have you Emily here is handling all logistics so here is Emily that's Emily her she is our valiant helper on all things great and so that's it for me I'm gonna turn it to Kate for a few guidelines and then we're gonna get started beautiful so before we kick off a quick request to you we're gonna ask you to do two things today the first is during each of these blocks of lightning talks you're going to be hearing very different disciplinary perspectives but we'd like you to listen for the threads that join them together and possibly also the gaps where are the things that that should be here that are not here yet so if you could sort of think in those sort of syncretic ways as we go through these lightning talks that would be fantastic we would love to hear that from you secondly during the group discussion in the afternoon we're gonna be asking you to provide essentially very concrete steps forward that's in terms of research agendas that's in terms of things that industry can do what are the concrete actions the industry should be taking and then thirdly what are some productive advocacy and activism paths that could address some of these issues as well so there the top three for you to be thinking about in the afternoon session now if you have a look we've also given you a bunch of posters and pens this is here as a sort of aid memoir because you're going to be hearing a lot of interesting very new research as you have questions and ideas use that jot them down but also write clearly because we're gonna be collecting them at the end of a break and at lunch and that's going to help also guide that group discussion this afternoon so know that those pieces of paper are very important but the occasional doodle is also happily appreciated so feel free to draw as well that is basically enough from me what we're gonna be doing now is kicking off a very first block of lightning talks this is an area which has become incredibly populated just in the last year a lot of very exciting papers looking specifically at the problematic area of bias and machine learning and inclusion so to kick that off a very first speaker is going to be joy bull and Winnie from the MIT Media Lab joy over to you [Applause] good morning good morning I'm joy put of code and the founder of the algorithmic justice league today I'm going to present the undersampled majority and so my path into AI starts with computer vision with the look at facial detection and recognition software I've had the experience of coding in a white mask or barring somebody else's lighter face to be detected and so that led my exploration into why and so looking at where we've come with facial recognition I go back to 2014 where face book release deep face which increased the state of the art by 27 percentage points we were really excited and we tracked this using the lfw benchmark but my question was how representative is this benchmark and are we getting a false sense of progress based on skis within these benchmarks so researchers in 2014 looked at the breakdown of lfw labeled faces in the wild and found that it was 77% male and eighty three and a half percent white and stop thinking huh maybe this is why my face wasn't being detected so well and we know that with data-centric technology data is destiny so the question is how fair is our fate so then I looked at the latest government benchmarks released in 2015 and these benchmarks were described as being the most geographically diverse so I thought why not check when I took a look at the benchmarks I look not just that gender but also phenotypic representation and so I labeled the faces using the Fitzpatrick type with six points lighter skin two darker skin and here's the breakdown as far as gender parity were close to where we were back in 2007 right still at 75% male and then when you look at the representation Fino typically we're at 80% her skin I decided to take it one step further and do an intersectional breakdown which I hadn't really seen much of in any of the papers I'd encountered and though what reason I wanted to do the intersectional breakdown is we don't have this aggregated metrics when we look at the performance we get ninety seven and a half percent right doing well on lfw but what does that really mean for subgroup populations when it comes to the government benchmark this is what I saw when I did the intersectional breakdown you see that the majority of this government data set that's used to benchmark for the industry of sixty percent lighter males and only 4.4 percent dark your females and when you consider people of color make up 27% of the u.s. population so you would expect at least 13 if we're doing it just based on representation this is not okay so our state is fair when it comes to pigmentation but this definitely is not where we need to stay we can't have data sets that are mainly male and pale and when we talk about diversity increasing gender diversity is great but we also need to think about phenotypic diversity as well moving forward and one way to do this is to think about full spectrum inclusion so this is the Jablonski map which shows the skin type representation around the world and we can see it is not 80 percent pale and of course the world is roughly 50/50 so this is part of the work of the algorithmic justice league looking at how we can create more inclusive benchmarks so we have a realistic view of where we are because if we have data that's largely male and pale we're going to fail at being inclusive and intersectional moving forward thank you thank you now we have Mart's hot who will be talking up next thanks Mart's all right thanks so much for inviting me this is by far the most well organized workshop I've ever seen certainly much better than the ones I've organized so I want to talk about biases beyond observation and the starting point for my talk is the insight that really most fairness criteria that people have discussed in recent years are what I call observational and what is an example of an observational criterion it's something like the fraction of accepted men should be equal to the fraction of accepted women okay so the rate of acceptance by some classifier should be the same and in these two groups okay that's observationally which means you can write it down as a probability that involves only like the sensitive attribute and the predictor and some other stuff and really this is most of the things we discussed for example the calm past debate that was ongoing last year and it's still going on to some extent everything that was set there is essentially an observational property okay things like false positive rates by different groups things like precision recall accuracy if you look at all these classification measures by group or whatever you want that's an observational criterion because you can write it as a probability involving all these different things okay so really most of what we've discussed in fairness to to this day is observational okay so you can put that in a nice definition which is just an observational criterion is a property of the Joint Distribution of the features in your classification setting the predictor and the outcome so really anything you can write down as a probability so you might say well geez isn't that everything and so let me tell you why it's not and why it's actually missing important things so the issue with observational is that it's completely passive okay so it's completely passive inference from what we observe and the way the world is right now at this point in time and the observations we can make about the existing world but there there is no form of intervention in this kind of analysis there is no what-if consideration what if I changed this what would happen you know how would we change the world based on these interventions it's purely passive and that's why it's such a limited approach and so with my collaborators Eric Eric price and Nancy Schreiber we turned this into silver of theorem by constructing two different worlds two different scenarios which mapped to the same joint distribution over features outcome and predictor but have completely different interpretations for fairness okay so they're completely different worlds from the perspective of fairness in how we should reason about these worlds but they met two identical joint distributions and that's one way of showing that no observational criterion in the world is going to tell them apart no matter how sophisticated your your probability statement that you're going to make it's not gonna tell these two worlds apart okay it's just not powerful and what's even more troubling is the following simple thing we show it's that observational criteria can't even tell you if you should be happy with the optimal predictor so if you're in a setting and you ask well you know it's optimal prediction fine would we be happy with a perfectly accurate predictor in this setting observational criteria can't even tell you if that the answer is yes or not yes or no because in the two worlds we construct in one case the perfect predictor is something most people would be fine with and in the other case it's not okay so we're not the first to stumble into this issue of observational criteria people have you know struggled with this for a long time and it is what motivated you know causal reasoning and today a prose book for example on causal reasoning is all about how do you go beyond observational criteria okay so people have studied this in other contexts what's tricky in fairness is that we can't just design a randomized trial based on you know race or gender that's not feasible so we need to instead understand the data and the generating process of the data and make reliable assumptions about how the data was created and generated and we have those assumptions then we can start to go beyond the limits of observational and we can you know have more meaningful criteria but of course that's a delicate approach and we started working that out a little bit in a recent paper with folks from the Max Planck Institute and tubing and but there is much to be done here and it's it's it's not easy but at the very least one thing that I advocate for the community as a step forward is let's stop to you know try to resolve which fairness criteria and everybody should use the sort of one definition that we should all agree to and let's instead understand the mechanical or the momentum the mechanisms behind the data and how its generated let's understand how the data was produced from the point of you know measurement collection to sampling you know how did we get to the data that we're looking at and if we understand the mechanisms behind the data generating process we can start to make progress Thanks thank you so much smarts for that I think really important provocation for the day next up we have Solon brokers from Cornell University and I think I'll hopefully build quite nicely on the previous two talks so there is in fact a considerable amount of work that's been going on for the past few years all I'm developing fare machine or any techniques engine but I'll try to talk about today is what problem are these actually solving so clearly you very heard that standard problem here might be that the models we build are not accurate maybe there are sort of haphazardly not accurate maybe they just don't do a very good job and maybe any error is seen as being problematic but really what often tends to be the case when it's been the focus a lot of workers these disparities in the accuracy rates along the lines of things like gender or race right and so maybe that's what we want to attend to we want to make sure that there's equal accuracy across groups but what we learn actually from the work on on the compass score is that it's not maybe enough to actually focus on accuracy maybe you have to go a bit further and focus on the difference and the types of errors that are made when when there is a mistake is that a type 1 error meaning a false positive or is it a false negative and how do those differ by group especially when the consequences of the errors differ but then we also might talk about bias training data as we kind of heard earlier this morning already where the problem here is that it's not just the accuracy rates might differ but that the evaluation techniques we use don't even tell you that you're going to do badly we're actually I think in a situation like this oops yep sorry or really what we're concerned with is unrecognized errors so not just at the accuracy rate my difference might differ but that we have no way even knowing what the real accuracy rate of these things will be in practice and similarly we can imagine a situation where it's not just that there's a bias in the way we collect the data but that the data actually encodes past discrimination so let's imagine we want to use machine learning for hiring decisions we use previous employee performance to figure out who in the future would be a good person to hire and let's imagine that we're using let's say their annual review at the end you know to train this model who gets a high annual review score well let's imagine this first case that the annual review process is somehow influenced by either conscious prejudice or implicit bias so that these scores don't reflect the actual performance of employees but rather the kind of judgment made by humans which then becomes the training data for these models and so you could imagine that again in a way this is a problem with unrecognized errors where the machine is going to predict that this person won't perform prayer terribly well but the reason for that is the score the bit that we've trained them on it's actually misrepresenting what passed people were actually able to do but let me complicate the story a little bit and imagine a slightly different sense of situation where the workplace that these people might actually spend their days can be hostile can be very unwelcoming to certain people can make them feel like not they're not part of a team or they're not giving them more desirable jobs such that the actual objective measure of the performance might be a faithful reflection of how they did under those circumstances but it would be hard to argue that building a model using that data would be fair because in a way it's suggesting that that person is responsible for not performing particularly well and so what we might want to consider is how do you actually make accurate predictions when the circumstances themselves aren't fair actually along the lines that think about Morris was saying so this all kind of leads to I think a focus on questions about these background conditions what that what the status quo is that really we shouldn't take something that is not available for us to change right so these predictions don't actually consider what we can do about the rest of the environment to make those predictions actually no longer valid that are actually probably the more desirable point of intervention and if we begin to think that way if we begin to say that it's not appropriate to make predictions based on the experience of people working in a hostile work environment you might also want to ask about the longer-term consequences of being subject to unfair disadvantage so the point at which people arrive at some important decision being made by machine learning they've already been subject to all sorts of injustice such that by just looking at them at that moment in time it may seem entirely fair to make a decision about them on that criteria and the prediction might even be accurate but it would be in the in the could be under conditions where you've accepted as a given the kind of cumulative disadvantage and injustice this person has suffered often to that point and so what I would just want to say is that I think there's a really important problem here that we want to make sure that yes we are predicting accurately but we also want to ask whether these tools and techniques are a mechanism to remedy past injustice more broadly and I think in approaching both these problems there are dangers so one certainly is that if we're going to use fair machine learning as a way to rectify historical injustice there's an implicit sort of property here that it can be a kind of mechanism for affirmative action and to do this under the cloak of something that's for a broadly called fair machine or name I think runs some serious risk especially when it becomes exposed as being something along these lines and similarly I think there's a real danger here of a sort of certifying decision-making as being fair when the conditions under which those decisions are being made remain deeply unfair and that we don't want to use machine learning just as a way to continue the status quo so thanks very much fantastic thank you so much Solon I think we're starting to see the threads already emerged in this group for our final talk we have Simone Brown from the University of Texas at Austin thanks so much good morning um so a quick google there we go a quick google image search of the term artificial intelligence we come up with photos of humanoids like this iRobot looking images of a white plastic faces that seemingly stare out into the distance it's nowhere and but these representational practices are important for how artificial intelligence is marketed and imagined and sold to the public but in the 1930s a curious link was made between robots an electrical company named Westinghouse minstrel imagery and the history of black people's labor in the United States designed by Thomas Kentner a sorry Phillip Thomas and Samuel Kitner West buy from Westinghouse Research Laboratories this robot sometimes called mr. Rastus robot sometimes called the mechanical slave and sometimes called the mechanical negro had dark rubber skin was dressed in denim overalls and wore a checked bandana around his necks an aesthetic gesture perhaps to the southern working-class African American farmers as both raw denim and checked cotton have a history of being labeled as Negro cloth or an apparel of enslaved people who were required to wear nothing finer Negro cloth laws in this way sought to establish a sartorial link between those that can labor on freely and those that can wear finery so mr. Estes the mechanical Negro was the black robot that was introduced around the time of the Great Depression invoked perhaps to soothe concerns around robots replacing human workers but unlike Westinghouse's other robots of the time this robot was a symbol of black servitude and the racial meanings attached to this mechanical slave as Lewis so keynotes in his book the sound of culture was perhaps to lessen the fear of the uncanny that these technological technologies represented so when this realistic mechanical Negro first appeared at the National Electric Light Association conference in San Francisco in 1930 it was kind of as a sideshow it was part of a Westinghouse as robots as a marketing strategy for its various household technologies and other innovations Rastas can bow say a few pre-recorded words and he had one major trick he was made to adapt the legend of William Tell his inventor would aim an arrow at an apple that was sat upon his head and a light that was on at the end of the arrow would activate a photoelectric cell in mr. Asus's eye and the Apple would explode it was quite a violent display so the name Rastus is part of this roster of depictions of a blackness that are often exaggerated representation that took the shape of figurines kitchen utensils postcards lawn ornaments and that also adorned the boxes of the instant food industry and these exaggerated representational practices work to rationalize the eken economic exploitation of black domestic workers as well as those who labored in low-paying conditions in the service sector so Rastus the mechanical slave is one story an old story but one of many that can get us to think about what the afterlife of slavery can teach us about artificial intelligence and our inhumane presence I'm sorry I meant to see in humans there is seemingly a certain historical continuity when it comes to what data becomes training data and who is in the room overseeing that's that training for example with this case that I'm sure that you all know about from 2015 so two points to wrap up first we must contend with our historical presence and second these technologies are not infallible for example the case of a pattern and object recognition or determination with the case of whistleblower Reynaldo Chavez of the Albuquerque PD who testified in a sworn affidavit for a case that there are backdoors that these patterns and objects can be manipulated heeeh testified that using axioms evidence.com a storage of police camera footage that he was able to I guess changed perhaps a pen into a knife or basically Photoshop items that come from a police camera footage and so I would just leave with this question to close what are our alternatives when these pull when these possibilities are there and that we have to contend with our historical present thank you thank you so much Simone please come and join us here on the the stools all the speakers you're invited to come back now for Q&A we have joy Mart's Solon Simone just little spots for you here and Thank You Simone for ending on such an important historical note and I hope you are now enjoying this combination of technical and sociological and historical perspectives on bias and I think it also really helps us start to question some of the somewhat more simple perspectives that bias is the thing that can just be removed or neutralized in data sets and machine learning systems it's got a lot more complex than that and there's a very deep history for us to contend with so now we want to open the floor to you this is your chance for questions great to see that Cathy's got one right off the bat we have Mike runners who are going to bring you mics so as you think of them please raise your hands and we'll have people come to you first up we have Kathy Inoue hi wonderful guys thank you so much I really appreciate it I'm really fascinated by your your discussion more--it's about how we don't we don't have enough information or if we just look at probability distributions and I this is something that has come to me in a sort of back backdoor way which is that I feel like instead of talking about probability distribution we should talk about the use case the moral use cases and I'm wondering if that is the lens through and also the others the side effect of that is it's very understandable to people like once you explain here's the moral issue in this context here's the moral if you have like an encyclopedia of like a hundred different use cases I feel like maybe that's the way to do it what do you think yes it's on okay thanks for the question it's something I would you know certainly think about as well and I think tying fairness to use cases is very important I think we should probably move from this kind of abstract discussion of fairness properties to understanding the tasks that we're trying to solve and sort of what our objective is and what our goals are and so I think that ties into it but I think it's not the only issue we also need to understand better where the data came from and what what sort of how it was measured how it was generated sampled and so on and what interventions we might be able to make to change the data that we have and good yeah if you make that part of the use case great thank you we have a question here from Alvin Narayan and up the back Solan made the point that sometimes what we call fair machine learning can be seen as not acknowledged affirmative action and I found that very interesting very provocative had never thought about that before and I was wondering if you could elaborate on that a little bit in a couple of ways do you see this as an occasional problem or as much of what we're talking about fall under this umbrella are there any examples you can give just a little bit more intuition on that yeah thanks Arvind I mean I think what I was trying to suggest is that many of the problems that fairness and machine learning has tried to address or ultimately reducible to questions of accuracy and they're going to be circumstance as though when that's not the case and so if the reason for instance the data reflects that certain people perform better on the job than others is that they were actually subject to discriminatory treatment in the workplace itself sort of what is the data generating mechanism in that case we might want to say that that's not an appropriate basis to then assess future people about their performance even if it might mean that those circumstances they would go on to perform less well than others so I think the idea then is if we compensate for that in some way by saying we should actually imagine how they would perform under better circumstances under more acceptable or desirable circumstances one thing that might happen is something along the lines of you know providing some kind of boost for people who otherwise would not be assessed as favorably giving current circumstances thank you have a question here at the back oh yeah thanks for your great talks um there was a bit of a theme of biased datasets and I really like the way I'm joy put it as skewed skewed datasets I thought that was very good wording um I'm wondering I guess for all of you does an unbiased data set exist or is there some kind of approach to collecting data sets that would handle removing the skew in some way or is that sort of a fake ideal and curious what you guys think as far as an unbiased data set that I have not come across and as long as human hands are involved in curating our data there will be some type of bias and maybe that biases to positive effect depending on the context in which you're using it I think what we need to do is move away from bias eradication to bias awareness and again contextual understanding in terms of how the data is being used so in the case of benchmarks we shouldn't be in a situation where we look at a single metric and delude ourselves into thinking we've made progress when you look back at the data and the skis themselves are not representative of target populations if you're looking at a national set or internationally that's why I talk about the undersampled majority which is not represented in this room I just add very briefly I think that's a very important point I want underline that you make that we should move away from bias eradication to awareness and an example I'd like to give is is genomic data or DNA it's it's being really hard to remove ancestry information or a sort of correlation with race for the DNA and so I think we should first understand what are we going to do with the data and what is the task we're trying to solve and then we can it's easier to talk about this issue we have a question here from Rob's fara and then over here to termed it so I guess I'm worried that the emphasis on fairness is accuracy misses the element of fairness as process and the ways in which the process might itself shape outcomes if you imagine a workplace review process for instance the dialogue that might occur in those cases itself can shape their outcomes in interesting ways I guess I'm worried about the framing here as you know taking a data set and producing an outcome and the only criteria is accuracy I mean are there ways of representing that sort of that question about the nature of the process itself as socially transformative in question this problematic I'd love to hear what Simone and so LAN have to say about that yeah I mean I certainly take the point I mean but I guess I would say is that so much of existing law and policy is focused on process and what is actually somewhat interesting and different about some of this recent work is that it's given us a way to think a bit more than then about process itself however I think what I was trying to suggest is along the lines of what you are also saying which is that if everything is reduced ultimately to a problem of accuracy we will not get at a lot of the much more fundamental institutional problems that we want to address and that those actually require intervention into the process into procedure and so we really do want to think about what other things we can put in place besides some technical mechanism at the point of decision-making that will actually really transform how these institutions function I think for me a question is more about the right of refusal for these you know facial recognition technologies or so so who are the communities that these technologies are often tested on first people who can't refuse often people crossing borders people who are claiming refugee status us students prisoners people who are on social assistance and so those are often that the testing grounds of these technologies so rather than think about how can we make these less biased or or quote-unquote unbiased it's thinking about the social conditions of where these technologies are applied and what are the technologies the kind of tactical resistance that we had maybe a politics of resistance that we can also talk about rather than you know making a better machine thank you Tim minute over here and then a question here from Heather wolf thanks for the great talks I had a question for Moritz I kind of agree with Cathy where she said that we should take the use case into account because for me I was actually thinking about this research question I do think we should model the data for sure like the underlying process of the data but what I would want in my head is some knob to say if you know I have external knowledge about how this you know data I could have been biased for example in the criminal justice case would be extreme policing you know in certain communities and I would want to know how that would affect my outcome in some way right so I wouldn't assume that I know exactly how the data was generated there's some uncertainty and they're like probably with generating this way or that way then I'd want a knob to adjust it and then another one was just a comment that we're talking about machine learning algorithms outputs being accuracy but I think right now in the AG community we're also working on a lot of other stuff like we're trying to make machines that reason we're working on visual question answering and I think in that case this question of fairness becomes even more complicated and more important so I think the accuracy case is probably like the simplest possible case we can think about but then in the future I can think of all sorts of other cases that are more complicated yeah so your first point is something that is excellent and I completely agree with so I don't have anything to add to that I think we should totally do that the second point I also strongly agree with and I think people are trying to apply a machine learning to a lot of scenarios where we don't have a good measure of accuracy even or we don't have or a criteria of what's a good solution our very subjective and that's that's a no whole nother can of worms where it's much harder to say anything reasonable but in some sense they're the failure point is even earlier when we use machine learning and we should maybe not use machine learning for all things like beauty contests or something that's not just not something we should be doing and so machine learning and I think we should start with very well-defined settings where we have a lot of control on what we want to measure excellent for a final question Heather off I think it segues in quite well so when we're thinking about you know data and so you mentioned a beauty contest rate this is a this is a social political cultural thing and so you know in the social sciences I'm gonna make a plug for the social sciences and the humanities here right I think one of the things that we do at least as political scientists is we do we attempt to do random sampling of populations to ensure that you are getting a representative sample and from what I've heard in this room and what I continue to hear when we talk about data generation and where we get data and how I get data is there's no random sampling here right it's it's so beyond random so one of the things about the skewed data is maybe part of the fairness solution is actually to rescue the data towards randomness rather than looking at okay so I'm getting data from policing and refugees and that population looks significantly different than the rest of the population of the world and so maybe what I need to do is intervene to rescue towards random so maybe that's a way towards fairness I'll Rob's point but again even that assumes a particular viewpoint of fairness that is itself culturally biased so I would probably kind of want to throw that Western and a fair thing happening in the in the discussion a great provocation final comments from the group before we move on to our next set I think the point on putting in a skew is interesting in terms of what we do for intentionality when it comes to curation acknowledging that the majority of our datasets we have right now are based on convenient sampling and so that that's something I continue to explore and another part I def we want to focus on is when it comes to phenotype this is something that I haven't seen much research on at least when you're looking within automated facial analysis when we only look at demographics we forget about the intra class variations so I'll leave it out there at that but that's something I've been thinking about this was a fantastic set of talks that spoke very closely to each other thank you so much everyone on the bias panel will ask you to take your seats now and we are gonna now have our next group which is going to be on labor and automation a huge round of applause for our speakers Thank You Meredith great great so now we have researchers and policymakers who are going to examine issues of Labor jobs AI and our systems of resource distributions so remember to keep writing your questions down on those post-its we're gonna collect those at the break-in at lunch and I want to give a warm welcome to Andy Stern so I think if there's one lesson that I have learned in my life it's whoops it's this go back it is that change is inevitable it's actually progress that's optional and leadership which is why we're all here I think can really make a difference right now and so let me explain I'll start with my perspective on AI and technological change and jobs and work which comes out of a long journey I took for three years trying to understand the future of work and I came away with this very simple perspective that there is the potential and the emphasis is potential on the greatest disruption of jobs in the history of the world and that's not just an overstatement or my thinking I think that if you kind of look at all the reputable research we now see coming out and you can add to this Oxford and McKinsey and many of the other technological leaders of our country and business leaders in academia I think it's becoming quite apparent that something big is going on in terms of jobs and work as we move forward and when they did a poll of 1896 pew experts I thought what was interesting is that half of the experts which is a obviously a very large number thought that there's a possibility of masses of people who are effectively unemployable and actually breakdowns in the social order and when I met with Andy Grove along this journey I mean this is the thing that he said to me that I think is really important for today's thinking is that we are I think at a strategic inflection point these are moments of significant change they often appear slowly not clear until events are in viewed in retrospect and denial which i think is what's happening about jobs and work is present and that is I think the status quo as we exist now at least amongst our political leadership so what do we do so I draw my wisdom from a great American philosopher Groucho Marx who says well money can't buy happiness it certainly lets you choose your own form of misery and from a great moral leader Martin Luther King who talked about the simplest approach to ending poverty and I would say economic stability is actually a universal basic income or a guaranteed income which is actually what I've spent my time thinking about giving every single adult American $1,000 a month allowing them to make their own choices providing economic stability for them and their family and allowing people to refigure out what are their needs purposes and opportunities in their life now I appreciate there are a million questions about it people ask about how much does it cost then is it politically feasible and who should actually get it and should you have to work and what does it mean about purpose and meaning in life which is why a number of us have created the Economic Security project which is trying to do what AI now is doing in this space to really experiment with the idea of cash transfers to do the necessary research as one of many scenarios because I think the most important thing we need to understand and is that we don't need to have a debate about the future to figure out who's right or wrong anymore than when Kim jong-un says he's gonna launch a weapon we don't debate will he or won't he and what will Donald Trump do but with rigor and intelligence our military and academic people in the security industry figure out scenarios and universal basic income to me is a scenario that we need to do and for all of our highfalutin thinking about strategy and how we can make these change I sort of end with what Milton Friedman said that only a crisis produces real change and what happens in a crisis that people that have good well-developed ideas lying around often find themselves in the center of the ability to make change and I think this is a moment where we need those well-developed ideas of change I think universal basic income is that right idea and as Winston Churchill said about democracy you know ubi is a terrible idea until you try everything else and what's your idea Thanks thank you Andy and now I'd like to introduce to the stage Karen Levy from Cornell University okay so there's a lot of a Xindi highlighter there's a bunch of well-founded concern lately about the economic impact of potentially massive job loss based on AI a lot of people are worried about the possibility that millions of workers will find themselves without the means to provide from themselves in the way that they traditionally have and there's a lot of a kind of mess really concerned about what we're gonna do about that there's a lot of concern as well about what sectors are most likely to be disrupted first and a lot of people agree that a prime target for a really significant economic disruption is long-haul trucking or a bunch of reasons for this some of which have to do with kind of the technical capacities around like the huge innovation and autonomous vehicles autonomous vehicles actually lend themselves really well to this style of driving that the trucking entails and then there's other factors too having to do with the nature of trucking work truckers because of the way the industry is organized now truck drivers get really tired they're really overworked and so they cause a lot of accidents on the road which are expensive and deadly and so there's a lot of interest kind of from the labor perspective to in finding ways to take these guys off the road and put robots in the cab so this is like a really big concern and a lot of the rhetoric is about this displacement right like there are a lot of people really concerned about the fact that there are two million truck drivers in the United States many of you may have seen this graphic from NPR a couple years ago you ain't using Bureau of Labor Statistics data to show the most common job in every state so you can see that those aqua states the most common job in 2014 was truck driver and so people are worried about like what does this mean right if there's massive job loss in this industry now I think I've been studying technology and regulation in truck driving for about six years now and I think this captures certainly a large part of the threat of automation to this sector but what I want to talk about today is actually a different threat which I think gets a lot less play um and I don't think that there's gonna be a discrete phase transition it doesn't go like human human human robot right like not all of a sudden will there be this huge massive displacement of workers instead being a truck driver actually involves a bunch of different types of work that aren't driving and those are harder to automate it at least harder to automate in the short term these are things like loading and unloading Freight securing stuff to a truck doing repairs and repair with customers these types of things some of those will be able to automate in time but we can't do it right away and therefore truckers are still gonna be necessary it's just that the nature of their work is gonna change and what we're gonna ask them to do is coordinate and integrate their work with machines now a lot of times people talk about like a handoff like a passing of the baton between the human and a machine but what we actually see happening in truck driving right now is not really a discrete parceling out of tasks what we see instead is actually the integration of the human into the machine in a way that's very gradual and very physical and actually quite intimate I'm gonna give you some examples of what I'm talking about so this is a cartoon that was in a trucking publication a few months ago showing like a slightly stylized but actually based on all real technologies this image of the Robo trucker and I'll tell you a little bit about some of these systems so Mercedes recently announced that it's working on a vest for truck drivers that will monitor their heart rate and their vital signs and if the driver is in danger of heart attack it will stop the truck immediately this is a system called seeing machines which uses a driver facing camera camera to monitor a driver's eyelids it detects if the driver's eyelids are closing or if the driver is starting to look away from the road and if it does that it alerts his boss that's logged like in his record for and it's used for as training data and it also this is the best part it causes the driver's seat to vibrate it kind of like gooses him back into attention these drivers are wearing what's called the smart cap which essentially is like constantly running an EEG to look at a drivers brainwaves in order to understand fatigue so if you get tired it will sound an alarm it can be configured to text your manager or even your family members there are other systems too some companies use predictive analytics to detect to predict whether or not a driver is at risk of being in a crash and it'll use not only driving data but also information about the drivers life like if their pay rate has changed recently if they started their day later in the morning than they usually do that sort of thing and so the key point I want to make here in a few more remaining seconds is the distance between these type two types of thread right so we hear a lot about this this threat of displacement right this threat that there are no longer gonna be truckers on the road and that that's gonna cause like tremendous tears to the social fabric what we hear less of is this concomitant threat which is that the nature of the work that remains it's gonna be really different right there's this displacement narrative but then there I think there's this kind of under theorize narrative about intrusion right and about the way that a is actually felt in this industry as this lights really forced hybridization of human and machine and that's what I wanted to highlight today thank you thank you Karen and now I'd like to welcome Jonathan Stern from McGill University hi everybody thanks the host thanks to Alan Ross likova Concordia my collaborator on this project I'm gonna talk about a small industry with a big impact on the sound of media the people here and that is audio mastering so audio mastering used to mean literally making the master record that you could then stamp other records from so began as record cutting but today you can think of it as something akin to typesetting many of you have published right so it's the moment where the manuscript begins to look like the book and so the best way I can illustrate this is actually with the difference between an unmastered and a mastered audio track so here's a little audio sample for you okaythat's unmastered here's master [Music] bigger deeper louder wider in short finished so for individual recordings mastering is basically the process that makes them sound finished for multiple recordings it is the process that makes them conform to a single standard and this is important for film for gaming for television for music so I've been looking at land or a company that standardizes and applies machine learning to the mastering process and they're pretty successful I wouldn't say it's perfect but I would say that it works pretty well and the reason for this is fairly interesting and that has to do with the structure of the mastering industry audio mastering is highly standardized it is in the parlance of Science and Technology studies heavily black boxed which is to say most people who use it don't really understand how it's done or how it works and the industry itself is heavily stratified so you have a few international experts you have specialists within sub industries like gaming or film or television or music you have local specialists and finally you have people who sort of can't afford professionals and do it themselves so effectively what Lander has done and here's a picture of their web interface is leverage to this black boxing right so they're very few very few options here you can basically you can low medium or high master and you can compare your master in the original recording and that's about it and this actually isn't that different from the experience of most artists who have their work mastered which is to say they send it off the mastering engineers does stuff sends it back to them and they might say can you change these one or two things so lander is a new company there I don't actually know if they're making money yet but they are making a big splash in the audio mastering field and it's important to several different constituencies in populations the first is in this sort of the happy story of machine learning is through automation they've made it much cheaper and so many people that don't have accident have access to audio mastering now do so for instance one big community that uses a lot of landers services is independent hip-hop producers so in a way this is an interesting converse of what challenge Symone were talking about before in terms of in terms of access and presence in machine learning based media but there's also the case that another group that's very interested in in services like this our video game industry and Hollywood Hollywood studios where for a game or a film you're talking about hundreds of different audio files that all need to be mastered quickly and efficiently and so this is a place where potentially machine learning could replace specialists and the last place where I really see this having an impact is not at the top of the industry but the local independent mastering engineers who are often the very people training these algorithms who are employed by small labels whose margins themselves record labels whose own margins have gotten smaller and smaller and are looking for ways to cut costs in terms of other implications this is pretty standard stuff but the thing I would point out is that more and more machine learning is creeping into these areas that shape the sound the look the feel you could say the aesthetics of media and with labor then comes the sort of not just not just people working for making not just people working to make media sound feel and look like they do but the decisions behind those things are increasingly being automated and being transformed thank you and now up to the stage Stephanie dick from the University of Pennsylvania thank you so much meredith and kate so at least as early as the 19th century pioneers of mechanical calculation imagined that we might be able to one day build machines that would automate or take over human mental labor in the same way that factory automation and industrialization at the time was taking over certain kinds of human physical labor and that vision is in some ways being borne out today as increasingly elements of cognitive labor knowledge construction problem-solving calculation modeling tasks that once belongs to the variously trained heads and hands of human beings are performed increasingly by automated systems but as historians of computing have shown time and again and as we've just heard from karen computers seldom simply replace people in the performance of a given task but rather through automation tasks are reimagined they are transformed so as to accommodate or to capitalize on the affordances of the machines that we have built and these transformations especially where cognitive labor is concerned often have really significant epistemological stakes and that's what i would like to point to today and because I'm a historian I'll offer a historical example that comes from the early history of a field called automated reasoning that sought to introduce computers to mathematical theorem proving and mathematical problem solving the example poses the question what is it that we know exactly when cognitive labor gets automated in different ways where's that knowledge situated who knows it and how so in the beginning in the 1950s in the early 1960s automated reasoning was imagined overwhelmingly in anthropomorphic terms practitioners imagine what kinds of contributions computers might be able to make to mathematical problem solving as measured against the different kinds of contributions that people make so for some they thought that one day computers would become just like human mathematicians they could be colleagues or mentors or coworkers others were less optimistic imagining the computer would be more like a graduate student and would be in need of significant guidance still yet others in the tradition we heard from Simone about this morning people use the language of servitude and slavery to talk about the kinds of contributions they thought what computers would be able to make specifically that they would do repeated menial tasks primarily and through each of these anthropomorphic imaginaries practitioners partitioned the work of mathematical problem solving and distributed it differently among humans and computers perceived as having different capacities or training and in so doing they also valued and devalued humanized and dehumanized certain forms of cognitive labor but this was only in the beginning the anthropomorphic language of the field changed somewhat during the second half of the 20th century as many practitioners wondered what forms of reasoning or problem-solving computers might be capable of that were utterly unlike their human counterparts here for example is a quote from John Allen Robinson in 1965 who imagined remaking our very idea of reasoning and the logics that capture it with computers in mind rather than people traditionally he writes a single step in deduction has been required for pragmatic and psychological reasons to be simple enough to be apprehended as correct by a human being in a single intellectual act when the agent carrying out the application of an inference principle is a modern machine the traditional limitation on the complexity of inference principles is no longer very appropriate more powerful principles become a possibility so Robinson is most famous for what's called the resolution paper which he imprinted she introduced in this same paper which was intended to be one of these computer oriented more combinatorially powerful principles so rules like the syllogism Socrates is a man all men are mortal therefore Socrates is mortal is meant to be of obviously correct to us whether it is or it isn't is another question but it is meant to be apprehended as correct in a single intellectual act in his words resolution permits on the other hand more complicated inferences from the premises all hounds howl at night anyone who has cats will not have any mice light sleepers do not of anything which house at night John has either a cat hound we can infer with resolution that if John is a light sleeper then John does not have any mice this is albeit a very baby prose example that's not actually that hard to work out but it gives you the sense that resolution is a different kind of inference rule not designed to be immediately apparent to us and accordingly resolution based proofs and solutions were not and are not easy to unpack to understand to read and follow all the way through if indeed that's even possible but resolution was very powerful resolution took over automated reasoning resolution based systems have been used to solve open problems and mathematics they've been ported into industry to solve any number of problems it is an exciting and a powerful vision but it also has a number of critics those who believe that the human exemplars understanding and modes of reasoning not be dismissed so quickly because they fear that our understanding and our agency is dismissed along with it they fear that such demonstrations can show us that something is the case but not why and I think the epistemological concerns raised by that kind of critique should stay with us as artificial intelligence systems move their way throughout many parts of our social worlds not just mathematics thank you that was excellent and Stephanie if you can just stay up here we're gonna have a quick question to answer Andy Johnathan Karen if you could join us again extra points for people who can thread the connections through these themes there are a number of them so great so do I see any hands all right let's go to : over here so connecting what Karen and Andy talked about and the theme of AI now rather than in the future recently I did a calculation that Walmart pays out about 40% of its market capitalization in wages every year to various types of workers while Facebook and Google pay about 1 to 2 percent of their market capitalization every year so putting aside anything in the future there is a AI now question of why these industries have perhaps the lowest labor share of any industries except for a few extractive ones in in history and what to do about that at present and I wonder whether any of you might speak to that I mean it seems to me that's just the nature of the business as they say that it's it's scalable it requires low levels of input to get high levels of value and I think this is just a redistribution question you know I don't think we can retool the business I think the question is whether it's the Microsoft robotics or any other ideas that we have I think this is more of a question not of changing business models but redistributing success in some way shape or form I would say for trucking I think the potential of a move towards AI follows on a lot of other moves that are designed to try and reduce that proportion right so like a familiar one that a lot of us have thought about in other contexts is the move from employees to independent contractors right and so in many ways this is seen as kind of like being another like the motivation from the trucking side is definitely reducing those labor laws great so I think we had a question from male Steenson hi thanks this also ties into the last session a little bit one thing we see with anything that's data oriented is an increase in visualization right we need to visualize the data in order to understand it but what we also see is a step away from representation right so if we were considering Simone's talk in the last session I think there's a question there of representation as opposed to visualization I'm wondering first I'm wondering two things first how do you see this taking place in the different case studies that you've given whether mastering or truck driving or a history of mathematical reasoning in artificial intelligence and whether something should be gained by representation or perhaps the opposite I think I'm not quite sure what you mean by representation and visualization because to my mind there are a number of visual modes that serve as modes of representation as well so could you clarify what you mean by the two terms right now I'll speak in an analogous field in architecture there is what's been referred to as a crisis of visual a crisis of representation art historians humanities scholars architectural historians have issues now with the notion of representation as opposed to visualization right so if we're if we've now put all our eggs in a basket of visualization what have we lost by representing sure so I guess I can say something about that it's not just that under the hood these more powerful more computer oriented modes of reasoning and problem-solving are hard to follow they don't come out on the page in a way that you can publish in the same way in the way that people can read and annotate in the same way there has been a kind of displacement of forms of writing and symbol systems that are designed to be more accessible to human intellectual work and that's certainly something that mathematicians are anxious about relative to increased automation I don't know if that addresses your point but it seems like a related problem I think it may be also contributors before but it's a really interesting question I think it potentially contributes to the uncanny Nisour the experience of like hybridizing your body with AI in the way that truck drivers do right to feel known in this like really internal visceral way that isn't visual right so I like have it be based on signals from your body that you have like are not represented to you in any way right but to just like have the truck wake you up right like I'm I'm extrapolating because I haven't asked this question of people but I would guess that maybe that that ties into your to what's happening in architecture as well great so I think we had a question from David then Blaise then Rob it's okay so thanks for thanks very much for a fascinating set of talks I think my question is is two related questions one's about a group of workers who I think haven't got a huge amount of attention so far in looking at questions of automation so tend to focus on kind of highly visible work like some manual labor and truck driving those kind of things there's a there's a sector of workers in those of white-collar industry who've done basic administration effectively and the enterprise IT industry which I'm a part has wisely spent most of neo decades trying to automate that work away there's been a whole category of work that's been beyond the reach of traditional software engineering because it required just enough judgment that you can't codify in normal programming languages that's now through AI machine learning solutions becoming addressable so I think there are hundreds of thousands if not millions of workers worldwide whose work can be addressed in that mode and I must bet for them I don't see the kind of the collaboration Co machine working I think the work will just go away and become fully automated so it's just wondering what your thoughts are on work that's being done to think about that sector and then a related question you know if you look at the press over the last last few weeks and months and an hour experience of big cyberattacks and virus infections the topic of resilience becomes becomes ever more important the good thing about humans is that they can you can affect them in viruses but not IT viruses and and they're kind of quite good at figuring out how to respond to crises what do you say do you see the need for human based resilience as a way of securing employment future um I'll say something about the first part of that question which is that I don't think machine learning is the I mean maybe it's a tipping point for certain kinds of white-collar labor and administration but if you think about if you think about examples other than manual labor corporations have been working to reduce the number of people involved in those for a long time so we see for instance the laments about the disappearance of retail now as an example of that in relationship to mail order but of course retail shopping has probably for decades although I don't have statistics on this had fewer and fewer people working in stores right a big-box store versus the corner hardware store for instance so it's a it's a larger structural thing of which machine learning is a part and that has to do with how companies think about their bottom lines right and that labor is seen as a cost that needs to be dealt with and gotten rid of rather than part of being a company is providing jobs for people right and so that leads you into questions of basic income and things like that but I think in general I mean one of the reasons mastering works as a field that could be automated is precisely because most people sent their tracks away to be mastered the attended mastering session where you were there with the engineer is a relatively rare thing and in that industry and that's that's why it works so I think the fact that people have been taken away already makes it easier to automate these things yeah the number one job that Columbia masters and Business used to get the number two jobs were stock analyst on Wall Street and traders on Wall Street now there are no jobs and just add to what Jonathan said I think the automation in particular of administrative work and also of other kinds of sort of accounting and calculating is part of the longer history of mechanical calculation and automation the earliest computers of course were constructed to replace the work of typically female human beings who were doing complicated calculations on mass on paper and they were replaced by machines like ENIAC so this gets back to Jonathan's it rains' comment at the very beginning that we have to think about what we mean by AI when we posed these questions because some of them are just related to the longer history of computing and mechanical calculation in in general so we're running up against the clock and we have a lot of questions so I'm going to start lumping them together and then we can give thoughts on all of them so we have blaze and then Rob and then Fred so we can go in that order and maybe try to keep it to one thought you just raised the question of labor and gender and one of the interesting things about truck driving is that that's an overwhelmingly male profession and that's true according to GLS of a lot of of the jobs that are in imminent risk of being fully automated in the near future you also raised the issue of instability of social instability a lot of male grievance a lot of job elimination social instability comments please yeah so if we could just pass it to Rob over here and then so one thing I liked about the recent talks is you guys talked a little bit about sort of heterogeneity of effects right so we heard about truck drivers and how maybe the effects are many different for long haul drivers versus short haul drivers the nice example of the mastering and how they're sort of maybe you're gonna be different effects for high-end mastering versus low end mastering right on I feel like to a certain extent that's been missing from the conversation we talked about so these these bigger issues that we think are gonna happen these sort of bigger effects I think are gonna happen there tends to be very negative I think might do sort of overly negative view about things that will happen and we sort of miss perhaps some of the positive benefits and so I feel like you guys were sort of talking about that a little bit um we got two great case studies I feel like would be great if we had sort of more systematic evidence about what was happening or what is happening across different industries and even within industries or occupations sort of across different types of those jobs different types those occupations so I'd just love to hear some some thoughts on that so the first Automation struggles that I know about are from the late 50s in the early 60s and in that period as now there's a terrible fear that automated beings will suddenly sort of take everything over the robots will come there will wipe us out they will take away our jobs I was struck that the stories that you were telling across the panel we're stories of a more frog-like slow boil where automation occurs slowly and in relation to people's lives and it's as much a process of integration as it is a process of overcoming of over running my question is if we understand it as a process of integration what might we learn from the close work that a number of you are doing that would allow us to speak back to and change the top-down robots are coming debate great so take your pick I mean I would say there's you know lots of evidence across the board we can do more studies and should you know about this constant downward pressure on jobs and income both you know we see in the labor participation we see it in the opioid crisis we see it at many different levels but we don't sort of want to recognize it because we have four point four percent unemployment and every president wants to tell you that you know happy days are coming and so I just think we really need to focus on the huge implications you know of getting this wrong because every previous economic revolution even when more jobs came had twenty to fifty years of paying for people that we could avoid I'll just say two quick things in response to blaze and Fred first in terms of gender like I actually have a whole paper about this that I can send you but but very broadly speaking I'm certainly the fact that truck drivers like buying their masculinity up in their work and in like the mechanical equipment like I there's a lot to be said about like the role of masculinity in that job specifically resistance to Authority and resistance you know autonomy and freedom and all that kind of stuff sexual freedom that like helps to helps me to understand why this is perceived as a threat not only to their ability to provide economically but also linked to themselves and then in response to Fred I don't actually have a great answer but I have an answer I have an answer that I think would be the wrong answer that I would caution against which is this which is that I think like the easy thing to do would be to dismiss these stories of like oh here's what's actually happening now as being kind of like this short term like oh well yeah this is what's happening for five years but like once there's no more truck driver then who cares like that sucked for five years and then now it's over but I think that's if that would be a fallacy right in part because there's always going to be a person right in part because like if we're interested in AI now and the ethic ethical and social and political implications now then like obviously an adequate answer and also in part because like to the extent that data is being drawn from these systems now like if you have really unwilling participants like participants who are trying to thwart the systems at every turn which they do then like you're getting bad data right so even if you don't care about the people as people or about the dignity of work like you might care about this as a technical problem I would I would like to say that of course there is a ton of women's labor that is also being automated certain areas like in caregiving for the elderly the education of young people department store work and secretarial and administrative work which have picked typically been gendered female there is huge investment and work being done to automate that kind of work as well there's also more investment in female gendered Android bodies than male gendered ones you don't see robots that look like men doing the work of men in factories but you do see robots that are made to look like women doing the job of a receptionist in a hotel for example so there are differences to note there's certainly the automation of a lot of female gender labor taking place to Fred of course I gave one example that I hoped raised something that I have actually seen in a number of different places this anxiety that automated systems may be able to show us that something is the case but not why that's a story that covers the entire history of this field of automated reasoning that I'm writing a book about but it also I think attaches to debates today about interpretability and non interpretive and interpretability of machine learning systems there's a debate between Peter Norvig and Noam Chomsky that raises a number of the same issues so although it was just a case study I think it is indicative of a larger epistemological set of questions that accompany the introduction of these automated tools and I guess I get to go last Rob I agree more systemic knowledge is good I don't like this sort of good bad sort of dichotomy though and this this also goes to Fred's point the way people generally talk about tech industry stuff is in impact language as if it's somehow outside of society technology called companies like all institutions or cultural institutions their political spaces they're fraught with the inequalities of the broader society even when we talk about things like basic income we need these sort of broader systemic understandings of power any inequality so in a way the answer is the same as it was in the 80s and 90s which is we need to get beyond this sort of this new iPhone is gonna change my life this new machine learning algorithm is gonna change everything and instead actually look at the people and the institutions and the regulations that are often bumping up against one another great that was fabulous and I saw a few more hands but unfortunately we are running up against time and we are a tightly time conference so we're gonna let you go to break please be in your seats by 11:50 we're gonna start promptly with the rights and liberties section and before you dismiss just a quick plug outside you will see what we're calling our interactive wall you can't have a tech related conference without one and it asks the question what is the most urgent thing this community needs to do in the next 12 months so we have post-its there if you have ideas provocations thoughts share them we're gonna think of this as kind of a time capsule for next year so have your break be back in your seats we'll get started with rights and liberties and ethics and governance after that and thank you so much to our speakers that was fabulous [Applause] you yes if you can hear us collapse wise here you can hear clapping now awaken you up we're getting you in that worked alright everybody I just want to say first of all again a giant thank you to all of our speakers this morning it was spectacular this the scope and scale of ideas that are necessary for these discussions were just brought so beautifully and elegantly so we're about to get more you'll notice we had very relaxing music for you as you were walking in that's because this is going to be the rights and liberties section and it's going to be some quite sometimes stressful sometimes confronting things that we have to think about so we also just want to say we're really liking some of the notes that you've been taking and we've been looking at those it's prepping us already for what's going to be an amazing discussion this afternoon so keep it up remember that's what the post-its are there for so keep them coming they're fantastic so right now we're going to kick off with rights and liberties and if I just turn to my beautiful program very first speaker is going to be Sarah brain from UT Austin who's going to be talking about her incredibly important ethnographic work looking at predictive policing and the LAPD thank you Sarah [Applause] great so I am a sociologist and I study how the police use big data as Kate mentioned I spent a couple of years doing fieldwork with the Los Angeles Police Department and in many ways their department that's sort of on the frontlines of the use of big data and predictive analytics this is the photo I took in Thai inside the real-time crime analysis Center which is kind of like the Situation Room in downtown LA so today I'm going to talk about two kinds of data-driven surveillance the first is dragnet surveillance and the second is directed surveillance so dragnet surveillance refers to tools that gather information on everyone rather than merely individuals under suspicion one example of a dragnet surveillance tool is the automatic license plate reader or Alper so Alpers are cameras that are mounted on top of cop cars and there's some static cameras at intersections or other locations and they take two photos of every car that passes through their line of vision one of the vehicle one of the license plate and then it records the time date and geo coordinates so dragnet surveillance tools like automatic license plate readers me that increasingly individuals who have no direct police contact are integrated into the law-enforcement corpus so in that sense dragnet surveillance tools represent sort of a proliferation of what can be understood as pre warrant surveillance so it makes possible everyday mass surveillance at this unprecedented scale pre-crime data can be mined for links once criminals suspicion comes into play and once in a database law enforcement can retro actively search and model alper data rather than only starting to gather information on an individual once they come under suspicion this means that information is routinely accumulated and lie and files are laying in wait so directed surveillance by contrast is focused on individuals who are under criminal suspicion one example of directed surveillance would be algorithmic policing we've probably all heard a fair bet about location-based predictive policing where historical crime data is inputted into an algorithm and then that predicts where and when future crime is likely to occur so I'm just gonna spend a little bit of time talking about person based predictive policing so person based predictive policing is promised on this idea that a small proportion of violent players are just proportional responsible for the majority of violent crime therefore focusing police resources on these individuals will have a large impact on reducing crime rates so in a division in the LAPD they use a point system in which individuals get five points if they're on parole or probation five points if they have a prior arrest with a handgun five points if they are in the gang database and then five points for a violent criminal history after that individuals get one point for every police contact so the cops stopped you and talked to you that's a point quantified policing in that sense can easily become a self-fulfilling prophecy it generates a feedback loop whereby an individual having a high points value is predictive of them being stopped and then they're stopped and that thus increases their points value in that sense mathematize police practices can place individuals who are already under suspicion under new and deeper forms of surveillance while appearing to be objective or in the words of one captain I interviewed just math just because directed surveillance is focused on individuals who are under criminal suspicion doesn't mean that they're the only ones involved so this is a network diagram I had an employee at Palantir technologies make for me based on a network diagram I obtained from someone in the LAPD this person in the middle this like Carmen Sandiego person is someone with direct police contact radiating out from that individual are all types of entities that he or she is related to it could be other people phone numbers cars and then on the lines I'm not sure if you can see it indicates how they are related so by being the lover of the person under suspicion and cohabiting with or on a cell phone for example now it's being what I call this secondary surveillance network you don't need to have any direct police contact you just need to have a social network tie to the individual of interest anyone new surveillance technology on its own might not be particularly consequential but the combinatorial power of using the person based point system in conjunction with automatic license plate reader data in conjunction with network models can grant authorities a level of insight into individuals lives that would historically constituted a Fourth Amendment search and thus required a warrant however because no one of those surveillance practices falls outside the parameters of the law in isolation neither does their combination by using a series of data points or what I call digital traces to automate the reconstruction of an individual's intentions and behaviors whether incriminating Oryx culpa Tory this lies or rests on the assumption of an infallible state and of actors who enter data and search databases without error or prejudice so I want us all to think about the digital traces we leave how are you leading an incriminating life thank you thanks so much Sarah really important work now we have Patrick ball from the Human Rights data analysis group thanks Patrick thank you and my clicker is here got it data's always lying and here's some data and I'm going to tell you how it's lying so I get to show the first graph of the conference I find that very exciting and I also get to talk about the first concrete algorithm this is a really tricky one we're gonna draw a line through the points I'm gonna draw a line through the points it's gonna be transformative we're going to use someone's height to predict their weight it's pretty exciting stuff now this brings up the notion of data and here's a bunch of data I found it online it's very big data very very big very very big hundreds of thousands of people combined height and weight measurements that I plotted a line through and that gives us a way to understand what we mean by a model the training data plus the algorithm gives us a model and in this case we have a trivial model that we can describe in seventh grade algebra that weight equals minus one twenty two point six plus three point five times someone's height in inches gives you their weight in pounds now this is a great model and I'm gonna show you how great it is by showing you a line drawn through points that I didn't show the model beforehand and look at that line that goes right through those points will someone please call Wired magazine this minute we are displacing the entire industry of scales we are transforming fashion Fitness all right calm down this is called an out-of-sample prediction and it is considered in most by most machine learning practitioners the way you test the model you've got data that you did show the model you put your algorithm let your model draw the line through it and you see how good a fit you've got and this notion of it being a quality control measure is both completely correct it's accurate but it's also absurdly optimistic it's absurdly optimistic because we randomly drew these points from the same data set we used to train the model and here's where it all gets really tricky because whoops what about those guys well it turns out that shoot I'm really sorry forgot to tell you the data I train the model on was of Hong Kong schoolchildren and these red points are major league baseball players turns out not the same turns out I could have used a different model but because I didn't know that because I didn't get out of my chair and go and look at the data in the real world I just took a data set that came to me by some automated data generation mechanism I felt really good about my model and I just made claims about it and not only did I make claims about it if I made sufficiently powerful claims people may have made policy on that basis and we would have kind of blown it for all these other people so how bad can it be right maybe we just put a caveat on it we say no don't worry about it it can't be that bad no it actually wasn't even all that great a model for the hunt school children because it turns out that when you ask people to opt into a study people who love their weight and are really happy about it opt right in and hop up on the scale and you get the blue dots and you draw a line through them people who are self-conscious about their weight opt out and so you've got a whole cloud of people who you don't observe and now you have a model that's really for not just Hong Kong schoolchildren but the fit and thinner Hong Kong school children not for everybody else so what is this model doing well the model is telling us the relationship among the variables we can observe and that we have observed it is not telling us other things so what are the points that were unlikely to be observed what did we miss and are we selling ourselves or as the proponent of this model selling us a caveat fantasy and I sometimes go back and forth is it a fantasy well that's the self delusion part is it a fallacy when we're selling it to someone else this is the fallacy in which we say well our model applies to this part of the world as though we know what that subpart is these whenever someone says well okay our model doesn't do that but it does this how do we actually even know it does that how can we define that if we don't know what we're missing in the training data if we don't know what the data excludes we don't know what the caveat should be and by the way does anyone even remember their caveat by the time they get to the conclusion no no it's only on the first page by the time you get to the end of the page we're selling it as though we were pitching a Wired magazine reporter so we all have to use these things but we should be very very careful about their limitations and finally this was mentioned in a couple of the previous talks and I think it's worth highlighting is there a feedback loop between the process of observation and the way the model works does the model then teach us something that we then use to take some action which generates new data that we put back into the model this is particularly problematic I think in predictive policing and risk based modeling in criminal justice context my colleagues and I have a couple of studies about this but it occurs in a number of other areas as well and I and Cathy's done a bunch of work on this as well it's really important how do these feedback loops then distort what we understand from the model itself and so let me leave you with some conclusions let's stop talking about algorithms and talk about models because once we talk about models we enable the conversation about the data the data that trained the model all algorithms in the same kind of classification I'm gonna do essentially the same thing let's not worry about that and so let's also stop talking about big data because big data that's not necessarily more representative actually tends to increase our certainty in the wrong answers thank you thank you so much Pat Rekha next up we have Ryan Kaler University of Washington Tech policy lab okay so hi everybody so I'm a law professor I think I might be the only law professor on this morning's program and the legal Academy doesn't tend to place a lot of emphasis on method it's a it's a kind of a something we're having a discussion about ongoing but cyber law itself or technology law has a species of an unstated one and what what we do is we essentially take a new technology we try to define it we try to place emphasis on what's really novel and how it's different we then think about how it changes human affordances like the people's actual and perceived capabilities in the world and then we go through the law and we look for legal or policy assumptions that no longer obtain in light of this change and then we make recommendations for how to restore the previous state of affairs right and that's that's a sort of method that we use so applied to artificial intelligence hard to define of course a set of techniques aimed at getting machines to sort of usefully approximate thinking what does it do its spots patterns we can't spot it solves problems differently it's it reacts faster and so forth presents a lot of challenges for law and policy I like a recent example of a paper I like in this vein is Andrea Roth who writes about machine testimony you can't challenge a machine witness the same way as you can a person in a trial then there's all kinds of recommendations about what to do about it so the problem of you can't have recommendations or explanations of neural nets people say we got to introduce transparency conduct audits spot test software verification and so forth so I don't disagree with this method and it's often what I think of myself as basically doing but it's Keith Crofford and I have written we can't lose sight of the broader context some technologies and I think AI is one of them constitute an invitation to take a kind of societal inventory not just for holes in the doctrine but for our common values we might ask not just how do we keep our existing rights intact in this crazy new world but why do we have these particular rights are there guiding principles that ought to fall away that don't make sense anymore are there promises that we can finally keep that we've never been able to before like the idea that you have a speedy criminal trial and if we keep those promises that have come to the expense of other promises like that trials be public and finally are there new values and rights that are as novel to us today as privacy was the common law at the turn of the 19th century as we perform this exercise I think it's important to look at the system as a system for those of you who are Game of Thrones fans either the books for the you guys remember the scene where Ned Stark is like you know makes this judgment and then he's got to be the one to hold the sword and execute right he says something like the man who passes the sentence should swing the sword well in one of my favorite Law Review articles violence in the word by Robert Cove are Cobra argues that the whole system the whole legal system exists in many ways to sever the connection between the person who makes a decision and those who met out that justice and I you know I think we should be thinking about whether it is a good thing to further remove the judge from the execution as I suspect artificial intelligence might so all you tend to do work on creating a kind of inventory of values in the coming year as part of a three year project at the tech policy lab around artificial intelligence and law in policy and in doing so I want to draw from deep experts much deeper than I in administrative criminal and civil procedure as well as people like you who are doing work exploring the design of fair systems so I hope that you will please get in touch so that's something that you want to work on now I realize of course that no discipline hosel monopoly on values like fairness or justice or equality I'm gonna start with the law because as I said I'm a law professor right that said I think law is a good place to start not only have Jurists been thinking about these issues like how to balance fairness and efficiency in decision-making for hundreds and hundreds of years but also historically law has had a monopoly on coercion so Robert kovar opens violence in the word that article I mentioned very memorably with the line legal interpretation takes place on the field of pain and death the idea being that these decisions matter and the fact that AI is now helping to make them matters - thank you very much thank you so much Ryan kalo next up we have Lester Mackey who's based right here in this town at Microsoft Research in New England thanks Alysa thanks everyone thanks so the question motivating my talk is how do we convince and equip the next generation of AI researchers to actually work on the pressing social problems of our time and what I'd like to give you is an imperfect and biased answer to that question in the form of a story with the hopes that all of you gathered here will come up with a better answer by the end of our our time together so this is a story of statistics for social good that's a volunteer working group at Stanford University of students faculty and researchers dedicated to promoting social good through effective data analysis and the story begins in 2013 when I joined the faculty at Stanford and learned about this interesting summer program at the University of Chicago called data science for social good their goal was to Train data scientists by having them work on actual problems of relevance to the government the Chicago government and and nonprofits and I thought this is a fantastic idea I'd love to contribute to something like this at Stanford what can we do like this of course I didn't really know how to get started so with invoking my usual field bias I sent out an email to other machine learning and statistics researchers asking them whether they could help me track down organizations and pressing social problems that could benefit from effective data analysis about 20 people responded to the initial invitation most of them were computer scientists and statisticians like me most of them had zero experience working on social data problems and in fact a number of the students told me that they'd longed to work on these sorts of problems but they never really knew how to get started they'd never really encountered these problems in their classes they didn't see people working on these problems in their fields and so they felt that these were not really legitimate problems for AI researchers to be working on what we ultimately decided to do was to take a divide-and-conquer approach to the problem finding problem each of us was responsible for reaching out to a particular problem partner this was an organization nonprofit a government agency a campus institution even an individual expert who actually had the expertise to know what the pressing social problems were in his or her domain and to know how better data analysis could influence those problems and we would the hope for this was that we could learn from the problem partner what the pressing inferential questions were what the relevant data sources were and how we as outsiders with some data analysis skills could help them answer these questions in-house the NGO typically at the end of a discussion we would bring the information back to the complete group the statistics for social good group form a team around some of the problems that were identified and start a collaboration and I should say that most of our cold calls and cold emails didn't go anywhere but every so often we encountered the right group with the right problem that matched our knowledge and expertise and that led to some interesting collaborations so just give you a couple of examples in the minutes that remain the first is with a program called spark point it's started by the United Way of the Bay Area these are centers that provide all sorts of services to low-income individuals to help them meet their financial goals so they'll give you resume building advice they'll help you open your first savings account they'll teach you how to keep your credit score up and they have very data-driven ways of keeping track of your progress so they didn't they measure your credit scores and measure debt levels every time you walk into one of the centers they will keep a record of that interaction every time you call our email they have a record of that interaction and so they came to us asking for a given forgiven individual with a given target goal what is the right bundle of services to provide and in what order should we provide those services and here's eight years of data on what we did provide and what the outcomes were a second example is with several organizations several campus institutions at Stanford as well as some partners in Denmark the goal here was designed interventions for people who would incur large amounts of healthcare costs but would be able to defer those healthcare costs if they were intervened with so essentially the first step here was to develop a predictive model of which people were going to incur preventable health care costs and again we're able to work with a large data set in this case of two million residents of the population of West Denmark from 2004 to 2011 and I'm giving you different examples to show just the variety of problems that we encountered each one was completely different the process to getting to the right to the right question was completely different this is a give you some food for thought about how we might organize these sorts of the sorts of teaching experiences in the future a third is with the global oncology initiative they are interested in investigating inequities in palliative care and the very first step was essentially visualizing these inequities in palliative care so we created for them and opioid Atlas that you could go to there's a link in the slides and it essentially shows what countries have access to opioids where's the demand and where are the gaps and one last example this is one of the first projects we worked on in fact it was brought in by a master student at Stanford who was very passionate about helping nonprofits it's through an organization called great nonprofits which is essentially a ratings database for nonprofits it helps to match both individuals who are in need of care people who would like to donate and people would like to donate their services that you know their times these organizations to find the right organizations for them there was a there was a prevailing hypothesis that those who are receiving services from an organization would not rate rate their organization poorly because they depended on those services and we were able to debunk that hypothesis so thank you thank you so much Lester and to bring us home we have Nathaniel Raymond from the hobbit humanitarian Institute Nathaniel alright this is cool because I feel like a contestant on The Price is Right that's always been a dream of mine so who knows what PII is saying personally identifiable information alright now who knows what di is I thought so demographically identifiable information and di I not PII is basically the fuel in the tank for AI but we don't have a conception of how rights and ethics relate to di I we actually don't even have other than my book chapter and group privacy last year in Springer which you know we don't have an accepted definition of di I but we're generating this type of new data and we're using PII based ethics PII based IR B's PII based readings of Belmont Nuremberg the Geneva Convention to use this dii data with some of the most vulnerable people in the world in experimental ways with absolutely no oversight so thus my provocative but hopefully also precise title data colonialism and that phrase it's very loaded intentionally and it came out of a moment I had last summer about this time I was discussing Shawn Martin McDonald's paper big Ebola big data disaster if you read one Ebola call detail records paper this year make sure it's it's Sean Maher McDonald's Ebola big day disaster and I was responding to it at data in society and I said this is data colonialism and then I got off the stage I'm like what the hell does that mean besides being buzzword bingo and I began with my team at the signal program on human security and technology to try to unpack in the past year with that men and so we get to a definition of colonialism meaning the imposition of unequal often extra legal power relationships on one group by another and in that sense we are in a state because of dii because we haven't contended with dii we hit a fork in the road as they say where I'm from and we took it we haven't had an intentional process to think about how we we retrofit our ethics and our rights for technologies such as AI that rely on dii based on an Instagram post by Meredith Whitaker I was introduced to Ursula Franklin last year and she said the late gray or so Franklin said technology is the way you do anything my provocation for you here is that rights and ethics are a technology stop thinking about them as separate from our stuff with the ones and the zeros and as Elizabeth Bishop said the bright objects that hypnotize the mind ethics and rights are actually preface encoder calls to use the other technology so in June of last year I told my team after the data colonialism mind I said okay but we're gonna stop innovating for one year we're gonna Luke turn off the computer and we're gonna step away from technology and we're going to get back to basics like ac/dc and the garage with the guitars just turn it up to ten and out of that process we created the signal code we spent six months reading all available rights and law and we came down to the existence of five rights the one I want to talk to you about in the last minute I have is ripe for data agency and data agency and thinking about that concept of data agency was an attempt to apply exists sting writes in ethics to the concept of dii and what does that mean it means we have to go beyond informed consent we have to begin thinking about what does inform notification meaningfully inform notification and meaning the meaningfully informed participation mean for communities in my context in humanitarian response and human rights documentation we are engaging in non-consensual human experimentation on the most vulnerable people in the world on the worst day of their lives that demands that we do more and we start thinking of ethics and right as a technology equal to ones and zeros thank you Nathaniel I'm gonna invite you to the stage if we could have all of the speakers back up here for a Q&A section that was such an extraordinary breadth of topics and what I really liked is that some of you who have spent a lot of time directly in the field I'm thinking of you Nathaniel and Patrick working specifically in human rights contexts are also bringing very sophisticated lenses to thinking about the data that has been generated there and where it succeeds but also where it occasionally fails us so again I would ask you to raise your hands extra bonus points if your questions can connect some of these themes we have two over here I'm gonna start with Jonathan and JZ I would also ask that we have mirror over here we'll take those first three and then we'll come to you as we go oh sorry Daniel was first Daniel gets Daniel was caught by the mic people before me so she gets to go first then Jonathan then JZ man what a great price of being forgotten that's fabulous you know right love you Kate okay so it's there is absolutely wonderful theme here right that the poverty of thinking about rights as individual rights and that we ought to understand the rights as collectives right so if we could understand the ser as you are talking about the right of the people the sort of first couple words of the Fourth Amendment as collective rights we might vet I think be able to be imaginative and think about technologies of total surveillance radar dragnet surveillance as you put so well and Ryan you're nudging us to think about values right the the broader systemic values not just on an individual level right but fairness justice equality more broadly understood and you know Nathaniel your demographic thinking about the certainly the poverty of consent that notion of consent and privacy law is it fails us when we think about demographic identifiable information so I wonder now well what's next right how do we translate and I don't mean to overlook Lester and Patrick wonderful contributions but maybe for all of us how do we think about translating collective rights and giving them real meaning in this collective context yeah I think that's a great question and in terms of the distinction I would sort of add a third dimension which is that of networked rights and so when we think about inclusion and different data systems rights and risks are very different according to why people are modeled right so in the criminal justice context of course the probability of being included in those systems and your risk being modeled in those systems can have a lot of implications and that might be different from those in other contexts like marketing or advertising for example but Ryan may be obviously there's a there's in some instances a tension between you know individual rights and and collective ones and sometimes individuals are not best positioned to vindicate the collective rights I mean in one instance that may be familiar is that in the context of the Fourth Amendment if you think about it right the people that are that are vindicating our Fourth Amendment rights are people who stand accused of a crime and they are you know they're sort of risen on d'etre is they is to get out of that crime and as a consequence people have argued that that that doesn't serve the whole public in terms of our ability to push back against the government right so there's definitely a tension there that I think it's important to think through I mean my point was maybe narrower which is that gosh I mean is something so radically different about artificial intelligence that there are certain rights that we could never have imagined granting to people or ones that that don't obtain anymore right and so that that's that's that's something I think we should think through and just push off on that I don't think there are new rights but I think there are new hazards that we didn't imagine and I think that the speed at which through mosaic effect PII can be weaponized into what the Marines call a bi or action based intelligence also known as a kill chain the same thing that we use dii for to do a good is the same thing that bad guys taking from my context what we use in Syria to do assessment is the same thing Assad can use to refine a double tap barrel bomb and may have done so so the the issue is is that the kinetics the velocity the speed of decision making in the granularity of targeting is now entirely different so it's not the rights that are different we need to translate them to the collective but need to do so within a theory of harm and if we don't have a theory of harm we're gonna be left saying Premium non nocere first do no harm without knowing what the harm is and that's where we are now so that's that's the biggie is a theory of harm based on evidence we don't have beautiful thank you we're getting a lot of questions for this panel so I'm going to start taking two at a time we have Jonathan and then JZ it's great to be opening for jay-z okay I comment and a question they go together the thing that struck me about Lester's story which is a great story and I'm totally happy it happened but that you'd have a bunch of machine learning researchers on a university campus saying where could we find a bunch of people who define and think through social ethical and political problems and didn't think to look elsewhere on the University campus to me that's a sort of evidence of the continued sort of CP snoke two cultures thing except here we're really talking about humanities and social sciences and engineering and that goes to the theme of coercion which really runs across all the papers which is to say that you have lots of machine learning practices that are extremely coercive in one way or another often the people undertaking them don't even understand that they're being coercive the police and the military might be exceptions so basically what do we do about that like if if the people practicing these things don't even understand that they're experimenting on vulnerable populations that they are effectively coercing people and you know whether the violence is legitimate or not to use the Max Weber line about the state having a monopoly on legitimate violence there's no question that private corporations systematically visit small violence's on people every day I mean all the stuff about airline travel and the news recently is a great example of that but you can think of countless ways in which the the extraction of value not only from workers but from customers is tied to coercive practices what do you do to build machine learning differently Wow okay question I love it professors are trained and also known as jay-z yes I just like to start by apologizing to those on the livestream who were expecting jay-z I thought one of the amazing things about this panel was the kind of kaleidoscope across it of modalities of gathering and storing and using mass data from public to private to academic and thinking that that might be a framework to realize at least in the past that we set up very different rules and expectations around it that may not make sense now in the public zone I think it gives rise to a question when I think of mass license plate surveillance for example which there are many many reasons to be chary of that it does bring into focus a question of whether getting a little bit of information on everyone to advance the solving of a crime or the preemption of something terrible maybe less privacy intrusive than the more traditional let's go to the usual suspects and really shake them down and get a warrant and you've gone completely into their lives and if it turns out they're innocent it's like we're sorry you know you can make your bed again we'll just leave through the back door and the one we came in and and I think we owe an answer to that question when is it okay and even salutary for all of us to suffer a little privacy intrusion rather than just a consistent view of us possibly for some of the reasons brought up in the bias panel doing a little bit and on the private gathering front when I compare it with academic and I don't know Nathaniel how much of your warnings about the exploitation of populations are really only to academics stuff like the common rule the limitations the IRB czar only applying as best I can tell if you're pursuing the disclosure of general knowledge and Facebook is like don't worry we won't tell a soul they pretty much get to do what they like and gather what they like which creates its own asymmetries and who can do anything in the space yes I'd love to figure out how to beat that and finally there's probably p2p is another form of gathering this stuff peer to peer neither academic nor public nor private and I haven't begun to figure that out yet Thanks great two important provocations please patrick is bursting to tell us something I can tell I am I want to jump off of Jonathan's point and say one of the things about the doing technology for good well I live in San Francisco so I hear this pretty much four or five times a day is that it worries me that we don't have a notion of malpractice because what I worry about much more than the privacy intrusions which have governed this panel is that we as experts will tell our nonprofit partners something and we'll be wrong because we have done a crappy job because we've used improper data we haven't thought about the data we haven't randomly sampled the data we haven't tested it and we'll tell them something that's wrong they will rest their legitimacy on that claim and then they will be discredited and we will walk away having put it on our website I'm not going to point any fingers like a Palantir but I'm not going to but we will have put it on our website gotten credit for it but nonetheless our partners will have been harmed because we did crappy work so I think we need to start talking about a notion of malpractice but in to just to contradict that same point I'd like to suggest that by creating limitations on the kinds of data we can collect in the space of trying to advance rights-based arguments we can also destroy our foundation for those arguments and some of the most powerful projects I've ever been a part of have been profiling projects we've done these in El Salvador in the Congo we're doing them now in the United States with police in which we track the perpetrators we link the violations not from a victim-centered point of view which is the way traditional human rights work has worked over the years but rather through technological means usually through like really fancy technology here I'm talking about sequel we will link the violations databases to the perpetrator career structures and develop dossiers on the perpetrators that we can use for accountability well one day the Belgian ambassador who had funded the project was visiting us at the UN project in the Congo and he was being he was just admiring it cooing as donors often do and he and I said this would be illegal in Belgium wouldn't it and he looked at me and smiled and nodded right so we should think about those asymmetries because are there no abusive officers that we should be tracking in Europe that does not strike me as correct yeah can I hop in tag okay so we got a lot to cover very quickly so the what Patrick's raising is the ethical principle of non-maleficence so we talked a lot about beneficence doing of good Maleficent's doing of intentional bad but really you've hit it right on the head is we don't have a con of non-maleficence what what is the prevention of negligence basically and so what does that come down to is a fundamentally missing building block and as someone who has been a perp tracker for my entire career including with satellites I totally agree we don't want to limit the incredible role that data can play in making that case and as someone who works at different times with ICC critical evidence that said it's about duty of care and articulating that duty of care so it's not about trying to say the line is here it's about saying what are the constituent parts of a legal duty of care in this context and it does come back to negligence prevention excellent we are now full of questions and fortune we'll have to hold it there but I've scheduled you in so if you've had your hand up you are going to be in this this set of blocks the next person we have is Amanda and then we're gonna go to mirror start thinking about the the new hazards that might be posed by AI and but a thing that's emerged across all of the different talks this morning has been not that necessarily AI poses new harms but the harms are in its power to exacerbate they dig 'not ease and intrusions that have always been imposed on sort of the same marginalized people and we hear a lot in the AI context about race and gender but I think the labor panel really brought to the forefront the the other aspect the interdisciplinary and also intersectional aspect of class so my question is when we think about it exacerbating these inequities by not just intensity but also pervasiveness and in the criminal justice context the insidiousness when we're thinking about rights and liberties truly depriving people mechanically of rights that there really should be a human who has to grapple with sending someone to prison or letting them out of prison or what that actual human human factor is how do we think about whether solving these exacerbated inequities that are posed by AI actually solves other broader and equities that have always existed but it's been easy to sort of put them off and kick them off to another day great so a really important question that I think also brings us back to the bias panel around what's the intervention here and mirror if we could add your question as well sure so this notion of consent is particularly important for me this is where it's at the point where at which a user says yes I am willing to give this exchange of data for a service and most users don't end up reading Terms of Use it's too much works 12 pages long I just hit yes and go on so my question is how do you guys think about going beyond informed consent when we can't even predict how that data may be used and whether that data will be used to retarget a particular individual or whether it will be used in some adjacent industry or category where that individual may not even benefit from the use of that data so great questions please Ryan yeah so um you know one of the things we think a lot about at the Tech policy lab is how is the problem that Tech policy doesn't often reflect non-mainstream voices right so the idea is that tech policy reflects the mainstream this is something we all know but we don't really have a way to deal with it um so while law professors don't emphasize method it turns out that my co-director is like an information science and computer science and so far they're obsessed with it and so we've come up with a methodology by which to incorporate non-mainstream or more diverse voices into tech policy and what we do is we take we sort of make our best effort to create a product like a white paper and then we identify plausible non mainstream stakeholders who will have thoughts about it and then we give it to them and say what's broken and then we have a methodology by which to incorporate the feedback into the tech policy and to Amanda's point not only does this hopefully in some Habermas Ian sense make it more you know democratic and conclusive but it dramatically improves the work product even at the level of like technical definition for example very quickly our technical definition of augmented reality said that augmented reality is where use you know you basically add a layer of reality information on to existing reality and we ran it by a disability panel that said actually for us often and we'll be substituting for a missing sense that actually then changed our policy recommendation so for example we were saying hey maybe we can have a system where augmented reality turns off in sensitive areas like the boardroom or the bedroom or you know or something like that or the bathroom but you know of course we changed that conclusion because that would be to take away a reliance that a particular group had at a moment that they were most vulnerable right so this is not exactly responsive but I think we need to have developed methods like this and we'll be publishing a long handbook on how we do the diverse voices project soon I think that consent question is an important one and I'm hoping that we can circle back to that too later this afternoon we're right coming up in our last couple of minutes but because we had such a long queue what I'm going to do is allow the two more people just have very quick questions if you can keep them short to the panel and particularly if they have short answers that would be fantastic Mike Antony Heather off thanks very much I'll try to be quick I guess the one thing I wanted to put on the table especially in a context of rights and liberties is sort of the US and American domination of notions of tensions between envelope rights and collectivities and especially around you know I think we don't want to necessarily just look to the Fourth Amendment or we don't want to look to American flavors of IR B's because there's so much broader viewpoint I'm thinking of sort of indigenous cultures knowledge management practices around the world in a lot of different ways and given that a lot of AI systems are sort of emerging from and dominated by American corporate entities I just want to make sure we don't lose the idea that there's lots of different kinds of individual rights not just attention between individual and collective 'ti and there's lots of different kinds of collectivity as well so it's a it's a rights colonialism as well as a data colonialism it's a great comment Heather last word yeah so on the the the idea that Patrick had brought up about you know kind of crappy data and you made a variable mitad variable bias I'm wondering if well use that selection bias I actually think it's a no mitad variable bias but I think what we're doing in a lot of this is what I've heard from out the panel to try to link everything right is what we're looking at is that we have these discussions of it's just math right it's just math and as long as I put in numbers I get out something that's going to be valued and completely not value loaded and what I'm wondering is is this because there is not just a bias in the data where we're selecting the data but who were actually selecting to process the data that they don't actually understand that this is a social phenomenon that there so it's a social science perspective yeah and then is there a need as an intervention point to talk about research design from critical perspectives and instead of just saying you know okay this is how you run your regression this is how you build your algorithm and actually saying no actually we have to start kind of questioning the positivist approach to it's just math I love that and what I'm gonna do is just to give everyone the panel one word on that because I think it's really important I want to begin with Sarah because of course you've been studying how people look at these systems without necessarily thinking about where the data comes from sure yeah so I think that the data analysis practices is really important just one super quick example is there's this one federal system where you can see how many times someone's name has been queried in a particular criminal justice database and so when I asked a detective in robbery/homicide why would you want to know how many times somebody's name has been queried in a database he said you know just because you haven't been caught it doesn't mean that you're not guilty and so if you're not doing anything wrong why would the cops be looking you up for you many times over the course of your life that's a good one Patrick I would distinguish really I think it's really useful for us to distinguish really clearly between the use of models for prediction and the use of models for inference when we're worried about inference we care what the predictor variables mean we care what whether they're measured carefully and we care what their weights are and they're something like omitted variable bias is gonna be really crucial and we really care what they mean if we're doing prediction we actually may not care at all and we may not care if it's completely arbitrary or badly measured all we care about is the quality of the prediction and in that case that's when selection bias really becomes crucial because at that point if we haven't sampled the population that we intend to predict on correctly our results are going to be wrong and wrong in these curious and uneven ways so I think it's useful to think about why we're building models in order to assess them know it short responses for my last three panelists very quickly it's my intuition that there are very few interesting problems that can be resolved today by reference to anyone when and so you know we need people in machine learning we knew people in statistics we need people that do polling who are obsessed with making sure that their samples are representative and we need all those folks in in dialogue with one another in order to solve these problems I think there seems to be perception that not only are the as the answer to the question we care about just math but it's also somehow magic that is given this data you I can you as a machine learning person should be able to answer any question that I have to ask I'm just just given assets as a single data set and so one thing that we try to really emphasize when we work with our project partners is you know this is this is the limit to the sort of infrared fill answer you can get out of the data you're giving us quite often someone has a causal question they give us a data set that's purely observational and they want to know what's going to happen in the future we tell them you can't actually answer that question given this data or you know you just give us a data set without any question there's no good way to generally answer all you know lucrative questions in the future I think we're at a pivotal moment and we don't recognize how pivotal it is and for those who've heard me talk before you're gonna yawn and say oh god he's about to talk about the blue baby experiment but after World War two at Johns Hopkins there was the famous blue baby experiment which led to heart surgery becoming a clinically accepted intervention before that it was designated NOLA Tanqueray which means unhand them or do not touch we are engaged in clinical and experimental without any clarity about what constitutes clinical artificial intelligence and what constitutes experimental and thus because we do not have a concept of unhand them or do not touch we have no safety break we have no red button and we have no bright lines so I'm not saying don't do it I'm saying it's time for us to call things by the right names and that is a process that involves everybody beautiful thank you for that important final note and thank you to the whole panel if you could thank them all [Applause] and now you've made it to the last import session of the day which is going to be ethics and governance Meredith indeed so um to be real about it nothing we've spoken about doesn't touch ethics and governance but right now we want to hit it on the nose we want to get perspectives from across disciplines history art philosophy research and just look at what are the limits and opportunities in our current definitions of ethics and governance so I would like to welcome to the stage Molly Stinson from Carnegie Mellon University to get us started I have 57 slides I have five minutes let's do this so when we look at AI and we look in the media we see a lot of things like AI is the new black AI is the new UI AI is the new electricity and we might also hear things like the future of computers is the mind of a toddler if not a blue baby or it might be a disembodied head people like Ellen musk we see him because techno tech moguls are declaring an era of artificial intelligence and I am even Adi the grid AI is a web design company that offers AI services and if you can read the small print it's really offensive Golan Levin had a really great thing to say about how awful it is and I also have this slide similar to Simone which is when you google what AI is this is what it looks like but the thing is the AI isn't the new anything because AI isn't new and if we look back to its history we could look there's some there are some definitions of AI that even harken back to the first the first century AD but if we look at where it began to develop after World War 2 in the 1940s and coming out of cybernetics we see a number of things it was John McCarthy who of course coined the term artificial intelligence in 1956 talking about making machines do things that would require intelligence if done by man he convened a group of researchers at Dartmouth the summer of 56 to work on the agenda of what would be the platform of artificial intelligence was people including Marvin Minsky Claude Shannon Raza Ashby of course McCarthy himself Frank Rosenblatt herb Simon Alan Newell and a number of others these are still a lot of the platform of what we consider AI today we talk a lot about learning when we're talking about AI and notions of learning precede the coining of the terms so Alan Turing in 1950 wrote about simulating a child's brain rather than starting with the brain of a did of an adult or looking at blank pages thinking of blank pages in a journal that one might fill in in 1952 when Arthur Samuel coined the term machine learning and it was in order to look at simulations of things if done by human beings or animals would be described as involving the process of learning how did he train it checkers and just as an aside the the visuals and these articles are just unbelievably beautiful neural networks also have their start in this time period in 1958 Frank Rosenblatt created the perceptron and it unleashed a media storm the New York Times wrote about it and said that it would be an electronic computer that experts predicted would be able to walk talk see write reproduce itself any conscious of its existence this is of course Navy funded I'll get back to that in a second this is something though that a decade later that Marvin Minsky and Seymour Papert wanted to see an end to be put to they put they published a book called perceptrons that argued that this line of research was effectively specious they incorrectly approved perceptrons wrong and as a result cut off funding to an entire area of connectionist AI for decades they later issued a an apology and I want to point out that we are sitting in the legacy of artificial intelligence of course with someone like Marvin Minsky who the AI lab here at MIT in 1969 and of course in the predecessor to the Media Lab the architecture machine group which was founded by Nicholas Negroponte II in 1967 and which folded into the Media Lab when it was founded in 1984 one of the things that the architecture machine group did it was a group of architects and electrical engineers who worked closely together tinkered with technology and built interfaces for artificial intelligence one of the things they did has worked very closely within the same funding structures of the AI lab in AI directives such as with the seek show or the seek project at the software show this is a dribbles blocks world a set of blocks a stacking arm a video I that Seymour Papert and Marvin Minsky had invented and a set of gerbils that did what gerbils do which is make a mess as Ted Nelson said about this our bodies our hardware our behavior software seek was a disaster in certain ways not least because it killed the gerbils there are other projects later on as the funding climate for AI changed like Aspen movie map or mapping by yourself which looked familiar to us today and are actually picking up with command and control imperatives in the AI lab here at MIT at that time I think the Nicholas Negroponte II really understood what it was to be working with AI in 1975 in his book soft architecture machines he said that I strongly believe it is very important to play with these ideas scientifically and explore applications of machine intelligence that are between being unbelievably oppressive unimaginably oppressive and unbelievably exciting and this is about agendas because these men set the agendas for AI and this this funding is a matter of Defense funding so someone like JCR Licklider whose notion of human-computer symbiosis was the operative idea for interactivity for decades and still is we could think of Karen Levy's talk earlier today someone like JCR Licklider was Nicholas Negroponte ease mentor a professor here the first director of the information processing office information processing technology office at DARPA and also a private contractor with bolt Beranek and Newman similarly the Office of Naval Research funded the agenda for for AI research particularly through the vision of a man named Marvin denna cough who Marvin Minsky once referred to as the grand old man of AI this is what Paul Edwards has called the closed world the circulation between the military industrial compound complex and keeping knowledge within a tight group of people who enjoyed working together and funding so what's at stake AI isn't the new anything but what's gained by saying that it is AI is about building worlds it's about modeling intelligence in fact it's about modeling ourselves AI is about research funding and it's about Defense funding it's about making new markets it's about capital and it's about power the question here I think that we're all looking at today is what do we want AI to be about and what do we want its agenda to be I'm tired of seeing well well we missed her for a second I'll bring her back although the mob will stick with Oprah there are the cyborgs with jQuery across their faces everybody gets some machine learning but I think what we need to be doing and I think what we are doing here is we're looking under the lid we need to look back in time and open up the agendas because AI needs us thank you Thank You Molly it was an excellent five minute fifty slide summary so now I'd like to welcome to the stage Kate darling from the MIT Media Lab [Applause] okay so our prompt for these lightning talks today was what is the key issue that we should be focusing our collective attention on right now and I think that this community has done a really good job of voicing some of those issues so I do want to use my five minutes to just throw something in that I don't think is a major problem but I think it's maybe an additional challenge that we are facing in AI governance and that is the challenge of public perception so as you all know there is an incredible amount of hype right now in the AI space and you can tell that there is because people in even tangentially related fields have all changed their Twitter BIOS to have the word AI in them and the media is all over it I mean everyone wants to talk about AI right now and that's really great it's really great that it's getting so much public attention but you know compared to other technologies like safe crypto currencies I think that people you know with cryptocurrency people have a hard time wrapping their mind around and what this actually is but with AI we don't have that problem we all know how to imagine AI because we've had this concept for a long time and we've had it in particular in science fiction and pop culture right these super computers that get really smart and that have human characteristics and that might become self-aware at some point and it's not that people are stupid and don't know that that is not you know complete accurate depiction of what's going on today but I think it's really unhelpful for all of us that we have this cartoonish image in the background of all of our conversations about AI and in some ways it's worse than not understanding the technology at all because I really get the sense that people tend to overestimate or even underestimate the capabilities of current a high based on their sci-fi understanding of it and this you know this fear or fascination with creating human level intelligence is I think as K and others have pointed out really distracting us from some of the issues that we need to be focusing on right now and some of the experts in the field are not actually helping with this so this is an actual quote from a well-known AI scientist who's not here today describing to the public how defines dqn system learned to play Atari video games it was like a newborn baby opening its eyes for the first time and then he says so if your baby did that woke up the first day in the hospital and by the end of the day it was eating all the doctors at Atari video games you'd be pretty terrified that's not helpful okay people are already weird enough about how we perceive these systems you don't need to encourage the anthropomorphism even further we are suckers for ascribing life to these machines and hence we we also do other things like we think they're better than they actually are we trust that algorithms aren't biased and that they're neutral where as we've heard today many times over that they are actually not also you know Madeline Elish and Tim Wang have done work showing that we we trust these systems and and if there's a human in the loop and a mistake happens we will assign more blame to the human than to the system and we're also starting to see companies effectively distance themselves just a little bit from responsibility for certain outcomes by saying oh you know the algorithm did it so we're still trying to recognize and understand all of the irrational ways that people over rely on these systems and trust them too much or trust them too little and why but the main point I want to make here is that the flawed ways that we think about and project onto AI can actually affect governance so people like Ryan Kahlo or Neil Richards and Bill Smart have all made the case that the ways that judges for example in the legal system understand and view robotic technology for example comparing robots to humans has a real impact Court decisions and a lawmaking and leads to some very problematic outcomes that are hypocritical and don't make any sense so it may be important to try to understand the irrational ways in which people view and treat algorithmic decision-making and the impact of that on the public discourse so we need to recognize for example that we are biased towards anthropomorphizing these systems in our perception in our language and we need to be vigilant about these biases and we need to try to focus the public attention on questions that matter and we also need to be transparent about the actual capabilities of the technology and translate this knowledge for the general public and for policy makers and so I'm I'm just really thankful to be here today in a room full of people who aren't only doing great and really interesting and important work but who are also intellectually honest and really care about getting the now in the future of artificial intelligence right thank you okay I would now like to welcome to the stage Rob Sparrow from Monash University so I believe that the amongst the challenges raised by AI now are a philosophical challenge and a political challenge and I'll try to say a little bit about H and how I think they're connected the philosophical challenge is to better understand the nature of ethics and in particular what it would mean to what it means to talk about ethical AI the political challenge is to sustain indeed even to imagine democracy when the design of these powerful technologies is increasingly the means whereby political power is exercised achieving a better understanding of the nature of ethics is a key challenge because debates about ethical AI are characterized by an equivocation between about who needs to be ethical is it the people who design AI who need to be ethical or do we need to be building ethics into the machines is it the machines themselves and a significant proportion of both popular and academic discussion is concerned with the second of these possibilities and I think that's a mistake but that discussion is made plausible by engineers and indeed by philosophers operating with an impoverished account of what it means to be ethical if we if we're utilitarians then ethics is a matter of performing complex calculations so that we don't which actions will maximize some good such as human happiness but machines could be better calculators than we are if we're Canton's then ethics is a matter of applying the rules in a moral rulebook and AI may eventually be capable of doing that better than us at least more consistently than us but is if as I've argued in a number of places ethics is a matter of being properly responsive to the demands of human relationships if our inspirations are Aristotle and vegan Stein rather than Bentham and can't then ethics is closely connected with our embodiment and to be ethical as to behave and feel and react in certain ways that only creatures with a particular biology are capable of and if I'm writing these thin machines can't be ethical now why does this matter it matters because talk of machines being ethical distracts us from the people who are really making the decisions that determine the decisions that machines make and whose choices that we do need to be evaluating ethically and in particular distracts us from the way in which the design of AI systems involves the exercise of political power which brings us to the political challenge to illustrate the nature of the political challenge I'm going to ask you to entertain a small flawed experiment imagine that a bearded Marxist revolutionary has hijacked the fox news broadcasts and appears on every television television in the country waving an ak-47 and a manifesto and in that manifesto he says all and only the things that we hear about the impacts of IAI so he gets on television he says his organization is going to put 30% of the population out of work it's going to censor the news that you read it's going to determine who goes to the prison who gets a house to live in it even gets even determines who you get to meet and who you get to be friends with regrettably he says this will also require putting the entire population under extensive impermanence of violence and it will also involve me becoming fabulously wealthy so imagine how people would react not to that circumstance I don't think anyone will say we have fabulous give that man a TED talk actually what they'd say what that's say that's appalling that's an appalling exercise in shaping our future without consulting us but that's what we're being asked to accept when it comes to the introduction of AI and what the experiment reveals there's a general democratic deficit in our thinking about technological change and this is it's it's technology and not just AI here moreover it seems to me that three of our traditional solutions the exercise of political power by private actors and seem no longer to be adequate our consent the notion that we can you know resolve this through individual consent fails due to the biggest nature of these technologies the idea that we can just ask the state to regulate is problematized both by regulatory capture and the strong integration between the manufacturers the designers and the state and also the hollowing out of the state in our current political circumstances and unfortunately socialism the universal basic income is threatened by the collapse of the social forces that might bring it about so that it seems to me is the political challenge to imagine new ways of returning power to citizens and it's not just an intellectual challenge but a practical challenge and I hope it's a challenge we will all take up all right so bring us home and maybe answer some of those questions artist Trevor Paglen yeah my talks not called that anymore it actually doesn't even have a title I wanted to talk today about the pressing problem of Soviet Socialist realist painting because I think with this kind of painting is that there's a kind of common sense built into these kinds of representations there's a kind of underlying philosophy of transparency of a more or less causal relationship between representations and things the theory here is that representations are simple things and then artwork should reflect that simplicity and this is a kind of aesthetic regime that I think we're seeing a kind of strong re-emergence of at the moment actually this sort of thing now obviously one of the major kind of a big application of AI is being able to analyze images I mean this is what that sort of thing looks like a green jacket a white horse a man on a horse etcetera etcetera etcetera and this is kind of thing is evolving pretty quickly the ability to kind of interpret images where's my next slide there it is okay here we are this is evolving quickly in where newer systems are trying to not only recognize objects in scenes and images but being able to associate them with metadata and like meta metadata if we will you know in other words look at a meal and not only know what that meal is about how many calories are in it look at a picture of a person to be able to estimate you know how what's their body fat percentage this sort of thing and so there's a kind of aesthetic genre going on here that in some ways echoes so list realism but we can call it I will dub it today autonomous hyper normal Megha Mehta realism it does echo this kind of socialist realism kind of stuff but here's the problem socialist realism sucks it really sucks it's a really limited aesthetic and conceptual palette and this is a feature of the genre this is not a bug and the reason for that is socialist realism is designed to make certain styles of thinking impossible it's a means of centralizing power it's a means of creating literally a kind of aesthetics of totalitarianism so when we look back at this autonomous hyper neoliberal it has a lot in common with that but I think it's it potentially actually a lot more insidious for a lot of reasons for that part of it is because this kind of it involves the autonomous attribution of meanings in the service of capital right it's a kind of hyper realism that's designed to see and to only be able to see aspects of images that can be turned into capital and what's more it operationalizes those meanings you know at the most benign version of that you know your pictures might be used to try to sell you stuff but it can also be that your pictures can be have enormous consequences for your ability to get a job get credit be able to get insurance and so on and so forth and in this kind of autonomous realism as many other people have pointed out he who controls the training sets controls the meanings of images and we have no or very few ways of trying to contest the meaning of those images and this of course represents an enormous centralization of power now what's more this happens invisibly done by proprietary systems that produce proprietary meanings in ways and places that are actually totally opaque to us and this is one of the weirder aspects of this emerging art genre if we want to call it that that it's a visual system that lives alongside us and it makes persistent and kind of molecular interventions into our lives but at the same time is largely invisible to us this is part of very famous genre of images that Marguerite made that even called the treachery of images and the point of these paintings is to kind of show a tension between a kind of unexamined common sense that says this is an apple and the actual Fran's ability of meanings themselves and I think this is not a semiotic stunt this is a really important thing because the freedom to say this is not an Apple is the same kind of freedom to say I am a man or I am a woman or I am neither man nor a woman but I am a person it's the freedom to say I'm the Palestinian it's the freedom to say black lives matter to say I am beautiful and its most basic it's the freedom of self-representation that goes hand in hand with self-determination and this is one of the many many reasons why this advent of autonomous hyper neoliberal met a mega realism scares me so much it's the fear that it robs us of our ability to determine ourselves by robbing us of the ability to define and the meanings of images for ourselves thank you so much thank you Trevor and there may be some questions so if you just want to take a seat all of the panelists what a note to go out on I'm gonna start scanning for hands I see one I see another um all right so Jessa and let's assemble the panelists great for those talks um so I I have a fear that this question comment is gonna come across as more combative than I need it but I want to talk about sci-fi a little bit because I totally agree that mainstream imagery of artificial intelligence can reinforce some problematic values and perceptions but I was thinking about how we could tie this to questions of representation and I'm wondering if science fiction can also be productive perhaps less in terms of how we imagine you know figuring or representing or visualizing artificial intelligence and more so in terms of figuring our relationships to technologies so thinking about the role of science fiction less in the first degree order of like how how it looks and more about how our relationships to those unfold I'm saying this this one who just read Octavia Butler's parables of the sower and was so much more inclined to become a proper than any prepper website could ever make me you know so there's like so I wanted to think about in terms of questions of representation sort of thinking maybe anthropomorphism isn't the problem it's that the an throws that we imagine are so homogenous and that if we were imagining different bodies you know thinking about our earlier panels or imagining animals in a positive sense like why do they have to reproduce us why can't they reproduce other things so just wondering if those might be questions that we could we could ponder oh absolutely and I should say I love science fiction I think science fiction should be required reading for everyone in school I think it really opens your mind up to you know different possibilities and and has exactly the effect that that you say so yes I think it can be very helpful I I sort of I guess Mormon also as you said kind of them the mainstream depictions of AI and robotics and sci-fi I'm thinking you know terminator I'm thinking major Hollywood movies I'm not thinking you know the science fiction that thinks deeply about society like Ursula K Le Guin or any any of the other stuff that that is thoughtful in that way and it's just something that I've noticed in in robotics in particular that people are so primed through science fiction have a very specific view of what the technology can and can't do and they're not opening their minds the way that they should so I I would love to see more of the good science-fiction enter the mainstream to open people's minds a little more great so we have quite a queue I'm gonna call up Peter and then Kathy and I'm gonna try to call on people who haven't asked a question yet and then get to the others at the end but if we can do two at a time because people are obviously inclined to questions so Peter thanks so I really like that ending note that Trevor left us with about self representation and self-determination I think it resonates a lot with this question of collective ethics that we had in the earlier set of discussions but I'm wondering it's to the extent that sort of historically marginalized groups were identified by being physically proximate and say I got Oh or so so recognize themselves as being of a class of a gender of a race of an ethnicity of a religious group and now we have these algorithms that are classifying us in these inscrutable ways that are not transparent and are selectively then disproportionately treating us in different ways how are we supposed to form a collective of a political will or political identity or a self to represent and to determine when we are being cut up as it were into all these different kinds of inscrutable groups and so how do we get political social action in that kind of framework great question and then let's let's get one from Kathy and see if the panel can hear I feel guilty because I did ask a question before but this will be quick um just a thought experiment which I've been playing with I wanted you guys to think about it I feel like the language of hype around this stuff has gone from big data objective to AI anthropomorphised and for me I'm wondering if you can see that as a sort of doubling down of the thing where the people building the algorithm say this is not me with an agenda this is something else the first one was there is no agenda because it's objective and now it's if there's an agenda it's the AI making the the agenda and I'm wondering how do we how do we make the maker of the algorithm accountable to your to your position so I think there's something about a lot of this about subjects and objects and again looking toward humanities-based approaches we might want to consider about the objectification that happens throughout right so when we begin to look at representation in different modes of representation then we begin to have the possibility for subjectivity but if we stick with visualization or if we stick with algorithms speaking for algorithms and sort of fetishization of the black box I think we begin to step away from that possibility I mean yes I was trying to suggest that the some of the literature about ethics and III which there's what we need to do is build an ethical AI is precisely a sort of conscious a temperature place attention from the people who are wielding power over everyone who's engaged with those systems and that power is wielded both in shaping the systems and shaping technological trajectories by investing in certain forms of research but also in the social relationships that are developed and in the affordances of those technologies and the way in which those technologies reconfigure relations with between people now I think we need to understand that as an exercise of power that should be within the purvey of a democratic politics we need I mean there's a general framing of technology now is we are going to solve a problem for you okay instead of seeing the problems that the world faces as things that need to be addressed collectively we're gonna hit look you know wait for the engineers to do it and and even if they got it right that would be problematic because of the power relations now how we shape new forms of subjectivity in that context that's tricky because they do simultaneously individualize us as they categorize us which is why I think the kind of new dot communism of the universal basic income is kind of interesting because it's not as though we haven't had people of work forever I mean you could make an argument that argument has been made for decades and at the very moment when you see this massive collapse of the trade union movement there's we being solved this story and it's a different story from different people that you know it's gonna be okay because you all get a crust and again you need to see the power relations about being given a crust in that context I don't think there are easy answers here when it comes to forming subjectivities they will be formed though I mean people will self-identify with the ways in which they're classified and they will come together under those classifications but the fact that people are going to be geographically dispersed and dispersed across the industries it's going to make that problematic in in ways that we'll see them play out we had Nathan Stephanie David and if we could take short questions and in a clump and then we'll try to get some others and after that thanks I appreciate that we have kind of a range of people thinking about both art and the politics as we as people proposed for example ethics in machine learning as well as these broader political movements I've been struck by how the move towards ethics tries to keep power in the hands of people who are already building the systems and how important political efforts are so I'm curious to hear if there are kind of the opposites of the Russian constructivism what kinds of art or representation what kinds of stories what kinds of political narrative might we turn to as we try to build the movements or institutions or policies that allow us to you know pursue justice in these in this time trevor maybe you want to give that a shot I don't know I think there's a temptation that I see further be a kind of new Leninism right like appropriate the means of like automation or something that that I'm kind of suspicious of you know I just I don't you know there's still huge power at power relationships in their that that I would contest I really don't know I think it's a hard question I really do and I even the idea of ethics in this conversation actually feels a little bit uncomfortable to me because it implies that you can almost kind of have a ethical structure on top of capitalism kind of in relation to this and maybe you can kind of around the edges a little bit but I don't think that's where we're at you know especially with these kind of systems evolving so much faster than the regulatory frameworks with around them which basically don't exist even in Europe which is much better than the US on this stuff they're still you know thousands of years behind in terms of technology time and so we even wonder whether framing this is a conversation about ethics is a good framework and a place to even start I don't talk just very crudely like let's talk about capitalism let's talk about neoliberalism let's start from there I want to add two things to that one thing is that right now it would be difficult to have the means to rest the means if you wanted to because there is no abstraction layer that an average person has access to so rest the means all you want but you're not making the data you're not being counted and you don't know what to do with it once you've got it and the other thing I think I forgot what I was going to say I also wanted to suggest there are other mechanisms and means there are things like speculative critical design which often is ham-fisted and bad but there's also some promising directions there in terms of designing and putting together different visions for futures and the notion of ethical robots has baked into the beginning of machine learning Warren McCulloch coined that term in 1956 wrote a series of papers about it and so there was some thought to ethics back then although I can't really speak right now to what that might have involved in detail at least I think one like tweet length point that we should continue to hammer home was I think was it Alex garland who did ex machina yeah so he said in an interview he said AI does not have its own agenda the people developing the AI have an agenda I think that's a really important thing to hammer home where the underlying structures that the people developing the AI are in the service themselves of have an agenda for tweet I mean just very quickly it may be that the alternative ways of making the future simply aren't about AI at all mass migration of people's you know the Arab Spring for all the you know troubled history of that I mean there's this stuff happening politically at the moment it's is simply not about the relationship between engineers and populations and and societies are at least not from below and we need to keep an eye on that as well when thinking about this stuff the future is not just going to be made by by engineers it's also going to be made by people who struggle in every way they can to maintain a decent life in the face of all these technologies that are being rolled out it's a political contestation and one end of that may not engage with AI very much at all so we're running up to time so I want to get two quick questions from Stephanie and David and then maybe a tweet length comment from the panel and then we're gonna move to lunch so it was really provoked by Rob's terrorist on TV example and so I would just want to ask what is it about AI that makes us stupid like we had really popular resistance to the stopwatch or the assembly line or enormous databases in the 60s and 70s but there seems to be this democratic deficit around AI in particular so what is it about a I that is turning off popular resistance and I have a really practical question when I think about ethics and governance and accountability and agenda setting I think about access probably because I'm a historian I want to know who is making decisions and why are they making them when are they making them what do those infrastructures look like and Molly pointed out of course it there's a very long legacy of Defense funding and with it infrastructures of classification and censorship people like me spend a lot of time filing Freedom of Information Act requests trying very hard to unclassified declassify records but increasingly also of course so much of this work is done in industry and is proprietary not just the data sets and the models but also the traces and records of how decisions are being made and who's making them and within what kinds of systems and communities and I'm wondering if any of you have any thoughts about how some of your concerns about the political dimensions and ethical dimensions of AI might map on to our thinking about where the records and the traces and the access might come for people like us in this room who want to ask those questions so the good news is we're gonna have a lot of time for discussion after lunch and we are about right at time so if any of the panel wants to offer a quick answer to those but knowing also that we'll have time to talk over lunch and beyond I mean I just want to totally agree with that and that I hope we get to talk about this this afternoon it's also mentioned a little bit in the report draft I don't know if you looked at it so I mean that's part of why I didn't talk about that is because I'm I'm assuming it's gonna be part of the larger conversation today so just in response to the first inquiry I mean I think one of the key questions here is about the causal relation between the introduction of a whole series of technologies that should be seen as part of the history of III and the massive depoliticization of the population that has occurred across the same period and it may be that that's contingent that it's just about the collapse of the the trade union movement or the consolidation of capitalism or it may be that these technologies are themselves producing new forms of subjectivity that mean people are people just go oh yet look there's something on Facebook that's terrible whatever and that it seems to me a really important question in this debate at the moment great so now it's time to lunch you details we're gonna be back at our seats at 2:20 and then we're gonna start the group discussion part and that's where we get to try to digest the huge amount of input that our brilliant speakers kind of gave us today and think about what we need to do so what are the research agendas what do practitioners need to do to contend with these issues you know what advocates and civil society folks you know how did those agendas begin to shape around this smorgasbord of pressing questions so let's think about that we want to come back we're gonna have a long discussion and this is also where we say goodbye to the live stream so thank you for joining us that the discussion will be offline and candid so we want people to be able to speak honestly and we're really excited there again if you have notes provocations questions you've written on your posters we want to collect those we want those to be fodder for that discussion and remember our interactive wall what is the most urgent thing this community needs to do in the next 12 months and with that please go get some food we will see you back here promptly at 2:20 I believe it is 2:30 and before that I want a huge round of applause for our speaker is that with absolutely fabulous [Applause] 2:30 you 