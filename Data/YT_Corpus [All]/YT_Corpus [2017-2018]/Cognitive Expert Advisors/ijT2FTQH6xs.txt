  LARRY: Let me just set this up a little bit. I thought it would be kind of interesting to go back and look at some famous quotes that we probably have all heard of, about the impact of any one technology. And my point for sharing these is that it's really, really difficult to expand into 5, 7, 10, 15 years and predict what that might be. But there are a couple really famous one. Ken Olson, at Digital Equipment Corporation in 1977 said, there is no reason anyone would want a computer in their home. I think, what he really meant was we're going to want one in our pocket and in my baseball cap and in my tooth brush, right? The second one was Robert Metcalfe, founder of 3M in 1995, who said, almost all of the many predictions now, being made about 1996, hinge on the Internets continuing exponential growth, but I predict, he said, the internet will soon go spectacularly supernova and in 1996 have catastrophic collapse. Interesting. In both cases, these were monumental time shifting, time changing technologies, and even some of our senior leaders didn't quite see it and get it. What we've been talking a lot about in our tech network is where's AI? Is AI one of these also see change type technologies that can begin to influence and change things in a fundamental way? Over the years, for a lot of you, I'm looking around the room who've been involved in instructional and educational technology. Back in the 30s we said, it's radio. Radio is going to be the thing that's going to change education. And then it was going to be instructional television, that's surely going to be it. And then things like, the personal computer and the internet actually came along and began to make some of these changes. Is AI also positioned to make that change? I'd like to start with Beverly. Beverly is a researcher deep into machine learning AI, both in the development side and also exploring trends in understanding what's happening in this field. And I've asked Beverly to share a little bit about what's the promise. What are some of the things that she's seeing that are really exciting, and exciting her in her research domain? And sort of get us excited about where things are going. Let me start with you, Beverly. BEVERLY: All right, so as-- thank you, Larry, and thank you for inviting me. As he has said, I've been researching exactly what AI can offer, especially to the K-12 population. I do a little bit in higher ed. And I'm going to just go quickly over the field, not in detail, of course, but I don't think it's going to end anytime soon. And there's a person, there's a brother's named Durfey-- I think is their name in California Philosophers. They write a book about what AI cannot do and one-- it said, you cannot build a chess set that will ever win. And then five years later they have to rewrite the book. And then five years later, they rewrite the book. I don't think it's going to go away anytime soon and we cannot see the limit of it right now. OK. And I just want to say, that digital learning is different, and this is not with AI, but in general. And what you see on the right is the usual way. This was the old way. This is the new way. What's the difference? Well, I think that the traditional way is very conventional speed. You know exactly how to read a book and how to go from one worksheet to another worksheet, but this guy is doing what I call, twitch speed. Everything's changing. You can get a lot of tremendous amount of information. The information is going to be parallel process. It's going to be multi-modal and graphical and it's going to be connected, he's going to work with all of his friends, not just stand-alone as in the old mode. And it's going to be more active than passive, and it's going to involve perhaps, reality and fantasy. And in the United States at least, this guy is doing these kind of things. Actually, the pictures are from kids in Japan. They're doing this kind of work outside of school. And guess what? School is exactly the same as the way it used to be. It's still-- no wonder students are bored in school, because this is what they're still doing. It's amazing how so many industries have been impacted by AI, including lately, the hotel industry, the cab industry, and yet, still education is not impacted for the most part. What I'm going to do is just quickly go through a little bit about what I think intelligent tutors are. How we are looking at emotion in learning. How we're looking at collaboration in learning, using AI to do that. And then I'm going to look at Big Data quickly and then a little bit about the perils. I just want to mention to you that for artificial intelligence, there's enough research out there to recognize that learning can be improved this much. This is conventional learning and as you know, the middle bit of your classroom will get C's, a few kids will get A's over a here, and a few kids will get C's and D's. And then there's something called, mastery learning, and we don't have to go through that, but some people might know what it is. And then there's one-on-one tutoring. And a long time ago, Blum discovered that these folks could learn, the folks who have a teacher, who's right there beside them, and the teacher recognizes what their knowledge is what their misconceptions are, and then that teacher can push those kids up about two sigma, is what it's called. And that is moving them that far up in terms of their grades. Well, what's happened now, is that these intelligent tutors that I'm going to talk a little bit about, have now, pushed people up that far. If you look at this book, which is [? Coulic ?] and Fletcher, they took a 50 different studies that used AI and there were certain criteria, but they found that generally, and it was by the way, across-- I haven't got it here, but it was across 7 or 10 countries and different continents, they found that the artificial intelligence systems could move children from a C grade to about a B+ grade. Pretty uniformly. And again, in different countries and different topics, et cetera. OK. And then I'm going to tell you now, about a system that we have built, just to give you some context. The system has little animated characters in it. It's going to help students learn mathematics. It was built at UMass and what was to Polytech. And it's going to look a little bit at a motion in learning, a students emotion and whether collaboration helps. In terms of student emotion, what we're saying, is that what should a student-- what should a system do in the moment when students are frustrated bored? And you might ask, how can a computer ever know that you're frustrated or bored? And the fact is, it's quite straightforward. Many people have already done this. And the way we used to do it is to put sensors on the hands and watch how they moved in their chair, and watch if the head goes back and forth or if they frown. We now, do it with mathematical models and we find the models are equivalent to doing it with sensors. We can detect when the student is frustrated by everything, it wants to leave, wants to stay. The question is, what to do? Do we increase challenge? Decrease challenge? Provide effective scaffolds? Suggest peer-to-peer collaboration? Et cetera. We have been in a process of A, discovering the affect of students. And we do it in three ways. One is to ask the students how they feel. Are you interested now? Are you bored? Do you want to get out of here? And then we do it with a mathematical model and we have done it with sensors. We're doing a fourth way now, and we put all these together to see if we can make a prediction. The fourth way is we're using computer vision to-- as long as the student is sitting there solving a math problem, we're taking a video of him or her and watching the head and watch whether it falls down on the table, whether it goes like this, et cetera. And trying with all four of these ways to determine the emotion of the student. And then what do we do? What we have done, is students who-- actually, I'll just go right to it. Carol Dweck has put out a bunch of statements about the best way to impact a student. And it's to talk about-- do you know that when we learn something new and our brain actually changes? The neuron and the configuration and they form new connections. We have found that positive results for students in terms of their learning of mathematics, in terms of their interest in mathematics, if you give them these kind of sentences. We have given them special sentences. There's about 50 of those growth buy-in sentences that I just showed you. And we've given those-- a lot of systems give this, very good, you got it right, or very bad, you got it wrong. Students seem to get distracted and angry when you give these kind of answers. And they really improve when you give these kind of answers. And then there's empathy, which is sometimes-- I don't-- I get frustrated also. So the little character is saying that. And what I can say, is that certain-- we've teasing apart now, who responds to which comment? High achieving males do not like any of these comments. They don't like a character. They paint ears on the character and [INAUDIBLE].. And it's because they're finished. They want to get done with what they're doing. They don't want any part of it. We find women enjoy talking to the little character. Students with disabilities enjoy, et cetera. In other words, we're getting to the point where we can see a difference and then apply material. In summary, we found significant correlation between a student who received more empathetic messages and who were more-- we asked them, are you confident? Are you patient? Do you have interest in this? And do you value math knowledge? And we have high correlation between more of those messages and them saying, students saying, we value math. And students who receive more success failure messages, tended to make more mistakes, to be less learning oriented, and stated that they were more confused. This is a thing that even if a teacher in a class of 30 can figure out this, they can't do much about it because there's 30 people in front of them. Let me just quickly show you what we've done then in collaboration. We don't like kids just talking to a computer. We want them to work together. We have asked them to work face-to-face, peer-to-peer, with a person next to them. And we know who they're sitting next to because we know who's in the room. And we find that collaboration increases the students interest, response to students negative affect provides a boost in their math learning and yields higher math performance. And let me just say, that the computer has-- the AI component of the computer has to say, to one-- the next activity is a special one, you will be working with Wendy. And Wendy gets a similar message that you will be working with Amy. But the system has to watch these people to make sure that this person is finished so that that person can start working with you. You can't just say, work together and then you're interrupting someone. It carefully invites the kids to collaborate and then gets a reaction afterwards. Again, using-- carefully using a model of the student and all of the knowledge and all of their affect. In summary here, collaboration increases the student's interest. Peer-to-peer aspects of response to students negative affect. And collaboration proves a boost in students math learning and yields higher math performance. OK. The reason we know all this-- I'm just saying, is because of the Big Data. And I'm going to show you quickly what we mean by Big Data. And I'm not going to go into any algorithms for how do you analyze it. But you'll see how this is quite different from looking at exam data. For instance, something called learning analytics is mostly look at end of semester, end of chapter, end of year results, add them together and do some analysis over it. I'm not talking about that kind of-- that level. I'm talking-- let me just go ahead and show you what I'm talking about. It's this kind of thing. We have from the student the comment-- we ask them what their interest is or excitement and they let us know. We ask them for an explanation. I am kind of excited because I actually feel like I'm learning. And I am kind of excited because I am provided a lot of tools, et cetera. And it's too hard and when people want to learn, they still want to have some fun. These are the actual words they are writing down after we ask them what their emotion is. And what we're trying to do is to analyze what's the relation between the affect that they're talking about and the number of hints they ask for, for instance. Or how quickly they get to a solution. Or are they guessing? Or are they just flipping from one problem to another? I am not feeling excited because math is not fun. And then this is another. This is all from the same system that I've been talking to you about, the math system. And we have gigabytes of data and this is what we talk-- Big Data is not really about the size, but it's about the analysis of that data. And so this is the kind of stuff we're working with. Again, the student is-- somewhere in here, the student is saying, that what their emotion is, but then we have that they begin a problem, attempt a problem, go onto the next problem, et cetera. And so this is the material we're going through and the analyzes. that we're making. Let me actually, just bring it back to-- OK. Big Data helps explain what students and teachers do in the classroom because if a teacher is effective, they are handling this kind of thing. The goal is to get to educations wicked problems, that is performance gaps that produce cycles of underachievement and cultural and racial differences. To identify children with similar learning difficulties. For teaching strategies and gender differences. And actually, these kind of data actually blurs the distinction between teaching and research in education because even though you're teaching, you can be doing research because you're finding about a teachers, about this data. And in fact, what we do is go to teachers and ask them what they want to find out. And we've been asking them what they want to know and then go to that data and try to find it out. This is just a few of the questions that some teachers have asked. How much my student knows, what are their gaps? How can I teach effectively? How can I have students become effective learners? What are the students' motivation? What is the ineffectiveness and engagement with technology? And who are my students at the beginning of the year? Which basic concepts have my students had a hard time retaining? We can go right down-- we don't go just to problem 6 or problem 14. We go to, well, they know about fraction denominators or they know about the tables or they know about Pythagorean theorem. We have it listed by every skill that they're supposed to know. Of course, in math it's much easier than some subjects. How to measure conceptual understanding, et cetera. And this is just the last bit, which is to say, we send out a-- for the teacher a listing that looks a little bit like this where the system has the student's name, which is the first name and then the last name initial. The system has made an assumption about this student, that it's a weak student. The system has seen that this student has skipped that many-- for this problem have skipped this problem. Has given up on this-- on this color has attempted something, but got it wrong, et cetera. The system is making these inferences. Solved on first. Solved with a hint. Solved with help. No data was found. And the system is making a judgment about the student and passing it on to the teacher. The teacher can use it to determine what next problem to do in the classroom or not at all. And similarly, the teacher receives a list of the student's name, the name of every problem that they've solved, the number-- and these are the problems. Pythagoras, XY linear algebra, et cetera. And the grades, et cetera. And the system's inference about that student. And then a mastery list, which is-- these are all the problems that child has solved down here. Each one is a problem. This is the mastery and the system sees that after four or five problems on the same topic, the student really starts to master. And that's a little helpful for the teacher, especially if a student is not doing that well. And then the last bit is that I have been looking at the ethics here, and it's very clear that teachers and schools are not taking data seriously at all. In other words, they hardly have people in IT. They are hardly looking at breaches of data. They are hardly looking at where the data is going. And that's just because of the newness and youngness of our field. But mainly, these kind of questions are not being addressed. Who benefits and under what conditions? Who has access to the data analysis? In other words, the data is all about the student and the maybe about the parents and maybe about the school, but people can sell this data, can pass it on to others to summarize it, and it's not clear where it's going now and there are very few rules about it. We need to define the moral and ethical virtues for super-intelligence. How should analyzes be measured and implemented? Et cetera. And what if the technology does not work? I mean, if you put something in the classroom, it doesn't work, teachers are frustrated, we're frustrated, what's to happen? And that's it. That's my quick story of what's going on in the field. LARRY: Beverly, thank you. BEVERLY: Yeah. You're welcome. LARRY: You've teed-up very nicely where we're going to go with this. That was excellent. Thank you, so much. One of the things that Beverly did in I think, her talk was to look at the potential impact of AI machine learning into the teaching and learning process and shared with us the potential gains that could be had. Renada is up here for a slightly different lens, if you will. And I'm going to guess for the last 20 some years, at least in your career, Renada, you've been functioning sort of the system level in higher education, looking at issues and challenges we have, challenges that students have. When you hear conversations like, Beverly and Jonathan's last evening about the potential of a AI, where do you see-- and I know you're also working on this transformation of education initiative. You've been in this space thinking a lot. What are the kinds of things that come to your mind about how we might be applying these to make our student experience a better one? RENADA: Thanks for the invitation to be here. And actually for this whole day, and even last evening, I was learning quite a bit. And I'm glad to hear that there is actually connections being strengthened in my brain as a result of this. This is good. I've been thinking a lot about this for quite some time, but I think from the standpoint of higher education we think about it in education, but we also think about it is all of higher education in terms of what we do. And so there are actually a couple of things I'd like to have people start to think about a little bit more. You've been seeing some examples of these things. I think there are three things we have to look at all the time when we're thinking about higher education. It is what are we doing right now? And that could be-- if some of the things that Beverly was just talking about-- these-- some of the technologies, the tools that are available. What is ready right now that we feel comfortable can be used in classrooms, in evaluation and assessment? I think some of the diagnostics that have been around and have been developing are good examples of where that learning has taken place for us as a profession. And we're getting better at it. I think what is available right now, but I also think about what is in development? And we saw a bit of that too. And then of course, then the longer term piece, what is on the horizon? And as we think about each of these things, we have to think not just the classroom space, but the way we interact with students. And so when I think about-- I'll give you a couple of examples. When I think about the entire enterprise of higher education, I think about every aspect. It's about the way we interface with students when they're inquiring about what we have to offer at an institution. Are they prepared for that education that we offer? Is there a good match for them? How do we move those students through the business operations of what we do. Those are important elements as well. And I also think about then of course, the classroom experience. They do not come to us for our tutoring, for example. Nor do they come to us because we have a good admissions process. They actually are coming because of the education they're going to get. All of these other things are supporting what they're doing. Make no mistake, if those things don't work well, we're hindering their ability to get that education. And so we have to think of all of those pieces of what we do when we're educating students. And also then of course, it's the lifelong learning piece. What happens next? What are they going to do in the next phase? That whole area I think, is ripe for artificial intelligence, cognitive operations and learning, chat box if we're thinking of that at an earlier stage, deep learning, machine learning. All of those things, I think have places in various segments of what we're talking about doing. But we have to think about what do we know now, what is developed now, that we can use. And then what is it that should be developed or is on the horizon. And then what is much further down the road. Some of the projects we've talked about are in each of those stages. I think that our efforts to look at our ability to have a student come in and do a good match with what they're bringing to the institution and then taking it to the next stage to actually have them enrolled in an academic program, have those evaluations or those credits or that experience that they have count and be applied correctly to the program that they're in so that they can make steady and regular progress. That's not just about finding ways for artificial intelligence to help us to do that. It's also about looking at our own processes and determining is it the process that works in that space. I mean, what works when a human does it and what works or could work when we have a machine, either helping us or taking over part of it, could also change our process. It's not just about replacing something, it's about looking at that whole process. And I think we're coming across that with some of our transfer credit. For those of you that are working in the transfer credit space, I think this is just a perfect place for us to look at. Are we really comparing a syllabus with the syllabus? Or are we comparing a syllabus with what's in a faculty members head of what the course is? Or are we comparing the syllabus to the curriculum that went through the faculty senate, which is actually a pretty good set of data and information for us. When we're doing this by as an individual, we're doing it one way, but if we're doing it with a machine helping us, what are we pulling from? What is that process? These are the things I think we have to be thinking about in higher education. It's not just about plugging in AI to replace some function, it's actually looking at our function. I think farther down the road we really have to look very hard, really very hard at what we're losing when we're implementing AI and ask the question, is it a loss we can afford? Or do we need to figure out how to pick that up later? It could be a loss we can afford, which is OK, but if it's not-- if the-- because you're going to gain things and you're also going to lose things, but if the gain is there and it's good gain, but if you're losing something, you have to figure out how to pick up that thing that you've lost. And that's an important element as well. I think those are some of the things to think about as we look at this. And I'll give you an example of when I think you lose something. It was the example a few years ago when people had removed penmanship from curricula. And there are lots of good reasons why that might have been something that needed to be done. But in fact, one of the things that handwriting does, is it actually activates a different part of your brain in a different way that affects memory and your ability to retain that information. So there might be something that is lost in this other thing that was done. It doesn't mean necessarily, you go back to or create or retain the penmanship part, but if what you've lost is something you really need to figure out how to do, then how do you embed something new into what the process is to do that again? What is that replacement that's going to do that? I think every time we implement something we have to be asking ourselves, what is the gain, and what might we have lost along the way, and can we afford to lose that thing or do we have to figure out a way to capture that going forward with something else, perhaps. But those are the things that we have to be looking for. LARRY: I think one of the points you are connecting in my head is being thoughtful, considerate, and deep about-- before we jump into a solution, perceived solution. What can be better than students typing rather than writing? I mean, it just makes sense, until you start to dig behind that and say, well, maybe there's something behind the writing process, the generative writing process that we're losing. I think that's a great-- and that's probably just one example, of many others. That's great. Thank you. Thank you. Alan, let me turn to you. At the very end of Beverly, she gave you a laundry list and I'm so excited we have you here because I knew you were going to answer everything for us on that list. Beverly painted this picture of the power of these tools and systems to impact the teaching and learning process. Renada just referenced the impact potential of these systems to improve the experience from beginning to end of the life of a student. But both of these, I'm going to guess, have-- again, to Renada's point, other elements, other dimensions that we need to be thinking about. When you're looking at this as a researcher, help us get our heads around the ethical issues, the social impact of some of these advances. ALAN: Yeah. Well, thank you, Larry. I'm glad they took the food off the table because I'm going to be a bit iconoclastic and I'm hoping nobody throws anything at me. We'll see how that goes. There is certainly this idea that AI is going to solve education. We have some measure and all we have to do is optimize for it. And that measure is grades or student happiness or this or that or some other thing, but the real question is exactly, what is lost? What changes as we replace teaching assistants that are-- use the teaching assistant-ship as a method for learning how to become a teacher with an agent? Is anything lost for those people, for the teachers themselves? Is anything lost in the students? As I look out in the audience, I see 10 to 15 people that are on their devices. I bet that's been all day. There is a lack of ability of the social interaction elements that we risk students losing. And we see this coming up in the psychological data. We see students that are less empathetic, have less ability to mentally model the people they interact with. We see students that have less ability to take perspectives of others, have less ability to navigate and manage social problems, social conflicts. And for lack-- we don't actually know exactly why these things happen, but there is some evidence that part of it is because we don't interact with as many people as we used to. We don't have to solve these sorts of social problems. There is an affect in the psychology called, the Michelangelo Effect and it's basically, this idea that the more that we interact, the more of your mannerisms I pick up, and the more of my mannerisms you pick up. But if I replace you with an artificially intelligent model, which has a normative model of some groups emotions, what have I learned? Have I just learned some average model of boredom or happiness or frustration? Or have I learned anything about you as a person, as an individual? And these are some of the very deep and very dark, potentially issues that we face. Can we create artificial intelligence systems which do more than provide a sort of moniker for what a person used to be? I mean, we saw IP Soft has mentioned that Amelia, I believe, is empathetic, but is that real empathy? If I call up and I say, I can't make it to work because my child died and I have to attend a funeral, what does Amelia say then? I hope the funeral goes well, we'll see you when you get back. These are very personal humanistic problems that can't be averaged away. I'm not saying, that we can't develop software that maybe is a bit different in our thinking or maybe the replacement is just not 100%. Maybe we have to find the niches that are good for agents, but be sure not to automate away all of the human aspects that education has been and still is. And so when I hear a lot about how artificial intelligence is the solution for education, I think we have to take a little bit of-- tone down the hubris in a sense sometimes and to realize that the human-to-human interaction is an element of the education. It's great if you can do math problems, but it's terrible if you can't interact with a teacher, if you can't interact with the learner. And so we have to consider both of these sides of the coins when we choose to explore and develop technologies that have the potential to replace large elements of the teaching community. LARRY: Alan, thank you. I think you've again, sort of cued up some of the big challenges in this domain. I find myself throughout these conversations at times, getting very excited about the potential for this to happen and I think of myself as a learner and I think, wow, I wish I had had this, or I wish I had had that. But then as I start to think through it again and listening to reflecting on Jonathan's comments last night in the idea behind Amelia or Watson or a lot of these tools is it also causes my hackles to go up and worry about, think about, how do we lose that interaction piece? Beverly, if I can go back to you. You spoke of your developing processes and tools addressing both the affect and the science, if you will, of a student learning. Can you just unpack that a little bit and maybe reflect with us with Alan's comments? BEVERLY: Yeah, I can certainly reflect to Alan's comment. But just let me say, that we think that affect is so important in learning and that if a child is in a good disposition, they will do more learning than if they're angry at someone or frustrated, et cetera. But in reflection to what Alan said, we are not teaching social skills, we're not teaching how to be empathetic. And that has really nothing to do with the computer. In fact, if you look at the Department of Labor, they say there will be more jobs, if you want to think about jobs, in the area of caring and compassion and social services in the future. Those jobs are going up, while the number of jobs actually in STEM-- hate to tell you this, but in science, those jobs are going down in terms of the numbers that are available. So we need to train our children, even if it's only because of the jobs that they're going to get in social services and in empathy. And we're doing nothing, very little about it. I mean, there are a few programs and they're going into grade schools and it's got to be part of our curriculum, we have to see how important it is. But to blame the computer for the fact that we don't have any is ridiculous because we didn't have any before the computer came. I mean, I'll grant you-- I mean, I work in a Department of Computer Science, I'll grant you that the lack of social skills is pretty significant at work with computer scientists. Actually, she's left-- the lady. But OK, that's a fact. But the truth is we need to train children, we need to talk about compassion, we have to have examples of it. We have to let them know what we're talking about and how to deal with it. And the fact-- it is true hundreds of people are going on to their social media now and they're spending more time with that than with their friends, but either they do know how to relate to their friends or they don't, and they're starting to learn. And it's different from our generation and the previous generation we're around, but we need to take that into consideration and teach it. LARRY: I'd like to open up the dialogue with you as well, the community. If you have a question, we have Clay on one side, Brad on the other. They have the mics. If you put your hand up, we'll get you a mic. Renada, I got to go back to you now, that we're kind of circling around. When you're looking at the issues of the student experience in higher education, my sense is some of the barriers that students are running into create affect, I'm angry, I'm frustrated. How much in your conversations are you hearing the teams that you're talking about, are we thinking about how we address that in the construction of these solutions? Is that part of it? RENADA: Well, I think what-- the way we've approached this, at least so far has been, we are looking at particular problems we're trying to solve. We start with the problem and are looking for, what is the way to approach this problem? When in dealing with-- and it's not so much the education piece, it's the interaction with students in the either transactional spaces, or it could be actually, in the assistance or tutoring spaces, but let's just stick with that sort of transactional. So yes, people get frustrated. We all do when we're interacting and not feeling as though either we are heard, we are understood, or we are getting what we need. So that happens. But I think what-- when we're looking at the kinds of systems we have at Penn State and those that are impacting our World Campus students, a lot of it is around some of the things that we know our processes simply cannot handle the volume of requests or needs or the variety or what-- you could just name the whole thing. And that responsibility is falling to a group of people that are trying to address all of those things. And so the problem we are trying to solve is really the problem of making sure that the individuals, the students in these cases, are getting the information they need, good information, high quality information, in a timely fashion so they can make good decisions. That's the problem we are trying to solve. The solution, we believe is in employing or deploying some of these technologies that will assist that advisor, that admissions counselor, that financial aid counselor, whoever that happens to be so that they can provide that kind of experience. Now, I think that the part that we are still uncertain about is even if we solve that particular problem, there is surely one on its heels. In some cases, it may just be the next problem that percolates to the top. But it may be a problem that is created because of the solutions we've created. And again, there's no reason not to proceed with the solution, it's that we have to anticipate that there's always going to be the next thing. And I'm going to give sort of like, an old story here. I say old because it deals with automobiles that weren't driven by a computer. They were-- really, the first ones were made out of wood, wood was the material that was used. And so they were fine and they didn't go very fast because the engines were not at the performance levels that they are today. So wood was OK and as steel became the frame of the car and the metal that was then going to be with the automobile was there, and the engine improved, what happened? You have the heavier car going faster. That combination was OK, except that the brakes were no longer really good enough to slow the car down in a reasonable amount of time. There's at some point in the history, it took about 60 feet for a car to stop that was 35 miles an hour, something like that because of the weight and the brakes that did not evolve as quickly. We didn't go back to wood. What we did was we improved the brakes. And then the car's fuel efficiency became an issue. Cars became a little bit lighter. Crash-worthiness became an issue and you had crumple zones put in. You had things that actually started to protect the occupant. We didn't go back to that heavier car, we looked at the solution we needed for the next thing that was happening. I think we always have to anticipate when we solve this particular problem, we may actually increase the efficiency of something to a very high point, which reveals another interesting problem down the road. And that's, I think what we have to always be looking for, is what is that next thing? It is not going to solve everything, but every problem should be treated as seriously as what that first was and thinking about the solutions. BEVERLY: Yeah. I want to say something and it's actually to you. You talked about a system that would advise students in college about-- I thought it was about their schedule, their curriculum. OK. I want you to know that your system has a cousin, first cousin, built in at Georgia Tech. The name is Jill Watson. Do you know about that system? SPEAKER 1: [INAUDIBLE] . BEVERLY: Right. But look, it's really much more difficult because what she says, where the need is to get a lot of students to get a good advice. In this case, this professor at Carnegie-- at Georgia Tech, he went into a course in engineering, I think it's chemical engineering, and he found-- he went through all the questions. And answers that have gone through for 5 years or 10 years, gigabytes of data, and he just put them into categories and then said, OK, if the child is-- the student is asking a question in this category, is to get the Dubai knowledge, then we'll answer this, if it's answering that, it would do that. But then every-- one year, just this year, this fall, he introduced Jill Watson. And he just took from all those categories what she would say. And it turned out-- and then he showed us examples. It's exactly, what you'd expect. It's correct answers and it's solid. And the kids, the students, never knew who was the agent and who was the human. And then-- except, some of them started to think well, it's a girl, can we invite her out to dinner? And then what do you-- they did start inviting her out to dinner. And then they started asking questions about whether she was available and was she nice and did she like doing this job. And then they wanted to know whether they were getting the agent or not the agent. But I'm saying, there were 15 or so teaching assistants plus this agent. And I'm not sure what was lost. I mean, what was gained was that the guys would not respond-- they were mostly guys who were TAs. They would not respond quickly, especially at midnight and 3:00 in the morning. Whereas Jill, if a student came online, the Jill would answer in two seconds at 3:00 in the morning. Didn't care about sleeping. ALAN: So I know a [? Ashook ?] [? Gowell ?] very well, he wrote my recommendation for coming to Penn State. BEVERLY: Why don't you say the whole name? I just-- ALAN: [? Ashook ?] [? Gowell. ?] BEVERLY: Yeah. So that's the-- ALAN: When he developed Joe Watson, he thought it was great. Students didn't like it. Students weren't happy. He got a lot of complaints because they said, what the hell am I paying for here? I'm paying for TAs, I'm paying for interact with my answers question, not a bot that sort of cycles around on-- leads me down wrong roads. BEVERLY: When does your information come? Because this came from yesterday, he said, that they were quite happy with the system. Maybe it's improved. ALAN: Yeah, it was last year. BEVERLY: OK. It was yesterday they said they were happy with it. AUDIENCE: Yeah-- BEVERLY: I mean, it has to be worked on. It's not perfect. AUDIENCE: I think the intelligence can be trained. IQ can be trained. Also the emotional can be trained too. The EQ can be trained too if you have enough data. You can't have the personality for this robot. [INTERPOSING VOICES] ALAN: The emotion part, I think is important because we've talked about frustration and boredom. And can you-- so what if we develop this sort of technological valium where you're never bored, you're never frustrated, it's always-- you're always happy around it. What happens when you walk out in real life and now, you really are frustrated? I mean, do you find some sort of less socially acceptable venue for that frustration, for than boredom? Our classrooms teach us more than just a series of facts. We learn how to manage those emotions, how to deal with the people that cause them. LARRY: Interesting. Can I just throw out-- this is just a thought, but do we yet have principles of ethical design in AI machine? In other words, here's the point where the machine can take it up to the artificial intelligence and at this point, it crosses some threshold, there's some principle guiding or filter that says, OK, from here it goes to a human or reverse? BEVERLY: No. There's very little study in that, in AI. However, there is work that shows that this brain let's say-- because people do use that term, can work at 20,000 times faster than the humans. So in education let's say, we had a group of teachers who are able to develop curriculum, who are able to teach, et cetera. If some group is also developing this AI, super AI, super intelligent, well, all of a sudden it could go on teaching thousands of people with thousands of topics every week or every day. And that-- how can-- the question is, how can humans even adjust to what that model teacher is-- what do we call it? We call it a robot. What that robot is teaching just in the field of education. Not only what that thing is teaching, but how do we constrain it and how do we answer these ethical questions? I mean, if we let it go, it has to be faster and better than most of the humans, a group of good human teachers. But the fact is it's our job to make sure that the data is kept in the right place, that the mood is appropriate, blah, blah, blah, blah. So it's-- we can't keep up with it now, and it's accelerating. LARRY: Interesting. Renada? RENADA: It's seems to me though that I think if we're sort of talking about this as though that learning that it's doing is separate from-- is not connected to the human at that point that it's learning. And I think that's the place where so many of these questions can actually start to be addressed and considered. If we think about having who is doing the training, who is doing that, if they aren't involved then I think we run a real risk of missing all of these pieces. If they are involved, then I think we stand a much better chance. I think our advisors are in a lot of ways, some of the most-- the strongest supporters of the importance of that interaction. They are the most skeptical, I think in a lot of ways of being removed from that process. And yet, I find that in the cases where we can demonstrate a path forward for them where a digital assistant can be so helpful to them to be effective at those interactions when they're having them, is where the needle just swings the other direction and they are on board. I mean, there's still going to be questioning, but for them to be involved in both the development of what that is, is I think where we hit that sweet spot of the importance of making sure the training is involving the right people. And we stand a better chance then, I think of addressing both of these things. LARRY: That's great. RENADA: They don't give up very easy. Teachers are the same way. I mean, they are really kind of sticklers when it comes to that. Which is a really good thing. LARRY: There are things they do not want to give up with that student interaction. RENADA: Well, and they know, they sort of know what the importance of that is and they know when they see it that it's OK. And I think as long as we keep that connection really tight and we consider that to be an important element as we move forward, then like I said, I think we have a better chance of actually getting it right. LARRY: Terrific. Good stuff. Let me go back to the crowd here. Craig, please. AUDIENCE: First of all, thank you so much for your participation. I was really looking forward to this conversation. You really have been brought up so many key issues. I think my question kind of builds upon what Larry said about principles. One of the goals we were hoping from the seminar is begin looking what is Penn State strategy around AI, both in and outside the classroom? And sometimes in higher ed, we go to extremes that we're going to get rid of faculty and staff or are we going to put our head in the sand? I think we've got to come up with a thoughtful strategy and so I'd ask you if you had 15 minutes with Dr. [? Barron, ?] you say, here's what you think the university should do. What do you think the universe should do to develop an overall artificial intelligence strategy, or kind of technology strategy from your experience? LARRY: Could we ask our external guest first? Beverly, what would you suggest to Craig, his advice? BEVERLY: From what I understand, your president is already very engrossed in this and sees the optimistic part of it, is that accurate? AUDIENCE: That would be true. Yes. BEVERLY: And I think that has to be supported. He has to be shown that there's absolutely great potential here. And to deny it is really contrary to the way the world is working. In terms of strategies I mean, he constantly has to think about the role, which I guess she would be best at that, about the role of the faculty and specifically about how you can augment what the faculty is doing. We clearly know what a faculty does in a college, but just think of AI as a lever to augment what that person does. And that should be the strategy. You never do anything with AI that's contrary to what is good for the student for learning or good for the teacher for teaching. And actually, professional development is part of that. Are we using AI for professional development so that teachers are not monotonously giving the same lecture every year, but use AI to find interesting new ways to teach? I think if he looks at the leveraging of this technology, that would be the best policy. LARRY: Thank you. Renada? RENADA: Yeah. I think that the strategy has to be multi-pronged in terms of what is the underpinning? What do we need as support systems? Needs to be considered for across the institution. And I'll name a couple that I think are important. So there-- they may not be the ones we necessarily think of. So I'll mention those. And then I think the other part of it is we have to look at places where we have particular challenges that we can tackle and really try to understand and think about them in a sequential way. What is it that's short term? What is it that's long term? What we're going to learn from here that's going to get there? There has to be some strategy built there. But I think this foundational piece for an institution is really important. And a foundational piece for me is that the richness of the data and actually thinking about where is our data, how is it used. And when I say data, I am talking about the data, so numbers essentially. But there is another piece of that, which is information. And information is another piece of that. And information as I go back to the transfer credit issue, we think of a syllabus as the piece of data that might be useful, but in fact, it's probably all that work the faculty senate does and the faculty do in writing those curricular proposals and course proposals about learning outcomes and so our learning outcomes. The data doesn't necessarily always exist where we think in certain spots, it's really across the board. We have to have a good data strategy understanding that. And the other piece, which is a little different, really kind of quite different, is if I think about my role right now and I think about my role, the person that like me, in 10 years or 15 years, I might have two or three digital assistants that are working with and for me. How are we situated to really support digital assistance for individuals at an individual level? That's sort of a very different kind of thing and you might call it human resources, but is it human resources? I don't know about that human part. I'm trying to figure out, is it my responsibility to make sure that my digital assistant is well trained? I don't know. BEVERLY: And happy. RENADA: And happy. I think those are a much longer term, but what does it mean when we are not just us working at this university, but there's a lot of digital assistants there? And I just-- I'm trying to wrap my head around what that means. I think we have to start to think about those things. LARRY: Alan, one second, here. It just dawns on me, then do our students also have digital assistants? And what does that mean in terms-- RENADA: Well, do we-- when they graduate from Penn State, is our gift to them a digital assistant for life? LARRY: Oh, interesting. RENADA: I mean, I think you think about what it is, that this will be, I have no idea. I don't want to be that person that says, no one will need a digital assistant if someone quotes me 10 years from now and says, what a fool. But I don't know what it is, but I don't think we should ever start to think going down this path is going to be as clear as, all of a sudden it's going to just open up and there we are. I think it's going to be we have to keep changing and charting and looking and asking and probing and making sure we're making the right decisions the whole way. LARRY: Great point. Good, thank you. ALAN: I would ask Dr. [? Barron ?] to look at the impacts. Let's study the impacts of how these things will change the students, the education, mission. We're not optimizing grades. If we are, then great, inflation's doing a great job of it. We don't even have to do anything. We're creating good people. And how do we optimize that, and how do we measure that? And what does that mean from a philosophical standpoint? Will an agent improve that? Take away from that or just change it? We don't know today. And as Larry pointed out, we don't even have the set of ethical principles to guide the creation of these things. Just a bunch of people out there creating whatever sort of optimized agent they want. We've got to understand the ramifications, the impact that it has on the culture of Penn State. And the people, on the people that choose to come here or choose not to come here and why they do. And if you never get to meet a professor, then-- many of us are here because we had maybe one or two champions that said, I really believe in you, I really think that you know you've got something special and you can make it. I don't know if a digital agent replaces that. BEVERLY: What about the thought that the digital age gives the professor time to spend with the student to mentor and to really be with that person personally. Doesn't that-- ALAN: It depends on if it's an assistant and what roles this thing takes. BEVERLY: What if a teacher doesn't-- ALAN: We haven't defined the role of this thing yes. BEVERLY: What if you all don't have to grade anymore? Wouldn't that be nice? And you could spend time with your students. ALAN: I would love that, but I don't have the students right now. And I don't know that it would be good for them. BEVERLY: There seems to be a lot questions. LARRY: Interesting. Good, good, good. Brad. AUDIENCE: Hello. Really fascinating discussion. And I want to pick up a thread that's coming from a couple of folks, Renada and Beverly, especially some of the comments around. The idea of having these sort of tuned digital assistants that are working with you individually and your workload. But also, Beverly, your comment about there's really no ethical map for how Big Data is being used in educational systems. And it makes me think about, if you look at previous generations of the way say, the desktop computer invaded schools. What happened? Well, you needed IT staff. My question is, what is the kind of emergent profession that needs to be a part of our learning ecosystem to support AI in the space of education? BEVERLY: Yeah. AUDIENCE: And what's the new profession that we need to help us build these highly individualized AI or folks that can help us navigate both code and ethics? BEVERLY: I think it's philosophers, psychologists. It's certainly outside of computer science, outside. I would not trust computer scientists, hardly in education and certainly not for ethics. LARRY: Well, that's encouraging. No, I understand your point though. That is a great question. That's a great question. Is there a whole new field or a whole new domain of skill sets and competencies we need to be considering? Alan, Renada, any thoughts on that one? ALAN: Cyber security becomes important if this is recording your emotional state from every interaction. I mean, that's very personal information. If I want to sort of stalk someone, can I just stalk their agent or hack their agent? Learn all about them, learn where they go, learn how they feel. These are concerns. LARRY: Interesting. BEVERLY: I want to say, there is interest in this field because there's one of the biggest research groups in AI called, Triple AI, the Association for the Advancement of AI, is now, at their next big meeting having a huge workshop on ethics. In every area. Ethics in banking and ethics and hotel management. And people are getting together and discussing it, but it's the first time I've seen that. It will happen this fall or spring. LARRY: Interesting. Great. Let's see, we've got a couple of minutes left. Other questions? Drew? Do you have another one? AUDIENCE: I just wanted to maybe, prompt and give you a chance to respond to your thoughts on the use of machine learning outside of those that we'd think of as having embodied personalities or anything like that. Simply, just machine learning to optimize things that are less treacherous. Like, how do we schedule classes in a more efficient way to allow our students to flow through the university in a better fashion and not have conflicts? Or to better predict the demand for certain courses year over year? Or maybe, we can better predict how much paper we need so we don't have to silo as much and if we can-- we need more things than just paper, but we have these consumable goods, and so maybe we could save money and pass that on to our students in some way. So in my mind a tremendous use case for a machine learning that is not coupled with all this maybe, psychological and other things that are scary and still need consideration. Maybe you could comment on that and its role in education. BEVERLY: In addition to counting paper and paper clips, things like that, there's also the thought of changing higher ed, having it more inclusive and having it more compatible, integrative, let's say, integrative. To be more integrative it would be that a scientist goes to the local winery and helps them figure out which soil is best and they do this as a six-week project, maybe before they even enter the college. In other words, projects that are meaningful to them where they actually show their ability and their scientific ability. And more inclusive means, having-- which the schools are really not as inclusive as they ought to be. But what if we apply AI to really changing higher ed in that way? As opposed to what you're talking about, which has been done in non-AI systems, but hasn't been done really in AI systems. But I mean, let's get a strategy for what will improve and move the colleges ahead. Bringing more people in who are non-representative minors right now and people who haven't been in college. And right now the statistic is something like 34% or 40% of people will not graduate college in four years. And actually, it's 30% will not graduate school in 30 years. Can you imagine what you can do in this society without having a high school diploma? That's outrageous. So why don't we attack some of those with AI, which I think is possible. RENADA: I think-- I agree with you about particularly, around the diversity issue and I think that this is a space that is particularly challenging because in the field, and a lot of this has come through a field that actually hasn't had a lot of diversity in it. The idea of what has been designed and that making sure that we have room for including those diverse perspectives whether it's gender, race, ethnicity, background, different disciplines, is really important because it does influence. I mean, if the learning is coming from us, but the learning isn't really representative of society then we're building in something that really isn't perhaps the best thing that it can be. Certainly, I would argue it's a big issue. But in going back to your question with regard to some of these other systems, again, I would go back and say, what is our problem? What is the problem? And there are probably lots of places that are problems. I think, actually having some AI, whatever that might be or machine learning that could constantly be monitoring our systems for-- as for security, would be a very interesting one. I mean, thinking about that constant vigilant AI that is looking for things that are breaches of security would be in an area and I would wonder if we have the capability. The other thing are just keeping systems up to date. I mean, we are moving away from having to be shut down from windows of time for updates. Remember when Angel, we used to have to do that, we used to have that little window when we-- OK. So that's gone. But there are still things that come in and get updated, but what if our systems were constantly being monitored and made as efficient as possible? I think looking at where we have a problem we are trying to solve either increases the efficiency for the university, increases resources that can be directed elsewhere, whatever it happens to be, let's think about it as the problem we're trying to solve. ALAN: I think it's fine. I mean, if you're going to have AI decide when to buy paper, fine. Not a problem at all. Logistics management. Of course, scheduling, fine. LARRY: Listen, I think we could probably talk about this for several days and maybe in the future we will. There will be a bot up here and you know. Alan, I happen to know you've got to get to class. There are those students we love. First of all, please join me in thanking our panelists for-- 