 one Denver Colorado it's the cube covering supercomputing seventeen brought to you by Intel hey welcome back everybody Jeff freek here with the keyword Denver Colorado at the supercomputing conference 2017 about 12,000 people talking about really the outer edges of computing it's pretty amazing the keynote was he's huge the Square Kilometre Array a new vocabulary word I I learned today so it's pretty exciting times and we're excited our next guest he's built Jenkins he's a product line manager for AI on FPGAs at Intel hello welcome thank you very much for having me nice to meet you nice to talk to you today absolutely you're right in the middle of this machine learning AI storm which we keep hearing more and more about the next generation of big data if you will that's right that's right it's the most dynamic industry I've seen since the telecom industry back in the 90s it's just evolving every every day every month so so in terms of making some announcements you know using kind of this combination of software programming and FPGAs on the acceleration stack to get more performance out of the data center I get that right sure yeah yeah pretty exciting the use of both the hardware as well as software on top of it to open up the solution stack open up the ecosystem so what is what are those things are you working on specifically so so I really build the first the enabling technology that brings the FPGA into that Intel ecosystem where Intel is trying to provide that solution from top to bottom to deliver AI products right that market FPGA is are a key piece of that because we provide a different way to accelerate those machine learning and AI workloads where we can be an offload engine to a CPU we could be inline analytics to offload the system and get higher performance that way and we tie into that overall Intel ecosystem of tools and products right so that's pretty interesting piece because the real-time streaming data is all the rage now right not in batch you want to get it now so how do you get it in how do you get it rhythm to the database how to get into the microprocessor so that's a really really important piece that's different then even what two years ago we didn't hear about much real-time yeah I mean I think it's like I said it's evolving quite a bit now a lot of people deal with training it's the science behind it the data scientists work to figure out what topologies they want to deploy and how they want to deploy them but now people are building products around it right and once they start deploying these techno into products they realized that they don't want to compensate for limitations in hardware they want to work around them so this this a lot of this evolution that we're building is is to try to find ways to more efficiently do that compute and and what we call inferencing the actual deployed machine learning scoring as they've will write in a product it's all about how quickly can I get the data out it's not about waiting two seconds to start the processing you know in an autonomous driven car where someone's crossing the road I'm not waiting two seconds I figure out it's a person I need it right away so I need to be able to do that with with video feeds right off a disk drive from the Ethernet data coming in I want to do that directly in line so that my processor can do what it's good at and we offload that processor to get better system performance right and then on the machine learning specifically because that is all the rage and it is learning right so there is a real-time aspect to it you talked about autonomous vehicles but there's also a continuous learning over time that's not necessarily dependent on learning immediately but continuous improvement over time so what are some of the unique challenges in machine learning and what are some of the ways you guys are trying to address those yeah I mean once you've trained the network people always have to go back and retrain they say ok I've got a good accuracy but I want better performance so then they start lowering the precision and they say well you know today we're at 32-bit may be 16-bit and then they start looking at into eight but the problem is their accuracy drops so they retrain that intake topology that network to get the performance benefit but with the higher accuracy but the flexibility of the FPGA actually allows people to take that network at 32 bit with the 32-bit train weights but deploy it in lower precision so we can abstract away the fact that the hardware so flexible we can do what we call floating point 11 bit floating point or even 8 bit floating point even here today at the show we've got a binary and ternary demo showcasing the flexibility that the FPGA can provide today with that building lock piece of hardware that the FPGA can be and and really provide not only the topologies that people are trying to build today but tomorrow right future proofing their their their hardware but then the Precision's that they may want to do so they don't have to retrain they can get less than a 1% accuracy loss but they can lower that precision to get all the performance benefits of that you know data scientists work to come up with a new architect right but it's interesting cuz there's trade-offs right there's no optimum solution its optimum as to what you're trying to optimize for so really the ability to change the ability to continue to work on those learning algorithms to be able to to change your your priority it's pretty key yeah I mean I mean a lot of times today you want this so it you know this has been the the mantra of the FPGA for thirty plus years you deploy it today it works fine maybe you build an ASIC out of it but what you want tomorrow is going to be different so maybe if it's changing so rapidly you build the ASIC because there's runway to that but if there isn't you may just say I have the FPGA I can just reprogram it to do what's the next architecture the next biology so it gives you that future proofing that capability to sustain different topologies different architectures different Precision's to kind of keep people going with the same piece of hardware without having to save spin on up a new ASIC right here right which is you know even then it's so dynamic it's probably faster than every year the way things are going today so the other thing you mentioned is typography and it's not the same typography you mentioned but you know this whole idea of edge right sure I'm moving more and more compute and store and smarts to the edge because it's just not going to be time you mentioned autonomous vehicles a lot of applications to get everything back up into the cloud it back into the data center so you guys were pushing this technology not only in the data center but progressively closer and closer to the edge absolutely I mean the data center has a need it's always going to be there but they're getting big right there's the amount of data that we're trying to process every day is growing right I always say that the the telecom industry started the information age well the Information Age has done a great job of collecting a lot of data we have to process that if you think about where again I'll maybe I'll allude back to autonomous vehicles you're talking about thousands of gigabytes per day of data generated smart factories exabytes of data generated today what are you going to do with all that has to be processed so we need that compute in the data center but we have to start pushing it out into the edge where I start thinking what even a show like this I want security so I want to do real-time weapons detection right security prevention I want to do Smart City applications just monitoring how traffic moves through a mall so I can control lighting and heating all of these things at the edge in the camera that's deployed on the street in the camera that's deployed in a mall right all of that we want to make those smarter so that we can do more compute to offload the amount of data that needs to be sent back to the data center right as much as possible relevant data gets sent back no shortage of demand for compute or networking is there no it's really a heterogeneous world right we need we need all the different compute we need all the different aspects of transmission of the data with 5g we need disk space to store it willing to cool it it's really becoming a heterogeneous world alright I'm gonna give you the last word so I can't believe what we're November of 2017 which is bananas what are you working on for 2018 what are some of your priorities if we talked a year from now what are we gonna be talking about yeah so Intel mean intel has acquired a lot of companies over the past couple of years now on AI you're seeing a lot of merging of the FPGA into that ecosystem we've got the Nirvana we've got more videos we've got mobile AI acquisitions Saffron technologies all of these things when the FPGA is kind of a key piece of that because it gives you that flexibility of the hardware to extend those pieces you're going to see a lot more stuff in the cloud a lot more stuff with partners next year and really enabling that edge to data center compute with things like binary neural networks ternary neural networks all the different nests generation of topologies to kind of keep that that leading-edge flexibility that the FPGA can provide from people's products tomorrow exciting times yeah all right look pill Jenkins there's a lot going on in computing if you're not getting your computer science degree kids think about it again he still chickens I'm Jeff Rick you're watching the cube from super computing 2017 thanks for watching thank you you 