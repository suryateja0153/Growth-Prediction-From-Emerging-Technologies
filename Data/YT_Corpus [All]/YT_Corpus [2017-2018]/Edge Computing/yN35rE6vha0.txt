 hello and thank you for joining us today my name is Joshua and on behalf of sundar we'd like to welcome you to the second in a series of three webinars jointly produced by Google and Intel the goal of these webinars is teaching more about our collective outtie efforts during the broadcast if you have any questions please type them into the chat window we'll get to those questions at the end of the webinar until then please enjoy the proceedings hello my name is Joshua Chong cloud architect and program manager into Google cloud partner engineering team welcome to the second in a three-part webinar series and I'm Martyn Kronberg Technical Evangelist with Intel so in the previous webinar we learned how to take all the data generated by our Gateway and set it up to GCP and in this webinar we're gonna show you guys how to make that data useful by the end of this talk we will have answered the following questions first how can we make all this data coming in from our devices useful now how do I manage these devices at scale and lastly how can GCP make it easy for me to set up my own global high speed data ingestion service to answer these questions we're going to take your data on a journey and the map of the journey looks like this what you see here is a series of Google cloud services each with its own role to play this pipeline takes your data from point A to point B in the previous webinar we learn how to use thing until nuke send data into Google Cloud IOT core that data ends up into pub/sub q from the pub sub q will use a cloud dataflow pipeline to take that data from the queue into a durable storage service called bigquery and from bigquery will visualize that data using cloud data studio to create wonderful dashboard for your users but before we dive in let me throw it back tomorrow to talk about the hardware setup sure so I just wanna take a quick look again at how the sensor data is actually getting into GCP so for this demo we're using the sensor data that's coming from our gateway here and we have our gateway attached to an Arduino 101 which we're using as a sensor hub attached to this hub are three environmental sensors temperature light and sound in order to read the sensor data we're using the Umrah and upm libraries from Intel we can use a really simple high level API to connect to the sensor hub and then read out our values from the sensors if you want to learn more about these libraries and how you can use them please refer back to the first webinar of this series so Josh do you wanna tell us a little more about that data journey absolutely before you begin I like to take a minute just to explain a little bit about our global network now this is important because your devices could be deployed all over the world and this has implications on how your devices communicate back to the cloud what you're seeing here is a map of our global network you'll notice you'll we have over a hundred peering locations what this means is that most likely will have a presence in the country where your advice are deployed into this also means that with a single global IP address you could send all your data to that simple single IP address and Google will automatically low balance out traffic for you so data can come in - are the closest pop or point of presence ride our backbone and into the services in - in the specific region that service was set up in - let's talk about the first service to explore on our journey and this is pub/sub or globally distributed highly scaled messaging service when data is sent to Google Cloud IOT it ends up in a puff subtopic multiple subscribers can subscribe to these topics and retrieve information it's a durable service meaning we will retain unacknowledged messages for up seven days it's a global service as well meaning you can write to a single URL or IP address and Google will handle the low balancing for you couple with either data flow or cloud functions pub/sub is your on-ramp to real-time streaming into the cloud let me demonstrate how easy it is to set up your own topic I'll also set up a simple subscription so what you see here is the google cloud pub/sub user interface to get there on your DCP landing page you would simply click on the hamburger menu and select pub/sub to create a topic you would select create a topic and then define your own topic in this case let's just call it demo click on create and just that easy you've created a topic but doesn't really do anything useful until you can create a subscription to create a subscription subscription simply go into the demo topic that you just created and then click on create subscription in this case we'll just call it demo and there you have it you have your first topic and subscription duo so let's do something useful with this now we've been sending data from the endpoint here into pub/sub let's see what that looks like okay and in this case I will issue the simple g-cloud command to pull data from that subscription or to an existing subscription I have that receive the data from the endpoint and there you go this is the data being sent from the sensors to the note2 through iot-cloud through pumps up and there you have it in the subscription you know Josh you talked about scaling up your gate infrastructure using the global GCP network and as you scale up the cloud in the backend you're also gonna want to scale up your harbor deployments in order to gather more sensor data be able to enact any commands that you want to send down so as you do this having a gateway is going to be key so the Gateway acts as a central hub for all the sensors and actuators in a local area you can send it the data gathered from any number of sensors using any kind of interface for instance if you're scaling out to a factory that has a lot of legacy sensors by which I mean sensors that do not have a way of connecting to the cloud themselves you're going to need a device that can't connect to the cloud and act as a data collection and communication hub so until has Gateway solutions that can interface with sensors over the local area network or using a serial connection like Modbus for canvas or short-range radios like Bluetooth ZigBee or z-wave as well as many more connection types so by having a single connection point to the cloud you're gonna inherently increase the security of your of your deployment you look at possible points of intrusion they're gonna limit the number of certificates you have to manage and you can use the Gateway to set up a firewall that only allows certain protocols through using an IP table overall is going to be much more secure than trying to manage a multitude of connections you can also run some data pre-processing at the Gateway before shipping it to the cloud from basic things like taking an average over time to much more complex data transformations and I'm gonna come back to this point a little bit later on with a few examples finally as a scale of your data increases you're going to want to also make sure that it's consistent so consistent data is going to be easier to parse as well as to manage and the game we can also package up that data for consistent cloud ingestion so in our case and in many cloud connection news cases we're using JSON to send the data back and forth so this data transport method is easy for humans to read and it's easy for machines to parse it's also quickly becoming a standard for IOT data transport so you'll be able to interface with many different services using the same data pipeline another aspect of scaling out your project is scaling out the scope of what the project itself can do so in order to help developers really expand the capabilities of their projects we have a so called path to product at Intel what the path the product provides is code samples proof of concepts as well as market research into specific verticals in the IOT space including smart home transportation autonomous driving so on it's all open source and it's all up on github under MIT licensing so you can use what you need from our code Josh do you want to tell us a little bit more about what the data can do in bigquery absolutely so bigquery is my favorite GCP product now in the journey you'll see that we're gonna skip over data flow and we'll get back to that in a second you'll see why for now let's talk about bigquery bigquery is Google's analytics database that's bilf a petabyte scale it's ideal fret and analytics use cases because you don't have to worry about deploying more resources when you're constantly pumping in more IOT data from these devices it's fully managed no ops data warehouse is built for petabytes calls mention it but it's super fast at the same time you get the convenience of SQL so if you're used to writing SQL you'll be able to work with bigquery easily and finally it's an externalize version of Google Dremel which is a database we've been using internally for over a decade so it's been hardened it's been tested and it's been put in production for well over ten years now so let me demonstrate how easy it is to setup your own database and issue some simple commands so with the first thing you'll notice is that you will you're using SQL constructs here so you're gonna select star from this table order by ascending order click on run creator is gonna run the query and notice how fast it was this table has over 7,000 rows let's try it in descending order we run the same query to see the data change a bit and there you go right so it's gonna sort this in descending or notice how superfast it was do you create a table again click on this down arrow super fast where to create a dataset we're gonna call it demo click on OK and from the day of demo you can create a new table save and you can either create from source or create an empty table let's just create an empty table and call it demo table keep that in mind we'll get back to that in a little bit and just add a couple fields let's try timestamp which is an integer and this fri temperature which is a float here so i'm gonna click on create table and there you have it your first bigquery table now that we have data coming into pub/sub and we have a place to store that data how do we get that data from the pub sub topic into bigquery there are a few options but today we'll focus on cloud dataflow if you are familiar with Apache Beam then you'll feel right at home with cloud dataflow cloud dataflow is a fully managed service for transforming and enriching data in stream or real time or in batch historical with equal reliability and expressiveness so no more complex workarounds or compromises needed at a higher level you define a source which we call inbound data and a sink out bomb data when you bring data in it becomes a collection in which a pipeline of operations which we call transforms can be performed the data source can be batch or it can be streamed and there is a time windowing concept which are lost from great extensibility you do all of this under a single programming model which reduces complexity before we go to a demo go throwback tomorrow to talk a little bit more about ETL sure so there's going to be three main parts of data handling that the Gateway does for us the extraction transformation and loading of data or ETL as it's referred to in data science we discussed the extraction of data or gathering the sensor data we talked about the loading of data or ingesting it into Google cloud platform now I want to talk a little bit more about the transformation of the data so raw data is gonna be a little rough there's gonna be outliers there's gonna be some noise and there's gonna be a time series of data that follow certain trends when we're dealing with a massive amount of data as a large scale industrial deployment will we want to be able to clean that data up as fast as possible so to facilitate this process in Todd's developed a set of libraries called the data analytics acceleration libraries or dahl these libraries cover a vast amount of data transformation methods linear regression outlier detection and low order moments or things like means sums standard deviations so these are just a few of the core tools that a data scientist can use to transform raw data and all these tools and many more are provided by the doll library and using that you can transform your raw data into something more usable they they can then be fed into our cloud analytics pipeline so these libraries have been optimized to run out until architecture in order to provide an extremely fast way to transform large amounts of data and the best part is that Dahl runs on Hadoop SPARC R and MATLAB so that means that you can use the data processing pipeline with which you are most familiar with if you want to try out these libraries for yourself go to software TOCOM slash intel - da al to download the libraries read about how to use them and try them out now that we have the ETL performing on the Intel device itself we can still use dataflow to transport the data from pub/sub topic into bigquery let's see how to get to date cloud dataflow you click on the hamburger menu click on data flow in order to have a number of services already running this particular pipeline is running to take the data from this device from the topic and into bigquery to create a new pipeline I simply create a job from template now this really is a template here because we don't have to do anything sophisticated here because the the nuke here is going to handle the ETL I give this a name I'm going to use the third option which is cloud pops up to bigquery now just select this option I'm gonna select two more I need to provide two more pieces of detail one is the actual topic since I have a pop subtopic already open here I'm gonna select this topic and from Vickrey I'm gonna select this table now you know why we skipped over cloud dataflow and went directly to big crate so so now that we have a big creative table defined you can click on run job and there you go that's your first cloud dataflow pipeline taking data from a pub sub topic and into bigquery at this point we've covered a lot of ground so let's quickly review what we've done thus far on our data journey we started out with the central information coming to cloud IOT 4 from there the data got into a puffed-up topic just now we created a pipeline to take that data from the topic and into bigquery now with the data in bigquery we can move on to the last step which is using cloud data studio which is a free tool that allows you to make beautiful charts from your streaming data instead of me talking let me walk you through a simple example to get to data studio you simply go to data studio Google comm to start click on the big blue plus button to start a new dashboard the first step when you create a dashboard is to create your first data source in this case I'm going to create one from my GCP project you click on this big blue button and you'll notice that data studio offers a number of data connectors this makes it easy to connect your data sources in this case we're gonna use bigquery because I've logged in with my GCP account and knows which projects I am and I have access to in this case we will be connecting with to the Intel webinar project as appropriate click on edge data and here's that demo bigquery table we've created but here's edge data data table and I'm gonna click on connect and connect this data source which we'll notice here is that I didn't encode up anything what data studio did was looked into that big tree table found the columns and automatically created these metrics for me well smart enough to identify that light was a number of styles and number temperatures a number even the timestamp which is specified as an integer in bigquery it was identified as a date which is correct here I'm gonna change a few parameters around I'm gonna make these averages instead of sums and let's add a bit more detail to the time here and to include both the day and the hour just make it a bit more interesting I'm gonna add that to the report okay here's your first dashboard let's create a table that just takes the raw data and dumps it to the screen to do so I click on this table icon draw up the table in the dashboard and you'll notice immediately the time stamps are already populated here it added like but we could add a couple other metrics and that's add sound and lastly let's add temperature and there you go so now you have a data dump of the information coming in from the sensor let's create a chart now to make it a bit more attractive so I'm gonna start with this time series chart it's essentially a line chart now drag into the dashboard now you notice that on the x-axis it'll automatically populate it with the timestamp by default it's adding the light but we could add the sound and let's also add the temperature and there you have it in about five minutes we created a beautiful table with all your data and a chart then gives it a bit more meaning than just a list of wrote raw numbers yeah so this visualization looks really great and obviously that you can easily share it with other people on your team that's right to do so you just click on this sharing icon and list your teammates that you want to share it with that's it great I mean this looks really good and I can definitely see how developers can use it to start pulling out business logic from their data before we sign off I just wanted to circle back and discuss a little bit more about advanced analytics they can do on the Intel base gateways so the type of data that we're able to visualize here and really in almost all cloud based frameworks is limited to what you can send by a JSON so numbers like floats are integers and strings of characters this works great for many date applications our demo here uses sensor data which encoded as an integer value between 0 and 10 23 which is a pretty standard way of doing analog to digital conversion but what if the data that you're gathering can't be encoded into a number or string so for instance more and more applications are based around gathering video or images and the only way to send an image with a JSON would be to break it up into an array of pixel values and this can be both lumber some and very bandwidth intensive so what you need to do then is some kind of advanced analytics at the edge before we get some that data up to the cloud so Ansel provides a whole host of tools for just that with the Intel system studio this is an IDE for C++ that integrates a whole host of libraries and SDKs which are created by Intel and meant for advanced analytics check out software.com slash system - studio to learn more download the IDE and to start using it and applying it to your project one of the tools that's included in this package is the computer vision SDK so this SDK was released into public beta about a month or so back and integrates a bunch of really useful tools for computer vision applications it's based around open CV which is an open source project for a computer vision originally developed at Intel back in 2000 it's grown quite a bit since then thanks to an extremely active developer community the CVS DK expands on the capabilities of open CV by adding in optimizers deep learning a visual algorithm designer and a lot more tools and of course it's been optimized to run on Intel CPUs so what does this mean in terms of our data flow well it means I just end up sending up an entire image you can analyze it with the CBS DK on the Gateway and then send a data format that's easy for our pipeline to ingest and then this way you can expand the kinds of data they can use in your application that's all we have for today we hope that today's session shows that with the combined power of Intel's hardware platforms with Google Cloud you too can build a highly scalable reliable data pipeline and please stay tuned for our next webinar in which we're going to demonstrate how you can use this data to build your own machine learning solutions thanks guys you you welcome back we hope you enjoyed the presentation and thank you for your questions we've gathered a number of questions that we hope we can answer to start off let's start with number one year howdy charge for bigquery now there are three dimensions to bigquery pricing the first one is storage the second one is streaming and the third is the query on the storage side we charge about two cents per brigid gigabyte per month on the Streamy side we only charge the cent for 200 megabytes per month and on the query side it's $5 per terabyte of querying with the first hair by being free now data coming into bigquery and data coming out of bigquery is free so if you have any questions there's more information on the website under bigquery in the Google cloud platform website number two another bigquery related question you only showed BIC we're using a UI is that the only way to interface with Vickery the answer is no there's multiple ways the first way is through our REST API is very well known what REST API is the second way is do or g-cloud command line interface the third way and I would say the most popular way people are interfacing with bigquery is to our native API so we do have API is built for Python dotnet and Java so as a developer you have options and how you want to leverage your create into your project here's the question on sensor libraries so the question is how do I learn more about sensor libraries used on the Intel device so Intel actually maintains a website called u PM m ra a io it has all the information about the sensor libraries the api's which sensors you could use this site was recently revamped so you should be able to access all this information yeah it should be easily accessible again the website is upm dot MRA a dot io great Thank You sundar number four how much does data studio cost there's a theme here developers are concerned about cost the good news about data studio is that it's a free product as the date of this recording and it'll it's currently in beta it's a free product however I do want to inform our developer community out there that if you're using bigquery as the data store to serve the data into daily estudio you will be charged for the currying and the storage of that bigquery data so watch that but the product itself today is a free product cool let's see okay all right great number number five another data studio related question it's a great question aside from the built-in data connectors are there third-party connectors available for data student the answer is yes if you google data suited community connectors you'll get a result the first result will be a page where we allow and educate developers to create their own data studio connectors so if the standard built-in connectors don't do it for you you can kind of you build your own and you use a product called act Apps Script which is not beyond the scope of this webinar but it's pretty easy to pick up as well as tons of information online in addition to that community page we actually have a gallery as well from third parties who have already created these connectors that gallery is linked off this community page so again search for data studio community connectors excellent here's a question on device security so are there any specific tools that help increase device security actually there are in fact very recently at the IOT World Congress we announced a specific feature called a secure device onboarding take a look at it it's more information is available with our wwn Telkom slash secure device onboarding with features like secure device onboarding you can actually onboard a device securely with pretty much a zero touch so you don't have to ship devices with a default username and password right that's a huge security risk so again the website link is WWE intel.com slash secure device on board great another IOT device related question which is with IOT devices we can have thousands deployed in the field how many topics and subscribers can I have in my DCP project so a great question because again with IOT devices you tend to work at scale right so within each project each can contain up to 10,000 subscribers Wow and 10,000 topics so hopefully that's enough for your for your use and lastly how long will data be stored and it pumps up topic well obviously acknowledge messages will be removed from the queue immediately however messages will persist in that queue for up to seven days before they're being removed so hopefully within that seven day window you'll be able to take that pups of data as shown in the presentation and move into some sort of persistent storage solution right like like bigquery with that oh yeah we finished up the Q&A portion of the presentation we'd like to thank you on behalf of sundar Intel and Google like to thank you for joining today's broadcast we hope you learned a lot and appreciate your time this morning thank you thank you you 