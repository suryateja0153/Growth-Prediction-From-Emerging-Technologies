 [Music] hello everyone thank you for coming in my name is Carolyn I'm product manager on Google Cloud Storage and today we'll talk about selecting the right storage class for your workload whatever it might be either content storage and delivery big data analytics or a long tail content that you may not be using as much I'll have quite a few speakers joining me on stage later on in this presentation I'll introduce them as we go but let's start with Google Cloud Storage overall so GCS or Google Cloud Storage is highly available durable and simple object storage we see a lot of success in DCs across different storage classes big data analytics Condor storage and delivery and cold storage as well and GCS is built on exactly the same infrastructures we use internally at Google for different object storage and chances are if you have not used this before probably you've used exactly the same technology for any other Google products and as I said exactly similar structures exactly the same engineers maintaining and building building the infrastructure same datacenters same desk same racks and same network so we see quite a lot of success across those scenarios and let me walk you through a couple examples how customers are using this yesterday so starting from left to right with common storage and delivery we see quite a few customers using this yes for serving video serving music serving images serving any general multimedia which is serving web sites and static content through GCS then on Big Data side if you have a lot of data if you have petabyte scale 100 petabytes scale or even exabyte scale you need to keep your data somewhere and GCS is the place to keep your exabyte scale data and then run compute jobs on top of that so we have quite a few customers who run big data analytics that run their customer pipelines custom workloads they do different genomics processing differently commerce analytics IOT different manufacturing modeling and you see quite a few of those examples here and then if you look at those customers many of them have hot data and working datasets that they were processing regularly but then they also have some data that they want to archive or backup for long term and pretty much every single customer will have some sort of in frequently accessed data or some sort of cold data other than that we also have customers that use dcs purely for storing cold data and long-term archive will also have quite a few partners that use GCS to help ingest data from one pram or help archive different enterprise data in dcs so this yesterday has four main storage classes and I'll walk in detail across all of them throughout this presentation so starting from left to right so left the hardest one is multi-regional storage it's built for common storage and delivery its G redundant so it's perfect for business continuity if a single region in dcs goes down or something happens to that region you don't need to worry about it will keep serving your requests because your data's Jared and n't across two or more regions within a multi regional location and we see customers use the storage or streaming videos serving images and website content and documents second class is a regional storage class it's built for data analytics within a single region if you are running some compute workloads you want to have your data stored right next to your computer clothes you want to have lowest latency possible highest throughput possible and regional storage class is the right place for storing data for different analytics for clothes or general compute workloads and what of those storage classes are built for a hot data and they provide highest level of availability across our four storage classes then moving to the right you will see two storage classes that are built for cold or cool data so that's near line which is built for data access less than once a month and then it's called line data that's accessed less than once a year on average and we see customers use those storage classes for serving a long tale content or for disaster recovery or for archive beautiful part about those two storage classes they have exactly the same API as any other storage class in dcs so it's very easy for you to integrate with those storage classes once you start using dcs either it's hot date or cold data you can use any other storage class because it's exactly the same API and for all of our storage classes we provide online latency so that means you don't need to engineer around data retrieval or anything like that because all of the latency is consistent across storage classes so let's go into detail into construction delivery I'll start with our multi regional storage and then I'll invite product manager from firebase to talk more about firebase and how is helpful for content storage and delivery so our multi-regional storage is best for content serving as I said before its G R done on the cross stone regions and we provide very simple API and very simple pricing so it's exactly the same API is for regional storage just provide location name in this case it's us.you or Asia and then you create a bucket and start uploading data you don't need to worry about your primary or secondary regions will take care of all of that it's a very simple API to use and because data is G redundant we'll take care of different file overs and it's very good for a business continuity as well because you don't need to worry about some region going down inside of these years so as I said we have three multi-regional locations today so I have a lot of different regions in Google cloud platform and we're building pretty much any region every month or every two months so right now I have three multi-regional locations that unite multiple regions inside of them so one of them is us the other one is EU and third one is Asia and customers that want to serve content either within those locations around the world they could choose one of those storage storage locations so let me talk a little bit more about that simple API for multi-regional storage classes so here's an example of creating a multi regional storage class with a common line utility it's called gsutil and I provide a type of storage class which is multi-regional I probably location u.s. and then I just named my bucket literally there is nothing else you need to provide we'll take care of everything else will take care of where to store your data and you don't need to worry about it and when you retrieve your data again you didn't need to specify any locations just provide your bucket name domain name and then name of your file that's it you don't need to worry about routing will take care of routing within our network you don't need to worry if it's east-west Europe us will take care of all of the routing on our Google Network multi-regional storage class was also optimized for end-user latency because there are multiple layers of caching that we have in dcs so even though you might be storing data in multi-regional us if you have customers that are accessing the data from Europe will cache the data in Europe and our data centers similarly in Asia Pacific will cache in our data centers in Asia Pacific and on top of that if you set your objects to be public we'll also catch those objects at the edge in our points of presence we have around hundred points of presence around the world and we'll leverage the same infrastructure as for other internal Google products or for instance YouTube and we'll cache the data in those points of presence so your consumers in large cities around the world will have extremely low latency because data will actually be served from location right next to them in the city in terms of price we charge very simple 2.6 cents per gigabyte month for your data in multi-regional storage class and it will take care of all their applications underneath you don't need to worry about paying for network to replicate your data across data centers you don't need to worry about secondary copy or tertiary copy everything is included in that simple 2.6 cents per gigabyte month price we also charge for read operations and write operations and we've recently decreased that price by 50 and 60 percent we decreased at half a year ago and we have very competitive price on storage and operations there in terms of availability typically the storage class runs at four nines availability and we provide three and a half nines SLA so if for some reason it drops within a given month below three and a half nines we'll pay you back for using the storage class depending on how available it was and it's also designed for 1190 durability all of our storage classes are durable so you don't need to worry about some storage class being less durable than the other we provide the same standard durability across all of them turns out a lot of data is actually being consumed on mobile phones so it's extremely important for our customers to be able to build applications and be able to serve the data and consume the data from mobile phones so I'd like to invite Mike McDonald who is a product manager on firebase to talk about using Google Cloud storage with firebase API thanks Grill how's everyone doing today good awesome how many of you have heard of firebase a couple people how many of you operate mobile apps hopefully oh come on you guys the world is mobile first so what firebase is oh I have a clicker firebase is Google's mobile platform so firebase allows you to develop your applications on cloud platform grow your user base using our we have pushed notifications we have application invites all of that kind of stuff and we also have a monetization platform so you can earn more money by integrating with our products but for those of you who are building mobile apps how many of your mobile apps are serving some kind of rich content so you're serving media you're serving images or videos all of that you need to be able to easily upload from the user's mobile device and then transmit those around the world firebase offers really great mobile support for Google Cloud storage through our SDKs and those SDKs when I go to the next slide are optimized for mobile content delivery so we handle tricky problems like really granular security so if you want your users to sign in via Facebook and upload a photo we can integrate with firebase authentication which does server lists sign on to Facebook and then lets you again without a server upload directly from your device to Google Cloud storage to any Google cloud storage bucket we also do that on the clients we have very robust networking so how many of you guys are enjoying the conference Wi-Fi things are pretty slow are you like trying to send tweets and take pictures and share them with people have you had any uploads fail it's because people aren't doing it robustly so our networks are gonna do or our SDKs we're gonna do resumable uploads and I'll actually show that because I've been having fun doing my demo earlier over this network and so we're gonna make sure that your uploads and downloads get to where they need to go even if the network is terrible and as Carl's been talking about it's all built for Google scale right so we support five terabyte uploads direct from your mobile phone and I actually have a standing thing you can tweet at me if someone ever finds out how to upload five terabytes from a mobile phone let me know cuz I have a job for you so let's switch over to the demo because listening to me talk about this is really not exciting how many people have programmed an iOS app or does Swift code look at all familiar people maybe hopefully it's not too crazy I have over here an iPhone app and it's gonna look really boring when I start it up because there's nothing going on but if you click that little camera button it's gonna take us over to a photo picker and what we're gonna do is we're gonna take a picture from the phone upload it to Google Cloud Storage and then download it again to the device so we're gonna do that all with firebase so this is actually about a hundred lines of code just as a shell all right yes this works we are going to and I didn't delete my sample there we go we're gonna first upload the image so we're going to create a reference to our fire our cloud storage bucket and then we are going to upload our image just using this put method and so that takes the image that we clicked and is gonna upload that to Google Cloud storage we'll also add an observer to let us know the progress because I was noticing earlier today it's pretty slow so what this is gonna do is it's just gonna print out and let us know how quickly things are going so I will rerun that and to prove to you that this is gonna work the app will open up over here you will select the photo by the way the apples system photos are about 20 Meg's in size they're pretty big oh and there we go actually the internet was pretty quick so you can see that we fully uploaded our file let's pop over to the firebase console and refresh the page and we can go ahead and see we now have a folder that says photos and we even have the photo in there that was like one line of code that uploaded a photo no no clapping okay thank you slice what small applause that's we're not done yet so now we're going to synchronize that so we're actually using another firebase product it's our real-time database and we've already written that data basically a shareable URL for that location in cloud storage and now we're to download that image and display it so all you need to do there is you can take that URL that we synchronized via our database and just say get the data and there are about 20 Meg's so I'm making sure that we have enough and then we simply put it in our app and reload I'll go ahead and start that back up and the nice thing actually about the database is it's gonna immediately take the first photo that I uploaded last and it's downloading it right now and there it is so that was actually pretty quick but because I like to live dangerously and because I'm a millennial I have another version of the app on my phone and I am going to take a selfie with all of you and let's see how long this takes so it's uploading right now and there we go oh I'm upside down sorry about that but there you go so that was maybe 10 lines of code including all of the application logic to upload and download from Google Cloud storage on an iPhone app thank you all very much and back to Kirill thank you Mike so all of that is still backed by Google Cloud storage so that means when you're leveraging firebase all of the data will be transferred by Google Cloud storage will be stored on Google Cloud storage and transfer by Google Network so it will be amazingly fast and it will scale to extremely large applications that's serving that content around the world so let's go back to big data analytics now so if you have a lot of data you need a place to store it before you analyze it and Google Cloud Storage regional bucket is the right place to store the data so data with GCS regional is redundant within a given region and if you select compute engine and GCS regional in the same region you'll get the lowest latency possible between those two and it's built for data access frequently so that means if you have some application and process the data a lot this is the best storage class to use similarly to any other storage class GCS is very deeply integrated with other products within Google cloud platform like data Pro ml Google compute engine genomics even bigquery so you can potentially store data in DC as an queried directly out of bigquery a federated query and a lot of customers use the storage class to run different pipelines and analyze their data from either their customer plans and Google compute engine or by our products and because it's so well integrated it's very easy to get started let's say when you're migrating from Hadoop on Prem migrating to Google cloud platform so one of our products in Google cloud platform is cloud data proc and it's essentially manage spark and Hadoop service and we have a cloud storage connector which is HDFS compliant as part of that product and that means it's very easy for you to move from Hadoop and pram to GCS and de the proc so here's the example at the bottom of changing your code essentially just changing one line it's how you're referencing a storage and everything else all the other code will keep working and that Cloud Storage connector will take care of everything and you don't need to change any other lines of code and that solves a lot of problems for customers because data is stored in GCS you don't need to worry about scaling the data over time like you couldn't you might need to worry on Prem the data is also accessible outside of your Hadoop clusters you can turn it down and your data will still be there it's still in GCS you can turn up another cluster and access the data so that provides a lot of simplicity and a lot of flexibility for our customers if you want to read more about data proc you can go to cloud google.com slash data problem similarly to our multi-regional storage which are a very simple price of 2 cents per gigabyte month which are all super operations and we provide three 9s availability isolate so as you see our SLA for regional storage class is slightly lower than the one from multi-regional where we provide three and a half nines and that's because regional storage is not G redundant as multi-regional storage class and it also has 11 eyes durability it has instant access like anything else in GCS and we have exactly the same API that you would use across all storage classes so once you start using regional storage class you can easily switch to using any other storage class but enough hearing for me I'd prefer to hear from a real customer so I'd like to invite fastly over dear customer and partner and specifically Arthur Bergman their CEO thank you thank you this works hello everyone my name is Arthur Bergman I'm CEO of fastly and we are both secured a customer and a partner of Google so what we are is a edge cloud so in in the old world you know you had a whole bunch of edge devices in your own data center and as you're moving through cloud those old expensive inflexible unreliable Hardware machines that you had at the edge of your data center have known no longer anywhere to live and so that's where we come in so we complement very well together with the central cloud so we integrated really well with Google so we provide caching edge compute TLS termination load balancing this ability as a complement to the storage and computer and etc that you get on another side so it works we have inter connections with Google in quite a few locations around the world to the Google Network and then we have our own pops all around the world where we store your content process your content defend your content to end-users whether those are humans or machines or etc all around the world and so could also at some point be described as the CDN we have they weren't really cool things that we found with Google and is the cloud functions so one of the core functionality we provide right is that you want the view that the rest of the world has of your content to be up-to-date so we do cache invalidation or purging in about 150 milliseconds worldwide and so here's a very simple cloud function for Google Cloud storage so when you upload or change an image or any piece of data on Google Cloud Storage that cache invalidation goes out worldwide and the new content is available from from any edge pop completely transparently so we we really like working with partners who are really really good and provide really good performance and service and reliability to our customers because at the end of the day we're here to provide fantastic reliability and performance to our customers and if we bring in partners to work with partners who don't that causes problems and so I went and looked at some performance metrics that we're collecting from Google Cloud Storage versus Amazon s3 and this is not a synthetic tests is actually looking at very very large data sets of content being fetched from both and the difference is pretty dramatic so the scale on the lower is different but what you're seeing there is a average time to first byte of three to five milliseconds versus 200 milliseconds right and so again this is very to two very large data sets where we're fetching cache misses from s3 or Google Cloud storage and in both cases its s3 East and you see GCS East so which is one of reasons we really love working with Google because this just creates a better experience for everyone both engineers at our customers and their end users so the the use cases we're looking at and we have some some pretty amazing customers together Spotify Vimeo and and firebases earlier are obviously around caching caching closer to user but also in the big data analytics side we integrated really well and from from a security point of view the real-time aspects of the platform and the integration with Google allows you to do some some pretty amazing stuff so well use cases are in this case they're pretty simple basically like Vimeo is obviously video and images Spotify is as people can guess a tremendous amount of audio but there's a lot of other things cool things that they do with us as well we also do real-time log files from the edge which is pretty key because as you're moving functionality that you used to have in your own sphere of control you need that you know when you had it in your own data center in your own control you knew what was going on in real-time and you need that once you move it out into the cloud as well and so we have these real-time logs that update in about a second and we can stream those into Google Cloud storage and then get that into bigquery in in pretty much real-time and something we use internally very much so being of seeing that the two-way integration that just creates a really nice experience for engineer developers and operations so internally we use that a lot for analyzing performance of TCP connections between end-users and us so we see on the order of three four million or TCP connections per second at any given time we'll have about 150 to 200 million TCP connections open and we want to know as much data about these as possible to detect problems on the Internet route around those problems etc and what we ended up using is bigquery and we put in about somewhere between about fifty and a hundred thousand records per second into bigquery we also use it for for edge enforcement which will go into the millisecond and then we have customers who use it for in ad tech space or for beacons they're collecting data through fastly and then instead of having to run their own data center like their own servers to terminate that request and put it into a Google Cloud storage they're just having fastly dump it directly into cloud storage so server less basically we use it for edge methanol it expect and we provide edge security for our customers and so we use school GCSE and bigquery and edge logs for that and basically very quickly can pull any kind of report from the edge to see what rules are hitting what kind of threats we're seeing normally scores etc and so and we can then expose that to our customers so the the total pipeline of end user fetching something from us we don't have it we fetch it from Google if we have it we give them we process it we apply with our rules and logic around that that we need to and then we log it into Google and then we use Google tools for analytics and so you can integrate that with whatever pipelines you have for other kind of analytics we're using tableau or something like that or one of the security packages and it works works really well and then you can use the standard Google visualizations that they have to just create reports really really fast so the the combination really lets you build a very very compelling replacement to kind of the old combination of having your own storage and then have a boring CDN out there it doesn't actually do what you wanted to do and either saw that and the that combination the two-way integration really allows you to drive a lot of value and the value really comes from to two points one is the the customer experience your end user experience I think a very important point is also the internal engineering and product experience right so you can it allows your developers it allows your product engineers your product managers to just move faster and to get new functionality out because all the api's all the documentation api's functions modules etc are just so accessible and we all and you don't have to go through it's kind of painful process that you traditionally had to do to get all these systems working together so you can release code faster keep your engineers happier and keep your by extension your end users faster so we're we're big fans of Google we use them internally we have very big customers together and we would love to have more of you using of us and Google together thanks everyone Thank You Artur so we've talked a lot about big data analytics and content serving but all of those customers they have some sort of cold data so over time they collect a lot of data and the data may not be as commonly used as it used to be when it was just first generated and typically customers would want to save money on the data I'm sure everyone knows to save money on their infrastructure and if the data is not being used that often probably you want to pay less for the data that's why we have two storage classes that are designed for cool and cold data's one of them is near line I don't know cold line I'll start with near line here so near line is a storage class that's designed for infrequently access data it provides the same instant access speed and the same API as other storage classes and it's extremely important for our customers so what that means if you're using multi regional storage class for content serving and delivery and then turns out not all of your data is frequently accessed and you want to store some of it in your line well turns out you can use all of your integrations you can use our partnership with fastly to serve the content around the world or you can use any big data tools that we have at Google cloud platform and you don't need to change anything because it's exactly the same API it's exactly the same latency you didn't do anything extra on top of that so some typical examples include storing historical data and then maybe analyzing the data in the future so sometimes you don't know how the data might be might end up being useful for you in the future so you want to keep it around and still probably have low latency and some chances to access the data in the future customers also using your line to serve long tail content so if you have a lot of different multimedia content some of it will not be as frequently accessed as the other ones so you can put your called content or your long kondeh Nearline so when your line we charge one cent per gigabyte month on storage and also charge for access of one cent per gigabyte of data read out of Nearline and if you work out those two numbers and storage price for regional multi-regional storage turns out that Nearline becomes the cheapest storage class for data that's accessed less than once a month and that's where the statement comes in that Nearline is built for data that a it's accessed less than once a month when your line we provide a cell like you provides to nine SLA and it's consistent API and consistent performance like with any other storage classes so if you've been with us for a couple years you might have seen near line two years ago and back then we had four second latency in your line and we've been working very hard and improving performance and reliability of new year line and any other storage class so over last year we've increased availability SLA on multi-regional storage class and also decreased latency in near line all the way from four seconds all the way down to instant access speed with an airline and we're committed to continuously improve our performance and reliability in Google Cloud storage and those improvements we've made with the near line last year bring it all the way to instant access speed allowed us to ship a new storage class that's called cold line we've launched it last October and cold line is best for call data specifically data that's accessed less than once a year on average similarly the near line it has all of the integration points it has exactly the same API but because it's built for very cold data customers usually use it for archive or storing multimedia source files so if you don't expect the transcoder your data but often you still want to have your multimedia sources somewhere and customers choose call line for that scenario and also disaster recovery and that's a very important point with disaster recovery in case something happens to your core data set you want to be able to pull the data out very quickly and usually customers cannot use tape media for this or any sort of media that has latency in it and with cold line you have instant access speed so you can start accessing the data right away and more than that because cold line has exactly the same API sign out of storage class you can start serving directly out of cold line or using data the data that you're storing call in for processing right away without any delay because API is exactly the same and customers find it much more simpler than any other solutions in the market that they have they just switch the traffic to different bucket and they start using it in terms of price we charge point seven cents per gigabyte month for cold line and we also charge five cents per gigabyte on data access and that works out that if you data's access less than once a year cold line becomes the cheapest storage class for you to use we also provide availability SLA for cold line we provide two nines availability is exactly the same as an airline and we also have minimal storage duration for a cold line at 90 days so let me show you a little bit about that API in that performance so people sometimes don't believe me do you really have exactly the same API well it's exactly the same here is an example of doing get or food HTTP put with cold line and it's exactly the same API as you've seen few slides back with multi-regional storage you provide name of your bucket in my case it was cold line I was the one to claim cold line as the name of my bucket you provide your domain name and then your file and that's it it's exactly the same we pay as anywhere else in terms of performance here's an example of reading one megabyte out of cold line so I have a utility that's called gsutil Google storage utility and I run perfectly agnostic with that utility and unloaded one megabyte file from code line and as you can see here it's between 150 and 200 milliseconds to download one megabyte file which is quite impressive many customers cannot achieve this with their other solutions for hard data and we achieve this with cold line less than 200 millisecond latency to pull out 1 Meg file out of coldest theory that we have so we see a lot of customers tearing the data from hostage to cold storage but they can also tear their storage from on-prem all the way to cloud and all the way to cold line so I'd like to invite karthik an ortho who is director of product management from dell MC to talk about our partnership and hearing to Google Cloud storage so for folks of you who don't know me my name is Dean I'm part of the product management team for Isilon which is part of the Dell business unit so Iceland you know we started as a purpose as a company back in 2000 2001 with a focus on trying to build out a scalable NASA platform to serve the needs of some specific because right specifically to life sciences to meet an entertainment specifically customers who need a lot of unstructured data and a lot of highly throughput and highly scalable data or time the company's grown as part of EMC and operas delve we've got about eight thousand plus enterprise customers using my salon what makes Isilon unique to all of these customers and why they continue to to invest and buy more of iceland is because it's it's a platform that's highly simple to manage it's extremely scalable we've got all of the enterprise great capabilities that customers expect on a platform and and all of the enterprise grade security elements to it everything from SEC compliance to do to you know typical security tools so all of the capabilities that a customer requires on the enterprise is now all available for your scale out nas platforms as well right what makes all of this very interesting to a lot of customers is the fact that you have that same level of efficiency and skill as the amount of data starts to grow and for me manage the data today you know that the largest part of your data that's growing is unstructured data which is roughly growing at about as markets as roughly about three times the rest of the end of price right and so being able to have a single user experience for your data which is both structured and unstructured is what customers look for right and that's going to where be from a business perspective and from an Isilon perspective approach approach the build that we do right which is give customers the exact same experience whether they're running the data on the edge clusters whether they're running this the data within a code data center whether they're running it in the cloud right and now what makes the cloud interesting is his customers look at at cloud both from a data mobility perspective and from a flexibility perspective but also looking for four ways to be able to offer lower-cost storage for some of their warm a hot and potentially even cold data right so from an EMC perspective and from a Dell perspective we've we've got our own solutions we've got a product called elastic cloud storage which allows customers to move data between Ice salon and an on-prem storage now what makes on Prem storage interesting is it gives customers the ability to access the data within the same protocol timeouts within the same expected reliability and response times from the cluster now what that means is when we started looking at okay we've gotten we've got an on-prem solution the form of Isilon we've got an object platform which is also on Prem customers are looking for off-premise Aleutians customers are looking for public cloud solutions and so we've been starting we started to look at a number of cloud targets that we can support and a key element is supporting different cloud targets is can we have a cloud target that enables customers to get the dollar per terabyte that they need for their storage and also can they get the predictable SFA's and the predictable Layton sees in terms of access to the data right and that's what brings us to do partnering with Google obviously as a very important partner for us is enabling customers to start at tier all of their cool data from the Isilon onto a Google Cloud storage target right so customers today using Isilon can put their performance data their high throughput data on a local on purim Isilon cluster but as the data starts to age and the data starts to gets cool you can start to move it off to a Google Cloud storage target and still be able to access it within the same namespace that they would with the data on Prem right well that gives customers is essentially a seamless experience right whether the data is on Prem whether the data is on a different tier of storage within the cluster or whether the data gets teared off to a Google Cloud storage target you get the same experience from a cluster perspective you have the same security credentials you have the same enterprise features it's largely agnostic and largely transparent to the customer whether the data is on Prem or whether that's in the cloud and that's important for for a lot of the enterprise customers and I'm sure a lot of you can relate to that as well is your end customers and end users essentially are going to say here's what I need from a storage perspective here's the expected s ole's and here's the expected performance characteristics that I expect from a storage u storage vendor go figure out how do you provide that to me and for us being able to do that with the vendor like Google enables customers to make that choice of whether they won't keep the data on Prem or in the cloud but actually keep it seamless for them so we've talked about tearing data from on prime to cold line and to near line turns out that many customers want to tear the data from hot storage to cold storage as well so before we go into tearing I'd like to remind you all of the storage classes that we have in Google Cloud Storage today so we have four of them it's multi regional regional near line and cold line so two left ones multi regional regional are hot storage classes multi-regional is G redundant across two or more regions there hundred miles apart and then regional is designed for hot data stored within a single region and multi-regional is typically used for common storage and delivery and regional is typically for big data analytics within a given region then we also have two storage classes for cool data that's near line for infrequent access data and also have cold line storage class that's for data that's accessed less than once a year that's typical archive and very cold data and across all of them we have the same durability eleven nines we have the same instant access speed we have the same API across all of them one interesting feature that we shipped last year was object level storage class and data lifecycle across those storage classes so we see many customers wanting to use multiple storage classes at the same time and aged out their data from hot storage to cold storage over time and pay less over time and we've seen customers copy the data from hot storage to cold storage and when they decide they need to use it they'll copy it back and that was quite inefficient for customers so we wanted to make it very simple for them and simplify that they don't need to worry about that so we've created per object storage class with project storage class you can keep your bucket you can keep your name of your objects and you just change settings on a given object and say now this object is an airline or now this object is called line but everything stayed the same so muck it's an object state in place it's the same object name the same bucket name the same API call so if your application is using that object it wouldn't notice any change and everything will keep working as intended and we also ship data life cycle across storage classes you can create a simple policy that will say if my objects are older than 30 days or some number of days them to Nearline as we see a lot of customers use their data very frequently within first 30 days but then they no longer use the data after that after those 30 days and typically they would move the data too near line and after a longer period of time don't move with the cold line as they don't expect to use the data much longer so what's cool about this project storage class and our consistent API across those storage classes once data ends up in cold line you don't need to worry about restoring it back or copying it somewhere you can keep accessing against the same API against the same object name and that provides very simple solution for our customers to do data between hot and cold storage I like to compare us to s3 because there are quite a few areas where we're winning so starting from the far right is multi-regional storage and our competitor does not have really direct competition in that area so you could create s3 standard with cross user application and you would need to pay for two original copies and you would need to pay for network to replicate across those two original copies and that adds up to much higher number of like 4.2 just for storage 4.2 cents per gigabyte month plus network to replicate across the regions and with us we have a much simpler solution with multi-regional storage just create one bucket and we'll take care of everything for you you don't need to pay for second copy you don't need to pay for Network to replicate it you don't need to worry about failing over between those two regions if one of them goes down we'll take care of all of that for you and because you provided simple API all of our storage classes have exactly the same API and exactly the same performance you don't need to configure anything extra outside of those storage classes so then if we move to a regional storage that's for hot data accessed within the region we have a quite comparable product it's GCS regional versus s3 standard and we are slightly cheaper than our competition around the world and then going to infrequently access data we have GCS near line which is at 1 cent per gigabyte month and our competition charges 1.25 cents per gigabyte month so again slightly cheaper there and then going to cold storage so have product that's called CCS cold line we launched it five months ago and was called line we have the same API and the same latency characteristics as any other storage classes and that means customers need to worry about integrating with it or treating it differently like anything else and there is no direct competition in that space so when customers think about storage and comparing it to Amazon s3 usually they think it's a poster post comparison between Google Cloud storage or GCS and s3 but turns out with the street need to add a lot of different layers to compare to what GCS provides so if you want to use really cold storage you need to add glacier you need to figure out how great with it you need to restore date out of Glacier Bay before it becomes usable if you want to have multi-regional storage or zero down on storage you need to use your R which is an extra API feel you can figure those regions and failover between them if you want to use high throughput Network or if you want to get high throughput for your data you need to use accelerated transfer and they charge extra for that with us we don't charge anything extra all of your data is accelerated out of the box because we use fantastic Google Network to transfer your data and then if you want to use CDN you need to include cloud front with Amazon with us edge caching is part of the product and as you've seen from our tour we have fantastic latency because of that because we have so many different tiers of caching and it's part of the core product that you don't need to pay anything extra you need to configure anything on top of that it's just part of our simple Google Cloud Storage to summarize we have a large portfolio that covers many different use cases so we have a multi regional storage class that's perfect for condon storage and delivery we have regional storage class that's perfect for big data analytics we have near line for infrequent access data and we have called ahead for cold data all of the storage classes are united by the same API the same online latency the same network the same peering that we have at Google now we have lifecycle across those storage classes that you don't need to worry about configuring or copying any data between those storage classes and then changing your API everything is very smooth for you right now and also have lots of different partnerships that help you integrate and help you use Google Cloud Storage in any scenario that you might have so you can learn more about low cloud storage cloud Google comstar storage and you might have heard that this morning we've announced the much larger free trial with Google cloud platform and also announced free tier which includes Google Cloud storage now so everyone has access to free five gigabytes in United States and it doesn't expire there is no limit on it so it's free tier feel free to use it and feel free to try out Google Cloud Storage so I have quite some time left so maybe a few questions if there are any questions in the audience so we had three guest speakers so all of us can answer your questions and I was asked if you have any questions please stand up and walk to the mic because we are recording so that will hear a question in the recording as well thank you [Music] [Music] 