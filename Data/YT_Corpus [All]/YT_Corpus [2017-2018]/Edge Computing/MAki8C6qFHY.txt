 I'm with PDFdata.io, I'm not going to get into anything related to what that does, this is not a sales pitch, but while I'm up here I feel I must take a minute to recognize how lucky I am to be here, as we all are, and maybe take a moment to recognize those outside those four walls that struggle to survive. Thank you. So programming data for display. And what does this mean? To really crudely characterize what we do and what we're interested in, we do computation, right? And how might you think about that? You get things coming into our world of computers, through all sorts of sensors and human input and machines talking to each other, computation and communication of all sorts happen in between and at the other end you have some kind of output, some kind of result and the output that I'm talking about right now is display, obviously one of the essential mediums with which we have to perceive these sorts of results, and we're all very interested and very immediate feedback from the sorts of work that we do with our computers, as you might have seen previously with the visualizations of the keys with that spirals of tonality display and animated visualization is a special way to have the data that we're performing. But it wasn't always like this where we could have things twirling around on screen. All through the prior decades of course we have had a lot of mediums for displaying the data and information and results of computation from telegraphs and teletypes through all sorts of dynamic displays, especially during the Cold War motivated a lot of this development, especially through air traffic control and simulations research for vector displays and then raster displays, but also printing has been around for hundreds of years, and one of the things that so many people throughout the computer revolution were concerned about was how to distribute the results of computation and put tools into the hands of as many people as possible, so that you could have modern equivalents to the movable type press essentially and so fax machines for example were a huge, you know, formational input to PDF and all sorts of page description languages that were sort of percolating around in the '60s, '70s, and '80 as computation power made it possible to relate with information from your computer in ways that were richer than teletype. And there's an interesting story of how CAD and CAM systems are a variety of display that happens to manifest itself through -- and it has correlation to 3D printing as well. Just to sort of set some ground terms, what page description language means is any characterization of a layout of a page that's more efficient or expressive than a simple bitmap. So if you're familiar with bitmap graphics or rasters, you might have a 1440 by whatever bitmap, and that's sufficient to describe what's on my laptop screen, perhaps, but that's obviously not sufficient to describe what you can print through a high resolution offset printer, or a vector-based plotter, and so there have been progressively more capable page description languages over the past 40 years, essentially, trying to tackle this problem of how do you describe what some set of data as a result of computations or communications should look on a page or a screen or on a plotter, and other media, but in a way that is independent of that medium. So there are dozens and dozens of documented page description languages, you can go on Wikipedia to find references for a variety of them. These are some interesting ones I'll talk about a little bit. And all of them are trying to characterize a page and something besides draw a pixel here, then draw a pixel here and draw a pixel here, etc., etc., obviously not practical, even today, never mind back in 1978. So I'm going to sort of going out an a limb a little bit and say that ASCII may have been one of the first page description languages and this isn't talked about in any of the Wikipedia page or any of the modern literature about page description languages because it is so primitive of a medium, but these first 32 control characters here make possible the -- make it possible to force an output devices, like a teletype to render a set of data in a way that is visually pleasing, practically consumable, and so on, so for example, fixed-width tabular data like this was commonplace for centuries, essentially, where you had first, you know, lead movable type printing presses being used to produce tables like this for newspaper consumption and things like that, and then on to, you know, teletype printers that produced displays of tabular data for efficient viewing and consumption by humans in a way that we can understand this data. Outside of this kind of visualization, because it is a visualization, understanding this data would be extremely laborious, right? And it's only because of the control characters that were baked into ASCII, for example, there are plenty of other competitors, but it has these control characters that allow the printhead at the time, to move forward a space, move back a space, move down, run to the next sheet of paper, etc. Thereby allowing an author of a program to fully control the materialized view, so to speak, of a set of data. And so as demands increased over the years and various considerations drove development for more time, essentially, we got to the use of CRTs and vector displays for both realtime -- this is actually an asteroids game that's being implemented on top of an oscilloscope, but exactly the same technology was used originally to implement air defense systems, air traffic control systems, and hear video games and all sorts of realtime simulations like planetariums back in the 60s, '70s, and '80s. You want to be able  to include raster images. So you can produce it onto a printer or other output device. And then probably the hardest nut to crack, so to speak, in the history of page description languages is text and typography. Not just because of -- not just because of -- there are so many factors, obviously, [laughter] So with both vector graphics and rasters, the domain is very well defined and constrained, right? So if you want to draw asteroids, I know what you need to do. You need to describe that I want to draw a line here and a line here and a line here, etc., etc., to produce the, you know, vector representations of your asteroids and spaceship or you know, nuclear missiles or whatever you happen to be doing. Likewise, raster images, an artist produces these laboriously by hand and they need to be ferried along to whatever is going to be doing the document. Text and typography, on the other hand, these are an enduring role throughout the construction of a document and throughout the maintenance of a document. There is this notion of fonts which honestly didn't exist prior to the more sophisticated page description languages likes Postscript and as you get no, if you remember the ASCII chart there, there's only 127 code points compared to for example, unicode today. So these are a lot of different challenges that page description languages have to cope with and the history of the most successful one, which happens to be PDF, is sort of littered with the inventors of it learning along the way how to cope with these particular challenges. So just a little bit of sort of thumbnail history, there was this company called Evans and Sutherland that in 1968, I believe was founded to custom produce systems for realtime simulation and CAD/CAM systems, so they built the projector systems in planetariums, and also the very high precision mechanical canons through computer aided machinery and the way they did that is they used an internally developed page description language where the page was the set or the operators over the quote-unquote page were a set of motions that a drill head or a router might make on a block of aluminum in order to produce a particular artifact. It was all the same set of fundamental technologies beings whether they were, you know, joking on the sealing of a planetarium or producing a cog for a machine. So a couple of fellows, Chuck Geschke and John Warnock worked at Evans and Sutherland for a number of years and went off in the mid '70s off to Xerox Park where they helped to develop all kinds of things there including interpress. It was essentially bytecode, so if you think of the production of a set of vector graphics, like I said, you want to put your pen down somewhere and you want to draw a line, 20 coordinate points this way and 10 down, and then along this way, 10Up and 12 right and an arc of a particular, using a particular set of parameters. Those were all encoded using bytecodes that were incredibly efficient and very necessary given the space and computational constraints at the time. We're again, talking about, you know, 1978 or so. Lower is that John Warnock  was upset. In 1970s, dollars it was a billion-dollar company, producing printing systems and they were using -- they would build a new page-description language for every printer that they produced. Obviously an incredibly laborious thing. And interpress was John and Chuck's attempt to make a device-independent output medium. So you could take the same interpress file and send it to a plotter, and a bitmap display and a laser printer, which they were experimenting with at Xerox, and have reasonable interpretations of that page description to the, you know, highest degree of fidelity that each device allowed. So they were frustrated with Xerox might being able to commercialize that and so they formed their own company called Adobe where they produced Postscript. Postscript ended up being baked into the first laser writer that Adobe and Apple essentially coproduced and that's essentially what set up desktop publishing and the consumerization of printing around publishing and design and all of that and I just described it in about 20 seconds, but the set of technical feats that were necessary are really astounding, because prior to Postscript and laser writer, I was talking about text and typography a second ago, there were, you know, a set of 6 fonts that each computer happened to come with if you were lucky. I remember I had an Omega as a kids and it literally had six fonts. They were hard-coded bitmap fonts. You had what you had available to you and there was no such thing as going and getting a new font that would look nice in your community flyer or whatever. In the process of developing Postscript they developed vector-based fonts and ways to describe those in an output-method-independent way and a way to encode those for different computing plat forms, including DOS and Apple IIs and the Macintosh eventually, etc. And then as a refinement to Postscript and I'll talk a little bit about Postscript again still, they developed PDF at Adobe and the first version was developed in 1983, I believe, and has developed over time to really saturate the set of use cases that people have for display and printing technology, and describing what needs to be displayed on any output device in an independent fashion. So this is Papers We Love. Sadly, as an accident of history, I suppose, there aren't a lot of papers that were published and peer reviewed about the development of interpress and Postscript and PDF and all the other page description languages back in those days. In part because a lot of this development was driven by commercial organizations as opposed to academia and so a lot of the information that is available about how these languages and technologies came to be is passed along as folklore essentially and I'm not sure who Brian Reid is, but there's an extremely comprehensive history of Postscript and interpress and a comparison between the two, from 1985 on a news group dedicated to laser printers. And probably 20 pages of solid news group text talking about the very minute technical differences between those, and how a Postscript represented an incremental but essential advance over Interpress, and if I remember, I'll such on some of those differences. And then later, in 1991, as Adobe started towards developing PDF, it's not a paper. Again, but it was an internal memo, essentially, within Adobe, where John Warnock laid out the broad vision of what they were called Camelot at the time which turned into page extension. And they were talking about the prevalence of fax machines and the problems that the limitations of that technology had for the people that were using it so if you were sending an contract or a love letter through a fax machine and it comes out on a horribly pixelated thermal mess on paper. That is not the kind of representation of intent that you want to have. When you go through the trouble of printing this beautiful document, you want to be able to send it to anybody anywhere in the world and have the other person on the other side of that communication channel see exactly what you intended. And fax machines were being used widely at the time, because of, you know, essential business needs, but they didn't capture that respect for the original intent of the author, essentially and so that's one of the key considerations that sort of drove a lot of feature development in PDF, being able to take this artifact, ship it somewhere, again just data, not over phone lines or the fax infrastructure and have that be materialized on somebody's machine exactly as you could see it on your side of the world. So that's a little bit of the history and how sort of from a human sense, how you know, the people involved in the project sort of arrived at PDF and I'll talk about a couple of technical characteristics of both Postscript and PDF. Their further antecedents are sort of lost to memory. I am aware of some interPressers, but I've never laid my hands on them. But I have some familiarity with Postscript and too much familiarity with PDF. So Postscript is a stack-based interpreted language. This is interesting insofar as other page interpreting languages are not fully declared programming languages at all. So SVG is an example of the latter where there is no stack, there is no higher-order abstraction of facilities within SVG you are strictly describing what should be presented on screen or via a printed medium. Postscript on the other hand is, as it says a stack-based interpreted language and so what this means is that you have a programming language semantics, you can write Postscript programs that can do anything in the course of producing out put suitable for a printer or immediate visual display device or other things. There's actually a little cottage community on GitHub of people using Postscript to do all sorts of non-printing, non-display-oriented things. There's tons of little Fibonacci examples and people that use things within Postscript that end up printing. That sort of thing. So you can do anything you want within Postscript. And Postscript provided for a lot of essential features that its predecessors didn't. For example, you can embed bitmaps within Postscript, where as in other page description languages, you would send the page description and then along with it, additional assets, images, headers, etc. that needed to be referred to using relatively fragile means, like a file name. And so this was -- this was -- ended up being problematic for things like fonts which you couldn't bake into Postscript, I don't believe, and so a very common thing and we run into this today, as people who are enthusiastic about academic papers, if someone happens to publish a Postscript file of a paper or that's the only version that happens to be available from 20 years ago, you might download it and you don't have the font that it was originally rendered in, and your system will automatically and usually unfortunately choose a bad alternative to render it in and it ends up being useless so again that goes back to one of the failings of Postscript that Adobe addressed within PDF where you could send a single contained blob across a wire and not have to worry about whether that person had the bitmaps or that person had the fonts that they needed to render that document faithfully. The most interesting thing to me for various reasons about Postscript is that it was an interpreted languages, but it wasn't interpreted on a -- like a general-purpose computer, necessarily. So you would have your computer attached to your laser writer, for example, in 1985, and you would send that Postscript file over to the a.m. Apple computer or whatever you happened to be using and that printer would evaluate it and render the result on each page and so this was an example of edge computing where the edges were these devices, where every single Postscript output device had a full-fledged, you know, two-spec Postscript interpreter running on its own hardware. Yeah. So like I said, I've been working on this for about an hour and a half or so. That's as far as I got with slides. [laughter] [applause] But thank you. But I have things I can show you. And someone's going to have to tell me about time, because I have no idea where I am. So if I remember I'm going to bring up -- so this is a -- this is the Camelot project. I wonder if it's worth trying to go to full screen here. Anyway, this is that Camelot white paper and it has some handy and easily digestible chunks of Postscript here so a big part of the Camelot paper is motivating the design. And the challenges that it has and one of the essential challenges that it -- that they identified was that because each device that renders a Postscript file needs to have that full fledged interpreter and because the language was a fully Turing-complete language that had all sorts of, even today, very interesting and useful features, for example, you could rebind any name at any time anywhere. And so this might remind you of a variety of dynamic programming languages that we use today that are difficult to optimize, and sometimes hard to use. And at this point in time, people were still writing Postscript by hand in order to produce very intricate designs, there were, you know, manual Postscript type setters essentially that would write Postscript to get, for example, the logo of a company just right and they would have this bit of Postscript that they would carry around in their documents around the country and one of their things was to simplify the set of operands so that it wasn't as powerful as a Postscript was with regard to the flexibility of forwarded to the programmer or the programmer that generated it. And here John Warnock is talking about this Postscript file that draws a ten-sided polygon that redefines this ang constant and then does a loop. This is -- this is repeat, this repeat operand will run this block of Postscript ten times and if you run through it in your head, it's producing a 10-sided equilateral polygon. What they wanted to do was get to a language where you strictly say what should be drawn on a page or through the output device so rather than allowing people to write subroutines, essentially, which is what this poly ends up being, that's a subroutine within that particular file, rather than allow people in programs to do these sorts of things, they essentially flattened Postscript into a language where it's still stack-based and it's still interpreted but you don't have the ability to define constants or functions for lack of a better term and you cannot rebind things like built-in operators. And so there are, you know, move and draw line operations within PDF, they're not called move to and line to. That was before PDF was built, but that's just a mock-up essentially. You do need to write out sequentially every single line, path, character, etc., that you might want to render within a PDF, which makes the interpreter simpler and makes them easier to optimize so that when you send a PDF over the wire to a printer that could render that PDF, it didn't need to be nearly as powerful. So they were already looking forward to, you know, portable computers and much -- still more constrained environments than what was typical in a work station at the time. So I think that's it for Postscript. Postscript is really cool. It's fun to tinker around with if you're slightly masochistic. Can people see that? OK, I'll bump it up one. Or a couple. Better? So I figured -- I think I've basically run through what I have, so I thought I would give a five-minute tour of what a PDF looks like internally. I'm sure you've all opened up PDFs in notepad accidentally and seen this garbage but I thought I would go through it and show a couple of the highlights. Within each PDF document -- each PDF document is organized as an object graph. This is interesting in a couple of different ways and actually addresses the same use cases that redefining procedures in Postscript had in that language, so you can define -- each object within a PDF can contain a set of operations, much like I just showed in the Camelot paper, describing how to draw a particular shape or what text to render at a particular point in the page, as well as sets of matrix operations that transform the vector space that describes the output device in an abstract way. So this makes it possible, for example, to have a single object within a PDF that describes how to draw a header in a document, and then throughout that document, each page's description can simply refer to that object. And so you don't repeat all the information that's necessary in order to render that header over and over and over for a 500-page document, you can describe it once and simply refer to its object. And so each PDF contains this object table. These are all byte offsets that you see and there's a lot of offsets in this particular PDF. There we go. Each of these is a byte offset within that particular file, that indicates where that object begins. And so  this is one reason why, for example, you can't just edit a PDF. A lot of people say, why can't I edit a PDF? There's a lot of reasons, but one of them is its essential structure is fixed at generation time, to a large extent. There are ways to add objects and mark these previously written objects as being overwritten essentially, but relatively few tools support that and so that's why PDFs are generally considered to be immutable once they leave their point of generation. One of the huge -- oops. Let me just find the -- I'm finding a font for you. And I'm following object references if you want. If you'd notice -- here we go. So one of the big innovations of PDF documents like I was saying was that rather than needing to ship the fonts necessary to render a Postscript file, for example, along with that Postscript file, you can embed and usually fonts are embedded within each PDF, except for there's a set of 14 fonts that were originally defined as part of the Adobe spec that are expected to be at every location. Which is like, you know, Times and Courier, and things like that, those basics that were available in 1994 or whatever. So within each PDF document you can embed whatever font you like, as well as encodings that map character codes that are used to output each character to that slot, essentially, within the font file, and this is where PDF documents sometimes run astray, where if you've ever tried to copy and paste text out of a PDF document, sometimes that works and sometimes it doesn't. Very often, the reason why it doesn't, unless it's just an image-based PDF where it's just a bunch of bitmaps from a scanner or whatever, usually why that happens is because as an optimization step, what you can do is take a font file that describes 156 or 256 or you know, 4,000 characters and when you output the PDF, if you know you only used 36 of them, you can shrink that font file, rewrite it so that character position 0 in that font, instead of being null, is the letter A. And the mapping between the character codes used within the PDF and the character codes that are used within the font are described in these dictionaries here. These are mappings between characters and in this case, separate objects that describe how to render each character. So that's one interesting side effect of the optimization steps that are available by dint of being able to embed any font file into a PDF. I wonder if this has any images. That one doesn't. Another interesting thing about PDFs -- Postscript could embed bitmaps, but they always based 64 that data, so it could be transmitted over ASCII-only conduits. PDF made no such constraint, and so if you open a random PDF that contains some images, you often find binary garbage floating around in it and so you have, you know, much to the chagrin of people that write PDF tools, you have this interleaving of clear text descriptions that relate to the object graph within the document, as well as metadata associated with each object. That's where you see this, this is a subtype of image, you have a width and height for the image, you know, how many bits per sample are found in that image, which color space is used, etc., etc., and then the key right here, this filter that uses the zip deflate algorithm, so as a parsing tool, you need to run your parser over that plain-text portion and then switch to a binary, you know, slurping of that data for however far its length goes in order to obtain the bitmap that you need to render on page or on the screen. And honestly I could do this for a long time, so I won't [laughter] I'm going to stop right there. Depending how time is, I can do more if people care. [laughter] But: -- so I'll stop. Thank you. [applause] 