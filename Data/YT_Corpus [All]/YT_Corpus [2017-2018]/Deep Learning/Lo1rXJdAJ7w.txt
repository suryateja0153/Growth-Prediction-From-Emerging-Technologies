 all right hi everyone my name is Peter really glad to be here really glad to bring the latest buzzword to cpp con so this talk is part of the series next year I might give a talk about blockchain with Super Plus maybe the year after we do html5 with C++ but this year we'll do deep learning so if you read the description of my talk you saw that I generally want to convey both machine learning from the researchers perspective as well as from the engineering back in perspective and the body of knowledge I want to cover is quite large so we need some kind of strategy and you probably all know the OSI model for networking which splits then the networking stack into various layers so I've taken the freedom of defining a similar model for machine learning which we'll use is a guide as we step through the different layers of abstraction of machine learning and he goes of the whole talk is actually just to show you that machine learning is not this this Python thing it's actually machine learning and C++ can appear in the same sentence and that's what I'll show you as we go down this stack so we begin not to task later you TAS clear I define as the level of abstraction where we think in terms of the high-level goal or end goal that we want to achieve with a machine learning system so we usually define such a task in terms of its input on its output a very common tasks might be classification in classification we have some data distribution say a set of images that we want to bin into a finite set of bins so a finite set of classes for example mapping images of animals into a finite set of classes a hundred different kinds of animals another kind of task that I've been doing research with are called generative models and generative models don't immediately appear useful but I'll give you an example how they might be useful imagine you're working on an open-world game like Skyrim and you want to generate lots of realistic looking sky so you don't want this guy to look similar you want to look at make it look different in different parts of the world so what you could do is you could go outside and take a thousand years of the sky that's what the course of a day and then you could use a generative model to simply generate more of that sky where the important detail is that a generative model will not just copy/paste the input to the output it will actually generate more of its input with variation so you could feed in your 1000 meters of the sky you know it will generate infinite or however many images of the you want and you could tile that over your world and have a very realistic sky one level with all that if we actually pin a particular task that we want to solve well usually have various model architectures that solve the same Kimani problem and for the task of generative models there is one kind of architecture that I like very much that's quite recent and these are called generative adversarial networks or Ganz for short and these are actually one of the few machine learning models where I think you could probably walk up to any random person on the street and explain it to them in a high-level and they would probably understand how a system like that could learn from data over time without any of the fancy math behind it so the way they work is that they have a generator which gets a variable Z which is a vector of say a hundred values so imagine an array of a hundred folding home values that's the input and it massages that and transforms that into an image so initially everything is initialized randomly so that image will be garbage but then there's a second component called the discriminator and it's task is to take an image and emit a probability of that image being real or fake so we wanted to emit a high probability for the images from our data distribution so the images of the sky that we took and we wanted to I'm at a low probability for the images that we generate with the generator so we train the discriminate to optimality and then the the learning comes in were the output of the discriminator from the generated images is used as feedback for the generator to tell it how well or badly it's generating images so you can imagine the generator being some kind of art counterfeiter who gets a bunch of numbers and uses his imagination to generate an image and the discriminant is a kind of art critic who tells how how realistic or how fake or not fake that image is and the feedback from the art critic is given to that artist to tell him how to change his strategy to generate more realistic images and the end goal or the result of that is actually that the generator learns to generate really realistic looking images and have some examples here so these faces here and I hope you can appreciate this to some extent even though it's quite abstract but these faces here were generated by taking a hundred random floating-point numbers and transforming them in some magical ways and you can see the images themselves are quite realist I mean there are some artifacts Oracle this this guy here has like this black blob can you see okay but the person on the lower left you can see some artifacts in the pieces but nevertheless I'm either really realistic looking images given nothing we're generated from random flowing phone numbers and what's also really interesting about these models is that because you have this space of this vector space where you can sample numbers from you can do really really interesting things with that space so for example here we can take the images on the very left and we can find the corresponding vector the corresponding code for those images and we can do the same thing for the images on their very rights and then you essentially have two points in space and then you can simply linearly interpolate between those two points in space and you can see that we can actually like interpolate between these faces on the bottom you can even see how the smile of the person becomes larger as we move from left to right so we have quite a lot of control about how our about what images we generate or these models so that's quite interesting now moving one level below that we have the layer layer for lack of a better name and the layer is actually very important because that's one of the most important levels of the stack at which researchers communicate with each other so in a deep neural network or any kind of neural network you can think of a layer similar to an optimization pass in a compiler so in a compiler you have different optimization passes like function inlining loop unrolling constant propagation that are each kind of stand-alone functions or transformations that take an input program produced an output program and similarly in a neural network you have layers which perform some kind of stand-alone transformation of some input same image and produce some kind of other image that is transformed in an important way and stacking together those layers leads to some output that we want for example for the discriminator you want to transform the image into a single probability value so let's zoom into the discriminator here and we can just disassemble it into the layers that make up this discriminative model and we can see a lot of convolutional layers that will go into your bits and also a flattened layer which is more of a utility layer that takes a 2d or 3d image and just flattens it out into a long vector and also a dense layer which is what you would usually expect from a neural network has like an input layer of lots of neurons and an output layer lots of neurons on those connected with weighted edges but let's look more at the convolutional layer and later on we'll actually see how to implement that so I just want to go about into the mechanics of convolutions convolution themselves are a very general term from signal processing that generally take two signals and combine them in a very special way but let's just look at the mechanics for now so we have this very exciting image with nine grayscale pixels and because they're grayscale we can look at them as floating point values between zero and one right and so for a convolution we need another thing which is a kernel and a kernel is just another small kind of patch with varying width well very important floating point values which are actually learned by the neural network so this kernel the kernel values are very important but the act of convolution itself is quite simple we simply plop this kernel onto the image and for each configuration of the image and the kernel we simply do a weighted sum of the values in the kernel and the values in the patch of the image and we multiply those and sum them up and get a single output value and then we slide the kernel over by one step or actually the amount by which we slide the kernel is called the stride so we might want to change that but again we just produce a weighted salmon we do that for every configuration of the image and the kernel and that's a convolution and I want to give you idea of our sorry first of all in 3d this looks very similar just that now we have a depth dimension for so for RGB images we'd have a red green and blue channel so it's a three dimensional value volume but still we'd have a kernel and we would just slide it across and produce a weighted sum and a single output pixel for each configuration of the image and the kernel now I want to give you a slight intuition about what this kernel actually do and why there why they're able to pick up very important information about images so on the left you have a kernel that would represent very simply a pattern so you can imagine if you have a graceful image and you slide this kernel across the image at any point where you have a vertical line with high pixel intensity the output of this way that Sam would be high so you would essentially you could say that the kernel activates at this point whereas in other parts of the image where you don't have such a vertical pattern you want to get would get a low value on the right is different kind of kernel I'm wondering if maybe anyone has an idea what that kernel on the right might pick up in an image I heard some things didn't sound really right it's actually an edge so it's quite interesting it's a very simple kernel but it picks up something quite interesting so imagine you have a surface with the same color because the pixel on the left is the negative of the one on the right those same color values will actually cancel out but there's one point in an image where these values would not be similar and that is at an edge of some surface right if you have a different color on the left and the color on the right this output value will be nonzero so this kind of kernel would pick up an edge and we'll look at actual code to do that later on before that let's move down to the grass layer the grass layer is very important because it's actually the most essential abstraction we have over a machine learning model from an engineering perspective so now we're really getting we've kind of left the domain of the researcher looking more and more into the engineering so if we take these last few layers here I'll show you what the grass abstraction means in a computation graph we essentially think of very primitive operations such as additions and multiplications and we connect different operations which are the nodes of the computation graph together with the inputs of that operation and then chain the output of each operation with further nodes further operations for example here even though the convolution operation itself consists of smaller operations like additions and multiplications because it's so important lots of libraries like Intel MPL and other libraries actually have them implemented very efficiently as basic routine so we can think of it also as a primitive operation and you can see here this convolution or comm 2d to come to thee because the 2d convolution takes as input an image also takes as input weight matrix WC which is this kernel that I mentioned earlier and also takes in a variable S which is the stride by which we slide the kernel and we would perform this operation and then the output of the convolution would go further into this other layer layer I mentioned which is the flatten operation the output that would go into a dense layer which is essentially just a matrix multiplication so again a matrix publication was actually a more complex operation which consists of lots of additions and multiplications but because it's so simple we also have it usually as a very basic operation and again it takes as input another matrix we multiply with which is another weight matrix and then at the end we have one final operate one final output which is the probability in our case and so this is a computation graph it's something very abstract and because it's abstract lots of it's the basis for lots of frameworks as well implementation of machine learning models and one thing we can do at this level of abstraction for example is actually solve the scheduling problem so imagine we have lots of devices or maybe even multiple machines in a cluster then at this level of abstraction we can think of how to place these operations onto different CPUs or GPUs on a single machine and maybe even on multiple machines so that's why this level of abstraction is very important at this point we can also talk about something different which is parallelism and so now we're really in more to the engineering there's two kinds of basic parallelism we can achieve with a machine learning model the first kind is called data parallelism and it's actually quite straightforward we do here is if we have multiple machines we simply copy paste our computation graph onto each machine and then we split our input data into multiple subsets and we simply evaluate the model for each subset on separate machines of course at the end then when we have our optimization process and we need to pass back the feedback from the evaluation to the models we need some kind of synchronization barrier so we usually have a separate dedicated parameter server in our cluster which would take the output of each model and do some kind of averaging and pass the feedback back to every single instance of the model on the various machines the other kind of parallelism is called model parallelism and in this case we actually really split the computation graph across different devices across different machines and this is different because in this case now usually we have for each layer every input is connected to every output of the next layer so in this case because the computation is graph is split we would actually have to do the synchronization on every layer of our neural network and this might seem like more of an overhead but there are actually various benefits to this kind of parallelism the first is that if your model is so large that it doesn't even fit on a single machine then model parallelism is the only way to even paralyze it at all the other way in which this kind of parallelism actually interesting is that in the previous case here after every evaluation we have to pass all the weights of our machine learning model to the parameter server and to give you some some rough numbers a popular machine learning model for image classification is called Alec's net and it has about 130 million trainable parameters so that's 130 million floating-point values on each machine for each of these model copies and of course transmitting that from each machine to the parameter server takes a large amount of time whereas in this case here on each particular layer we might only have say a million to 10 million parameters so the latency of each transmission will actually be lower even though we have more synchronization points so these are the kinds of consideration we would have to make if we want to paralyze our model there's another very important distinction between various computation graphs in various machine learning frameworks that you would encounter out in the wild and that is between static and dynamic computation graphs now in these static computation graph model you define your graph once and what this means for your programming is if you look at this fictional programming language here which is like have C++ have Python and a third of go then the first thing you should notice that the matrix the matrices have to have fixed static shapes so you need to define them ahead of time and the other really important part is that if you imagine this would be an interpreted language then if the interpreter runs through the line saying D equals s times a plus B this would not actually produce a value instead D would be a handle to the computation graph representing its value so that's why we could actually say that this kind of programming is declarative we're actually not computing anything we're just describing a graph similarly e would not be an actual matrix it would just be the computation graph that would lead to the value of that matrix and what this means is actually two things the first thing is that we usually cannot use the programming language does native control though so you usually have we are things like this if Clause operation because you can actually use thermal control all you have to add operations to the computation graph that does the control flow for you so in this case you would have this kind of if Clause operation which takes a condition computation graph and then two other computation graphs and then during the evaluation of the graph would return the one or the other result and the other thing that this means is that you usually have some kind of evaluate function which takes computation graph and then for example distributes it across lots of machines a lot a lot a lot across lots of devices and then actually computes the output value so this is the first time you actually get a real output value the other kind of graph our dynamic graphs and I if I wouldn't have mentioned the word graph you wouldn't even think of this the graph at all but in this case the graph is more or less defined by run this means that the matrices don't have to be fixed size anymore it also means that we can use the programming languages native control cell and it means that when the interpreter runs through if it's an interpreted language if the interpreter runs through D equals s times a plus B we get an actual output value and this also means that we don't have to do any kind of the evaluation of the computation graph at the end the reason why this is a graph is that usually these frameworks have to still trace the output the inputs to the outputs and mainly because at the end you have to compute some kind of feedback and then use the optimally optimizers output to update your graph so you still need some kind of graph structure just but now it's actually computed at each evaluation of the graph all right now moving on to the next layer the up layer I would say is the last level of abstraction before we move into actual code so the idea of an op is that you would have some kind of abstraction over the implementation of some operation for multiple devices so imagine you have a CPU and a GPU then for some kind of operation like the convolution you would have different implementations for CPU of them for a GPU and the app might be some kind of abstraction over that where the framework would then use whatever invitation is most efficient for your setup and here I want to talk more about the algorithms that we usually use for machine learning operations and this is very much about one thing which is getting to Bulba so this is Bob and Bob is the guy who works at Intel or Nvidia or AMD and for the past 20 years bob has been doing nothing else than optimizing 128-bit want.we matrix addition so bob is not a little fun a lot of fun at parties but he definitely knows all about matrix addition and I'm actually not joking I've been told there are guys who really for the past 20 years of in optimizing one addition but my point is that we have these very primitive and often used operations which have been hyper optimized for a very long time and so a basic strategy we have in machine learning is that very often we'll want to kind of reduce our operation to something for which we already have one of these efficient solutions so for example matrix multiplication and I'll give you an example of that going back to convolution there is a way to actually reduce the convolution problem down to a matrix multiplication and then just hand it over to Bob who has an efficient implementation in like Intel MKL or another library so the way we can do this is essentially just by for each configuration of the image and the kernel kind of take all the values of the image and just flatten them out into a long row into a long row vector like that and for the next configuration we can do the same thing we just slot it out and just put it into another row of the matrix one thing you should notice here is that between the first row and the other second row we will actually be sharing 2/3 of the values which is one consideration but it actually turns out that because matrix multiplication is so efficient if we have enough memory this is actually still a very efficient limitation but let me just continue so we would do this for every configuration of the model and the image on the kernel and that we have a matrix so that's the first part now we need and I'm another matrix and so what we can do now is also simply flatten out the kernel and put that into a column and if we have multiple kernels we can simply do that for all the kernels that we have and put them into the columns and now we can simply put a cross here and we have a matrix multiplication and this is actually very similar for lots and lots of operations where you want to simply reduce them to one of these basic linear algebra operations that we usually have very efficient solutions to right now moving on to the kernel layer here we're actually talking a lot more about implementations so in general what a chrome really is is an implementation of an operation for a specific device so if we have both CPUs and GPUs in our system then we'll have one kernel for the CPU and one kernel for GPU for the same operation and so here is one kind of function that we would want to colonel for this is called a sigmoid activation function and in general what an activation function is is going back to the mathematical model of neurons in the brain you have neurons which are connected to other neurons via synapses and synopses essentially have a kind of wait that's where the idea of waited that just comes from and then the the input neurons would effectively have some kind of signal and the weights of instead of C's would be multiplied with those signals and we have a weighted sum and that produces some kind of activation and the activation function in this mathematical model of a neuron would be modeling the rate at which in Iran itself would fire giving these inputs and firing means sending off its own signal and so that's what activation functions do we also have other purposes for example in our case of the discriminator this particular kernel this particular activation function has a nice property of mapping functions into a range of 0 and 1 which we can then interpret as probabilities that's what we also need but I just want once you actually use this as an example of a kernel and there's two things you need for the implementation of your kernel the first is a forward path which is actually just the definition of the kernel and for in this case the sigmoid usually denoted denote it as Sigma and the input is usually said and so Sigma of Z would be this is the definition of the function is simply 1 over 1 plus e to the minus set and then what we also need for a kernel is a backward pass a backward pass is the derivative and we need this for the optimization process so for the sigmoid in this case it's actually quite convenient the derivative is defined in terms of the function itself so it's the sigmoid at set times 1 minus of Sigma instead and in this case I can actually very quickly give you a look into this look small well a kernel would look like this is eigen again as a matrix manipulation library that's used by tensor flow and I just want to give you a very quick look at what a forward pass but kernel would look like we'll look at a lot more code later on so here is the scalar sigmoid up and this is what 10244 example uses for the sigmoid operation it's very simple it's 1 over 1 plus e to the minus X and if we want to look at the backward pass this is now in tensorflow itself just again to give you a taste usually for the optimization we need to define the chain rule which has which multiplies two derivatives derivatives so the output gradient would be one of the derivatives and here is the one we're calculating or it's our self which is Sigma I had said times 1 minus Sigma and said so this is a taste of the forward and backward pass of a kernel I want to talk about something very important at this point which is actually one of the most important aspects and maybe secrets of machine learning engineering all together and that is quantization so for this I want to first tell you that and well in various kinds of machine learning models one of our goals is to create robust architectures and that means for example if you're creating an image classifier for species of cats then for particular species you can't always expect to have perfect images right so on some images you'll have different lighting behaviors you'll have different objects in the foreground and background sometimes the cats will look friendly or they'll probably not the case sometimes they look like they hate you or something they look like you hate you a lot so generally you want your machine wing walls to be robust against no variations of the input robust against noise and now we can do a kind of thought experiment if you think of a 64-bit floating-point value with a 20 significant digits imagine we're modeling some kind of noise which is simply additive white noise so random value that's added to that floating-point value and imagine now that the noise just so happens to be the negative of the last 15 digits of that floating-point value then the result of that is the same floating point value but with only five significant digits so if our machine learning model is robust against noise this means that the output should still be the same and in turn that means that if that is the case we can simply always use reduced floating-point precision for all of our floating-point values in a machine learning model and so quantization actually happens to be or has turned out to be one of the most important factors of machine learning hardware and machine learning frameworks at all and so the way quantization works in some case is that we can even go from 64 bits down to 8 bits and what this means is that we can use eight times less memory and if you have 130 million floating-point values dividing that by eight is a huge thing and also means we can send more things across the network and it also means of course that doing math with 8-bit floating-point values is a lot faster than with 64-bit floating-point values and the white quantization could work for example is that if you have the sigmoid here we know that the sigmoid has an output range of 0 and 1 which means that we can add a kind of quantized up to our computation graph which will simply linearly quantize that value that 64-bit floating-point value into a single 8-bit character or chart which word for example map the value of 0.25 to 64 in values between 0 and 64 0 and 0.25 to some value in between and now we have a 8-bit care which we can send to some hardware accelerator accelerator which knows how to for example do matrix multiplication with these quantized floating-point values and those are really fast and for example if you're interested there's this library called gem low p and so gem stands for general matrix to matrix multiply which can actually operate on these low precision floating point values really efficiently right now I'm moving on to the lowest layer layer of our stack which is hardware I want to give a demo idea of the kind of hardware we use in machine learning there's three basic kinds or three kinds of processing architectures the first two are kind of obvious the first one is CPU and I want to kind of make a metaphor with fish so we all know that CPUs are kind of very strong processors well usually a few of them just as they're usually very few shorts I mean I've hopefully never encountered them but there's usually very few of them but they're very strong weak and they're very general they can do lots of things and in contrast GPUs would be more like a swarm or a school of piranhas where each individual processor itself is not very powerful but in their masses they're still extremely powerful and it has actually turned out that GPOs are one of the core factors of why I'm even up here talking about keep learning probably why you can know about keep learning so DPS have been the most important hardware innovation for machine learning in recent years and the reason why is that well what GPUs are really good at is doing lots and lots of small computations for each pixel of your screen and as you saw earlier for if neural networks we often have two blots and lots of small operations for varies many small units small neurons and that's why GPUs are actually just really really well-suited to neural network computation and I'll show some examples of GPUs later on and the last kind of processor are a6 which are weird so the thing about a 6 or application-specific independent integrated circuit sorry is that they're really really good at one thing usually and that thing is for machine learning often matrix multiplication so they really good at one thing they're not very general they can't do everything like a CPU or they even less general than a single GPU core but they're really really good at the one thing that they do and that's a six and I'll give an example of them in a bit all right now we're moving back up the stack and now I'm going to show more practical aspects of machine one alright so here's an example of a GPU and I wanna give some typical examples of machine learning devices so this is a Titan X Nvidia and this is the kind of GPU you would often find in machining labs so this is a we have four of these in my lab and they are reasonably cheap they cost between a thousand two thousand dollars and they can nevertheless speed up your computation by a large factor so to give you an idea training on a particular data set called imagenet which has around 1.5 million images on an intel xeon phi takes around 43 days and on this Titan X takes six days so that is quite reasonable just different and of course there's lots of new optimizations afterwards just a few weeks ago Facebook a research released a paper on training on the same data set and one hour on 256 GPUs so we're definitely making progress in this area nevertheless this is a good example of a GPU it has 3840 cores and because it's cuda cores each of those run 32 threads and it has 12 gigabytes for on-chip memory and i also want to many quickly clarify what GPUs have little off because I think that's maybe sometimes unclear what GPO is have a little of little of this first of all memory on chip memory so if you have a strong CPU node you'd have something between 64 and 128 gigs so strong GPU nodes would only have 12 gigs of ram and what you also have little of is bandwidth on the PCI Express between the CPU and the GPU but the one thing they do have very much of and which is something sometimes unclear is bandwidth on the trip itself so the Intel Xeon Phi which is Intel's kind of flagship parallel processor has around 200 gigs per second bandwidth so this GPU has more than twice twice that so they do have very high bandwidth and the other thing you have a lot of is flops so this Titan X can do 12 teraflops the insulin fire I believe can do around two writes another example here I want to give is of a typical machine learning server this one is called Big Basin it's designed and deployed by Facebook but it's actually part of the Open Compute Project which means it's open source it doesn't quite mean you can get to on the server and I will just appear in front of you but you can get cloned the plans and build it in your garage and it has 8 Nvidia Tesla P 100 GPUs which gives it about ten point six teraflops per GPU it also has 16 gigs of RAM and has n V links between the GPUs which are faster than PCI Express and they also have support for 16-bit float as I mentioned quantization and reduced 40-foot precision is one of the most important factors for machine loading hardware so that's a typical server now this is a typical or one of many Asics for machine learning it's called the TPU the tensor processing unit from Google and some things I'm going to say here are not quite right because Google just release TPO 2 but they also didn't release the specs for it yet so I can't be more right than I am and but the original TPO released last year was a coprocessor that's why I highlight it because it's not a general piece of compute like a GP or a CPU it's a coprocessor you attach it to another device and that also means that it only has 24 megabytes of on-chip memory or the one thing it's really good at is computation of matrix multiplications and you can see for these 8-bit quantized operations they can do 92 Terra ops which if you compare that to the equivalent Titan X is very little it's a lot more and also for so those are Tarot ops great ends and even for flops it can do a 42 flops that compared to twelve teraflops or teraflops so that's a typical ASIC and there's lots of other devices lots of startups coming up coming up with exciting new architectures to solve this entirely new way of computing so I mean neural networks are really extremely parallel GP with already parallel but we need even more parallel Hardware so there's as for example startup called graph poor from Bristol in the UK which has which is developing a new kind of architecture with a thousand cores and a very different kind of computing architects architecture Essaouira so we're no longer talking for Norman or Harvard we're talking a completely new way of computing all right now I'm going to show code in a bit first of all on the kernel layer there's two very important libraries that lots of frameworks like tensor flow or cafe or other frameworks use to implement their operations and those are ku tienen and Intel MKL so hoodie and NS from Nvidia and it's a very low level primitive library for well for in case of Kunene m for things like convolutions or activation functions or other kinds of neural network layers and in the case of cool glass for things like matrix multiplications and on the other side you have Intel MKL which is quite and also so who here is used and kael before all right so quite a few people who probably maybe have not done it for neural networks but just for general linear algebra or other purposes so these two libraries are used by frameworks like tensor flow or other frameworks to do their low-level grunt work and at this point I'm going to switch over and show you and the implementation of a convolution with cody annum alright big enough right so cuz I'm coding we can actually run enough forwards so this here for solos it's just code to load and save an image with open CVS I'm just gonna skip that the one thing I want to say is that both could enum and MPL these are c / c++ libraries and we all know that when it says C / C++ it's gonna be really bad and it's gonna involve a lot of pointers so it's a C / C++ library but it's not really a bad thing because these are supposed to be low-level libraries wrapped by other frameworks so they don't really need to have a very pretty user interface they just have to be portable essentially and also because it ca can be wrapped to other libraries or other languages like Python or Ruby or whatever so the way kudiye networks is that it has descriptors for the various of operands to its operations so here we have for example a tensor descriptor so first of all a tensor is simply a multi-dimensional array and so for example an image will be a 3d image would be a 3d tensor and if we stack multiple of those 3d tensors together we get a 40 tensor and so on and so pudina operates with those tensors and what we need for a tensor essentially is to describe the way it's laid out in memory so in this case for example we would say that it has how many rows at how many columns it has how many channels it has and also because it because could en usually operates on batches we can also say how many images we have in a single temperature and what we also specify here is the layout and that's actually one of the great things about qu d NN is because different frameworks like temperature flow or other frameworks will have different ways in which they like to lay out their temperatures and memory and ku TM has quite a lot of support for different kinds of layouts so for example in this case the N would be the images then we would have the height and the width and then finally would have the channels whereas other libraries might like to prefer this one down here which is first the images and then the channels and them the height and the width so switching that around so could you not have support for that in this case we would have fine a descriptor for the input and then another descriptor for the kernel which we convolve the image with so the input is the image and then we have another descriptor for the convolution algorithm that we use in this case we're specifying if these things were specifying the amount of zero padding we add around the image we specify how much what these brideís or the amount of which we slide the kernel across the image the convolution algorithm and also the data type of a convolution them a skipping down a bit here what's also interesting at this point is that we can specify different kind of convolution algorithms so usually want to specify this fastest but you might also want to specify one that uses lower memory or also you might have X was a choice for example you can pass career and convolution forward I'll go explicit gem and that would actually use the algorithm that I described earlier well you actually will explicitly model the convolution as a matrix multiplication the problem with that I also outlined that earlier is that you have lots of copies of data between individual patches of the image so the implicit version would be a bit more smart about that and how it does the actual operation nevertheless here the workspace is something that could en uses to know how much memory it needs for its operations and the actual kernel that we're using is this one here and this is essentially along the same lines as the CRO I showed earlier for the edges so on a single color patch this will still be 0 because I'll be the same color but on edges it'll be a nonzero value and you'll see that it's the actual convolution itself happens down here so you can see all the good stuff the actual Malick's that's we know its ceaseless c / c++ but here's the actual convolution operation because it's see the functions take a million parameters of course but the actual convolution happens here then we copy it to the output we're using cuda memory so this is cooler by the way this I put all my code open-source but this won't run on your CPU if you don't have NVIDIA GPU well then we save the image and now I'm going to switch over here pop up and I can log in to my lab cluster alright here is all the stuff ok I already made it earlier anyway we can apply it we can use this code to apply a convolution using cv become logo this looks like this so that's the image that work involving down there and now we can apply the convolution operation it's actually happening we can copy over the image and that should open up right and so I mentioned this was an edge detection kernel and as you can see using that particular kernel for the convolution we picked up exactly the edges of the image and so in a neural network you would hope that the optimization process would lead to the neural network actually learning this kernel on its own and got these edges and then do more interesting stuff with it so that was a convolution with could you name I have an equivalent version using mkl and my rep oh that you can check out yourself later on if you want to so that was that I think we how much time we have left more than all right so another sample and what you give is actually implementing your own kernel for tensorflow that's the next sample look at that all right so tensorflow I'll talk about more more about it in a bit but it's essentially one of the biggest deep learning libraries I'm going to show you how you would implement your own kernel for it so we're going to implement the sigmoid operation which I talked about earlier just that we're gonna call it CPP constic modes so it's far superior of course so essentially it boils down to very little code so that's the entire file you register an operation that you call that we call CPP con sigmoid and you specify the types that you want use it for a floater double so tensorflow has its own type system and you describe the input and the output the this thing here just checks at the output shape is equal to the input shape and then down here is the actual magic it's very little and as I mentioned earlier tensorflow uses I again so you'll be dealing a lot with IgAN which is a very nice library for tensor or a sort of matrix manipulation in general and down here we perform this operation that I mentioned earlier it's 1 over 1 plus e to the minus n and the nice thing about I again is that you can use tensors or matrices just as if they were single values so this means that if we do - enforcer inputted entire matrix that will do an element wise negation and doing that dot X will do an element element wise exponentiation and 1 plus that will do element was addition of 1 and then we do that inverse which computes the inverse of that and that is the final output value and now you can try that out and close this all right let's go up here Raman alright so I'm making it I'm just actually compiling to go through this file with make and now we can go into tensile flow or actually into Python so this is not Python this is T plus plus 49 and I just looks very much like Python we've made a lot of progress since then so first of all I just want to very quickly show you this demonstrate to you this idea of static graphs what that actually means so if I define a constant with tensorflow like this this anything is not actually a value it's just an operation that is a handle to that value so it represents the computation graft leading to that value and if I define B and then C as a plus B C is not a value it's just an operation representing this computation graph and then you usually have to do something like define a session that's what tensorflow calls it and then you can pass to the run method of that session your a tensor or your computation graph and it would map it to devices map it to the multiple machines you have multiple machines but ultimately produce the actual output value so that's what static Rams mean static graph means in our case we're more interested in actually loading our own kernel which we got we do this with TF that load up kernel so we compile our kernel into your shared library and then tensorflow is a nice mechanism for just loading that into its frameworks framework so we can load that and we can get it let's call it m dot I think it should be here that's so this is the kernel different thing we defined and now we can actually just run that man let me do s dot so this is the function that we have and we can pass it something like 0.5 and if this works then as you remember maybe earlier the sigmoid at 0 its equal to 0.5 so if it works it should hopefully be equal to 0.5 and now we need to create a new session to evaluate it okay it work so as you can see with just like around 50 lines of code we added a kernel to tensorflow the one thing that we aren't doing here which I mentioned earlier is defining the backward pass which creates C the derivative the reason why is that most of the derivatives intensive are actually defined in Python and that's I'll talk about that more later but that's actually one of the flaws or not flaws one of the problems with tension flow that other libraries solve in a better way okay so the one thing that I didn't want this talk to be exclusively is a comparison of frameworks nevertheless I do want to very briefly give you an idea of the different kinds of frameworks we have in the deep learning space the first one is tensorflow you've probably ever heard it I already heard about it and you'll see that most of these frameworks have some kind of corporate name attached to it whether you like it or not and so this is tensorflow was released by Google in 2015 so quite recently and it used as a static Roth model which you just saw so you're not actually computing anything yet you're just defining a graph in a declarative way and passing that on to your machines and then evaluating the graph and it has support for gpo's it can distribute your model automatically for you which is quite nice it does not have very stable C++ API and the reason why is that well a lot intensive flow is actually defined in Python and for example the relative derivatives so lots of the backward passes are defined in Python and that is actually something that the tensorflow team is currently trying to solve so they're kind of working backwards now because they define so much in Python but notice that people actually want to know for example Rob tensorflow in other languages or just programming from C++ themselves they're now kind of working backwards to move more into plain c or into c++ and so that they can have more api is based on top of that but it's actually so the one thing that's nice about tensorflow is that it's like batteries and lunch box included so you can simply define your model it will discrete it automatically for you there's also parts inside tend to flow to for example create a rest api for your model or help you with your training data so there's lots and lots of stuff inside the library the one thing that I I use template the one thing I don't like about it is that it's so low level so you really define these basic additions and multiplications and the solo level that's there's around 10 different competing higher level libraries based on top of tensorflow that all define the same like dense layer all the same convolution and so what can happen is I you you read ten different papers and look at the code for different for all of those papers and each of those implementations will use a different higher level every on top of tensorflow so that can be quite annoying the other tool I want to talk about our PI torch and cafe and I put them on the same slide because they're kind of complementary so they're both developed by Facebook and Facebook took a bit of a different approach they kind of actually split the responsibilities between research and deployments so PI tours on the Left uses dynamic graphs and I'll tell you that Diamond AMA graphs are actually a lot lot lot nicer to work with from a research perspective because you can actually use Python or C++ or whatever pythons native control fool you can actually use if clauses and actual while loops and that makes a huge difference and especially in terms of debug ability because your while loop is not in a graph somewhere on a distant device and so that makes a big difference at the same time it's still very fast so the one thing that's good about static graphs usually is that you can actually use things like compile them for example if you have the operation x times X then in a computation graph you can imagine this as having an input X and the output x squared and then two edges going from X to x squared and then you could run a compiler which would for example fuse those two edges into a single x squared operation which means you only have to transfer the data once to that operation and because it's defined that broth is to find ahead of time you can do things like optimizations and compilation but you can't do that with the Nama graphs but in this case PI torch is still very fast but it's in pure Python on the other side then you have coffee too which is based on coffee which first holiday actually has a surplus API that it's quite usable not very usable but it exists and the idea is usually that you would define you would do your research in PI torch and then export your code into a format understood by coffee too and to use a static graphs and I can do things like compilation or optimization and it's through your model across many machines but it's it's a lot less nice to work with but you have this split of responsibilities which is - the last frame we're going to talk about is MX net MX it's used to be a community driven project so not affiliate with any company but then Amazon thought it would need a deep learning framework to and it was a lot easier to take this one than to argue with Google about accepting pull requests so MX n is now devolved by Amazon which is actually great because it's a great framework and Amazon puts a lot of effort into making it even better and it's actually going into Apache right now so I'm very soon it will be apache MX now right now it's just MX net and the one thing that I like very much about MXN is that it's actually very modular so they for example they've abstracted the entire scheduling engine outside of the framework into a separate library that you can use for other kind of HPC scheduling problems for example so it's very modular which is nice and it also has the only really nice C++ API with which you could actually define a proper model yourself and I think we actually have time to show you how that would look like so let's look at that we're going to define a very simple machine learning model actually not simple I'm not going to very much detail with it I just want to show you that I mixed it has actually very usable research API in C++ so this is C++ as we all love it and we're defining a static graph with MX net using C++ so as you can see an MX 10 has these symbols a symbol is a node in the computation graph and we can simply use reasonably knife usually facing API for example do a convolution and the convolution would be a computation graph node itself and we're passing that through these functions to create a neural network in this case this is an image classifier but we can use it has a very nice API that's what I'm searching at the bottom here and here I can show you how to actually train it which is also very straightforward so it's not very much code here we're defining the nodes of a computation graph here I'm initializing them with value sampled from a normal distribution and the actual training itself happens down here we're just calling dot forward which does the forward passes on the criminals and our backward which is just the backward passes on the colonels and that's essentially it's it's not it's around two and fifty lines of code to create a full-fledged neural network using this pretty an IC plus plus API and now we can do something here let's go okay so can try this out I think I need to remake this okay I have some environment variables here this should hopefully be fast okay this is actually gonna keep trying live this is gonna take six days so I'm I'll see you back next week no it's actually it just takes a run a minute also I'm store until move on I think this is actually the last part so we can actually just start taking questions now and this should be finished in a minute or two and I'll show you a demo of using a real neural network in C++ alright so any questions [Applause] hi you showed performance with Nvidia's Pascal GPU Kaiba tried running with Volta architecture and did you get any performance improvements now I have not run it with full time and getting access to GPS a-gps at all is a nice thing and I don't have that modern hardware in our lab I imagine it's a lot more efficient using those more modern GPUs I mean NVIDIA has also a lot better hardware than the one I showed it was just more an example of a typical GPU you would find in a lab but I have not used the Volta and I've seen like impressive numbers so I want to know if it actually performs that well in you know all right yes so you said you can reduce your precision of the variables through 8-bit because it doesn't really matter first the new o-net should is supposed to be robust against variations yeah but there must be some trade-off because you can't reduce it to like one bit right so I like what is the trade-off that you you're doing at this point well I mean you're not paying the same accuracy for sure and I was also important to know is that the training itself would still happen with 64-bit floats but once the mole was trained once it were its robust against noise you can use reduce floating-point precision you can't scale to one bit actually there are no networks that use a single bit and they work quite well but it's the trade-off you're making is accuracy so with 8-bit floats you might not get 98 99 % you might get 97 % so you'll have reduced accuracy because you'll have to do this precision but it's the point is that you have so much more performance that it's actually good for those thank you yes all right so if everyone anyone still around this is now trained and I made a cute little cute app so anyone with half a mind would actually tell you that doing a live demo with a stochastic program is the worst idea ever I'll risk my neck to see if the classifier that I showed you actually works so this is a handwritten digit classifier and this works then maybe this would yes actually do something so the code I showed you with around 50 lines of code actually trained a very nice cute little neural network that predicts the digit that I'm drawing here reasonably well I believe at least 50% of the time it works every time all right yeah that's it for my talk so my question is you show a lot of neural network frameworks are there any other sort of general classifier frameworks for learning models like such as SVM or Bayesian network stuff like that you mean more like higher-level networks are you mean for more general machine learning I mean like in C++ for example are there other frameworks that you're aware of that can do these other kind of learning you mean higher level tests or do you mean other kind of machine learning except for neural networks the other machine learning yeah there's one called d-lab glab is quite old and it's quite mature and it's years been used for a long time for other things like SVM's or other kinds of machine learning models they have some pretty nice neural network api as well but it's very useful for a lot older and more and more mature stuff yeah thanks sure so what would you say is the main benefit of doing deep learning C++ compared to Python does it train any faster or predict any now there's actually the only reason why you would want to actually define your model in C++ is because you have a pure C++ code base the a lot if it's possible the much smarter way of doing it is to use Python which is just a lot faster to iterate with and then train your model and then export it and load it in C++ using for example the tensorflow API which has a very sweet API to just load a trade model and then you can integrate that into your code your C++ service and just use predictions from C++ so that's actually the better way but sometimes you just might prefer C++ for fun or sometimes you'd have a choice of using an external code thank you coming back to that resolution question is it possible to or like do a couple like the bulk of the training path this with with low resolution and then at the end just just train continue training with a higher resolution once it's like sort of good stop doing that in that precision you could probably do that people have generally found that doing the reduced position during training is not a good idea okay at the same time I think yeah I mean doing where this prison would probably reduce the training time but I imagine the idea is that if it takes two weeks to train then it's trained and you're done and then afterwards the performance really counts once you run your service but that might be an interesting idea all right you [Applause] 