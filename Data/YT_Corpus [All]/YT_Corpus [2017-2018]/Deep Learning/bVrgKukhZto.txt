 [Music] so welcome to yet another exciting topic on deep neural networks and this is where we are going to continue from our previous discussions on neural networks and that was about using a very easy simple neural network and then we use just one hundred new runs over there to train and then classify these microscopic images of WBC's into whether they are leukemia or they are not blooming now here I'm going to speak about something which is quite upcoming in the community and actually has got the community by your buzz and that's called as deep learning and neural networks are in fact one of the major runners in this whole factor whole whole family of deep learning engines in which we are going to work so but before I stand down I would come down to this basic definition of learning now when we say learning it's it's not by definition that there should be a set of features and then I should be able to classify something and do it this is one of the major misconceptions people have in their mind this can be a regression problem as we had seen done for decision trees and random forest ants but for a very simple stuff let's define what this learning is and the definition which Tom Mitchell puts down is from quite simple he says that a computer program is said to learn from experiences II with respect to certain class of tasks T and performance measure P now you would be saying that it is learning only if its performance at the task T as measured by P is going to improve with experience II so you need to know one thing that since we have already done that basic class for neural networks so one thing in mind is that this experience is the number of epochs and the number of times you are looking at the same data now as you were looking at the same data as the whole neural network was looking at the same data it was trying to look at the data and find out what is the error and then again back propagate this error this was the looking back phenomena upon it now as it was updating itself what you could see is that it was doing the same task which was just classifying the data but the measure of performance which is P which is that cost function over there was increasing as it was looking at more number of samples over a longer duration of time the same samples over multiple number of times or a larger number of samples so that's where he said that the neural network was learning because this definition was being satisfied over there now over here as per the definition and by its own basic concept it's noble said that you need to have features over there know where we are saying that you need to be able to classify now that's where we get into the actual problem now if we look into learning as its own paradigm I would take a very simple example from very scenic images and I'm not going to take down very specific medical images over there because you need to have a much wider understanding for them let's let's look at a very simple problem of scene image classification and on that let's say that this is one of the images which I am going to take now you you would often be seeing just if you haven't been to this particular place and what you would be seeing is just four people over there somehow if you are looking into much more detail you would see me fine standing on as one of the portions in that photograph as well and there is something written in Chinese and it's it's is some sort of a forest and amount and this is what you would be seeing over there now if a computer is given done with the simple task of can it interpret this particular image then how is it going to start now what it will be doing is something of this sort it's going to break this whole image into some small small regions okay now in the in the whole time it's not seeing just one image maybe it's seeing a million of those images and over time it's learning what to do so it is going to do something like it's going to break it down into multiple number of such small blocks and then identify what each of these blocks mean over there and then from there it's going to look into certain things which it knows over there as having seen something earlier and then finally it's able to look into the whole stuff and identify people and then some sort of create a sentence now if you look at this curve over here the fun which you see is that as you are increasing your experience which is the more the number of images you are looking through your performance is going to increase which is sort of say accuracy in identifying objects so the number of objects you can identify so over epochs or over the whole data set this is increasing and this is when we say it is learning now all of these names are basically not real so they are just indicated names in any way so I don't know whether they these were their real names or not but this was just to make it as a much more compelling problem and give it a much more personal flavour now let's look at what it was doing when it was trying to learn and that's about how was it learning so you have this whole image so the first thing with your computer program learns to do is divide it into number of segments which are called a salient segment so these are the segments which define a certain object present over there now on each of the salient segments it would try to objectify whether it can identify certain objects present in those silient segments over there now from that objectification it will try to detect what those objects are and say it is able to detect humans over there by seeing that there are human-like faces and then say on the other side of it it some of these segments which were not properly detected although they were objective and they were not detected they might be going down onto another pipeline where the purpose of this pipeline is to recognize these inanimate objects now from there it can recognize these humans if there is a large database over which it has looked at all the faces of those humans and from there it can actually go on to describe this whole scene by using this complete information now this is a way in which a computer program is going to do and and inherently if you look at - this whole paradigm in which it is solving you would be seeing that this is a hierarchical framework in which it is solving there is necessarily hierarchy so you need to first find out the salient segments and then you would be objectifying then some parts of these objectified things you would be detecting humans on some of them you would be recognizing in animation you come down eventually to a place where you are able to close the whole pipe and able to describe the scene now the question is we do like ugly on one part that this is a hierarchical process in which any any sort of a vision problem is being solved today and any kind of a computer vision problem any sort of a medical image analysis in respect of computer vision is also being solved today do you agree to this whole point over here now the question is is this whole thing unique or can there be a non uniqueness to this hierarchy as well which is what we need to answer now let's do a very simple experiment over there let me just take a clone of the same Network and try to reshuffle around certain of these blocks and then again reconnect all of them so once I have done all of this what you would see is that in both of them on the green pipeline as well as on the blue pipeline you actually have the same number of blocks the same blocks doing the same things but what I have done is the order in which they are going to do this is where I have brought in a difference okay now this is also a valid one this is also a valid one both blue and green are valid ways of solving the problem the question comes is which one do I choose this is something which is going to plague the community for quite long as to which pipeline so there can be person a who says that I will go by pipeline Green there can be person B who says I can go by pipeline of blue and you would keep on debating for say time immemorial as to who is correct and there might not possibly be a solution and that's how you we see in recent days that somebody or else has a different way of finding it out and nobody is very sure about whether their hierarchy is an absolute hierarchy in some way of doing it now in terms of an industrial problem this poses a very major challenge and that challenge is that since I am NOT able to define which is a very unique pipeline in the way of solving it out so that I will have to invest a lot many more manpower in order to optimize which of them is going to be my best product outcome over there so typically the cost factor associated with developing a software for medical image analysis in this case would keep on increasing by the number of such replicable models which we are going to create such identically similar performing pipelines so although the codes will be same which had been developed in each of them but then there will not be much of a good use going down across them there will be a team of people who will solve out on the green pipeline there will be a team of people solving on the blue pipeline and I don't know there can be infinite such number of pipelines actually none of us know so there this is going to escalate the cost if my final objective is to come up with the best software to solve this particular problem now this is one problem this is one challenge the question is is this the only challenge which is going to give me pressure now that will not happen because if you look into it say let's look into one of these blocks which is about detecting humans now one way of detecting human maybe I take a patch of that salient object on that I find out certain features so maybe local binary patterns wavelets and histogram of oriented gradients now I use all of them I plot them on to a body part recognition manifold which is going to recognize my body part so there is a head there are two arms and there are two legs now based on that it can project it down to some sort of a human appearance manifold and from there it will recognize that this is a valid human this is one way of detecting that there were humans present on the image they can also be another way of detecting it now and so certain different portion might say that what I could do is I would do a Croma clustering and a posterior line and that would lead to finding or some sort of a silhouette over that so portion in different poses how they would look like and then you can have a library of this silhouette images and you just match down with the silhouettes now once you have matched on with the silhouettes then you can actually project it this this matching distance onto some sort of a manifold and then be able to recognize humans now you see that even for detecting humans there can be the first block that can be the second block and both of them may be equally good and equally bad and people would still be just been keeping on arguing as to which one to use now this is a major problem and these are actually very very big problems which are plaguing the community today and that's where deep learning comes to play I would draw your attention on to a problem which we solved a few just two years ago and what this whole problem was that see you have some sort of a computational model which finds out so this is about computational imaging as we had discussed in the previous ones so where you would be looking into different natures of tissues present by just looking into the say ultrasonic signal or optical speckle signals over there now over here we would be having one model to extract out features which is again guided by certain good knowledge about the physics part of how signals get propagated then we will have another model just to map these features extracted onto these tissue types over there now the question comes is can I have one single model which can do all of this or in a very simple terms there is a lot of mathematics which goes into this whole place and for a lot of people how this would appear is as if within that mathematics there is some sort of a weird miracle which happens and this is solving my whole problem so if I don't want to do this and can can I get down on an actual network over there which will be solving the same problem without me having to bother ties to what none of what sort of mathematics is going on and in fact it's not so hard to imagine because we have a very simple solution to this one as well so what we propose down was something like this it's called as an autoencoder solution so this in the in the subsequent slide you will be having a view of what they mean down but if you look over here say I take down one small patch from this image and then I'm going to feed it through this small block which is called as a denoising auto-encoder whatever comes out from the output I again feed it into another block which is also denoising auto-encoder but this block takes an input from this block only it can this block cannot take an input from here this is what you need to remember and then you keep on passing this through over here and this thing is called as logistics figuration layer which is very similar to a sigmoid transfer function actually and you are able to find not a probability of the different classes of tissues over here I basically have four different classes of tissues which are marked up on this image which is an Oct image of the skin so this is my whole pipeline which I would be solving and how this whole pipeline and inherently assuming that this pipeline somehow learns by itself to solve all of these complex mathematics which we were thinking of as a miracle which is happening in the earlier case now if we look over here what we design is a network very similar to a neural network but it's with more number of hidden layers over there and that's the point from where comes the term called as a deep neural network so you increase the number of such hierarchical representation layers under the assumption that on each of these hierarchical mappings you would be mapping down one small extra feature extra amount of information onto your network and that will help you in solving a much complex problem onto a much simpler solution over there so if you look into this particular network over here it appears very similar to a multi-layer perceptron so you basically have one one layer of single perceptrons over there you stack on these layers and you keep on continuing and that's how you are going to get this whole network the challenge obviously exists in how you are going to train this network and that's where a lot of people will come in to with their expertise and we will also be solving a subsequent problem where I will show you how we can figure out a way of taking care of these problems as well but the challenge is it once I have somehow been able to train this network do do individual representations over here make any sense which is say I have this first neuron second or third neuron these are say there is a patch of 5 cross 5 so there will be 25 pixels over there and I'm just ordering each of these 25 pixels over there now all of these 25 pixels connected to say this first node over here so there will be a 5 cross 5 weight matrix which is also going to connect to this first part for the second node also I will be getting a Phi plus 5 weight matrix which is going to connect for the third node also I am going to get it now the point is are all of these weight matrix go to be the same or are they going to be different now if they are different how will the network make sure that they are different these are questions which we need to answer on top of that so this is from where I am going to map down to my first neuron similarly I am going to map down to all the hidden neurons over here now from these neurons I will again be having my weight matrices connected down to each of these neurons over here so these are going to be some sort of a dependent analysis of all the representations we are learning over here now that's where comes in the beauty of a deep neural network actually using a set of very small operations in order to solve a very complex function so let's look into what that meant so in terms of learning my representations what I would do is say I have this neural network which I have learned on over here now from that let's look into the first representation over there so what I have is basically a matrix of 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 so it's a 20 20 matrix which means that I basically have 400 nodes over here each of this patch is again made up of a set number of points over here this is basically 21 cross 21 which is the total number of nodes I have over here okay now each of them if you can if you would soom into them you would be looking that they somehow look like wavelets they have some sort of a definition and some of them are inverse form of it so this one looks like more of a Mexican hat kind of a wavelet this looks like an inverse of that particular wavelet and there are some ones which are perfectly like flat regions like this one or you can have a perfect white one which is a DC shift or a Unity multiplier over there as well so there are multiple of them and a lot of them actually look very similar to each other so they might be different they might be similar as well but what comes out at the end of here is that here whatever it learns each of them is basically some sort of a correlation kernel which you dis learning so your future extractors are getting learnt over here itself without me telling what kind of feature extractions to take place so your network is just doing classification by taking in an image but one image it's going to extract out different features so this feature extractions are what's happening over here now let's look into the second layer over here what happens over there now on this this average gray value is basically a commodity of zero that has an intensity of zero wherever it's a white small dot over there they are all bright spots which are corresponding to a very high positive value and the black dots over here they correspond to a very load very high negative value over them now what this does is this particular matrix will be mapping all of this so there are 20 across 20 or 400 such nodes which are mapped over here and here we have 1 2 3 4 5 6 7 8 9 10 so there is a 10% or 100 such nodes over here so each of them is a 400 is a 220 plus 2000 red patch element now each of them is giving a weight to each of such correlation corners which we had learned or some sort of features which we were going so this is a weighted combination of features which is getting learnt in this second layer so similarly if I keep on looking at the third layer fourth layer fifth layer they would just be looking into a hierarchical combination of how we can learn down these features one by one and this is where I meant to say that a deep neural network itself can synthesize the hierarchy from the data without you having to explicitly say what how to synthesize this hierarchy and at the same time it could also be doing a classification problem for you so the hierarchy get synthesized as well as how you are going to combine each of these features at different levels of hierarchy all of them are getting synthesized together now if we get back to our old problem on that scene classification where there were four people and everybody and then we had this parallelism that there can be two different possible hierarchical combinations in which we can solve the problem and the question which had in was in our mind is and if we have this two different combinations then which one do I choose I just cannot arbitrarily choose one of them there should be some basis of choosing in term now if I say that today we have techniques where you can just give a simple network over there and let the network decide the order in which it can choose a hierarchy won't that be making much more of an exciting proposition for us because we will not have to bother any more with feature engineering we will not have to bother any more with classification margins and stuff we can just feed in images on the input side of it and just get a classification over there under the fact that we can somehow code this whole neural network this can be computationally solvable and tractable the gradient should be existing there should be a head back propagation throughout it and we are able to solve it we would definitely be making lot more promising results onto it and this is one result particular result which I would like to show so this is about optical coherence tomography OCT image which we had studied about so this is an Oct image of a skin of a mice and this is about the skin of a mice which is healing so this there was certain wound in which the skin was peeled off and the mice is now healing and over here you would see that the epithelium is not yet really formed or nowhere here some part of the epithelium is formed and the full thickness of the skin is not yet restored because typically it's about this order of a thickness which is roughly about one millimeter thick roughly about one to one and a half millimeters thick but this is roughly 0.5 millimeter thick so it's still in a healing state now on this one this was the ground truth by looking into the histology which was knocked down by a histology and this is the sort of predictions which we are able to get down over there although they look very shabby but given a point that the whole network was actually trained with very healthy samples where it was a full thickness skin of one and a half to two millimeters on which these labels were obtained and each single layer was perfectly present so your speckle formation was also in a much more clearer way so here we have been able to reproduce a similar way over here so you can get a very clear and distinct epithelium you get the papillary dermis coming down perfectly the dermis and AHA Depot's up you're pretty perfect over here and here we never used a single sort of feature extraction over there what was fed to the network was just the small patches and what it was asked to predict was just this tissue levels over here and the network lost by itself what sort of features to extract and what hierarchy should it be following in order to get to this particular problem solution so this is the beauty of deep neural networks and with that I would conclude the first part of about deep neural networks in the subsequent lecture you would be studying about the history of deep neural networks and how they have been working the way they are working and a survey of certain examples where in the recent past we have had great successes in solving medical image analysis problems with deep neural networks and so following that I would also be entering into showing you the solution to using a deep neural networks in order to solve the same ll classification problem which we were doing by the classical way of first extracting features and then doing a classification so here we will start with the point that can we just put in the image itself as it is and solve the whole problem where it gives me a classification coming down to it so let's be excited for the next part of the lecture until then thank you 