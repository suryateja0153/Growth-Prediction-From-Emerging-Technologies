 [Music] so thank you for fitting the house I am really impressed that tensorflow is is getting so much attention but I think the technology deserves it so I'm happy about that so today yesterday we built a neural network for recognizing handwritten digits and we went to Daniel networks and convolutional neural networks and today I hope to build with you another kind of network the recurrent neural network so let's go a couple of reminders from yesterday you remember we started with this initial one layer neural network and I need to I need you to remember the formula for one layer of the neural network because we will be using it so this is we were reading in pixels but it works for any input vector of course and you remember we said that the neuron do weighted sums of all of their inputs they added bias and they feed that through some activation function your softmax it can be another function is just a function value in value out but usually in Murrell networks it's a nonlinear function and we wrote this this one layer neural network using a matrix multiply lavalla we sing all that as this formula and you remember we did this not for just one image but we actually wrote this formula processing 100 images at the time so in X we have a batch of images a whole batch 100 images and then x times W are all the weighted sum for our neurons we have the biases we see that through our activation function and we obtain a batch of predictions so in our case since we were classifying handwritten digits those predictions or 10 numbers which are the probabilities of these digits being 0 a 1 a 2 a 3 and so on and shall we obtain those probabilities of the outputs of our 10 neuron ok so whenever you see this formula we will see it again a lot today you think one layer of a neural network ok and then also what I need you to remember is that once we get our output from the neural network the way we train it is that we give it example it produces some prediction and then we say no no no that's not what we wanted this is what you should predict we give it the correct answer and to do that we have to encode this correct answer in a similar format so it's call it's very you know basic type of encoding it's called one hot encoding and basically here if we have 10 categories specifying one answer category means encoding it as 10 zeros with just one one someone in the middle and one index of the one here is at index 6 means that the correct answer was a 6 ok so in this shape it becomes possible to compute a distance between what the network predicts and what we know to be true and so in that distance we call that our error function or sometimes it's called the loss function that's what we use to guide our training so during training we give it an example produces an output we say no no that's not solving what we wanted compute the distance between what the network says and what we know to be true and from that distance we derive the gradient and then we follow the gradient and that modifies the weights and biases and that's what the training is about ok so now let's look at this neural network so it should look familiar it's as extra as an input it has a middle layer using the hyperbolic tangent as an activation function so we've seen the sigmoid last time which is the let's say that you know the simplest possible function going from zero to one continuously the hyperbolic tangent is the simplest possible function going from minus 1 to 1 continuously it's just the sigmoid shifted and then a second layer which is a soft max layer so that we read something else but the specificity is here that the output of this intermediate green layer is actually fed back in the next time step in the inputs so the real input into one cell of a recurrent neural network is the input concatenated to the output of the inner layer from the previous step and we call it the coldest of the state so it's actually a state machine you feed it inputs it produces outputs but it you also feed it a state it produces an output stage which you feed back in in the next time step and that's why it's called a recurrent neural network is because it it is a point on time sequences at each step in time you feed in one input vector concatenated to the previous state turn the crank once that produces some outputs from this middle layer as well as a result and you feed that back as the new input stage for the next X input which you have in your sequence so it can be represented here there as I'm showing you the neurons inside but here is basically the API of one recurrent neural network cell it has an input it has an output which you then usually feed into a softmax layer to make sense of it to produce predictions I mean probabilities and it has an input stage that produces an output say that you loop back in as the input state that's the state machine part okay so now oh yes and the parameter for this is the internal size of this middle layer that's what is adjustable usually your input is whatever your input is and your output is whatever you're trying to predict so those are not adjustable parameters also here it is written in equations again the the input is the real input at time t concatenated to the previous state then we see that through here you should recognize one layer of a neural network you should recognize this this formula using the hyperbolic tangent as an activation function so I put it over there and this produces an output HT which is both used as our new state and as the output that will be set into the softmax layer to actually produce a vector of probabilities between 0 and 1 ok so now how do you train this thing so typically this is used for natural language processing for instance so a typical input will be a character and a character will be again one hot encoded into let's say a 100 component vector if we are using we will be using here an alphabet of 100 possible characters so one character is encoded into a 100 element vector as 100 for 99 zeros and a one at the ASCII index of that character so we put a character in we propagate through the neural networks we propagate through the soft wax layer we obtain the correct as an output if that is not the character we wanted well we compute the difference between what he said and what we know to be true and use retro propagation to call to fix the weights and biases inside of the cell to get better results that's very classical training but what if the result was wrong not because the weights and biases inside of the cell were wrong but because the input the state input H minus one was wrong that input is a constant in this problem but it's not much you can do about it so here we're stuck what is the solution well the solution is to replicate this cell and now if you if you so this is a replica it's reusing the exact same weight okay now let's see the output y1 is bad I said I don't know no that's not it this was the correct output and training so I know what the correct output is supposed to be so from that I compute the error the gradient I retro propagate I can fix the weights and biases in the cells to get a better output and if needed I can fix the weights and biases to get a better h0 the state flowing between those two stages of the cell so now I have a handle on at least h0 I still have no handle at all h minus 1 if it is H minus 1 that was wrong there is nothing I can do so that is how you train neural recurrent neural networks you have to unroll them across a certain length and give them a sequence of let's say characters it will produce a sequence of output characters if you are training here you know what the answer was supposed to be so you use that to computer you know your error function do your retro propagation adjust the weights and biases and it will work to a certain extent what expect oh yes small detail if you want to go deep you can actually stack the cells why well two cells stacked like this the the API remains the same it's there is still an input it's still an output that feeds into the softmax layer and there is still an input state and an output stage that you feed back it's just that the output stage now is slightly bigger so that's how you go deep in regular neural network you stack those cells and that becomes a new cell which still has input output input stage output stage and of course you unroll it so let's take this sentence let's say now we use not characters but words as our input of course there are technical problems doing that a typical alphabet is maybe 100 characters a typical vocabulary is around 30,000 words so here 100 coding gives you a vector of 30,000 components for each word it's a bit heavy I won't go into the details of how you handle that it's called embedding whatever let's just assume that we we solve this problem so we have this sentence Michael was born in Paris blah blah blah blah blah and at the end we have his mother tongue is so if we train this model on English probably it will have figured out that his his mother song is is followed by the name of the language like English German Russian something here however the correct answer is French because this guy was born in France so let's imagine that we we see again we have unrolled this neural network over let's say 30 words or let's say ten words ten words and at the end we have his mother tongue is and we are asking the network to predict what is the next word and the network says English so now what we want to do is put on the outputs a centre that blah blah blah his mother tongue is French and Jew retro propagation but for this to work the beginning of the sentence the the part where the information about parish and where he is born is has to be part of that that example and that example is longer than n-word which is our unrolled size there is simply no way whatsoever of signal of putting that correct example was correct output into a network that we unrolled over only 10 words because the distance is more than 10 and that's a fundamental limitation if you want to capture this information this behavior that if he was born in France probably his mother languages is French you will have to unroll this network over a long enough sequence to be able to input this full example into it and if you do that you will probably unroll it here over how many 50 words and like that if you do that the problem is that you end up with a very deep neural network yesterday we see we see neural networks of five layers the big ones like Inception and so on or 40 50 60 70 layers you see you know we have three examples and we already see that we should be going to 50 or 100 layers just to solve this so in a recurrent neural networks you always end up using very deep neural networks and when I say deep is because you know the the state signal has to go through all those cells and remember in each cell the state signal is concatenated to the input to go through a neural network layer produces a new state which goes to the next cell that is concurrent to the input goes through another neural network layer so from here to the end which others at least one neural network layer per cell that's Hawaii deep neural networks have a technical problem they tend not to converge when you train them I won't go into the mathematical details it's called the vanishing gradient problem basically your gradient becomes zero and since you use your gradient to go forward that's a bit of a problem so the solution was invented I won't go into the mathematical explanations of why the solution works I just want you to understand how it works so would you learn explanation using the arrow soup of the diagram on the left or the incomprehensible equations on the right which ones do you prefer hello you know I'm a developer and those equations they look a little bit like codes and they're you know and I do code sorry but on the arrows you see at least one thing so I'll do some hands what you mean waving mathematics here again you see that is the state is actually split in two you have the H State in the sea State and the sea line there is actually configured in such a way that the network can decide to persist information on it to read it unchanged from iteration to iteration and that is somehow what explains that it initially line up many of those since it has the possibility of reading some part of the state unchanged it goes around those vanishing gradients problems end of hand waving mathematics so but still let's see how it works in practice and actually it's based on a concept of gauge so again we concatenate the inner real inputs to the state from the previous step and we compute three you recognize the formulas neural network later okay the Sigma is for the sigmoid activation function so the Sigma outputs values between 0 and 1 and we call those gates because we will actually be multiplying these numbers to another vector to get it you know if you multiply something by a very small value there is not much that goes through if you multiply something by something that is close to 1 almost all of the information goes to so that's how we will be using them now we are done our input becomes well we have to size adapt our input I put on the side the sizes of all the vectors we are working with that just to tell you that there is nothing to see there inside of the cell everything is of size n that's the parameter that you decide as the size of yourself okay but our input there they are not there what they are so we first need one neural network layer to adapt the size of our inputs to size n so that becomes our new input engine and now the C Line the way you read this is that this is a kind of memory so the new state of the memory is the old state of the memory without what we chose to forget we multiplied by this forget gate this is a series of numbers between 0 and 1 plus what we chose to remember from our new input that's the way to read it so these we multiply our new input by the update gate again numbers between 0 and 1 that shows which part of the information we want to retain from this input into our internal memory and then our new state is simply the memory the the hyperbolic tangent here that's not a neural network layer that's just a size adaptation to put it between minus 1 and 1 so it's basically the memory cell multiplied by the result gate so here we choose what part of our internal memory we want to expose to the outside as a result so that's the you know physical interpretation of these equations we have those three gates we size adapt our input and then the new memory is the old memory - what we want to forget plus what we want to remember from this from the input and the result is this memory cell modulo what we want to actually expose as an output at that step okay and now this HD will actually become part of the new state and also drive the softmax layer if we add a softmax layer which is represented here by this yellow circle we usually represent the softmax later as external to a cell so this is called an LS pm and this was invented specifically to make recurrent neural networks work and to solve this death problem that if you were unrolling over a large sequence they tended not to converge you will have to believe me on the mathematics with this they converge but you will have to say I'm sure someone noticed that this choice of equations and this choice of arrows was somehow arbitrary I mean why point them here and not there many combinations exist lots of different variations of those Orang n cells have been devised and someone published a paper a recap paper where he tested all of them and found them to do all exactly the same thing so in the end the one we use is called the group and I won't go into the details is basically a cheaper lsdm did here are the equations not very different same API but only two gates instead of three gates and each gate has weights and biases so we save a part of our computational cycle not computing those Siraj equation biases okay so we will use the group and now let's implement you know network that does a language model so we will be training on sequences of characters and when I say language model it's actually a network that we will use to predict will trainee to predict what the next character is like here st. job I will teach it to produce the same sequence shifted by one so actually I will teach it to understand that the next character should be an N because this is st. John so how do we do that in tensorflow now I'm using a higher level API of contra flow than what I have been using yesterday I just call three cells that creates a GRU cell and I call this higher level because you've seen this GRU cell has actually a couple of neural network layers inside it has two gates that are please two layers so it has a host of ways and biases which are actually defined in the background when I call this that's why it's a higher level API it does its own ways and by us the declarations in the background now I said we want to go deep so let's stack this cell three high that's how we do deep recurrent neural networks there is a temporal call for that it's called meet multi errand and cell you give it a cell you say how many times you want to stack it and that gives you another cell because we have seen already that these three stacked cells actually have the same API as oneself you can use it as a new cell and now we need to unroll this for that we call intensive for this dynamic RNN function which is a bit of magic and that's what will unroll this sequence so how many times you don't see it in the parameters because it's actually specified in the shape of the input tensor X okay if this input tensor has eight or let's say sir characters in it it will be unrolled over the sequence of 30 characters and actually the little part of magic with its magic that we will not be using it here but this dynamic Aaron what it can do also remember that we will be training this on batches as always always turn on batches so in this case all my batches will be sequences of the same size that's the case in my model in different in other models I might not have sequences of the same size dynamic error none can handle that if you pass it a batch of sequences even if they are not of the same size alongside that you pass the actual sizes and it will for each sentence in the batch under all your network the correct number of times and then also pass the output from the correct stage it's super helpful will not be using it here because all of our series you know sequences have the same size but that is super helpful all right so now we need to implement our softmax layer from those H double second 0 to H second 8 well basically the outputs at the bottom we know how to do is a flux layer ok but here since we have unrolled remember each stack here is a copy of the previous one we are sharing the waste so on the on the softmax side we have to share the weight as well so we could do this using the the 10 to the 80 is you know define one softmax layer and then for the next one : api that we treat the weight of the previous one area and we uses them and that's so complicated here actually there is a little hack that you can use remember we are always training on batches ok so this will be taking a batch of sequences out putting a batch of sequences each sequence is a sequence of characters so what is the difference between having let's say eight softmax cells that each process a batch of 100 characters or having just one that processes 800 of them that's the same thing let's just do one and we will put all of those outputs in the same bag and just use that one cell anyway we were supposed to be sharing the weight so defining just one seller is a very good way of doing that so that's what I do with my reshape operation there I take all of those outputs and and and you have to remember that there is a batch of output on each of those arrows and I put them in the same bag feed them through just one softmax layer and then I will reshape them back into the correct shape to chew on to finish again using the higher-level api's intent flow okay so when I call linear that just that does just the weight itself one layer and it computes simply the weighted sums no oxidation function and then I call softmax and that applies the softmax activation function and linear again defines the weights and biases in the background that's why I call it a higher-level function and now I'm ready to compute my loss function and derive it and actually train the network it's just as complicated to understand how recurrent neural networks work and it's just as complicated to actually feed them data correctly you see lots of adults so we will have to do quite a bit of plumbing to make this happen let's try to get our inputs and outputs right okay so we will be inputting sequences of characters by batches so my input is a catch of sequences the sequence length they have that I have chosen is searching I will be unrolling over 30 characters usually on the diagrams I only represent eight of them because 30 would not fit on my slide but in the code it was 30 now I need to one hut encode them so I'm adding a new size each character becomes a vector of 100 components because I am working with an alphabet of 100 possible characters so now it's batch size sequence lines and alpha size those are my actual input my state again I have a batch of state since I'm feeding in a batch of inputs I will produce a batch of output state and they state each of those state vectors is of course of size n cell size whatever cell size I have chosen to use remember each cell has this one configuration parameter which is it internal size but since I have stacked those cells three high it will actually the the actual output state here will be three times the sell-side okay we are ready to write this model so I define a placeholder for my input sequences a batch of sequences of size sequence length I want how to encode them which is why I'm adding a new size to dispenser which is the size of my alphabet again each character becomes a vector of 100 components to be really precise my alpha sorry is 98 so ninety-eight components I'm working with an alphabet of 98 characters here I need to define a place folder for my correct answers and actually the correct answers are very easy to obtain here I'm just teaching it to output the same sequence shifted by one so basically to predict what the last character will be so again the correct answers will be a batch of sequences of 30 characters which I want how to encode I need a placeholder also for my input state and we have seen that the batch of input space we have seen that the input state is made of three of those internal vectors so that three times cell size and now I'm ready to write my model so the model is what was here okay that's the model this model with this little trick that we have seen before this model at the output of its softmax layer actually produces an output that is batch size x sequence lines you remember we put all the characters from the batches and from the different stages of the unrolled sequence in the same bag and now to determine character from those probabilities I use Arg max why because each of those vectors is you know 100 components with probabilities or ik max is a function that gives me the index of the biggest number in this vector so in the index in this vector is actually asking code of the character that that has been predicted so these are my predictions now in ASCII encoding in characters and I just need to reshape them back to have again a batch of sequences of 30 predicted characters and now I'm ready to input to give my loss to an optimizer and asks tensorflow to optimize to actually train line network so this is the step as yesterday with this loss tensor so compute the gradient from this gradient it can sorry and this loss is of course the difference between the sequence of characters that was predicted and the sequence of characters that I wanted to predict this difference becomes a loss that loss derived becomes a gradient we take a small step along this gradient which is actually in the space of whites and biases so taking a small step means we modify slightly our weights and biases and continue that's the training one last little gotcha so we have to take our input text and actually cut it up in those sequences of 30 characters so initially I saw it well as easy you know you take a piece of text how do you cut it up in sequences of characters well you cut and cut and cut and cut and then you need a batch of them you take the first 100 sequences you have and you put that in a batch that did not work why let's see here that's my first batch let's see the first sequence in the batch the quick you know what that is going to be the quick brown fox something well when my noodle network processes the quick it also outputs an output state and in the next iteration that output state will become the input stage for the next sequence if I want this to be correct that input state must correspond to the continuation of the quick brown fox and so on which means that the sentence has to continue over all of the first slots of all of my batches it's a non completely trivial way of batching here you cut up your text in batches in sequences but the way to batch them together since you have to pass the correct state at each stage is that the beginning of the text has to be split across the first item in the batches and then from some point far far far later in the text you can start filling the second line of the batches it's just lumping I wrote for you the five lines of code that does this it's five lines I spent four hours doing it including tests yes I I don't do arithmetic it's it's a full of modules and divides and I wrote unit tests and a hackett until the unit test passed it's called test-driven debugging sorry test-driven development that's what developers do all right so yeah small dots on the batching but whatever just use the code on this is not actually important just use use the the function that will cut up the text correctly for you and you're ready to train and this is actually the full code of this neural network on one slide so let's go through this again a placeholder for my input sequences I want her to encode them I'm actually including inputting sequences of characters okay and and and all the people with cameras this is on github and the github legal is on the last slide so please take pictures my twitter handle is is over there tweet them but then you will you will be able to go and github and actually retrieve this then my expected output y underscore again I define a placeholder for them I will need to feed them during training and the first thing I do is add a high one hot encoder I will also need and this is different from normal neural network I will also need a placeholder for my input States remember RNN have an input input and input state to input now I'm ready to write my model so I chose the blue cell I stack it three high and I unroll it as many times as X has component in it so here my unroll size is sequence length and that's absurd it I chose 30 characters as the unroll size of my recurrent neural network I do my little trick with the softmax so I can implement one softmax node I see the output through my softmax node here I apply org max to retrieve from the softmax probably he's the highest probability and that the character I'm predicting I reshape this back to have a batch of predicted sequences also somewhere in the middle in there I have those probabilities I take those probabilities and I compute the distance between what it says and what I wanted that's my loss I give my loss to the optimizer I obtain a training step in this training step is actually that gradient which is computed on this batch of Australian characters and which if I follow it by a little step will modify my weapon biases and bring me to somewhere where this network works better when it has a smaller error function and now my training loop you will you will see this is very familiar to what we had previously we use this magic blundering function that I gave you to load sequences of characters in the correct way and once I have a sequence of characters I run session dot run of my training step I have to give in the input characters I have to give it the expected output and since this is a recurrent neural network I have to give it the input state this will give me an output state and you see the magic line why this is a regular neural network that's the last line there in the read input state becomes sorry the output state becomes the new interest rate that's why it's recurrent we are passing the state round all right so we are done with Delta regular neural networking now we want to actually train it so let's go to a demo I will be training this on the complete works of William Shakespeare that's not quite big data the complete works of William Shakespeare are five megabytes yes that puts things in perspective he but it's good stuff so in here we see he's it's training on sequences so here are those sequences of 30 characters and here is a batch of them it's actually training of those sequences here predicting not much at all it's just the beginning and from time to time I stopped the training and I take just my one cell remember I have just one cell it's replicated for the for the purpose of training but it's just one cell and this one cell I can it has become well once it will be trained it will have become a language model what I can do with it with it is generate a new Shakespeare play how do I do that well I take the cell I put in garbage a random character that gives me an output character for the probable probability of an output character which is the next character and an output state i feedback the output state in the input and i feedback the output character has a new input and i continue and this is a state machine that will start generating text you see here it's yeah that's not quite Shakespearean it's training and it's a bit slow on my machine I usually have a GPU connected here to - - it brings me the knife 10x Peter plus 6x roughly but still well it has done 50 more batches I will leave it running let's go and see so here on the slides what it is so at the beginning it gives you this as I said not quite Shakespeare but after only so an epic what we call an epic is one is when you have seen the entire training data at once so after having seen only a tenth of what Shakespeare produced in his life this is what we have well it's still not quite Shakespeare but you see there is some structure to it after 210 hey this looks better it's it's starting to actually spell English almost correctly and there are those things in in capital letters at the beginning that are starting to look like characters the character names even slightly less later all look and you have to remember that this is a neural network that is predicting character like character it first has to to learn to spell English before going to higher orders of structure so it's still not completely exact English but it's it's starting to look like in general like English at least Shakespeare in English and you see it has character names and it's actually inventing new character names here for Gia in Henry Brutus who can kill me no seriously who can tell me if Shakespeare actually used Henry Bluetooth in his work well you can't I am giving you the answer and he didn't but it's a very credible Shakespearean is character name and this is what you get after searching a box so it actually has another title there is a knack that is a scene after the scene it tells you where this is happening and look if now it knows how to put cynic indications in brackets who enters with whom and so on it has even picked up stuff like character names on all caps and when the character is a function like Lord or Chamberlain it's only the first character that is a capital it has picked up completely correctly as well and it's actually English so now that we have this let's try to this was on slides I will stop this what I have done previously is that I train this for actually searching a box and I saved my weights and biases so I'm ready to just replay it and generate a new Shakespeare play let's generate a new one live in front of you here it is let me stop it sorry about that if someone brave enough to come and play hallucinating Shakespeare on the stage with me come on yes thank you big applause thank you please come up you will have to speak loudly but that's how it is in a theatre you don't have a microphone you speak so you can read off the screen here we we will alternate so maybe I start and then you do the next one so let's say enter Bardolph and bolt the manner of with my best oars that you shall not see him and we are now to be the brother's wife and force to be so many and most great what are so again what needs thy life then what they do not dote on thee the word will be at thee and take my heart to thee and thy distemper will thou beat me well to say God save my son thank you so much thank you that was fantastic thank you actually I I try to do this also on the Python code of tensorflow itself that was fun so in the in the beginning you had this looks like Python maybe but very very quickly it actually picks up pythonic structures like you know those keywords and its generating something that looks like like function calls slightly later it actually correctly uses the keyword with function name or hallucination function name is actually quite invented in the function name and a column at the end it's still getting the the nested parentheses wrong and after a longer while it can recite the Apache License in full yes it's open source compliance open source compliant and more interestingly for us designers of recurrent neural networks it can actually close and open the nested parentheses right to a depth of three which is quite impressive and what I find fantastic it has is that it has figured out how to how to do Python comments and it's giving me tensorflow advice in those comments but look it makes sense check that we have both color tensors for being invalid to a vector of 1 indicating the total loss of the same shape as the shape of the template I'm sure this makes just as much sense as everything that I've been saying since the beginning here all right and small credit to a gentleman called Android karpati who actually wrote this neural network for the society published a blog about it he tried it on many different things he generated a business book for startups here and he tried to generate an algebra book in an in in latex actually after training this produced almost valid latex so he had to hack it a little bit to make it compile but then this looks like an algebra book there is all even an attempt at the diagram and the line I prefer is how the neural network solved how to write a proof look at the very tough proof or method that's so clever all right so that's basically all I wanted to show you well this is how we generated so we I take just one cell and basically in a loop I feed in a character I take the output feedback as the input and feed the output state as the input state and just do this in a loop a couple of applications of this oh yes actually we still have a little bit of time this time I've been using tensorflow tensor boards to visualize my inputs and outputs what is my tensor board somewhere sorry I'll find it here in the last session I was just throwing the output into into a matplotlib which is the very standard Python plotting library but there is a tool dedicated to visualizing training in tensor flow it's called a tensor board and I advise you to use it especially if you do a distributed training or training on remote servers it can connect to a bucket and and get the information from there and visualize it so here again I have configured when I was training this network I configured it to actually do training and validation I put one Shakespeare play aside for for validation to test my network and if you remember the session from yesterday I find it very important to follow my loss curves both the training and the test loss curve on the screen this is what I got and actually III so first of all who sees something wrong overfit yeah and so now the question is why is it overfitting here I will give you the answer because you can't guess it but here I was actually training on a small subset of the Shakespeare corpus so here it was over - over overfitting the of black holes data and since I have this on the curve I want you to show it to you because you certainly remember that somewhere what is it this one somewhere here I had this helpful engineering chart which allows you to you know to interpret what overfitting is and we went yesterday through the bad Network we went through too many neurons we never had a not enough data so I tried with not enough data and yes it also gives you this very recognizable pattern in the curve and as soon as I train with more data this is what I have so here the two curves follow each other closely and I know that I have solved a problem so actually I was doing this because I was trying to add a drop out into my network to make it work better no it was misbehaving just because of lack of data Drobo's would not have fault that all right and and and so a couple of applications practical applications to finish we've seen how to produce a character character by character model yeah we can also use this not character by character but word by word so as I said previously with a word it's a bit more complicated because to one hanko the word you need to encode it on the vector of this time 30,000 components because that's the typical size of a vocabulary in a typical language so those are a big big so on the inputs there is actually a very simple solution how do you reduce the size of the big vector well you use one layer of the neural network and produce less output it's called embeddings and that layer can either be part of your training then your embeddings are learned as part of the training or you can use some neural network that has been already trained typically trained on the English language generically and that just encodes words into smaller vectors there is a very famous what is it neural network that has been built for that is called work - back already trained available on github you can use that to encode your English words if your problem gives this English word has a small electricity and so once we have solved this problem of how to input words instead of characters you can for example use the regular new neural network like this to predict not what the next word is but a categorization of the sequence and this is used in newspapers to automatically categorize articles as the geopolitics science sports and so on what's very well how do you do translation well to do translation that's how Google Translate works you tack two of those recurrent networks and two ends on the first one you apply an English sentence plus a stop symbol and then you continue and you ask it to output the French sentence and what you have on the inputs there is a choice normally you should be inputting what your network outputs but people have also tried to input what the network should output so both options exist and they give you different results you can read about this in literature so this is how translation works of course you have a big problem at the end I won't go into that because to do the softmax layers there you actually want to produce a vector of 30,000 probabilities that's a bit heavy so there are ways of mitigating that but that's an active area of research one that is implemented in a in tensorflow is called sample softmax but there are many others because this is an active area of research how to do this softmax later to produce 30,000 probabilities each time which is a bit heavy and one more is image labeling so here is a very simplified version of image labeling image labeling is you take an image and you want to produce a sentence like this is a little girl holding a teddy bear this is a truck in the desert so that is actually also a translation problem you take vectors from an image and you apply a recurrent neural network to produce the sequence of words which you want to be the description description of this this image how do you encode an image as a vector well there are plenty of solutions one of them is to take an off-the-shelf image recognition neural network like Inception and just chop off the last couple of layers normally what Inception gives you is categories this is a truck this is a beach this is a lizard that's not what you want but all the top layers are actually encoding an image in some meaningful way into a vector you can use that as a fixed end coding function and input the vector corresponding to the image here produce this output sequence and sometimes it works really well this is what has both generated a herd of elephants walking across the dry grass field and so on and then sometimes yeah not quite thank you [Music] you 