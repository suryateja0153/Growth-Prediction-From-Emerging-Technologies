 [MUSIC PLAYING] - So good afternoon, everybody. I'm very glad to see here so many friendly and familiar faces. Thank you for coming here this hour. It's really a great honor for me to be here on the stage and I would like to think the Radcliffe Institute for giving me this opportunity for the amazing fellowship program which I'm part of. And to thank Dean Cohen and Professor Vichniac, Rebecca, Sharon, all the Radcliffe staff working tirelessly both in front and behind the scenes to make this all happen. Thank you very much. I guess I'm also the first one to talk about science in these series of Radcliffe public talks and I must say that I feel a little bit nervous and excited after the wonderful lecture of Professor Brian Greene that all of you attended maybe two weeks ago. And I think the bar is set so unachievably high that it's probably impossible to make. His ability to explain very complex notions in modern physics and string theory that probably requires the most complicated math that has ever been invented in very simple layman terms is really extraordinary. So I was actually, as Judy mentioned, two years ago, I was invited to speak at the World Economic Forum in all these events that gathers all the mighty people of the world. And I prepared a wonderful presentation about the latest and greatest stuff that we're doing in our research, but their reaction was oh, actually our audience, maybe in the best case, would read Scientific American. So they want to hear a nice story and see cool pictures. And so I hope I have prepared an interesting story for you tonight. Or actually, three stories-- one about the past, my personal account how I got interested and involved in this field and basically got into a career in research. The present, the current, state of the earth and the cool technology that exists today. And maybe a bit more ambitiously, and maybe a little bit also more arrogantly, the future. Or at least what I believe to be the future or at least potentially interesting and promising research directions on which I will be working during this year here at Radcliffe and of course, our intent to show plenty of pictures. And in fact, it's not the case that there is a saying that the picture is worth a thousand words. And if we look at the amount of the information that our brain receives from different senses, most of it-- about 90%-- comes from vision. And the site is arguably the most important of our senses, and we use it continuously in our everyday life. We start learning to recognize visual objects, basically, from age zero-- the moment we open our eyes way before we learn how to speak. And therefore, we take these amazing capability of our own brain of visual perception almost for granted without actually realizing how complex the process that is going behind the scenes in our brain that makes it work. My main field of research is computer vision-- a field where we try to give the machines the remarkable capability to see and understand and perceive the world around it. Anecdotally, the birth of computer vision as we know it is credited to a summer project at the MIT just next door here in Cambridge in the 1960s. And there, a group of researchers-- three people, unfortunately, two of them are not with us anymore-- they got a television camera that could be connected to a computer. And if you think of the technology in the '60s, that was super high technology. They assigned graduate students with the task of making the computer describe what it sees basically, parse the visual objects in the scene. And though I'm told that MIT students are very good, probably among the best in the world, even they failed to complete this project. So it didn't work well at the end. Probably otherwise, I wouldn't be standing here to speak about computer vision. And even 50 years since, the problem is, I would say, quite far from being solved. So speaking seriously, one of the first applications that computer vision researchers considered was to recognize human faces. And for us, it's very important, of course, for social interactions when we talk to people we know from the faces to whom we are talking. And you can also think of other applications like security applications when the computer needs to know who is the person in front of it. The human face appears to be very structured object. We all have a nose, a mouth, pair of eyes. And what makes us different from each other is how these different facial parts look like and how they are related. What are the spatial relations between them. So the first works on facial recognition tired to automatically detect these features of the face and compute their geometric relations, in this way, creating a kind of geometric descriptor that would able to identify the individual. And at the first glance, it may sound like an easy task. But the problem is that we are looking at an image. Basically, a two dimensional image that comes from a projection of three dimensional world by means of a camera that mixes together a lot of different information-- the initial color of the object, its reflected properties, its geometry, its position in space, the way it is illuminated. Basically, we get a single image that has all this information mixed together and extracting different components is a very challenging task. So if you look at these pictures of a human face, it is even hard to tell that that's the same person so differently it is illuminated. And for a computer, it would be very hard to distinguish between what constitutes a facial feature and what is a kind of false feature-- what comes from external factors like light and shadow. And that's why many face recognition, of the past at least, that worked very well in the lab in very controlled conditions failed miserably when taken into outdoor environment. Lighting is just one of different factors that can affect the appearance of the face. You can also think of, for example, head orientation or maybe even facial expressions. So talking about illumination, probably the most famous example of the jokes-- so to say that light can play-- is this image taken by the NASA Viking mission on Mars in the '70s. This bizarre face-looking structure baptized by the journalist as the Martian Spinx, or face on Mars, gave rise to numerous conspiracy theories and ideas about intelligent extraterrestrial life. But subsequent photos show that it's just a rock formation that appeared in this weird form because of particularly wacky illumination. So if you were impatient to get signs of intelligent life on Mars, at least not that time. It didn't happen. Let me show you another example. You can look at this drawing and tell me if it frightens you. Does it frighten you? You certainly recall the little Antoine de Saint-Exupery posing this question, right? And people replied, why should anyone be frightened by a hat? It's a hat. However, of course, what he meant was not a hat but a huge snake. A boa constrictor digesting an elephant that would look like this. Again, this is another example of how ambiguous a two dimensional projection of three dimensional objects could be. So since we humans we don't have the ability to explicitly sense depth, we just get a rough idea of it through our binocular vision or other implicit cues such as shading and shadows. We attribute way more importance to the appearance than to the 3D geometry. Let's do the following experiment. That's an image that appeared in a book that we wrote about 10 years ago. This is a 3D scan of a human face. You can see the three dimensional surface. Now imagine that we had major make up that would allow us to draw anything on this face. We can do it, of course, with computer graphic tools using texture-mapping techniques. So if you were able to paint something, let's paint the face of another person. Let's paint the face of George W. Bush who was the President of the United States at that time. Or if you don't like for some reason George W. Bush, let's paint Osama bin Laden, who was that number one wanted terrorist at some point. And it's exactly the same three dimensional surface just painted differently. And we perceive it as different people. Whereas, if we could explicitly sense the three dimensional structure, if you will could analyze the geometry, we'll be able to tell immediately that that's not Bush or bin Laden. It's just an imposter that painted himself in Bush or bin Laden's style. So this 3D structure is not affected by makeup, illumination or head orientation. And it contains a lot of useful information, potentially, that could make face recognition better. So we looked into ways of using 3D geometry to make face recognition more reliable. And this was 15 years ago. I was still an undergraduate student at [INAUDIBLE] as Judy mentioned in her introduction. And this was one of my first two years research projects during my undergraduate studies that I would say now, to a large extent, has shaped my research interest and also the future academic career. And maybe on a more personal note, also has created a wonderful bond and friendship with my former PhD advisor, Professor [INAUDIBLE] from the [INAUDIBLE]. Long friend and collaborator, not only in size, but also in commercial endeavors. So in particular, we're interested in facial expressions because our face is non rigid. It can deform and the deformations of the face make the life of facial recognition systems that are based on these measurements of distances between facial features very hard. Talking about distances on the face, there are several ways of measuring distances. We can just take two points and connect them with a straight line. This is what we call the Euclidean distance. And as I said, such distances can change significantly when the face undergoes deformations due to facial expressions. Another option is to look at the lengths of the shortest curve connecting two points. In technical jargon, this is called the geodesic distances. And we found out that such distances undergo much less variation, or approximately invariant, under facial expressions. And now we can measure geodesic distances on surfaces. There are efficient numerical algorithms to do it. But we didn't want to use any of landmarks such as eyes or nose tip. First, because it's hard to reliably detect them. And second, because there are very few stable facial points that you can detect on the face, while many other parts of the facial surface-- like the forehead for example-- it's impossible to detect anything meaningful there. So basically, we end up with a collection of points, geodesic distances measured between them. And now we need to compare them because we want then to tell whether two people are similar or not. So it would be nice if we would have some way to represent these distances in some space that is convenient to work with. For example, if we could use a Euclidean space or, even better, two dimensional Euclidean space-- the plane. And these kind of problems arise in cartography. Assume that you are given a selection of cities with distances between them. And, of course, here we're talking about geodesic distance because when we travel on earth, we don't dig a tunnel under the ground. We actually fly over the spherical surface, so it cannot be a straight line. It must be a geodesic distance. For example, here the geodesic distance between San Francisco and New York would be 2,500 miles. So we would like to create a plane or map of this vertical surface of the Earth, such that the Euclidean distances between these cities are basically represented by Euclidean distances on the map that you can measure with a ruler. And these new distances are equal to the original geodesic ones. And this is called an isometric embedding where the term "isometric" means distance preserving in Greek. So how should we do such a map? If you ask a mathematician, you know that mathematicians are especially annoying type of people. And when you ask them how to do something, they will not tell you the answer, they will ask you whether it's at all possible. So is it possible to create such a map? And is it indeed possible to find an isometric embedding of a curved surface-- for example, they've given the sphere-- into the plate? And that's out of this question which has to do with another apparently unrelated question. Is the circle perimeter always equal to 2 pi r? So it sounds like a silly question. This is a formula we all learned in school. And indeed, it has been an undisputed fact in geometry since the ancient Greeks. And they took about 2,000 years until Reimann and Lobachevsky formulated what we call today non-Euclidean geometry. It's where circles can be longer or shorter than flat ones or, maybe more famously, whether parallel lines can intersect or not. So the degree to which the perimeter of a circle on a surface is different from the school formula 2pir is called curvature. And I'm not as good as Brian Greene in visualizing 11 dimensional Calabi-Yau manifolds. Fortunately, we're talking here about two dimensional manifolds and I brought some paper models. So I will show you what curvature means on paper. So this is a flat circle, right? So basically, here you can see it has radius, r, and the perimeter is equal exactly to 2 pi r. Now if I wanted to create positively curved surface, by this definition, I need to create what is called the perimeter defect. I need to make the perimeter shorter. And by making the perimeter shorter, I get curvature. Luckily, around this point, the surface looks like a sphere. So it's positively curved. If you want to make a negatively curved surface, you need to actually increase the perimeter and you get a surface that looks like a [INAUDIBLE].. In geometric terms, this is called a hyperbolic surface. So this is curvature. It's really a fundamental concept in geometry that has many definitions. For example, you can also consider the definition of the curvature by just taking a piece of paper, bending it, and looking at different directions of curves that pass for a point. And the smallest and the largest curvature, what are called the principal curvature, as their product defines exactly the same notion that we've seen before. This notion of curvature dates back to Karl Friedrich Gauss, probably the most prominent mathematician that has ever lived. And when he wrote down the expression for the curvature that bears his name today-- the Gaussian curvature-- he was surprised to find out that it can be expressed entirely in terms of the intrinsic structure of the surface, what we call the metric. Meaning that if we deform the surface without stretching or tearing it, such deformation will not change the curvature of the surface. And again, piece of paper is a good model. You can bend it, but you cannot stretch it. It is not elastic. Gauss was so surprised with his observation that he named the result-- and he was writing in Latin-- theorema egregium or the remarkable theorem. And Gauss was not only a great mathematician, but also a very modest man. And if he thought of something as remarkable, it must really be. And of course, you may think, this is just some weird geometric idea. Why on earth should you care? Well, I believe that you should. It appears that curvature plays a fundamental role in the modern description of our universe Einstein's general relativity theory models gravitation by curvature of the space-time. It is slightly more complicated because the spaces there are higher dimensional, but the basic idea is the same. And again, here I cannot refrain from referring to the lecture of Professor Greene where he masterfully showed the meaning of these concepts. But not only on cosmic scales do you encounter the notion of curvature and manifestation of the remarkable theorem of Gauss, but in much more prosaic situations when, for example, you eat pizza. So again, I have here a paper model. And I beg the forgiveness of my Italian wife for calling this piece of paper a pizza. Think of it as playing with zero curvature. So when you grab a piece of pizza, you instinctively tend to bend it like, right? By bending it like this, you create here a direction in which it is curved. Now remember that the Gaussian curvature is defined as product of principle curvature. So it has a curvature here that is non-zero, so you must create a reach, a direction in which the curvature would be zero-- basically, a straight line. And this gives rigidity to this piece of pizza that prevents the topping from falling and spotting your shirt or pants. And of course, this is just a mathematical abstraction, so in practice, real pizza might have some elasticity. So if disaster happens sometime, don't blame it on Gauss. And of course, the remarkable theorem had an impact in cartography which brings us back to our original question. So now you know the answer that the sphere has constant positive curvature and the plane has zero curvature. So they cannot be isometric and, even worse, by different arguments, usually such isometric embedding v will not be possible even to Euclidean spaces of higher dimension. So what we can still do is to find representation that distorts the distances the least. So it will not be a true isometric, it will be approximate isometric. In this way, we can get the Euclidean representation of the face that allows to undo the facial expressions. And this way, making the comparison of two facial surfaces much easier. During my masters and PhD studies, I worked on ways to efficiently compute such minimal distortion embeddings and their applications to problems in computer vision and computer graphics, in particular analysis of deformable three-dimensional objects. And we also built a face recognition system. It was one of the first 3D face recognition systems that also allowed to cope with facial expressions, which is one of the difficult problems in this domain, that was based exactly on these geometric ideas. And here we can see a screenshot from these systems. again, that was many years ago. That shows the recognition process-- the creation of these four-dimensional Euclidean representation. And this, as Judy mentioned, was joint work with my brother who is also a computer scientist. He works in Israel and he works exactly in the same field. We've published a lot of papers together. And it's been a successful collaboration already 37 years. And again, as Judy mentioned, what makes the story especially interesting and curious that my brother and I are identical twins. And this is a composite portrait of us from that period. So you can see my picture on the left in this portrait. No actually, I'm on the right. Sorry. This is yet another proof that we looked similar and we are still similar, but of course there are some differences in geometric structure of our faces. The 3D geometry of our face is not totally under genetic control. So our system was apparently able to detect these differences and tell us apart. And the media picked up this story of face recognition system able to distinguish between identical twins. This way I got my 15 minutes of glory. Our research was featured on CNN with quite an improbable title "Twins Rock Face Recognition Puzzle." And of course, we tried our system on other pairs of twins-- identical twins- also quite successfully. So this looked like a cool technology and we tried to commercialize it. That was a period of booms of start-ups in Israel. In all these startup nations, everybody was trying to make money out of technology. So we thought why not to try, but quickly found out, to our great disappointment, that there was really no market for it, or at least there was no market for it 15 years ago. And one of the reasons is being three-dimensional face recognition system requires a three-dimensional input-- a three-dimensional scanner. And 15 years ago, if you wanted to buy three-dimensional scanner, you would just have a handful of devices that you could really count on the fingers of one hand. And most of them would look like this. It's a big box of the size weighing probably several kilograms and it costs like a new car. So definitely a showstopper for any serious commercial application. And in fact, the 3D sensor at the time were so prohibitively expensive that for our 3D facing commission system, we had to develop our own 3D sensor and you can see it here in this picture. It was a structured light sensor. We used a digital slide projector to illuminate the face with a sequence of patterns that-- captured by digital camera-- allowed to infer the 3D structure of the face. I remind you this was 15 years ago in 2002. The year when the movie "Minority Report" hit the screens and for those of you who have not seen it-- just one second spoiler-- it's a dystopian vision of how our world might look like in 2050. And one of the most famous scenes in this movie is where Tom Cruise uses his hand gestures to manipulate virtual objects on a huge holographic screen-- a vision of how we might be interacting with our future intelligence machines in 50 years from now. And in fact, the idea of gesture-based interfaces has been around for a while. An interface like this that would be based only on vision requires the ability to recognize and track different parts of the human hand, which is a difficult problem because our hand is a 3D object and because of two dimensional projection, there are occlusions and ambiguities so it's not easy. Apparently, the creators of "Minority Report" were so skeptical about the ability of computer vision researchers to solve this hand tracking problem 50 years from now in the very distant future that they equipped Tom Cruise with these light gloves that you can see in this movie scene where-- which, of course, makes the life of hand tracking much easier than tracking simply bare hands. So at least this part of "Minority Report" remains science fiction for a very brief moment of time. I hope the rest remains science fiction forever. In the matter of a few years, Microsoft released the highly successful Kinect product. It was a plug-in for their Xbox gaming consoles that allowed users to control their games with bare hands and gestures. No gloves were required. So it was even better than envisioned in "Minority Report." And this ability was made possible by a novel 3D sensor that was developed by an Israeli company called PrimeSense that projected invisible infrared light pattern on the objects in front of it, and by simple triangulation techniques recovered the geometric structure that was accurate to a few millimeters. And this gave a lot more useful information than standard two dimensional image because in 3D, the hand tracking becomes much easier to solve many of the ambiguities. Microsoft, using this 3D information, we're able to use simple computer vision and machine learning techniques to do efficient hand tracking. So Kinect was by all means a revolutionary product, the first of its kind. And at that point, we again returned to our old ideas of trying to make the 3D face recognition into a commercial product, but it became very quickly apparent that the sensor itself is much more interesting than the particular application of 3D face recognition. The investors were betting big time on the future of 3D sensors. Kinect, however revolutionary it was, was still large and clunky. It required an external power supply so you cannot simply put such a device into a mobile laptop, tablet, or even cellular phone. And we thought that we could make a much more cheaper device. So it became a startup called Invision based in Israel. And in 2012, we were acquired by Intel. And somehow surprisingly, even though it was pretty dramatic moment in life for me and for my colleagues, I don't have any pictures from that moment. Just this picture where the logo of Invision on the office door is replaced by the logo for Intel. And of course, it took another several years and the huge effort of many teams inside Intel before it became a commercial product. Actually, the first 3D camera that could be mass manufactured into the form factor that would allow to fit it into mobile devices-- our technology got the name Real Sense and it was presented by the Intel CEO at the Consumer Electronics Show in Las Vegas in the beginning of 2015. So I have here one of the first samples of the camera from a few years back. So you can see that it's a tiny, thin device that easily goes into [INAUDIBLE] left of display. And for the launch of the product, Intel shot this funny commercial that was aired on Christmas, 2014. For those of you who are fans of the "Big Bang Theory," you can recognize the character. - What's that? Wow! Hello, humans. Focus on me. - Focus on the guard. Don't tell anyone what you've seen in here. - Have you seen what's in there? - They have Intel. This is where it all changes. - So first of all, the Intel lab doesn't look like this obviously. And here it says coming soon. Of course now it is a successful product with several generations and multiple millions of units shipped worldwide. And so if Kinect made 3D sensing affordable, RealSense allowed to make it ubiquitous, and today, you can have a 3D sensor in your laptop or tablet. And this opens the door to many interesting and important applications. Let me show just one example of an application developed by a Swiss company called FaceShift. - He name's Mandy. The profile says 6'4", good with an axe, and likes the music of Lana Del Rey. Sultry, sexy, and when necessary, extremely violent. - So what you see here is a computer graphics character that is animated in real time by a human actor that performs in front of a 3D sensor. And this kind of technology is called motion capture. Nowadays, it's widely-used by Hollywood studios to produce special effects in movies like "Avatar." But a typical motion capture setup as big as this room would cost millions of dollars and requires to places special markers on the face and other body parts of the actor, while FaceShift did it without any markers. They marketed it as markerless motion capture and could do practically the same with just a small 3D sensor that costs maybe $20 or $30. So it was really remarkable. FaceShift was acquired by Apple two years ago. And if you've seen the presentation of the new iPhone 10 a few weeks ago, Apple has really taken it to completely next level. There are emoji-animated emoticons are, presumably at least, powered by the FaceShift technology. And the piece of hardware that enables this capability is a tiny 3D sensor that is built into the phone, what Apple calls the TrueDepth camera. And again, Apple also acquired PrimeSense that provided that technology for the first version of Microsoft Kinect and again, presumably-- nobody really knows except for people that work at Apple that build these products what exactly they do-- but presumably, that's a very miniaturized version of the same technology that powered Kinect. And you could also notice that the new iPhone 10 abandoned the fingerprint scanner and instead they do face recognition in order to unlock the phone. So interestingly, and maybe a little bit ironically, brings us back to the good old face recognition problem from which we started. And now a lot of things have changed. Until the past five years, the predominant way face recognition-- but in general, I would say most of the problems in computer vision-- were approached was by designing so-called feature descriptors-- a smart and carefully engineered way of capturing the local content of an image. For example, computing histograms of local derivatives or image gradients. And some of these descriptors had become extremely popular, for example the paper describing the famous [INAUDIBLE] descriptor is arguably one of the most cited papers, not only in computer vision, but in the field of computer science with over 40,000 citations. I checked Google Scholar just a few days ago. So the problem though with such hand-crafted features is that it's very hard to decide a priori what constitutes meaningful information and what is the result of noise or external factors such as different illumination. A paradigm shift that happened recently in computer vision is instead of trying to engineer a local descriptor basically hand-crafted by determining what makes a good feature, let's assume that we have a template that we can pass over the image and detect if it correlates well with image content depending on the application for example, if we're looking for-- human eyes or etc. And we can use many such templates, basically a bank of filters, which are combined together through some nonlinear function. And we can repeat this process several times applying filters to the output of the previous layer again and again. And the key here is that you can use these filters more or less the way you want. Basically, there is no prior assumption about how they look like. The only thing that you care is to select these filters in a way that will make your final goal, your task work the best. In this case, for example, tell apart different people. And it turns out that in applications like this in image analysis and face recognition, the filters that will be learned in the first layers of the system will be very simple visual features, oriented edges, corners. Features in the second layer will capture more complicated structures such as eyes or noses, and in the very deep layers you will find units that may be activated by just that specific face of that specific person. You know that among neuroscientists there is a joke about the grandmother neuron-- a hypothetical, imaginary brain cell that is activated when you see just the face of your grandmother. And the reference to neurons in brain here is not the case because such systems are called artificial neural networks. And this particular architecture that I described is a convolutional neural network and they originated as simplified mathematical descriptions of the human brain. And the most important advantage of artificial neural networks is that they can learn from data. And roughly speaking, if we cannot build an axiomatic model of what makes a face look like a face, we can get many examples of faces and, in particular, faces of similar people versus different people, and we can train a neural network to tell them apart. in fact, artificial neural networks are not new at all. The first works in this domain date back to the '50s and the '60s of the past century, mainly by neuroscientists attempting to model how the human brain learns. And in particular, the convolutional neural network that I showed in my example was invented in the '80s by Yann LeCun who is currently the director of AI research at Facebook. But only recently the computational power of modern processors has become sufficient to make it possible to design very deep and complex neural networks that can adequately model the complexity of visual objects. So only recently, the computational power of modern processors have become sufficient to make sufficiently complex models that can adequately cope with the task of visual object recognition. We also have efficient methods to train such networks and we also-- probably that's the most important part-- we also have very large collections of images on which these networks can train. And neural networks in these settings what is called supervised learning are very hungry for data. And in order to learn to recognize images, they literally require millions of examples to learn from. So the convergence of these three trends-- the computational power, the efficient architectures and methods, and also the availability of big data, and last but not least, the persistence of a few research teams in the world that continued working on these methods for the dark ages of AI where neural networks became almost a bad word and extremely unpopular-- so people willing to risk their academic career to push for these ideas-- all has led to the breakthrough or a household name, a phenomenon that we called today deep learning. And I'm sure that you have heard about deep learning in the past couple of years. Even if not, it is very likely that you are deep neural networks even without being aware of it. For example, the Siri voice recognition in your iPhone or if you have a Tesla car, the autopilot feature-- they all are based on deep learning. In particular in the field of computer vision, deep learning had a groundbreaking effect amounting to a little revolution that nearly wiped out previous approaches in a matter of just five years. And I must admit that somehow I have pity for the fact that what turns out to work the best are brute force methods and black box, because we don't really understand how these neural networks work and why they work. These are now the best methods. And especially when dealing with geometry with 3D objects, we have a lot of understanding how geometry works. We have very beautiful differential geometric models, and throwing these models away is really a pity. So the question is can we combine the best of these two worlds? Can we treat the data correctly from the geometric perspective, but also take advantage of the powerful tools available today in the field of machine learning? Or can we build some prior knowledge or reasonable geometric model of our data into the neural network architecture and learn only what doesn't fit well into this model? This is what we call geometric deep learning. It's a new trend in machine learning that is followed by several research groups. Basically, it's an attempt to generalize successful deep learning methods to data that has non-euclidean underlying structure. One of the main challenges in generalizing deep neural networks to geometric non-Euclidean structures is that we need to reinvent all the building blocks of neural networks. For example, take the convolution operation that is used in convolutional neural networks. In an image, we can simply slide a template across the image, right? The way it works is you take a block of pixels, you multiply it by template, you sum up the results and that's the output of the convolution. Then you move your template to a new location. So on the surface, because of the curvature, the local structure of the surface will differ at different points as you can see here. So this notion of page changes throughout the surface, or in technical terms, we don't have shift invariance which determines a lot of important characteristics of convolution in the Euclidean setting. So there are several ways of defining filtering operations that look like convolutions on non-Euclidean spaces. The one I would like to show here is based on modeling heat propagation on surfaces. The first systematic study of heat was done by Joseph Fourier, French mathematician, actually a contemporary of Gauss. And besides being a talented mathematician, he was also a gifted administrator and this cost Fourier a part of his scientific career as he received from Napoleon an offer that he could not refuse-- an appointment as the prefect of the city of Grenoble. And in his work "Analytic Theory of Heat," he formulated some of the very basic equations that are taught today in all mathematical physics courses throughout the world. And also introduce the basics of what we call today free analysis. So his book was published almost 200 years ago, so we are using pretty old stuff here. This is the partial differential equation that governs the heat diffusion on surfaces. It's exactly the same equation Fourier derived in his work. The only difference here is that the operators on the right hand side are defined intrinsically on the surface. We call this equation isotropic because the speed of the heat propagation on the surface here is assumed to be constant and uniform in every direction in every point. So the way of thinking of it is that you pierce this poor horse with an infinitely hot needle at the point, and then you allow as time goes for the heat to spread throughout the entire surface. And basically, the heat propagation follows the structure of the surface. In particular, it depends on the curvature. And the great thing about this equation is that it is intrinsic so the result will not change no matter how we deform this horse shape as long as the deformations remain isometric, or in other words, they don't stretch or they don't tear the surface. So far, we assumed that the heat flows equally fast everywhere. We can consider a more complicated and isotropic heat diffusion where the heat propagation speed depends on position and direction. It is modeled if you're interested in math, by this matrix A in the right hand side of the equation. And in this example, you can see that the heat flows faster in the horizontal direction. Actually the term horizontal here is totally appropriate because the diffusion process is intrinsic and defined on the surface itself. So on the surface, you can talk only about local directions with respect to some local system of coordinates. we can design an isotropic heat diffusion that flows in a prescribed direction, and controlling the direction of the diffusion and the diffusion time-- the size of this heat blob on the surface-- we can construct the locals system of coordinates that is intrinsic by definition. And these coordinates can be used to define a local page and using which you can define operations similar to the template matching we've seen in convolutional neural networks. We used these and similar constructions to develop the first deep neural networks in manifolds where the architecture counts correctly for the structure of the underlying geometric object and has a built-in invariance to deformation. And this is particularly important when dealing with deformable 3D shapes like our body. The deformations of the human body can be approximated reasonably well as isometries, but where they deviate from this model, we can compensate by learning. So in other words, we just learn what cannot be modeled axiomatically and as a result, end up with way simpler network that requires orders of magnitude, less parameters than traditional Euclidean architectures, and also require much less data to train. So it all reduces to much simpler things. When we applied intrinsic convolutional neural networks to learn correspondence between deformable shapes, this is one of the cornerstone problems in many applications in computer vision and graphics. For example, it's a crucial step in the markerless motion capture that I mentioned before. Here, you can see a few examples of correspondence between human shapes visualized by color texture, independent of the poles and individual characteristics of the human body. Geometric deep learning currently holds the record of the best quality 3D correspondence on several standard benchmarks. If so far we are talking about surfaces as examples of geometric objects, there are other structures that can be approached by geometric deep learning methods, such as graphs and networks. And in the remaining time, I would like to show a few results, and maybe, mostly ideas and promising research directions in that regard. In social networks are probably the first example that comes to my mind. Mathematically speaking, it's a graph where the nodes or the vertices represent users and the edges represent the social relations. So you mentioned that we are given a social graph, say from Facebook, and we have information about the users such as their gender, age, education, background, religion, whatever. And assume that for some users, we know how they voted in the recent elections. So geometric deep learning on the social graph can use this data to predict whether a particular user voted for Clinton or for Trump from the information of his friends and their voting patterns and the structure of the graph. So in a sense, the saying, tell me who your friends are and I will tell you who you are has never been more true. Here's another application. I'm sure that many of you watch movies on Netflix. An important problem for Netflix is how to recommend new movies to users predicting whether they might like them or not. And this is called recommendation system and you can imagine what kind of commercial value or impact it might have for online retail platforms such as Amazon or Google. 10 years ago, Netflix even announced the competition bearing a $1 million prize for a search team that would improve on their prediction results. So assume that users can give scores to movies on a scale from 1 to 10. Mathematically, we can think of them as a huge matrix. Netflix has tens of millions of users and hundreds of thousands of movie titles. And the matrix is only very, very sparsely sampled because a single person can see that many movies in his or her lifetime. So the recommendation problem consists of predicting and filling in the missing values of this matrix. Standard approaches use algebraic techniques trying to find some low dimensional model that explains the data, basically by minimizing what is called matrix rank, or more correctly, a convex proxy of the rank, usually the [INAUDIBLE].. Now assume that we know the relations between the users. For example, in the form of the social network that we've seen before, we can use the geometric deep learning to learn how to aggregate scores of the users social neighborhood-- friends and friends of friends-- in order to predict whether he or she will like a movie or not. And we're able to show a significant improvement over standard approaches using these type of techniques. So geometric deep learning seems to have also big promise for recommender systems as well. The final example I would like to mention is from the domain of chemistry and this is work actually done here at Harvard. One of the hard problems in material or drug design is that you need to search over a huge space of potential candidates before you discover that right molecule that does the job. And today, this is still done either by very complicated simulation on supercomputers-- molecular dynamics-- or by experiment in the lab. And if you can think of molecules as graphs where the nodes model atoms and edges are chemical bonds, we can employ geometric deep learning for future rescreening a molecule to determine whether it has certain properties. For example, in drug design, this could be efficiency against certain pathogen or its toxicity. So these methods have already been tested in several real life applications, for example, for the design of new materials. And the promise is huge, potentially cutting times of molecular discovery and design by orders of magnitude. So in general, graphs appear to be a very natural model of relation or interaction between different objects or things. Graphs can model relations between different regions in the human brain in functional magnetic resonance imaging in order to better understand how the brain works or how to diagnose and cure neurological diseases, or interactions between particles in a large Hadron Collider where energetic beams of protons are banged together. And the hope is to discover new physics or maybe spatial relations, spatial structures of proteins that constituents the cells of every living thing. And my hope is that harnessing the power of machine learning and combining it with meaningful geometric structures for this kind of data and these kinds of problems would bringing both quantitative and qualitative breakthroughs. And perhaps we'll see some new exciting science and technology coming out of this research. Thank you very much. [MUSIC PLAYING] 