 very very deep neural networks are difficult to train because of vanishing and exploding gradients types of problems in this video you learn about skip connections which allows you to take the activation from one layer and suddenly feed it to another layer even much deeper in the neural network and using that you really build business which enables you to Train very very deep in that service sometimes even net worth of over a hundred layers let's take a look rest nets are built out of something called a residual block let's first describe what that is here are two layers of a neural network where you start off with some activation and there L then you go to a L plus 1 and then the activation two layers later is a L plus two so to go through the steps in this computation you have a L and then the first thing you do is you apply this linear operator to it which is governed by this equation to go from a L to compute Z L plus one by multiplying by the weight matrix and adding that bias vector after that you apply the relu the linearity to get a o plus one and that's governed by this equation where a L plus one is G of ZL plus 1 then in the next layer you apply this linear step again so it's governed by that equation right so this is quite similar to this equation we saw on the left and then finally you apply another relu operation which is now governed by that equation where g here would be the relu non-linearity and this gives you you know a L plus 2 so in other words for information from al to flow to al plus 2 it needs to go through all of these steps which I'm going to call the main of this set of layers in a position that we're going to make a change to this we're gonna take Al and just fast forward it copy it much further into the neural network to here and just add al before applying the non-linearity they really non-linearity and I'm gonna call this the shortcut so rather than needing to follow the main path the information from Al can now follow a shortcut to go much deeper into the neural network and what that means is that this last equation goes away and we instead have that the output al plus 2 is the rather non-linearity G applied to ZL plus 2 as before but now plus al so the addition of this al here it makes this a residual block and in pictures you can also modify this picture on top by drawing this if your shortcut to go here I'm going to draw it as going into this second layer here because the shortcut is actually added before the relative non-linearity so each of these nodes here where that applies a linear function and a value so al was being injected after the linear part but before the rather part and sometimes instead of the terms shortcut you also hear the term skip connection and that refers to Al just skipping over a layer or kind of skipping over almost two layers in order to pass this information deeper into the neural network so what the inventors of ResNet so that would be climbing her Sally Jiang shouting Ren and Jenson what they found was that using residual blocks allows you to train much deeper neural networks and the way you build a resident is by taking many of these residual blocks blocks like these and stacking them together to form a deep network so let's look at this network this is not a residual network um this is called as a playing that's work this is a terminology of the resonant paper to turn this into residence what you do is you add all those skip connections although those short circuit connections like so so every two layers ends up with that additional change that we saw on their previous slide to turn each of these into a residual block so this picture shows five residual blocks stack together and this is a residual Network and it turns out that if you use you know a standard optimization algorithm such as gradient descents or one of the fancier optimization algorithms to train a plane network so without all the extra residual without all the extra shortcuts or skipped connections I just drew in and perkily you find that as you increase the number of layers the training error will tend to decrease after a while but then they'll tend to go back up and in theory as you make a new network deeper you know it should only do better and better on the training set right so the theory in theory having a deeper network should only hope but in practice on reality having a play network so now the ResNet but having play network this very deep means that your optimization algorithm just as much harder time in training and so in reality your training error gets worse if you pick a network that's too deep but what happens with res Nets is that even as a number of layers gets deeper you can have the performance of the trading ever to keep on going down you know even retrain a network with over a hundred layers and then now some people experimenting with networks there are over a thousand layers although I don't see that use much in practice yet but by taking these activations being XOR these intermediate activations and allowing it to go much deeper than your network this really hosts with the vanishing and exploding gradient problems and allows you to Train much deeper neural networks without really appreciable loss in performance and maybe at some point this will Plateau this will flatten out and doesn't help that much the deeper and deeper networks but residents are so even effective that helping train very deep networks so you've now gotten an overview of how resonance work and in fact in this week's program exercise you get to implement these ideas and see it work for yourself but next I want to share of you better intuition or even more intuition about why resinous work so well let's go onto the next video 