 [Music] okay hi everyone can you all hear me yes okay well hi from sunny Berlin and I guess as a very quick intro I did AI research for about 20 years including through a couple startups and a PhD and for the last five years I've been doing blockchain stuff and my focus these days for the last few years has been ocean protocol so tonight I'll be talking about an protocol as well as a government and drawing and some of my experiences when I was hoping advise the Estonian government as well so yeah that's a quick summary and I will get started so yes my title of my talk as well is ocean protocol and eat government you'll also see that in my talk I will give a lot of different pictures with jellyfish this is kind of two reasons one is jellyfish are in the ocean so that's a good excuse for the second more important one is I think jellyfish are super cool if if an AI were an animal it would probably be a jellyfish so hence lots of jellyfish let's get going so in the talk I'll talk a bit about sort of things a bit from a government perspective first is how can government go digital native cost-effectively and how can they get the most out of data and AI so the outline I'm going to first of all talk about Estonia as a case study and sort of how data fits their identity and payments and secondly I'm going to talk about data and the I which is really a lot about pushing protocol so yes sorry so as a case study Estonia Estonia is truly a digital native nation and well let's just dive in so a very brief recent history of Estonia is the following in 1981 from the Soviet Union 92 new constitution and in 2001 so still 18 years ago they introduced a data platform for Yusuf Isaac at the government called X Road using that on top of that then there were citizen access for digital identity and signatures a few years later Estonia joined the EU and a few years later yet Estonia started to use the euro and then the most recent five years they kicked off a residency so validating the idea in 2014 in 2015 shipping the MVP and going to market in 2016 I'll zoom in on that a little later they're also actually so it sort of digitally focused that in 2017 they floated the idea of s coin an Estonia token and they actually had 700 million hits to the press from that so it was actually a huge amount of press around that and there was even pushback from the EU but they always didn't plan to replace the euro so it didn't matter and actually right on the heels of that though they introduced laws to be really token friendly so things like making it easy to start a foundation clarity between a security token and a utility token and with these things actually they become that within six months the number two place in the world for doing I cos are just out only after Singapore so let's talk a bit more about Estonia this slide is actually taken from a slide deck from my collaborators within the government of Estonia and I just want to illustrate they actually frame themselves as country as a service so they have api's for authentication digital signing tax returns incorporation of companies company profiles and personal profiles so really API is you know not just software as a service but whole country and the top is an example of different things where you can access this in a more user friendly fashion incorporation bank accounts credit cards and so on so country is a service and in fact since they introduced this pot in 2001 on the foundation of that data management platform they built identity and then in the last 18 years they've built a whole bunch of applications on top so some of them on digital signing any business register cabinet so when they make a government decision how it happens is someone floats the bill digitally and then it is simultaneously spread to all the different branches of government and the Prime Minister and then over the next 24-48 hours etc they can all sign asynchronously or not sign if they choose not to they can choose to vote against it and sign against them they have an electronic land register they have electronic voting and they've had this for a long time actually they have location services they have ela which is public access to every piece of draft law that is submitted since February of 2003 he police eat prescriptions residency which I'll talk more about mobile identity and mobile payments a population register which is a state database for basic information and what every person in Estonia eSchool e tax so you can file your tax online it's been like this for a long time electronic health records that's a big one so it's an integration with all the different hospitals in Estonia and the personal data electronic identity card so every citizen has their own ID card and the e-residency of course they have their own identity cards - just a subset about and more so this is basically government applications that are sitting on top of the Estonia platform and to just really in on in service with the residency so euros and C they actually went through three phases that I touched on before the first phase was validation and they started by just putting out something on the overall budget for this phase was just 40,000 euros they put in a landing page that basically asked you know what's your name what country are you in what's your email are you interested and from this they did a study to see you know why would you use this for business are you a fan etc they had their very first ear esident sign up through this they actually just had a very basic shoestring way of doing this and then here is it was a British a very famous British journalist and then finally once they got learnings from that then they could put together a team and a board and a budget to do the next phase so they did this over the span of 69 months the next phase was just building that MVP from the learnings in the first phase and overall they they had to put together some processes things like online application working with the embassies due diligence so they had a kyc each person legals handling that handling bank accounts visa centres services so even though estonia the country on you know the native language is English this service is in English of course because they're targeting the globe I guess they could have done Chinese too but they're going for the Western world api's for signing for cooperating and so on and then finally partners for things like business support FinTech 18 customer support probably the most crucial partner there is their banking prayers so this is what they built in their MVP once again landed it really on the cheap they like to think of residency as startup within government and they really approach it like this so the initial team was quite small I know even from the early days in 2016 they were stolen only add about 8 or 10 people so very very small and pays 3 the go to market was September 2016 onwards so they put together materials they they added more services on top they decided which target market is to go for such as do they want to target digital nomads you know people who live on airplanes or good they want to target people who want to run businesses in the EU and so on and they did community building going to more locations and they put together an advisory board as well so it was my honor to be part of that advisory board about 15 people from around the world so this was basically their go to market and they've been growing ever since I think they've had it with 40,000 to 50,000 residents since so you know to go backwards and talk a bit about the platform in Estonia remember they have all these applications and this this is a screenshot actually from two and a half years ago so it's even more now but if you actually take all those applications you can actually break it apart into three layers the bottom layer is the data platform itself where they have digital signatures built in and then they have data management with something called X Road that they developed internally from middleware and really for their platform to is identity where they have an electronic identity card that sits on top of the digital signatures and the data management as well as payments and they have basically a stable coin it's called the euro and they have you know payments infrastructure on top of that and then the government application sent up with that on the top left this is once again things like tax hell land register voting and so on as well as mobile identity install and of course they also have third-party applications that can build on top of this platform so things like integrations with Nasdaq with the suite bank Thunder being which where people can actually raise money for their startups using a residency in fact thunder beam when they went live their first application was getting investment from from Tim Draper DFJ and Tim Draper he was one of the very first people to get a residency so he uses the residency card to do a digital signature to fund the to fund funder beam so they ate their own dog food from day one which is pretty cool so the foundation is data and then on top of that identity and payments and on top of that the applications whether internally for government or for third party through parties so Estonia has actually read quite a few rewards from being digitally native a lot less money spent on administering in the government and their estimate is and I've seen stats it's 5 5 X - 10 X cheaper compared to non digital native countries they also can have more comprehensive faster and simpler government services for citizens so filing tax in Estonia is a breeze it's on the order of 5 minutes 10 minutes as well as finally more opportunities for citizens you can for example put out digital businesses that can be targeting all the e residents throughout the world Eve residents throughout the world and a lot of other things too simply because you have these API is to require that you might not have had before so that's sort of the first segment of my talk which is really about an example of fully digitally native nation and it's one that's often cited and if you guys have questions about Estonia in the questions part I after my talk I'm happy to field them so yeah I've had the pleasure doing with this journey for many years okay but I prefer if we can do questions at the tail end if that's all right okay okay great so I'm gonna do that now I'm going to go on to data and AI and you know sort of all the starts so from a government perspective there are quite a few potential benefits of embracing AI first off it's saving more money right how might you save money well insights to better target your dollars and so you just you know more effective spending of the money that you have the second thing is automation of course the automation part is a double-edged sword because if you automate something people might lose jobs so that's yeah double edged but also for further improving government services examples include health and there's a lot of use cases for AI in healthcare for example earlier diagnosis of diseases like Parkinson's and so on more accurate diagnosis right now for example in non AI one-third of all radiology is false negatives so basically one-third of all people who have a disease like cancer or Parkinson's it isn't diagnosed by radiologists which is actually terrible so you might learn about it you know way later in cancer or something maybe when it's too late as well as once diagnosis has been made you can have more targeted treatment so that's not a very nice use case also from a government perspective AI health watts for online services so that you people can ask questions more often and get more quick answers and there's many many more ways to further improve government services with you now this this slide is actually pretty interesting I'm going to take a bit of time with it and basically the summary is that AI loves data and I'll start with an example from from my own past my very first got a job was in 1997 working for the Canadian Department of Defense and they had actually paid me to do a research and I was given a dataset of 20,000 different data points for audio radar so in this I had to take in raw audio waveforms audio radar waveforms and then say this is a bird this is a tank this is a plane flying by this is people marching and so on and the idea was if I could classify among these different types of signals and better than they wouldn't need a person sitting inside of the tank listening to the audio right radar and instead they would just only need the driver and whatever person so it basically reduces the number people driving around in the tank at the beginning of the summer I had 55 percent accuracy so 45 percent error which is really terrible and at the end of the summer after toiling and toiling and playing with neural networks and genetic programming and many many things I had improved the accuracy by 10 percent so I was going from 55 percent accuracy to 65 percent and that still meant that one third of the time it still failed and meant it was not usable to deploy to the field I worked in work to work now the thing is I had a fixed data set to work with and you know I worked in toiled and I tried very hard but I couldn't do a lot better and this is actually very common or was common in the world of AI where a PhD student or some researchers given a data set asked to do as well as they can and that's that and that's sort of the that was the the standard for AI for decades and a remarkable thing happened though in the 2000s people started realizing hey let's just keep the algorithms the same and instead add more data and so this plot here what it shows is on the far left it shows the accuracy for four different algorithms the blue is the window algorithm the the red is the KN and a couple other algorithms they're pretty simple and you can see the accuracy is between 75% and 85% now something happens though the x-axis is the number of data points basically and if you go from 200,000 data points going up by 10 X than another 10 X then another 10 X you get 2 I thought it's an X more data points and the accuracy goes up to 95 98 percent so the error goes from you know 25 % AR down to 5 percent without changing the algorithms and that's the critical thing so having way more data dramatically improves the error to the point where the application could be deployable so this was research first this plot is actually from a paper in 2001 out of Microsoft in the mid-2000s Google published a paper called the unreasonable effectiveness of data and they showed this across many many different applications where they kept the algorithm the same and just added way more data 10x 100x 2000 X and with that Google realized they didn't need to improve their AI necessarily just get more data so what do they do well they went and bought companies that had satellites flying around they went in but mapping companies they went and incentivized people to basically give them more data in many many ways like recaptures online and so on and so forth so Google basically turned themselves into not an AI company but a data company they call themselves any a company but I see that simply as a head-fake what they're really doing is getting more data but the summary here is simply AI loves data AI people typically don't talk about this because from the perspective ania of an AI person it's actually pretty embarrassing right you don't need any a person to go and invent a fancy algorithm in it to do a PhD you just get more data and so so basically that's the story here but more data is good for AI for more accurate so what this means is to unlock AI for various use cases in government and otherwise we need to unlock data and then the question is how do we are not data and this has been our focus at Ocean protocol and tonight I'm going to focus on two things that ocean protocol does to unlock data one of them is to connect the data halves with the data have-nots and the second is to unlock private data so I'm gonna first of all focus on collecting connecting data halves with data have-nots and traditionally the data halves include folks like governments enterprises and otherwise and they data have-nots are typically AI folks doing startups thinking okay great I can you know go and create really fancy AAA models but because they're a start-up they don't have a lot of data so how do they get connected and the data halves by the way you don't have as much AI expertise so what ocean is about first of all is connecting these folks the folks on the left the government's the NGOs the enterprises that have a lot of data and compute and on the right with the AI people the data scientists the problem solvers so on the Left basically what ocean helps to unlock is people creating data marketplaces data sharing tools data Commons you know data markets that are where the data is free and on the right you know the data scientists using their data science tools such as scikit-learn tensorflow and otherwise all within things like jupiter which are notebooks and then overall then ocean becomes a substrate for connecting these two sides for basically where you can have a flow of jupiter notebooks that specify the data on one side with the data science in the other side so that's really one of the things that ocean is designed to do is to connect the data has they didn't have knots ocean does this at the heart with using blockchain technology you know we're built on top of EDM using a solidity sommore contract and really at the heart it's decentralized orchestration and I'll talk more about that and so what we envision is with this substrate we want a whole a thousand flowers to bloom of data marketplaces data Commons data science tools and so on to connect these people have the data and want the data so that's the first way to unlock data the second way is to unlock private data and what I mean by this is the following well the most valuable data is private data you know and and people lock it up for good reason because it's private you know they it's potentially worth a lot of money and also they have privacy concerns and control concerns so let's say you know we're thinking about AI for predicting Parkinson's where we want to help diagnose someone who has Parkinson's as early as we can so let's see your Hospital you want to detect Parkinson's if you have an AI researcher in a house for example maybe you have data you know from your hospital but that's maybe only you know how many Parkinson's people have you had going through in the last 5 years or 10 years or 15 maybe a hundred maybe a thousand but obviously there's thousands of hospitals throughout the world and imagine instead if you can actually build an AI model off of all of that data right this would be way better now of course each Hospital has major concerns about privacy right it's gonna kill GDP or laws and all that as well as you know as an individual how do you feel about your data being shared in a big big way and finally control right because of the uneasiness about privacy then you feel the need to control your data you do know I'm wanted to leave your premises so how do you reconcile this another way to frame that this dilemma is the following you know on the left you can have less data on the right you can have more unit but if you have more data they'll get more accurate models to predict Parkinson's sooner to treat Parkinson's better etc you'll save more lives but it hurts privacy it hurts control so what we aim for a notion is the following we unlock the data of more that we unlock the benefit of more data while keeping control and privacy how bring compute to the data so the data stays on premise behind their firewall inside the hospital the government inside the enterprise and then the modeling algorithm is brought to the data and the modeling algorithm is trained at the side of the data the model is stored at the side of the data and only the predictions go to the final party and by doing this it reconciles the the challenge of control and privacy because the data never leaves the premises and just the benefits of predictions do so so that's the summary there and by doing this then we have we still have accuracy up with more data lives saved but also we're retaining privacy in control so this is really what what Ocean is aiming to do so it's got decentralized orchestration in the center which is basically bringing compute to the data so to give you a couple examples of applications on top of ocean ocean is still a pretty young project in many ways we just had the live Network Google live within the last week we've been invaded for a few months now and so one application running on top is ocean Commons and it's open data on an open substrate so you can go to Commons the ocean protocol calm and play with it for yourself where you're seeing this a marketplace to find open datasets and to publish them in the ocean network and you can drill into various things we've got a channel for AF for good for example that the consumption side is we have something called manta ray and this is data science on hosted jupiter notebooks so you can go to date data science totient protocol comm and you'll see this thing called manta ray and with it you can click a couple buttons and you can launch your own post a Jupiter notebook diverging ocean and that's basically a way to use Jupiter notebooks which are interactive notebooks that data scientists can use for data sharing and for scripting and so on and these are meant as demonstrators of applications that people can build on top but we envision that a lot fancier applications can be built this is just the starting point so talk about those applications and kind of where we're headed so we have a collaborator called connected legs and they're based out of Munich and what they are doing is well predicting Parkinson's like the example I gave before and they have collaborators inside the technical and University of Munich as well as the National University of Singapore and so what they're doing is they're building models to predict Parkinson's ie I take models and right now to start with these models can only be using data inside Munich because of GDP our laws in the eurozone and same thing in Singapore with Singapore gaiter laws well with ocean where they're headed is towards sharing the data effectively merging those datasets to build a model across both jurisdictions and in the end the model will never ever leave the premise and so people can then still get predictions off of the model to predict Parkinson's and so on without the issues of data privacy and so on another example is we're working with the IM da which is data authorities of the government of the singapore and what they're interested in is sandboxes for the usage by the Singapore government our agency other agencies their enterprises and startups to build data centric applications with regulator to recover so because there are things like gdpr and so on how do you test out your applications in a usage context with various parties involved in in sort of a Sandboxie way and this is what we're doing with ocean working with the MDA and Singapore another example is with Grover Asia it's a spin-off of the World Economic Forum and it's helping small farmers to allocate their fertilizer better to increase yields another example is the AI Commons so in this case a a commons was it's a recent movement and nonprofit foundation with founders from the itu from XPrize and from other wise and overall it's good towards using ocean as a substrate to help scale AI for good applications applications that can help the UN sustainable development goals another collaborator that we're working with is Energy web planation and energy wind pollination they just launched their other chain recently and on top of that so it's a energy web phone nation it's consortium of more than 100 organizations including many of the top energy utilities in Europe in Germany in Netherlands and more and so they've launched this chain and there are several applications on top and one application that they have is energy web authentication so by having energy with authenticated data in combination with ocean then we can have applications that have sort of vet and data that can be then used in other applications in Iceland a specific application in the energy world is ver so what Verve is doing is they have smart meters that are connected to blockchain and these smart meters can let their users say hey I want to make my data available on this marketplace for data and then my data can be sold for third parties who want to look at it to see if there's benefits for their own models for example it turns out with these smart meters they have very high sample rates so they can with the right AI on top you can detect whether or not someone's washing machine is broken and in fact whether it's a Samsung washing machine versus an LG washing machine and of course this is of great interest to the Samsung and LG s of the world because then they can then a specifically target customer saying hey are you interested in buying a new washing machine so that's what verb is going for and we're working with them on basically data marketplace for energy data I'll talk a bit about ocean and open technology now and it'll start to link back to the government side so first of all you know under the hood what's making this possible to unlock data unlock data with marketplaces as well as unlock data be a compete to the data and at the heart of it it's a decentralized network for access management so what we have here is a computer tag where you have a data provider providing well data and an algorithm and that goes into compute and the compute is well computing the algorithm training a model off of the data that model can get stored and then from that you can do analytics prediction curation and more so this is a very simple de compute today but of course you can get more and more complicated within the world of big data there's already orchestration infrastructure out there things like Apache think pachi spark and so on so you can think of ocean as an instance of this but focusing on sort of unstoppable orchestration if you will be centralized orchestration and so with an ocean you specify something called a service execution agreement a sea which is sort of like a solidity smart contract and in fact that uses solidity but it's targeted towards these networks that do storage and compute ocean itself is isn't doing storage itself or compute itself instead it brings in third party networks to do storage and compute and these can be centralized or decentralized it could be centralized behind the firewall for storage decentralized for storage like Falco norcia or centralized cloud like say Amazon s3 same thing for compute it can be centralized behind a firewall like your your local phone to compute or local data center it can be centralized cloud like ec2 or it can be decentralized out such as Gollum or things that are tuned towards privacy like such as enigma or tuned towards AI such a singularity now so ocean basically sits on top of those and is designed to be calling in those various services bit by bit a bit more about the tech stack this is a bit of a little bit over all so you know we're using modern software tooling we're leveraging decentralized said an identifier the IDs as well as the centralized objects DDoS verifiable credentials Jason Jason LD all this stuff is going through the III see of course it's leveraging we're built on top of solidity and the deployment is to work is on a parity POA network so it's a private EVM Network in this sense but it's publicly accessible so there's a extensibility via customer contracts of course so you can have any functionality on top and from a government perspective you can have plug-in identity things like you port sovereign and so on which also of course you GID Studios etc because it's for contracts you can a plug-in payments there's opportunities for stable coins etc it's decentralized of course so there's no single point of failure and you can get provenance in the transactions and this can be for all the key components identity payments and data of course provenance of data is a big thing actually in the world who big data management so for interoperability to other networks ocean itself already has a token bridge running to be a theorem main map and in fact the ocean tokens themselves live on the aether and maintenance and then go across this this year see 20 token bridge so then they run as wrapped token since I'd be ocean PA network and this gets the benefits of scalability in the ocean network yet at the same time having the security of the etherium main net and the full permissionless decentralization of a theorem make that so people can basically choose how much value to put at risk when they're running on on ocean we also a key thing for ocean for the privacy preserve preservation and whatnot we use a parry secret store which provides proxy encryption and what this means is let's say that I want to share a data set with you well a simple way to do this if it's just you and me it would be with something like at diffie-hellman sharing but of course this is only a one-to-one sharing but what if I want to share to many people and the answer for that is proxy encryption and parody secrets are essentially implements that so this allows us to have scalable secret sharing it allows us to have permissions granted to many many parties you know we have SDKs in Python in JavaScript and these are called squid so we have squid pythons goodbye squidgy ass and also actually squid Java and of course it's a already Jupiter you know integration people can use it out of anaconda so I could learn tensorflow thanks to the Python SDK it's all open source we've got thorough documentation etc you can go to Doc's Direction protocol calm and a bit of protocol itself it's a protocol so you know it's in the sense of specification it's how machines communicate it's a community 8,000 stakeholders - and 50 ambassadors across I think more than 100 nations and above 30 advisers from the blockchain space AI space and more it's a non-profit foundation it's actually based out of Singapore and the mission of the foundation is to steward the protocol and the community and finally it's networks that implement the protocol both public and private so I've talked mostly about the public network is this POA network but people can take the protocol and deploy as their own Pio a network for various use cases and that actually has a lot of use cases where people don't need the token as much and just really want to deploy within a government deployment or within consortium and so on so characteristics is decentralized of course it's it can be global and open with the public network or you can have a more private permission never it's reconciling privacy and you can actually plug in legals - which is pretty cool and this is because these service execution agreements you can have PDFs attached and have actual contracts so you can leverage a Ricardian contract type set up here and pull in actual equals we've got a bunch of collaborators everything from energy web flow nation XPrize to parody the government of Singapore with a Singapore and otherwise a lot of other blockchain companies gnosis chain link and more other consortiums like mobi which is the mobility consortium as well as use cases within bigan enterprises such as with Aviva we're working with Unilever Roche Johnson & Johnson and more so going back to the e-government to kind of tie these things together so we see that you know ocean because it's fits really well as a day and platform that can be very useful for government organizations and offices so what we have here is each one of these organizations organization one two three four that could be a different converted office and they can each have their own database and they can expose for that database via Access Management devotion and that of course can be your own private network or you could use the ocean public network in a permissions fashion because it's really about the ocean is permissioning network in a sense at the end of the day it's Google Drive without the Google so to reconcile loop this back to what Estonia has as a set up this generalizes for whatever governments whether it's Ukraine or otherwise at the very bottom level you can have a platform for data which has the digital signatures and of course leveraging dids videos and so on as well as data management which is where ocean comes in and not just data management in the traditional way but also to unlock on data for use in AI with things like data marketplaces and bring compute the data then on top of that you can of middleware identity and payments as the key things just like before and the case of identity this would then can leverage on identity standards CD D ID CDOs as well as identity infrastructure from the likes of sovereign and you port same thing for payments emerging payments infrastructure that we've been seeing with stable coins as well as tiny tot stable coins that tie into the euro and otherwise and finally applications on top for governments whether it's for tax or for voting or and so on as well as to third-party applications leveraging this so I see that ocean could be very useful for government applications across the board it's sort of as a nation but also for just very specific one-off applications within and of course the benefits are similar to what attracted looking for you know using modern software tools and building blocks di DS DDoS extensibility via EVMS or contracts SDKs in Python and JavaScript you know modern versions like my script six as well as parity secret store there's no single point of failure you get provenance of transactions and you're unlocking data for use by AI and overall you know ocean we've actually had a longtime collaboration with the government of Singapore myself personally I've had a long time collaboration with the government of Estonia but we're looking for other collaborators and we're in talks with several nations on this and so yeah just sort of putting the word out that we're looking for more collaborations for cost-effective data management in governments so to conclude I started with the questions helping governments go digital native cost-effectively and how can governments get the most out of data and AI and I've talked about the digital native case study of Estonia which at its heart has data and on top identity and payments and I've talked about data and AI and how they relate and how ocean can help and specifically how ocean does theta access manager and by doing this it unlocks data in two main ways connecting the data halves with the data have-nots the data marketplaces and Commons and also unlocking the benefits of more data without losing control or privacy by bringing compute to the data and yeah so I guess there's a few things one of them you might have seen in 2014 there was something called the d5 the digital five including Estonia in Canada and then a couple years later became the d7 they added a couple more and now it's the d9 so there's nine nations that have committed to being digital if you will and they already have a lot of digital aspects already and they have several sort of core digital principles including open source software open data open standards etc etc so so that's a trend and it's clearly growing which i think is quite exciting that's sort of a broad macro level thing I think the other angle is just in my own conversations with governments from around there around the world and I probably talked to about eight or nine in the last six months and I see there's a couple challenges they're having related to data one of them is and in fact you can kind of tie them together so there's several governments where they have strong provinces or States sort of like the Government of Canada you know I come from Canada and the province it's a federation of provinces really so each province the federal government wants data sharing from each province for say healthcare data and they say hey give us all your data and the provinces say well you know we have regulations so we you have to follow those regulations to be sharing utterly but the problem is once you have the different regulations from the different provinces you if you have to follow all the regulations you end up with no data and so that's one problem the other problem is Canada has a healthy history of profits as being sceptical towards Kennedy that the federal government around around many things so they're they're hesitant to share you know super private data to something federal so that's that's really a problem that they're encountering you know how do they get the benefits of more data yet at the same time reconcile privacy and control so this is a big deal um and it's actually come to a team more recently in Canada because Canada effort has super clusters for super clusters for various things for healthcare for AI and for others and they're really running into this and I've seen it I'm using that as a specific example but I'm seeing that in other governments as well where they have you know national efforts on AI and as soon as they have a national effort on AI then the first thing that happens is they may say what about data and they look at their data strategy and they realize they don't have one or it's very very bad and I you know Germany is an example of that too where they've declared that they have a national AI strategy but then they went to implement it and they've actually allocated earmark money for two years and hardly any of that money has actually been deployed or they've deployed some money towards data centers but they actually don't have enough internal expertise to do a good job on that so basically this sort of relation from the national government level down to the states or the different regions is a problem and the other one is just with within different government departments so department a doesn't talk to you Department B doesn't talk to Department C but the national government might want to get do bi business intelligence or analytics across all the departments and they have real troubles doing that and I've talked to CIOs of governments themselves governments and then they encounter this problem again and again interestingly this is also a problem with in big enterprises especially if they've gone through a bunch of merger and acquisitions such as Volkswagen which has you know various car companies below each with its own personality so those would be the the macro trends basically you know all wanting to have AI strategies once they drill into that they need to have data strategies once they drill into that they realize they there's real challenges on this benefits of merging the data with privacy and control and yeah so that's kind of the summary there that as the macro level and then that's the first thing I mentioned which was this desire to go more digital and these movements on you know the d9 thank you [Music] I just lost my presentation so I have a question about the security here there so how many is a was a creator of model can be convinced and crews out of processing my model gains a date as a great didn't gold is really truly noble Schoology I'm mister last sentence I'm sorry I was listening but it's just really hard yeah so that's a great question actually and just to make sure that I've heard it correctly you know how can I make sure that the data doesn't escape how can I make sure that the model doesn't escape after it's been created and how we see things is is the following actually I'll start with a story we looked at ocean and people said I'm really worried about data escaping all this you know I don't want to share my data because it's private and the first thing we did was look around at all the different sort of new emerging technology around probably see preserve and compute things like SGX and other trusted execution environments with folks like enigma and so on things like homomorphic encryption which do compute unencrypted data on things like multi-party compute which compute across you know have many many parties compute each only seeing a fraction of it things like zero knowledge proves and with each of these it became pretty clear that none of the technology was mature or scalable yet it has great promise but it's not there yet also the second thing is especially talking with governments and large enterprises even if the aforementioned technologies were a bit more mature there are hard to trust from the perspective of governments and enterprises where their data is really really precious and so they got really even if it's even if the technology is great they don't want the data to leave their premises and I can y-you know let's see it leaves the premises because let's say homework encryption works really well well along comes quite a computing and decrypts everything you know in five years or ten years so you really need to keep it behind premises for the future especially with data that's sensitive over a longer period of time so we realized that we needed to have compute coming to the data so then the question is how do you do that in a way that it's fully secure and I would say we don't have a perfect answer but the answer is that we we give is you have to vet the each line of code for training the model to make sure that it isn't on spitting out data externally you can have docker containers that are focused simply on running internally and can't talk to the external network so we collaborate with fit to chain for example for for this sort of thing on secure docker containers so embedded data and the good thing is neural network training and other model training algorithms are actually pretty small so you can you can inspect those in detail you have to be very careful though about the libraries they import because each library itself could have a lot going on and that's where you know just inspecting how much data is actually leaving is very important the second thing is the models themselves now you could let the model itself leave the premises but if a model is trained well then it can actually leak personally identifiable information too that's why we advocate that the model itself needs to stay on premise next to the data all the time and you have a limited number of queries to the final user and you need to rate them the queries otherwise someone could query them a lot of lots and lots you know a million times ten million times and we can start the model themselves externally so this is why overall we talk about you know the orchestration needs to be decentralized not just the model building but the orchestration and finally the final models - does that answer your question yeah [Music] [Music] [Applause] the question on your point of view how many people and so I think there's two questions how many people will adopt ocean and how many people will adopt cryptocurrencies and on Ocean I see that ocean is a back-end technology so generally final and users won't be thinking about just like right now you know when they're using Facebook they're not thinking about rocks DB under the hood which has been in Facebook for a long time or MongoDB or any of that right so I see that ocean as a back-end technology they're not thinking about on the more broad question how many people you know do I think will adopt a cryptocurrency III believe that you know blockchain technology itself is new fundamental technology for infrastructure of civilization just like TCP a just like in the past you know we have you know air and the wind and the soil as as infrastructure and then on top of that the electric grid and roads and everything and then on top of that we've started to add digital technology most notably tcp/ip and so now that we have we can have stateful decentralized networks with a business model attached I see that there's going to be more technologies that sit on top with this on you know in the case of ocean I see it as infrastructure for civilization that's essentially a permissioning layer as well as orchestration and on top of that though other applications do include sources of value do include payments do include identity and I see that the way to help drive these to incentivize the usage of these it is via so I believe that you know at that level people will be using tokens and as stores the value which does hit you know the final consumer just like with Bitcoin right now so I think that this will you know grow as fundamental technology for civilization and people everyone will use it right so I think the future is you know tokens some of these tokens may be you know the Euro that has been issued by the Central Bank of Europe and that actually is in the roadmap for example right I do see also that you know China kind countries that I'd like to see every transaction for their citizens such as China or Russia they're gonna roll out their own blockchains - with tokens and it's not going to be very nice from a privacy perspective but it will exist so so that's you know from the the use case of store value as well as from from payments but there will be other use pieces - like identity and so on right you know like it or not libros here and that will be distribution of blockchain technology to 2 billion people so and that's an identity solution it's a payment solution um and maybe more so yeah I think you know there all of me the whole world will be using blockchain in a visible way to the world but there also be a lot of watching technology there will be you know in the background [Music] well at first it will feel slow and then and then it will feel fast you know it's an interesting question right I think there's already deployment you know a lot millions of people are holding between already I think something like 9 percent of all Americans hold a Bitcoin or another cryptocurrency so that's starting to basically hit mainstream and you know a lot of other nations won't be far behind and in terms of the backend stuff right you know myself before I was doing crypto I mentioned I was doing AI and I know technology that was used to design computer chips and that technology was used by TSMC and Apple and NVIDIA and most of the other major semiconductors so basically it's touch every person on the planet AI has touched every person on the planet then in this very backend way right so you know probably in some way crypto has already touched every person on the planet in a back-end way if not yet soon so you know Libre the timeline I don't know the exact timeline but it's within the next you know three months six months nine months they're going to be deploying and Facebook is their channel so that's two billion people so that's pretty fast right I also see though because of Facebook it means that enterprises are now much more open to tokens than they were before and that actually changes things because you know rewind to 2015 banks wanted watching without tokens then in 2017 and reprise is one of blockchain without tokens but now though because you know one of the most valuable companies in the planet has gone full steam ahead with tokens that changes the landscape a lot so I think you know tokens are going to be pervasive not only with consumers but also within enterprises sorry I missed out again yeah it's it's right probably others like Venezuela probably has higher penetration than USA because they have more pain right maybe Ukraine I don't know so I just know like you know is already the second question is there's oh you will improve cooperation yeah so that's a great question I think so examples of useful models I to me the biggest use case for opening data is health so and you know from a personal level I would love it if I could dump in my genome and my guts genome and my all my personal behavior being recorded and dump it into some AI in the decentralized cloud and then it gives back predictions of what is my chance of getting Parkinson's etc as well as what are things that I can do to intervene to minimize risk to maximize my health over time right and you know we're seeing pieces of that puzzle being assembled by things like 23andme where the business model is actually data silo and fortunately or things like clue which is a consumer app targeting women to start with for gathering data to Apple watch and so on right so we're seeing pieces but that's kind of one of the big aims and in the near term it's just you know people are solving very specific problems like predicting Parkinson's so from the perspective of government national government you want your citizens to be healthier right the healthier they are then the more productive they are and society the higher your GDP this is why for example China for better course has built into its Social Credit system where you increase your Social Credit if you go to the gym if you exercise if you eat Bo and after you many insurance providers health insurance providers do this as well in America and otherwise so there's incentives that are starting to come for this so you know models to predict Parkinson's you know give directions and Wakita and so on that's a big use case so and that can be within one nation at a time and that's probably how it will start as nations will you know have internal nation data I just like Estonia has just like Singapore has and they will you know drive that and then they will also they already many many governments have had mandates for open data but there hasn't been much incentive to really make it usable so maybe they make it open but it's just PDFs that you have to scan but if they start to see that there's some real use cases for others to use it and easier to share it and then they will open it up more they'll put it on to ocean there will be third-party services that makes that data more accessible on with you know more tabular data and so on to the to the extent you can put it into things like Wolfram Alpha that can then compute on a truly meaningful waves and so there will be more and more incentive to use government data and I think that from this these two use cases things like healthcare and so on as well as just the open data that governments already have getting used more that will help to drive things how I see you know what's gonna drive nations to share the data between various nations well it's actually starting to happen in a limited scale for example Finland and Estonia are used they're both using X Road and from the data management system created by Estonia to start sharing their data to what extent for the cheryan in a big way I'm not sure I don't know if their incentives are that strong yet because they each have their own notion of digital sovereignty I see that the biggest reason will be to compete effectively against China China has one point four billion people these people do not care about their privacy as much because of Chinese culture so China can with a single market build data off of 1.4 billion people and so if your USA with 300 plus million people you know your AI models aren't gonna be as good if your Estonia with 1.4 million people that's 1,000 X smaller than China you have no chance in building am models that are competitive compared to China's models so what do you do well China won't be incentivize to share with others so you're gonna have to start you know sharing with with your data with Finland and then with bigger and bigger nations you know France and Germany and the UK and over time we'll probably see two or three big emergences where things just started you know a billion people at a pop right India EU and Europe sorry EU and US and North America maybe and we'll see how this all works out right but it'll all be all you know 1 billion plus people right probably around the health care data around others traffic we'll see but it's basically because just like oil you know rewind 100 years oil it was a major tool for national competition and and back and forth and there was a lot of dynamics around nation and nations power with oil it's going to be a similar thing with AI and the thing that drives a is data so data is going to become a critical tool for in coming years yeah so that's a great question in fact I didn't dive into that in this talk but I will now yeah the short answer is federated learning so let's unpack that what federated learning does is imagine I'm trying to build a model to predict Parkinson's um I create a neural network I randomly initialize the weights and it goes into it lives in the decentralized network on its own inside ocean basically and it's got access control rate-limiting etc then it goes to hospital number one and says hey can we I'd like to get my weights updated to update the model training basically based on the data from hospital number one and same you know great and it goes to hospital number two and say Berlin it updates itself based on the data there and then and then it goes to hospital number three and say Singapore Hospital number four and sake yeah and it keeps doing this hospital number five Number six number seven in each case it's getting a delta of the training wait to update itself but it's not giving that you know it's not giving the model itself to the hospital right it's just doing an update and by doing this then you can basically learn across a thousand or 10,000 data sets compared to before where you had to be centralized now in fact Google started pioneering this technology about three years ago and and they continue to improve it over time there's even a variant of tensorflow called tensorflow federated now which is actually super cool right so they have T centralized the data one challenge with their version of federated learning is it is decentralizing the data but it is still centralizing the orchestration and it is centralizing the final data sorry the final model so you need to decentralized all three you need to decentralize the data you need to decentralize the orchestration and you need to be centralized the final model and by doing all three that's how you retain the privacy otherwise let's say you centralize the orchestration then any you have to trust the organization doing the orchestration if you centralize the final model the entity that can see that can then reverse-engineer and you can have private information leak up from there so you need to decentralized all three and the heart of this is the centralized the key to this is decentralized orchestration which is what ocean is about we don't have you know ocean we don't have decentralized federated learning yet but ocean has the bones for that as time goes on and we're very interested in in fleshing this out because we see that's going to be a key application for for many many use cases [Applause] thank you 