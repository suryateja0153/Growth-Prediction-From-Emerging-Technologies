 [Music] so quick background on pleat psmith we automate device setup intelligence patching and security for your company's Apple devices one of the coolest things we do is make it so that factory fresh laptops and iPhones and iPads can be dropped drop shipped directly to remote employees without prior manual setup by IT and as soon as they're unboxed and connect to Wi-Fi they automatically enroll all the option settings are installed automatically over the Internet securely we built our product on GCP in part due to Google's high bar for security and we actually roll our own kubernetes cluster on top of GCE and use hash core vault for secrets management PKI and crypto as a service if we were actually going to deploy kubernetes today we would use gke where we're only rolling around cluster for some legacy reasons awesome so this talk assumes that you have some knowledge of containers and kubernetes and focuses on practically protecting your workloads in gke specifically we're gonna dive into what's your responsibility versus what Google handles for you I will say also just in general security in the cloud is that shared responsibility between your cloud provider and you Google does a lot of things to protect the underlying infrastructure like encryption at rest by default and also providing capabilities to you that you can use to protect your workloads like I am as newer infrastructure models emerge it's not always easy to figure out what you're actually on the hook for so that's really what we're diving into on this talk so let's jump in and review some kubernetes basics from a security point of view to make sure that we're all on the same page so within that kubernetes cluster there's only really three core fundamental pieces these are the parts of the diagram that you see in dark grey and let's walk through them from the bottom up so at the bottom layer you've got the storage that's at CD think of this like the kubernetes hard drive next up you've got the master that controls your entire cluster and then the next layer up you've got the nodes that's the machines virtual machines that are running pods and pods are just collections of one or more containers so let's now walk through the damage that can be done at each layer from the top down so at the top layer you've got the containers again in the pod if an attacker can control a container they can do things like abuse compute to do things like mine cryptocurrency for example schedule arbitrary workloads on the node if they're able to escape the container potentially access customer data and yeah if there's if there isn't a strong security boundary there they could potentially escape to the node from the node if an attacker control can control a node that means they can control all the pods running on a node they can do things like abuse compute at the node layer kill existing workloads and run arbitrary workloads they also may try to attack the master the next layer up and do things like you know denial of service attacks for example so next up you've got the master itself that controls your entire cluster controls workloads where they're scheduled everything so if an attacker compromises your master that's pretty bad they can compete completely take your environment offline and short of killing the entire you know cluster there's really not much you're going to be able to do to be able to react to that type of attack finally Etsy d again like your kubernetes hard drive right that stores the entire cluster state so it's kind of like stealing a harddrive right they're gonna get all your configs the attacker could create modifier or destroy your entire cluster and they may even be able to recreate or model your entire cluster elsewhere so let's look at Google's shared responsibility model and how that applies to Google Cloud so you might have heard this term shared responsibility model before it refers to how in the cloud the cloud provider and the user split the responsibility to protect your what you what you're running so to be successful at this your cloud provider it has to be really clear about which aspects of security they're responsible for and which aspects and security you're responsible for generally providers are responsible for securing infrastructure and you're responsible for securing workloads let's break this down for Google Club so as you can see in the diagram here Google cloud manages the blue bits whereas the user manages the Green infrastructure-as-a-service on the left users are responsible for things like network security the deployment itself and web app security whereas Google is responsible for cathode just the core infrastructure for platform-as-a-service in the middle Google takes on more of the responsibilities including components like authentication identity and operating the service the user is still responsible for deploying and securing their application and on the right for software as a service Google is also responsible for these pieces and so you as the user only responsible for protecting your data and content using the appropriate access policies this break down is actually available on a white paper that we published late last year on Incident Response and I'll link to that at the end of the talk but looking at this where it is BKE fall right it's probably closest to the middle that is Google manages a lot of the day-to-day running of a service but you're still responsible for your specific workloads that run on gk so let's quickly recap what actually happens at that bottom layer first the blue that goes across all of the bottom and how Google protects those services on Google Club so one of the key security differentiators for google cloud is our core infrastructure right the infrastructure doesn't rely on any single technology to make it secure rather we build security from a progressive layers that deliver defense-in-depth so starting from the bottom up with hardware Google purpose builds hardware in our data centers from chip to network in order to ensure a secure supply chain service deployment we have a build test and deployment pipeline that ensures that code running on our servers and our data centers is meant to be there so that any application binary that runs the infrastructure is purposely deployed this lets us enforce requirements that like ensuring that code is reviewed by a second reviewer and in other such policies and requirements we might have once deployed we don't assume any trust between services they actually need to authenticate to each other in the environment so we were built we built this in fresh infrastructure to be multi-tenant from the get-go at the storage layer data stored and infrastructure is automatically encrypted at rest and distributed for available a team rather reliability we also moment strict data access controls and robust logging features at the storage layer underlying several of the applications that you can use on Google Cloud Adeiny in access all identities users and services are strongly authenticated this authentication for users for example using hardware second factor helps protect against phishing attacks secure communication communications over the Internet to our cloud services are encrypted and communications between services internally are authenticated and encrypted when they go between physical boundaries and operational and device security the scale of our infrastructure allows us to absorb a lot of attacks including DDoS attacks and our ops team works 24/7 to respond to threats and incidents all of this is explained in detail in another way paper called our infrastructure security design my paper that I'll also link to at the end of the talk but just note that the way that Google does a lot of these things is unique write anything that runs on top of this stack including Google's workloads but also Google Cloud workloads that our users want to run benefit from having these underlying protections so go into gke beyond that base infrastructure let's look at kubernetes if you're running kubernetes yourself for example on GCE on-prem whatever it happens to be you're responsible for managing all of it including the control plane and day-to-day operational work we often see users start with their own deployment and then when it becomes too much to handle migrate over to GK if instead of used gke kubernetes engine is a lot of the heavy lifting for you so for kubernetes engine at a high level the items on the left google is responsible for protecting in addition to the underlying infrastructure that I mentioned already the kubernetes distribution so a Koopa dainese engine makes available the latest upstream version of kubernetes supporting several minor versions providing updates to these is Google's responsibility in the control plane in crew Bernays engine Google manages the control plane which includes a master VMs the API server and other components running on those VMs and the NCD database this includes upgrades again including patching scaling and repairs all backed by an SLO the operating systems that your nodes run such as container optimized OS or costs and new boon to kubernetes engine promptly makes any patches to these images available note that this is the system running on your no it's not not the OS running inside your containers right think about container to OSS what do you think about this Google cloud and as in the last piece that Google manages for you our Google cloud integrations for I am called audit logging cloud security command center cloud kms staff driver etc these enable controls available for for is workloads across Google cloud directly in GK conversely the user is responsible for protecting the pieces on the right the nodes that run your workloads including VM images and your configurations you're responsible for any extra software installed in the nodes or any configurations changes made to the defaults you're also responsible for keeping your nodes updated Google provides hardened VM images and configurations by default manages the containers that are necessary to run gke and provides patches for your OS you're just responsible for upgrading if you use node auto upgrade and move that responsibility of upgrading the nodes back to Google and you're responsible for your workloads including your application code docker files container images content and your running containers and pods this includes leveraging gke features and other GCP features to help protect what your weight you're running in containers so let's go and through the couple of bit components here the control plane the worker nodes and then your workloads to see how we would protect each piece so the control plane first Google I just mentioned Google is responsible for securing the control plane which is the component of kubernetes that manages how kubernetes communicates with the cluster and applies the users desired state to the cluster this is Google's responsibility what does that mean well if you look at how gke is architected in gke the hosted control plane components run in a Google owned GCP project and they control resources that are in a user's GCP project each customers control plane components run on GCE instances and these instances are single tenants meaning that each instance runs the control plane and its components for only one customer so what's actually in that control plane a couple of different pieces first the master VM it's a set of virtual machines that runs master components like I just mentioned these are single tenants GCE VMs in a Google owned project Google uses container optimized OS or costs for these and manages the health of these VMs including updating the OS the API server scheduler controller manager and other controllers run on top of these master VMs these are the typical functions of your communities cluster after all the API that you're interacting with when use gke is the same as a normal you know criminals api that you get by running it yourself it just happens to be managed by Google the control plan also manages xcd which is a database behind kubernetes which maintains state just like the rest of GCP contents stored in at CD and gke is encrypted at the filesystem layer by default so you don't have to do anything to get that at CD only listens on to TCP ports 1 to communicate with the kubernetes api and the other four server-to-server communication when transmitting traffic from one end CD server to another these communications are protected using mutual TLS next piece the control plan also runs the cluster CA and manages the clusters root of trust key material each cluster has its own root certificate authority the API server and cubelets rely on kubernetes cluster root CA for trust each cluster runs its own CA so that if one cluster CA would be compromised no other cluster CA would be affected an internal Google service manages root keys for the CA and these are privately held by Google outside of the cluster I am authentication and authorization is handled by the master as well this works the same way for the criminales api server and at CD as it's done for other Google cloud services and lastly the master includes the audit logging configuration for the master components this provides a detailed record available in stackdriver of calls that are made to the Carbonite ease API server that was quite a lot so the goal here and throughout kubernetes engine as you'll see is to give you strong security everywhere control where you need it insane defaults in case you never quite get around to configuring that thing yourself so keeping the goal of sane defaults in mind we've been making hardening changes to the control plane over time so let's see what's actually changed recently let's look at the changes in some of the recent gke releases first in 1.7 the kubernetes dashboard and gke had its highly privileged admin access removed you might have heard of a couple of public hacks of kerbin Eddie's in which case you've probably seen the dashboard prominently featured Tesla Aviva Weight Watchers were all affected in late 2017 and early 2018 where they had their dashboards deployed on the public web not password protected and had privileged cloud service account credentials in their attacker has found the dashboards one and took the credentials logged into to the cloud service and started cryptocurrency mining so on gke we had actually made this hardening change before these attacks occurred in 1.8 gke enabled role-based access control or are back by default in the v1 API our back is a method of restricting access to resources based on roles and permissions in kubernetes its implemented to allow you control access to pods deployments config maps jobs basically anything you can think of it's meant to replace it replaces the prior authentication method of attribute based access control and provides a more intuitive way of handling permissions by using our back you can more easily follow the principle of least privilege to set roles with minimum permissions to resources inside your cluster in 1.83 GK enabled cloud audit logging with the default current IDs policy so that both your GCP API and the kubernetes api audit logs are written to stack driver for admit activities does lets you easily go back and see who did what and when in your cluster in 1.10 in response to the attacks that I mentioned earlier that were happening in the wild against Tesla and other I just lost my mic ok good in response to the attack which occurred in the wild against Tesla and others we disabled the coronaries dashboard by default and gke instead you can use the cloud console to have this the same functionality the monitor clusters the next bit requires a little bit of explanation so criminales offers several authentication methods in gke the supported methods are open ID connect tokens x.509 client certificates and static passwords gke manage manages authentication for G cloud by using the Open ID connect token method setting up the community configuration for you getting an access to and keeping it up-to-date the other authentication methods x.509 certain static passwords prevent present a wider surface of attack so basic auth has never really been a best practice and x.509 certain kerbin nes are poor are poorly auditable so unless your application specifically needs these you shouldn't be enabling these in your cluster so starting in 1.12 gke no longer generates these credentials for you by default when you create a new cluster and also ng in 1.12 gke removed access to the GCE legacy and metadata server api's for your cluster some practical attacks that were happening in the wild against kubernetes rely on access to the VMS metadata server to extract the nodes credentials notably Shopify last year had a bug bounty where a researcher accessed this metadata and had broader access to the cluster than expected so we removed that by default note that all of these hardening efforts that I've been talking about change the defaults for new clusters right we can't do this for older clusters to allow for backwards compatibility so if you have an older cluster that you've been continually upgrading please go in and make sure to change some these defaults yourselves to get the the leading edge security so critically though Google is also responsible for patching anything running in the control plane we have a high bar for security and but you know bugs do occur in in open source projects and things that we you that are running in your clusters so we want to make sure to promptly patch these Google automatically upgrades the master VMs but you're responsible for upgrading our notes and I'll talk more about auto node auto operate in a second so looking at you know there's since there are so many different pieces here let's quickly let this go over what this looks like in practice it depends where the vulnerability is if the vulnerability is in the kernel or an operating system Google will apply the patch to affected components including obtaining and applying the patch to host images for kubernetes costs and open to Spector in meltdown and L 1tf are examples of such vulnerabilities if the vulnerabilities in kubernetes well Google's involved in free Benes product security team and often helps develop and test patches for kubernetes vulnerabilities since gke is an official distribution google receives the patch as part of the private distributors list if the vol is in a component used in GK use default configuration for example calicos contain a network interface CNI or a net CD then in this case Google may receive a pact from upstream kubernetes from a partner or from the distributor list of another open-source project and lastly if the vuln is in gke itself for example if it's discovered through vulnerability rewards program then google is responsible for developing and applying the fix in all of these cases Google makes the patches available as part of general kubernetes engine releases which are the patch releases and bug fixes as soon as possible given the level of risk embargo time and any other contextual factors where the vulnerability is severe enough or user action is required we publish a Security Bulletin detailing this information on our website so that was the control plane now look at the notes the worker notes for your notes Google does most of the hard work by providing the components that you need and the patches that you need for those when there's a security issue but it's still your responsibility to upgrade in order to reap those benefits in kubernetes your worker nodes are where you run your workloads your worker nodes in gke consists of a few different surfaces that need to be protected including the node OS the container runtime kubernetes components like the cubelet and queue proxy and google system containers for monitoring and logging google provides all of these components and where necessary you decide what you want for your notes so you get to choose the operating system between costs and a blue two and you choose a runtime that you'd like separate containers run for critized components like queue proxy and cube dns and Google specific add-ons to provide logging monitoring and other services Google's responsible for those containers in all of these cases Google is responsible for patching these components but you're responsible for deciding when to upgrade to apply those patches so for example a patch to cost to container D or kubernetes would all be provided by gke but the user has to upgrade to apply them let's talk a little bit more about the OS choice gku provides two options for your node OS again this is the OS running on your node not the OS running inside of your container you can use container optimized OS or cots and Ubuntu as mentioned earlier GK's control plane uses costs you're deciding what you want to run on your worker nodes so why should you consider costs well it has a couple of nice security properties cost is based off of chromium OS Google maintains all of these components is able to rebuild from source if a new vulnerability is discovered and needs to be patched often allowing us to make a patch available sooner than if using a third party distribution cost is minimal and has a smaller attack surface qasas purpose-built to run containers and so has a smaller footprint than other os's this means less code less complexity and therefore less patching right there's less stuff to patch if you never had the component in the first place cost is hardened with good security defaults I really want to emphasize this this is very critical for some of the recent vulnerabilities we've had like run C these were prevented but preventable because of cost of security properties a firewall restricts all TCP UDP traffic except SSH on port 22 and load pin prevents kernel modules from being installed a read-only root filesystem and verified boot makes persistence for attackers more difficult cos also allows security features like set comm to further constrain your application to its normal behavior and EB PF for third party security products to monitor lastly cost offers automatic updates so in gke we provide cost updates on a weekly basis as part of a normal release process your nodes will automatically download the updates in the background and then upgrade when you reboot so we've been talking about notes and how your primary responsibility for these nodes to protect them is to update them right to make sure you benefit from the hard work that Google does to patch the pieces running on your nodes what if even that you could just hand over to Google and that's exactly what no dotto upgrade is it's funny we I think we talked a lot about no-doubter upgrade as being a reliability feature but I entirely think about it as being a security feature right node upgrades help keep your nodes up to date with the latest stable version of kubernetes this means you stay up to date with feature changes but it also means that Google can apply the security patch to your notes when it's critical note auto upgrade is available for costs and also more recently for Ubuntu so you have no reason not to turn it on so I mentioned that note Auto keeps you in the latest stable patch how does this work if there's like a newly disclosed security vulnerability well a patch with the vulnerability fix is rolled into the first possible release after public disclosure often this this is a patch that's released the day of a disclosure but then takes multiple days to actually roll out and be available everywhere then several weeks later once a release is considered stable it's applied to the whole fleet for some security patches where it's particularly critical this is actually done much sooner so why would you not turn on auto-grade well you might need deeper validation of an update yourself or want additional control and predictability for your SRE team to be on call if this is the case consider turning on auto upgrade with a maintenance window so that you'll only apply it will only apply it during that time period we also see users turn off node or upgrade around major launches or events like Black Friday where changes are particularly sensitive to their environment note auto upgrade works by creating a new node and moving the workload over to that node let me quickly explain how surge auto upgrade works which are newer developments to note auto upgrade first we create a new surge VM and once it's up and running create the node on that VM that takes a few minutes once the new node is ready the old node is drained of the workload and once it's fully drained the workload resumes on the new node training the old node takes a few seconds and with the workload up and running on the new node we can now delete the old VM so we heard in the keynote this morning talk about live migration right this is applying the same concept for your workloads for your nodes running in your environment live migration also underlies all of gke so you get that automatically anyways and because of the security benefits and ease of maintenance we're slowly moving to making auto upgrade the default for new clusters in gke so four clusters created in the UI since about October auto upgrade is checked by default four clusters created with G cloud starting for G cloud beta in 113 you longer have to pass the enable auto upgrade flag if you don't want to if you if you don't sorry if you to enable this if you don't want to use this you can pass the - - no enable auto upgrade flag and it's not yet the default in the API though so be careful if you're an opt-in if you're primarily using the API in all of these cases though you can still opt out by unchecking a box or passing a flight great let's talk about workload security so what we've been talking about so far is the underlying infrastructure that runs your workloads but of course you still have to worry about the workloads themselves and application security and other aspects of your workloads protecting that is your responsibility not googles so when we talk about hardening your workloads there's really two places you need to look to secure them your kubernetes configs that pertain to your workloads and obviously the workloads themselves so last year my and I presented a roadmap for securing your organization on kubernetes as amateurs some of the things are handled automatically by gke and some aren't last year specifically in terms of kubernetes configs we talked about a few of these things not all of them things like using a network policy using namespaces and setting a pod security policy and one that we didn't go into in detail last year is protecting secrets so we'll talk a little bit more about that today in terms of additional controls you can put in place for your application you can verify binaries scan images for known vulnerabilities and you sandboxing to learn more about all of this stuff check out that talk from last year it's called kubernetes for enterprise security requirements and for an up-to-date list of best practices we recommend checking out the hardening or cluster security guide this year we wanted to focus on a few of those areas we didn't get to dive into last year like I mentioned such as protecting secrets so what is a secret a secret can be a lot of different things technically it's any sensitive data that your application needs either at build time or a run time some common examples are TLS private keys API keys username and password combinations to access your database for example or external services like your payment's processors such as a stripe API key things that are not considered secrets are your code and general configuration files so let's say you're using nginx as your web server the nginx config file is not a secret but the private key the TLS private key that's referenced by that config is a secret it's also worth calling out that there is sensitive data or personally identifiable information and PII these things are not considered quote-unquote secrets even though they're obviously sensitive data and you still want to protect that sensitive data or PII but generally you want to do that by encrypting it before it's you know written to the database for example and you wouldn't typically store that type of sensitive data directly inside your secrets management system so by default in kubernetes secrets are stored in that storage layer a net CD but they have no additional protection they're protected in the same way as the rest of that CD so a pod can access secrets via the filesystem either as an environment variable or via an API call and effectively all nodes can access all secrets and users with access to add CD can XML trade secrets in plain text starting in kubernetes 1.7 kubernetes offers application layer encryption for differential protection of secrets using this feature your secrets are encrypted with a key that's specified in the local @ CD secret config but this option is actually not recommended because it doesn't actually improve your security if you think about your threat model it's actually the same exact threat model as it was before since the critical data that an attacker would access is being stored on the same place as the key so it doesn't really help you starting in kubernetes 1.10 kubernetes offers something called envelope encryption of secrets using a kms provider we're not going to go into a lot of detail here about a quick summary of how it works is that there are two keys and then the data that's actually being encrypted so in this case the data being encrypted obviously is the kubernetes secrets that are stored with a net CD key number one is called the key encryption key or kek the only purpose of that key is to encrypt the and key the second key is called the data encryption key and as you could probably guess it's only purpose is to encrypt the data for more details on how all this stuff works check out the talk by Seth Vargo and Alex Janikowski called securing kubernetes secrets it's worth pointing out that on Google cloud we've used this functionality that's from from kubernetes natively into gke so we support this with a feature called application layer secrets encryption for gke which is in beta you can use a keen cloud KMS to encrypt the data encryption key that encrypts your secrets in gke it addresses the encryption requirements that a lot of users have for those secrets and provides that separation of duties between who manages the secrets and your clusters and who manages keys so at policemen we currently use hashing Corp volt for secrets management this is because we currently run a hybrid environment what I call a hybrid environment where some of our application runs and VMs and some runs in containers so because of that we rely on something called the GCP authentication plugin for hash a core vault to bootstrap secrets onto new instances if you're also running that kind of hybrid hello okay so if you're also running a hybrid type of cloud infrastructure where you have both instances or VMs as well as kubernetes pods or collections of containers then vault is actually a great solution for that you do have that that challenge though of how do you you know safely bootstrap those secrets onto new instances and pods and that's where things really get tricky so make sure you're safely bootstrapping those secrets you want to use the GCP off method for your VMs with vault and you want to use the kubernetes off method to bootstrap secrets onto your pods with kubernetes and of course regardless you want to be really careful with your ACLs to make sure that you're scoping the access to your secrets to only the pods or only the VMS that actually need access to those secrets right I will actually say though if you're starting fresh today and you don't have you know kind of legacy reasons for rolling your own kubernetes cluster I highly recommend just going with gke and using encrypted secrets if you want to learn more on that topic check out the talked keyless entry securely accessing GCP services from kubernetes that talk is tomorrow Wednesday April 10th from 2:00 10:00 to 3:00 p.m. awesome so other ways we can help with your workload security you know Google offers a few products and features to make this easier first manage based images and disrelish images so I was talking earlier a lot about the node image the node OS this is about what's running inside your container right you're gonna have a base image that you build your container on top of and you build that into an application image and that's actually what you deploy so your your container may just take it by is built by taking that that operating system base images and adding the the the libraries the packages the binaries that you actually need manage base images is our one option and these are images that are patched for security vulnerabilities automatically by Google with the most recently available patches from upstream Google maintains several base images for building its own applications including things like App Engine and so manage base images actually have a couple of nice security properties as well then make them a desirable choice for your applications specifically there regularly scanned for known vulnerabilities from the CVE database the scanning is done using the same functionality as container registry vulnerability scanning if you've seen that before and we're patched for these exists it's applied they're built reproducibly so that there is a verifiable path from source code to the binary that means that the image can be verified to ensure that no flaws have been introduced by comparing it to source and lastly they're stored on google cloud so you can pull them directly from your environment without having to traverse networks you can pull these images using private Google access managed base images are available for Debian Ubuntu and CentOS another option an alternative to menace base images are destroy less images these images only contain your application and its runtime dependencies greatly reducing the potential surface of attack write a package with a newly discovered vulnerability can't affect you if you don't actually have the package in the first place right this is how you become more secure less stuff dis for those images remove package managers shells or other programs that you might find in a standard Linux distribution so that you're focusing on what's actually important which is you know dealing with the the vulnerability scanner results that you have and leaving it was less to maintain this rule is images are a language focused and available for many languages including Java Python node.js net and more as well as a static image for statically compiled languages like go rest in D both distress images and manage base images are good choices for your containers it really depends on what works in your environment if you need a full Linux distribution including features like a package manager or a shell then manage base images are a good choice if you want the most lockdown option possible then distres distress images might be better for you so now that you've have a base image and you've actually built your container image you have a something that you're storing in your in your image registry that you're going to deploy so for those application images you'll also want to avoid including any known vulnerabilities and those to assess that in Google container registry you can use vulnerability scanning GCR vulnerability scanning when enabled scans all the images in your private registry for known CVEs from the CVE database this is displayed in your registry with info as to whether a fix is available and the severity of the vulnerability etc so that you can actually take action this scan is conducted on a new image when it's added to the registry as well as when new for existing images when a new vulnerability is discovered and added to the database GCR vulnerability scanning is available for Debian Ubuntu CentOS and Alpine images and at the end of the day if you're responding to something happening in your environment you can look up at called security scanner sorry that made no sense you can also use cloud security scanner to scan for common web application vulnerabilities unlike for other services security scanner sorry like for other services security scanner can be used on public facing applications hosted on gke it scans for cross-site scripting mixed content outdated libraries clear text passwords and other common like OS top ten like bones so now we've protected our infrastructure right the control plane the worker nodes and your workloads themselves and unfortunately insecurity bad things still happen so what do you do what do you when you have to respond to a security incident now it's also worth noting I'm making a difference here between incidents and vulnerabilities right a new vulnerability if it's not currently being exploited in your environment is not an incident you should have that handle by your normal vulnerability assessment response team so responding to attacks on Google's infrastructure is Google's responsibility whereas responding to attacks on user workloads is the users responsibility basically we're each responsible for responding to attacks pieces that we protect that makes sense so how do you handle Incident Response so before we answer that question you know what we have to define what does that actually mean being responsible for you know incident response on your workloads well it really means to start you need to understand the incident response lifecycle and so let's just quickly go over those stages that means being able to identify when an incident actually occurs establishing a runbook for what happens once an incident is declared staffing your own response team who will actually be following that runbook in the case of an incident understanding the scope of the incident by conducting forensics and finally notifying any affected users if that's applicable and developing and executing on a remediation plan so if you're responding to an incident and you're on Google cloud there's a few tools that you can leverage with gke to Kentucky to help conduct your own incident response first up is absolutely critical you need to make sure you have a centralized log aggregation system set up on Google cloud stack driver could help with this it comes prepackaged with common queries that are helpful for addressing incidents specifically that occur on gke so for example in the case of a cluster compromised you might want to quickly check what resources were asked accessed by the attacker and you can leverage those types of built-in queries instead of spending valuable minutes on writing those queries from scratch yourself and next stackdriver Incident Response and management is a new product from Google that's an alpha it's a set of new tooling that they're specifically developing to help you reduce time to mitigation in an incident response scenario on Google Cloud if you're interested in more material on incident response including how Google themselves uses a really awesome open-source tool called ger internally I highly recommend a talk from last year's Google cloud next called cloud forensics 101 and if you're a start-up that's just getting started with incident response there's a really great starting point in the form of a blog post by someone named Brian McKeon it's titled an incident response plan for startups and you can find the link at the end of this talk finally I do want to call out Brian's not a Googler he's formerly a director of Incident Response of Facebook as well as director of security at corn base and he currently has his own consulting firm called r10 and security so if you are responding to an incident GCP then you probably want it's a cloud security command center of first cloud security command center is the single pane of glass for security events in your environment for scans that you run in your environment you can see the results that apply via GK clusters here as you're looking to further protect yourself from potential container specific attacks Google cloud already has a range of container security partners integrated with cloud CSEC these look for attacks against your container workloads and then alert you directly natively in CSEC these include aqua security capsule 8 stock rocks Cystic secure and twist lock and we're always looking at Anwar to our partner offering so we've talked a lot about a you know a handful of security features that you can leverage in Google Chrome earnings engine and Google cloud this actually lots more gke also offers features like private clusters and pod security policy to help you protect your workloads another one that I didn't talk about I just want to quickly emphasize is binary authorization which is inspired by what Google does internally to verify code before it's deployed so you can actually use that to enforce what gets the put into your environment based on an attestation so for example to enforce that code must come out of your build pipeline or that it must be scanned by your vulnerability scanner using these pieces together it leads to a stronger security posture because after all strong security is really about defense-in-depth so putting it all together you know let's answer that initial question of who protect what in gke google is responsible for protecting the control plane which includes your master vm @ çd and controllers and you're responsible for protecting your worker notes including deploying patches to the OS runtime in kubernetes components and of course securing your own workload so what could you do today to start securing Cabernets on gke we'd recommend that you stay at the forefront of security by using node auto upgrade protecting your workload from common image and application vulnerabilities using managed base images or destroy those images and GCR valen scanning and following the Korean engine hardening guide to get on the latest set of configurations that we'd recommend and then you have a bit more time check out some of the additional content that we covered in additional documents we link to so here's a set of links to learn more the first one is a blog post that covers the content of today's talk there's a landing page on container security a link to an overview on security the hardening guide and the security Bolton's that's what I mentioned earlier when there's a new vulnerability disclosed that's where we post information has an RSS feed you can use that to you know post directly to your team / channel for example and then the last two points there are incident response white paper and our infrastructure security design white paper and if you're next the rest of this week check out some of the other talks on container security going on so we're in the upper left here but there's a talk later today on secrets and on networking and then tomorrow on securely accessing GCP services from kubernetes Thursday on software supply chain policy management and G wiser that's it for us thank you so much for your time [Music] 