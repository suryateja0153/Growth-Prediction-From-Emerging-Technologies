 first presentation uh is alzheimer's disease  drug discovery center data sharing platform   uh presented uh live by uh rob  quick so take it away please rob i need to i need to unmute myself  i guess can you hear me okay   we can hear you just fine yep okay great you're  seeing my slide and not my my notes hopefully   um correct so good afternoon um my name is rob  i'm going to talk to you about the alzheimer's   disease drug discovery center and the open  access data sharing portal that is part of that   but before i start i hope i learned something  from yesterday's plenary and i want to give   kind of a big overview statement about what i  think the alzheimer's disease drug discovery   uh center and this oads portion of it really  means so i provided a lot of effort in the last   almost 20 years to various different academic  research computing projects some of that work   was with projects like ligo or the atlas of cms  detectors at cern and they've led to massively   important discoveries and even some nobel  prizes there however a few of the projects   that i've worked on really have the potential for  the immediate real world impact like the gateway   project that the cyber infrastructure integration  research center is working on now with the iu   school of medicine for sharing drug discovery  research for the out for alzheimer's disease so   getting into this um a note before we start about  alzheimer's disease if this is something people   are already familiar with please bear with me but  i thought i should say something about the disease   and i saw there were about 20 people in the room  statistics say that about 1 in 10 adults over 65   will have some form of alzheimer's and dementia  about one in three adults over uh 85 so if there's   20 people in the room that means two of them  will likely suffer from alzheimer's by 65 and   um six or seven by the six or seven  of the 20 people in the room by   the time they reach 85. so um now there's some  i i can i think assume this is an educated group   um and there is some decrease in risk with  levels of education for various reasons however   the ad and i'll say ad for  alzheimer's disease quite a bit   but the alzheimer's disease landscape is is a  complicated and there are no known biomarkers for   early detection and there's also a high failure  rate in drug discovery programs that have been   attempted so far in fact just down the road uh  some of my neighbors work for lilly and the drug   discovery uh program for 80 has been discontinued  there um or at least uh the one the portion that   my neighbors have worked for so um and as everyone  can can picture if they haven't uh known someone   with alzheimer's disease it's it's really a  heartbreaking disease that destroys memory   skills it destroys thinking skills and eventually  the ability to carry out uh daily activities so   let me talk about 83c my goal in submitting  this paper was really to put this project   together in my own head more than it was to  share it with a larger audience but one way   to put a project together in your own head is to  prepare to talk about it with a larger audience   so let's talk about the primary goals of  the project overall it's a five-year project   uh funded by the national uh initiative on health  or nih um and the the uh award number is on the   first slide if anybody's interested in that we're  starting year number two of five years and the   goals are to identify uh promising a.d drugs and  create a portfolio of targets for further research   to allow medicinal chemists and structural  biologists and other researchers to research the   property of these drugs and validate the targets  in the end providing something we call a target   enablement package that can move on for additional  testing to pre-clinical or clinical phases   third to identify molecules that have a  pharmacological or biological activity likely   to be therapeutically useful these are called  lead molecules and then to share the findings   openly and really that last step is where the  circ at indiana university comes in into play so   um sharing the stadium openly along with  facilitating access to computational research   is what we're trying to do here  and this is done in something   and i don't know if you can see my cursor  called the oads or the open access data system   most data though not all of it will come from  open source repositories things like temple or   pubchem or the ad knowledge portal and this  will be analyzed by chemist biologists and   other informatics researchers once that research  is completed it will be pushed to these target   enablement packages that i mentioned and the ads  itself will then have both an interface for these   target enablement packages to be pulled from  but it'll also push to other ad research portals   these things are are the treat id portal the  amp id portal in synapse.org so it'll it'll not   only serve up the results it will push them to  other important databases for consumption there and really i'll go back to that picture the  circ is working here in the bcb area along with   the open access data system in developing that  and bcb is the bioinformatics and computational   biology core there's stuff that uh is not  necessarily done as the core research by the   uh chemists and biologists and and collaborative  researchers that is outside the traditional   drug design or drug discovery process so the  bioinformatics and computational biology core has   three main goals uh designing and running the  infrastructure for the ad3c project and i should   say cyber infrastructure there not  necessarily the other infrastructure   looking for new new methods for predicting  and prioritizing these target drugs and   finally providing bioinformatics support and as it  turns out other general it support to the project so the components as i saw him and sitting back  and just kind of thinking about what the main   components were of the portal and the project as  we were trying to develop them are one software   that either a researcher contributed or is is open  source needs to be brought into the portal um the   data from both local and publicly hosted data sets  the computational resources resources for running   analysis and services that we're running currently  on exceed and then some iu resources but also   have our persistent services hosted on jetstream  and then a ui for the bcb portal itself which   we're using the apache area bada middleware stack  and a repository uh where interested researchers   can pull the data outputs and then a handful of  public repositories would push the data outputs   too and so these are are kind of the core  uh structural or architectural components um let's see and so we we got that down and i  got that pretty straight in my head and and   i i think uh um we had some ideas as to what is  what to do but but wait um the the group that was   mostly made up of the medicinal chemists but also  the structural biology guys came and said well uh   there's this drug discovery environment called  the cdd vault that we would really like to to   use as our work area for doing common drug  discovery type research and this this cdd vault   is a commercial project a commercially  provided cloud hosted environment and   it really is a collaborative  environment for the researchers to   do those first steps of both pulling the open  source data and doing some initial analysis so we we kind of had to take a  step back here again and it wasn't   this bcb portal wasn't as long as covering all  the researchers here it was only covering a   a portion of them but we still needed to get  all the stuff from both the bcb portal and the   vendor to database into the uh into the oads  um and so we took a step back um with these   additional complexities um and said you know if  we're if we're looking at a standard computational   gateway we're looking at a data sharing portal  now we're talking about integrating external   commercially hosted cloud services let's take a  step back and really dig into a data management   plan and this is what a lot of the work in year  one of the project went into was was really trying   to say you know what is the data flow where does  the data come from what are the various steps   and where will it be housed at various uh steps  along the uh process and where would in the end be   be delivered to so we stepped  back did a data management plan   and this data management plan provided a lot of  things but really what it provided was a picture   of the data flow through the project so we we  provided this data management plan it defined all   the things that data management plans define their  roles and responsibilities onboarding off-boarding   the sponsor requirements the sharing  requirements quality assurance quality control   and then the the physical where would  it be stored how long it would be stored   how would that storage be organized um and and  also brought in the intellectual property rights   lawyers which as everybody knows once you get  the institutional lawyers involved it goes to a   another level of complexity um and while i like  to think the motivation was really understanding   the data this was also necessary to purchase the  externally hosted software especially something   that has potential for inter intellectual  property to be involved we really needed to   understand this not only for the architects you  know architecturing of future components but also   um so we could feel safe that the data was flowing  through these externally hosted cloud services in   a way that was secure and would not be lost so  and i think the dmp uh was critical not only for   that purchase process but also it's going to be  critical as we design the open access data system   so i'll go to takeaways and it looks like i'm  right on time here so that that's pretty good   so the project became much more complex with the  addition of commercial components there was a lot   of general i.t support the group needed not only  gateway development so we went in thinking this   was a gateway development project it turned out  being a let's help these alzheimer's researchers   many of who were moving from commercial to  academic environments for the first time worked through their i.t issues and so  while some of this fit into the standard   computational gateway i'm i'm pretty positive  that we'll have to have extensions to what is   a traditional gateway for the data sharing  components or the the the external data   sharing components um and we're all early  in the process as i said year one of five   um and i i feel that complexity may  begin that may continue to grow in fact   there's a second commercial project  that is not externally hosted that is   locally hosted i've installed that on jetstream  and we're running local services now along with   the externally hosted services and these all  have to be integrated and talk to each other   and i i want to say if there's one takeaway  from this uh start the dmp uh early in the   process it's critical to design especially  if you're doing something like data sharing   um and and uh integrating with  external external services and   and possibly okay so i had i said i had one take  away from this the dmp two two takeaways from   this um the drug discovery research i think is is  super critical to the future of alzheimer's and   i think uh what we learn here can  also be put towards drug discovery for   other diseases i know there are similar type  portals for many other diseases that are doing   drug discovery and we hope that the lessons that  we learn about alzheimer's are transferable to   other areas where drug discovery is important to  the research that's being done and i'll stop there great thanks rob uh do you want to encourage  anyone if you have any questions drop them in   the chat or you can have to raise your hand um  rob i'll start with i don't know if you got a   chance to see the plenary a few hours ago but it  was all about interoperability of gateways and   i mean as you were going through a presentation  i was just amazed the number of things that you   were having to interoperate with in terms of not  just different server infrastructure stuff you   mentioned jet stream and on-prem stuff but  you you know you said all there's all these   databases there's all these different things so  i don't know if there's more you want to talk   about along that lines did you get if you got  to see it maybe there's some insights you have   from the plenary just want to kind of tee that up  as a question yeah so so interoperability is a a   a great question and interoperability especially  in the data realms is something that's being   tackled by many groups in fact i have a  small nsf grant that looks at general data   interoperability it looks a little less at other  interoperability through systems and such but   so i think that the data interoperability  especially in a a data heavy project like this is   going to be super important how do people get not  only the data but all the associated metadata and   the providence information about what's happening  and what they're pulling from this data portal   so i think that's a super important question  and i think that interoperability is is going   to be one of the major questions  of the next five to 10 years   um i see a question chat that uh it's  kind of an amusing question but i think   we know the answer to it uh did you dmp  yourself or did your funder require it so the funder in both nih and nsf require  a data management plan the data management   plan you submit with a proposal is usually not  sufficient to really have for a data management   plan especially when you're moving data externally  so yes there was a data management plan was   it sufficient to understand and really operate  moving through five years it wasn't and in fact   the dmp should be a living document and  change as the project evolves so it's hard   to submit a dmp up front and say that that's uh  encompassing of everything that you're going to do   okay and we'll get one more quick question in uh  will the project handle uh pii or hipaa will be   hipaa compliant so no there's nothing the the  only restriction on the data so it's not uh um   critically restricted data so there's no  uh personal information in there there is   intellectual property information as i mentioned  during the thing so there was some restriction in   that we can't move the target enablement package  to be public until it is properly approved by   the intellectual property and patent lawyers  so there there's a restriction in the data   but not a restriction in that we're we're  using uh probably 90 of what we'll be using   is actually public data to begin with right  so it's coming from places like kimball and   the pubmed and and such so um there's  nothing there that will identify people   um so there's no need at this point to be  hipaa compliant there is i need to be careful   with the data because um it does have uh legal  ramifications in in ip and um and patent areas   okay well thank you rob uh obviously please  stay because we're going to have a general q a   here at the end but let's move on to our second  presentation uh it's accelerating neuroimaging   research with brain forge uh it's presented  it's actually a pre-recorded presentation   uh by eric vernier but i see eric is on as  well so he'll hopefully be available for   questions at the end and mike do  you want to go ahead and play that hello my name is eric verner and i am from  the translational the center for translational   research in neural imaging and data science  also known as trends here in atlanta georgia   i'm here to introduce brain forge today a  web-based platform for neuroimaging research brainforge is a cloud-enabled web-based  analysis platform for neuroimaging research   brainforge allows you to  archive data from your study   set up the analyses you want to run and  then process data across multiple modalities   when your results are finished you can share  them with other collaborators on on the web the reason we created brain  forge was for multiple reasons   they were divided into four main categories  software issues reproducibility issues   computational resource issues and data sharing  issues now the first is software issues   neural imaging pipelines are quite  complicated and they require lots of advance   lots of advanced software now it's difficult  for a beginning grad student or even a seasoned   neuro imaging researcher to set up some  of these soft these programs sometimes   especially on a on an environment controlled  that they don't have administrative access on   it's it's also difficult because not  a lot of neuro imaging researchers are   excellent programmers either so  they they don't create code that's   maintainable now this leads into reproducibility  issues even if someone can create write great code   for to do a study that does what they need a  lot of times it's not maintainable there will   be hard-coded paths or it'll be difficult  to read or there will be aspects about the   about the environment that are not that are not  encoded and not saved anywhere in the text for   instance the version of software that the person  ran or the operating system that they ran on   now this is difficult when maybe a year or two  down the road someone tries to replicate their   analyses and is unable to next are computational  resource issues now neuroimaging data is quite   vast it could be maybe one maybe one terabyte for  a study of 100 subjects just in the original data   and then maybe 10 times that in derived data  once you have once you have processed the data   now just storing the data is difficult but  also running it can be hard for many people   when you move up to thousands of subjects which  are needed to for an effect size of to capture   small effect sizes many people do not have the  the resources to to run all the processing that is   needed there are cues like exceed where it which  in which you can run data for free but this is not   easily but the queues are very long so it's not  easy for researchers to do their research on time furthermore there's data sharing issues   once someone has written their code and run their  analysis everything often stays on the file system   or the hard drive that on which the analysis  was was originally run now data sharing then if   someone wants to if someone wants to find this  analysis they have to have access to the local   file system now this is it makes this is very hard  when you want to share your results with other researchers i'm grateful should we tackle all of  these issues our software uses pipelines that are   that have been run and tested in many published  studies by dr calhoun and many other researchers   at trends and these are all encapsulated inside  of docker and singularity containers which   essentially which use all of the encapsulate all  of the the operating system and software libraries   and and scripts needed to run the analyses  that researchers actually have to run   so it saves a great deal of time for researchers  who want to pre-process and analyze their subjects   additionally the use of docker containers  lends itself to much better reproducibility   containerization means that you have the exact  software library and the operating system for   all within one file the the docker container  and then if someone else wants to run it   they're able to they can just take that  container and run it on their own files   furthermore we keep track of the the provenance  the parameters used and the and we store in the   database the the file and all the associated  metadata that was used for an analysis   at trends we have great computational  resources we utilize the hpc the high   performance computing cluster at gsu and can tap  into amazon web services to run to run analyses furthermore with our web interface we're  able to we're able to easily share results   with other researchers outside of our own  lab here's an overview of our architecture you can see on the left is the coins architecture  coins is another product another study management   product that trends has created about 10  years ago and is still using today we have   brain forge integrates with trends and  pulls data collected by coins so that   the the data can be processed  inside of brain forge   brainforge has both separate front-end and  back-end code bases and uses a postgres   database we use slurm as the as the executor  on our hpc to run many analyses at once and here's an overview of the  computational resources we have at trends   we have 49 servers over almost 2 500 cores   almost 40 terabytes of memory among all of  these servers and 60 graphical processing units   we also have 1.5 petabytes of network storage  and can tap into unlimited aws resources now here's an overview of the  analyses that we can run on brainforge   we can run analyses in neural  imaging preprocessing we can run   linear models using statistical parametric  the statistical parametric mapping toolbox   we can run analyses in gift the group ica for  fmri toolbox which is another product of the   calhoun lab furthermore we can do statistical  modeling with regression and classification here's a screenshot of the brainforge ui this is  the pre-processing summary brainforge allows you   to select um within a within uh within a study  you'll have many subjects then each subject will   have one or more sessions and which is just a a  period of time that someone went to go get scanned   in an mri scanner and then within those sessions  you'll have multiple acquisitions or series   say a structural acquisition a functional  mri acquisition or a diffusion image what brainforge allows you to do  is associate each one of those   each one of those series with a different pipeline  and set of parameters and allows you to then you   can cue them up and run them for every subject  in your study now you can see for this analysis   that we have or for this study we have vbm  voxel-based morphometry which is a structural mri   analysis queued up for all of the t1w  32-channel npr one millimeter series   and then below that you're able to see the total  number of of sessions that have been completed   and on each line below that gives you another  subject and session so you can see that say   for the top subject that analysis has been set  up but not run the next one below had an error   and the one below that was completed then you can  see the total number of completed 107 out of 261. we also have we also support bids which  is the dominant standard for data data set   dominant data set specification  for neural imaging data and here's an overview of all  the processing we do we can do   vbm we can do functional mri using two different  libraries we can do arterial spin labeling   diffusion tensor imaging and diffusion  kurtosis imaging using fsl which is good for um   finding connectivity um or connectivity  between uh different regions of the brain   then here's all the here's all the toolbox are  all the analyses we do with gift that includes   group ica df and c and mancova also free surfer  and general linear models using spm an overview   of voxel-based morphometry you can see the images  at different steps of processing and in different   tissue types here's fmri pre-processing you can  see the mean image at various states of proc   of pre-processing we also do free surfer  which does a parcelation of the brain   and can do a regression analysis across many  subjects after the initial pre-processing is done   here's arterial spin labeling which also shows  you a 3d view of the of the processing results   here's diffusion tensor imaging which  shows you movement correction over time   and the fraction fractional anisotropy  images registered to a standard space do the gift analyses including  17 variants of group independent   component analysis gift is one of the two  major fm group ica toolboxes in use today   here's an example of group ica here's the montage  plots time courses and spectra for each component   here's spm glm you can see the parts of  the brain that are associated with a task and the design matrix in the future we're going  to do mri qc and meg pipeline historical qa view   allow users to run their own analyses and  further develop our command line interface   we're also going to do jupiter notebooks  to let users do interactive analyses furthermore we have a neural mark  portal which will allow people to do   normative comparisons of their data against  a reference data set of thousands of images   this includes a brain age calculation and  if you want to learn more please contact me   email me at ewerner gsu.edu or find  me on twitter thank you very much bye   okay great eric i think i see you on  um or if you want to unmute yourself there we go uh and encourage everybody hey  eric um we've got a first question is is   the trend gateway based on any existing gateway  platform framework or did you guys write your own no we just wrote our own from um  just software development libraries   so the backend is based on the django uh  the back end is written with the django   rest framework and the front end  is written with react using the ant   ui framework so now we didn't we kind of  wrote it from scratch to a large extent okay um i'll ask one more quick question and  that's you mentioned reproducibility is one of   the important goals and and i heard you mention  that you can share the data at any stage in the   pipeline but what about the actual containers that  have the workflows you know how if somebody wants   to reproduce look at those are those shareable you  know what happens when you update those you can   talk a little bit about reproducibility of that  aspect of it yeah sure okay so for right now the   containers themselves are are being held privately  on docker hub we're probably going to open source   them and share them but we haven't pulled the  trigger on that yet um so we're uh so yeah for   for people who have used our service and who are  paying customers or who are who have just um i   guess sorry the more more accurately people who  have used the you people who are users who run   their data with our platform will we can share  them we can share those containers with them and   then they can um then they can run them they can  see what's going on introspect them and whatnot   so the in terms of like reproducibility and  maintainability um it's it gets pretty um i guess   gets pretty hairy um because what you have to do  is um you know you have to you have to navigate   your way through different um different versions  like patches minor releases and major releases   that that break past functionality so every time  you change the interface right you have to um you   have to change the front end code and the back end  code um and so you have to so you have to create   like uh so you have to find a way to to do that  in a way that's um you can maintain the old stuff   in case people want to rerun it and you also want  to maintain the um and you also do you want to um you want people to be able to select from  different versions of the of an analysis   and so we're just kind of putting together the the  plans for that um in a very like fine-grained way   and uh we'll be we'll be doing that soon  but um currently we just don't have um   we we are still relatively new so  we're mostly start we're we haven't   really had that many revisions on our  on our images that run the analyses okay great well thank you hopefully you're stick  around for the general q a but let's move on   to our third presentation which is uh enabling  secure and effective biomedical data sharing   through cyber infrastructure gateways this  is another recorded uh presentation by tyler   phillips and i know both tyler and sriago are  on to answer questions at the end so mike please   play the video hi i'm tyler phillips and i'm here  presenting with chile gold and we're going over   our paper enabling secure and effective biomedical  data sharing through cyber infrastructure gateways   and worthless iupui or indiana university purdue  university indianapolis and indianapolis indiana so first i'll go over the motivation and introduce  our project so the motivation of local logic   so we have this jet stream uh cloud computing  service uh funded by the nsf and it's widely   used by indiana healthcare researchers to enhance  collaborative research so the type of data that   they're dealing with there is you know healthcare  data related to like uh policemen and firefighters   in the indianapolis area so although it's widely  used the jet stream security features do not   satisfy the requirements involving sensitive  healthcare data and protected health information   so this drives the need and motivation for  securing the cyber influence structures   that jet stream uses through an  easy to use extensible and robust   cyber security architecture that can protect  sensitive information being shared you know for   these scientific collaborations so here i'll turn  it over to schleyer to give us the introduction   okay so the aim of the dinosaur project is to  develop integrated and trustworthy holistic   secure workflow protection architecture that  for cyber infrastructure such as jet stream   uh the proposed cyber security architecture  supports rules role in data hierarchies as well   as the dynamic changes of road and hierarchical  relations within the scientific infrastructure   so uh now we'll provide a overview of our  project so currently uh dynastop works on   openmrs openmrs is a community developed open  source electronic medical record framework   that is used for exchanging healthcare information  so this system is based on open standards such   as hl7 for achieving interoperability with other  systems while transferring or exchanging patient   data so as mentioned currently dinosaur project  works with the openmrs database but however   in the future this can be applied to other  databases so the project runs on an openmrs   image that is a template of virtual machine with  pre-configured security uh techniques in it that   will allow the users to securely access  information stored in this database so in   order to address our motivation of securing this  openmls image and allowing resources to use it we   add some security components into the image which  then researchers can deploy through the jet stream   tool so the some of the security components  we embed into the opening glass image here is   biometric capsule based uh privacy preserving  facial authentication which uh we'll see but   it also includes two-factor authentication we  include dual-level key management which is a   cryptography-based hierarchical access control  model and we include secure digital provenance   so i'll go over each of these in detail later in  the presentation but again these are the security   components for baking into the openmls image  okay so as mentioned earlier jitstream is a cloud   computing environment that is supported by nsf for  collaborative research so it is designed to allow   to allow the researchers to get access to  various computing and data analysis resources   so jet stream allows the user to get access to  virtual machine images pre-configured with an   operating system in software that allows to do  scientific computations in domain specific tasks   so openmrs image will be built with all the  security features and components implemented in it   so that when users will deploy a vm with our image  it will allow the users to remotely access and   share sensitive data information to perform their  research so these are some of the steps on how   we can deploy a jet stream vm using openmrs image  so first of all you need to create a create your   exceed account so once you create that so it  will ask you to enter your username and password   and once you are authenticated you'll see  this jetstream dashboard that allows you to   launch a new instance of vm so there you can click  on the images options and then type our image   that is openmrs and you'll see the openmrs base  image the latest version is 1.3 and then you can   hit on the launch button that will allow you to  deploy a vm with this image so now that we know uh   you know the overall architecture of the platform  i'll talk about some of the security components   that we're adding into the openmls image that will  allow the researchers to collaborate you know more   easily and securely so the first aspect is in the  current open mls image the only authentication   method or scheme that they that you're allowed  to use is uh you know like a username password   login but here we've added facial authentication  so due to the proliferation of biometric sensors   and smart devices uh biometric authentication  is recently enlarged as a promising solution   to the lost theft and usability issue based on  traditional authenticated authentication methods   so we talked to some nurses and healthcare uh  you know um professionals in the indianapolis   area and they said that within this open mls  uh you know image it'd be much more helpful to   have you know biometric authentication which  is more useful for them on the job but one   issue facing you know biometric authentication  is that you know if your biometric template or   you know facial attributes are stolen the you  know the feature vector which represents that   it's a lot hard to cancel and really set that up  you know compared to how you would a password so   we need to secure those biometric templates used  for the facial authentication and that's where   this biometric capsule scheme from 2019 comes  in how it works is it fuses the biometrics or   that feature vector of the user with the left-lens  subject in order to conceal the user's biometrics   so if you look at the bottom here this workflow  you'll notice that you know you have the typical   biometric pipeline here you take a user signal  you know by taking a picture of them you sample   and pre-process that image you extract features  and then you would perform some classification but   you'll notice in the biometric uh pipeline here  it also includes that relevant subject or less   workflow so each user is assigned an ls  and each time the biometrics are sampled   and used for authentication the biometrics are  fused with the biometrics of that electron subject   to conceal that user's biometrics so there's a  few advantages of this one we can just make the   rs determine based on the user's role within the  organization so then at authentication time a user   needs to only provide their username provide the  rule within the organization to determine the ls   and then allow you know the picture to be taken  to provide a facial biometrics to the system it   should be noted although we're using the secure  facial authentication scheme here by fusing the   biometrics we're still able to leverage state  of the art deep learning technologies during   this facial authentication so if we think about  um next moving on to access control data access   control operation access control you know with  the sensitive health care data um you know in you   know in the existing scheme and in many you know  health care domains uh organizations typically uh   set up the access control policies which users  have access to which corresponding data based   on a rule-based access control model so what  the rule-based access control model is pretty   straightforward you just define which users have  access to which data but unfortunately you know   there's no security component that's baked into  that rule-based access control model so there's   nothing to you know enforce your access control  policies so that's why we've upgraded the openmls   access control model to a cryptography based  access control model where there's cryptographic   keys given to each user or set of group of users  like nurses and doctors to actually explicitly   enforce those access control policies which we  define and it raises a few key issues such as   hierarchical model design how to generate that  model how to manage those cryptographic keys   so we use this 2019 ieee tps paper to carry that  out and what that ends up being from the users   perspective in an open mls image is that you  know some manager or you know uh admin within a   hospital organization using this openmls image can  you know how perform a user friendly or generate   a user-friendly definition of different rules  and the data object leverages to the left here   and then we have a model generation algorithm  which automatically generates all the keys to   to generate that access control model and enforce  those um you know access policies which we define   so the last part of this which is still ongoing  is where um adding a secure digital provenance   component into the openmls image so this is based  on a 2014 paper and what this is sort of similar   to is blockchain technology if you're familiar  with that so useless utilize several cryptographic   primitives including cryptographic hash functions  private publicly key cryptography schemes and a   digital signatures in order to generate a hash  chain so what this hash stream does is it records   the ownership um in a of a digital object that  digital objects current state any modifications   performed upon that digital object as well as  you know when a user is trying to add uh access   to that digital object so this will be valuable to  you know integrate into the database component of   the openmls image and this is an ongoing walk as  i mentioned okay so next is a usability experiment   so once the biocapsule authentication system was  implemented we did a usability testing to see how   it works so uh we asked the students from privacy  and security course to perform some of the tasks   which is mentioned in the next slide and then send  out a survey to them to answer questions regarding   the three authentication methods used uh so these  were the tasks that the students were asked to   perform first is like logging into openmrs using  three different authentication methods that is   the biometric facial authentication single sign-on  and the username and password method and then they   were asked to perform some of this basic task like  creating patient records and then logging in as   provider as patient with different access and all  those things then uh these were the five variables   on which the questions were based and asked that  is the efficiency and effectiveness satisfaction   preference concerns and confidence for the three  authentication methods so once we got the results   we developed a formative formative partial least  square structured equation modeling to measure the   relationship between the usability and security so  from the results it was found that the bio capsule   facial authentication was the most preferred  usable and secure among the three authentication   methods used so this concludes that our the  dinosaur project allows to build a secure holistic   multi-layer protecting cyber security architecture  that can be ported to multiple virtual machines   on jet stream it allows for uh scientific data  so that researchers and students can remotely   access and share protected data in a convenient  yet finely controlled manner thank you thank you okay well well great um so i know both tyler  and shreya are on so feel free to meet yourself   and anybody has any questions you put them  in group chat i'll start by saying i think   it's super cool that you all are reporting  actually you did and reporting on a usability   study because i think that's one of the things  that in the community we don't talk about enough   we i know we don't perform them enough uh because  you know they that's really important the the end   usability so i guess i'll just heat up as to  is there anything else you want to say about   usability study i don't know maybe did you  look at any type of visual accessibility type   aspects of it or anything like that yeah so i  think um the the main motivation is to um you know   get resources more flexibility in terms of you  know the security components that they're using   if they're using this openmls dimensional research  so uh you know we can add all the security   components you know under the sun but at the  end of the day it used to be usable so uh we   have a couple different experiments underway  you know primarily led by schleyer here but um   they're just uh getting feedback from students  and these students are both you know taking   our uh security courses at our university as  well as students studying you know uh you know   more things more on the health care side of  things so we can get both of those perspectives   so um the advantage of the biometric uh you know  authentication that we found is you know once you   get that it's kind of comfortable somewhat first  you know getting it all registered in the system   with your face and stuff but once you get past  that initial point a lot of people before that   the username and password because you know most  phones and smart devices nowadays have the um   you know the biometric sensor  or embedded camera you know   in them so you know you have that flexibility  to use that you know a version of the image great um we do have another question  chad and it is is the data actually   in the vm along with the tools uh so at  this stage we're just using uh fake data   you know just to make sure all the tools are  in place but you know we're on the last year   of this uh nsfc ici project so now we're  moving on to you know this uh experiment   the you know more experimental phase pass  development where we're getting this usability   feedback and that is you know you know um part  of our plan over the next you know final year   of this project is to start trying to you know  integrate real data and see you know how our   intended audience you know these researchers  to receive uh you know or uh a final adapter okay well i want to open it up now to kind of  a general q a for everybody so so thank you for   that tyler um and i guess so again if anybody has  any questions feel free to drop them into the chat   um but i i want to start with uh i think as i was  listening to all three presentations you all had   comments in some form or fashion about how the  target communities are not naturally inclined to   using computational tools or using you know these  types of resources so i i'd like to start with   saying if they're not naturally inclined what do  you do for awareness just to how do you make your   your target community aware of these tools and the  fact that they can you know do their jobs better   faster quicker whatever as a result of this um so  why don't we uh maps anybody jumping in why would   we just kind of go down an order i see rob you're  smirking so i'm gonna pick on you to answer first   sure so so um luckily we had um a target  community at least the target for the   the uh the portal of the researchers the medicinal  chemists and the structural biologists who had an   idea of what they needed to do drug discovery they  had done it before in the past and they they came   to us relatively quickly after reviewing and said  hey this is not what we're we're used to working   in we're going to need you to integrate  with something else and they brought the   product that they were comfortable with to the  project um and we you know had it in the budget   to do that so it was it was not too bad now the  open access data sharing portal is again still   being developed and how people use that i think  is one of the things we're doing there is is both   providing a place to pull the data and pushing  the data out to places that the community is   familiar with already right so um they may not  be familiar with what we develop as the oads   but they may already be using the ad knowledge  portal or synapse.org or something like that so   part of that is just putting it where they're  used to so they don't have to learn another system   and i think that's a a advantage we have in  oads is that we have other systems that that the   research community is used to using and we should  be able to create apis to get that data to where   it to a familiar state that they're  they're used to dealing with it okay eric anything you want to share or comment on maybe um see do you have uh maybe if you could cue  me with a more um specific well how how are you   how how are you i mean you purchase a variety  of of angles but let's just do this how are   you marketing this how are you communicating to  the neuroimaging community uh about brain forge   and making them aware of the fact that it's  there oh right um yeah so right now a lot of   um our first customer is really ourselves so we're  um we're running our own data on it and we have   we have contracts with other pis and research  centers to to um to analyze their data but we have   so we used to just do all this manually like you  know with shell scripts and whatnot so we created   a web interface to make it more um you know just  to create more of a an accounting trail you know   an audit trail or whatever you know so you can  see what happened and also to run it more easily   so in terms of so really right  now we're at like word of mouth   and i think part of the marketing really is  just showing up to this conference but also um   but but also like writing papers um  writing uh showing up to like conferences   things like that um we haven't really started  a full-scale marketing campaign but um   i think we're we're we're still somewhat  in like beta mode and i think once we're   we feel like we're really solid then we're  gonna do get more heavily into marketing it okay great tyler how about you all anything more  sure yeah i think um so my professor uh sort   of the motivation of the business we've been  consulting with some like interested parties   here in the indianapolis indiana area as we go  here and i think uh but you know for my part on   my security privacy perspective here you need  to you know make sure especially in you know   the healthcare domain that all the necessary  you know security and privacy components are   there but you also want to give the user the you  know the flexibility to fine-tune things to the   you know so um the the access  control component of our   project is definitely that as well as  you know the usability of the facial authentication uh well i'll do one more question kind of around  robin i'm not seeing any other questions in the in   the chat um you all talked an awful lot about  the technical side of this but i think as we   all know particularly when you're dealing and  there's some questions here about phi and hipaa   and things like that but you know when you're  dealing with any type of medical data there's   contractual regulatory type things  as well so i'm just curious if   anything in your experience going through this  really surprised you or jumped out or any real   big lessons learned on that side not necessarily  the techno side but on the on the regulatory legal   however you want to phrase it side of what  you're dealing with so um so let's rob go ahead   sure so i think the thing uh that actually  surprised me is that when we found out that we   you know i went into this thinking it was going  to be clinical data from alzheimer's disease   patients and so it would be subject to a lot of  the hipaa and pii restrictions which it turns out   that it wasn't so i i almost had the opposite uh  rather than tightening um i had a loose inning   when i found out that it was only intellectual  property issues and so we were able to make   that commercial connection a lot easier once we  realized that there was no pii going off campus   right or going to a a cloud service um now as i  said it didn't become more complex even with the   the intellectual property in getting the  lawyers involved and having them look at the   i looked through the data management plan but  the the uh advantage here is that that good   data management and made that process go much  smoother than i think it would have without it   great eric anything to surprise you on kind  of the regulatory business requirements side   well we um yeah we have a lot of experience  dealing with hipaa data so right now   we one of the prerequisites for any data we  accept is that it doesn't include any phi   so we um we pull data from coins which does  hold phi but it's anonymized before we pull it   um so we um and then for other customers what  we're going we just tell them to anonymize it   ahead of time but i think what we're also we  also have planned on our roadmap um just a phi   flagging script or routine that will help  us determine if there is some phi like   in the headers and various files you  can include it's possible to include phi   so we'll um sift through all those and warn  someone if something does contain phi and   then i think it's also important we haven't had a  deal with this yet but it's it's good to consider   that um just to have like disclaimers on  your website that say you know this is   not to be used as a clinical tool um just  so you don't get in any trouble with the   fda um or give any anybody the wrong idea and  also i think it's good to have disclaimers like um   you know just that limit your legal  liability a bit just say like you know your data you know it's we're not perfect you  know it's maybe possible that your data may get   lost or something may happen so you  know just be be aware that this uh   you know that you take some risks by using our  platform i mean you just have to i think yeah yeah make sure you don't claim to  cure hair loss or something you know uh okay and we'll we'll give the final word  to either tyler or sria whoever wants to speak   anything that surprised you or lessons learned on  your side um during the development phase we've   been working with primarily like anonymized or  you know fake data so um we haven't really run   up against that yet but and also i've been pretty  insulated from that because my uh professor and   the professor was working on this project and  you know taking care of that so but yeah um come   you know uh before this party this is my first  project working kind of you know even in the   healthcare domain i'm usually doing most of my  work in the financial domain so this is kind of   you know blown my mind seeing you know everything  that goes into you know securing health care data   preserving people's privacy so i've learned  a lot to this project but i don't know   i have any you know specific comments  on the you know legal side or anything okay well we're at the end of the hour here so  uh thank you for all three of the presentations   this was really interesting um  it is the last session of the day   so uh enjoy the rest of your your day and evening  or afternoon depending on what time zone you're in   uh we will have the next plenary tomorrow  it's titled usability uh featuring uh   paul parsons and uh hiram uh i'm gonna get this  wrong kierkendall i think is how he pronounced his   name i know he was on here earlier that's going  to be at 1 pm eastern 10 a.m pacific tomorrow   so thank you again all feel free to  jump into any of the other uh kind of   social sessions or anything that's that's on  kiko chat and have a great rest of your day 