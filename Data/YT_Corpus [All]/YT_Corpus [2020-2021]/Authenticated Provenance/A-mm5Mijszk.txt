 Hi, I'm Julia. I work at the Library of Congress and this morning we're gonna have a little bit of a show-and-tell We'll see how this goes so well the resources like QC tools and the artifact atlas have gone a very long way for creating a shared and growing knowledge base How can we help non specialized staff or staff with only intermittent work with these types of workflows. And I count myself as one of those types of people And what of born-digital workflows or flows with unclear chains of custody. What is an artifact? And what is business as usual? With digital files conversations invariably focus on levels of fixity at the frame and file level but what are some of the less obvious errors that we can try to look for and understand collectively today. And how can you have our colleagues... and how have some of our colleagues work through some of these obvious errors? Today for the first part of the session. We'll look through some errors in a show and tell that's open to audience participation So some of these files are not very pleasant to listen to or watch We'll just see how it goes. First this is the audio if you have sensitivity to audio, you might not want to a... It's not mirrored. It's okay. This one's mostly audio, but for the latter ones Yeah Okay Could we get the audio on it? One sec. Sorry. We'll move on to the next one that's the only one with audio but in any case what you see on this next screen Are some of the visual errors. So in this case, this was the collection of born-digital a QuickTime ProRes 422 HQ files Where the checksums matched and yet later through various batch ingest and transcoding processes major errors like the ones that we can see right here occurred And to give a bit more background these files were created on the go. Throughout the country. Over a long period of time. So many hard drives. Many people. Before eventually getting distributed to their two archival repositories the Library of Congress and Smithsonian NMAAHC. And I think one of the other archivists is here today In extreme cases, approximately four of the files didn't even play. Essential header metadata was missing. According to prompts, that we heard about yesterday and Ashley's presentation, that the moov atom was not found. In other cases, errors were fleeting and virtually undetectable to the naked eye. Some of these errors can be seen easily in the file's truncated metadata and the lack of general technical metadata as seen in the top left. Well, this case has been in some ways great for advocating for earlier checksum creation towards the time of file creation This is still largely. I'm she unachievable two most standard archival donors And even with savvy and willing donor adoption None of them at least with me have been able to create and maintain a change log to document purposeful modifications made to files before or after updating checksums. Leaving us with potentially many other assets with verified files, but flaws like the ones seen today. Blake, one of my project partners, has been attending I don't know if... he might have stepped out of the room and if he is here if you wanted to add anything to this use case. But maybe just start off some discussion on this particular one before we move on. I'm curious given the way the moov atom not found issue is probably very prevalent given the way it's written It's a common error. If other people have had experience with it and have been able to actually fix the moov atom or recover those files in our case We were only able to recover those simply because we had extra copies of those files so there's nothing we could do in that case. And perhaps do people generally as a matter of course rewrite any of these types of files to move that to the beginning with the... I believe there's like a fast start flag. Anybody? In the back. Carl Eugen Hoyos from ffmpeg. In general, you cannot read moov for mp3 file if this item is missing. However, the question is always why it is missing. If the file was actually written on the camera, then this atom is written... rewritten continuously So there is a chance that a copy of an former atom is available within the file and you can point the start of the file to that atom at least theoretically I've seen such cases. So in the general case where for like a tool like ffmpeg transcodes a file and somehow is forced to quit before the very end and cannot write the atom simply because the process is scans is terminated by the operating system then in general there's no way to recover the file if they recover if the file is video only or audio only you generally don't have a problem because then you only have one stream and that you can recover from the file. You don't need the atom. But if they are audio and video interleaved you would have to go basically manually through the file to find out what is audio and what this video. This is a very, very time-consuming effort and It is not guaranteed to succeed in the general case But you're saying if it was a camera raw file that you'd have a higher chance of success because it's continuously being written? If it's if the camera itself writes a QuickTime file then as far as I remember It typically, it does not wait until the end to write this item once but it writes it continuously while writing video and audio data. So there are different copies of the atom within the file and if there's only a problem when writing the last one Then you may have a chance to recover an earlier one to be able to read at least most of the file. I'm not saying this always works, but there's at least a theoretical chance. An indication that this is the case is that you find three atoms within the file usually would make no sense for the camera to write free atoms These are atoms that are just jumped over when reading the file. So if the file contains free atoms several of them then there's a good chance that the last of them is a moov atom that you can use. But this is just like I've seen such file so it can happen. I'm sure this is very prevalent So I'm curious if other people have had experience with trying to struggle with this before we move on to the next use case. Of the moov atoms not found at least or Hi. My name is Gareth Harbord. I work for the Met Police doing video forensics. I've had to repair a few files Like this in the past. I've done it manually. It's a nightmare, but you can do it The starting point you need is working file produced for the same system ideally at the same time with similar encoding characteristics There is some software on the market that does it now. It's paid for software I think one of them might be called Stellar Video and there's there is another one Basically, if you take the donor file, it would do its best to remap it. It's not an easy process, but it can be done Thank you. Unless somebody else has something to add I think we can move on to our next questionable file. Yeah, okay. So we have audio and we need to adjust it somehow Get out some audio from the laptop. Sorry for this. It's noise. I'll just fill it in. It's weird noise Most of the following errors are more visual or at least more compelling as a visual so we can move on. If I could get okay. So that captures a moment of transition that we saw in the video but what happens in this particular subset of files is The expected duration of this this comes from a broadcast This comes from WGBH Boston a broadcast organization The duration is correct but what will happen is it'll go through that spinning motion where it starts splitting apart and then stay on a static image like the one on the right for the rest of the expected 40 minutes or whatever the broadcast capture was meant to be. So it is reading something from metadata. header metadata, I suppose, about the expected duration and size. And in their case It actually went through all the automated QCs. Now it's only through manual you see later on that a staff member noticed this happening, which prompted them to then create a new QC method or they would create thumbnails throughout through hundreds of files to see if at some point in the thumbnails You got that static image for the rest of it. So to give a bit more context. It's born-digital. Broadcast. There were no checksums created because there was a high pressure, high production sort of environment. And the files are simply copied back and forth to different storage servers through vendors. So it was not what a one-off This is part of a whole batch of other errors that they were discovered and this is courtesy of Rebecca Fraimow I should mention that she's happy to also take comments and questions via email or Twitter. I'm sure she's sleeping right now, but again, I guess I would throw it out to the crowd. Has anyone experienced this? Has anybody maybe, you know, have a sense of why that type of error would happen? Given the clues I suppose I've given you How often does this type of filling in happen? Carl! I grabbed the mic first. I'll run it back though. I was wondering. What was I gonna ask? Did you try in different playback methods? like using VLC, using FFplay, etc. And it's fine if I was consistent throughout the different I'm sure. Yeah, because it was hundreds of hundreds of files that were found this way. Yeah And they weren't transcoding. They're simply copying things back and forth. So there shouldn't have been manipulation. Go ahead. Yeah. So I first took to you yeah, yeah First to your answer it if you use VLC and FF play to check if to to like double check playback the problem is that in both cases, you will use the same library. So you will not you will in some cases at least not get a solution. Can somebody tell me what the codec is here because I cannot read it sadly 2VUY Okay, so sorry, in this case. I just want to mention that there are cases for compressed video that where the decoder is so like surprised about the data that he just stops decoding. This is apparently not the case here, but it can be. If this is V21. V21O So if it's uncompressed raw video then the data on the hard drive itself is damaged because there is no other explanation for this behavior in that case and then it's not really multimedia, but more like hard drive related. You think it's harder related? Oh another comment So actually in VLC, we actually have our own mp4 so, QuickTime demuxer, which is completely separate cut from lib ad format in from ffmpeg so actually between VLC and FF play you might have different behavior The real story we have on demuxers because we needed HLS - and etc, better support Before it was in FFmprg. And we keep maintaining it and adding more stuff So actually depending on how you play it you might be able to have different behavior between the two I don't know which one of the demuxer would be more resilient to errors in the file But it's possible they behave differently and you might be able to recover with one another So if you if it works in VLC, for example, the VLC has a transcoder on transmuxer so if it works that's possible to use the demuxer from VLC to transcode the file into another file And then it will be a correct file that you can use anywhere, so I'll recommend to make sure that she tests, but I kind of imagine that she did test that at some point I'm hoping. yes suppose. You probably used VLC So that means in that case the demuxers from VLC doesn't work In VLC, it's also possible to force the use of the LibAV format demuxer instead of the VLC one so you have both options to test with the 2 demuxers and Hopefully one of them is able to read the file. There's a comment in the back. Yeah I'd be curious. Was it looked at in a hex editor at all? Because I guess if it was a oh just if it was Such a static born digital image. I wonder Whether you can see the same yeah! Repeat. Repeat. Exactly! And it would save that doing much investigation in different tools and demuxer. Like it's just...it's the data is fundamentally broken and repetitive. I don't have access to all these full files necessarily but I think for a number of these that is a logical next step. So it would be curious as well. That's what I was gonna say as well I've gotten a lot of value of opening it up in a hex editor and finding working with something similar to this I had a similar thing that would freeze. It would play for a little bit and then freeze on one frame for the duration but I keep in mind that the duration is basically the metadata that's like at the header So it knows how long it's supposed to be and it's just sort of filling in my understanding What the rest of the stuff should be in this Miller plane But when I open up at hex editor that the data is not there it was 0-0-0-0-0-0 which is interesting We can move along we've got a couple more. Four or five more when we had one right now it got please I just wanted to ask They're really born-digital material? because I kind of reminds me of what I've experienced from mini DV. When capturing the kind of freeze frames in between It's just a it reminded me of that and I wanted to ask if maybe they were originally mini DV or DV. I know this is a relatively recent find Rebecca actually gave a much fuller presentation at code for lib a few years ago So and she reassured me that they were born-digital, but I don't know about the carriers I can ask her about that and maybe you guys can compare it now, it's perhaps Dave Rice. I wanted to recommend, there's a field in media info often that says if a file is truncated or not yeah, so it likely that the moov header is not the beginning of the of the file and lists the duration, but then the file is truncated. It's trying to present the intended duration, but doesn't have enough content to fill it. So, it stalls out So the file might be shorter like as if the transfer was interrupted. Or as Ashley was saying, you know, perhaps in the transfer it's zero-padded at the end because of a migration error Cool. Well definitely look at that. The last case actually had some of that is truncated in the technical metadata But did not display that type of behavior. It's a different I guess Visual error of some kind like more the traditional glitch, I guess So, let's move to the next use case number three Okay, we can just play it as is. It's good. But thank you Can you see...? Oh, I'm sorry. Well, I think you were able to see that horizontal artifact, hopefully Okay, so I think that was one of the more aesthetically pleasing ones that we'll see This is from a one inch transfer. And it's potentially related to a break and I don't know I'm speculating Calculated relationships between screen aspect ratio, display aspect ratio, and pixel aspect ratio. The display aspect ratios 4:3 as expected. The SAR is 1:1. Which would mean as expected that the pixel aspect ratio should be non square for this expected PAL tape, but then they're also noted technical characteristics that point to PAL standards Sorry, you probably can't see very well in the bottom. So with all the standards 720 576 25 But if you also look it says standards NTSC, so they're conflicting. Seemingly conflicting technical characteristics. and also in one of the outputs says the Pixel aspect ratio is 1.0, which is weird so I have questions about where MediaInfo for example pulls the pixel aspect ratio. Reading online it seems like it's interpreted from those very technical other technical characteristics to infer that so this could mean that the file itself is wrong, or is it a type of wrong? Can we talk more in depth about the wrong or? perhaps someone here could help us So these kind of questions How useful is my answer actually? It's useful, I mean I wanted to make sure to hunt down what you would say, but If it's inferring it from the other characteristics Shouldn't it say? PAL? So but also then we did see that visual error. The horizontal...I defer to you. Would be useful to know the source of a metadata for example? It is a bit longer to code, but it is possible. And it is some stuff to code so it is not doable in one day, but it is something possible to say Where I pick such kind of information for example Anyone else wanna talk about PAL and NTSC? Yes. Um, can you just go back to the MediaInfo for output Oh, yeah, um So just the question about the square pixel, I mean I see that quite a lot with some capture tools like I think Final Cut Pro 7 wouldn't insert any pixel aspect ratio metadata, so you'd get 720 by 576 with square pixels and it wouldn't be 4 by 3 to be 5 by 4 and So, I mean I don't But that's not related to what we saw potentially? No. I don't think is I think. That that's very common if you put it if you put it The files into a timeline and exports that Apple would write the correct metadata But yeah I've seen a huge amount of files like this and they can even come from vendors and stuff and Usually just correct it when we're transcoding or something but as for yeah, where it's getting the NTSC or like the Yeah, a few puzzling like is that down here? It's a separate I'm just trying to get visually Okay. Well, no. Anyway, I just wanted to make a point about the square pixel Thank you. Has anybody else seen that? Artifact? Sure. If I can find it. Oh, where is it? Sort of like a wrinkle that moves up and down Why do you think there is an issue? I mean Of course there's an issue, but I have a suspicion and I may be wrong that that this is how the file is actually encoded. I mean that's part of the question. Is this business as usual? Some would say that this is a error of some kind Because I think it's a it's a an unknown visual error or business as usual. I don't know. It's a 264 video. Did I see that correctly or because it says AVC on the on the MediaInfo panel Is that correct? Yeah, so if there would be a problem in the 264 file in the video stream that would would Lead to such artifacts then your screen would be full of of horrible error messages. So the indication that you did not say there are error messages is most likely that this is how the file was encoded. Whatever did this and this leads me to another point that I really want to make because I regularly get user requests and bug reports and I work on them Nobody here disagrees that there is an issue in the file because it says NTSC although it has 25 frames which simply isn't possible but this is not necessarily the reason for the visual issue you see. I mean, it could be part of the capture. I mean, they're layers here with this one since it's coming from That's possible. Yeah, so [audience member]: There are a bunch of other comments Yeah, hi I'm Alex from the BFI. Now to me that looks like a an analog artifact. It looks like there's a problem with the TBC in the one-inch machine. I've seen it before Having had a lot of experience with two-inch and one-inch And it looks like that's how it was ingested. And I think that's why you're getting all these funny Because you can't have PAL and NTSC together. It's just impossible. Yeah, right, okay so because I think because of the problems with the time base corrector attached to the one-inch machine You're getting all this crap coming up That could be. Yeah So perhaps in this case they have to go back and try to redo it, if possible. Yes Hi I'm Peter. I would like it would be interesting if it was on the analog side But I'm just guessing if it would happen on the digital side before actual encoding like you get in the uncompressed raw signal and then like two or three pixels in the line are dropped Then the whole thing shifts for certain lines and then it fills it up again So you've got these and if this occurs in a regularly unregular interval You might have this moving up and down because it seems like it starts in the middle of a line Draw several lines with an offset. Now imagine you lose like three pixels you get this odd thing but I'm just making this up from what it could be when it comes in So I want to thank anonymous who contributed this file This archivist speculates that actually it was an in-between process at some point where the files were transferred and then there's some intervening hardware or software that had presets potentially that then created the two standards somehow or something. Some intervening process at after the transfer in between. I want to sort of emphasize what Jerome said, I feel like I came and asked him Are you being a little polite? Is it true that people could pay to fix this problem or to get more analysis work done on this prod button. That's very true So I think that for the people that you're speaking on behalf of the come from really well funded institutions. We should consider doing that Agree. Yeah, the MediaInfo part of the problem. Yeah, no, no I definitely wasn't saying that no I was just trying to You know understand I think we can move on to the next one unless somebody else has something to say. Sorry Alright. Alright. Here we go Yeah, this is this is going through a filter It's a bit plain filter and shoot in QC tools. And if you could see before and I'll pull it up again the bits decrease and difference as you move from the left to the right if you're on unfamiliar with that tool This file comes courtesy of Eddy Colloton Again he's very interested to hear any feedback and I think this use case actually involves a couple of different things that are Kind of interesting at least to me to think about and speculate on. If we.... so you can see a lot of difference, but in if we go back to this slide I wonder for those that are unfamiliar with QC tools or this filter and ffmpeg. If you could sort of explain what's happening here. Visually. Sure. So it's actually playing back in real time the video and so I have a screenshot in this slide show where you can see how It's representing the visual imagery, but as it moves from left to right in the slicing of the bits the differentiation decreases. Like so as you get toward the top high level bit potentially. There's no difference Which we'll see in the next slide So this is the one that sort of initially spurred my curiosity. So to give a bit more background This comes from an HDCam vendor transfer It went through I forget the deck model, but there are a number of decks that handle both digital and analog inputs and put out taken both 8 bit and 10 bit and sort of information and put out through an SDI 10 bits, but in this case and he was also speculating so he'd be happy to hear your feedback. You can't really tell the difference, right? The filtering makes it virtually indistinguishable. So this is another case. We're looking at the hex might make it more clear if there's a high level of information captured Or if it's the same at that point Here's a two different moving images where you can actually see more differentiation These are with different decks again with HDCam. So 8-bit with the bit plane filter for 10- bit So you can sort of get a sense of if you're unfamiliar with this tool how it slices it up visually So perhaps it's a benign problem or really not a problem at all, but I think it's interesting to think about up sampling Does that...is that really only just a sense that an issue in terms of a space How much do deck specifications matter in his case part of his questions were how much to go back to the vendor and question the specifics of the transfer and to sort of interrogate what was happening there And then some more tweets. So that's Eddy on top if you haven't met him There's been some discussion and I missed AMIA, but I know this came up at the last AMIA about deck variation in terms of like the quality of capture and outputs So again, it's important to note that all these paths automated QC So it's more just like on Eddy. Eddy and I are like should we care on some level or how much does it matter and how? How can you have these conversations I suppose And this is another tweet I dug out from 2016 where they are talking about various deck captures. Various deck quality differences and Kieran notes that he sees a difference in the V Channel potentially when using Let's see BetaSP in a DigiBeta deck and I don't know I don't want to just spit out model names because I just want to think through the concepts here. So Perhaps. I don't know if people want to discuss some of this Thank you. I can't really remember the V thing, but I do remember it I think there was extra noise when extracting or looking at the extract planes filters and you're just it was I think in the white channel when I was comparing DigiBeta decks versus beta SP decks It's just more noticeable. But if you go back to the the bit plane images Yeah. So like I actually think that probably looks fine like that doesn't seem to have the issue and in the next one's just um, yeah like These are other HDCam vendors Yeah, like in the in the top to image like generally you just see that continued degradation in bits 9 and 10 But you see that, you know, that's sudden They're not quite identical, but you know that there's a shift in 9 and 10 and that generally seems to happen and it's also very helpful to look at the vector scopes and waveforms in QC tools as well are in FFmpeg, which are you know? You see a lot of like stair-stepping and it's almost like someone just scratched their nails across it or something like that Yeah and then that's generally an indicator I mean these these are good. He doesn't yeah, there's a difference, but with some of his captures he's like I really don't see a difference. So what's the point essentially? Is one of the points, you know raised but it turns out we're running out of time this took more time than I anticipated so We'll do one more quick one and cut out the last one which is about Dolby E, which is just a big problem I'll tell you you don't want to use Dolby E. And play this other fun one Let's try this So just there we go Haha, yeah, it's fun. What's going on? I'll stop. Try to stop Okay This file is created about 2005 Some speculation, but presumably we went through firewire route It's a DV transfer resulting in confused QuickRime wrap DV file I ran it through a couple of the different tools out there The The DV rescue, predecessor DVAnalyzer looking at the dimensions again the SAR, PAR, DAR and while most media metadata outputs seemed fine at least glancingly. There's one slight discrepancy that it seems to me at least with the DV rescue aspect ratio where it spits out Aspect ratio is 16:9 when most of the standard tools say 4:3 And most players that I quickly had them play through Exhibited that behavior, but Annie Schweikert, who is here with us today. Thank you for contributing this. Was able to have it playback normally through Actually, why don't you do you want to give us a little more background in information Annie? Yeah, it is a DV file wrapped in QuickTime It plays fine back in both QuickTime 10 and 7, but in every other player I tried and on Windows, Mac, and Linux it plays like that so it Yeah, that's mostly the context I have. I assumed it was a problem between Or discrepancy between the QuickTime wrapper and the DV codec but I could not pinpoint it myself. So if anyone knows I would love to also know Maybe we're gonna have that conversation shortly after since you here And Yvonne, one of the discussants can move us along to a second very different phase of this presentation So Yeah, thank you so much. And I guess the reason why I'm up here is because when I saw a Julia's proposal I was immediately intrigued by this idea of Exploring different ways of gleaning information about the provenance, the creation, and are the intentions behind a moving image file Just based on analyzing the image and the metadata and I believe this kind of understanding has potential application outside of like digitizing, QC, or Transcoding QC or preservation metadata to the field that I work in which is Human Rights documentation So right now in the Human Rights documentation space there is a lot of talk about authentication and verification of information. Especially photos and videos that are being shared on social media. On other open sources in the intelligence or security sense of the word So meaning like published sources or of photos and videos from anonymous sources there's also An emerging maybe overblown concern about deep fakes or synthesized media Although at present, I would say the greater real concern is what we at WITNESS have called shallow fakes, which are a sort of more simply missed contextualized, recycled, or lightly edited images So in the human rights world, investigators, journalists, and legal advocates are really grappling to find ways to identify and assert the authenticity of media that they're receiving or that they're collecting Often, you know that show evidence of grave crimes and that often have don't have provenance chain of custody or contextual information attached to them And in addition more broadly there's also a great need to raise public literacy about the alteration of images to address this very virulent spread of harmful misinformation that we're seeing globally. So the question I wanted to ask was, what can we glean about the provenance, creation, and intentionality behind media files based on an analysis of the audiovisual attributes of the media file like we've been doing this morning And what are some accessible ways to do that analysis? I'm especially keen to ask this audience here the open source technology community Because in the last couple of years there have been numerous proprietary solutions that have emerged claiming to solve this authentication problem with their own systems and computer vision and image processing techniques. The commercial solutions are not only costly, but they're also competing to dominate the market and have their IP form the basis of any future open standards for creating trustworthy media For trustworthy media capture and verification. And TruePic and Amber are just two examples. There are many others who are emerging in this space right now. So are there techniques and tools that can be shared by the open source community That human rights advocates can use to with indicate and verify media. So that people aren't reliant on costly solutions What knowledge that identify, manipulated media can the open source community share with the broader public to raise their literacy about manipulation of images? In what role can the open-source community play in the development of standards around trustworthy media capture and verification? And we probably don't have time to discuss any of those questions But I would love to discuss that with any of you during any of the breaks And just before and I should note that there are a couple of open source projects that I know of that can help people create more easily verifiable video documentation. ProofMode is one that we've been involved with with the Guardian Project and Tella is another one developed by Horizontal. But both of these apps work by collecting additional device and sensor metadata From the point of capture on the on the phone as well as a hash and cryptographic signature They don't use any sort of image data analysis or computer visioning or other techniques on the image itself to analyze its contents. And I'll wrap it up. Thanks. [applause] 