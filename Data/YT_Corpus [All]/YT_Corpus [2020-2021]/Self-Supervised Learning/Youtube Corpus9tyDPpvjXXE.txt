 Hi, thanks for the invitation to the Samsung AI Forum. I am very glad and be honored to be here. I am Kyungyhyun Cho, I am associate professor of computer science and data science New York University. Today I’m going to share with you my recent work done together with Nathan Ng, a PhD student of University of Toronto and my collaborator Marzeh Ghassemi who is a professor of University at Toronto as well. And it’s called SSMBA - Self-Supervised Manifold Based Data Augmentation. So, when you first talk about manifold assumption which is at the core of data science, machine learning, and natural language processing. What manifold assumption is, is that the high-dimensional data  lie roughly on a low- dimensional manifold. And then you can see how this assumption manifests itself in the real life by thinking of how digits that are written down by humans  can only change in a very small number of ways. You cannot change the digit in anyways you want while keeping  the identity as well as digit-ness as it is. And then it was actively discussed by Yoshua Bengio’s technical report from 2009. Now what is the science behind this manifold assumption? One way to view this manifold assumption and to use  it in machine learning is to look at it from the view of Vicinal Risk Minimization (VRM) that was  introduced by Chapelle, Weston, Bottou, and Vapnik in 2001. In this VRM framework, unlike in empirical risk minimization, we’re not only considering each and every training instance but also a nearby neighborhood for each of those training instances. That is, we’re not going make the classifier to work only on  each datapoint that we observe, but also to behave smoothly around these labeled instances. And then in order to do so, we need to somehow define the local density estimate,  which reflect the manifold structure behind there. And in order to use this VRM in practice, we have to of course use it in a stochastic gradient descent framework. And then in this stochastic gradient descent framework, VRM ends up being identical or very similar to data augmentation. For each mini-batch, we have many training instances, and for each training instance within this mini-batch,  we’re now going to add some noise, or we’re going to sample another point from its neighborhood according to this local density estimate, or the local manifold structure. And then we’re going to take the sample neighbor  instead of the original training instance as if this neighbor was the original training sample. And then by computing those tested-gradient using  those augmented inputs instead of the original inputs, we get the stochastic gradient of this Vicinal Risk Minimization (VRM). So, what that means is that the data augmentation is equivalent to sampling from this local density estimate in Vicinal Risk Minimization (VRM). Now of course in doing so, there are two questions that must be answered. First, what is this local density estimate Px'(x), or  another way to put it, is what is this data manifold. And second question is, what should be the label of this newly sampled neighbor  instance instead of the original label. Should we simply use the same label as it is? Or should we try to adjust the label according to the neighbor  that we have sampled from this local density estimate. So, let’s talk about how people have been answering these  two questions in different domains. Of course, the first domain that we often get to, when we  think about the machine learning algorithm, is computer vision. And in computer vision, people have identified a priority: “what are necessary manifold structure, or at least  a local manifold structure, given any image?” We know that any given image such as handwritten  digit or a picture of this teacup pig you see on the slides, we know that what are the kind of transformations that we can apply ourselves without changing the identity of an object within this image. For instance, in the paper by Alex Krizhevsky and others in 2012 which, somehow, some people has been saying that revolutionized the field of computer vision by introducing a very deep convolutional network. They use the horizontal/vertical flips, random crops, and pixel intensity augmentation because they knew a priority that those are the transformation  to which the identity of an object is invariant. Now of course before I have to push this direction quite further, Last year Gubuk Hijau have proposed that what we need to look into is not even to try to design this transformations ourselves  as machine learning researchers, but simply try to go into those image manipulation toolkits. And in their case, they use the ImageMagic toolkit, and then just apply all possible transformation that have been implemented by the computer vision researchers and developers over the past decade. And they call it RandAugment, short for the Random Augmentation. Now of course, computer vision is not only the domain that we’re interested in. One domain that I’m particularly interested in, is natural language processing. And last year, Wei and Zou proposed so called easy  data augmentation or EDA where, simply said, that we know that the words can be replaced randomly with this synonym, and generally the meaning of the sentence should not change. And also, we know that we can always randomly insert some words, or randomly swop some of the words, or randomly drop in some words, do not alter the meaning of sentences too much. Because as you are hearing me, I’m making a lot of mistakes  and inserting randomly to fill the words, however you are largely getting what I’m saying. So, this is also a very domain-specific way to augment the data. And in this particular case for natural language processing, of course, we can be a bit more sophisticated. Already from 2017, people have realized we can  now use a new machine translation system which have gotten so much better in recent years in order to generate many paraphrases of any given sentence of paragraph. And paraphrase here refers to, “another way to write given text without altering the meaning.” And then how do we do that with the machine translation system? We’re going the exploit the fact that machine translation systems are imperfect, and machine translation is many-to-many mapping. So we’re going to start from a given sentence; apply, or to translate this into another language using machine translation system; and given that translation we’re going to use a reverse translation model, in order to project it back to the original sentence… original language. And what we get is a paraphrase of the original sentence. And then this turned out to work quite well. And of course, what do you do if you don’t know the domain, or you’re working with the domain where you don’t really have any idea about this local density estimate or local manifold structure. Already from 1991, people have realized that simply adding some  unstructured noise such as gaussian noise to the input makes a neural network more robust to the small changes to the input, thereby making it generalize better to unseen examples. In fact, in 1996, Chris Bishop showed that adding  some gaussian noise to the input during training is largely equivalent to try to make this neural net or  the resulting neural net as smooth as possible. This is amazing in a sense that it does not require any domain knowledge, but it’s quite limited in a sense that we have to somehow cook-up a noise distribution that we know how to sample easily from. And we know that the distribution makes sense in terms of the structure of the data. So then, of course the question here is, can we in fact breach this domain-specific data augmentation  method and domain agnostic data augmentation methods by learning this data manifold from often abundant amount unlabeled examples. And that’s where the idea of denoising autoencoders comes in. In 2010, Pascal Vincent and others from University of Montreal proposed a very interesting way to train an autoencoder called, denoising criterion. The idea is extremely simple. You train a neural net to denoise an artificially corrupted training instances. And this artificially corrupted instance is a very important idea here. We have already talked about in the previous slide that the N  is easy to generate or create a local density estimate, from which we can sample a lot of the neighbors. It’s unfortunate that we don’t know that whether such an  unstructured noise distribution reflects that data manifold or the local structures underlying this data. And what this denoising autoencoder does is to transform  this unstructured noise distribution into a local density estimate from which we can sample and reflects the underlying data distribution. And how it does is to trying to figure out what is the direction in this high-dimensional space that could be ignored. And that was 2010. Let’s fast-forward all the way to 2020. And nowadays, everyone in natural language processing  as well as even in computer vision are psyched about the idea of so called Mask Language Model which is represented by ELMo, BERT, and RoBERTa, which were all proposed and released publicly in 2018. And they have become the fractal standard in natural language processing. A mainstream views of these mask language models of BERTs has been to fine-tune this pretrained model for a downstream task. And then it turned out that a lot of the problem that we  believed were interesting and were difficult, turned out to be very easily solved even with the small amount of training instances when we fine-tuned these pretrained mask language model. However, a few people have noticed how much these mask language models, when trained on a vast amount of unlabeled data, knew about the word, in 2019. For instance, as you see on your presentation slide,  BERT will be able to fill in some of the missing words, and then when you look at what kind of answers BERT this mask language model gives you, you notice that all those answers are all plausible. In other words, this BERT has learned what are the directions of the plausible answers of the plausible  language in this high-dimensional natural language space. And now you see where I’m getting at. And this is where we propose, or I’m going to describe  you what our proposal that is SSMBA *Self-Supervised Manifold Based Data Augmentation: Instead of using either the domain-specific augmentation  nor the domain-agnostic augmentation, we propose here to use mask language model or  more generally denoising autoencoder as a way to sample from the domain-specific local density estimate without having to know about the transformation’s,  so the plausible transformation’s a priority. Because these denoising autoencoders will learn  themselves what those local manifolds are, and then allow us to sample from it. So we start from some training set, for each training instance, we’re going to apply the artificial corruption function, and then fit the corrupted example into denoising autoencoder. And then denoising autoencoder is going to give us a reconstruction that is in fact a sample from the neighborhood of the original training instance. And then this sample is a sample from a distribution that we flex the underlying data manifold. And then we use that example instead of the original training instance. Now, where does the self-supervision comes in? Self-supervision comes in from the fact that we’re not  necessarily going to use the original label, but we’re going to use the label provided by the original classifier, or the previous classifier that was trained on the original clean data. So we’re exploiting the fact that these classifiers tend to  generalize to a small neighborhood in the data, so that we can trust the labels predicted from this  classifier as long as we’re sampling from the nearby points near by point centered at the training instance. And then these equations on the slide that you  see are what I have just described to you in words. So since we don’t really have much time, I’m not going to go into the detail. But what I wanted to point you out here was that effectively what we propose here is a modern form, or the modern implementation of  Stochastic Vicinal Risk Minimization that was proposed already in 2001 that was nearly two decades ago. Now of course the question is, how well does it work? So we tried this approach called SSMBA on many different problems. First problem that we tried it on was text-classification. For text-classification, we tried it upon two different subtests. One is sentiment analysis where we tried our model to look at a review of a product such as movies or restaurants, or actual products from Amazon, and then tried to predict what was the star-rating that reviewer gave. So that’s the proxy to the sentiment of the reviewer  at the time when the reviewer was writing the review. The second subtest was natural language inference. This is a problem where given a pair of sentences. One sentence is premise and other sentence is hypothesis. And we want to know if one of the entails the other,  or one of the contradicts the other. Of course there’s the option that they may be in fact neutral with each other, and then this classifier needs to tell whether these pair of  sentences belong to entailment, contradiction, or neutral categories. And then we tried it on two separate datasets as well. As baselines that we compare against … so compare SSMBA against,  we tried easy data augmentation that we talked about on supervised data augmentation we also talked about,  this is based on the machine translation systems. And then we also tried to conditional BERT contextual augmentation which is quite similar to SSMBA in a way that they use mask language model, but the motivation then how it works differs significantly from the proposed SSMBA. So interesting thing about this data augmentation algorithm is that this is more of a meta-algorithm in a sense that it doesn’t really matter what kind of classifier you use, or what kind of loss function you use as well. So we decided to try different underlying classifiers including  recurrent neural net, using LSTM units, convolutional neural network, and RoBERTa which is one of the largest mask language models that we fine-tune. So as you can see from this slide, which is a bit dense. I can tell you that the proposed SSMBA works better than all the  other augmentation approaches that we have compared to. And also importantly, SSMBA never degrades  over the baseline that was trained without any augmentation. This is an important point because the whole point of the augmentation procedure or the data augmentation is to ensure that the classifier is going to get better. But then, if we don’t know that it won’t degrade the underlying classifier, it becomes effectively impossible to use in practice. And then this trend was clear across different  categories of products that we have looked at, given in the case of the restaurant review. And the same thing we could observe from this  natural language inference task as well. So what are the effect of setting different hyperparameters or “knobs” of the SSMBA? So what we have seen is that the improvements that we see from using SSMBA for data augmentation does not diminish even if we have a large amount of training instances. That’s probably because the input space is so high-dimensional, however many training instances we collect it’s never  enough to cover the entire meaningful data manifold. And it turned out that it is really important to use as many neighborhood samples as possible although it does convert to relatively quickly. So in our experiment, in most of our experiments, we have this five neighbor sample from the  denoising autoencoder for each training instance. Then it turned out that the quality of denoising autoencoders  itself is not too important although it is important to use a reasonable denoising autoencoder. And then finally we want to also look at whether it is  necessary to use this self-supervision, and it turned out - and perhaps as expected - that if you have a good  classifier to start with it does help to use self-supervision. Of course, classification is one thing, but classification is pretty dull. So we wanted to try it on the machine translation. In particular, we wanted to look at how SSMBA  this data augmentation technique that we have proposed could help with the low-resource machine translation. We looked at German to English translation with the high mismatch between the training and test domains. And also we looked at one of the most extreme case of  the low-resource machine translation, translating German into Romansh, which is a language that is spoken in Switzerland. And as baseline we tried a bunch of existing approaches, however we did not compare it against an existing  approach that uses a large scale target-side monolingual corpus. That’s because those approaches are simply  complementary and orthogonal to what we propose in SSMBA. So in the future what we like to see is whether we can gain further in terms of the translation quality by using SSMBA  together with, for instance, back translation. Similarly, to what we have seen with the classification, what we have observed is that the SSMBA indeed works better than any of the existing baseline data  augmentation algorithms that we have tried, and also SSMBA never degrades over the baseline, which was not necessarily the case with the baselines that we prepared against. So to slowly wrap up, this kind of manifold-based data augmentation in hindsight  looks so obvious that we should’ve thought long time ago, but it’s always easy to connect all these dots looking backward. And by looking backward, in my mind, I see these kinds of connection or the network of the dots. So we already knew about the data augmentation and its utility from only 90’s, and then we knew about the importance of the manifold assumption all along, but was very carefully formulated in 2006. And then denoising autoencoder was proposed already in  2008 largely based on the intuition of manifold assumption. While these things are happening, vicinal risk minimization was proposed largely independently in 2001. Now what we know is that because we now see all these pictures of all these dots, we can now connect it, and then we end up with the SSMBA which turned out to be one of the state-of-the art approaches of agnostic data augmentation. And then it works for classification as well as machine translation without the issue of degradating the quality of the baseline classifiers. So before I finish, I would like to point out two possible future directions that I believe we should pursue so that we can make SSMBA  even better and more broadly applicable. First is the robust optimization. So robust optimization is unlike the usual empirical risk minimization where our goal was to maximize the accuracy on the average case. On the other hand, this robust optimization focuses on the worst-case scenario. That is, we want to look at the worst possible local perturbation to the input that’s going to make the loss go up as high as possible and then try to ensure that the loss on that worst-case  perturbation is going to be as low as possible. Now this makes a lot of sense when we can solve this inner loop of  finding the worst perturbation, except this is really difficult to do so when we’re working  in a high-dimensional discrete space. Now if we knew, or if we could use such a learned manifold  structure from a denoising autoencoder, can we solve this inner loop approximately but much more efficiently in order to make the robust optimization much more feasible, and also thereby allowing us to build a classifier that’s going to be more robust to the adversarial examples. Second direction that I believe is really interesting is  trying to go beyond this local augmentation. Most of the augmentation techniques that we have used  in our community including SSMBA that I have just described you relies on the local perturbation, so given a training instance  we’re going to look at a nearby point. And we want, I believe, to go beyond a small local neighborhood. Unfortunately, I have absolutely no idea what to do about it but nevertheless it’s interesting and it’s more pursuable. And that concludes my presentation. Thank you very much again inviting me to the Samsung AI Forum, And that’s it! 