 welcome back everyone so this is already the last uh talk of the reinforcement learning summer school and um i want to thank um because before i introduce our next speaker i want to thank the organization around this event um really it's been a pleasure to host this event along with the seafarer staff so thank you to jackie thank you thank you to yeah um wendy as well as nadine from mila who's been uh helping us to prepare this event throughout the last uh six eight weeks actually so um really without them uh none of the logistical uh none of the logistics would work out around this event um aaron and i were only here for for basically the program and and we just had the privilege to pick up to pick the speaker list so thanks a lot and i also wanted just to say that i i hope that um we've succeeded in making you feel part of our community um i really believe that that science is not something that that belongs to select few it really belongs to all of us and and it's our it's our job as scientists to challenge assumptions and to provide new ways of thinking and seeing the world so that's why we need new people and i feel like just quoting actually our good colleague uh from montreal uh who's actually leading uh the deepmind team in montreal who once told me that he thinks that for him the greatest innovation in the ai over the last few years was not so much algorithmic but the fact that there's such a big influx of new researchers in our field that for him this is the the big game changer in our field that we're gonna have so many new ways of thinking about our field that is completely gonna change it and i think this is really true i think that this is our best asset and and our best tools for your future is to come up with new perspective and new ways of understanding our problem and that's really how we're going to be able to solve the ai's greatest challenges and also how we're gonna have the most impact in our society um so with that i would say uh uh thank you for participating let's hope to see you at some point in milan montreal and now um let me introduce you uh chelsea finn who's our last speaker of our session so today we've heard from angela and rupam and different ways to apply reinforcement learning on real systems and i think that the common topic among all these talks was was the importance of data efficiency in real world systems and this is a constraint that is super important as well when you have safety constraints um so today we're going to see probably a third approach a an approach that chelsea has been pushing and is the it's the approach of meta learning and reinforcement learning it's gained a lot of interest from her uh initial work in 2017 and since then it's really create a new field and so chelsea finn is an assistant professor in the computer science department at stanford she's also a cfair fellow she received the acm 2018 doctoral dissertation award for her thesis and more recently she's also received the microsoft faculty fellowship award so i can say without hesitation that chelsea's work has truly pushed your field in new directions and i'm very happy to welcome her chelsea thank you for the introduction i'm excited to be here too uh and hope to hope that you learned something through um through what i'll talk about today um and actually let me make sure that i can see the participants so the um if you have any questions throughout this i'd encourage you to raise your hands and i'll pause at a few points during the lecture uh to take your questions uh so that this can hopefully be a little bit more interactive than uh simply me talking at you um yeah so today i'm going to be talking about meta reinforcement learning and first i want to give some motivation for uh why we we want to do something like my learning so uh to start off i'm actually going to give you a little test um the answer you can kind of keep to yourself or write down on a piece of paper or something um since we we aren't in person uh but the test is what is as follows so i'm going to give you a training data set these are your training data set is the six images on the left and you want to be able to classify the image on the right and in particular the left most three images are paintings that were painted by brock and the second column are three paintings that were painted by cezanne and now your goal is from these six images in your training data set can you classify the painting on the right as being from brock or cezanne um so i'll give you a second to think about this um and look at the kind of the training data and learn from the training data how to kind of classify this new image um and hopefully the answer that you most most of you came up with is uh brock that's the correct answer and you can tell that by the kind of style of painting and the kind of straighter lines that you see in um in the paintings by brock and also the paintings in the test example now how did you accomplish this task so uh you only had six training examples and yet you were able to actually or usually i hope many of you are able to classify the image on the right and the way that you accomplish this is you have previous experience um and maybe you haven't seen these particular paintings before maybe you actually haven't even seen paintings from these painters before but you have previous experience looking at images and you aren't starting from scratch tabula rasa to learn this task and so kind of the key idea behind metal learning that i'll talk about today is can we enable systems to leverage previous experience in a way that allows them to learn from such a small amount of data and in particular we'll be talking about reinforcement learning and uh humans are kind of remarkably good at reinforcement learning for the same reason so you can see kind of kids can learn how to kind of stack objects on top of each other in certain orders and so forth without kind of a lot of trial and error experience and the reason is it for this is that people also have previous experience when they're learning different tasks like different motor control tasks so if we want to build a machine learning system that's as efficient and can learn from just six examples or a few examples we need to figure out how we can leverage some previous experience about the world so how might we do this um if you of course if you just train a neural network from scratch on this date on this task you'll do pretty poorly uh and so if you are thinking about how to get a machine to accomplish this task uh there's a few different ways you could go um and that kind of if you were thinking about this a long time ago uh one kind of classic approach in computer vision was to provide some prior knowledge in the form of a model of the image formation process uh if you kind of move forward maybe a decade uh the approaches were more like providing information about geometry as prior knowledge um then more recently people were looking at things like sift features and hog features as prior knowledge um or maybe you would kind of pre-train from imagenet and fine-tune from those features or do domain adaptation from other painters and each of these kind of things in this list are different ways of injecting prior knowledge or prior experience into machine learning systems and into kind of ai systems now one of the things you might notice is that uh kind of as you go down this list fewer of the prior knowledge is coming from human knowledge and they're becoming increasingly more data-driven priors uh and also arguably the the things towards the bottom of this of this list have achieved greater success than some of the approaches at the top so this suggests that maybe we should move in the direction of building in priors that are acquired from data and from previous experience when trying to learn new tasks and perhaps at the extreme of this is thinking about if we can try to explicitly learn priors from previous data and previous experience that lead to efficient downstream learning and this is kind of the key idea behind metal learning essentially can we learn how to learn from previous experience so that when we have a new downstream task we can learn that very efficiently okay so that's the high level motivation behind metal learning and today i'll be talking about specifically the meta reinforcement learning problem statement um and so i'll first go over kind of the problem formulation and some examples then i'll talk about two primary methods for solving the meta reinforcement learning problem um then i'll talk about the learning to explore problem which is kind of a key component of meta reinforcement learning and i'll talk i'll end by talking about some open problems in meta rl and some of the kind of latest developments and challenges okay so let's get started with talking about actually the problem formulation and some examples of meta reinforcement learning problems so before we move go to the meta reinforced learning problem let's first just look at the meta learning problem and to do this we can start with kind of defining some notation in the context of supervised learning so in supervised learning you might have inputs x and outputs y and you want to learn a function that maps from x to y maybe parametrized by theta and the way that you learn this is through data of input output pairs now the idea behind metal learning is that we're essentially going to nest uh nest this problem a bit so in the meta supervised learning case our goal is basically to solve the problem that you saw on the second slide which is you're given this little training data set which consists of k input output pairs so in the k shot learning case you have k of these input output pairs in your training data set and using this data you want to then classify a new input x test to produce the corresponding label y test so you can essentially view the metal learning problem in the context of supervised learning as learning this mapping from a training data set and a new input to the corresponding label the way you go about training or kind of learning such a learning procedure this function f can be viewed as a learning procedure as it's learning from or taking in data learning from that data and classifying a new input the way that you go about doing this is you have data that's in the form of different data sets um so you have data sets for different tasks each of these data sets has a number of input output pairs and typically for each of these tasks you assume that you have at least k examples so that you can sample k samples to be used for d train and at least one example to be used to measure whether your learner generalized to that task and then you train this across a number of different tasks denoted as di okay so that's kind of notationally what the problem looks like and this this view on metal learning is actually pretty nice and useful because it essentially reduces the problem of metal learning to the design and optimization of this function f essentially if we can design a good function that can take us and put some data and also optimize this function then we have a learning procedure that we can apply to new tasks so now let's look at an example of what this actually looks like so a canonical example in metal learning is fushot classification this is very similar to the problem that you saw on the second slide where you're given this training data set in this case your training data set has one example of five different classes and then your goal is to classify new images as being among those five classes so basically kind of the same kind of problem that you saw previously with the paintings and the way that you go about solving this is let's say you have data from other image classes from image classes that are other other than lines and bowls and so forth and you try to structure that data in a way that matches what you're going to be seeing at test time which is exactly this q shot learning problem in particular if you can take five other image classes shown here kind of break it up into a little train set and a little test set and do this for multiple sets of five classes train your function across these train your your med learner across these training classes to take as input a training data set and produce uh correct labels on each task test set um essentially learning how to learn each of these tasks so that at test time you've been given these images you can effectively classify them even if these classes are different from any of the classes that you saw during metatrading okay um and so that's this is kind of an example but in reality any any of these tasks shown in each of these rows could be other learning problems they don't have to be classification problems you could be trying to learn some regression problem or trying to predict um the dynamics of a robot for different tasks there's lots of different ways that you could structure these tasks the key thing is that you want this test task shown at the top to be to be kind of representative of the kinds of tasks that you're training it on okay so now that we've looked at the meta supervised learning problem what about the meta reinforcement learning problem um the way that this looks is we're basically just going to replace each of those rows with reinforcement learning problems instead of um instead of supervised learning problems so uh instead of having a supervised learning problem as shown here we're going to replace our inputs x with our states s produce our outputs replace our outputs with actions a and then our goal is to learn a policy pi that maps from states to actions using uh some state action reward next state transitions so this is kind of one way to view the reinforcement learning problem where your goal is to learn pi such that you maximize your expected rewards and the way that we can kind of turn this into a better reinforcement learning problem is again we're going to be trying to learn from a small amount of data such that for example this the small amount of data may be k rollouts from some policy we want to learn from this such that when given a new state we can effectively predict a good action to take from that state and so you could again view this as some function or some learner f that takes as input this training data set that it's trying to learn from this is kind of its experience in its current task as well as the current state and predicts the corresponding action and again the data that we'll have will be a data set of data sets where each data set corresponds to a different task and each data set is collected for different tasks okay so here the problem is also the design and optimization of this function f how it's actually going to learn uh how it's going to actually learn from this data and how we're going to optimize it to learn from this data but another important part about the meta reinforcement learning problem is how to collect appropriate data basically how to collect dtrain because this is also in your control in a reinforcement learning setting as well uh essentially this this part of that problem is the learning to explore part like figuring out how you should collect this training data um how to explore for a new task such that that data gives you effective information for solving the task okay um so kind of as a concrete example uh one thing we might want to do is maybe we want to solve figure out how to navigate a maze from some examples so given a small amount of experience in one maze you want to figure out how to solve that maze and the way that you do this is this is kind of what happens this is meta test time the way that you do this is during meta training time you learn how to learn many other mazes um so you give it kind of experience in this maze create it to solve this maze with a small amount of data and and so forth okay um one final note about the meta reinforcement learning problem is before i mentioned that this d train could be k rollouts from some policy this is kind of the episodic variant of meta reinforcement learning a more online variant could be one where this training data set corresponds to just a kind of a few time steps of from your policy and you're trying to essentially learn on the fly from this data just in kind of a matter of time steps rather than from multiple rollouts okay i'll pause here to see if there are any questions on the problem statement before moving on to the methods so feel free to raise your hand if you have a question i also see that there's some q a [Music] that we could also go to toga do you want to ask your question um thank you um to my understanding when we uh remember formalizing the meta learning problem early on to me kind of felt like we were just doing a transfer learning on on a number of different batches so i i'm not sure how what would you say would be the significant differences between a classical transfer learning or just mini-batch-based learning versus meta-learning tasks yeah that's a good question so in um because the transfer learning problem and the meta learning problem are very similar i think that where things differ the most are in actually how the algorithms are derived and specifically you can view metal learning approaches as ones that explicitly optimize for good transfer performance they kind of explicitly optimized for performance for learning on different tasks so that you can learn a new task quickly whereas transfer learning approaches often use they may not often kind of explicitly optimize for that fast adaptation performance for example by by optimizing for like features to be overlapping or something they use kind of more hand-coded mechanisms to try to enable good transfer okay um i don't i can also try to see if i um see the q a yeah so another question is it seems like my supervised learning is similar to semi-supervised learning um so i guess one thing that i'll mention is that typically the data sets that you see are when you have a data set of data sets we typically assume that those each of those data sets from previous tasks has the corresponding labels so we're not actually using any unlabeled data in this context we're using kind of batches of data from different tasks okay cool so um i'll hold off on questions for now um and we'll talk about some of the metal learning methods and if some of these questions are still um if people still have some of these questions after going through the methods then we can uh we can definitely talk about those as well oh actually one question that i'll ask quickly they'll answer quickly is someone uh maddie asks do you train on all tasks simultaneously or sequentially a standard metal learning setup will assume that you train on typically you're allowed to access all of the tasks at once rather than the tasks and sequence although although there are some meta learning problem statements where you actually only get the tasks in sequence and many of the algorithms that i'll discuss today can also be applied to that sequential setting as well okay so let's talk about some of the method classes for actually solving this problem uh and there's really two primary classes that are used in the context of meta reinforcement learning um and so once you kind of understand these two primary classes then you you'll understand kind of most of meta-rl so in the problem statement i talked about this um i talked about how we want to kind of have this function that takes its input a training data set and a state and outputs the next state or outputs the corresponding action um and so one really simple way to do this is to essentially parameterize this learner with just a big neural network so for example it could be a recurrent neural network that takes as input the data sequentially um and outputs an action uh and that's basically it you could have different structures as well this doesn't necessarily need to be recurrent you could use an attention based model you could use temporal convolutions um you could use something that just kind of takes as input all the data and then aggregates them and then produces kind of a context there's a lot of different things you could choose for the architecture here but the key thing is that we're just going to parameterize f with this big neural network that takes as input the data um as well as the current state and outputs uh the action for that state uh and note that this kind of takes as input the reward at the previous time step which is pretty important if you want to be able to adapt to different reward functions now you might ask well how does this compare to just doing reinforcement learning with the recurrent policy uh and it's quite similar with a couple with well one change is that you're passing in rewards into this policy whereas typically in like if you just apply a recurrent policy you may not be passing rewards but the bigger difference is that the hidden state of your recurrent network is actually maintained across episodes within a task and so this allows you to actually adapt to a task across multiple episodes and kind of maintain memory about that task and kind of learn about that task across multiple multiple episodes okay um so that's kind of the gist of of this approach it's quite simple and you could optimize this this function with standard reinforcement learning methods with like policy gradient methods or with q learning like methods and so i'll highlight just one case study of using this kind of approach which is a paper back in 2018 on using a mix of attention basically using attention-based models and convolution-based models for this uh metal learner and in particular what the architecture looked like is they trained a policy that looks like this where they take in the state at each time step as well as the action from the previous time step and the reward from the previous time step the architecture interleaves 1d convolutions and attention mechanisms and at each time step kind of outputs the corresponding action you then execute that action and then observe the reward and pass in the that reward and the action at the next time step so that's uh that's kind of the gist of the method and they trained this i believe with some um some policy gradient algorithm and they applied this actually to the maze navigation problem that i mentioned at the beginning and in particular they wanted to learn how to visually navigate different mazes and with the goal of being able to kind of quickly learn how to navigate a new maze at test time so they trained it on a thousand small mazes and then evaluated on both held out small mazes as well as new larger bases as well and so what this looks like is this is it's collecting its kind of training data in this first episode right here and you can see that it's kind of exploring this maze a bit um going in this this image on the left is kind of what it's observing and this is the top-down view right here that the agent doesn't get to see it explores its environment for the first episode and then after that it knows exactly how to solve the task so essentially after it's kind of explored sufficiently it knows enough about this maze or it's kind of learned how to navigate this maze such that at the next episode when it holds over the state of the attention kind of like when transfers over the information from the attention and the one decompolations it can um it can solve the task uh immediately and they also applied this to a larger maze so here's one example of it uh effectively learning how to navigate a larger maze and so we can see in this first episode it is again exploring the maze and learning how to navigate this maze and then in the second episode it can it's kind of learned how to navigate this particular maze such that it can go directly to the goal in the most efficient so those are some of the qualitative examples and then quantitatively they evaluated um the performance in the small in small bases and large mazes uh for the first episode and the second episode and what they find is that they're able to do um significantly better than a random agent their architecture is also significantly better than using an lstm and also they're able to kind of significantly improve in the second episode compared to the first episode which means that it is actually kind of learning something about that maze and uh and solving it in fewer time steps okay um now one kind of quick digression on this approach is kind of the connection between these methods and contextual policies um and what i mean by contextual policy is some is a policy that's conditioned on some context omega for example omega might be the position that you want to like stack a block in or maybe the direction that you want a robot to walk in um and in many ways you could actually view this uh kind of a contextual policy in some ways kind of as a special case of these kinds of models um where basically the the experience or the data is serving as context to the policy uh yeah and then this kind of this last part of the recurrent network is is the actual policy part um so you can essentially just view these this kind of metal method as a contextual policy with experience or data as context um and also if i'm not sure if it was if goal conditioned rl and goal condition value functions or universal value functions were covered um but you can also view this sort of metal learning method as a way to adapt to different goals but in a more general way so rewards allow you to actually adapt to any task even if that task isn't a goal reaching task and so in that way kind of these rewards are a strict generalization of goal-based tasks um also in meta-rl you're typically adapting to new tasks rather than trying to generalize in zero shot to new goals so that's kind of another distinction with goal condition policies is that you're adapting in k shot versus generalizing to a goal in zero shot okay um so any questions on the on this part so it looks like there's a question about um the maze navigation task from uh sumana who says that how is the base navigation task meta because we're learning to navigate based on visual representations where size in the sha and shape might not matter um so in this case the uh the kind of size and the shape of the maze basically the shape of the maze affects your strategy for navigating that maze um in some ways you can there's i think that there's kind of gray areas between things that are metal learning and things that are just kind of learning a policy and recurrent networks are one thing that kind of blur this line a little bit um but the basically in this context you basically need to adapt with data and experience in this maze in order to do well and that's exactly why we saw that we saw this kind of significant improvement quantitatively between the first episode and the second episode uh tommy do you have your hand up do you want to ask a question uh can you hear me yeah um so in meta rl setup we first collect some context data from a set of calls from some some kind of policy so my question is can you collect the construct context data on an online setup my like not stopping the collection procedure but continuing online and collecting data on the go yeah that's a good question so it's actually worth mentioning that in in these kinds of approaches it is actually in a very online setting where you're actually collecting data and also predicting how to act and by training this this kind of um this recursive neural network model it's actually being trained to um kind of continuously collect data and when it feels like it knows enough information about the task it will stop exploring and simply just kind of take actions that give it to the goal so this is actually really in kind of an online sort of setting um i'm going to ask one more question that's actually in the q a that's asking about this paper by chad at all that showed that um that kind of representation learning and metric learning kinds of approaches uh can reach kind of performance that's very similar to that of a meta learning approach the question is i wonder if we should still focus on metal learning and one of the things that i'll mention is that i think that the the tasks that are looking looking at in that paper uh and actually i think that may the findings of that paper are specific to the image classification setting um and i think that in those settings just learning good features is good enough to solve those those sets of tasks whereas i think that in other kinds of metal learning task distributions including in the reinforcement learning setting learning good features isn't enough to be able to adapt to the task you need to be able to explore effectively and and adapting to the data that you've collected during exploration also requires a more sophisticated adaptation procedure than simply doing linear regression on top of features okay um i'll hold off on questions for now uh to try to get through a little bit more content so this was um kind of what i call a black box approach in the sense that you're parameterizing your network with your parameterizing your learner with this big neural network and some of the benefits of this approach is that it's quite general uh and also very expressive a recurrent neural network can approximate any uh any function under kind of fairly mild assumptions and so we can really approximate um any sort of learning procedure here further there's also a variety of design choices in the architecture which gives you some flexibility in terms of how you want it to actually process the data now the downside to this approach is that it's a pretty complex model that has a has to learn a very complex thing which is learning from data it has to actually be able to process this data and actually kind of translate that data into something information about the task that informs it about how to act in the future and as a result because it has to learn this pretty complex task completely from scratch it can sometimes be quite difficult to train these models and sometimes they also require a fairly impractical amount of data in order to optimize them effectively um so some of that motivates the next class of approaches that we'll looked at which is optimization-based approaches and the kind of starting point for optimization-based meta-learning is this idea of fine-tuning so the way that fine-tuning works is you have a set of pre-trained parameters theta and you run gradient descent on those pre-trained parameters using training data for a new task for example you might pre-train some neural network parameters on the data set like imagenet and then fine-tune parameters on a more modestly sized data set for some new tasks that you care about to get some parameters fi that work well for that new task so this works actually quite well um and people have shown actually a lot of success of this in computer vision settings but also in natural language settings as well uh and we'd like to be able to extend this to the fuchsia learning setting now unfortunately if you pre-train on something like imagenet and then fine-tune on only a few examples it's likely not to work very well and you're likely to over fit to those small examples to that small data set and so instead what we're going to try to do is explicitly optimize for a set of pre-trained parameters such that fine-tuning works well and so the way that you can write this out is something like this where you're going to have some set of pre-trained parameters theta you're going to run one or a few steps of gradient descent from those parameters on your new task and then optimize for the performance of those fine-tuned parameters on held out data and you'll do this not just for one task but you'll do this for all of the tasks in your metatrading set such that you could hopefully transfer to a new task so essentially you're kind of like embedding grading descent into this meta learning process and specifically optimizing for your initial pre-trained parameters so this kind of key idea is to learn this initial set of parameters or these initial features that transfer effectively with a small number of gradient steps and a small amount of data so let's look at a kind of an illustrative example or actually let's first look at a diagram of kind of what this looks like visually so say that theta is the parameter vector that you're meta learning and phi i star is the optimal parameter vector for task i then you can view the meta trading processes this thick black line here and what is trying to do is if you're at kind of this point in the meta training process and you take a gradient step with respect to task three you're quite far from the optimum for task three and likewise for the other tasks but at the end of the metatrading process if you take a grading step with respect to task three you are close to the optimum and likewise if you take a gradient step for task one or for task two so this is kind of a kind of more conceptual diagram for what this is trying to do it's trying to get to you is kind of just set in the parameter space that allows you to quickly fine tune to um the optimal parameter vector for a different task um so this algorithm is called model agnostic metal learning and it's an example of one optimization based metal learning algorithm and there are kind of other ways that you can twist this such you can learn the learning rate here for example or you could use other inner optimizations not just gradient descent and in the context of reinforcement learning what this might look like is trying to learn a set of initial features or a representation space under which reinforcement learning is very quick um so as kind of an illustrative example say you want this uh quadruped ant agent to run in different directions and you meditate it to be able to run in those different directions at the end of the mediterranean procedure but before you take a gradient step you get a policy that looks like this essentially the um the ant is essentially like running in place it's kind of ready to run in any given direction if you then take a gradient step one gradient step with respect to the task of running backward you get a policy that looks like this that runs backward and if you take one gradient step with respect to the task of running forward you get a policy that looks like this and this is just kind of with a with just a single gradient step okay so that's kind of the intuition behind this kind of approach now what does this look like on a more realistic task distribution not just where you want to run in different directions but when you want to adapt to different scenarios so we can combine this approach with a model based reinforcement learning setting where we want to be able to adapt to different dynamics in the environment and in particular we want to be able to adapt completely online and what we're going to do is we're going to store our recent history we're then going to adapt to those last m time steps using just one step of gradient descent on your learn dynamics model and then we'll get our updated model parameters and run planning or model predictive control npc which is kind of a fancy word to say that we're going to be planning um at each time seth using this model uh take it action in the environment then add this and then kind of continue to adapt our model and take actions under the adapted model now of course if you start from scratch and adapt your model with one gradient step you won't do very well so what we do is we meta train this agent to be able to adapt to different dynamics using the mammal or the modeling gnostic metal learning algorithm and train it such that it can adapt with just a single gradient step okay so this is kind of one example of being able to adapt to different dynamics some examples of dynamics variation might be if you the leg of an agent or you their agent needs to kind of run across some platforms and the platforms have different buoyancy kind of like a pure um and if you're on just kind of standard model based rl in these environments without any adaptation you get behavior that looks like this so the videos are a bit um are a bit shaky but so as soon as the the lag is crippled the agent isn't able to move very far and uh in this case when it's trained on these different platforms with different buoyancy uh the cheetah isn't able to figure out how to effectively navigate this terrain and eventually flips over however if we combine this with a metal learning approach and train it to quickly adapt online to these dynamic changes it can figure out how to essentially adapt uh online to changes such as a crippled leg or um or these varying buoyancy uh these platforms with varying buoyancy okay um and because this is actually a model based uh meta rl approach it's actually quite efficient in comparison to model three approaches and that means that it's actually quite practical to run it on a real robot so we took the same algorithm we collected data of this robot this legged velociraptor robot on different terrains on turf on styrofoam on carpet and we trained it to be able to quickly adopt a dynamics model using mammal to these different terrains um and there's more than just these three three trains there was about i think 10 trains in the data set and then we wanted to evaluate to different different other changes and dynamics such as the robot being on a slope missing one of its legs having a payload or having calibration errors and in this case the first example is we put the the robot on the slope and if you just treat it with model based rl which doesn't have the ability to adapt it isn't able to effectively walk in a straight line up the slope and ends up gearing up to the right whereas by using metal learning it's able to adapt online to these to the change in dynamics caused by the slope and able to effectively make it to the top of the slope in kind of roughly a straight line and then in this last example we actually took off the front right leg of the robot uh if you train this with model based rl it isn't because it's kind of lost its front right leg it veers off to the right and if you actually trade it to adopt using metal learning is able to adapt and learn how to learn this new dynamics model online and effectively follow the straight line okay cool so there's essentially these two different kinds of meta reinforcement learning methods one is these black box methods where the neural network is implementing this learned learning procedure and then second is these optimization based metal learning methods where you're kind of embedding gradient descent into your learning procedure now you can essentially view them as very similar kinds of approaches they kind of both take this form f but the key difference with the optimization based metal earner is that inside this function f we have this gradient operator that is updating our parameters as a function of the training data set rather than just ingesting that training data into a neural network um so as i mentioned before some of the benefits of the black box approach is that it's quite general and expressive there's also a variety of design choices in architecture but it can be challenging to train whereas an optimization based method it's usually typical typically easier to train because you have this inductive bias of sgd built in kind of at initialization is already doing some amount of learning with gradient descent it's also model agnostic and so if you have an architecture already it's fairly easy to plug and play with your existing architecture um the downside is that sometimes with reinforcement learning methods the gradient that you get from a reinforcement learning method like a policy gradient or gradient of bellman error are often not very informative about the data and so sometimes it can actually be challenging to combine this approach combine kind of an optimization based approach with policy gradients for example if the gradients you're getting are very noisy or high variance okay so this is a good point to pause for questions toga do you want to ask your question yep thank you um so the the image in in describing the model agnostic uh metal learner paper was quite illustrative um to my understanding what it does is that it learns kind of their generalizable task invariant parameters which kind of call like i i think in a sense um it's uh we're trying to find better priors so that for a specific test it just uses that prior to generalize well do you think there's some sense of bayesian methods uh to be explored in that direction because um having a prior um immediately suggests that maybe it's a bayesian learning scheme yeah that's a great question so it turns out that you could actually show that with some fairly crude approximations that mammal is actually approximating uh kind of um the inference procedure in a hierarchical bayesian model where the where the gradient step is doing something that looks like uh kind of posterior inference and the kind of metal learning process is learning uh the prior in that model um so there's works on air by aaron grant on that and there's also some works um by myself and by others that actually try to make these methods more basin essentially uh and a key component of that is to when estimating your posterior of your task parameters actually representing a distribution over parameters or a distribution over functions um that quantify your hypotheses about what the correct function is for that task um as a quick follow-up uh do you think there's some sort of uh parallelism between you know how a neural networks early layers learn kind of more general representations and it just proceeds to more specific features versus um meta learner learning kind of tasking variant generalizable representations and we kind of fine-tune it for uh test specific features do you think there's some sort of one-to-one correspondence between these two um potentially i mean in some tasks we find that we actually only need to fine-tune the later layers uh and the kind of the earlier layers are more general and they don't need to be fine-tuned um in other tasks we do actually find that the low-level features do actually need to be adapted um and uh need to be kind of adapted to be like more task specific should be do you want to ask your question yeah i had a question on whether um there exists methods to evaluate the degree of overlapping substructure between tasks um so that we can ensure that the policy transfers um well at test time i see so you're trying to you're trying to think about what um how tasks share structure in a way um right so i would imagine that the the dataset that we sort of create for um training a meta learner um they should have some degree of overlapping substructure so for example in the case of the maze um i guess like we we deployed the agent in another maze that shared mostly the same features at test time um but are there like i guess methods to evaluate um how much substructure or like how much overlapping substructure attached should have yeah that's a really good question um i think that that there are perhaps some works but it's also quite a challenging thing to do especially with neural networks where it's a little bit hard to interpret exactly how they're learning things and how they're sharing structure across tasks there is an approach called modular meta learning that tries to actually make this more explicit and more interpretable and they actually are they kind of learn how to combine different modules and show which modules are shared with between which tasks i see okay cool thanks uh how about one more question from farshid can you hear me yeah hello okay so uh the um first order metal learning optimization based ones like reptile they are not that much away from mammals so what are your take on that so why is that why simply aggregating the weights could do as as much um could perform as much as like mammal so how come yeah so as background for other people the um there's kind of you can construct i guess first the one thing you might notice in the mammal objective is that there is a gradient operator inside here and you're also minimizing over theta so when you use um like sgd on this objective overall there actually are second order derivatives that show up as a kind of by nature of having a gradient here and a gradient here and you can derive a first order version of this algorithm by simply dropping all of the second order components and other and otherwise maintaining the um kind of the gradient of this meta objective um and then there's also another algorithm called reptile that's actually very similar to the first order version of mammal um and then the question is why does why do those first order methods work well um we observed this in the original manual paper as well uh for some of the few shot image classification tasks uh and for those tasks i mean the short answer is we don't exactly know it's actually not at all principled to drop those second components the second order components and we've actually found that they aren't really close to zero either um but we also have observed that in some more challenging tasks like fu shot imitation learning using a first order thing doesn't work at all so i think that it has to do with the nature of the task and how well conditioned this optimization problem is to be able to ignore those second order um second order terms um i think that in some of the simpler kind of shot image classification problems it's okay to drop it but in other ones it's not thanks thank you okay great so for the sake of time let's move on um so maybe i'll just go here so we talked a bit about the pros and cons now what i'd like to focus on is this problem of learning how to explore essentially how should we collect d trade so so far we've kind of largely haven't been kind of thinking about this problem at all we've just been collecting d train according to our kind of current policy or current model um and this is reflective of kind of the standard solution to this problem which is to simply optimize for exploration and execution of the task end to end with respect to the reward of your execution performance uh and this is the approach that's taken by kind of a wide range of papers in better reinforcement learning it's simple which is nice it also in principle will lead to the the optimal strategy for exploring however the downside for this approach is that it can actually be a quite difficult optimization problem when exploration is difficult for a new task so what i'll briefly talk about is some alternative solutions for solving this problem and actually first i'll explain in what scenarios this might be a challenging optimization problem so let's look at an example of a hard exploration problem so say we want a robot chef that can cook for us and what we do is we kind of meditate it across these different kitchens maybe kind of we have a few different kitchens in a factory and we trade it to be able to adapt to those different kitchens and then after it kind of does its metatrading in the factory we then kind of ship out the robot and deploy it into your own kitchen and you want the robot to be able to learn how to perform cooking tasks in your own kitchen now this is a pretty challenging problem because uh in order to learn how to cook you need to figure out where all the utensils are or where all the ingredients are um and you also need to kind of figure out that exploration of the of everything in your kitchen doesn't necessarily matter like looking at the decorations um isn't really helpful for learning how to solve cooking leg tasks uh so we'll be doing mediterranean across these previous kitchens and then at meta test time we're going to be given at least one exploration episode to kind of learn where all the ingredients are and then one execution episode that leverages that exploration information to perform these cooking tests okay so here's kind of a running canonical example now why is end-to-end training of exploration hard um so the reason why this is hard is we have a bit of a chicken and egg problem which is that we need to do two things we need to learn how to find the ingredients or we need to learn how to explore and we need to learn how to cook and if you have a bad exploration policy if you can't find ingredients that it's going to be very difficult to learn how to cook you're going to have kind of a heart really hard time learning how to execute tasks and further if you don't know how to cook things um you have a bad kind of execution strategy then any exploration policy that you try will have low reward simply because you can't cook and so this leads to this kind of coupling problem where learning exploration depends on having a good execution strategy and learning execution depends on knowing how to explore so let's think about some kind of alternative solutions rather than an end-to-end approach to allow us to solve this coupling problem so the first solution we'll look at is leveraging alternative exploration strategies rather than trying to learn them end to end and what this might look like is one alternative strategy would be to use posterior sampling which is also called thompson sampling this is the approach that was taken in the pearl paper and what this looks like is first you learn a distribution over a latent variable that represents your task and this distribution is either conditioned on no data if you don't have any data yet or is conditioned on the data that you've collected so far and then you also learn a corresponding policy for that task and then once you have these the way that you can explore is simply to sample z from your current posterior hence the name posterior sampling and sample from your policy for that task uh and then kind of iteratively reapply the sampling procedure by kind of iteratively sampling from the environment and recomputing your posterior so the way that this looks like in practice is say you have a simple task distribution which is to navigate to these different goal positions shown as these light blue circles and maybe in this instance the actual task is to go to this blue dark blue circle here and for each of these tasks you only get a reward if you're within the circle now if you sample some trajectories from your prior over tasks you'll get trajectories that are going to different circles and then you can then take this data and then update your post year and then kind of re-sample tractories that will bring you to positions that are different from the things that you saw before so these are shown in the lighter pink color and then using this data you can then update your posterior again and because you had one trajectory that reached the goal your posterior should now focus on this goal and that's exactly what we see here where the rest of your data is going to the goal so this is um this is one one way to explore one strategy to explore this can be quite effective um but it may also not be optimal in some cases so it may not be optimal for example if your goals are very far away and maybe there's a sign on the wall that tells you what is the correct goal posterior sample sampling will never actually look at that sign it will never actually go out of its way to look at that sign it will simply sample goals whereas the optimal exploration strategy would just be to look at that sign and then go to the correct goal okay so that's one approach another approach is to use kind of intrinsic rewards or some kind of self-supervised objective to explore that's what's done in this paper uh and the last strategy might be to try to explore such that you can effectively predict the dynamics and the reward for each task and that was what was done in the medicare paper and the way this looks like is you would train a model of your dynamics and your reward conditioned on your experience and then try to collect experience so that this model is accurate and in the same environment that i mentioned before we actually see that a strategy like this will actually learn a better exploration strategy where it actually kind of moves around the circle rather than sampling uh more inefficiently but this may not this well this is kind of much more effective in this environment it may not be optimal in other environments for example if you have a lot of distractors in the environment or if you have very complex and high dimensional state dynamics then learning this model will be very difficult okay so the pros of this these kinds of approaches is that they're very easy to optimize and many of them are actually based on very principled strategies for example in posterior sampling you could actually quantify how optimal this is but they can be sub-optimal by arbitrarily large amounts in some environments okay so the last approach that we'll look at for solving the exploration strategy is to try to decouple exploration and exploitation or an execution by acquiring representations of task relevant information and kind of the key idea is if we can acquire this representation of what's relevant to the task we can then explore in a way that allows us to recover that information so the way that we do this is we first to learn we first learn this representation of the key information for the task and then learn to explore by recovering that information so to do this first step we condition on an identifier of the kitchen this could be an identifier that looks like this that identifies like the wall color the ingredients and the decoration it could also just be a one hot identifier for the kitchen and you train an execution policy conditioned on this this identifier of the task and critically we also bottleneck this representation using an information bottleneck to encourage it to discard parts of this task identifier or bits of this task identifier that aren't relevant for actually solving the task so that we don't care about the decorations here we mostly just care about where the ingredients are and then once we learn this execution policy which we can do with simple standard rl we can then train an exploration policy to produce exploration episodes that recover this information captured in z by maximizing information gain at each time step and then i'm at a test time we can deploy this exploration strategy which will produce an exploration episode that has the right information which can then be passed to the execution policy okay um so we'll call this decoupled reward free exploration and execution in meta earl or dream um it actually turns out that you can prove that this approach is actually consistent with the end-to-end objective um so this will actually recover the optimal exploration strategy if provided enough data okay oh and it's also worth mentioning that this approach will also in principle be significantly more efficient than the end-to-end approach because you aren't um because you're kind of decoupling this these this uh decoupling the exploration execution phases so if we look at a challenging 3d visual navigation problem um in this case unlike the other tasks the the goal will be to learn to go to a certain key block or ball of a certain color and the color will be specified by the sign here and the agent will start over here such that actually has to walk around the barrier and go to the side to kind of read what color object it should go to and this is all with pixel observations and as far as reward and what we find is with this approach here it is able to learn to kind of go around the barrier and go and read the sign during the exploration episode so this is something that you wouldn't get from posterior sampling and it's also something that would be very difficult to learn end to end when you have visual observations and sparse rewards and then in the execution episode um it figures out that it should go to um it's kind of given that it should go to the key and because it figured out that you have the blue key it's able to immediately go to the blue key and then if you look at kind of average return this approach is able to do um achieve like pretty near optimal reward um these existing end-to-end approaches do very poorly because exactly because of this coupling problem it's very hard to learn this exploration strategy end to end and then uh thompson sampling achieves uh would achieve something like this pink line here so essentially this is an upper bound on how well the pearl algorithm would do okay um so kind of to summarize how we might learn how to explore there's this end-to-end approach which can lead to the optimal exploration strategy and principle but it is very hard to optimize when exploration is hard alternative strategies are easy to optimize and many of them are based on principled strategies but they can be sub-optimal in some environments and then if we use this decoupled approach at least the optimal strategy in principle it's also easy to optimize in practice and one downside is that it does require this task identifier to identify kind of a one-hot id that identifies the task however in practice this identifier is is typically pretty easy to get okay um so for the sake of time i think i should skip questions here and try to start to wrap up and talk about some of the challenges and latest developments in meta-rl so what are the open challenges in better reinforcement learning so the first one that i would say is trying to figure out how to adapt to entirely new tasks so before i showed how we could adapt to different dynamics models like different terrains for example or adapt to running in different directions or different mazes but in many situations we may want to adapt to something that's kind of more distinctly different from what's in the meta training set um and if we want to be able to do this something to kind of adapt to something that's quite distinct we need to still follow this kind of guiding principle of machine learning in general which is that the things we see at training should the distribution of things that we see at training should match the distribution of things that we see at testing and so if we want to adapt to something completely new this means that our task distribution needs to be quite broad so that when we kind of see something new that's still within this broad distribution of tasks okay so how do we where do we actually get this broad distribution of tasks um we could look to existing rl benchmarks but things like open eye gym or the atari benchmark they aren't particularly suitable for this either because the tasks are quite different from one another or there's aren't enough tasks to actually support generalization to new tasks so with this in mind we actually recently developed a benchmark for studying this problem that we called the meta world benchmark and it has actually 50 tasks from pretty distinct manipulation settings um where the goal is to kind of manipulate an object in a new way or like kind of manipulate something in the environment and uh kind of this satisfied a number of designerada that we think should make it effective for studying metal learning algorithms including 50 qualitatively distinct tasks um a shaped reward function for each of the tasks so that you don't have to worry about rl failing on these tasks you just can focus on the transfer problem um and it does have a unified state and action space to help facilitate transfer as well um and it turns out if you actually apply metal learning algorithms existing metal learning algorithms to this benchmark they don't they don't do very well um at all it's challenging to scale approaches to this so far um so that's kind of a good open challenge for for the field um cool another challenge is figuring out robustness to our distribution tasks so many times the task that you want to be able to transfer to is a little bit different than what you saw during metatrading um and you want to be able to robust be robust to that and this there's been some initial work at this in the supervised learning setting uh but to find knowledge no one has looked at this in the meta reinforcement learning setting the third open challenge in my mind is figuring out where the tasks should even come from uh essentially the we need to somehow define these tasks for metatrading and we kind of metal learning algorithms assume that these tasks are given but this means that a human or something needs to provide them to the agent and kind of so far we looked at manually defined distributions of tasks and correspond and corresponding rewards uh but it'd be nice to actually have mechanisms for the algorithm itself to come up with tasks for itself to solve um and kind of one initial approach to this would be to try to take some skill discovery methods which learn different skills and then perform better learning across these skills to be able to adapt to a new skill um and there's been some promising approaches promising results with this approach compared to simply learning from scratch uh but i think that there's kind of still a lot of work to be done on that front uh and then lastly also just uh i think that the reinforcement learning algorithms that we that metal rl algorithms build on uh is really a core aspect of them if the rl algorithm isn't very good then the meta rl algorithm also won't be very good uh so i think that we need to continue to improve the stability and effectiveness of rl algorithms themselves okay um so one key takeaway if you kind of learn nothing from this talk the one thing that i think you should kind of take away is that we should start i think moving away from reinforcement learning from scratch and meta rl provides one approach to do this but not the only approach um in the slides i put a bunch of references for other works or some of the works that i covered today that i think can be helpful if you're interested in learning more about some of these topics i underlined some of the ones that i think are some of the key references for learning about some of these different topics and i'll leave this in the slides for you to take a look at as as much as you're interested in and also it's worth mentioning that i teach a course at stanford on metal learning um and all the lecture videos are available online uh this was a bit of a kind of a whirlwind of meta reinforcement learning the course goes in more depth and um and goes a bit slower than what you can cover in a one hour lecture great so i'd be happy to take a couple more questions and we also have the breakout session at um in a few minutes as well do you wanna hand yeah go ahead uh thank you uh i really enjoyed the talk i have a sort of basic question which it's sort of related to the uh open-ended questions that you put up about where do these tasks come from so my question is how do you decide on the number of training tasks and um have you do you have any initial results on if you just keep supplying your metal learning algorithm with more data do the results just keep getting better and better or they peak or plateau is there any analysis done on that yeah that's a great question so um typically you want a fair number uh a decently large number of tasks um basically the more tasks you like having more tasks will allow you to kind of generalize to new tasks better similar to in machine learning having more data allows you to generalize to new data points well as well um as kind of a concrete example typically um in kind of meta supervised learning settings we'll assume that we have like at least 50 to 100 tasks um although the more you have the better of course and like with the omniglot data set for example you actually have more like a thousand tasks um and in these reinforcement learning settings we also typically look at a similar number of tasks around the order of 50 to 100. okay thank you yeah and actually oh the second part of your question was like how do they keep on getting better as you give them more data yeah i would say that as you give them more tasks they definitely keep on getting better um at least from what i've seen although as i mentioned kind of like having these very broad task distributions are still a big challenge for these these algorithms and in our experiences okay so they don't really uh they don't um generalize well yet to uh something that is extremely out of distribution right yeah okay um although one thing i will say is that gradient-based methods tend to generalize better than black box methods because you're still at least running gradient descent whereas if you just kind of apply a neural network to out-of-distribution training data then there's really nothing you can ex you can't necessarily expect it to generalize to anything out of distribution but like you you don't even know if it's going to improve on like the training loss on that data whereas with gradient descent you at least know that given kind of out of distribution data it will at least do as well as like fine tuning for example awesome thank you great um do you want to ask a question yeah sure uh thanks thanks for the talk first and um my question is about i'm new to the math and learning perspective so my um it feels like the mental learning problem is more like a learning to generalize so my question is about is there any way for us to measure the generalization gap between different distributions of tasks yeah and also like something if there are something like the pipe learning framework course to quantize and do some theoretical analysis on this problem yeah so you can you can definitely view metal learning as trying to learn to generalize with a small amount of data um and ultimately hope to kind of generalize to new tasks there is a little bit of theoretical um there is definitely some theoretical work on trying to quantify like the distance between tasks and transferability between tasks for example there's some work by alessandro chile who recently completed his phd at ucla i believe with with stefano suado on trying to kind of build some theoretical framework between transfer transferability between tasks um i've yet to see like a theoretical like i think that that alessandra's work is is quite interesting but i've yet to see a framework that's really um in practice like effective at informing transferability and so forth i think that part of the challenge is that the transferability between tasks depends very much on the model and set of features and what you've already learned for example if you know things about um about physics then or maybe if you know things about um yeah if you if you know certain things then kind of two tasks might be more similar to each other versus if you don't know some kind of common factor then they might look very different yeah i'm just wondering if we can have a more principle way of instead of random search on designing the model architectures as you said the task data distribution matters a lot and also the multi-modal architectures matters a lot so yeah yeah i definitely think that they're like at least right now it's more of a kind of an empirical study of what can transfer and what can't um another way to look at it that we a perspective that we took in the meta world paper is that you can have kind of parametric variability of tasks where there's some underlying parameter that's continuously varying and non-parametric variability where you kind of have things that aren't multiple tasks that kind of don't lie on this single continuous spectrum but i think there's kind of a lot of work to be done to try to formalize things that are useful in practice yeah i totally agree with that especially the earlier of the parametric meta world model so maybe in the next time the next step would also be to like uh learn the parametric real-world tasks learn to parametrize the real-world task distribution yeah yeah thank you okay maybe we take one more quick question from henry can you hear me okay yeah okay um so i just had a question about how you can kind of formally define distances or similarities between types of tasks so whether there's kind of like a strict metric or like formal metric that you have over task space to kind of determine how similar tasks are the agent can solve or how different two tasks are that an agent might be able to solve yeah so this is kind of getting at what um what john was asking uh i think that there aren't really i think that such a metric would need to depend on the model that you've currently learned um one metric is basically just how well fine tuning works like if if you fine tune does that lead to good performance um when you kind of pre-train on one task and transfer to another task uh but i think that there aren't really any any metrics that people have come up with that um that are kind of useful in practice and uh yeah and like theoretically grounded okay thank you okay i think we're at the four o'clock mark thank you so much uh chelsea i know you wanted to get as many questions answered as possible the good news is is that uh we will be having a breakout session with chelsea in roughly what 10 to 15 minutes and we're carrying over those questions to continue the conversation thank you so much uh i wanted to close off uh the the conference because it was our last lecture of the five day deep learning reinforcement learning summer school thank you all thank you chelsea thank you to all of the other speakers on behalf of seafarer on behalf of mila i want to thank you for joining us and i do want it because i did get a couple of questions on this i do want to tell you that the slack channel that has been buzzing with the interaction is going to be open until the end of the month so more questions more interactions with speakers between each other uh keep at it because it will be available until the end of august um it's not over yet we have the breakout sessions and lastly we were going to laugh with ai improv at five o'clock so don't miss it uh hosted by amy institute so let's take a quick break give chelsea a moment to catch her breath and head over to the breakout sessions the links are on the daily agenda thank you all so much it has been a pleasure bye everyone 