  SIDDHARTHA SRINIVASA: Hello, everyone. Welcome to the Robotics Colloquium. For those who don't come to this colloquium that often, we meet Fridays at this time, at 1:30. So please do come. It's a public event. If you have speaker suggestions, please do let any of the robotics faculty know. We'd love to love to honor that. I'm really excited to have Jeannette Bohg here today. Jeannette is a professor at Stanford. She did her PhD at KTH. I remember your PhD thesis. And then went and was a group leader at Max Planck. Did some really fantastic work in her thesis on multimodal tensor fusion for robotics, and has continued that on, and done so much more. I think what I really appreciate about Jeannette's work is that she does fundamental perception finding control algorithms, but also puts that on real physical systems. So we get to see a lot of that. And as you all know, as you start exploring the whole robotics stack, the most interesting problems lie at the intersection of different pieces-- perception prediction, prediction planning, perception and planning. And Jeannette's work really exemplifies how these different components can be brought together to solve really hard problems. Jeannette will be here for the rest of the day, and I know her day's pretty packed with meetings. But if you'd like to meet with her, we'll try to find ways in which we can make that happen. But without further ado, Jeannette. JEANNETTE BOHG: Thank you very much, Sid, for the introduction. Yeah, so I'm very happy to be invited. Thank you for inviting me. It's my second time in Seattle, actually. And I'm really happy. It's been always very sunny both times I've been here. And I've seen for the first time also Mount Rainier. Thank you, Byron, for telling me how to pronounce it. OK, so today's talk is going to be how to embrace uncertainty in robot manipulation, as opposed to trying to get rid of it, and then ignoring it in a way. So my research is basically driven by a puzzle. And that's a puzzle of why humans can so effortlessly manipulate any kind of object. Like you, for example, eating these cookies right now, potentially. And it's really funny, or it's really puzzling, to me why it is so hard to actually reproduce that kind of skill on a robot. And it's really not clear what is in that secret sauce that makes humans so amazing in this. And we really haven't figured out how we will enable robotic manipulation, so that they can be really helpful to people in the future. So I have a few thoughts on maybe how we can achieve this. But today, I want to focus on one particular point, which I already brought up. And that's the one of we should probably embrace uncertainty. So what do I exactly mean by that? So one of the places where I typically don't publish is actually in computer vision and machine learning conferences, or very seldom. I have very few papers on this. And I thought a little bit about that. And I think the reason for this is that, in these two fields, very often, success is measured in terms of accuracy. So for example, in computer vision, this can be in terms of pixels, or preferably maybe segmenting or labeling each pixel in the image, or in a [INAUDIBLE] cloud, or maybe predicting forward in machine learning what is going to happen next to the absolute maximum possible accuracy. And that, of course, is a very interesting and challenging problem, and has its own merit and should be done. But I would also argue that total accuracy is actually maybe never achievable. And also, this total accuracy is not necessarily what a robot actually needs. And if you think about this picture, for example, where you see a kid playing with toys, you can imagine this kid basically playing around with these objects maybe dropping them. Maybe they're slipping. Maybe trying to take one of these things and put them into the tractor, or whatever, and it's not going to work. And it's just going to recover from these things. So it's really a very intricate process. But it's probably not about maximizing accuracy, but rather on how to recover from maybe failures and how to try again. So this kind of task success here is really a function of really fumbling around. And I think all of us here-- I mean, if you're a roboticist, you would be extremely impressed if there was a robot being able to do what the kid is probably doing here right now. And so I think if you were to take on this task of playing around with these objects and bringing them in a particular configuration, then trying to achieve total accuracy and explaining every pixel on this image actually will not necessarily give you test success. And so for this reason, I think maybe accuracy, and the way maybe computer vision of thinking about it, or machine learning in terms of prediction is thinking about it, is maybe not the right goal, or it's definitely not the only goal, to strive for in robotics. So what do we do in robotics. I think in robotics, what we need to do is to close fast feedback loops around really messy high-dimensional sensory data to actually do interpret the scene and really make very fast decisions. But I would argue that we need to accept, then, that we will never achieve total accuracy. And instead, I would argue let's trade accuracy with speed, and work on methods that may be reactive and continuously adept, and can react to failure, instead of maybe trying to plan everything out based on the assumption that you have some perfect perception method or some perfect prediction method. I do think, obviously, learning and computer vision and machine learning methods and planning methods should be in there. But I think the goal should be a bit different. And even if you take that view, there are still challenges, or challenging questions. So you still have to ask yourself, if I don't need total accuracy, like what is actually the important information? And how off can I be to still have a functioning robot? But just to summarize one method that I'm trying to convey in this talk is that if you work in robotics, you actually have to say maybe goodbye to perfectionism. I mean, people in, for example, computer vision often strive for, or for in terms of prediction. And if you work in robotics, you really have to somewhat, I think, accept the fact that everything will be uncertain at a certain base level. And you have to somewhat embrace that chaos of this uncertainty that comes with a robot, actually. So once you accept the fact that, oh, maybe we will never reduce uncertainty, then I think one good focus is actually on controllers and planners that can continuously adapt. And in today's talk, I want to go through three example projects, where the focus is exactly on these kinds of things. So the first project that I want to talk about, that project is about how two robots can help each other, or one robot helps another robot, to learn a manipulation task better. That is difficult because of all the uncertainty that comes in. And in this project, we are trying to basically funnel the uncertainty through physical fixtures. In the second project, I'm going to talk about a task and motion planning framework that we reformulate in such a way that it can actually deal with a dynamically changing environment, and where the plans are basically staying valid, even though the environment changes. And in the third project, I'm going to talk about a difficult planning and perception problem, namely when you have tried to manipulate deformable objects. So these are the three projects. And I'm going to start with the scaffolding project. So for those of you who work with a robot, all of you know that actually, autonomously learning robot skills is pretty challenging. So here, you have these three tasks of peg insertion, turning a wrench, or shallow depth in turn. So for example, something that people who assemble your phone, or you do sometimes when you put the battery into your phone, have to do. So these are pretty difficult tasks. And the way we want to approach this problem is, instead of trying to develop better perception methods or better controllers, what we are going to try here is to actually scaffold the learning process that this robot actually has to go through. And the contribution here is that we formulate scaffolding as a two-loop learning problem. And let me go through that part. So actually, scaffolding is a concept from developmental learning, I believe. I don't know so much about this. I just know it's a concept there. But you can also think about all the things out there that we, as people, actually use to help in learning things or building things. So for example, we use scaffolds to build buildings. Maybe to learn certain skills. For example, for riding a bike, you can use training wheels. And actually, in automation and assembly, fixtures are a very common thing that you actually use to make assembly easier. But, of course, all of these things are typically figured out. Like how you place these fixtures, for example, is typically figured out by some smart engineers in a trial and error period. So our goal here was really to construct a framework for robots to autonomously discover how to ease manipulation skill learning. And so let's take this peg insertion task as an example. It's pretty hard. It's a difficult problem, especially if you don't know what exactly the geometry is, and if you have noisy perception to smoothly and efficiently insert this peg. But if you put a fixture at the right place that aligns just the right way, with these, basically, corners of this hole, then suddenly, the task actually becomes quite easy, because that particular fixture funnels the uncertainty into that place. So basically, the fixture blocks and prevents a lot of these kind of false motions that a robot can go through. And therefore, this is basically how this funnel works, by basically restricting this motion. OK, so now what we want to automate is, basically, both the manipulation skill learning by this top robot, and the fixed replacement by the lower robot right. And as an approach, we have this two-loop framework. SIDDHARTHA SRINIVASA: Can I ask a quick question? JEANNETTE BOHG: Yes. SIDDHARTHA SRINIVASA: I mean, going back to your concept of training wheels, I think the purpose of the training wheels is to enable you to be able to ride the bike without the training wheels. JEANNETTE BOHG: Yes. I come to that later, how we get rid of the fixtures. SIDDHARTHA SRINIVASA: And the other point being that peg and hole insertion-- like the idea of chamfer in peg and hole insertions came about to be able to create preimages that are attractive. So I-- yeah, well we can come to that. But I guess my question would be, I'd love to understand-- and you promised that you'll talk about it, how the training wheels can be removed. JEANNETTE BOHG: Yes, so that I will definitely address. And also, the chamfers, yes. So people came up with the idea that just makes this easier, and just also funnels uncertainty. And by having, actually, this automatic process, you can actually optimize this online, where it's maybe not so obvious how to place them. I mean, we select pretty obvious problems here. But you could see how this could potentially be used for not-so-obvious problems. And I will show you how you can remove them again and what happens then. So OK, so this is the approach. We have an inner and outer loop, where in the outer loop, you'll have basically this robot here in blue that tries to basically place this fixture. And the inner loop takes the depth image as input. That is basically from the overhead camera, as you see here in blue. And it uses that as input, and then decides how to place the fixture. And then the inner loop has, basically, this robot that just uses whatever model-free IL method, not so important, to actually learn this particular manipulation task. And then basically, the reward to the the outer loop is on how fast the inner-loop robot learns. So the faster the inner-loop robot learns, the higher the reward for the outer loop. AUDIENCE: Quick question. JEANNETTE BOHG: Yes AUDIENCE: So it feels a bit like placing the fixture might almost be as hard as putting the peg in then? JEANNETTE BOHG: Not so much, because-- well, OK, we simplified this way. First of all, we designed a fixture for this particular task. Then we did define the exploration space in which the fixture can actually be placed. But typically, the fixture replacement is actually easier, at least in these tasks that we considered here, because they don't have to look tightly fit or whatever. So it's not such a difficult kind of contact rich task. AUDIENCE: [INAUDIBLE] JEANNETTE BOHG: Yes. OK, so how do we formulate these two loops? So the most interesting one here that actually also has an algorithmic contribution is the outer loop, where we use a contextual bandit approach. So again, you basically have this context the depth image. And you basically want to figure out, or you have to contextualize depth image. And as action, you have the placement of this fixture. And you want to figure out for each of these actions what the expected reward is. You want to learn that in order to then pick the right one. So the challenge with this is that this particular Q-function that we are trying to learn is actually discontinuous. So let me explain that. So we are basically trying to estimate this Q-function that we approximate with, in this case, a CNN that takes in this depth image as input, as context. And then also as input it has this action, which is basically this placement of the fixture. So if you have this Q-function, at test times, you basically try to find the action that maximizes, given the context, the expected reward. And so in practice what we do is, we use CEM, like [INAUDIBLE] from [INAUDIBLE] paper, if you're familiar with that, where we basically sample a bunch of action, fit a Gaussian to the expected reward, and home in on this. So it's a hack, but it works. OK, but that's not so interesting. The actual challenge is how to learn that Q-function, which I mentioned to you as discontinuous. So how does it actually-- so this is roughly how it looks like. So on the y-axis, you have the reward. And here on the x-axis, you have the action, which is basically the placement of the fixture. And here, we're looking at this particular-- basically moving this L-shaped object to the left. And you see that, basically, insertion is not possible if the hole is covered. And you suddenly get a jump in the expected reward as soon as this L-shaped is exactly aligned with this hole. And you get less of you if you move it away. So that's how this Q-function looks like, and why it's discontinuous. And now for training, we propose a new algorithm that is called smooth zooming algorithm. It's basically a combination of two algorithms. And let me just go through this. So this is the pseudocode that I'm going to go through step by step. Again, we have to discontinuous Q-function. And remember, we're going to use a bandit approach. So basically what we do now is initialize a bunch of arms in this continuously valued function that discretize the space. And then for each of these, then we basically sample one of the arms and one of these covering bots. Imagine that this is higher dimensional. And then we sample one of the actions within that arm. So let's say we have selected this particular action in there, and sampled a particular action. We tested it. We got a reward back. And then, basically, for each of these arms for this bias, we maintain an average reward and the number of samples that we got. And we shrink the radius of the spiral, of the covering ball if it's much multi-dimensional, dependent on the number of samples. And then you see that you actually have some space that is not covered anymore. So we continue creating new arms, basically, and continue sampling in the space. OK, so this is basically like a visualization of this algorithm for learning this discontinuous Q-function. OK, so now the inner loops is maybe less interesting, but it's so important to know that. So basically, the inner arm is providing the reward to the outer loop by actually learning the manipulation of task. And here, we use A3C, just like a model-free RL method. And one thing, of course, we wanted to know, is it actually faster for the second robot to learn this manipulation task if the fixture is placed in the right way. And here are basically a bunch of learning curves for all of these three tasks that we reconsider. And maybe let's look at the peg insertion one. So you see, in the orange curve there, that if a fixture's place in exactly the right place, the learning curve is basically much steeper. So it learns much faster on how to do it. And then if you have a suboptimal placement or null fixture, it takes much longer, actually, to learn this task. And there is a similar story with the other task. OK, so now we went through this two-loop approach of having the outer loop with the multi-arm bandit, and the inner loop that provides the reward, basically based on how fast it learns. And if we put this together, here's the visualization of how this learned Q-map actually looks like for a fixture post-selection than a test type. So here, for example, these three pictures for this particular orientation of this peg. And you see that you are expecting a high reward if, basically, this fixture is placed in such a way that it's easier for the robot to place it. SIDDHARTHA SRINIVASA: I have a quick question. So you talked about the reward being how easy it is for it to learn. JEANNETTE BOHG: Yes. SIDDHARTHA SRINIVASA: But I guess you motivated uncertainty by saying how easy is it for it to execute. How are they correlated? JEANNETTE BOHG: Well, that's a good question. SIDDHARTHA SRINIVASA: [INAUDIBLE] basically [INAUDIBLE]. JEANNETTE BOHG: So I guess how easy it is to learn, in the learning curve, you see based on-- I mean, the learning curve is basically the success rate. Which means that early on, the robot has a higher success rate, which indicates that it's also easier to actually execute this particular task. I mean, basically, you have the exploration space cut physically, actually, by these fixtures. So therefore, the task becomes easier. SIDDHARTHA SRINIVASA: I guess that's the testable hypothesis. JEANNETTE BOHG: Right. I guess the task becomes easier-- well, it becomes easier, because even if you do wrong motion against a fixture, you want to actually go there. And assuming that the fixture is placed in a right way, that means your only escape route is either downward or backwards. So it's literally, like, the space of wrong motion is basically reduced. So therefore, it should be easier to do the task as well. SIDDHARTHA SRINIVASA: I mean this is-- I completely understand that that's the case here. It's not a generalizable statement. [INTERPOSING VOICES] SIDDHARTHA SRINIVASA: You can compute easier minima, harder minima, easier. One is the question around the ability to find the basis of attraction. The other is the question of the stability of that attraction. Like, you can have a shallow minima that are unstable. But yeah, I see the point definitely for this particular case. JEANNETTE BOHG: For this particular case, OK. But I agree with you, it's not generalizable. Was there a question as well? AUDIENCE: Is there [INAUDIBLE] that holds the fixture state, or is it compliant? Because if it would be compliant, it will still if it's a bit off-- JEANNETTE BOHG: We control this pretty stiffly. I mean, yeah. AUDIENCE: So it can't-- if it's covered a little bit of the hole, it won't allow it to still to go in there. JEANNETTE BOHG: I think, in practice, there is some movement. But it's still small enough for this to have an effect. And really, we don't need another robot. I'm going to show you a video. Actually, let me show you this video. So this is why we need the robot, because we can just let it run. But we could just have a person here, or some other way. It's just cute to have a robot help the other robot. SIDDHARTHA SRINIVASA: It was showing what it was hallucinating. JEANNETTE BOHG: Yeah, so this was actually doing the CEM. So literally, it had the Q-map basically learned. And then now, it's sampling it, fitting the Gaussian, sampling it again. And this is for the three tasks that you're seeing right now. So this task is also easier, because now you can't hit the wrong orientation. And here, this is a bit more complex over many [INAUDIBLE],, because there are two steps-- rotating in, and then sliding out. Also in this case, the better, we can't move up here. Sorry your question was, if it was stiffly controlled. Like, yeah, it definitely has pretty stiff gains to not give in. Yeah. OK, so one thing I wanted to also show is that it can actually generalize over pack geometry. So again, we have trained this policy on the square peg. And now we just we use exactly the same one for these other peg geometries, because it's a similar structure of the problem what will actually work. Also, this was only trained in simulation, by the way. And we just replay it, which was surprising to us, that it worked so well. AUDIENCE: Just one question. Did you constrain how long you were around the inner loop RL? JEANNETTE BOHG: Yes. There was a maximum time. AUDIENCE: That would be smaller than if you just-- like in your plot, where you show performance and learning curves. JEANNETTE BOHG: Yeah AUDIENCE: And then RL was trained probably much longer and learn much slower. JEANNETTE BOHG: Sorry, I didn't quite understand the question. So can you repeat it? AUDIENCE: So you said there were many times that outerloop would be caught. And then they would then evoke the innerloop RL. I'm just wondering how long, then, each inner loop RL would take. Because if each inner loop RL would still take a long time, if your pack placement was just unlucky, and you prevent this task from completing, it take a long time. JEANNETTE BOHG: And there is some maximum time to run it. And then basically, the reward is the success rate of this policy at that stopping time, so that it doesn't run forever. And then during training, we sample a new arm, basically, in this bandit approach. AUDIENCE: So the hope is that if the placement is good within this limited time, you would already be able to solve the task. JEANNETTE BOHG: Yes, exactly. OK, so just to summarize this first project, basically, the main idea was here to learn to place physical fixtures to funnel uncertainty, which is an idea that has been had in automation. But there, it's typically engineered. We also made an algorithmic contribution here, which is this particular algorithm for learning this discontinuous Q-function. And the result is that, really, if you have the fixed replaced optimally, then actually, the learning of the manipulation task is dramatically sped up. AUDIENCE: Quick question about that. So you are comparing having a scaffold and if it's optimal or slightly suboptimal it does speed up learning. JEANNETTE BOHG: Yes. AUDIENCE: But then you have to do a lot of learning in order to learn the scaffold placement. Is that, then, a lot slower than learning without the scaffold had it just been detached. JEANNETTE BOHG: That's a good question. Well, I guess the good thing is that, once you have the scaffold there, as I showed, right you can actually generalize it to many different things. Also, we did learn that in simulation, and it did generalize as is to the real world. So it's probably longer if you have to learn for all the fixtures-- I mean, for all the fixture placement in total. But because it's a physical reduction of the possible actions that you can actually do and after failure modes, it actually works pretty well in the real world as well. AUDIENCE: So why do we need to learn the fixture location? That's something you can pretty much infer from geometry. JEANNETTE BOHG: OK, I mean, obviously, we made really nice fixtures that are clearly really useful for this task, and we did constrain the exploration space for these fixtures as well. And yes, for these tasks, I look at this. I know exactly what to do. But just imagine tasks that are maybe not so obvious. And then it could be interesting to first, maybe given a fixture, to have basically a method figure this out autonomously. And also, we didn't actually assume that we know anything about the geometry here. That's maybe another thing that I should mention. We didn't let the robot know anything about the squareness of the peg. OK, but one question that I actually had as a backup slide was about what happens if you take this fixture away. Let me just go there. So exactly as Sid said, like the goal of putting the training wheels is then to actually take them away again. And so we wanted to know, OK, if we take now this policy that was learned with exploiting the fixture, is it still going to work? And it's not. So if you just remove the fixture as this, and actually then maybe try to train again to do it without a fixture, it actually doesn't work so well. So what we tried instead to say, OK, what happens if we actually, instead of a physical fixture, use a potential field that is virtual, and place it at this optimal position while we will also removing the fixture? And then you can actually start adapting to the case where you don't have these training wheels anymore-- in this case, the fixtures with the potential field. So we found this was actually pretty interesting, because now you have a virtual fixture. So you don't even need another robot. You don't even need a physical item that you maybe have to print again, or whatever. You can actually play around in simulation with the geometry of these potential fields to see what is actually a good geometry, for example, for a fixture, and is pretty cheap now to play around with if it's virtual. So this is a pretty interesting direction for us as well. Does that answer your question. SIDDHARTHA SRINIVASA: Yes, and again, I think that it's interesting that it works here. And I guess another way of thinking about your potential field might be that it's a set of subjective priors on the policies that would work. I'm also curious why the potential field would not be symmetric. I mean, you have a L-shaped fixture. It was really-- the value value was-- JEANNETTE BOHG: I mean, this looks like an inside experiment. We just wanted to know, OK, exactly as you said, like you said, you want to remove the fixture as well. OK, how do we do this? And this was more like an initial experiment. But I think it's interesting to now explore, OK, how can you optimize over the geometry as well, which we haven't done. SIDDHARTHA SRINIVASA: No, that's really interesting. I mean, I think there's this problem around, like, Lagrangian relaxtion, for example, to solve constraint optimization problems, where you put a big barrier, and then you relax the barrier over time. And one can imagine what you're doing here would be like a physical version of it. You put the barrier in place, and then you soften that. But I guess the curiosity I had was that the history of peg and hole insertion, the way you solve the peg and hole problem is you do the deliberate miss. Then you slide it in. And then you move something, and then you jiggle it in, and then you have this remote center of compliance. So it is quite fascinating that this works without it. But you can imagine cases where you remove the training wheels, and you just relearn a different policy. JEANNETTE BOHG: Right. Oh, I see what you mean. So you would have expected this to maybe actually find a completely different strategy. SIDDHARTHA SRINIVASA: For example. I think that part of it is in the ingenuity of the fixture design. Like clearly, when my kids are learning to ride their bikes with their training wheels, when the training wheels come off, they don't have to relearn the new strategy. JEANNETTE BOHG: Yeah. SIDDHARTHA SRINIVASA: So it is remarkable to me that we can design fixtures that, when they are removed, the true policy lies within that class, which is interesting. JEANNETTE BOHG: Yeah, so I think this was, like, an initial idea, just to see what the effect is. And I think it opened some really interesting questions that you are also bringing up right now. If you have this policy that does exploit it, does the policy without a fixture actually lie somewhere close to that one? Maybe not, and we found, actually, that it's not. That's why we left this potential field in there. But yeah, there's some interesting questions. AUDIENCE: Does this allow you to add noise into your motor movements and still recover the feature set. So humans are-- JEANNETTE BOHG: What procedure set. AUDIENCE: So recover the policy. So humans are really bad at being accurate at movements. But if you go to a carpentry shop, we use fixtures all the time to replicate it. So we can use our noisy movements to still get precise features. Can you do the same thing here, where you basically can just add noise into your motor signal-- JEANNETTE BOHG: I mean, basically, we use a real robot. That means there's noise in the motor signal. AUDIENCE: Right. But can you up the noise, even beyond what the real robot would do? JEANNETTE BOHG: I think actually with, this physical feature, you can. It's not that we tested this, because especially-- well, actually, this upper one is a Franka, which is pretty good. But the low one is a really crappy Kinova arm. Sid, we should also talk about that new Kinova arm, which I'm really disappointed by. SIDDHARTHA SRINIVASA: I take all the blame for-- JEANNETTE BOHG: No, you shouldn't. We can just commiserate maybe or something. Anyway, I don't want to blame anyone, except for maybe Kinova. All right, so I had a summary. Let's maybe move on to the other two projects. So we talked about this one, which was the learning how to scaffold. Now let's talk about a different problem, where we wanted to do task and motion planning, and the scene is actually changing. And you want to, of course, always execute it in the real world with lots of noise and uncertainty. So let's look at this. So this is an example that has actually turned up in Marc Toussaint's work from RSS 2018, more or less. So you basically have this robot that wants to place a small box on the large box. But the problem is that the small box is actually outside of the workspace. So it can figure out, by combining symbolic and geometric reasoning, that it can use this hook to actually get the little box closer. And then reach for the box and place it on this shelf. But now, what happens if you have actually moving targets. So here, Toki is actually moving these things around, moving the little box around. And of course, we still don't want the robot to throw its hands up and just give up. It should actually be able to adapt and still actually execute that particular plan. OK, so that's the problem we want to solve. And we basically adapt this particular approach from Marc Toussaint's work from RSS 2018 that is logic-geometric programming. And we take it and we reformulate it to be able to do what I just showed you. So I just want to quickly go through this optimization problem, which basically just combines a logic planner with a motion optimizer. So essentially, here, we have the terminal cost and the objective function. And here, we have this trajectory cost. Looks pretty standard. And the x and u here are the continuous state and actions. So in Marc's work, this was, for example, the joint ankles and whatever-- the joint velocity or accelerations that you sent. And then here, you have all the constraints. So for example, this is the continuous system dynamics, which actually changes based on this discrete state that comes from this logic planner. It's like a manipulation primitive. And we also have a discrete action. Here's, it's a. So basically, this continuous system dynamics change is dependent on the discrete state. And also, this particular constraint describes how the instantaneous system dynamics are when the discrete action ak here actually changes. And this is basically coming from the logic plan. So you are in a particular state of the system-- a discrete state. You take an action primitive. And then you end up in the next successor state. OK, so this is this particular optimization problem down here that, basically, Marc proposed in his paper. And for the logic plan, he just uses something called STRIPS planning, which basically has a bunch of propositions and actions that have pre and post conditions, a bunch of objects and initial states and a goal. And then given these particular predicates and everything, you can essentially plan out with this tree logical plan. So this is something we actually adopted. We didn't change. So we just used that. But what we did change was the way the trajectory optimization actually works with this. So this is the formulation that, basically, Marc used in this paper. And here, it's basically the objective function of the sum of the trajectory across all time steps. And that basically results in a band [INAUDIBLE] that can be averted efficiently and has nice properties. The constraint functions, though, as you see here, they depend on the entire trajectory. And that makes it pretty complex and couples them in time. And the main problem here with this approach that we see is that actually, once you plan the trajectory in joint space and the environment changes, you basically you have to re-plan and get it completely. It's going to be invalid. So we really thought about that problem. Like, how can we deal with this dynamic environment? And we reformulated this approach. Instead of with joint angles, we are actually using relative Cartesian frames instead. So for example here, what we basically wanted to achieve is to decouple the constraints that are now not coupled over time anymore. So what do we have here? Here, for example, I've shown two steps in this plan. We have this arm. The next steps I want to do is to actually grasp this box. So therefore, it wants to control the end-effector, the origins here, and align that, basically, with the target box. And then once it does that, it wants to put it on the little shelf. So now it wants to control, actually, this particular box frame, and put it in some position relative to this little shelf somewhere. So the nice thing here, the way this is decoupled is that-- when an actual robot is grasping this, it's not going to be sitting nicely in that arm in exactly the way you wanted it. No, it's going to be in a really weird way. But that doesn't really matter. The way you pick this object up doesn't really matter if your target is defined in terms of relative frames relative to this target frame. So if you only control the box relative to this box, then it doesn't matter how you have grasped that box. So that's basically what we exploit here in this new formulation, and how we get rid of, basically, this problem-- that if the environment changes, you have to re-plan. So essentially, this is how our optimization problem looks, where we essentially plan a sequence of key relative Cartesian frames relative to the different frames of the objects in the scene that we are looking at. And so this slide here is basically these relative Cartesian frames. SIDDHARTHA SRINIVASA: Quick qualifying question. It makes a lot of sense. There are certain things that can't be converted to relative [INAUDIBLE]. Like for example, in workspace, depends on the corner frame of the robot. How do you address that? JEANNETTE BOHG: Yeah, so that's true. So by getting rid of the joint state, for example, we also can't deal so easily with the joint limits, which is annoying. And with the work space, that's something we do in the symbolic planner right now. So the constraints of that are basically encoded there. We don't deal with the joint limits, for example, or with the singularities or whatever. OK, let me show you the full system. And it's actually an interesting question. Which layer is taking care of what? There's not a black and white answer here. But here's what we did. So we basically have a planning execution layer that is interleaved like that. So we have the symbolic planner that does the STRIPS planning, and produces this action skeleton. And then we have this frame optimizer that plans the sequence of key frames of Cartesian frames. And then what we do is actually use a reactive controller that-- in our case, we just use an operational space controller that basically just tries to align the [INAUDIBLE] effector to the box, or tries to align the box frame to bring it to that desired relative Cartesian frame, essentially. And now that means that, if the environment changes, these relative frames that are attached to these moving objects are still exactly valid. And therefore, the plan is still valid. So essentially, we do the STRIPS planning once. We keep it fixed. Then we also optimize once these key frames of Cartesian frames. And then we just use a reactive controller that continuously adapts to these movements. And we control this in the kilohertz. So now what would be nice, and something we haven't done yet, is if we basically maybe, when the environment changes, determine, like, actually we want a different Cartesian frame. We maybe want to re-plan, because this and that condition changed. Or we want to say, oh, I actually want a different symbolic plan. So these kinds of things we haven't yet addressed. It's one of the open questions. Does that answer your question? SIDDHARTHA SRINIVASA: I thought you had done both of those. It's good. Now it makes sense to me. JEANNETTE BOHG: So this, we haven't yet addressed. But it's a very interesting question to look at. So here is basically what comes out. So here, you have the Tower of Hanoi. System. And because we use the STRIPS one, we actually get multiple possible solutions out. And then of course, the Cartesian frames are adapted to this here. I mean, you know how this was going to finish. And I already showed you this, where now that we have this optimization of all these relative Cartesian frames, the plan stays valid, even if things are moving around. And of course, we also wanted to see how this now actually works in the real world. So we basically implemented or integrated this entire system on a real robot with a model-based tracker. And we had to actually combine it with markers in order to make it robust. So both of them were necessary. So there is some work to be done there. But essentially, we can show here that, even though if we perturb the objects and everything, this plan still works out. All right. Yeah, just summarize this particular approach. So here the main idea-- so basically, what we wanted to achieve is to have a planner and our task motion planner that actually has a valid plan, even if the object or the environment changes. And the main idea was here to optimize the Cartesian frames. And the algorithmic contribution here was to essentially reformulate this trajectory optimization problem in terms of these frames. And therefore, we achieved this task and motion planning in dynamic environments. And I think I'm actually out of time. So I don't think I going to talk in detail about this particular project on manipulating deformable objects. So I'm just going to really briefly maybe talk about this. Essentially, what we wanted to achieve with a robot that can basically bring an object that has a very high-dimensional stage space into a goal image. And we wanted this system to be robust to occlusion. So you see the robot is actually occluding the object. And this goal image isn't even of the same appearance as the object that is being manipulated here. And we have all these annoying sources of uncertainty. And we want to be able to solve it with our system. There's lots of annoying dynamics that are difficult to model. So we essentially had an approach using receding horizon control Basically, we used MPPI for planning out how to do this manipulation. And the main contribution here was really the perception model, which means you basically have to perceive a 64-dimensional state. And it's pretty amazing because it can solve all of these difficult situations. And it can even deal with intersections and everything. And if you're interested in finding out how this actually works, then you can look at this paper. So you've already seen this. But essentially, under the hood, we represent this object in state space-- this linearly formable object. And we propose a differentiable renderer that allows us to do self-supervised learning and robust state estimation. And then we have, basically, an entire system that can do this manipulation part. OK, so with this I just want to close. So I showed you at least two of these projects in detail, and just wishy-washied over this one. And basically, in these three projects, I really tried to impress on you that maybe instead of dealing with much better perception methods or much better prediction methods, maybe we just have to accept that this will never be perfect. And then shift the focus towards other solutions on how to deal with the uncertainty that is always going to be there in the system. And yeah, so that is pretty much what I wanted to say. Thank you very much for your attention. These are all the students who did all this amazing work. And these are my funding sources. Thank you. [APPLAUSE] AUDIENCE: A quick question about the last part. So you said the novelty really represented this rope as [INAUDIBLE]. JEANNETTE BOHG: Yeah, so I would say it's a bit hard to argue that this is a contribution, because something in the physically interpretable state space has basically been done all the time. But it's more that we chose to take that representation in the physical meaningful space of this configuration of the rope. And then what this allowed us is to use an interpretable dynamics model from a physics simulator. It allowed us to easily compute the cost between the goal state and the current state. And then the contribution was really in this perception model. So I wouldn't say that our contribution is that we chose to represent that in state space. But what was your-- sorry, I forgot, actually your question. AUDIENCE: Yeah, my question is if the-- [INTERPOSING VOICES] AUDIENCE: Yeah. JEANNETTE BOHG: So the way we deal with this is basically by this top-level part, by using a receding horizon controller, essentially. Or this is basically an MPC model, where we say, OK, we just want to plan an optimal sequence of action over a certain horizon. We only execute the first part, because we know perception is going to be imperfect and there's probably going to be errors. And then we check what actually happens and we plan. And this part up there is relatively easy to do, because we chose to do an interpretable state representation or use test. AUDIENCE: And you trained the perception model, then, in a supervised way? JEANNETTE BOHG: Yes, so what we did was a two-stage process, where we first trained it in simulation, because that's easy. But then of course, this doesn't transfer to the real world. So what we did there was a pretty-- I couldn't talk about this. It's basically a self-supervised way to learn from real examples. So the robot is essentially playing around with this rope, and uses the model that was trained in simulation to get a rough idea of what the configuration is. And it uses a differentiable renderer, basically, to improve the labeling of these play data, if you want, and then uses that to fine-tune the model as well. So basically, there is some process where the entire system does automatic labeling, and then retrains to actually work in the real world. AUDIENCE: So the render mode matches the internal model, then, to the observation source? JEANNETTE BOHG: Yeah, basically, it looks like that, where we have basically this input image, and the perception networks give us an initial estimate. We render this initial estimate, and then find a model, basically, of how this looks like. Label, again, the image based on this model. And then we compute an image space lost to basically back-propagate to the rope, and basically improve this, or to actually fine-tune the perception network. And this part here is basically independent of the actual appearance of the color. It's like it trains in parameters that are actually being estimated based on whatever data it sees. So that's why it works with the green background and the white rope, and the red rope on the white background, and all of these things. So that's roughly how it looks. AUDIENCE: Were there motivating domains for this work? JEANNETTE BOHG: What are the motivating domains? So any kind of thing you want to do with a rope, for example. Maybe you want to do suturing in automatic. I mean, this is pretty far out, to be honest. But that could be a long-term vision towards this. So we haven't really looked at knotting yet. That is something we are going to do now. But any kind of thing you want to do in a household where you want to rope something together. Also we want to go towards two-dimensional objects that are not just ropes, and try to see if we can extend this towards-- and then you can think of folding. Yeah, these are the applications. SIDDHARTHA SRINIVASA: Cool. Thank you so much. JEANNETTE BOHG: Thank you. [APPLAUSE] 