 to be able to introduce Alec Radford Alec Radford is a research scientist at open the eye alec has pioneered many of the latest advances in AI for natural language processing you might be familiar already with GPT and GPT to which Alec led those efforts at open AI and of course earlier in the semester we covered BC GN which was the first Gann incarnation that could start generating realistic looking images and that was also led by Alex it's a real honor to have Alec with us today and yeah now Mike please take it away from here yeah totally I'm super excited to be here and present because this course is like my favorite research topic unsupervised learning and yeah just really excited to chat with you all today so today I'm gonna focus on the NLP and tech side and I'm just gonna start the timer and today I'll be talking about just kind of generally learning from text in a scalable unsupervised kind of fashion kind of give a history of the field and some of the you know main techniques and approaches and kind of walk through the methods and kind of where we are today as well as providing some commentary on kind of supervised learning versus on supervised learning in NLP and why I think you know unsupervised methods are so important in this space yeah so let's I guess get started so learning from text you know one of the I think prerequisites to kind of start with is standard supervised learning requires kind of you know what we'd say is machine learning great data and what I mean by that is kind of your canonical machine learning data set is something at least in an academic context is something like you go use a crowd worker pipeline and you very carefully curate gold labeled standards for some data you're trying to annotate and this is a pretty involved expensive process and you often are emphasizing kind of quality and specificity and preciseness to the thing you care about the task you're trying to predict and maybe a very specific targeted data distribution and what this often means is you get a small amount of very high quality data and even some of the largest efforts in space just because you have paid human feedback often involved and sometimes your ensemble in predictions of three five or more laborers it's often a few hundred thousand examples is like a big data set especially for NLP in computer vision you sometimes see you know things like imagenet where they push that to a million or ten million but those are kind of afar outliers and you know very many canonical NLP datasets might only have five or 10,000 labeled examples so there's not really a lot of machine learning great data out there at least compared to what kind of the current learning complexities and efficiencies of current models are you know one of the primary criticisms of modern supervised learning deep learning in particular is how data intensive it is so we really have to get that number down and this lecture is basically going to be discussing all the variety of methods that have been developed for using natural language that kind of is available beyond kind of the machine learning great data and unsupervised or scalable self supervised methods for hoping to somehow pre-trained do some auxilary objective or tasks or you know hand design some method that allows you to improve performance once you flip the switch and go to a supervised learning on the standard machine learning great data or in the limit as we'll talk later get rid of the need entirely to have a classic supervised learning data side and potentially begin to learn tasks in a purely unsupervised way and evaluate them in a like zero shot setting so there's a variety of methods this lecture is going to focus primarily on auto regressive maximum-likelihood language models they're kind of the core and I think they're the most common uniting thread that kind of carries the early days this field through to kind of the current modern methods but I want to you know make clear at the front that there's many proxy objectives and tasks that have been designed in actual image processing to somehow you know do something before the thing you care about in order to do better on my thing you care about and there's quite a lot and in particular in the last year or two we've now seen that area really kind of grow dramatically and in many cases they now I'll perform the standard language model based methods that I kind of will as the core of the presentation and we'll talk more about the details of the differences as we get to those parts so some more motivation in intro as we've kind of going so I think I think one of the ways to think about this is like what do we do with the Internet so you know the wild Internet appears and you can either have your glowing brain ask representation on the left we can laugh at or we can make it you know how messy and random and weird and difficult it might be for algorithms to learn from it on the right so that's good old Geocities and so you know there's a lot of skepticism I think about kind of these approaches that might kind of at the highest level look kind of silly or kind of whimsical to be like let's just throw an algorithm at the internet and see what comes out the other end but I think that's actually kind of one of the like one seven summaries of basically what modern NLP has been seeing a lot of success from and you know I think one of the reasons why is just the Internet is so big there's so much data on it and we're starting to see some very exciting methods of learning from this kind of messy large-scale and curated data and so there's a great tweet from from an LP researcher just kind of showing just how is are they big and you know kind of just massive the Internet is where you can go and find an article about how to open doors and you know there's often a lot of arguments saying that oh you know we're not going to you know and it feels wrong in the limit to be like yes let's just throw algorithms at the internet and see what happens like that doesn't match human experience that doesn't match kind of the grounded embodied agents that you know we think of you know intelligent systems and instead is this kind of just like processing bits or abstract tokens and so there's a lot of skepticism about this approach but I think that just quantities of scale and other methods play very well with current techniques and you know you see lots of arguments about things like oh there's this long tail and we're never going to be able to deal with composition and really it's just maybe brute force can get us surprisingly far in the in the new term not saying that these methods or techniques are and I'll be all but at least today there's I think strong evidence that we shouldn't dismiss this somewhat silly approach at a high level so let's start with kind of I think what would be the like simplest starting point that we can convert from this kind of high-level idea into something that looks like a machine learning algorithm so we process a bunch of texts on the internet let's say and we're going to build this matrix called the word word co-occurrence matrix and so what we can kind of think of is it's a square matrix where the ith entry corresponds to for a given word like water the count of another word and whether they co-occur with each other so it might be you have to define when a co-occurrence is so that just means that the two happened to be present together and you might define a window of this for instance they both occur in the same sentence or within five words of each other or in the limit you can go quite far with like just happen to occur in the same document on the internet and so you're just gonna brute force kind of countless it's just counting that's all it is we're just going over you know tons and tons of text and we're just building up this this table basically so just a lookup table and it just tells you oh the word steam and water co-occur 250 times or you know the word steam is just in the data set 3 to 24 times total or you know the words hot and water you know 19500 forty times so that's all we're doing and this is a way you know one this is incredibly scalable you can just run a spark job over the entire internet with this kind of system you can quickly get this giant table and it's you know I'm not computationally intensive it's just counting and processing and tokenization this thing can be run on a common desktop and get very far and it's simple it's just counting so how good is counting a bunch of stuff like we're we're talking about something incredibly basic it's just kind of how often do these two things occur together and I think you know one of one of the big takeaways that I'm gonna have a lot of during this presentation is just how far these simple methods that are scalable and with large amounts of data can get so this is a great example of a paper called combining retrieval statistics and inference to answer elementary science questions it's from Clark at all AI - from 2016 and what they do is they take the same data structure this word word co-occurrence matrix they did let me start with the task so the task is elementary science questions so it's just I believe through 5th grade kind of you know elementary school kind of simple settings questions so they're multiple choice therefore no possible answers and there are these kind of simple things like a student crumpled up a flat sheet of paper into ramble what property the one who changed hardness color master shape or you know what property of a mirror makes it possible for a student to see an image in it is it volume magnetism reflectiveness or connectivity so this is a kind of thing that like you know again it's pretty basic in terms of like the high levels they're you know relatively simple facts and they don't require all that much in the form of reasoning or comprehension but there's still the kind of thing that we do give to is you know kids learning about the world and so you might think that like oh you know this is the kind of thing where to understand a mirror you really need to you know exist in the world and to you know learn about all these properties or to have a teacher and how are we gonna get there we're just kind of this brute force thing that just counts a bunch of words and puts them into a table and then starts looking them up and you know the takeaway here is that it can work surprisingly well so you can't quite pass these examples so the specific solver that we're gonna use talking about in a second is the PMI solver and that gets to about 60% but random guesses 25% so we basically almost you know have the error rate and get to addy with just this very dumb brute brute force approach so what actually is this solver they call it the point-wise mutual information solver and what you can think of it as is it just scores all of these possible answers so we have this sentence of context of you know the question and then we have you know four possible answers so what we do is we loop over the basically the sentence and we just look for the word toward co-occurrences and we just keep counting them up and we use this scoring formula which is the log of a ratio between two probabilities the first the P of XY is the joint which is basically the co-occurrence and so that gets you that count that's basically looking it up directly from that table the IJ entry for XY and then you normal by this kind of baseline assumption which is that the words should not Co occur more than by chance so that would be just their gonna depend of probabilities multiplied together as you can imagine those may be quite small and product into two gether makes them even smaller but some words co-occur together so a mirror occurs with reflective or you know electricity occurs with lightning or you know crumpled up might co-occur with like hardness and so that's all this method does is it kind of just says these basic associations between words and that can get you surprisingly far it doesn't feel like real learning you know maybe and it does it's definitely not very human-like but it's just an example of kind of the power of basic methods and how something that you know doesn't involve any you know you know intelligence or hand waving that we might make about you know complicated systems it's just a big lookup table you know a smart job but you might run on the internet and it can give you surprisingly far so there's a problem with working with these word to word co-occurrence matrices they're huge so let's say we have a million word vocabulary so we have a million words by a million words just to have the full version naively and then you might store it with in 32 hopefully you don't need them in 64 so that's four bytes so storing this whole matrix in memory in a dense representation is four terabytes you know that's still huge for today most machines don't have that much memory in them so and you know if we were to kind of like start working with like how do we use this system or how do we kind of make it more general you know we just have this matrix and there's you can definitely design hand-coded algorithms to let go look up entries and query on it and we see that they can get quite far but you know we'd like to do more and how does this slot into NLP more broadly so we want to come up with a more compact but faithful representation of the relations between the words and the information they represent and we could just say that we really just want to find a way of representing this J co-occurrence matrix as something more like what we know from deep learning and machine learning in general so here's the algorithm called glove from and in 10 Li the Stanford NLP in 2014 so we take that matrix of word word co-occurrences like I mentioned it's cheap so you can run this thing when we like a trillion tokens and each entry X I X IJ would be the count of word I co-occurring context for J and what we're going to do instead is we're going to you know learning an approximation of this full matrix and the way we're going to do it is we're going to say we're going to redefined a word as a low dimensional or at least compared to you know a million by a million matrix much more low dimensional vector so we're gonna learn a dense distributed representation of a word and all we're gonna say is this very simple model such that we're trying to predict the log prob or the or the log co-occurrence counts of the X IJ entry and then we're going to do it is we're gonna look up the rector representation of word I and the vector representation of word J we're just gonna say their dot product should be proportional to the log occurrence count and that's all this is and so it's really simple and you can just use a weighted like square to error loss so that's what this this FX IJ is a basically a weighting function to account for the fact that some words are way more common and you don't want to over train this thing on like those words and you might also want to like clip because you might have like extremely long tail frequency distributions and things like that so but at the other day you just have there your ID WJ and you had some bias terms and you're just trying to compare that to the log of the rock codes count so this allows us to go from that giant m by m matrix which might be a million by a million to an M by n matrix where there's M words and each is an N dimensional vector and often this turns out that these can approximate that full co-occurrence matrix quite well and they're much much smaller dimensionality so they might be just 300 dimensions and you know there's a question of what does this thing learn and how does it approximate that but empirically it just cannot compress it quite well and this might make sense because you can imagine that so many many words just never occur with each other all that often and in fact simple sparse storage of that full matrix can get a lot smaller already but then we work mostly with them distributed representations these days in deep learning so we're going smash it into the framework we know there's another word of this yep the question so do you still have to first build the full matrix and then you run this or so this is a way of having had the full matrix you then run this as a way of like kind of compressing or we representing the matrix chronic Thanks mm-hmm so now as an example where you don't have to build that full matrix so there's another variant of very similar kind of and I think usually a more well-known version of kind of an algorithms class called word effect and so weird effect is instead a kind of predictive framework where instead of saying we've got this kind of you know abstract like co-occurrence matrix then we're going to try to like compress it and we represent it as word vectors we're gonna just work with natural sequence of text so you might have you know a five-word sentence like the cat sat on the mat and what you're gonna do is there's going to be a model that's trained to take a local context window like you know the cat said maybe two words of past context in two words of next context we're going to do an incredibly simple linear operation like summing them and then we're just going to try to predict that word in the center so this is called the continuous bag of words representation continuous because it's a distributed representation bag of words because the operation that composes the context is just sum or a bag and then we just predict the output and we can parameterize that as like the log probability of the word in the center of the context and there's the inverse version of this which is the skip grand model which given a central word of context tries to predict the window and so this uses kind of the more standard approach of like online training and it just streams over a bunch of examples of text you can use mini-batch training it looks like your standard algorithms now the same way I mentioned some tricks with like the using the log co-occurrence or a real waiting function you need those same kind of things here again many words span many different ranges of frequencies where you might have words like the be literally 7% of all your data so you if you now usually train or direct algorithm without subsampling or resampling based on the frequency distribution seven percent of your computes going to modeling the word though and then you know some important word in New York City or something or phrase is just basically lost in the noise so we use a real weighting function I believe it's the inverse fifth root so it just works and that just heavily truncates the frequency distribution so they're basically doing the same thing today this is a predictive framework where it's it takes in a sequence and it tries to predict some subset of that sequence with a very simple linear model and you just have word of the same word event betting table we talked about but they both do about the same thing and they're kind of the canonical first round of distributed or scalable kind of unsupervised cell supervised representations for a multi again there's there's no you know human supervision classically involved in these algorithms they just kind of have this automated procedure to just churn through large amounts of data and you know we're Tyvek came out of google in like 2013 and you know one of the first things that is written on a big city you cluster with like a very efficient C++ implementation and shove a bunch of words through it and it works really well and so let's kind of talk about what this does so for this graph I'm gonna talk about how I'm gonna interrupt for a moment if you go back yep so on the Left it's the word words are represented by vectors and then your average and you're supposed to get two vector representing the middle word on the right where did he embedding slid they're the same embeddings wte so they're both inputs and targets so you you would basically slice out some word WT from from your from your list you would then also pull a sequence of context to be predicted like the were before and the word after and then you would have the same prediction objective like the whole or all of that word at that location and there's other approximations that kind of just embossing over right now how to do this efficiently because computing a full normalization of the predictions over like a full million size vocabulary is very expensive so you often you can use a tree structure or a sub sampling algorithm where you might normalize over only a randomly selected subset and you can weight that subset and things like this all production negative sampling is a prediction some kind of inner product between WT & WT - - or yeah so that would be how you'd get the logit for the log problems it's a dumb profit as well yeah sorry I should've been clear about that operation thank you cool thanks Alec so yeah what do we do with these things so this is where kind of a lot of the first wave of kind of modern you know modern modern is a contentious word but kind of NLP starting to leverage large-scale and supervised data started figuring out how to use these things so these examples on the left are with glove and what we see is kind of a suite of tasks so there's the Stanford sentiment tree bank which is predicting for a sentence of a movie review is it a positive review that they like the movie or is it a negative review you know like movie IMDB is another central analysis data set but it's a paragraph of context t-rex six and 50 are classifying kind of types of questions like who what where when and SLI is a much fancier thing of logical entailment so it's kind of measuring the relation between two sentences a premise sentence and a hypothesis sentence and you're basically trying to say given the premise does the following sentence follow logically from it it being tailed is it kind of irrelevant or containing information that's maybe correct but maybe not wrong which would be a neutral or is it actually a contradiction with the previous sentence so you know it might be the first sentence is like you know a woman is walking a dog and then the second sentence is like a man is playing with a cat and that would just be a contradiction of the first sentence so that's s Noy and it's some sensible objective and it's kind of this more complex operation because it's doing logical reasoning supposedly and it's doing it over semantic concepts like you might need to know the relations between playing an instrument or you know that saxophone is an instrument so that if the premise is a man playing saxophone you need to know that the hypothesis might be you know in tailing it if it's the man is playing a musical instrument so that one has like kind of an interesting relation to some more semantic content and the final example here is squad which is answering dataset so you get a paragraph from Wikipedia and you have to predict you know given a question what the answer is from that paragraph and so for all of these data sets again this is a pretty broad suite of tasks you see multiple absolute percentage performance jumps from slotting in word vectors compared to randomly initialized components of the models that were used to predict the so you can always do random initialization kind of standard canonical thing and deep learning or you could use these pre trained vectors and so they really do seem to help in terms of data efficiency and you can see in some cases like for question answering that you can get a 10% plus absolute improvement here for glove glove plus code is another thing which we'll come to in a bit and you know why might these be helping so much so that's the kind of empirical data well on the right here we kind of have some of the work that God did to kind of inspect the properties of these word of vectors so they would for instance have a query vector like the word frog and then they would show all of the different possible nearest words in terms of just cosine similarity to that first word so you can see that you know immediately it's the plural version of it frog two frogs and you know toad is very similar to frog Ronna is like I guess more scientific you name and then you get slightly farther on things like wizard so you can see how that can simplify the problem space if we have a distributed model and we have an input that's asking a question about a frog if we don't have any knowledge of the structure of language or the relations between the word frog and toad it's you know naive or basically impossible for that model to then potentially generalize the same question asked about a toad instead but if we have this dense distributed representation that is bringing together these words kind of into this similar feature space then you might expect that well if the you know representation frog is very similar the representation for toad the model might just be able to generalize and handle that and you know there's even more relations and properties which is beyond just similarity in that embedding space you can also get very interesting relations like the concept of like kind of you know like the CEO to a company might all be expressed in kind of the same same subspace or the same direction in the embedding space or connecting a zip code to its city and you can see kind of how you know how could this be happening well co-occurrence is kind of get you this don't they you know Honolulu if you want to predict this random number you've got to eventually figure out that oh these you know do occur together because they are the code of one to the other and so yeah you you get a lot of structure even though again all we were doing or you know one of the views of all these algorithms are doing is they're just processing very local relations in a very simple fashion and it's just scaleable and simple but it can work quite well as a starting point now you know these aren't the end of obviously it's only thirty minutes in two or three hour lecture so there's a long way to go so kind of these are all cool and whatnot and they really did drive the first few years of modern deep learning NLP and helping to move these models to much higher performance but kind of what might be the issues with them so you know obviously languages a lot more than just the counts of words it has a ton of structure on top of than in addition to words and and furthermore context is very important and these kind of fix static representations of words that we're learning are just insufficient in many cases so you might have for instance three different sentences I went to the riverbank I made it withdrawal from the bank or I wouldn't Bank on it and all of them have the word Bank being used in a very different context and you know basically representing a different thing it's a now and a reverb or you know it or just a phrase of expression and so you really need to learn how to do more complex things but if you're just counting whether two words happen to occur in the sentence or you know in word to vector kind of looking at a very short window and just using an averaging operator you you you can't really model all that much complexity so we need to do more and you know there's also just kind of the design space right now you have this million by 300 dimensional matrix so it's like word vectors and then the question is just what do we do with that and you know obviously we figured out quite a lot of ways to use them but there's a lot that's still up to the practitioner and this often involves a lot of tests specific models slapped on top of it and that's where a lot of the first few years of research and NLP for deep learning went was kind of designing all these tests specific models a model for doing question answering a model for doing summarization a model for doing sentiment analysis and they would all kind of take this common input of the word vectors and slap them in but then there was a huge amount of design on top of that and these models got progressively more and more complex with more and more details and so you can kind of think this does like well we only really did the first step sure learning word vectors is great but they're really kind of like learning just the edge detectors in computer vision and they get us something but we know like in you know debugging for computer vision there's a lot more that goes into comment than just some edge detectors at the beginning system and that's true for an LP as well so there's a lot more going on in language beyond just these input representations so kind of how do we get there well we're going to take a little bit of a detour into the history of language models and kind of walk through how this kind of method and kind of set of generative models kind of ended up providing one of the methods for moving beyond just word vectors and kind of introducing the second wave of modern NLP methods that use unsupervised or self supervised methods so fun overview real quickly is kind of seventy years of history here on one slide where we kind of are looking at a language model what is a language model well it models language and it's a generative model so hopefully depending on how nicely it's set up we can draw samples from it to understand kind of what distribution it's actually learned and how well it's actually approximated the real distribution of language so without getting in the details of how you sample you can kind of see this kind of list here so very early there's this thing called a three gram model from Claude Shannon himself in the 1950s and this kind of still makes basically gibberish they also point to ninety-nine point six billion dollars from two hundred four six three percent of interest rate stores as Mexico and Brazil in market conditions well that's basically gibberish but notice that there's still a bit of like local correlation and structure it says a lot of numbers and then it mentions interest rates after six point three percent or six three percent and that's like all kind of right and you can see how there's the tiniest bit of structure in there beyond just like what it would look like I could be just drew words independently according to their frequencies and then there's been a lot of investment in this kind of field in area over the last few years so Ilya sutskever in 2011 kind of introduced a character RNN for the task and so here anytime there's a prompt it's highlighted in yellow which means it's a manually specified kind of prefix and then you condition on that and you sample from that so the meaning of life is the tradition of ancient human reproduction that's almost a sentence it is less favorable to the good boy for when to remove her bigger so it quickly fell apart in the second part but it almost got something there and it's still gibberish but it at least shows another hint of structure and then there's a Rafale or falls paper from 2016 which is basically a much bigger word level version of that RNN from 2011 and just kind of use a scale and a lot more data and here's a sample drawn from it with even more new technologies coming onto the market quickly during the past three years the increasing number of companies was now Ted called the ever-changing and ever changing environmental challenges online so that's basically a sentence at this point there's a weird thing where it repeats itself with ever-changing and ever changing but we've now got a phrase you know multiple phrases or clauses and kind of longer term structure there so that's a big amount of progress and again as we talked about with word vectors a lot of their failure is that they don't exploit contexts and they're kind of these isolated representations of only single words so the fact that these language models were starting to learn context as you looked at and inspected their samples is kind of a clue that they're going in the right direction towards some of the functionality behaviors we might want in a natural image processing so then the next major step came in 2017 2018 with the introduction of the transformer based architecture we'll talk a little bit about that later if that's appropriate but its handles long term dependencies much better through self intention mechanisms and then you start to see potentially multiple sentences that kind of flow together and then the final one here is GPT - which can kind of take potentially a pretty low probability or difficult to understand prompt that you know probably isn't in the training data like scientist discovering a herd of unicorns living in a you know remote previously unexplored Valley in the Andes Mountains and they're able to speak English and then it can write something that looks like a news article on the top of that this does cherry-picked all howl like 20 times i sat there till I got a good one but it's progress and most of these are cherry picked so it's cherry picks again cherry fix all the way down and yeah at that point you basically have something that just reads like a full news article and it keeps characters and names persistent and you know pulls information from the source sentence over you know multiple paragraphs and this is all a lot of progress being driven in the last few years so kind of now that we have just like a look at the cool samples let's like get into the details here so this is going to be about statistical or probabilistic language modeling and kind of the way we formulate this or is we interpret language as a high dimensional discrete data distribution that we want to model and kind of the set up since this is statistical method is we're going to observe a bunch of strings of language and the framing here with a probabilistic language model is we want to learn a function that can just compute the probability or density of new sentences so we want to be able to compute Oh what is the probability of the sentence is it going to rain today and we're just going to give it a bunch of strings and somehow we're going to design a model that can compute this quantity so what does it mean to compute the probability of a string you know what what should the probability of that sum is the cat sat on the mat B well you know there's some people who kind of think that this might not be the most well defined concept or there's a lot of reason for skepticism potentially Noam Chomsky in 1969 has a very famous quote but it must be recognized that the notion of the probability of a sentence is an entirely useless one under any known interpretation of this term of this term so some people were quite skeptical to be fair this is well before that kind of any of the modern renaissance and he goes on to kind of explain a bit more that there's quite likely that statistical methods can work but it's a good example of kind of where we're coming from and some of the contrast in the field so let's instead kind of like talk about why this concept might be useful like why do we want to know what the probability of this is and this is where I think it begins to see the connection between oh what does the generative and how like we end up using it or why might actually learn useful functionality for downstream tasks or for transfer and so you know we could compare for instance the probability of the sentence the cat sat on the mat so the probability of the sentence the cat sets on the mat and you know we would expect that let's say we somehow have the true function here we don't know how to learn it yet but we just assume we have like the ground truth of the probabilities of these two sentences well it should assign more probability to the grammatically correct one and that gives you something like grammar and that's you know an important part of language is understanding its structure and what are the valid sentences or not but you know should the probability of the sentence the cat sets on the mat be zero well no because sometimes people fudge their keyboard or miss type it should be much lower but it shouldn't be all the way to zero for instance and then you can kind of get to more interesting sentences that you could query you could say you know what's the probability in the sentence the hyena stephannie met and compare that to the sentence the cat sat on the mat and you know we would say well as a human being asked this you would say well hyenas you know are wild animals they don't often sit on mats unless they're at the zoo or something so this kind of shows how to do this to compute this probability correctly you would need to start to have world knowledge what is a common operator you know what is a common environment for a hyena you know what is even sitting on a mat mean and then you can ask other questions again you could start to get two conditional probabilities too depending on how you set up this generative model is you might be able to query you know given that the prefix is the substring two plus two equals you know what should the probability of the completion for be it probably shouldn't be one because people sometimes joke that two plus two equals five but maybe if you had bit more context you would be able to disambiguate which of those two you might predict and then finally kind of coming back to some of the data sets or tests we've already mentioned if you have the prefix that movie was terrible I'd rate it one star out of five you you really should know that that is a likely completion and so to do that completion and to generate that sentence and to know that string is likely you basically have to have that language model somehow I've learned what sentiment analysis is and what is a little you know a likely relation between the concept of like one star or five stars the kind of you know reception of the movie or the description of the movie before that and so with that one it kind of becomes clear that in the limit these functions that these language models learn and compute should be quite useful traditionally we approach that as a supervised learning problem right we were gonna like oh let's go build a data set let's go collect you know a bunch of crowd laborers and have them assign ratings to a bunch of different movie reviews that's what the Stanford sentiment tree Bank is but in the limit this kind of unsupervised scalable method of just like fit a probability distribution to strings of language should eventually maybe be able to handle a test like this without any of the classic supervised learning framework being used and yet so that kind of extends much more broadly those are kind of some you know canonical example or some toy examples but this actually can be quite useful and this is kind of where language models got their start in many cases 30 or 40 years ago or 20 years ago in kind of machine learning so they're often used for speech recognition and machine translation which again are traditionally approach to supervised tasks with pairs of transcripts that are somehow aligned and you know a major promise is that we can somehow leverage these language models to you know really help with these problems and for speech recognition for instance you could prune the space of possible transcriptions from an acoustic model there's a variance example from Jeffrey Hanson of you know how to tell the difference between the sentence recognize each and recognize speech you know they're very similar from a you know a raw audio perspective but if you have context you know that these can be quite different things and wrecking a nice Beach is just also a much less likely string than to recognize speech and for translation for instance you could rewrite possible translations based on a monolingual language model so if you have an English to French translation system and you have some proposal of the French translation you could say well hey language model that I've trained already how likely do you think the sentence is in French and there's a lot of work on integrating this directly into decoders and using them as restoring mechanisms Oh statistical language model has really got their start often as in these tasks so let's move towards actually having a computational model of language so first maybe we'll do some pre-processing like lower case so we'll take some maybe messed up text and turn it into just all lowercase to simplify it well then you know maybe said a vocabulary size to just like make the distribution easier to handle to set it to like you know a million tokens or something so we might substitute a rare word like countertop with like an unknown token just so we kind of don't have to deal with this like potentially open-ended probability of observing a new novel where I've never seen before and then finally we'll use something like a tokenizer which will take a input string and return a sequence of tokens so it'll chunk it into a sequence somehow with kind of some rules or logic and you know this is another example of classic and LP work on designing tokenizer x' so we might take you know the cats out of the man and choke it into just the words and you know throw that punctuation on the end there and then you know because this is machine learning we basically are we representing these words as you know unique identifiers or indices and that's again a way to get a window into how a machine learning model really sees natural language you know we come in as humans with so much understanding in context and from like lived experience but you know if you try to train a naive supervised learning model and you started from random visualization it's a lot harder to understand what 223 in 1924 742 followed by 101 23 etc is and I think this helps you get into the mindset of when people talk about machine learning models being spurious pattern matchers or just learning weird correlations that aren't true if you if you've looked at a bunch and it's like tried to do natural and processing tasks as a human where your inputs are represented in this format you'd probably be a lot worse than current machine learning models already are and it'd be understandable if you made kind of mistakes as an algorithm trying to figure out how to make sense of any of this especially once you get to some more complicated task like do these two sentences logically reason or follow each other you could even just do a simpler thing like split it into spaces so there's a huge design space here and I'm just providing a few examples right now okay so there's character level there's byte level which would be kind of working on you know if you just work on characters how do you deal with non ASCII text or text in there you know non a Roman numeral or sorry standard like lettering systems so you could work on like a standard encoding scheme like utf-8 byte stream you could also work on unicode symbols or code points and then there's kind of these middle grounds between word level and character level which would be something like by parent coding and this one actually turns out to be super important so I'm kind of just covering it as part of general NLP methods and it's used by quite a lot of methods in the space now so what this does it starts with the character level vocabulary and it kind of just merges the two most common pairs of characters at a time so you might have T and H be the most common pair of words or characters and then you'll combine them into a new token called you have th and you'll merge that and you'll resub stitute it in all of your words and then you'll run this loop again and so if you run this and kind of just keep merging and merging and merging it learns basically a tree of merges that quickly pop out words full words like the and you know common endings like IMG and he and two and this learns something that kind of lets us handle potentially the full distribution of of language while also having maybe the efficiency of representing semantic chunks like words instead of operating on these characters which might result in strings that are five or ten times or five times longer and require like much more compute and have much longer term dependencies that are difficult to handle then standard board models so if I clear encoding from recur Center is all over the place and is a very common middle ground to back off to character level if you see something rare you don't know or to handle like all these different languages while still having some sort of like kind of sensible handling of common words and frequencies hey Alec is there a common kind of number of bite pairs that you want to end up with cuz it sounds like you start with byte level which is just 256 possibilities and then you could imagine that you can have many many by two pairs and sometimes it goes beyond pairs I think right where you recombine an existing pair with an initial just a bite like the iron and the G yeah yeah when do you stop so yeah that's a good question usually you have a heuristic for merging only across or only within words so you won't merge across like word boundaries with like whitespace or things like that and that just helps with efficiency because otherwise you'll start wasting emerges on things like you know come in like pairs of you know maybe fill our words or stop words and the other thing is you you could just in the limit run this all the way out to a full vocab but we often work on this kind of middle ground where you know to get good coverage of of natural language you often need a hundred thousand plus words and you know in the limit if you want to start having you know common names and places you need really like million sized vocabularies and that can just be incredibly competition expensive so you'll often stick this in a middle ground of like 32 K bps and you're absolutely right that it'll merge all the way up to a full word like you will get things like you know neurobiology in the limit that would be merged all the way with BPA just by doing merges over and over again yep thank you cool so how do we compute the probability of extreme well the dumbest model is we can just assume a uniform like prior over tokens and assume all of our independence we just product they're probably independent probabilities together to compute for any arbitrary sequence dumbest model but we'll start somewhere all right so let's get rid of some of these dumb assumptions so we could suddenly like say well we know some words are more common than others and that kind of word co-occurrence matrix has that diagonal term which is just the frequencies or counts of words so we could use that instead and you know that would just allow us to say well the word these really comments were going to send more probability mass to it and you know the word supercalifragilisticexpialidocious is just pretty rare so this would be called a unigram model where all we do is we just product proportional to the probabilities of the tokens from like the empirical distribution and again we can estimate that just by counting a time we can then go a bit farther and start to exploit context so again we've talked before about how important context might eat and this is where you can start to see you language well begin to handle a potentially so you can say instead that we're instead of estimating just like the you know diagonal of that of that matrix we can use that full matrix basically and say well yeah then that you know we just saw the word the how often is the word cat after the word though and so we can kind of conditioned on that previous token and you know use a modified version of that like look at our count table and start to handle a little bit of context so that's a bigram model by Gram language model but there's a problem of generalization here and this is where kind of counting methods eventually like hit their limit and yeah we can brute force them with all the data on the internet but at the other day they're not flexible enough so let's say you've never seen a word before like self attention you can't assign zero probability to that if you're trying to optimize for like log loss or something because you just get an infinite loss and you know if we just start going to longer and longer strings this count method explodes and the observances of every substring get rare and rare and this just kind of hits a wall so in the like 80s and 90s the way we kind of handle this is we kind of accepted that we couldn't handle the longer term dependencies here and we kind of use clever smoothing methods of mixture models where you might put a lot of your probability on you know the you know grammar by grammar trigram model which is more expressive but you'll smear probability backing off if you don't see a word for instance or don't have a match to that unigram model or uniform model in the limit and so this was kind of what you saw a language models in the 80s and 90s spend a lot of their time on is they kind of were these very they were still basically count tables and statistical count tables but they optimized for kind of achieving something looking more like generalization of a simple form of kind of combining these mixture models and so this is a good review paper if you ever want to kind of go back through the history of this and all the different methods develop there you start to get things that look more like representation learning and even multi-layer models so they'll start doing things like clustering over parts of speech or substituting for that so it's a very hand engineered way of potentially adding expressiveness but it's a good history of kind of where these methods came from so you know since we're talking about NLP and language models is one of the core workhorses here kind of how do you evaluate and interpret a language model well probabilities are often within running or of 0 since language is a huge discrete space in the sentence might you know or a document might just be very long and so the most common way of evaluating these models and saying how well does it do is we use a quantity that's not dependent on the length so we use like the average negative log probability per token and you know this token definition again might be arbitrary character level or might be weird level and so if we're using character level we might convert from you know base e to base two and report bits per character or bits per byte you see a lot of common language modeling benchmarks work in this setting and word level language model is often exponentiate that quantity and report what they call the perplexity and set so yeah it's just giving you bigger numbers and better improvements because you're working on expensive log scale so how do we ground these numbers they're kind of abstract or random quantities you know what is the difference between one point two three bits per character and one point two bits per character especially if you just spent pretty much your life working on a paper and that's the number you got out so you know it's important to understand these quantities our data set dependent it's really easy to guess all zeroes it's really hard to guess the archive and you know you can start calibrating the scales by saying well random guessing would get you you know log two of you know one over 256 so eight bits per character and human estimates from not the best studies but the only ones we've got I've kind of tried to peg on people in the range of like zero point six to one point three bits per character and the best of the models now are often a little bit lower than one bit per character so that range probably is lower for humans and we're somewhere you know getting okay but not matching humans on these kind of quantities and you know way of grinding perplexities is gonna use the same random baseline so it ends up just matching the vocabulary size for like a standard model so random guessing would be a plexi of 50k and one way of thinking about perplexity is as like a branching factor of language so flexi to the N is like the space of possible generations of length then how many your model might assign so you have a perplexity of 10 for a language model and you generate you know to two like word sequence there might be a hundred kind of high probability events in that space and human level estimates again Europe between five and ten and an example though again is this is always data set dependent always problem dependent if you have a lot of well constrained context like in translation these numbers can be a lot lower and best models are often like three perplexity on translation so you're picking between maybe three as possible likely words and you know that kind of agrees with like maybe there's a few different ways for a human translate to census but it's not a huge space by any means so evaluation type 2 is kind of what we talked about so that evaluation type 1 is very much the generative model perspective of like well how good of a probabilistic model is this and so type 2 is instead kind of transfer and the things we're really talking about and caring about more there's a lot of ways we could use these language models so you could say how well does a better language mall potentially improve the word error rate for your speech recognition system or the blue score for your translation system or the Younger suit for your document classification and this is kind of where NLP has really taken off leveraging these language models and kind of the history of the last five years has been discovering more and more ways we could use smarter and smarter language models or better and better language models to do more and more things so let's go through kind of the history here of kind of the sequence of developing real context models models that can generalize better than these kind of count based methods we've so far been kind of using it for all of our discussion so the first one here is surprisingly you know like honestly this paper is amazing if you go back and read it it's from yatra Ben geo and from 2003 and it has a ton of very modern things in it and has skipped connections like you see in things like resonates in 2003 you know it's learning distributed representations of words and this is kind of that core concept we mentioned right at the beginning of like representing a word by a vector with learned values for each location this is like the paper that kind of really introduced this in the neural setting and they were doing like large-scale distributed asynchronous SGD on a cluster even back then in 2003 they had to do it because single mushy peas were so slow so this is like I think it's 64 128 CPU cluster and it would take them I think a month to train a model with like three layers and you know sixty hidden units so this is a still a and grand model but we're using a multi-layer perceptron to compute the kind of conditioning on the context so instead of this kind of you know cow based method we have an MLP that looks at you know the index for reward t minus one the index forward to minus two and you know T minus let's say just a three word context so these three of these vectors can and together of you know the last three words seen we then run it through a hidden layer and then we feed it through a soft max to try to predict what the next word would be so this is a trigram language model still but we've changed the model from a count based method to a distributed setting with an MLP and you know this was kind of the first paper that heroically showed that they could match the performance of some of those super optimized and grand models but again it took like ten days or a month and it was on a giant cluster so you know neural language models really had some catch up to play compared to these smart quick count methods and this is a lot of what took this so long was just unfortunately they do need a lot more compute so then the next major step here was kind of moving away from these fixed context windows which are kind of unsatisfying we know that as humans we can look back in pieces of text and condition on multiple sentences but these kind of methods always so far have had fixed context windows and have only been able to process or condition on just the last few words so this is kind of where our n ends come in and Thomas week loves 2010 paper it's kind of the first modern deep learning version of this that kind of started working quite well so we replaced that MLP with a recurrent neural network and that allows for handling potentially unbounded context now it handles that context in a learn fashion so you'll get an input word vector at one time step and you'll have this context buffer which is a learned memory state that the RNN kind of modifies and updates and you'll use that to kind of represent a running summary of everything you've seen that's important for predicting the next potential word this has potentially unbounded context but in practice we'll train it with method methods like truncated back prop where we only update for and compute you know how to modify the the transition function of the state for up to like maybe 32 words or 64 words so it might be biased in that way but it's kind of just still can potentially learn to encode a lot of information into a learned memory system instead of kind of using like hard coded methods of just like keeping the explicit input presentations so here we get again like probably one of the first real language models where on that previous one we were still using a type 1 evaluation we're just like how well can you predict the next word but here with my cloths paper they showed that if you ran this for a speech translation system you can get actually a much lower word error rate on you not only predict better and you start really improving over the the traditional like in ground based language models but if you look at this word error rate table here you actually see that it improves the speech recognition system so your transcriber will make much potentially like you know over 1 to 2 so the k10 baseline here is 13.5 percent were word error rate so you messed up 13.5 percent words and using all these are nouns together you could actually reduce that by over two points and so you're talking about like a 20 percent error reduction which is quite significant yeah this is like a lot of early language models were actually published in speech conferences because this was such a important and exciting application of them to start with and again you don't need to collect a bunch of speech transcription data here in the limit you could just run this thing over like New York Times articles and then use it to help potentially with your speech transcriber and that's where a lot of the power comes in from an unsupervised scaleable method and transfer capabilities so we already showed samples from this one but it's kind of a slightly different version where all these models so far have been operating on words and kind of pre-built tokenizer x' to split it off and chunk it and kind of fix vocabularies the exciting thing with character level models so it's the same kind of architecture or recurrent Network it approximates a richer transition function where you might have a different set of weights with multiplicative interactions this was back when we thought optimization was hard so it's using second-order optimizers because RNs are scary and we still haven't gotten used to like just first-order methods working well and you know it begins to handle much longer short dependencies when you work on character level you're you're you know suddenly talk about sequences that are five times longer so you start having models that maybe handle hundreds of time steps and that starts you know abstractly meaning maybe you could have a model that could actually parse a paragraph or parse a page and you know it wasn't a lot better than Engram models in terms of its perplexities but it was very easy to sample from and this was kind of one of the first I think demos that people might have seen online of the language model back on some meats University of Toronto static website from like 20 2011 so the next male like not like quick question when you look at character level models versus word level models can you directly compare the perplexity uh if you're careful yes so you know it in the limit these are both just predicting a sequence and if you set it up correctly you could just like you know here would be the simplest way to do it with a character level model sorry I should clarify you can go one way so you can you can compute for a character or byte level model the perplexities that it would assign to a ward level model but some word level models might have limitations like using unknown tokens or out of vocabulary that means they can't actually compute probabilities of arbitrary sentences whereas that someone in the expressive benefits of a character level model so the simplest way to do this would be you would convert the word level model like the token sequence of processed like let's just say it's split on spaces you'll just rejoin on spaces and then compute the probabilities the the character level model assigns and you'll have an adjustment factor you could just sum the probabilities over the full sequence and then renormalize by the relevant metric and we'll actually be using that later to talk about how to compare different language almost more appropriately but again you need to have the expressivity to handle an arbitrary string to be able to compute this and you know old models because their computation is often worked with small vocabularies so they wouldn't truly be computing the probability of arbitrary strings because they might normalize them in various ways got it thank you yep so the next step is kind of going to multi-layer ellis TMS and also introducing the LS TM again even though it came out in 2000 and kind of gotten realized primarily by you know one of the major people that we popularized it was Alex greys and kind of 2013 ish so this is gonna character level model except we now have a gated RNN which uses kind of these multiplicative gates and more complicated transition dynamics to better store state and to help compared to like a multiplicative Arn in with kind of credit assignment and just trainability and you start to get the things that like can handle a kind of arbitrary arbitrary strings of text so you get you know something that's learning how to parse Wikipedia markdown or XML and Andre Carpathia kind of really popularized these models with like some blog post in 2015 showing that they're like you know work full of tech they work for XML they were Python programs you know they're not generating valid things but they kind of can handle this they really have this flexibility of kind of you know feeling exciting from an unsupervised learning perspective you give them some data distribution of like Python programs or something and you just have a you know train over that and then you get something that looks like it's really drawn from that distribution so we kind of like talking out through like a lot of the early work here and although there was one example with the Thomas pickle off paper of Thomas paper of actually having an application a lot of this was kind of just like competing for competing sake on type one evals or like look at the funny samples so again one of the very fascinating things about the last few years of NLP has been how we figured out how to really use these things much more broadly across the board and this is where I think it really starts to get exciting so one of the first papers to do this it was the skip thought vectors paper from Jimmy tauros and collaborators in 2015 and so what they did is they proposed learning a RN and sequencing coder to provide context to in a language model and basically to learn how to use a sentence level feature extractor so what I mean by that is let's say we have a sentence I could see the cat on the steps what this model is trying to do is it first ingest this context sentence in the middle and they call it skip thought vectors because it's you can think of this is basically that skip grande model that was again you take a word in the center and then you predict the word before in the word after this is generalizing it to sentences and it's using an RN end to kind of learn to summarize the context of the long sequence and handle kind of predicting complex dependencies between multiple words so we encode that center sentence with an RNN we iterate over it in the left-to-right fashion and then we have Anna linguish model that predicts the previous sentence so what might have happened before the sentence and then a language model that also predicts the suffix sentence that comes after it so what they then do is they say well you know a model that does this test very well should learn to kind of summarize this sentence in the middle and for our nen's it's you know still these distributed representations so you have this state vector that's representing kind of an alert fashion all of the previous words you've seen so importantly that's now generalized from representations of single words to representations of sequences that can exploit context and potentially handle more complex properties and just big u8 meanings of words and they showed across the board that these models healthily outperformed classic methods like so Cibao with word effect would be the simplest version so you know what does a document how do you represent a document with word effect well one of the future observations you could take is to just average the embeddings of each of the each of the words in the document and that would be what this like Cibao baseline here is on a bunch of different data sets and so you could instead say well we're gonna you know we somehow learned this sorry we somehow learned this sequence extractor we could run it take its feature representation for each sentence and use that instead and you kind of see that these models you know if you use the combined skip of the like bi-directional models and using the forward and backward versions you can actually get these to start to outperform the words effect models kind of across the board and it wasn't really like this paper was kind of exciting especially from the breadth of things they do they have things like image captioning representations that they learn and they kind of show you know with analysis methods like tease me that you see clustering according to classes you know it was kind of like on the edge where it showed some pretty exciting promise and it was you know a lot a lot stronger than potentially a super baseline but there were other still discriminative methods for like training models from scratch that were still matching it with like you know well-designed comment architectures or things like this so although this had be like a very exciting kind of oh it's a learn feature extractor that's able to handle long term contexts and dependencies it it kind of worked but it wasn't like sweeping the sodas away it was you know exciting and honestly I think a lot of people ended up using it more as a language model where they saw some cool demos of having to generate multiple sentences but it never really quite you know blew everyone away from its quality so you know this is like a good early hit but it didn't quite you know it wasn't a homerun by any means and so this is where androids paper from 2015 semi-supervised sequence learning kind of comes at it from a slightly different angle so again for skip thought vectors we just used this vector representation as an input to a model and we fix the model itself and we just like train another model on top of this vector representation and it's a rebekah representation summarizing the whole sentence so maybe that's kind of a difficult test to summarize all the complexities of long sentences short sentences so what died all did instead was they said we'll take this language model that we've learned and we're just going to fine-tune it directly we're not going to like precache the features like kind of words of Exile we're just gonna you know take it whatever parameters that language model learned predicting the sequence we're going to use that as an initialization point for training a supervised model for a downstream task and this is the one that started to get good results and they were showing compared to standard supervised learning on you know datasets with like 20,000 labeled examples and stuff like that that these models could get quite far and so you see in the limit that you know if you you kind of have all of your different baselines here of you know word vectors feeding as inputs but then we could use like a sequence auto encoder or sequence language model and fine-tune that and you start getting quite large drops here and what's kind of cool here is these two different rows here one of these is pre-training only on the IMDB movie reviews so basically the same data set it's a two-stage algorithm and then this third table here or this third row here is using a bunch of unlabeled Amazon reviews and that's you know starting to get towards transfer learning starting to get towards well we can run this thing over a lot of data and as we get more compute we can just get more data from the internet we can feed in more and we see that that actually improves things significantly over only using like the small standard supervised learning dataset in isolation some of this might have just been at the time that it was difficult to Train language models and train Armand's back in the day but you know as we'll see for the rest of this lecture the methods have kind of continued to hold on top of this and continue to make progress this is the first one where it got a strong soda and you know there were strong bass lines before and people started like really I mean well to be fair it came out and not much work happened in the space for the next two years but it kind of and a lot of that was because it like really just killed it on these sentences datasets and not not as much elsewhere and this really took some further work to kind of figure out how do we make this a generalizable approach that works kind of everywhere the same way that like plugging in word vectors does so moving back one one moment to a type one evil there was a followed paper or a neck in the next year that kind of really started to push on scale and compute used for training language models as we mentioned before they've kind of always been compute limited so this was a that Google paper that showed kind of the first language of all that could generate something like a coherent sentence and a lot of it was using a larger data set so the billion word benchmark was a big data set at the time and they use a an HK hidden unit projection LST M which is basically a low rank factorization of like the transition transition matrix just to keep the parameter count down while keeping the state size hi it's character aware with some improvements that let it process the character level inputs so you kind of see on the right that this is starting get to be a kind of complex system and then they throw you know a large vocabulary they throw a 32 King 4k 40s at it so 32 GPUs for three weeks and they kind of really got a huge improvement over the previous results and at this point those old Engram language models the old statistical methods were in the mid 40s or even in the 50s and 60s were hybrid systems and suddenly you're at like 23.7 so you basically have this metric you know again it's exponentiated so it's actually like a 20 percent reduction in like just actual log loss but you know you're starting to see a lot of significant progress at this space just throwing scale at it and this has ended up being you know something that was really developed just to push it and say how far can we get you know sentence quality can we start to get something that looks like coherence and one of the surprising results is it turned out that this actually paved the way for further methods even though it was just designed to be a really good language model and just better predict the next word it ends up laying the foundations for talk about in a little bit called Elmo that really was the first one to crack how do we use these Ellen's all over the place and start seeing it working for question answering and you know summarization and all these different domains so there's kind of a bit of the Tibbett here we're at an hour should we stop for a little bit or let me check out a stopping point I'm gonna go a bit farther we could go to about an hour 30 and stop for a little bit longer there is my kind of conference yeah so you know I've motivated scale a little bit so like I mentioned there's a whole internet out there there's so much information and that perfect language model would you know basically from one view need to fit the Internet into its parameters given how big it is it's not surprising that we're going to need a big model to do that we're going to need a lot of compute pretentiously to do it to get as close as possible and for many of these tests we're talking about where you want to learn long term dependencies we want to learn complicated tasks you know they might be quite rare they also are quite difficult so you know the closer you get the better you are to maybe learning real interesting behaviors first kind of a basic system that just like is locally plugging a few words together so another you know just vivid way of pointing this out is a small character RNN is basically gibberish you know this is what happens you know this can be a very good architecture but if you don't give it capacity it just can't really learn language you know there's so many words there's so many objects there's so many relations you really need a lot of expressivity to handle all that complexity and you know another way of putting pointing this out is classic resources that were built by humans trying to map out kind of like the relations between all words in natural language you know build hierarchies over them so there's there's really heroic efforts here like wordnet they were larger than many of the language models we were still training especially a few years ago so it might have like five point five million relational features in this package and you know when you have it zipped on disk or unzipped on disk it's like already 55 megabytes and you know a lot of common language model is especially early on we're only a few megabytes for parameters and so we know this is probably going to be very inefficient and you know we're probably going to need quite large models and right now you know the answer we have so far is to kind of address this facts with scale and you know hopefully we do find out more efficient and we'll talk a bit about that later too but right now you know kind of the first dumb thing you try is brute force if we scale and you know another reason why this is worth investing in is it's now a very well validated empirical trend so across the bottom here is for both language modeling and and for like computer vision kind of the performance of models laid out on log scale plots where you see you have a large scale x-axis which might be the amount of words you train on so every new tic is a doubling of the data set size you know block scale is not great because it quickly gets inefficient but these trends are incredibly linear they're very predictable so like it's almost like the natural kind of domain to think about is like what how does this look on a log scale and you see that again for language models on the right left and for like the performance of like captioning cysts or sorry image consecration systems on image net in the middle so these are quite consistent trends and they span now quite a few orders of magnitude so so far they've continued to improve from 6 million parameters up to 600 million on like image net and you know data set sizes spanning that's probably over two to two orders of magnitude or two orders of magnitude there yeah and also computers becoming available as investment of more resources in Jim machine learning and AI and improvements in Hardware and distributive training have kind of allowed for you know even though there's there's this logarithmic or this heavy demand for additional compute to see kind of finite sized improvements at least as of yet kind of the industry as a whole has been developing techniques and systems to keep providing that additional compute to keep these trend lines going so that was kind of just a quick digression on likewise scale might be important and it really intimately plays into like where these language models came from and how they kind of had their success so here's a like kind of a cute example looking at kind of starting to get away from just learning these kind of feature representations that could then be reused by downstream tasks towards maybe we can learn the tests themselves without having to have standard human label feedback and kind of shared that intuition with like the talking about the you know computing the probability of the string I rate this you know one star out of five after seeing the prefix of the of the product review so this is a paper I did in a 20-17 which was like kind of a very targeted experiment here and one of the hypotheses I was working on was that maybe just data was the model neck you know our models are so inefficient that if we were able to just tile kind of in an unsupervised fashion the landscape of one domain we might care about like product reviews we could maybe do quite well so we made a much larger dataset that's our rather we used an existing data set from from I think UCSD and Amazon in partnership which had 40 gigabytes of text so that was way bigger than that billion word benchmark and it's all in just one domain and we trained a byte level language model on this for you know a reasonable amount of computer month on for tiny Nexus the model ended up under fitting a lot but you know one of the most interesting things about this is if we go and poke into that model and say well you have this hidden state that summarizes everything you've seen and we do probes over that we found that actually there was a single unit within this language model which very vividly indirectly just computes a running estimates of what is the sentiment of the characters I've seen so far in the review so you can see that you know as it turns on this is one of Michael Crichton's best books and so we have green colored as positive and red colored as negative so again there's no supervised learning going on here this is all just unsupervised prediction of a byte stream it just sees a stream of bytes 40 billion in a row and they're all just you know numbers 0 to 256 and it somehow figures out in order to better predict this text you know it recovers this useful feature which is well as this review gonna be excited or you know dismissive and you know it can handle complexity where you know I can switch from a great start you know it's something where it's like you know you know here in the middle seriously the screenplay and the directing were horrendous and then it suddenly drops off and it's you know performance analysis and starts going negative you know I can't fault the actors I know good novels especially are hard but this may be the absolute worst disparity in quality between a null and screen adaptation forever so it really does it and it turns out that if we just threshold on this unit so we're not even fitting parameters we're fitting one parameter it actually was matching these old words avec or by ground baselines and even things like skip thought vectors and it's just a single unit in the model and we're just running it over this over the document and you know threshold the value at zero and so this is a histogram for positive reviews and negative reviews of what this system does so this is kind of showing I think in a very clean and pure way how you can really do some unsupervised representation learning here and start to learn something that really helps potentially with downstream tasks it's very hand engineered it was very targeted we knew that like you know product reviews sentiment is a very important feature so we were kind of really hoping that something like this would happen and it would learn a really good representation but it was you know it's still like kind of shows a proof point when with limited scale but lots of data you can get something done here a follow-up work we did was with scott gray was pushing on kind of model signs again so we said maybe hidden state size is the bottleneck so again these standard LS teams and RNs summarize the entire past context as a fixed length feature vector and so that might be for a standard model in a big model like 4096 units or that were false model was 8 K units and you know if we had like three hundred dimensional word vectors if you naively just concatenated them into your that state representation you could only handle like 30 in a row with like a you know an 8k or 9k state size that's only about a sentence or two so we thought that you know maybe it just turned out that models were really limited by their state size and so we pushed on these kind of blocks sparse methods that kind of allowed us to train with much larger state sizes that we would factorize the weight matrices such that they would be represented kind of as this two layered system of having a dense sub dense block and a lot of sparse blocks that are pruned away and we saw that these were slightly efficient more efficient in terms of parameters and they also worked better on things like set analysis when evaluated by these linear models which is like a standard probing for how good of a feature representation have I learned that's partially because when your model is just like lots of features and that's all they wear their expressiveness comes from you know when your summer ability is easier and high dimensional spaces and yeah I was this was kind of like explaining some of the history of I was pushing on trying to get these things to work and figure out how do I like really you know push their performance potentially and so this is like showing again that that performance analysis of these units learned by these models so this is how that kind of representation evolves and we show kind of data efficiency on the x-axis here so in the limit we know there's that zero shot performance of fitting a threshold to zero examples and that actually turned out to be about about here on this graph if you use all the data to probe and find it but if you just fit kind of naively as you saw more and more data you you know could start with like in the limit only needing 10 labeled examples to beat some of the original supervised learning baselines which just train systems from scratch there's this recurrent neural tenser Network paper from a searcher at all very early do planning work here with a you know really cool complex model and we were able to imagine it with just ten labeled examples whereas it was trained on all 8,000 in this case and then as we kind of keep adding more and more data we see that the representations learned by these language models can be quite powerful and you're kind of able to like quickly sweep through kind of in the limit you know if you don't have any pre training you started getting into these increasingly complex and desperate desperate maybe a judgy word ensembles of 30 different models to hit sodas and then we're able to just use this model that exploits this unsupervised learning on a lot more data to push significantly higher and then that small world improvement with blocks bars had another large jump above that and so this is kind of one of the precursors that or kind of heralds what's about to happen on every task over the next few years this is 2017 as like this field really starts taking off so we mentioned this kind of cool and interesting thing of learning a single feature within one of these networks that kind of really shows some representation learning going on so there's another really great paper I love here from Royce warts and collaborators in 2017 that I think again starts to speak to hey these language models that are you know recurrent networks or more expressive neural networks are really actually learning something interesting and beginning to be useful for downstream tasks that might be difficult so this is a data set called the story closed task so what you do is you have a paragraph of context in this case Karen was assigned a roommate for her first year in college yeah they go to a music show together and it goes really well and then you turn you're trying to train a system to predict which is the right ending and which is the wrong end and so this fits very cleanly or this is what Rory was quite clever about was realizing that this fits very cleanly into the generative modeling framework you could say well what is the probability of the right ending versus what is the probability of the wrong ending and again as we get better language models they should start to learn to exploit context and assign correct like you know the correct probabilities to these different strings and so very early work kind of took the classic supervised learning approach of just throwing you know a you know a model maybe with even word vectors pretty trained at the system and treating it as like a binary classification task but in this case the story close task it's difficult to generate this data they only had 2,000 labeled examples so a purely supervised discriminatory system really couldn't get that far and they actually were basically not performing much better than random and so what Roy was able to show is that well if you exploit tons more additional data which was available of like training on small short stories and then you use this model to score the endings so it just produces a single scalar which is like the ratio of the probabilities is sinky my trick that we talked about before but commuted for a language model where you say well what is the probability of the N being given the story and you normalize by the probability the ending in isolation and this this trick just helps a bit compared to just computing only the probability the ending given story that actually still works quite well but you get a fair amount more and so they were able to significantly improve the performance on this data set again in the limit just using that single feature the RN NLM features here they got a almost 10% jump in performance just by using the generative model off the shelf there's no discernible it's not exploiting statistic you know spurious correlations here because it doesn't see any labels it's just fitting a threshold of what it already thinks is the right ending versus wrong I'm being you know another quick inner loop of scaling so these kind of all are happening Nestle together and I think this gives kind of a sense of how you know research feels often involved where you see these different authors and different people pushing down different lines of work and then kind of things come together in exciting ways so this is work from Noma shows here really just pushing on maybe parameter counting the bottleneck you know maybe that's what's been holding back language models and so they really went crazy here and they they train models that have these what they call sparse again a mixture of experts layers so you have your standard LS teams and pink on top and bottom of this model and then in the middle you sandwich in what's called its mixture of experts layer and what this has is it has a gaining network that decides to pick basically a two layer fully connected Network and it says which ones just slotted in for this given word so you think that you know maybe you want to memorize a lot of information and when you see you know they went to the city blank or something the mixture network and the gating Network will say oh I should load up like you know the expert that handles you know where cities are in the world or is this kind of just a hand wavy high-level intuition and particularly when you train this thing at large scale because it's sparse only one experts being evaluated for any given location at a time so you can group these and you can have many of these experts being trained in parallel and so they're able to push to like you know an eye-popping 137 billion parameters in this language model it's all on this very specific sub module but it ends up being more computer ficient and it has like a lot of clever and very impressive system engineering work to handle how do you run this thing at scale and you know have it be efficient when handling so many parameters there so now we come back to type to evals and kind of the standard slutted in and see how it does and this is like really the paper that kind of set this field off it's called a elmo from peter's at all this day i to work again and they or elmo is the name of them all but it's really about deep contextualized word representations and this is kind of where there's the clean mark between the word defect error and the like language model era and so the way era and so the way they do this is they kind of cleverly say well what do word vectors do they slot in kind of as inputs and they rear nth you know this discrete identity tour ID identifier token of like you know word you know cab being ID 256 with a distributed representation as we discussed before things like contexts are missing in this case so this paper talks about how to use a language model to do the same thing they're substituting the input representation but instead what they're using is a deep bi-directional language model so this is kind of the schematic here where they have a for Dallas TM that will first take in its own learn word representations and it right over the sentence in a left/right fashion and then they want to you know have context not just for sentence words that happened in the past but words that might be about to happen so they also run a backwards L s TM in the other direction from the right to the left and then they have this Bihar cable or sorry you sees me a deep model with multiple layers so they run two layers rails TM and then what they do is they learn weighted averages of the word vector so maybe for some low-level tasks you only want those input representations but maybe for some tasks you really want that kind of long-range context and so you might want to use the higher-level layers and so then they rear nth instead of feeding in those kind of like one-to-one look up in a table what the word vector is they have this RNN language model that processes the sentence or a piece of text in both directions and it learns to you know reuse its hidden state representation as the input to the model instead of the word vector representation so kind of seeing all those early results like skip thought vector showing that well you could learn and distribute representation of the sentence this one does it but it does it at a word level and it just cleanly slots in where word vectors used to go and so what this is quite nice by is it allows you to have very direct comparisons with prior work and across the board they basically show that like simple baseline models which substituted to use these representations instead to reward effective representations we're outperforming very well-engineered very tuned state-of-the-art systems that were like squeezing as much performance as they could add award vectors and they're getting you know quite large numbers here where you see you know 10 20 % relative or sorry yet relative error improvements and importantly they kind of have that clean sweep of very many different tasks like question-answering entailments coreference ner so even classical tests like you know part of speech recognition like and you know this kind of really just swept everything and it was very clean it kind of like made clear that okay you know it word vectors were great but it's time to you know here comes the new thing and you know the other very important and fascinating thing I find about this is this model was that language model that was developed or the limit of all they used for this system is that language model that were fall developed in 2016 at Google to just along with co-authors like Orioles that they really were just pushing on four-plex keys they're just pushing on how well can we get a generative model this text and then you know two years later or you know two years later someone just was like wait a second this thing is learning amazing representations and you know those two works are separated by two years and completely different research labs and they just discovered that you know these language models are really doing something here yeah so that's kind of like really where things turned and you see you know again looking at data efficiency that when you're at very low amounts of data you get huge improvements like 10 plus percent absolute improvements so that really feels like you know as you get more and more supervised data you can begin to overcome the limitations of you know training from scratch but in the limit you know you want to use as little data as possible you want to learn as quickly as possible so this is like very exciting and it's kind of like really got everyone to start stirring and paying attention to this field yeah so final one before the break is kind of you could think of it as pretty much in the same vein as Elmo and what we did instead is we took a better language model again so transformers came out and we were really excited by their ability to handle longer range dependencies and they were also very computer efficient so you you could train them quite well and quite fast so we swap out the recurrent network or the LS TM in the language model for a transformer based language model and if we want we could talk a bit about a self attention and transform based architectures in a bit but now just think of it as like we subbed in a different better architecture and it's slightly larger we use a similar data set of books it's the same data set that skip thought vectors introduced slash trained on and we we just fine-tune it the same way that Android I it all did and this exciting thing here is we saw that we no longer needed these tests specific architectures for each task so you know a lot of the cleanliness of Elmo was that because it was just substituting the impact representation you could reuse all those and engineered architectures and often they would you know counteract for the issues of handling long term dependencies in or an end with like an attention layer the like but they still require you know that engineering of each these tasks for each of these different architectures which means that you're still leaving performance on the head room you know it's not like where you're initializing the middle layer features of a CNN instead of like the edge detectors of the lower layers but then we still are sticking new layers on top so we were trying to like really kind of move towards a general-purpose framework that kind of a lot of Surrey is the same architecture everywhere and not have to have as much of these tests specific engineering which requires a lot of effort and time and grad student hours to like push those systems for so we have this transfer based language model and we kind of showed that for a fair variety of question of tasks primarily classification we kind of take the same model and without having to modify it or introduce new layers we could just fine-tune it with only a linear classifier in text typed on top and we could across-the-board do quite well and in many cases we were outperforming ensembles the same way that elmo is doing before and using basically the same unified architecture to perform quite a lot of different tasks and the glue benchmark had recently come out as like kind of a standard multi test benchmark and this is kind of one of the first major ones to bump up accuracy there and reduce the complexity of it and you know there's two particular things that I'd like to focus on for discussing some of the results from that paper which is if we oblate the number of features transferred we really see that this is a 12 transformer 12 self attention block model we really see that you need all those layers and the random initialization of higher layers was not working well at the time it may be you know as always that you figure out better initialization methods and you can close that gap but you see kind of cleanly that we're transferring a deep heart you know a deep distributed representation and the you know the deeper it was the better it was generalizing and that seemed to hold true across multiple data sets and was a very clean kind of performance increase as you just transfer more and more of those blocks so Elmo is a 2 layer model and now we're going to like a 12 layer model and then this rightmost graph is really the one that I want to focus on and this kind of links together some of the hints and pieces we've been seeing so far through like many of the different papers which is kind of this interesting behavior sometimes the language model is learning a supervised task or a task we kind of thought needed supervision to classically trained in the machine learning framework without any direct explicit labeling or supervision of it so what we did here is we took this transform language ball and we kind of design these heuristic ways of having it compute probabilities the same way that right where Schwartz was doing and we kind of started to extend that beyond just you know the very specific thing like which of these two sentences is most likely so like for instance we could take a language model and do exactly that example at the beginning and ask it well you just saw a movie sentence review do you think the word very positive or very negative it's more likely after seeing this sentence so this would be this probe here which is sentiment analysis in blue and so we show over the course of training this language model we evaluate this kind of zero shot performance probe and we call it zero shot because and this is a broader you know we didn't invent zero shot by any means but it just means evaluating on a task or data set or a class that we've never seen before and we haven't done standard supervised learning to update the representations or to train the model to do this and so we see that kind of as you train you steadily improve performance we've normalized test performance so that zero is random guessing and one was the overall state of the art do you still see across the board that these models are you know nowhere near soda and often they're less than 50% of between soda and random guessing but they're showing clear and steady improvements and they're showing that even on tasks like question answering you could actually you know take a paragraph of like a question answering task and asked it well which of these answers do you think is more likely and you know there's no supervised training here it was trying to predict books and then you ask it like a 5th grade science question and it starts to sorry I shouldn't answer promotoras it so much but you just probe it you know you can compute some conditional probabilities from it you start to see progress being made on you know some potentially quite far afield task the final point to make your to is self attention and transformers really seem to help a lot here where as we did the same exact model or you know it's equivalent size and similar compute with an LS TM and we were seeing that especially on the zero shot tasks sometimes it could do relatively well but on some of them especially ones that handle long range dependencies you really need these self attention layers handle long term dependencies cool so I think it's we're at about half time for the lecture and I think that's probably good time for a break then fantastic Alec thank you let's let's take a break till 6.50 specific time well about eight minutes does that sound good yeah okay great and I'll pause the recording for a moment in here I'm sure if you have a certain like limitations on how large your Monica tree life is everything you need to run in like a particular device and you can to like train a large model or a cartoon elephant this one are there any strategies yeah I mean so I admittedly have been emphasizing the need for scale but it's kind of a continuous spectrum thing and there's some work we'll be talking about later that kind of focuses on efficiency and kind of how far you can push models of a given capacity in size you know probably the answer here I think from a pragmatic perspective is to kind of use whatever is the largest thing you can fit into the given device framework or kind of you know resource specifications you have but then kind of really pushing on how far you can you can take that thing and some of the methods and techniques that have been developed especially in the last year or two of kind of increased efficiencies by factors of maybe five to ten so there's I think a lot of promise there from you know really pushing even with a fixed size and many of those still fit on single GPUs yeah thanks cool yeah so yeah I guess given it seems like the class has gone over transforms a few times I won't do the super detailed version here so yeah I guess we'll just kind of look through that real quickly so we've kind of talked right now so far about standard mostly standard language models and kind of using different architectures you know character level aren't ends and 2ls TM or double all those teams and transfer based language models and they're always kind of trained with the standard auto regressive left right or in the case of Elmo adding a backwards right-left language model and you know that's nice it's a it's a clear framework it allows you to compete probabilities easily it allows you to sample kind of in just a iterated fashion it's not the fastest but it's quite simple to do you just feed in the sample in the distribution over the next word and then you feed that as a new input and conditioned on it and then resample and so it's it's it's a very clean in like general framework but it may actually not be all that optimal so it's cool and exciting to see some of the things that these language models are doing and some of the work as I was just mentioning has really pushed farther by walking away from that very explicit like left/right auto regressive language modeling strategy so this is a common leaderboard it's called the glute benchmark and it combines a set of like nine tasks together and this is this was pretty important for the field to kind of standardize on the set of tasks people reported on as you can imagine especially early on when the research is kind of scattered and not all that standardized you kind of see you know people picking their favorite benchmarks I was totally guilty of this myself I really cared about seven classification just because I happen to you know find that to be an interesting task and worked on it a lot and so you know my favorite report is that a classification someone else is to report on you know in tailmon and someone else report on crushing answering you gotta have this lack of commonality and comparison points so the blue benchmark came in and said we're going to standardize we're gonna focus on 7th level comparison tasks primarily and we're going to kind of have a suite of them and we're basically gonna say hey you should report on all of them so you can't hide your bad results on one and this helped drive a lot of progress too so this is a screenshot of kind of where this leaderboard has gone showing all these new improvements and methods so there's the BIOS TM Elmo baseline autumn that we mentioned and GT one would have slotted in slightly above that but then there's at now ranked 20 from Jacob Devlin and crew and then there's Facebook ai's Roberta as another big jump and so we saw it like on the average metric here which kind of just averages the performance across all these different tasks it went from 70 to 80 between the biles team elmo bench baseline - yeah - burn so that was a big jump there and then an almost equally sized jump happened with Berk - Roberta which we'll talk about in a bit and then there's newer things like Elektra and t5 so this kind of whole area has really you know really exploded in the last year two years in terms of the amount of teams and basically every major research lab Microsoft like Stanford and why you like pretty much you know AI too you see a huge amount of people you know I do every everyone everywhere has been kind of pushing what they can do on this this kind of benchmark and really seeing a lot of progress so we're going to go through kind of each of some of these improvements that these are highlighted select a few there's many others so sorry if I dropped here you briefly was a cool recording spec on so an SST too is like synth analysis like we mentioned before so it's kind of a you know again a diverse suite of tasks here so how do we kind of what are these big improvements we're seeing beyond the standard left-right language models and there's one more point Domanick which is there is a human baseline here and it's slotted in actually in the middle it's in 12th place now so what does it mean like are these models actually better than people and you know the answer really is no and it's complicated and confusing and we'll chat about this a bit more later and supervised learning is always playing tricks on you but you know now these models are like really really you know went from like in the last two or three years because of leveraging unsupervised pre training and kind of scalable methods to really make quite a lot of progress in this space very quickly so this is Bert so what Bert does is it basically finds a very great way to hybridize a language model objective with kind of the importance of like bidirectionality so again you know by default we have this like left/right auto regressive factorization where we say given the previous words predict the next word in the language model that's like what GPT one does and so what we see with that is you're not able to exploit context for the right you're not able to see you know you by masking the model you have to prevent it from being able to just look at the next word and say well I see my sequence that it's cat so I'll just learn to copy it over in predict cat so that has a major limitation and when we when we release GP t1 we actually like weren't able to do well on or you know we found that some of the question-answering datasets we just couldn't do as long because we weren't able to exploit bidirectional context whereas elmo was the old bi-directional image model and with an LST M and you know because they trained afford one of their backward one and the average of the representations that works quite well for shell models and that gets you that my directional context and that can help a ton and you know they performed us I still have performed this on some data sets because of that and then Bert basically figures out how to have bi-directional context within a self attention model and the way they do this is they change the objective so they're no longer doing this like standard you know maximum likelihood training on like just the data distribution they use this kind of proxy tasks called mast language modeling so again you know at the bottom here we could see left-right LM is like the cat sat on the and then you blink out a word and it's supposed to predict math right language modeling would be we'd go the other way around and same at the onset cat mask and predict you know there and so what master LUN does is it just takes your input sequence and it's it corrupts a few locations 15% in the case of bird and it trains the model to predict what's at those masked locations so you know might in in this case there's no like left requirement to write requirement it just randomly selects 15% and this allows you to have bi-directional like representations you can't leak the word because it's masked in the inputs whereas for a standard left-right right-left Ella you kind of hide that this is probably one detail of self attention models that you have that you have the self attention matrix and that kind of defines the connectivity pattern between different locations in your sequence of inputs that you're processing and so you used masked self attention matrix for standard left/right language modeling or right left wing which following where you masked the upper triangle and that prevents you from that future blinking and so you know we say bi-directional context that corresponds to training the same self attention transformer basically except you don't longer have this masking to prevent future locations like J you know after I being able to have I look at and attend to position J after I so that that's kind of the architectural detail that corresponds to this change and it kind of makes sense that having that ability to look at both sides of context helps with disambiguation it helps with information processing and you know information flow through the model because you know the model can like query back and you know for things like question answering for instance if you have a if you have the question after the context you can't update the representations of the context you know in a left-right Auto regressive model after you've seen the question because they're mass then they're hidden from it so the model isn't doing any you know right context dependent processing but in bird it can actually you know bidirectionally attend and quickly passed information forward and backward and this is just what you see if anyone who actually does like self attention architecture from scratch on a supervised test they always use bi-directional or almost always instead of no mess mess self attention matrices and so this turns out to have a huge boost on glue so that that bump I believe between GP t1 and Burt was like they went from I will GPT one had like an average of 78 or something or sorry excuse me I think this got reworked and sorry we excluded WN a lie it was like a bump of like five five plus percent so it basically got a double the Headroom on gbg1 and they show with very careful controls that for the exact same model in the exact same setting you know it does look quite a bit better so bidirectionality makes sense also for sentence comparison tasks like entailment where you have two sentences you're comparing really want them all to be able to compute them and tend back and forth between them and you know look at one and then the other that just seems like correct behavior to do whereas 251 would just go left right and then you'd be done so yeah Burt ended up being kind of the thing after Elmo Elmo kicked it off especially in the research side and got a lot of people to start investing in this space bird is kind of the thing that moved this to the point where suddenly it was like you know ready for like more commercialization or you know production ready basically and so this is now deployed in Google search and its really like kind of showing up everywhere you know if you go to basically any leaderboard burg variants are often very near the top now and pretty much most NLP tasks and just like GPT one they use the same architecture everywhere they remove the need for kind of having these tests specific modules on top and so this was another like incredibly strong step so you know that was Bert I guess there's one more point to make which is because it's predicting these masking tokens it's only predicting like you know you have to set that mass percentage and by default it's often said to like 15 percent so you should understand that like your left/right model it actually predicts a lot more words because it'll predict the full sequence within a single fordpass whereas by default you'd have to run a Bert you know model like six times two on average see every predict every token so it turns out that they often learn a bit slower early but then they just keep training and they begin to learn how to use the bidirectional representations to their benefit and then they continue to outperform left/right language models now the problem is you can't sample from it and it's no longer quite as clear that it's like you know you can't compute a correct I'm normally like correctly normalized probability over the sequence without a lot of work there's some research and to figure out how to do this with clever methods but kind of it removes some of the elegance of like sampling and having easy density or probability estimates for kind of trading off this representation capability so Roberta is if we go back to this leaderboard the next big jump up from 80 point five to eighty eight point one you know kind of you know like as a benchmark or you know important event it kind of you know solidly is above the supposed human average baselines here so what is Roberta Roberta is a very well executed engineering refinement on Burt it's it's a good example of how so often in this field kind of you know the second pass at an approach with maybe the same very similar model architecture algorithm I've can just by careful engineering and fine tuning and tweaking still have tons of extra Headroom to it so they better tune the hyper parameters they remove a few hacks that the original Burt had so for instance the original point for computational reasons predicted most of its training on a relatively short context length and I believe out of 28 tokens and then right at the end of training double that two times two up to 512 tokens for prediction and so they just train at 512 the whole way through it's the same model capacity it has the same runtime per sequence length but they just have you know they spend the pre-training compute to buy that and when you're thinking about deploying the system you know one of the important criteria to realize is especially when you're talking about a system that might get deployed broadly and used across the you know across the world and many different companies once it's released most of the compute is going into inference time it's not actually going into training time and so that means that if you have a method of getting further performance improvements by spending more flops at pre training time often it can be quite worth it from like kind of a full ecosystem view of where is the compute being spent this is one of the counterintuitive things I think about how you think about these systems so they also do better data generation it turned out that the original bird kind of from a simplicity perspective cache the masking so they only actually masked the sequences once and they always print mask location and so you can simply change that to an online setting where you keep sampling the mask and that helps with overfitting and they also use a more flexible vocab scheme these kind of a full BP scheme that can do kind of full utf-8 byte sequences so you can handle any string at least with the standard byte sequence representation and then they just train longer with more compute so as we mentioned before bird is only predicting like one of six tokens on average so that just means it's under train for the coolant amount of time and you can actually just keep training it longer with more GPUs and continue to see higher and higher performance and so I mentioned Roberta Bert was on the leaderboards everywhere well now about you know eight months later it's Roberta everywhere on the leaderboards and that's still true today largely except for a few like targeted things is I think if you go to the our general PD leaderboard you're gonna find that model in the first place is some variants of like a row burger or something so that's like an example again of where you know it's not you know like there's no super clever new algorithm or approach or you know and even for bird it's a pretty you know it's a pretty precise refinement of previous of like gt1 but it can have a huge impact when it's just well executed and you know is I think somewhat you know exciting from one view where it's like okay we're kind of really finding that there's a lot of fertile ground here and with like kind of the right tweaks and you know clever insights we can continue to make further progress so this is where a lecture comes in and this is like I think one of the ones that first shows kind of another new interesting algorithmic potential improvement and someone excitingly shows that it's much more efficient so we mentioned the masking for bird so there's actually this kind of gap here which is the problem is when you were training you're masking all these input sequences and you know you may sample the masquerades so you you're kind of crunching your inputs but then when you want to train a test time or when you want to transfer to some downstream task it doesn't make sense to corrupt the inputs right because if you were doing some analysis and you mask the token you know this was a mask movie you don't know if it's going to be a great movie or a terrible movie in that mask location so bird just kind of like as a few tricks to minimize this impact but if the other day it's kind of this train test gap where you trained it with one distribution with mask inputs and then you want to test it and and and predict with it on a different one and it turns out though that gap actually looks to contribute to some performance issues the other gap is again it's only predicting 15% of tokens so it may also be learning slower than it could because he would have to F crop six times to see the same same predicted segment of data so when Elektra does is a very clever hybrid system so they have a bird or basically a mini bird inside of it so it's the standard math language modeling technique and then you sample from it and you say well you know for that first word that we've asked what do you think is the right word so sample from it's uh its distribution over next tokens and then you're going to feed it into this discriminator which is the actual Electra model and what its job is to do is to predict whether or not the token at any given location is the original token or a replaced token so if the generator gets it wrong again it might sample something kind of reasonable and kind of correct like cooked verse eight the job of the discriminator or the Electra system is to just estimate is this the correct one or wrong not so it's just a binary classification task but it's done at every location it's basically saying was this input corrupted and that allows it to you know one it has a natural distribution and it may be because this math language model could be quite good a lot closer to the real input distribution so you don't have this shock when you transfer it for your downstream tasks and you also speed things up because you're taking a loss and propagating a gradient for every location because you're always estimating is it the correct one or the wrong one and that's like still can be a difficult task for every location rather than like kind of the degenerate thing for like 80 percent of tokens which is just like the egg and B function for birth and so when we look across the board here we see that this model kind of the standard models and you get like glove or selma over spurt and they're all kind of like smashed up right here on 0 because and your TP one starts to move over and then you get kind of you know roberto scales with more and more compute and then the graph keeps going it is hidden you can see that electro is kind of across the board can be quite a lot more efficient and often by like factors of 5 for kind of equivalent performance on a data set so that's quite exciting and in the limit they show that for instance in Elektra a small electrical model so quite a lot smaller than even a GPT one by exploiting bidirectionality in this dense training function can actually outperform GPT one in two days on a single be 100 whereas gbg1 was 25 days on 8p 6000 partially this is because I have P 16 verse F P 32 but it really shows like how you know I think unfortunately some people have and it kind of makes sense because I've talked about the importance of scale what not you know some people have kind of like written this whole subfield office like whoever has the most GPUs is gonna win and you know oh it's all just training bigger models and maybe you know as a new when you're a grad student or as a hobbyist I don't have access to the resources to do interesting work in this space but if paper like electro it's really exciting because it shows that you know a single commercial GPU can actually still have very interesting results in this space nominally they still run the foreign version of the model on a TPU pod but you know there's here you're already having last year's model being beaten in a day or two on a single GPU next year so I think that's a very exciting point in this is from Clarke it all blew the system for Google Clarion I think it's Kevin sorry his first name yeah so it's it's really exciting work here there's this final one kind of this is like kind of the deluxe result coming out of space from Colin Rafal and collaborators at Google and this is like kind of after the first crazy year of like well there's you know bird and now there's Roberta and others you know like you know all these things coming out one after the other every few months bumping up the leaderboard this is the paper that like took a step back and kind of more systematically studying the space analyzed it used a lot of compute to do it but kind of really brought a lot of things together and kind of very carefully curated it's a it's a treasure trove of information for this space it's 50 pages long there's pages and pages of table so with hundreds of numbers and them so it can take a while to work through it but I really recommend it is like one of the ways to like get up to speed on this whole area and all the techniques and all the different ways so they again systematically study this so their standard language modeling objectives there's bird style masking there's there's there's their own kind of things like spam based extensions of bird and then they also look at differences in the architecture so there's your standard left/right language model there's encoder decoders which could have like bi-directional encoder that processes like the previous sentence kind of skip thought style and then a autoregressive decoder and then there's a corporate hybrid called a prefix L M which is a single well you could have untied weights but you think of it as like a partial and tying of the masking and a self attention matrix where you allow some part of the sequence to do bi-directional attention like in the past and then you switch over at some point to doing auto regressive like language modeling so you can get the benefits potentially of for past contexts doing bidirectional representations or in the limit if your downstream task is always just going to be bi-directional you can just run it in purely bi-directional mode so it's kind of trading a hybrid system and I think that was also a quite clever improvement they had the other thing I really like about this paper is it goes even farther in terms of elegance of kind of this shared framework for doing all tasks and all predictions so kind of one of the trends has been moving away from these custom architectures to the kind of shared pre-trained models that are a little bit more monolithic and can be used across a wide range of tasks with high performance and so tv5 you know typically like you know what do t1 and Bert the only difference we would do is we still flawed in the linear classifier at the end you like predict which of the right classes it's correct so what t5 says instead is and this is extra something that Brian McCann and collaborators at Salesforce introduced about two years ago is they basically say we're gonna phrase everything is like pure natural language pure question-answering or something so we're going to give the model like you know a command or a prompt as the prefix like translate in English sentence to German and then it'll give it the English that is good and then t5 just through natural language rests that - you know dust is good or something and you know for all of these it does this basically so for the coolest sentences it'll predict the language phrase not acceptable and for you know STS STS B here's a kind of almost silly version where it's a it's a continuous valued sentence similarity prediction task and then they just have an output discreet token 3.8 so it has the but you know because it's pre-trained it's learned kind of the continuum of numbers and the similarities between them but it's kind of like this for me to see a regression test reframed as discrete token prediction and you know again it's quite general you can do summarize and everything and so we saw a little bit of this what's like when we were probing with like where Schwartz's work just using natural language like probabilities from language model or like some of those zero shot transfers transform like GPT one and so like to five really goes through and shows yeah you can actually exploit the natural language that it's learned and that helps with the transfer and helps with the fine-tuning tasks potentially so yeah to five it's a really good kind of overview of like all the work in this space and then they also just threw a big model on it to at the end in that gets you another bump on those leaderboards that we were talking about so that's that in fourth place though now so others have done some more things on top of it so that's kind of I think the core like set of literature I wanted to cover here and ideas at this point we've kind of gone through the history of kind of language models and how they've been adapted and used across kind of you know the winding history here of how kind of NLP like kind of really took off with these and supervised and self supervised methods and kind of figured out how to use them and all these different papers that kind of found pieces of the puzzle and propose different methods that it or didn't work and kind of combined well with other you know modeling improvements and everything I think it's a really cool story I'm excited that I was able to chat through that with y'all today the last bit here now we still have about fifteen minutes left but we should maybe leave a little bit for questions at the end to is kind of just a bit of more high-level thoughts on this is an unsupervised learning course why do we need it you know what's wrong with the current paradigm of supervised learning and I'm sure you see motivation you know and there's been great discussion already on this topic here but kind of I would like to share a bit of my own like thoughts and opinions here so you know I think a motivating thing here again kind of we've had this thread running through a lot of the discussions so far on this in this talk has been how well does supervised learning work and you know what should we expect of it and so kind of concurrent with some of the stuff taking off in the last few years was kind of a lot of work that started critically evaluating kind of deep learning for supervised NLP and so you know for a national ears in France for instance this is a three-way classification task and even before pre training really took off so isom is using just word vectors and it was a very you know well design architecture they nominally got to you know average human accuracy of a I believe a single Turk worker it may have been an unsound three so it's like whoa is this done did we already hit human accuracy and you know like I think everyone kind of knew well no because clearly these models are you know still making weird mistakes and this is kind of where in the last few years there's been a lot of great work starting to really quantify these concepts of how robust our models how well do they work kind of distribution kind of pressuring and challenging the standards supervised learning paradigm of you know training on an iid training set and evaluating on another ID split held out data and basically showing that that's no longer sufficient and something's going wrong somewhere in supervised learning that means this is a being too fortunate to algorithms and you know not being fortunate enough to humans and so this is a great paper from a Sutra and I believe called annotation artifacts of natural image inference data and so when you do hear people talking about these models are exploiting statistical artifacts and Maya sees of the train distribution you know this is a paper that really nailed that down and showed it quite conclusively so you know they kind of start from a high level well how were these datasets created these supervised data sets you know admittedly they're kind of artificial you're paying people to label these tasks they're not natural instances of the task it kind of what people can come up with on the top of their head or you know they can have very good experimental methodology in data sets like a semi multi know why are some of the best we've got in terms of like very good set ups you know curated by people who really know what they're doing but you still run into the issue of like well you've got to have a human generate an example and you know maybe they're less creative than you think so the data is actually drawn from a much more narrow distribution that it should be and so this paper kind of went through critically and showed a lot of these artifacts actually showing up so you know a worker would be told make a you know a negative or a contradictory label and so they would just be like oh I'll just slap a knot and top with all copy the sentence you know and it's not quite this bad but it gives you the idea of what's going on is they'll copy the premise sentences the hypothesis and they'll just put a knot in it or they'll you know to have entailment they'll just restate the sentence in a more generic or abstract way and so you might go from you know you know you know a dog is you know playing to an animal is playing or a pet is playing or stuff like that or you'll add some kind of like super information like tall or sad or popular to hint at the neutral class which is like well it might be true or am I not but it's not clear that way and so what they showed is somewhat disturbingly if you only trained to model on the hypothesis sentence so the second sentence and again semantically this task is defined as the logic correlation between two sentences but have you trained a model only on the second sentence to predict which of the classes it would be it actually did it basically got half of them right you know it went from 33 to 66 percent or so it was a large bump and you know by default we know that model can't be doing the true task because it's just predicting you know given only at the random second sentence so this is a great example of where you can see that standard supervised learning might be picking up on these spurious correlations or artifacts and you know assume when you evaluate it on a new card set which is the set of answers that the model that only looks in the sentence evidence can get right drops quickly from like 16 percentage points from 80 to 88 to 72 percent and this shows up across the board there's now probably a dozen papers in this space if not more that show that kind of these analyzed systems that you know nominally we're supposed to have human level accuracy actually are not consistent or not robust or not systematically generalizing so you know this is another one from Glockner that you know very carefully constructs kind of these probe sets so it's like permuting objects and the sentences are permuting you know synonyms or antonyms and you know on these probes they show that you know actually again drops quite a lot and then a final point here is on distributional robustness so this is a paper from Devon called learning evaluating general linguistic intelligence and so what they showed is those near sort of question-answering models which again on squad it's you take a wikipedia sentence and you take burden which we've already talked about how much of an improvement it's had and you know how you know it's improved scores a ton so you take that sentence that question answering well that's trainable capilla and you just run out on a different data sets it's still question answering except maybe we run it on like trivia like trivia factoids that are sourced from like google search results or maybe we run it in a more conversational framework with kind of you know to to people asking questions between each other and we see that just like a gracie's can crater or you know f ones basically actually metric here so it's you know the same task and we know that people when you ask them a question on one task force on another they're gonna do about the same you know maybe yes one task is a little bit harder than the other but you don't see them like suddenly you know lose half their accuracy this is again just hit set some of the distribution a lot more muscles tissues and brittleness we're seeing and again this is still some of the best stuff we've got it's combining supervised learning and unsupervised learning but there's hints as we're going to go through here that these self supervised methods and unsupervised pre training is really helping with the robustness we're still not there yet but we're making progress and all that's being driven by moving away from a purely supervised learning framework to moving to these like hybrid Android training and pre training methods so as I mentioned like there's a lot of things that could be going on current techniques are brittle they're memorizing and so generalizing they're exploiting spurious correlations you know also stop learning once you want to get to you know memorizing your training set you just wall turns off because the gradient ties ISM training lost goes to zero so it just kind of feels incorrect so there's like a lot of different routes we could go down to make progress we can do better models and architectures we could do more data we can go down different paths all together so obviously since I'm kind of talking about unsupervised learning in a supervised learning class I'm gonna talk about how that's a very exciting one but you know we could always keep working in the supervised learning paradigm and just say well we're gonna have better models and we're gonna get more data we're some kind of purses problems in the same way and so this was like kind of what I'd say a lot of like early deep learning was really highlighting was kind of you know we were working on supervised learning datasets we kind of were seeing you know these new architectures that were exploiting priors and inductive biases of the data domain we're really helping a ton so on images you know this is the grand story of we added you know comets and they are a great fit for the domain and they kind of cleverly quote encode you know all these equivariance and translation and you know shared weights and all this structure and that helps a ton with their accuracies and then we just use a large supervised notice and let HDD figure it all out for us and so this kind of led to I think of mindset of heavily emphasizing architecture engineering you know there's a very large design space here someone cynically it allows for a lot of different papers to be written and you know you can really kind of combine and contrast like all these building blocks like we really like playing with these blocks and you know a lot of really good work has been done that like does empirically push the state of the art by exploiting you know properties of domains and you know an example of that is this diagram on the left so does anyone want to guess the name of this model well sorry because it's kind of hypothetical it's called a simple model so this has got six different color embeddings and you know there's screws and character models and by attentions and MLPs and you know it starts to get quite complex when you're really all you've got is inductive biases and kind of the standard supervised learning datasets so it's a heroic effort but you're kind of exploiting more and more details and getting more and more complex to make progress when you kind of have locked in these other constraints like the dataset size and the paradigm of training and on the right is another one that I think is like almost looks like it's like you know some like pentagram or something you know they look like kind of these very cool like architectures and they're very quite fun to look at and kind of look through all the work that's been done on creating these systems and again like we said there's all these different methods of having a deck Tobias and it can really help a lot and so they're all important and very impactful and please don't take this as like criticising kind of the standard approach of like iterating and hell climbing on supervised learning with like better and better architectures but I think it's a bit like this where really when you treat a data set in isolation if we come back to how people learn and experience the world it's so varied it's so diverse there's so much experience in information and knowledge you're leveraging before you ever saw this data set and some machine learning models when they're started in isolation on a supervisor zone you get a set by itself are kind of like you know that supervised data set is like a peak in a very big space it's a small peak and we can add more and more data and make that peak more you know taller and wider and that might help with robustness and generalization but at the other day it's kind of a little bit futile I think you know the real way to solve these tasks or at least the way that people do it is they don't sit down and memorize you know a million different examples they somehow learn a much more general set of tasks behavior and transfer of knowledge and information instead of like just becoming a master at a very specific isolated domain you know we're amazing because of our Gen not because of our you know or well we were amazing for both because we can do incredible things in specific domains but at least machine learning is starting to see that I'm very targeted supervised data says you can do it models that do a bit better and so then there's papers that even on architecture engineering show kind of somewhat critically that some of these you know fancy or new architectures that we saw them don't quite improve as much as you think or with more careful oblations don't show much of a benefit so you know there's a one of the famous examples here is they took a baseline Alice T and gave it some love this is kind of a common story for language modeling and show that it was about performing kind of a lot of new recent state-of-the-art models if you just have careful comparisons in careful trying to me so you know maybe we need to back off and rethink beyond just pure supervised learning on test specific datasets and you know I think one of the reasons to frame this is the largest supervised days so you know basically in the world what I'm aware of publicly is gft 300 million actually there's a Facebook one that I haven't talked about their Instagram pre training but this work this was true a little while ago so there's a straight a million images 18,000 classes if you if you do like a very simple like loose bound on how much information content you get you have like log 18,000 bits per image and you have 300 million of them so that ends up with about getting 300 50 megabytes of constraint on the function you can learn so this is the world's biggest data set and in terms of the correct function that we're trying to approximate with supervised learning you know we only are able to pump about from this kind of slightly naive in toyish view about 530 megabytes of information into the system from the supervision here but you know like like trying to connect this back to everything we've been talking about today there's you know terabytes and petabytes of actual raw natural language on the internet so if we figure out how to exploit all that information in some reasonable way there's a hell of a lot horror there that we should hopefully and again we're gonna be a lot less efficient you know gold labeled supervised data you know per bit is probably helping far more and less specify and learn a task but we only have a little bit of it it's like you know yawns kickin ology of the cherry on top first kind of you know everything else we need to be able to do and I kind of tried to take the supervised learning approach for language for I spent most of 2015 myself building what I hoped would be an image time for text it was a very large weekly supervised data set where we basically did classification over edit communities and we built like on a 50 million training examples over a thousand communities we turned our Nan's to predict everything and we're hoping they would learn useful features and representations kind of skip thought style it was pretty concurrent work at the time except we were going with the supervisor route instead the unsupervised route and the sad thing was the unsupervised model beat us so skip thought vectors was beating you know just by doing a large bottle objective was beating this system that we built with like your middle e weekly supervised data but we were like oh yeah this is the gold label so you know these are the right things to predict they're semantically aligned with classification and this kind of really made me quite confused and kind of skeptical so what's going on in his face because you or at least supervised learning and got me really excited more on the gener of long ago and some of us I'm excited because we just weren't seeing the supervised learning pull through here because it's just I think is a little bit too weak of a supervision source and a little bit too specific so like again the big question I think is a lot more you know in terms of like novel research frontiers how do we go from kind of isolated peaks of competency that you can very quickly you know fall down if you change the problem just a little bit you know quickly collapse in terms of task mastery how do we go to systems that perform you know and then much more general robust kind of you know maybe they're not nearly as good in terms of competency on any given specific task but how do they perform much more broadly across the board and again so this this is an example of kind of the classic architecture engineered approach like one of the kind of you know incredibly well done versions here that's exploiting so much information with inductive biases is using a word net which is like that great hand curated data set and so we see that it like gets and you know because it's able to exploit all this site information of you know helping with like learning oh these are in it you know synonyms or antonyms or this is you know more abstract or less abstract you know a child or a parent and in terms of like semantic hierarchy of a different word so you can see how that's bringing in the information that should help with generalization and so it actually does better on those kind of systemic evals so this is one way of like widening that peak and someone excitedly though if we just slot GPT one in as well it performs just as well on the more robust transfer setting so there we didn't have to you know manually curate that the relations between words or build Ward Annette we kind of let a language model figure it out and so I think this really again is one of the proof points that you know some supervisory training is really figuring out the same relation is the same kind of connecting the concepts connecting them and helping with robustness and generalization and there's some new work from Tim Hendricks this week that I've which I have put in these slides showing that Berkeley as follows are much more robust a t-distribution than classic purely supervisor models with like LS TNS or cnn's so I think that's starting to get much more well empirically founded than kind of me spouting off one or two numbers from like the models I know so kind of at the high level takeaway here kind of this is just a hurrah message for everyone taking this course is you know I really think that one of the most promising methods of moving forward here is in terms of like really lying tasks and robust systems that actually you know perform the things we want them to is we need to move away from standard supervised learning instead of manually specifying what to predict through the creation of large supervised data sets we need to figure out how to learn from and predict everything out there and you know one of the ways that you can think of this is like every time we build a data set we're sitting the importance of everything in that data set to one and everything else in the world and all the other useful information may be out there is set to zero so like when you start with a model from scratch you should really get in that supervised learning as well as head and be like oh it's almost a hopeless task you know they know so little and we've hidden so much from them when we only give them this one canonical gold standard data set and of course they're gonna cheat however they can because they're you know great at optimizing the objectives we give them but if they don't have the foundations with which to truly you know build off of all they can do is exploit clever spurious correlations so yeah I think this kind of comes together with all the work we've been chatting about of like a potential recipe for and you know I think this is getting proved out with t5 and all the future work here of how to kind of combine a bunch of pieces together we need high capacity and flexible model classes so they can handle a lot of different tasks we need algorithms for extracting information running the structure across many different domains so this would be basically you know a lot of things we talked about it turned out language modeling you actually just worked really well as one of these it's an incredibly old idea but that algorithm just or you know method just worked quite well in terms of people there's a lot of different clever approaches to specify and proxy tasks but this simple one has gone it's quite far and you're unfortunately still going to need because these are dumb models that don't you know have anywhere near the robustness or generality of humans you're going to need a lot of data tiling everything but at least it'll be unsupervised and so we have it available and you're going to need unfortunately at least to get the you know the soda grind a little more you can in fed some amount of compute with which to learn them but again that may produce a model that's actually quite small and efficient to run a test time and I think that's one of the hopeful direction is going for is you you know train these big models and you know Google or Facebook or open the I you know burns the GPU years to to get that model but then you know you're able to distill it and prune in and release it and then it can still run on your own laptop and or on you know a single GPU and you know that means that there's downstream tasks that you may want to investigate or you know build models on are much more efficient because you've amortized all this compute that went into pre training and now you're able to you know use that during the fine-tuning so it may actually be that like bird is actually you know though it took a ton of compute to train bird and Roberto may actually have reduced the overall volume of compute needed to achieve a given level of result and may actually widen the amount of usefulness and test that can be tackled in the field because it can transfer and you know been and be beneficial to everything downstream and you know I think it's very reasonable that some people in the field kind of look at all this coming together and are like well you know I don't find that satisfying and so I think that's a valid view and so you know maybe backing up and working towards you know more grounded learning and there's lots of really interesting work in this space now of you know moving towards reinforcement learning and granted learning with you know multimodal agents and all this kind of stuff that connects to you know more what feels like you know true learning about the world instead of just seeing abstract bits of text you know that I think that's a very valid approach but right now you know we've been just seeing that it's been driving a good chunk of empirical progress over the last few years you know there's a whole other set of methods here that's multitask learning and I think that that's actually been showing a lot of promise in the last when I made these slides this slide last year I think I was a little bit more pessimistic on it and there's actually been a lot of good work like I'm tdmn and others that's been making progress on on this set of methods but it still kind of relies on us building a data set so for multi task learning you train on a bunch of different tests together and you kind of hope that you get transferred nationally between them but often they're all supervised tasks and t5 is a good paper actually like really talk through the nuances of what's the test learning for gendered pre training and one of the surprising things they share now is when you do it well and you kind of exactly emulate the pre trained and fine-tuned framework if you do multitask pre-training followed by supervised fine-tuning you still need the uh you still need the unsupervised objective of like math slang which blah blah like but you can get rid of or you can at least find very similar performance in many cases compared to having to do the giant pre-training on you know the full internet for instance so they're still having room left and it's actually improving these methods and the final one here to just chat a little bit about is some of the fall work we did it and they're open ion gt2 and this is kind of like what I've been chatting through here is kind of like a lot of the motivation that went to this project so we collected more data compared to GPT one and we collected much more diverse and heterogeneous data so we're hoping that we have models that would generalize better and see a much broader set of tasks so it's 40 gigabytes of text 10 billion to go cans 8 million webpages we scale up the models just because we kind of saw those trend lines and you know I think there's a lot of reasonable arguments for why you just need bigger models to handle complex tasks and it's just a language model which predicts everything so immediately it still left right on aggressive model so it has some drawbacks compared to things like Bert but it's just a language model and so what we focused on in this case was purely how well this mala could do across you know many different tasks in at zero shot setting so we we never fine-tuned it because you know supervised learning is tricky and it learns to exploit spurious correlations and dependencies so we're only ever saying well you did all your pre training work and we had you predict a bunch of words how well can you handle this new data distribution you've not only never seen before I mean really you know we trained on a lot of data so we actually see a bit of a lot of data distributions you're not letting it like specifically turn specific tasks but that specific label we're just saying run it and see what I can do and we show that like it actually begins to do something particularly as you scale the model across a wide range of canonical and LP tasks so it's purely unsupervised there's no you know there's no direct human labeling or supervision going on here but this model can actually you know you can feed it a paragraph in the mask of question and you get transfer and linkage well can give the right answer sometimes often they're just matching kind of old baselines and they still have a huge gap to the you know human performance but I feel like this is a much better measure of potentially what the like underlying performance of these systems might be because we're not doing supervised training here and you know yeah and surprisingly we know our models are still worse than people so that kind of shows up here but it also shows a promising trend line where in some cases like there's very domain-specific algorithms for unsupervised translation middlee it's been a year so that speech should be up here now is there like some great follow up work from Ferran well there's pushing and supervising on team farther but this is just a language model with no real customization and we're just seeing that it begins to do translation between the cumulation French you know you can tack a tldr on the end of a document and get something like summarization it's pretty garbage on the official metrics because it's only barely matching read three random sentences from the article but kind of quantitatively and qualitatively if you ask people which you prefer it looks a lot better than these numbers show because this is a kind of very coarse evaluation metric and then the final thing here is like question answering so it's kind of shows domain knowledge and kind of like kind of world knowledge and potentially a lot of factoid information and this one we unsurprisingly see a really strong scaling curve with model capacity so like how is this working how does this kind of unsupervised system it's just a language model begin to translation question answering and reading comprehension well if we go through an inspector data said it turns out there's actually just like kind of natural occurrences of tasks and you're turning them all to predict the next words so that's you know it's easy an English sentences then you're like then it happens to just have you know inside of the middle of this article that someone wrote a training example of English to French so it's a much more natural way of learning and when you have very large data sets you just actually begin to have a non-trivial data and so you see for translation for summarization like if we just like crap through the data set how many times does TLDR up here well there's not a thousand training examples in quotes here and how many times does like someone asked her who what where when how why question well there's six million of those so we're kind of seeing that these kind of systems that you know don't make assumptions and silly about any specific task we kind of try to predict everything kind of really begin to make some progress I mean again like one of those areas we saw the most on is this question answering an open domain question answering where you're just asking like what is the capital of you know Paris or in what year was star wars released and you know I think that this kind of gives you a very clear picture of why supervised learning with like kind of the standard approach just is never going to really be able to solve this kind of task so on the x-axis we have a number of training examples seen and again this is log scale and yeah if you start with a Randal initialize what model there's no way it's going to be able to do question answering I don't open domain you know there's no way it can have the information for you know what is the capital of Paris until it's seen that training example and there's very little generalization there you just need so much data to approach this from a naive supervised learning approach whereas we have bigger models that have more capacity you know in the limit they very quickly began to do non trivially on these data sets and then they kind of fine tune in and learn how to better extract the information that's somehow contained within the weights to various degrees so again this red baseline here is completely randomly emitted and these are basically random guessing numbers the entire way through you know those data sets doll is 20,000 labeled examples but as we try bigger and bigger language models we see that they really begin to make persons and t5 I think has continued pushing this quite a lot farther to where they're actually sometimes matching with only a neural model that's never looking at documents with like the actual factoids in them it just from its parameters is able to answer quite competitively on some of these tasks yeah we're pretty much into a conversational period at this point but um you know some of the takeaways I would kind of say from this and kind of you know really pushing on language models for a few years here's performances you know not usually limited by something single paper fixes this is a very long history of you know I think we probably talked about 25 papers during the trajectory of research here and usually it's always someone chipping away on one specific access you know diminishing returns basically mean there's always some other bottleneck so if you scale to compute but not the data you'll get back there if you scale you know the parameter size you'll just need more computer or if you scale you know the Moloch caste but don't increase that is that it'll just over fit or you could try to scale via like you know human intuition and you can use fancier models but maybe that's just more difficult to train so kind of I tell you that like you know particularly if you have a little bit more of an engineering mindset here kind of the pragmatic approach of kind of pushing on all these axes together may allow kind of for a larger effect size to show up than pushing on anyone in isolation this is an unfortunate tension I think in research and science where you often want to you know microscopically measure effect sizes and walk controlled oblations and experiments in isolation but you know if you change a few things together you might actually see morgan outsized effect because that's like one of the things we do we typically to where we get more data get more you know a bigger model through everything cut it together let me try to see if that really pushes toward qualitatively different behavior maybe yeah I mean I really could transition in a question period at any point now you know there's a little bit more advice at the end just saying that don't work on large scale models particularly you know like as things like a lecture so show you can work on the smaller models and see the same effects showing up they're not going to have the same accuracy curves but you know we know from scaling laws and kind of all those trend lines that if you start seeing an effect that's robust at small scale probably fingers crossed it'll also hold at larger scale so you can do a lot more more rapid development and you know and this I think works quite well you know you should try ten as many or ten times as many times models that are just ten times smaller each and you know that way you can run 10 times as many experiments in parallel this is still you know a large research field so there's a lot of things you got to try and you know beat all the behaviors in a paper like GP d2 which kind of I feel like gets pointed to as a canonical like the computer Big Data kind of thing they still show up on models you can train on a single desktop it you know it takes a week to about to see the hints of that middle E but you know gbg small you can train quite well and I've got a week on like a for GPU setup and then after you get proofs of concept on like your algorithm or your idea then you can scale up if you can have the computer resources you're going to get a you know a low enough time on the cluster or thanks and kind of the same strategy used back to the day with like the seventh unit where the initial proofs of concept were 512 dimensional as teams that took a day or two on standard hardware and then you know for the final version then we kicked off a big run with the model that took 16 times the computer and you know how do you not go insane when you wait for a model to train from month well we like to do this thing at opening eye where you boot your big model over before you go on vacation so you put that before winter break and you just let her train over the break and luckily fortunately you're in the set of sight machine for the whole time but don't stare at that graph every day you won't make nearly as much progress if you're just staring every day at that number but often models surprise you when you give them more time to learn so you know when you're really trying to push that result at the end it's a really good idea to try that if it's available in the option yeah and again one of the other spreading things just about this field has been how far we've gotten pushing often where the developers of one paper or modeling architecture let me just push them log probability or the you know type 1 evals and then someone else coming along in another paper and showed oh this thing's actually great a type 2 evals so I think that's you know really reassuring and you know I'd often say that you could work on one or the other in isolation and often you see things that robustly scale or contribute on both sides you know there's some gotcha as always with scaling you know things break when you at some point you can't extrapolate too far and things just change so you've got to watch out a bit for that you know for like a model like 2 PT 2 real on one of my collaborators was like we were originally trying to train these deeper bigger models and they just weren't working better and we had to fix an initialization technique and rearm came up with this and it helped you know continue scaling so when you see you're scaling like not happening in the way you'd expect or the try mods kind of suggest it's also in some that something's wrong you need to like tweak it or fine-tune it or come up with like to actually do the clever work I don't do much of that myself to fix it up and try to keep making progress yeah and then the other thing is just like writing efficient and smart code these days luckily hardware is proving and for the same price point so with things like FP 16 1/2 precision compute if you switch over to that with things like with example being GPT one the original version took 25 days and FP 32 and one generation older hardware and then the same next generation hardware where you you know a lot of people did a great job optimizing this to Scott grey in particular it's amazing GPU engineer and researcher and opening I we worked with some blocks part part the box of our spork is basically his work and he was able to optimize these down by almost order of magnitude on just the next generations hardware from a lot of you know great improvements across the field so often if you write efficient code and use all the right tricks in terms of accelerating your models you can ring a lot out of the same you know the same level of hardware and just be efficient about that we have a library called the block sparse library that can help with that and provides a lot of these opsin honestly also libraries like right origin are doing a great job merging these in providing their own ops kind of more integrated into these these kind of wrappers so that's I think exciting for the fellows at all yeah you know in terms of sweet spots for computer you know for 20 ATI desktops still can do along with space they just cost a fair amount of money and then you know your standard 80 100 bucks on a cloud provider is a very is a medium scale compute platform you know papers like electro can do a lot which 'single be 100 and I mean a 20 ATI is basically the cheapy 100 for 4 or 5 times less oh yeah that's about it honestly I think we have where you have about 15 minutes left for questions and you know I have a few more random slides everyone this this is really great Alec thank you so much let's see if people have some questions hey Alec yeah how question so I was wondering if you could give you a views on like what do you see 0 shot language modeling something that could could could be production quality performance over time or do you think it's always gonna be lower than a collecting supervises and fine tuning some big current model just try to understand like the space between GPT and like Bert like models yeah oh yeah so you know right now it is absolutely garbage from a production perspective like GPT - I mean well okay there's hints of life there you know for reading comprehension it's it's matching some of the original neural supervise baselines so I'd say there's hints of life there we're still talking about you know you need to do a lot more research and if you looked at kind of those scaling laws for like what kind of you know GPT - looked like like if you draw those out there's still quite a lot of order of magnitudes left to go so from a pragmatic or practical perspective it's not really there right now and that might be the scary answer which is you know our models do rely on exploiting and you know I don't override this view but you know like it may just be to actually do these tests correctly you do just need you know much more compute and something like the zero shot setting so it's kind of like working I think I see it kind of is like working with you know like letting shoes or something like resistance training I think it's a fascinating research area to push on because it does have some of these exciting qualities from like maybe representing the you know much more difficult and hopefully much more true representation of test performance but yeah it still has a long way to go so I think it's a fascinating research direction but here's a lot of pushing to be done on that thank you and yeah I think for a pragmatic perspective like you said you know you really should find tune on some supervised data and you know like I mentioned Burke models are still showing quite good robustness out of distribution there I don't think there's been any good work comparing pure zero shot to learning of a task to like supervise the fine-tuning of a pre train model but I think we're talking about something that's like a few years out at least thank you thank you I saw a question here earlier you motivated Ellen's by comparing probabilities of pairs of strings to exact knowledge such as cats at first cat sets has this intuition comparing sentences I guess with exact knowledge been used for training general models a text or Maxo like that ubiquitous so maybe this is about a comparative or contrastive method for training generative models where you compare sentences and know that like one should have higher probability than the other there was one one paper for reps in tation learning perspective which it's not quite the generative model side but it's representation learning CPC you know is that whole family of contrast methods is dominating you know unsupervised learning for image representations so it's somewhat of a contrast where in NLP we haven't seen it really tick off yet so I think it's a very exciting research direction in the original CPC paper actually had some results that were promising on natural language but they you know like the original CPC paper in general we're exciting but nowhere near stay the art and a lot of the refinements in the last year or two on the image side really pushed that quite far I think you might have had a lecture just on that or about two so it would be very cool to see if someone could do that kind of similarly for natural language but if it was about kind of exploiting more structured knowledge about like differences and encoding that into the generative model there is some pretty interesting work on this particularly from some more the like linguistic heavy folks and field of combining kind of hybrid systems of you know neural and with like kind of something like grammar constraints or the like and it's you know I'd say it's primarily focused a little bit more on you know the settings where you might expect encoding that inductive bias to help which is like smaller data sets but you know at least personally my kind of I find some degree of at least from a pragmatic perspective a lot of current language modeling benchmarks I think are quite artificial because they work with such small amounts of data which from pragmatic perspective just doesn't make sense because there's all of what could be out there there's it's so easy to just write a scrape or your stuff or download a shard of comic roll and that's more data than you basically ever going to need to work with or you know be able to process and so I think at least from a pragmatic perspective we should really be for how to use the large volumes of data we have you know I think it's a very valid other approach to push on data in isolation and you know how how much you know how data efficient we can get with limited set of data but I think it's probably add just to farm in extreme when you have you know only a million words of training data and things like country things so Alec a follow-up question on that it seems like one way to to learn languages read the entire internet right another way to learn language is the way I think most people learn language which is absolutely you kind of I don't know how many words or how large the data set would be that somebody encounters by the time maybe they're six years old and they can speak pretty well maybe at that point they have any notion of kind of how much data is required in that context compared to how much data is required here oh it's it's awful at least for you know in for neural models I think it's um yeah for like a six-year-old child I think it's maybe you know I just bashed on 1 million words being unrealistic but I think it's about one to ten million so you know compared to GPT to being ten billion tokens there's orders there's three orders of magnitude at least of headroom there potentially and i think that again understandably motivates why a lot of people do work on that city but my guess would be that to really make progress in that setting a lot of that is because of transfer between modalities and you know actually you know interacting with very high quality sources of supervision like other people and you know being you're grounded agent that interacts with you know video and audio and like i think that that research is very interesting longer-term and you know we're probably going to saturate kind of what we can do with these ungrounded giant systems in the next few years or maybe it's even already starting in the last year so that's like very i think exciting next round of work and clearly like the numbers just show there's a huge amount of room to go got it thank you makes that work well for when you are laying apply to like other abilities like video surgeon educators okay yeah so genetic did is actually a great example there there's a really I think Joshua Meyer and collaborators between I think was it proud and it's an NYU team in slush fair I think Rob Fergus is not working a lot on this so they took Bert and they applied it to protein sequences or I think sorry amino acid sequences and probably I don't have strong bio background but much of bio background but they were showing that the same methods or you know learning like a lot of the structure in those different domains so like kind of the central unit analysis or the sentiment you know example I gave for pure language there was also another paper from I believe church the church lab at Harvard where they took like literally my code and ran it over amino acid sequences and we're showing that there was like instead of a central unit there was like a like a beta sheet unit or so current course finding like secondary or tertiary structure of proteins the models were having units that were like understanding you know or even though these are like very nonparametric kind of abstract models that just like you know have a bunch of parameters that just factorize a probability distribution they're somehow learning the structure of the domain or hints of that so I think that's very exciting and that's another line of work I think given how exciting this stuff has been for MLP and how much of an impact it's made over the last few years whether it could work in other domains would be quite interesting you know there's definitely differences so for video I think video just needs so much compute that it's like still maybe quite a few years off just because of the volume of data and you know the amount of compute that might be necessary but maybe I'm just being cynical there whereas I'm images you know there's a weird contrast which is like I mentioned the contrast in methods are doing quite well and if you just run a generative model where you know actually okay that's not quite right there's one paper from deep mind called big by again where they took it immediately you know pretty different generative model and they were showing that those are starting to learn quite good representations or images at least from the standards of unsupervised learning still being crushed by the latest moco's or sim clears but they're you know they're quite promising and you know showing a kind of a foothold of this generative model kind of approach in other domains and maybe you know one more piece of context to shine on there I think there's some one of a nicety to language because it's produced by you know people it kind of is naturally designed to be very clean and very high-level and yeah it removes all the noise so when I think we run and try to train the same generative models or approaches in domains like images or video it may just be that like when you're dealing with raw natural audio signals are you know sorry not raw natural signals they have so much noise like particularly a likelihood based generative model is just like spending so much effort and capacity trying to predict all that noise and the you know this noise to signal ratio it's just a lot cooler that that just like makes it a much more difficult task right now yeah you know it's it's I think it's a very interesting research question so Alec we're about out of time here give any closing thoughts oh yeah let's wrap it up we're mostly there I guess you know what one thing again is like you know III one of the things that I really enjoyed about being able to have the opportunity to this talk was kind of going through and showing that full history here and kind of you know I think it's it's a great example of how there's so many pieces that built on top of each other and you know there's so many different authors and so many different institutions that really contributed to this and you know even given that open area there's been a lot of climbers that have pushed on this stuff over the last few years and you know it really you see it just evolved like so many different pieces of the research with all the different you know things being brought to bear new models new datasets you know new approaches so I think it's a really great and you know exciting example is like a very rapidly evolving research field that managed you know do some exciting things in the last little bit yeah well thank you fantastic lecture Thank You Alec we'll stop the recording here right future is great I hope continue pushing everyone 