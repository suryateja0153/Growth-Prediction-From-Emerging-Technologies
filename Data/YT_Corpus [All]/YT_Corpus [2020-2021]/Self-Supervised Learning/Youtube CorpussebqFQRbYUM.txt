 Today we'll be presenting our work on robot-supervised learning for object segmentation With robot-supervised learning, we can create pixelwise labels for unseen objects that are grasped by a robot manipulator We separate the object segmentation tasks into two parts- foreground-background segmentation and robot-object segmentation To perform foreground-background segmentation We rely on the robot's RGB-D camera and kinematic model for foreground localization We project the robot's link locations into the depth image, over-segment the image, and select segments that correspond to link locations. To account for noisy depth data the depth segmentation is used to initialize a GrabCut segmentation on the RGB image We can apply this technique to images of the manipulator with and without objects grasped Foreground-background segmentations of manipulator only images are used to train a robot self-recognition network The training data for this deep convolutional neural network is augmented with artificial backgrounds and foregrounds Finally, on images of the robot manipulator holding unknown objects, we take the difference of the foreground segmentations and the self-recognition outputs to create object annotations 