 in this paper we consider the problem of self-supervised visual representation learning from videos by predicting the future intuitively if the model is able to solve such tasks we will imagine that it has fully understood the action patterns in videos however predicting the future is not an easy task why first if we consider predicting the future at the pixel level like the future frames they are full of stochasticity like illumination changes second the future may not always be predictable it can have multiple modes let's see an example here imagine i present you the following frames which is the beginning of a video with a hammer throwing action can you predict the future frame sure you may be able to predict the rough scene like the human still stands on the field with some trees in the background but how about the details the trees might have tiny movement the cloud might move a few pixels due to the wind the shadow might change a little bit because it's approaching sunset note that if the model is asked to predict the future frame all of these details matter we can see the future prediction involves so many uncertainties okay now can you predict the future frame even the rough human action perhaps this is also not easy this example is hammer throw do we know in the near future if the human is going to throw away the hammer or spin for another cycle throwing and spinning are the two modes of the future for this video both of them can happen thus we see predicting future involves multiple hypotheses we propose memory augmented dpc that simultaneously deals with uncertainties and multiple hypotheses to deal with uncertainties our model predicts the future at the feature level to only encode the high level semantics we also use contrastive loss as a training objective to avoid an overstrict constraint to deal with multiple hypotheses we propose a compressive memory module which is able to consider multiple features simultaneously now i will briefly go through our approach at the beginning we extract a spatial temporal feature from the input video clips to get a context to representation then we prepare a shared memory bank it just contains some vectors to predict the future states we simply infer a probability distribution over this memory bank and the predicted future state is a convex combination of the memory slots note that the memory bank is shared for all videos and simply contains chainable parameters that are optimized during self-supervised training ideally the memory bank stores all the possible future states and the model selects the corresponding feature states for the given input video in this example one peak in the probability distribution can indicate the spinning action the other peak can indicate the throwing action for the training objective we use spatial temporal contrasting blocks as in our previous work dpc we get our predicted feature as explained before and we can also get the ground choose features from the future of the video if the predicted feature and the ground choose feature come from the same video and the same spatial temporal position then they form a positive pair otherwise they form an active pair with a contrastive loss the predicted future does not have to be exact as long as it can emit a higher similarity score with a positive sample than with other negative samples it would incur a low loss thus the model can perfectly ignore modeling the low-level stochasticity and only focusing on the semantics in detail our architecture looks like this the input video is cut into multi-pressure clips and we extract the video feature in two steps first we use the shared feature encoder f to extract a feature for all the short clips then we aggregated the feature over time with the function g to get a context feature ct that covers all the past information up to time t after that the model starts to predict a future from the context feature ct with the memory module it predicts multiple steps into the future meanwhile we can also get the ground choose features from the future video and computed the contrastive loss the objective is to learn a good representation for the encoder function f our architecture is universal to any choices of encoder function f and aggregation function g in our experiments we use 2d 3d resnet as the encoder function f and we use single layer giu as the function g furthermore we improve the representation quality with two extensions the first is a classical two stream network for videos namely the original rgb stream and the optical flow stream the second is a bi-directional temporal aggregator the details can be found in our paper the model is changed from scratch on ucf 101 or kinetics 400 in a self-supervised way in other words without using any labels next i will show a visualization of what is learnt by the memory module and then evaluate the quality of the length representations on four downstream tasks here we visualize the probability distribution that the model uses to select the memory slots for the same action the model learns to produce similar distributions for another action the probability has a different pattern note that all of these are learned without using any labels our model uses a probability distribution to choose memory slot but what's captured by the memory slot following the last example we choose the three peaks in the probability distribution and find their corresponding memory slots and use each memory slot to find the nearest neighbor from the improved video the result shows the chosen memory slots match with different states of the swing action in the next few slides i will show our performance on downstream tasks but i will start by explaining which feature we use for these tasks recall that this is our mem dpc architecture and this is the architecture for downstream tasks in detail we remove the memory module and the future prediction pipeline and we take the context representation ct and average pull it into a vector for evaluation next for video retrieval we show a model prediction on ucf 101 without any labels and evaluated for video retrieval on yourself 101 in detail we use testing set videos to retrieve training set of videos and we compare our method with a network chain with supervised learning on kinetics 400. the left is a query video on the right are the top three nearest neighbors here is another example as you can see the representation length from self-supervised chaining is also capable of retrieving correct actions more quantitative numbers can be found in our paper for action classification our model is pre-trained on kinetics 400 without any labels and we show the classification results on user 101 in this figure we compare our method with other recent self-supervised methods for the fine-tuning protocol on yourself 101 note that all methods use only the visual modality when comparing with others we note that the different architectures and modalities makes the comparison extremely difficult but here we are trying to present the result as fairly as possible clearly our method outperforms all the other methods and is comparable with the dynamo net which is pre-chained on two years of data compared to hours 28 days furthermore this figure shows other self-supervised methods that use extra modalities like audio and text recent works like xdc and arrow achieve accented performance on action classification but they use audio video data with a duration of 21 years and 13 years respectively whereas we only use visual data and only 28 days for efficient learning our model is pre-channel yourself without any labels and evaluated on usf we fine-tune the network for action classification in a supervised way but with limited number of labeled data we compared to a representation initialized with random weights the left is rgb input and the right is optical flow input it shows that when the model is pre-chained on yourself 101 with self-supervised learning it only needs less than half of the label data to get comparable results with that from randomly initialized weights lastly we evaluate our representation on the unintentional action classification task on the oops data set our model is pre-chained on kinetics 400 and the oops dataset without any labels and it's evaluated on oops the oops dataset defines three types of actions along the temporal axis they are intentional transitional and unintentional like this basically this task is a three-way classification task on this task we outperform a very strong baseline that is supervised pre-training on the kinetic 700 under the fine-tuning protocol thank you 