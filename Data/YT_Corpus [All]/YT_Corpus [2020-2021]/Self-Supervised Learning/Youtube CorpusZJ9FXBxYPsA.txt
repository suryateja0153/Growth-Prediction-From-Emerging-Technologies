 ensemble models in machine learning combine the decisions from multiple models to improve the overall performance that is why ensemble models are more preferred when compared to simple individual models keeping the importance of ensemble techniques in mind we have come up with this tutorial on ensemble learning now before we start off with the session i'd like to inform you guys that we have launched a completely free platform called the great learning academy where you have access to free courses such as artificial intelligence cloud data science and digital marketing you can check out the details in the description below now let's have a glance at the agenda we'll start off by understanding the mathematical concept behind decision tree then we'll learn about concepts such as genie index and entropy after that we'll comprehensively learn the concept of bagging and boosting and finally we'll have a demo where we'll implement all the ensemble techniques so usually what i do is actually we've missed one class in middle so there was a break given by great learning to you guys so what i usually do is in the previous session itself let us say today's first week's top topic is the decision tree if i'm not wrong yes so what i do is i will show you guys decision tree also and along with that i will showcase you some of the other techniques for the second e-course so that when you're watching the videos it'll be easy for you to get the [Music] definitely see decision tree um is a part of classification algorithm so the algorithms that you did was logistic regression svm knn decision tree is a kind of part of them itself and post decision tree we are going to talk about ensemble techniques okay yeah it's fine okay okay good so now uh for today's session what we'll do is i'll give you um we'll calculate will will understand decision tree manually so we'll calculate using an excel file so i'll show you what all stuff is needed what is not needed and what all you need to know for decision tree step by step post that uh we will talk about something on do ensemble techniques what exactly is ensemble and why you need them um thirdly we are going to talk about some of the techniques like random forest bagging and boosting okay i'll give you a good overview on all the three of them and so that will be the videos will be pretty easy for you guys so we'll spend around one hour basically on this part of it uh fourth 30 minutes i'll show you a code on to the same thing so i'll take you guys through a python code which implements almost everything here so we'll spend around 30 minutes on that and also let us say around 40 minutes on that because the bigger code and remaining 20 minutes um i will show you guys um a chatbot today so i want you guys to develop something so as a starting phase let's start with the chat bot uh so this basically this chatbot right now will do it on a manual scale so the the data and all whatever is there we'll type it manually post that i will show you guys how to make an ai enabled chatbot artificial intelligence so till we reach neural networks at least we will be able to do this so that um it will be easy the model will be easy for you guys to understand something like that we can we'll start from this week okay so let's start with uh decision tree so uh guys so far just to give you an overview what have we done under under classification techniques in uh supervised learning what we do we know how to differentiate between ones and zeros okay so we have multiple algorithms like logistic regression then we have k n n svm name base which helped us to understand or train the model and understand which is one which is zero basically so keeping the same concept in mind there is one more algorithm wherein what we are going to do is very simple if i show you a decision tree at a very high level so first we'll see on a very high level and then we will go on to you know calculations steps probabilities and finally we'll implement one manually something like that so let us say you have got a target variable here and you have got some independent variables which is color and diameter okay so now we are supposed to make a decision tree on this so what we will do is first let us say i choose using some matrix i find out that i will give a priority to diameter so i will use diameter first as my separator so i will say if the diameter is say greater than 3 greater than equal to 3 put the stuff here if it is not if the condition is false put the remaining stuff over here i will show you guys how to choose how to give a preference of uh to these stuff but for now just for high level view imagine diameter was chosen first so if you see here currently we have apple mangoes and grapes okay so when i do this first split after that i have got apples and mangoes here and i have got grape here so can i say that somehow unknowingly i have understood that if the diameter is less than three it's a grape do people see that that's a pattern for a grape okay so whenever you get a homogeneous data so whenever you have the target column after the split if the target column is exactly similar that means we have got a trick to or a pattern to identify what is the grade so we call this as a leaf node i'll show you what is the leaf node for now remember this is the node which cannot be splitted further that's it we cannot go beyond this so far clear guys okay now moving on if you observe this particular node we have got apples and mangos still we have got two different types of data we are still we have not segregated how to identify apple and how to identify mango so in this case diameter is gone anyway because all of the diameters are kind of similar here so we will not use this anymore let us use the remaining columns the column is color so if i say all who are green go one side all who are yellow go another side so if you again look at the target columns here we have got one type of variable here also we have got one type of similar variable so i can say this is my leaf node number one we cannot split it further similarly this is my leaf node number two and this is my leaf node number three okay so now we have learned something that if there are some characteristics the answer is going to be something so now for example for my future data if i get color as red for example if my diameter is say 2 identify the fruit so what i will do first of all i will focus on the diameter part of the diameter is 2 so definitely it's going to come here and since we have already done it there is no more split but for example if there was a split further we could have easily said the color is red definitely it's a grape or else if you want to take another example let us say for a future we say color is green diameter is 5 identify the which fruit it is so if diameter is 5 that means it will go here and if the color is green it will go here that means it's an apple this is called decision tree on a very high level so so far so good ashish raju yes perfect good please do stop me okay if in case you find it a little weird because um one i'm sorry yeah so now if we if we dive deep into it okay then how do we do the splits or how do we take a decision of which particular column see right now i've given you only two columns so it's easy for me to demo it but tomorrow let us say in your implementation you have got 100 columns on as an in india independent variables and you have got one target so in this case how to choose the best one so in that case we have got okay so before that give me one minute i will give you the tree structure here itself so prior to that let's understand what is a tree and what all other different terms which we use here in the decision tree so again i will go back to a very simple diagram wherein let us say this is the a tree a tree can have a root node basically which gets filtered out again this particular have will have branches again it's possible that this will have more branches and probably there will be one place where you will have a kind of a leaf so this is the thing which cannot be split further this is the last thing to be taken care of so if you enter here definitely at the end of the cycle the data or wherever you are you are going to come and sit on this particular node no matter how many splits you do this is the final rest resting place for the data to come on the same concept is followed here so if you look at the decision tree how it looks like is let us say this is your first node post this you are splitting the node into two different parts this one and this one after that again let us say you are splitting into two different parts this one this one this one this one this goes on till you find a resting leaf node to it okay so this is how a decision tree output looks like so i'll show you a screenshot later on once we are comfortable with it not all we are supposed to know now so we understood from that diagram that i just drew what is the leaf node what's the root node what's a leaf node splitting what's a branch pruning will come back later parent node and a child node so if you see in my example here this is a parent node this is a parent node this is a child node again for this is a parent node this is the child something okay and yeah so now as i said we are going to choose a criteria so if you observe this data set that i have shown in the left hand side i will say i will start splitting my nodes with temperature or some of you say i will start splitting my node with humidity or some of you say okay fine i will start splitting by windy so how will you decide which is the best parameter to split with any idea guys how will you do it or my question i let me reframe my question does it matter how we split it which one we use first which one you second which one use third and last does it matter actually um guys there's a question for all of you ashish morley yeah it wouldn't matter what the based on which parameter we would split i think we should use we would be using the guinea variable to determine whether we gain or okay choose after we select a particular root node okay so how will that matter basically if i say if i have chosen windy first or if i have chosen this particular column first why why does it matter basically the accuracy of our prediction whether it's a place yes or no it is in fact a declaration okay okay fine so uh if i go back in you know the old school way how we used to do it uh back in the days is let us ignore genie and entropy in all this complex stuff let us try to understand on a very statistical way can i say something like that that if let us let me find a variance that is the variance of this one is v1 variance of this row is v2 v3 and v4 out of all of these the v4 is having the least variance v2 is having the second least v1 is having the third least and v2 is sorry v3 is having the highest variance so if i show you all these variances of these columns which one you think should be chosen the first the column with the highest variance or the column with the lowest variance logically see if i no variance correct that's the correct answer so why do we do this is let us say we choose the if you if you look here this is this one i feel has the lowest variance because it has only true and false so i can say we can start splitting our stuff with windy itself nothing wrong into that so why why do we do that is so that initially itself we have a very good classification true is on one side false is on the other side there is no third variance or third variable here which will create an unwanted nuisance so this is what was the starting of decision tree which people used to use it in older times now we have uh two different criterias one is genie and one is entropy so i'll show you one by one what is gini one is entropy so genie is the one which i have shown here so and the entropy is the one which i've shown here so i'll explain you in detail what is entropy and how do we calculate a decision tree using the entropy concept in detail okay now keeping this in mind yeah okay so yeah so now let's talk about one of these factors using which we will say that the output of entropy let us say entropy will help us to say that the the the column with the lowest entropy should be given the first priority so i will show you what is entropy the name looks very complex but it's a very simple concept instead of entropy you remember it means measure of impurity how impure is your column so basically if you have let us say um in my session like today's session for example today's session is dedicated to ensembl techniques correct but we are currently doing classification technique of decision tree also so i will call this as an impurity why because this particular thing does not belong here what if they would have also put svm over here for example this will become highly impure so today's session i cannot say it's only ensemble there are some classifications also okay so now just see an example here where there is a factory worker and he supposed to his job is very simple he's supposed to pick up an app fruit from this particular left-hand side basket and he's supposed to pick up a label from this side and he's supposed to paste them together and put it in the box for delivery that's this job so here if he picks up any randomly random object from here and any random object from here it's going to be similar it's going to match basically that fruit versus the label on the fruit it's going to match both of them are will be hinting on apple only correct so in that case the impurity will be zero there is no uh error here or there is no impure data here hundred percent delivery will be good what if the same worker has given a task with his habit of doing the same thing he is given a task of now he is supposed to handle apple bananas and berries together okay so now he is as old habit he will pick up any fruit and he will pick up any label and he'll paste it together so in one of these stuff we found out apples and berries like this okay so in that case what's going to happen this is not going to be zero so entropy will be definitely higher high higher because this is not matching anyway so it's highly impure data yeah so from the formula side if you look at it it is very simple negativity of the probability so basically negative probability into log of the same probability and how much ever you have you have to sum them up together to give an example let us say probability of yes is percent probability of no is fifty percent so when you put fifty percent here fifty percent here and calculate so it will be like uh minus of point five log of point five so it'll be twice into minus of log of point five if you do that we are going to get highest entropy that is 1. so if you look at the formula bar here i have just replicated the same thing in the form of a formula where i have put our probabilities okay so for example if i change this up let us say i say probability of yes is 80 and probability of no is only 20 sorry 20 in that case entropy value will come a little down because we have got good amount of majority here what if i make it 0.99 and make it 0.01 in this case it will be almost zero and what if i make this one and i will make this zero okay because of the issue it will not take it up but for now just understand it will be completely zero something like this which i have shown here okay so this is what is entropy that's it are we good everyone okay yeah perfect so this is entropy so now what we'll do is we'll try to find out entropy for all of these columns one by one i'll show you how to do that and we will put it as a table so if you see here this is the entropy of all our columns and we'll find out the terms which are having the least entropy so that we can start our splitting using those columns so before moving on to there now let me show you one example of decision tree how it looks like so this is how a normal decision tree will look like in the industry okay and if i enhance it bigger if i make i'll show you one of the nodes let us take a root node itself just just just to interrupt yeah um in in real life you know are we doing this type of calculation so everything goes with the python uh no in real life we don't do it like this python will take care of everything okay just to show you know how how the back end everything is excellent exactly my problem is um basics so see what happens is uh when when we directly jump onto uh this thing uh what is entropy and if i don't show you what is the calculations or not yeah the understanding of decision tree splitting so once you implement a decision tree let us say this is your decision tree that you implemented okay now tomorrow there are chances that this is going to overfit you people understand the concept of overfitting if not i'll show you what is worth yeah this is a very good case of overfitting overfitting means too much of data or too much of management done if you see here there's two too many leaf nodes flying around so in that case concept of entropy or knowledge of entropy will help you basically that's the reason i sometimes i go into pretty basic stuff and show you calculations on excel file okay perfect so ah just having i i'll we will revisit this nodes again but just to show you what is happening here there is a column named plas and the splitting criteria is it should be less than equal to something if it is okay go here if it is not okay go here something like that the genie so this particular when i took a screenshot i had implemented gini on that so it's again a splitting criteria which chose this particular column so if you see the genie genie is kind of a number which gives us a hint which which basically which column to choose on next is how many samples are there how much data is there in this we have 537 samples here out of that 354 are going this side 183 are going this side so when you add them up you are going to get 537 again and this particular box the majority class is 0 the target class that you are looking at this is the target class so if this was a leaf node so let me show you how a leaf node so this is a leaf node so if a data is sitting here finally it will be of class 0. so the target column has currently two classes zeros and ones if you look at this this particular leaf node is gathering all the ones this one is gathering all the zeros which satisfy the above conditions guys simple right so we'll revisit this anyway we'll generate it in python i'll show you but this is how the output looks like okay so now are we good are you guys frankly speaking are we good looking at this do we get any feel out of it or it's um it's not that great guys if i give you visually this that this is your answer is it good to see this it'll be confusing right complicated complicated so this is called an overfitting issue so this might be a good model but good model only while training so let me give you a view on this and what what do we exactly mean by that see it's very simple guys have i uh have i given you any example earlier on overfitting what do you mean by overfitting no i don't know right okay i'll give you a live example so uh for example i have a five-year-old daughter okay and uh say this girl uh uh is already started some stuff on python so when i mention sits and then homely environment along with me for example and if i give her certain stuff to add subtract multiply delete print whatever i give basic stuff at least 90 percent of the commands she is able to execute properly for example now the same thing is for example there is a competition and i take her to school okay say there's a school and i take her there the same person the same person who was trained on to doing all the commands let us say the efficiency of this girl over there becomes 75 might be the environmental change or there might be any other change but the same command which i gave here and now when i give here she's able to do it only 75 so this is called overfitting overfitting means during training the model because your model is made as for the training data everybody agrees to that correct we are using training data to make the model now testing data is a little different data the columns will be same but not necessary that the pattern within the data will be exactly similar so in the testing phase usually the models are having a lesser accuracy if you guys have observed in the previous models never training and testing data will be perfectly same as accuracy this is called over fitting issue okay for example instead of 75 if she would have done 87 then i will say it's not an overfitting because the performance is almost comparable but if i say 75 the difference is 15 percent that i feel the model is capable enough to do this much but due to certain situation it is not doing that so to overcome this we usually have multiple techniques for decision tree the technique is called pruning okay in pruning what do we do is if you can count here how many levels we have if you see here how many split levels we have let us quickly count it up split level one two three four 5 6 7 8 9 10 11 12. so we have got 12 different levels of split performed here this is a possibility that because of we are training our data and we are you know creating this complex structure that might be a possibility that the testing data will get confused and sit in the wrong boxes or wrong leaf nodes basically so to avoid that what we'll do is we'll divide it by two so in that case i will say divide or have the depth of this data as only six for example sometimes it is less than fifty percent let us make it four or five whatever so when i do that what do i get out is i get a model like this a model will look like this now can i can i ask you guys if you see the previous decision tree and if you see this decision this is kind of okay for us to visualize i can say at least yeah same is the case with the testing data the testing data will not get that confused here now let me give you an example of this very simple let us say you are calling airtel for example and there is an ivr so it will ask you um please enter your language so it will ask you english say hindi from where i am is canada then let us say other states marathi and so lot of languages we have in india so let us say initially itself you have so many options now after that what's going to happen let us say i chose english this option now once i go in it's going to ask me say prepaid and postpaid let us say i go for post paid for example so under post paid what all options we can have we can have billing we can have complaints we can have uh add-on offer or something like that so let us say you have what four or five options like this now let us say i choose billing and inside billing it will ask me do you want to know the latest bill you want to pay the bill you want to have a blah blah blah so a lot of things will happen so imagine how complex so for one particular language is going to go this complex so for these languages also it's going to go pretty complex so this is the model that we showed we saw first which is around 12 levels something like that so it is good it will work very nice but tomorrow let us say if a person comes chooses this language but then somehow in middle he wants to do something else she has to go back and again go through all the pain so i will i say that adding too much of uh you know um like not accuracy i will say comfort like this is also not good yeah so this is kind of an overfit model so instead of that what if we just say there are only two languages say english and for example hindi okay or as the local language of the state and then inside also you'll have a limited uh silos you don't have to go around in and around so this will kind of it'll be easy for the testing data to rest otherwise if an unknown data comes unknown data will get scattered here this is one example if you did not get the pruning part no issues i will show you again in the python code also but as a very high level view pruning is used to reduce the overfitting issue usually decision tree 99 percent of the models will overfit this is the observation from my side you always have to go ahead and do pruning in our decision and we will see how to do pruning later on guys so on a very high level can i say at least we got 20 30 percent of pruning what is pruning and what do we mean by that ashish nitish raju murli are they good yeah perfect so now next one next one is uh yeah so this is what is pruning so when i share this excel file with you know this will help you to understand what is pruning and criteria splitter max step you will see in the python right now what do you mean by all those things now coming on to the concept of information gain so now information gain is kind of related to entropy here so you can use two terms either you can say i will choose my columns as for entropy or i will choose my column as per the information now they are kind of reversely proportionate to each other that means more than entropy lesser will be the information given by that particular column so let us say if you have got three columns the entropy is one here entropy is 0.5 here entropy is 0.1 here that means this is the column which is lowest entropy but it will have a good amount of information present with an interval which will help us to split so it's not important to know the formula but just for the calculation part of it i have shown you guys a formula okay so this is information again now look at the data set that we are going to do today to split let us say if you guys remember our knife base session the same data set we used the golf one where there is sunny overcast and rainy as a day temperature is hot mid and cool humidity is high normal windy is true and false and depending on this data the golf club is to decide whether to keep the golf course open or to keep it closed okay so now the same implementation i want to do it using decision tree now the good thing for decision tree is guys if you remember svm if you remember naive bayes if you remember k n n the limitation to the target column was around two variables one or zero true or false yes or no but in decision tree it can be multiple it can be one zero two or even it can be four you can do these many not a problem okay so now let's see uh how we can use decision tree to split it so for the steps part what i've done is ah nothing to get confused here i will show you step by step this is what happens on the back end of python this is how probability tables are made and stored in python okay but in the real life thing we just have one liner code which will do the complete job for us in python decision tree um function okay now the first thing the aim here is to get this done so for now imagine this is empty this is the final aim so now what i'm going to do is i am going to pick up one of the columns i will show you how to build up the table and remaining columns are self understood you will get it so we will start with the data set first column that is outlook we'll start with this one for now okay so say in outlook we have number of yes is nine and number of nodes sorry in the entire data set number of yes's is 9. so if you just count the number of yes's here we have 9 and number of nodes are nothing but 5. okay so 9 and 5. so the entropy of the whole data set or entropy of the i'll say target column basically the entropy of the target column is p of yes is 9 by 14 p of no is 5 by 40 as simple as that okay so 64 36 so if you put into entropy formula that we have this is the entropy formula if you put both the probabilities you are going to end up with 0.94 as a entropy so one quick question for all of you this particular column is it pure or it is highly impure point nine four is the value so is it pure or impure it's highly impure highly so this is the indication that number of yes's and nos are kind of out of sync they are not completely what you say 50 50 c if in case this was 50 for example if this was 0.5 and this was also 0.5 it will be completely impure why because now we'll get confused which one to choose because they are both in 50 15 number but for now it is yeah so for now at least we are okay that yes is more knows are yes but still the impurity level is very high so now coming on to the individual entropy part of it where i'll give me one minute guys i'm sorry yeah so coming on to the first column that is outlook so what what are the splits we have we have sunny we have overcast and we have rain so keeping this in mind definitely this is a impure data set it is not completely sunny it has got three more competitors to be dealt with here so in this case the first one sunny there are total five occurrences so uh number of yes's so what do what do i mean is if it is sunny so let me choose if it is say sunny okay and if it is yes how many we have data only two okay and how many we have no we will have three no's okay so out of five out of five in total three or two of three of them are yes's two of sorry two of them are yes three of them are no so these are the probabilities and this is the entropy of sunny similar to that we will calculate overcast now overcast the good thing is all of them are yes's there are four occurrences all of them are yes's so entropy are zero there is no impurity at all all of them are turning out to be yes so if for future if we say that the day is having the day is a type of overcast definitely there is no impurity there same way we'll find out rainy we got 0.97 so this this and this is your impurity factor of that particular column okay now how will we use this particular formula one minute where is the formula here we'll put the details in this formula to calculate information gain and we'll put the final steps so for that for information gain um sunny is equal to 0.35 how sunny is nothing but 5 5 by 14 five is my number fourteen is my total occurrences into the information are the entropy that we have this is going to give us the information gain of sunny over cash anything you multiply with zero it will be zero only and the third one is also similar 5 by 14 into 0.97 that is 0.35 so when you add all three of them you are going to get the total entropy of the particular column okay next the final information gained the formula is total entropy minus weighted average into entropy of each feature so now what is the total entropy 0.94 okay and what is the total information out of the outlook this one okay this is what we calculate here is the bracket and one so when i subtract these two and when i round it off we are going to get point two five so this is like zero point seven eight something so that is the reason we are getting point two so this is the information gain of the column this is the entropy of the same data i am going to push it here this is the entropy this is the information okay similar to that i will calculate for temperature humidity and windy nice it's okay now yeah at least the part you got it right yeah perfect so now we will try to create a hierarchy here that which particular column we will use first to start this split so for that we'll start with entropy find out the uh what do you say column with lowest entropy that will be outlook next humidity third windy force will be temperature correct and the same stuff should follow with information gain but information gain will find out the column with highest level of information again that is if you compare all this this is number one ah this is number two this is number three number four correct now this and this should match it's matching perfectly matching so we can go ahead with the entropy and we will start our split part of it so the goal to show you this probability table was this is what happens on the back ends of the programmer who has written the decision tree classifier within sklearn has basically returned this logic to give us a split like this okay so once we are done so please remember the hierarchy the hierarchy will start with outlook then we'll go to humidity then we'll go to windy then go to temperature so now let's start the split here we have got the data set so let me expand a little this is my data set so i will start with my i give the preference one two three and four okay so let's start with one so how do we split this we have three splits sunny overcast and rain so here i've done it if you guys can see tough so one minute i'll just make it middle yeah so this is all sunnies these are all overcast and these are all rings now the thing is you have to focus on the target variable look at the target variables for all three splits is it homogeneous guys hostile entropy is here is there entropy here okay so we cannot say it's a leaf node look at this one the right side one still has impurity yes look at the middle one it has no impurity correct that means in future if if the client says tell me the probability of the golf course keeping open if the day is overcast i can blindly say answer should be yes this is a leaf node number one so that's why i put it as a red one which is one some of them it's very easy some of the pattern is very nice where within first split on itself you will get the leaf but not always possible again depends on our data now first one done what's the second one to start with we will now go to humidity and we'll split them up so let us say this particular table that we have we will use humidity to split it now humidity can be uh what is that high or normal so here all our highs here all are normal now again go and check the target variable do we have entropy impurity no do we have impurity no so this is my leaf node number two is my leaf node number three so now if i say that my futuristic data is sunny okay and my humidity is high if i say that i can easily give a prediction that keep the golf course closed right because it's going to end up to this particular node simple next one is windy so let's try to split the remaining ones with respect to windy so if you see windy wind is true and false so this one is all my trues this one will be all my falses if you look at the target column again we are able to find there is no impurity here so if i say that my futuristic data is rainy and also it has wind uh is equal to true so basically it's purely windy in that case golf course is closed why there are parameters if you if you know if the golf ball is a long shot definitely the ball is going to deflect because of the wingspan so in that case logically it fits out so basically whatever exercises we did we found out the patterns within the data that's it okay any conclusions no it's pretty straightforward so there is no you know hidden stuff at least in decision tree but majorly decision tree will be an overfit problem so uh after some point you will stop uh using decision tree and we will use random forest itself so now let us come to a concept of random forest and then we will go to ensemble so before that uh simple pros and cons so for the pros part i will say it is very simple it is very handy um if in case the data is not huge if in case the columns are not more in number it is very good but if in case a column number increases then the split also increases and again the diagram will become as horrible as the one i showed you with a lot of splits around so in the negative part i will say it cannot handle much of the columns and also overfitting is a very big problem okay so now okay so just in case um let us say this particular implementation would have gone for two more layers of the splits for example and we found out that the training let us say found out there was an overfitting issue so what we could have done we could have removed one of these splits and we will directly out of four splits we will bring them to only two splits we'll say we do not want much depth into it we just want two splits till we get our leaf nodes so i'll show you in the python how to do that but for now this is called pruning now moving on to a concept of random forest and then i will show you what is ensemble so because random forest and decision tree are perfectly same except some technique so if you see here i have got a data set which is this is the target variable and these are the attributes attached to that target variable now how can i start let us say if you look at this this is my decision tree if i say if the fruit so for now please ignore these bags and all that for now just imagine if this is a decision tree if the diameter is greater than three if it is false put the fruit here if it is true put the fruits here entropy zero entropy one so still the entropy is one go down now down what will happen choose another factor to split what is another split right if the color is red put him here if the color is orange put it so now can i say using this decision tree i have classified cherries oranges and apples nice correct everyone got this yes yeah good now how many parameters we have used we have used diameter and color we have used only two of them still two of them are not used so let us try to even include them so here i will start with color and i will put diameter okay here i will start with color i will say what is the shape of the fruit here i will start with diameter i will say what is the season next one i can say what is the season i will start with what is the shape we can do multiple combinations here right so we are not talking about entropy here i am not saying that entropy will choose this okay in random forest there is no entropy at all that's why it's called random randomly you choose any column make a decision tree randomly choose another columns make a decision tree decision tree so these are called estimators the one which you see here one two three four four decision trees are running parallely so in this case n estimators is nothing but number of so this is this is the way random forest is exactly similar to decision tree except in decision tree you will have only one run in random forest you will have multiple runs like this ok now i will show you why do we have multiple runs and all but for now remember one simple concept whenever you implement decision tree immediately after decision tree you implement random forest also and definitely you will observe random forest will give you a better accuracy than decision tree okay nice simple so far now i'll come come to uh what do you say ensemble techniques okay good so now what is an ensemble technique again please do not take the words and the complexity of words this the names are very complex but the concepts are very simple so for me what do i mean by ensemble is very simple you remember like this assemble that's it now there are multiple types of ensemble techniques that we can do now as an overview what is ensemble is let us say this is your data set which looks like this okay in all the models that we have done so far what we have done we have asked only one person that identify this and tell us what it is if this person whom you trust tells it's an apple we directly take it as a face value and say yeah it's an app okay this is what we have done so far what if i increase the decision makers let us say instead of asking one person now i am asking five people that all five of you can you please identify what is this from my data set i've picked up a data tell me what is it so some of them are saying it's an apple some of them are wrongly classifying it as an orange so now what i can do is at least i have a majority here so i can say with some confidence that three out of five people are saying it's an apple definitely let me vote for it get the majority and i will say yes it's an these are called ensemble techniques simple guys yeah are we good with it a high level game of what is ensemble and for your better understanding i have also given some text you guys can read it across once i share it with you but so far whatever i explained the same thing goes here now there can be multiple types of ensemble techniques available to us so sometimes they all are of same algorithm they all belong to the same algorithm example is say random forest okay when i showed you random forest i told you guys don't worry about these i'll show you what is this now all of them are using decision tree method itself except entropy everything is same so all estimators that we have all the people that we have who are judging the fruit are from the same background so either you can have an ensemble technique like that or you can combine them let us say so far what you have done you have done linear regression you have done logistic regression you have done svm you have done uh naive bayes you have done decision tree put all of your algorithms on learnings together give them one set of data let them identify it finally take a vote and tell whatever is the majority is your classifier simple so two ways it is either you have all of these people of the same class or category or you differentiate them so for today's session still you guys till you guys are comfortable with ensemble i will show you um techniques like random forest boosting and bagging okay where all the people who are identifying your stuff belong to the same model not a different model and in the next session something extra i will show you guys where we'll bring in all this svm and all together and we'll build up a model okay so coming to now again back to random forest so i hope now you people got it why we have multiple bags i have shown here wherein each one of them is nothing but an algorithm so now what i am doing is not possible for me to take whole data set and put it in the uh model it's not possible so for that reason we take a concept where we randomly choose a data we randomly choose stuff from our data set training data set and we put it okay so guys when i when i say randomly chosen data do people see any challenge here so let us say let us go back to a very basic concept where i am saying i have got five columns or five rows i'm sorry and you will have multiple columns here this is your data set now when i say rand or i say random of let us say this data set name is a when i say random of a what it's going to do it's going to choose randomly five values for me so let us say choose one two then again one one four another possibility is one two two two two possible another possibility is one one one one three correct you people see some challenge here a repetition repetition exactly so what's going to happen if you repeat the same rules again what's going to happen the result will be similar exactly what is it called think about it you guys are you are on right track the only thing is i'm trying to you know make the answer a little bit better so the prediction won't be right because uh it's of the same data values correct so can i say biasing my model the machine learn let us say this is your machine learning model correct yeah if i give a similar data what's going to happen it will be biased towards certain type of learning and it will in future when a new data set comes it will not be able to identify itself an example is let us say out of these four techniques i always keep showing you random forest and i have never shown you uh bagging boosting so what will happen in future whenever you have a project you will use only random forest you will not or you will refrain from using other that might not be very good the same issue will happen with your model the model will perform very good on training data set but it will not perform on the testing data it will fail on the testing so in turn biasing will lead to what unbiased yeah biasing will lead to overfitting yeah yeah okay so this is what i was trying to show that since we are doing randomly we are choosing it randomly we are aware or we are unwantedly creating this kind of problem okay this is called out of bag error so what is out of bag let me go back out of bag is if you look at this particular sample that we have taken can i say 3 and 5 is missing yeah yeah from this one i'll say 3 4 5 is missing from here i will say 2 4 and 5 is missing this is called out of bag so our model our model will not be able to get trained on these things yeah right got it guys yep yes yes there is one meter i'm really sorry yeah okay so this is called ob era so we'll talk in detail once we come to a perfect algorithm which explains us this but random forest is a very good model except this issue so we have an option in random forest where we can switch it off switch it off means we will not acknowledge that this error is occurring so we will ignore that we are missing certain values out of it that is the reason that these models will be let us say random forest gives us an accuracy a decision tree gives an accuracy of say 80 percent in the same case random forest will give us 90 percent better than that but still the remaining 10 percent we missed out it will be perfectly and only because of this issue so i will show you guys later on how to deal with it and uh you know how to make sure everything is okay yeah perfect so this is called random forest any questions for me on that not for my own correct okay so uh keep uh the concept of decision tree and random forest aside apart from that we have got different types of ensemble techniques again depending on different type of data sets and variety of data sets we have so some of the data sets might not be that good in splitting so if you see here the data set that we had add some good variety of splits possible sometimes not all the data sets have that so what are we going to do is there is another technique which we call it as bagging so bagging is the short form of bootstrap aggregation now guys this is the content of next week please remember that so why am i doing this is we'll do this in detail again random for us boosting and bagging again next week but when i do this no i have seen usually the batches they are very happy or they are very okay or you know the videos goes very well for them so that at least you will come to know you have a good idea what is it and when you see the match behind it from the professor's video it will be it will be better but that's the reason we are doing this but it's not a part of this week anyway so now what i'm doing what i'm doing is there is something called bootstrapping and then we are aggregating it and we are finally uh building a model onto it so what does it mean it's very simple for example imagine yours this is your train and test split data okay as usual as i said we will randomly choose a data this is what we call it as a bootstrap we'll randomly choose some data from training and will push it to these boxes as an input to some model okay we will do it for n number of times so if you say here if for bagging number of estimators for me is 4 that means 4 times i am going to do the same split like this yeah now in this algorithm the originality within the data set should be at least 63 percent now what do i mean by that is let us say again we have got this as our input data and i give when i do my random stuff and i put the data in the boxes let us say this is my data so if you look at this the originality here is only two two unique numbers but the total is five so two by five ratio when you take it should be at least 63 percent of data original if not then the model will kind of fail okay so this is called this is a kind of warning or a hinter that not always bootstrapping helps a lot now what do we do is we push this stuff to the model we train the model we put in our testing data our future data to check it we get the output okay you compare the output with the original so predicted output versus original output you compare and whatever you get you take a majority based decision here so let us say here we got one here we got one here we got one and here it's predicting 0. so in that case we can say that all three of them are or out of 4 3 of them are saying 1 that means the predicted output should be 1. okay so this is all this is what is called bagging which is bootstrap aggregation the only thing you will see in your videos is you will see this kind of diagram in the video and what i have done is i have transformed this diagram to a very simple diagram something more visual which looks like this okay here i have described the same thing as i said so you can go through this this is what the professor discusses in the video it will be easy for you to grasp so again the complete organization of this algorithm works on you take the data through multiple stuff and finally what you do is you aggregate them and check what is the majority okay this is called aggregation yeah christian what is the difference between backing and a random forest they look like the same yeah here we are using decision tree as a split correct see here how we are splitting is the decision tree concept here we are not using decision tree here we are using some other algorithm so we will see it next week i'll show you in detail what all we use to classify okay but the input input you are perfectly right exactly as per the random ob error so here also we are having ob and here also we are having guys are we good with the bagging in a very high level view so each of this bags represent a different algorithm right uh yes no uh actually no no actually no uh the the one which i told you the different algorithm one that you have to do manually but in bagging they have their internal calculations some internal classification model is going on so each one of them will be similar algorithms not a different one okay okay okay so i'll show you the match part of it also behind so probably once you are good with the videos next week i will take you guys through also what model is there and what we do okay good so coming to another concept of boosting now boosting is kind of similar to svm i will show you how and why so here if you see we had n estimators different estimators here also we had n estimators in boosting we'll have only one estimator but why we had this an estimators guys so that we can have decision on majority here we since we have only one what we'll do is we will take this estimator n number of times in a feedback loop now what happens is same ob issue happens we are bootstrapping it okay we are taking randomly chosen variables let us say this is one two three and four for example or else to make it simple classification model let us say this is zero zero zero and 1 and the output that we get got might be 0 1 1 1 something like that so in this case what will happen this 0 and this 0 is matching remaining 1 and 1 is matching remaining these two are not correct so what we'll do is we will take those two inputs error erroneous inputs back and those inputs now mandatory will occupy the first two places of input these are wrong luckily wrongly classified okay and then we'll fetch new more data here so now my output should be let us say 0 1 1 0 something like that so now i found out 0 and 0 matching 0 and 1 not matching one and one matching one and zero not much so now again i will take this and this is an input and mandatorily they will come and sit here and remaining whatever is left out will be filled out by other data this process goes on till you get the least error okay or till you get repeated output so let us say this is 0 0 0 1 this is our original stuff let us say i keep getting always 0 0 1 1 if i did 100 iterations i keep getting same so it's ok we are having one error remaining 3 are good so either you take this face value or you take this whichever is the best one the model will stop at that point till you get the low errors got it right so instead of having multiple estimators here we are using the same estimator multiple times to get a correct answer okay now in the videos one small hint for all of you in the videos the professor might be talking about giving weightage to errors that means whatever is error here no we are putting them back this is what he means by the video so it's just observe this particular sentence he repeats many times sometimes some of the students get confused what do you mean by weightage to errors this is what it means by that question just to when we input the data in this algorithm we don't input all the data we just input a random sample and then we keep on iterating it right we do we put all the data in one go and then we no no no it's a random sample you are right okay a random sample that is the only problem i feel with ensemble techniques this particular thing this draws us back sometimes what we do is if you do not want to use random you want to don't want to use a boosting classifier you can keep this algorithm in your mind okay and you can design your own function which has no random values which has got uh what do you say top first 10 values or first 20 values something like that you can pitch up every time we can design our own loop also nothing wrong but the only thing is we need time and you know skills for that so that's the reason i always adjust and move on but if we remove this particular thing no randomness over here then the algorithm works very nice but i hope you got uh the picture the negative feedback loop that we do here is will compensate for these things okay so is does it look like our svm do people remember svm right it had learning rate you remember the same learning rate is going to repeat again in boosting okay and do we check if for example in the first iteration we said the two variables are correct so these two data points are correct so do we check even the last iteration of the algorithm comes in the the data that we have said that's okay in the previous situation so we go back and check that whether they still hold for the final duration no we don't get a check but we generate a confusion matrix so we'll come to know at the end what is the current status so here the matrix is confusion matrix so again we come to know how many of them are identified correctly once how many of them identified correctly okay yeah perfect so this is called ensemble techniques that's it we are done okay now there are two type of boosting again so let me if in case you feel it's confusing too much then do stop me else the one which i showed you above was ada boosting or adaptive boosting the same diagram i have represented in the form of a visual diagram like this there is something called gradient boosting also wherein ah if you observe here if you observe here what we are doing is let us say in the first iteration you are able to split between two okay let me expand it one minute guys we understand the concept of gradient what do we mean by gradient do we get it a little bit but not done a gradient is a kind of line which separates two stuff that's all you think about gradient for this case okay now just observe here let us say there is a data set which looks something like this for example okay and we are supposed to divide the stuff between these two now look at this particular gradient there are chances that there see if uh if you see this particular point and if you see this particular point it is not necessary that they will lie in one zone and these people will line one so what i'll do is i'll increase one more gradient here like this so let us say i i understand that this is still not a good classification and i increase a step like this okay so if you look here now look at the second increase step i can feel that there is a good amount of classification between the points on the top and on the below so we are able to gradient them and on the last one if you see if i increase my step here and if i increase one more step basically here we are going to get a very good amount of segregation between let us say two different set of points if you remember svm guys you know we did somewhere here somewhere and then we are putting a line what if there was a non-linear data and in that case what we could have done is we could have put a gradient like this so rather than putting a straight line between the points let us say one set of points was like this another set of points was kind of like this so rather than if we are not able to put a straight point what i will do is i will put a gradient like this correct guys this is what is the gradient boosting this is what is your ada boosting usually their accuracies will be perfectly or nearly same plus or minus five percent why because both of them are using learning rate the only thing is we are using a line here and here they are using a concept like comparison of error and learning rate something like that okay good so these are the two popular boosting techniques and now i will show you a python implementation where i will show you all the different algorithms together and we will compare all of them and we will check which is the best and will move on so this is the algorithm wind quality now let me show this the data set let me show you what the data set looks like if you look at the data set the data set is coming from a wine yard okay and this this vineyard basically has some barrels and these barrels are some of them are one year old some of them are 10 year old and some of them are say 30 years old for example every week let us say there is a wine tester or a quality analyst who will come he'll put a machine or a you know some kind of strip so which when comes out it tells how good the wine is and if depending on that they classify the wine as say quality number five quality number six quality number seven something another quality number eight might be we don't know so this is our target variable and these are our independent variables okay now look at the target variable guys look at this target variable can you use any other technique here which we have done so far i'm not talking about ensemble technique okay apart from those today's session whatever i've taught you will you be able to use any other technique what do you know so far yeah we can use sbm we can but look at the number of splits we have five six seven and eight we have four different splits here or else if i show you total number of target variables let me see linear regression okay i agree linear regression could be used yes but if i talk only about classification techniques in will be able to use uh log r name base svm k n here no right because the number of splits is too much svm will go crazy onto this direct or not svm makes only one line how many lines we have to make one two three four five six lines not possible right okay okay so this is the line of difference where you have to choose that since the target variables are more than two in number they are going to go ahead with decision tree or any of the ensemble techniques correct guys this is a way you will make a decision in future because i have seen many times some of this batches are sometimes confused that this is my data set what algorithm should i use so this is the way you are supposed to choose it on yeah now look at the independent variables now independent variables are something like the quality or the chemical composition of wine say acidity volatility citric acid sugar chlorides alcohol sulphates so a lot of attributes attached to it and depending on that we are supposed to make a machine learning model so that in future when the company gets these values in the computer the computer should directly tell that this particular barrel should be filled or should be sent to the bottling unit and to be filled in quality number five body okay depending on the age and also we are not worried about how it happens but this is what we are supposed to do this is the business problem so keeping this in mind we'll move on uh the first thing i usually go and check is the data types because sometimes even if the data data types looks like let's say number sometimes it comes as an object so just have a look everything should be good everything is okay size is 6 1600 cross 12 and the total is if you multiply these two together you are going to get around 19 thousand one eighty eight data points okay it's a medium sized data now moving on uh statistics so for this let me increase the size of my yeah so if you look at it the most important columns to look for is mean and standard deviation so if we can quickly look at it yes there is a high value number and there is also a low value number now not always we can say it's a scaling problem why because some of the units so we should also go and check the units of it so for this particular case we will ignore it that okay there might be or might not be a scaling problem let's kind of ignore it and also for further analysis you can go and check all the minimum maximum values you can check your iqr part from this so guys i hope you guys are okay with this now right the eda part of it yeah so we are good with understanding the dot describe table and all know so now now next thing is whenever you have multiple target variables available there are chances of target imbalance so have we talked about this target imbalance earlier or anybody knows what this problem is see look at this target so these are my targets yeah we have talked about it okay perfect so i hope you people got it then so look at this quality number five quality number six they are in highest and when you compare it with other quality data the data is very less so my machine will not be able to learn effectively for quality number three it will have some learnings definitely but not as good as these two so what are we supposed to do last time if i would have explained you guys target imbalancing uh how to solve it and also either you can do those techniques up scaling up up sampling and down sampling or else in today's session i will show you some innovative way to move on so one way is uh called combining the target so if you see here number of fives six sevens fours eight and three these are my total targets what will i do is i'll combine three four and five together as one i'll come keep six as it is i'll combine seven and eight together as one yeah so now when i see to it this is a huge target imbalance problem and when i see this at least the target balances kind of solved i will not say perfectly balanced but yes it's better than what we see above now again all these things are to be done uh post approval so we are not sure how critical it is for a client to combine these two together because the quality might be different or how good it is to combine three and five together so it's like i will say uh if we are buying the say a base variant car okay and i am combining the base variant with mid variant and we have got high variant for example okay i cannot say i'll compare combine these three together because this one and this one is going to have a huge difference whereas this one this one is kind of okay i will say some features are up and down but definitely not good so if in case you are good with it and the client approves it nothing wrong in doing something like that at least it's going to solve the problem without disturbing the data okay so this is how i have solved the target imbalance for this particular data set next if we want to look at the distribution of each and every variable just for sample i have shown you guys a fixed acidity here so if you look at the you know this particular curve i can say it's a kind of normal except it is skewed on the right hand side okay so kind of follows normality now coming to the heat map to understand what features are important now guys be very careful if you have multiple features like this no it's not always necessary that all of them will be contributing equally or majorly to your answers so in this case have a look at it so compare your target variable with all the other variables if you look at it alcohol plays a major role sulfates yes uh from the positive side again citric acid is a good one volatile acidity is negatively impacting so there are certain factors which are highly impacting like this certain factors which are slowly impacting like this okay so just for our understanding tomorrow if you find some of the factors are kind of the the the relationship is kind of 0.0001 in that case you can say that i'll i'll remove this not a big deal for me yeah so the covariance or relationship is very less you can remove it so this is where i use a pair plot or heat map or covariance correlation matrix so this we just saw no now moving on the first thing that you have to do is your train and test split so as usual i am doing the standard train interest split with 70 30 ratio and some random state so i hope last time i think we would have discussed on random state uh as low as possible it's good it's going to take more time to generate the train and test set if it is very low but yes keep it around less than 10 percent of your original data set size and i usually choose a very low number but any random number next we will start with our decision tree classifier so very simple this is the name of my model this is my decision tree classifier and the criteria that i am going to use is entropy another criteria you can use here is gini also nothing wrong okay but since we talked about entropy today i will show you entropy next session i'll show you genius so when i do that you are making your model post that you are fitting your model on to what on your training data now the only problem here is we see is if you see here there are so many hyper parameters available in this function if you look at this decision reclassifier function there are multiple parameters available inside which we never use it so by default python takes it as the default values except we have used currently entropy now you may ask me why don't we use it for the first implementation it is not advisable to play around with this but tomorrow when we are finalized with this model when we say this model works perfect and we are okay with it then we will play around with hyper parameters to improvise on the accuracy let us say the accuracy of this model we got it as 90 so tomorrow when you will change some of these variables there are chances that we might bring up the accuracy by two more percent say 92 okay now what are these parameters i will go in detail don't worry but for now we are using criteria and when i put my training score my training score will be always hundred percent for decision tree any idea guys why 99.99 of the times your decision tree score will be 100 any idea why because it keeps on splitting until the entire data is a there is no entropy correct exactly so the whole concept of decision tree is built on training data only okay so unless and until it finds a very good uh leaf node it's going to give a split that's the reason the accuracy will be very high now look at the testing data guys testing accuracy you find the problem so when i use the same on my testing dataset it kind of gives me 63.75 percent so there is a difference of around 37 agreed so this is an overfitting issue and 99 of the times you will get an overfitting in decision tree so to avoid this what we'll do is we will do pruning now pruning is very simple is you choose your same classifier again from above give your criteria as entropy and if you see there is a feature called max depth in your hyper parameter list so max depth you make it 50 or around 50 percent of your total features so if you see here how many features we have we have around 11 features correct so what you do is you divide by 2 so we get around 5.5 so you can say my max depth where is it gone yeah my max depth is around five or six nothing wrong okay so once you do that i'll show the accuracy and then we'll go to the table so once you do that you get an accuracy of 78 as your training and testing accuracy of 61 so now look at the above model which is 1 is to 63 and look at this model which is 78 is to 61. still there is a difference i agree but which one you think is better or stable guys which one is more stable um 35 35 means line item number 35 oh line oh yeah correct the second one correct so again you would have observed that the accuracy has gone down drastically training accuracy is just 78 now earlier it was 100 percent that means on training side it was very perfect and even on a testing site from 63 it has gone down to 61. so i agree the accuracy has gone down but at least the stability is good why if you tell the client that we are giving you ideal model so he'll be expecting ideal outputs in future but when this happens he's going to come back with an escalation rather than that we'll give a stable model we'll not also claim too much we'll say 78 percent and if we gets around plus or minus 10 15 percent it's okay okay so this is it and uh just to visualize this if in case you want to visualize this this is how it looks like so the same graph which i showed you on the on on this one where is it gone this is the one okay so what i've done is python is not that great to be visualized here so what i do is i store my images on our local directory using this particular command okay this one good so this is how you can do it it's not mandatory to you know do this graphs and visualize this is just for our purpose the major part lies into our stabilizing the model so this is your decision tree classifier okay okay perfect now there is one more uh additional feature using which you could be able to improvise on your accuracy which is called feature importance so feature importance is an inbuilt feature of decision tree so if you if you see here this is the name of our pruned model the the model that we pruned above this one okay the same model there is something called compute feature importances so wherein python will be able to give us the importance or how much each feature is contributing towards the decision making okay so if you look at it alcohol sulfates acidity these are the ones which are this is contributing 32 percent 13.9 percent 13 so this is this is one way to find out which features are important and which are not important like these ones which is kind of collaborating with the heat map that we saw alcohol sulfates acidity yeah as you observe this same numbers we are getting krishna can you explain how do you read a heat map it's okay heat map okay yeah yeah let me show you so heat map is like okay so let us say for example you have a matrix a okay what we do is we transpose it up and when we multiply these two so what we are going to get let us say there are three values in your matrix and when we do this dot multiplication we are going to get a 3 cross 3 matrix for example so this if you if you if you see the diagonal elements will be perfectly same so if i compare a and a i will get an amplified version of a if i multiply a and a what i am going to get an amplified version of a similar to that if you can see here so all the diagonal elements if you see there will be perfectly one okay and the other side so these are mirror replica like this both are kind of symmetric to each other so if you see this point one two you will see point one two here also if you see minus point zero six two you will see minus point zero six two here okay so you can choose one of the sides so let us say for now you ignore the left hand side the the upper side let's see the downside now in one minute i'm sorry yeah so if you want to compare let us say your quality this is your target variable and if you want to compare with one of the independent variables let us say you want to compare it with alcohol so in that case we can say that okay one minute yeah we will use this part of it i'm sorry we'll use the down part so if you want to compare your quality okay with respect to alcohol so what is the quality with respect to alcohol is this is the quality line and this is the alcohol line so in that case we are getting 48 percent of uh what is say dependencies that means if colic quality increases by one the alcohol level will increase by 0.48 percentage a dependency basically or a relation correct sulfates 25 ph this much this much or else when you can go this way also whichever you feel easy got it simple um not yet but then you know when we were doing our previous project so because we had this heat map but um how do we conclude or how do we you know if you have to find some result or you don't want to okay you know explain something other features or points we need to look or keep in mind when we are using this heat map okay so i will say if you find a relation which is usually more than 50 okay that is called high covariance but in this case we will not find it because there are multiple variables so here we have to adjust it with 48 25 this is kind of relatively high covariance as compared to these variables if you feel okay this is what max you can do we can you will not get any any any strong output from this this will just give us a hint that in future a few of the variables go up and down as especially these two intervals you are going to have a huge impact on your volatility that's all there is no statistical or equation or output so if we want we can ignore this yeah yeah definitely you can ignore it nothing wrong okay but i will say yeah if you want to do a good ed you know uh please uh use this i will request you why because uh in exploratory data analysis either you use uh have you used spare plot anytime no no we use box plot a pair plot is a graphical visualization of this plot i will show you today so i will show you how to use spare plot how to use the correlation matrix or how to use heat map they convey the same meaning whichever you feel easy you can use i usually use this because it's more visual and it gives me some good amount of you know numbers like okay okay so again i'm saying solely for the purpose of understanding nothing more than that for this the same answers you will get it using this alcohol and sulfates are the one which is contributing majorly to the um okay and this is also again for visual purposes so there is a module in your certification or this course called feature engineering which will come after two modules from now there we will be talking about how we can remove some of the features so usually the rule is if the feature importance is less than five percent and if you want it you can remove some of the features from your decision tree and definitely the decision tree accuracy is going to kind of go up but also we are going to lose some good information so we will talk about feature engineering and accuracy increase and all in that module but for now just for your understanding i have kept it so that you can you should be able to collaborate this with respect to heat map okay they convey almost the same meaning that's it i wanted to show now moving on what i've done here is i have made a table which will keep appending every time we add we keep adding our uh what is say ensemble techniques so this is this is a code basically which prints the static uh labels and every time we get a new uh model name and a number we'll append it so for the first thing we got an accuracy after pruning or after reducing the max depth to 6 or after improvising the best the best we got is 61 percent and we have to be happy with it right so moving on as soon as you put decision tree you have to go ahead with random forest so in random forest ah this is the library from where you import the classifier okay and please remember there are two type of random forests available one is called random forest regressor and one is called random forest classifier regressor is like when you want to use random forest onto continuous numbers you should use regression which is not in our topic today but i will try to show you guys something extra and there is something called classifier so once you guys are good with all these techniques now in the last session of this module i will show you regressors also yeah so next if you see here this is my model this is my memory learned model which i am going to store in my memory this is the standard one from sklearn now there are many hyper parameters available as we saw in decision tree if you observe here there are so many things available we are not using it similar to that in random forest also there are multiple things available but for now i am using only n estimators n estimators is how many you want where is it where is my excel file yeah how many of these bags you want basically okay so here i am saying i want 50 of them now again somebody or some of you will want to be wondering how do we choose 50 for now just choose numbers 10 20 30 40 50 60. once we progress on to this once the concepts are pretty clear i will show you also the best optimal technique to choose this numbers okay so now one question to all of you if you guys understood random for us properly is it good to have more number here or is it good to have a lower number here i'll take you guys back to the excel file you when you look at it you will i think that is pretty clear so look at my excel file if you look at this particular thing is it good to have more or less regression more more yes you are right and why more so that we will be able to uh you know do more calibration and give us uh more uh accurate prediction okay so it's like whenever you think about estimators now just put that for example you have got one mentor or let us say you have any questions so you will email me and i will give you some answers apart from that let us say you access to some two more mentors mr mukeshwa also and some other expert data centers from great learning imagine all of them if you have access and you ask the same question it'll be better for you because if you know if you it will be like multiple validations done by multiple people something like that so you keep that in mind but the only issue is if you have more estimators no the time i i feel the training time will be kind of little more sometimes because of parallel processing happening or sometimes if you are not able to parallel process it will you know do batches so in that case time will be a little problem but apart from that should not be a big deal so but um could it not lead to a little confusion as well if uh you know yeah 50 50 kind of yeah yeah yeah definitely it is possible yes but majorly what i've seen so far in my experience random forest runs pretty good yeah but yes you are right there are chances of crashing because of 50 50 chances yes always possible okay so i'll do one thing and in that case i'll note this down i'll try to give you a data set which has this problem we'll see how to solve it okay good so now moving on uh this is our training model now we already fitted it trained it now we'll use the test to predict it and then we'll use a test and the original actual versus predicted one to gather the accuracy so once we do it we'll put the accuracy in our table so look at the random forest one we got an accuracy of 68. so i i told you usually random forest accuracy will be far better than decision tree the only reason is we have got different validators over here to help us to make a better decision yeah now moving on to ada boost for the random forest equation the random forest accuracy of the training data set would that be 100 as well uh not actually not actually here so usually uh in random forest i do not calculate the accuracy uh for training data because um okay in that case yeah you are right you just want to know whether it's an over fit or not right that's what you're looking for yeah yes okay so i'll do one thing i'll try to catch that also i'll show you how to do that let's see i i usually don't do it but yes you are right it's good to check it out so i'll do one thing the code which i send you today know i will include the testing and training accuracy so you'll come to know you'll be able to differentiate both of them yeah okay good perfect okay okay now moving on so usually guys from now know uh whatever accuracies i show you know will be hundred percent uh testing accuracies so from now on we'll not focus more on to training part of it we'll just visualize under testing part because this is what we are interested on now moving on uh ada boost so the flow of it remains same we have to get the ada booster we have to put the an estimator so here let us say i'm putting 100 randomly chosen uh learning rate if you remember from svm the same concept comes in and the random state how much you want to choose at a point so when i put all these things and when i calculate my accuracies so the flow is same you build your model you predict your model put your score and then put it on the table we get around 62 percent of accuracy okay so far random forest is performing better but if you ask me for a stable model much more stabler model i will say ada boosting will do a better one why because it has something called learning rate it's like a self-healing kind of algorithm okay next we'll try with the bagging part of it so again import the bagging classifier and guys remember one thing whatever classifiers we have in the opposite case we have got regressors also so regressors will be taking care of continuous values next put your classifier put your estimators put uh maximum samples okay for now i will say ignore this will not get into detail of these things okay uh we learn all these things once we you know we are good with these classifiers and you guys will get it how to improvise what is this stuff now bootstrapping so now this is a very controversial topic we are see there is a possibility that python is telling us if you want you need not to tell me that or else we will ignore that that there is a bootstrapping issue that means the out of bag issue is happening so if i am saying bootstrap is equal to true also say ob score is equal to true so definitely it is going to penalize us it's going to tell us that yes there are some data sets which are out of the bag for this particular thing and hence the accuracy score will go a little down but if you say both of them as false then the accuracy score will be better but in the testing phase you are going to fail a little okay that's the only difference so always remember one thing keep them true people sometimes switch it off and then you know keep trying on the testing part that accuracy is not increasing yeah so in this case we are good with it even though we'll get a lesser accuracy it's okay no problem now moving on we will predict it and we'll put it on the table we get 67.5 as a bagging part now as a second boosting technique gradient boosting so when i put gradient boosting also same as our ada boosting you see the parameters are exactly same learning rate estimators and random state and they are perfectly same the accuracy we get is 63. so just have a look ada boost gives 62 gradient gives 63 so as i told you usually both of them are kind of same so you don't have to do both of them you can do one of them it will give you almost an accuracy so now if i show you guys this thing what do you guys think should be the best model here it should be random for random forest exactly yes after random flower the best one falls out to be bragging i will say boost and this one is of no use yeah now observe here what matrix i am using i am using only x do we have a matrix available any more matrices yeah confusion matrix correct so because this accuracy is coming from confusion matrix only so under confusion matrix what all other things do you guys remember precision recall yeah yeah so now i will do one thing i will share the code with you guys i will show you anyway next week the solution no issues see it's not fair from our side just to judge a model depending only on accuracy we should also use we have free to use we have precisions we have recall factors so what you guys do is instead of accuracy or keep accuracy as it is add decision where does precision and uh recall come uh i think um not read this before i haven't owned you guys guys i've shown you this in our log our and all if you see logger okay in our uh classifier they were also called in the video classes as well yeah i will do it no problem no that's fine i must have missed it okay see very simple uh okay let me redo it nothing wrong with that very simple is let us say this is your this is your zero this is your one this is your zero this is called true positive this is called price false positive this is your false negative this is your true negative correct so now we are interested in this we are not interested in these two these are errors for us okay depending on that we are building this formula look at the accuracy formula tp plus tn divided by all of them basically accuracy is these two divided by the whole table the question is precision is this divided by these two and recall is this divided by b factorial factor is also a very good important factor tomorrow let us say you have a conjunction with the model let us say she's telling me kind of confused so what you can do is even you can use recall factor and if the recall factor of say bagging was more so you can say we'll ignore random forest i want to choose bagging that's the reason i said you can include one more matrix over here okay try to do that when i share the code with you else i am going to show you a solution in the next improvised data set next time okay guys simple so this is a complete view of uh ensemble techniques so this is the uh you know good side of ensemble techniques good side in the means in the senses your model that you are going to implement is only this much in size next time once you are comfortable with this i will show you how you can include svm log r all the unsolvable techniques together and then we'll pass the same model through svm log r and all that this model not possible because of the type of target variables we have but i'll use some other model wherein we just have two of them and we'll use this type of technique a different uh uh you know detour kind of thing from our normal ones even you can use it okay so i am done for ensemble if you have any questions do let me know and now if you watch the videos it will be more clearer to you guys okay perfect all right yeah so i think we are five minutes before the time any more thing you want to discuss because now if i start with uh actually i wanted to show you guys uh something onto a chat bot part of it but again a little less time but no issues next week will be free because we are done with the next week's contents also we'll have around one hour next week so i'll show you guys the thing okay great okay okay yeah question if you can show something on the just a so i'll give you a quick view on tensorflow then yeah tensorflow is yet to come right yeah your tensorflow will be after four four modules for you guys so this is the neural network excel that i usually do we will do manual calculation now what is tensorflow if you want to understand tensorflow it's very simple it's an open source artificial intelligence library using data flow graphs to build a model so uh guys for so what is the purpose of tensorflow you're using soon yeah i'm trying to build stock price exists okay okay see my data is a continuous data with the date okay okay fine so if uh my experience with tensorflow majorly lies onto neural networks so let me explain you in that way uh what i want to do is i want to build a simple model of continuous numbers where if i give something it should predict something out of it so let us say if i give 0 it should also tell it's a 0 so i want to build a network like this i want to make the network learn and depending on my weights so these are some of the internal stuff which i multiply with this inputs and i manipulate such a way that we are able to debug the output back so it's like if you remember we'll we will we have a signal we'll modulate the signal at the end of the thing we'll demodulate the signal and get the answer back something like that so we're trying to learn a network or we are trying to make a network like this so tensorflow will give me an opportunity to do this manipulations and on the top of tensorflow i am using keras keras will help me to build this layers basically so this is what my exposure to tensorflow is yeah so even if i have an implementation if i show you also it will be completely related to neural networks so i am not sure how it will be useful to you in that way but if in case you want a very specific one do email me i'll try to build up some model for you intensively [Music] yeah it's a pretty newly um like google has done a lot of new stuff on this so till now what we we used to do is we used to we used to code it so basically if i show you the simulation of neural networks we used to code it manually but now no need to do that what a good amount of keras and tensorflow libraries yeah so do me a favor now just send me a problem statement and a sample data definitely i'll try to show you some good implementation on tensorflow okay perfect okay so i think uh i'm done for the time uh if any questions for an ensemble do let me know else we'll close down i'll share the details with you along with excel files so that you can refer it during the videos okay thank you all right thanks for sharing thank you and guys one more thing uh if you have any questions in the middle of the week don't hesitate to email or whatsapp or call me also you have an open option uh krishna we would like to you know once go through the project because uh we get those projects but uh please tell us yeah you know how do we go about you know like if i've been given us a problem okay what is how's the flow what are we supposed to look at how are you supposed to think okay they're doing it but you know it's we check kaggle and all that but what's the thought process is something that we would like to understand okay perfect so i'll take up uh your previous modules project next week and i'll show you last session if i'm not wrong uh we completed the ensemble techniques right so along with an example theory plus example is it correct it's true it's true okay so any questions on to any of these ensemble techniques or any doubts specific okay perfect so uh today um if you remember last time uh we basically questioned one question so we have covered a number of these techniques a classification as well as a regression but we have not been told in what situation what circumstances which technique should be implemented and why one technique should be maintaining a particular situation without a particular data set what is the more appropriate technique to follow a to solve a particular question a problem what should be the better technique to do that these things have not been covered anyways okay okay so um i will i will try to show you something on that then today so we'll start with uh what i usually called it called called as a mind map so where we'll try to differentiate between all the techniques and we will see how we can use okay yeah so apart from that any other questions on to ensembl or any other stuff till we have done so far no right okay so uh then let's uh probably spend some time on understanding what technique could be used uh where so we will take an example of say the classification techniques that we have done so far so if i start with uh okay let us first name it three four five okay so let's start with uh logistic regression for example okay uh nail base then we have svm and we have k n then we have a decision tree uh six there's a random forest seven tests uh i would call it uh what else we have bagging and the eighth one i will call it as boosting okay now on to the real world basically if we are talking about live classifications if the data set so i will say if i have a target variable which is exactly equal to 2 or not more than 2 then i will say use these techniques i'm sorry use these techniques so the first four that you see are specifically used for where the data type is or the target variable is not more than two in number so what do i mean by that it does ones and zeros trues and falses yes and no something like that so in that case definitely this is the only option that you have it apart from that even you can use ensemble techniques also to them nothing wrong in that so i can say if you have two target variables i can say even we can cover this thing up but in the opposite case when the target variable becomes greater than equal to two basically in that case you can use the lower half of it there are some cases that i have seen people have modified svm and try to do a classification on to multiple levels but again it gets a little complicated so in this case i will stick to two itself so this is the first difference using which you can say which one to choose depending on the target variable okay linear regression also there right yeah so i was just talking about classification basically so if you talk about linear regression so in that we have target variable is nothing but your continuous form so if you have a target variable which is like 4.96 or 4000 or 5900 something or 0.4 in these cases when the target variable are not classes like this in that case definitely you have to go with the regressors okay so the first difference you can make out is from the target variable itself it can be an example for more than two target variables like yeah i'll give you an example so now if i think i have a code ready i will show you in the code itself not this find it outside yeah so if you look at this data set for example i will show the target variable directly if you look at this target variables i have got target variables which are 5 6 7 8 and 3. so these are actually classes so you have what multiple or 42 percent of the data is target variable is 5. so this is a clear example of target variable greater than equal to 2 correct so if you count it we have around 6 target variables so in this case if you implement k n definitely it's not going to work okay you see for example in two yes or no or one right real time example okay see for example uh let us say okay i'll take an example of this data set let us say there is a wine yard okay and there are some characteristics of the wine so let us say these are some characteristics of the wine and depending on those characteristics of that barrel we are saying we will and we let us say we have some bottles quality a quality b quality c and quality d so depending on certain characteristics i am saying some of them are of quality a okay some of these are quality b some of them are quality c some of them are quality d yeah so in this case if you observe the total number of target variables is greater than equal to two that means the variety is more than two so variety is we have a's we have b's we have c's and we have these something like clustering this is sorry something like clustering uh yes it is clustering but the difference is here we have a target variable available in clustering we do not have a target variable we are generating a target variable okay so in clustering that's what we do we do group by right yes exactly we do group exactly so if uh if you guys want to focus a little on that um okay i will i will give you guys a touch on clustering today as an example okay what is the difference between our techniques versus clustering so is the clustering module open next model is open for you guys or not yet then oh no problems i i hope you got the concept of more than one so depending on this you can choose your algorithm so this is the first method second method is if you have not done major changes in your data set let us say if you have not done major cleaning part of it or if you remember last time we would have discussed about target imbalance correct so if you do not have major target imbalances in your dataset or if you have your data set very normal everything is good in the data set there are no major changes you have done in that case if it if you are if the data of the target variable is exactly true you can go ahead with a very lenient technique called logistic regression or kn this will work very nice on a very cleaner data set where you have not done any of your where there is no intervention basically from our side okay next if you have a data set which is full of categorical variables categorical variables means let us say you have got your independent variables and some of the variables are like yeses and no's male female married yes married not married then you can have multiple where where you have majority of them is like each column is just a category of two variables yes or no or something like this in this case it's a probability based data set basically so a probability will work very nice on this in that case you can go ahead with a nail based one others will also work definitely but this will work a little better okay and the last one is your svm now svm is called a penalizing algorithm it's a very strict algorithm so let us say you have done a lot of changes in your data wherein you have uh you know basically removed all of your zeros and replace it by means or medians or you have done a targeting balance where you have virtually created some of the data sets and pasted it so in those cases where you feel that you have done some changes and it might impact your data in that case you can go ahead with svm by because scm will take care that the data classification is done by the way because there is a you know feedback loop given back always so there is a learning rate kind of thing so in worst case out of these four top four you can use svm industry but again svm can be used for any of the other data sets also as i said you can use it the data sets which you use for log r can also take svm nothing wrong but if you are looking at a very specific way to choose your algorithm this is the one for the top four of them yeah okay now how to choose the bottom four so the good thing about bottom four algorithm is it will work with two target variables also it will work with greater than equal to two target variables also so let us say when you implemented all of these on an average you got around 78 percent of testing accuracy what if i want to improvise on this so if you want to improvise you can go ahead and implement these two at least there are chances that you are going to pull up your accuracy so 80 85 percent nothing wrong with it so just for checking purpose it's always a good practice to include all of them together in one bag okay next next is uh let us say if you are talking about the last four of them so if you want to understand which one to do where so if uh say your if for example if you are doing decision tree and random forest these both will always go together so just in case if you are choosing decision tree random forest comes free with that definitely you have to implement both of them together this is one case next again as we saw in svm that if it if we have done multiple changes and if you want to be a little stricter on your data set in that case from the classification or ensemble type you can use boosting because boosting has a similar characteristics like svm where you have got learning rate coming into picture okay and for basically for bagging um i will say a random forest is a time consuming algorithm boosting is also kind of time consuming algorithm so if your application is dynamically changing or if your learning time is very less in that case you can say i will go ahead with the background but if you take my suggestion i will say if you are doing ensemble do all four of them together because you'll get a variety of accuracies here and you'll be able to choose the best one out rather than you know assuming and moving that it's a good practice to do it together okay um uh are we good now ashish yeah yeah perfect are there any other classifications uh not actually these are it i think so this is what even i use majorly i don't use anything beyond this yeah and most probably i i um try to find myself within this ensemble itself so i don't even use svm at all nowadays because this ensemble works pretty well but if you want a complete list definitely i can look onto web and pass it on to you guys okay okay perfect so this was the thing now again somebody tell me last week um i showed you guys one um uh did we find data set last week no we did not go for better we did not go through this data set right okay just one minute and a a question one more thing we can also go through the regression algorithms which one is better investigation linear regression quadratic regression that way yeah yeah okay see for uh it's regressors are very simple so if i say uh let us say it's very simple like you put a line and if you find that the line is like this the relationship is this way definitely blindly we have to go ahead with linear regression okay next for example if you find out that you are not able to find a linear relationship with between both of them then you have two algorithms one is called polynomial regression and one is called quadratic quadratic chord regression basically so in this case if you remember our linear regression relationship is what y is equal to mx plus c linearly related in this one you will have a polynomial of n dimension here you will have a quad basically of four dimensions so i will do one thing i'll try to arrange some data sets and samples on this because it's not a part of your session um curriculum anyway i'll try to give you guys a simple data set how to identify it's a polynomial how to identify its quadratic and an implementation on both of them yeah okay i'll make sure i'll do that in probably next session or next frame session but um i will say uh when you look at regressors nowadays regressors are getting extinct knowledge so except i think linear uh regression or polynomial i'll say worst case both of them are majorly used onto financial domains especially onto you know where you have where you want to predict share market basically or you want to predict a value which a loan value which you can give it to a customer or something like that so right now i'll say these regressors are majorly used in financials post other industries other algorithms like classification algorithms or unsupervised learning algorithms have taken over basically are there any other regressors apart from these three are allowed yeah we have decision tree regressor we have random forest regressors uh we have uh i think bagging i have not seen but at least i have seen i have worked on these two regressors okay i'll give you all the regresses all the four of them a very good view on to that and i think uh in your next module if i'm not wrong uh this is an integral part of your next module i think the models are changed for the newer batches and as a project you will be given to work on some of these requests so i'll make sure before that i give you guys the one all four so we'll do one thing today is i will take you guys through one more data set now i use this usually for featurization uh case study but again it's a good example of ensemble and then i will introduce you guys to uh some concepts on what is featurization and post that um i will take you guys through some of these unsupervised learning techniques like clustering for today's session and post that we will implement one chatbot today we'll start up uh with uh chatbot implementation so i think we will take uh around i think one two three will take three weeks to complete this chatbot so in this chatbot what we'll do is we'll implement one um simple chatbot where we will hard code the interactions and we will see what happens post this second week we will make an nlp based chatbot so i hope you people have heard about natural language processing okay so we'll make an nlp based chatbot where the chatbot will actually understand what you are talking about and try to interact with you and the last one is we will make an ai based chatbot so basically a convolution neural network based chatbot so for this what i will do is i will split this into two sessions uh one week i will give you a brief idea about ai around 30 minutes and the next week we'll try to i'll show you the python code and we'll try to implement the same thing okay so it is a three to four weeks process so as an extra topic we will do this every week apart from our normal uh videos and contents everyone's okay with it yeah yeah good perfect so let's start with our data set first uh the ensemble one and then we'll move on so what do we have today is we have got a data set like this now the uh there is a wine yard basically and this particular wine yard yeah so this particular vineyard has got a lot of barrels and each barrel has you know some liquid which is filled up for some time basically we don't have an idea on that the quality one is your target so depending on what is the quality of your barrel they have differentiated this as some numbers say three four five six seven and eight so these are the qualities given to each and every barrel so let us say if the dip machine in the barrel they'll come to know the quality of the barrel is three that means they can take a decision of filling it in a cheaper bottle of wine or retaining it till it attains a maturity of other levels and let us say if they find out the bottle the barrel is of level eight that means it it is ready for production tend to be filled in a higher or superior quality wine something like that they are doing so if you see here on the left hand side you have got a set of independent variables that's what we are assuming okay and this is your target variable so whenever you have a case like this as i said the very first thing is you can use ensembl techniques as usual because you cannot use the first four classifications because the number of uh target variable is more than two here okay so this is the first hint of using ensemble techniques now uh what i what i usually go through is i try to check out the info part of it so it's a good habit where you'll come to know if there is anything missing here or not first way to check second way is just to confirm everything is in form of numbers sometimes in a csv file when we are importing it csv file sometimes changes the format so even though it looks like number sometimes it is not so doing some panda operations along us it will create some issues so first thing is we are checking and uh interaction between intent float should be good not a problem at all okay now now there is one more thing you remember in python in any other languages if you are interacting uh let us say int and float together the answer is supposed to be float only so at the end if you wish you can convert everything into float and keep it or you keep it as an integer nothing wrong we will keep we always will retain this as an integer y the first thing is it's a target variable so if in case some of the interior variables were integers you could have you know done at type conversion here next so question what is the main difference between teacher float here uh the thing is uh float we will it will be represented as this will be represented as this and if you put float it will give four to five zeros like this so it's all about the memory difference that's it okay this will get a bigger memory cell this will get a lower memory so tomorrow if you are making an optimized code or if if your code is going through a quality check so in that case uh this something like this will they will they will argue with you that it's taking more execution time to fetch the data because here it will be more fetch cycles as compared to this one sometimes so in that case it's better always to convert into a simple or the the majority ones yeah okay also sometimes uh people you know they try to convert uh let us say uh they round the figure up so usually the float gives us six figures if i'm not wrong uh we can round it up to two if you want it yeah so it will it will it will avoid some execution cycles it will also try to change the values here because if you see here it's the three point uh you don't have a yeah let us say we have a larger number like 0.098 so if you round it off to one or two you will get 0.01 so sometimes you feel it's a better value than this one yeah so worst case you can do the rounding of else retaining the original thing is good enough next comes our describe function so again uh i will say here i usually use this function to understand uh whether the data is scaled or not scaled usually i take it in this way these two i compare and i'll try to visualize them so definitely there will be some variables which will be extreme but that does not mean the data is of not not a scale because some of these units might be of higher level yeah so in that case we should not worry much but if in case you feel that they are of similar units and still it's giving an issue then i will say it's a scaling problem okay so this is one thing now always remember for supervised learning we are not worried much about scaling at all because in supervised learning we have got equations and our equations will will take care of our machine learned parts but as soon as we enter unsupervised learning modules all the algorithms mandatorily before moving on to usl we are supposed to scale the whole data keep this in mind okay so scaling should not be a big deal as of now so next what i've done is i am trying to find out correlations just in case that uh how important are some of my features when they interact with each other so if you see here i have used the correlation function here and secondly what i am doing is i am filtering more out of it i am saying if you have a positive correlation which is more than 55 percent or if you have a negative correlation which is more than 55 percent so in this case what i'm doing is i'm filtering only that data out of it okay i do not i do not like if i have a correlation of 0.3 i'm not worried much it's not a big number but if i have a correlation i'll say 0.59 and if i have a negative correlation of 0.75 these two will create a lot of impact for me so if you look at it some of the variables are here citric acid and fixed acidity is around 67 percent correlation that means if this increases by some percentage this is definitely going to increase by some substitution so it's always good to have a eda on to your datasets so once we enter you uh unsupervised learning and featurization and all this modules you will get this concept in a better way that why having better variables see it's not necessary to use all of them yeah because some of them are important some of them might not be important so this is how we try to find out so the last so once you finish your machine learning you'll have one more module where you will try to uh you know change these stuff you try to understand that how if i for example if i delete this particular column how to determine first of all it's not important and if it is not important if i delete it it will not make much difference to my you know output so these stuff will come under that so for now it's a little early for you guys so don't worry if in case it looks scary but for now the meaning is the correlation that's it next if we are not happy with the smaller one we can have a larger plot here and if we look at it if i show you guys the quality so this is my target variable and these are my independent variables so you try to find out the ones which are impacting quality the most so in that case i will say alcohol sulfates um which else citric acid negative side is volatile acidity so these are some of the major variables that we will look on and if we look at very lowly impacting variables we have as residual sugar chlorides and all this one so if tomorrow you feel that they are too much you can even try to remove them okay perfect so this is uh this is the eda next comes your target so please remember from now on when you are doing ensemble always do a target imbalanced check so we are good with targeting balance right we have discussed this topic earlier if i'm not wrong yeah so have i shown you how to take care of these problems using python did yes oh good so keeping that concept in mind if we look at this particular problem if you see here uh 5 6 and i will say that's it these two are dominating so if you take the percentage addition of these two it's around 42 plus 40 that is 82 percent of the occurrences in the target variable are only from 5 and 6. now there can be two possibilities here one is actual data set is like this that means the barrels are pretty old or the barrels are such a way that they were they were stacked together at a particular time so that quality five and six is dominating here else there is a possibility that we have got a sample data here we are not having a complete data so in this case whenever i have a sample data i try to request from the customer the whole thing because if we get a complete data set it's better but never ever in the industry expect it to be equal here always there will be a target imbalance for sure now it is on our side or it is on us whether we consider this problem or we ignore this problem sometimes ignoring this problem is okay but what happens is if we ignore this the biasing issue will be always at the back of your mind it will haunt you guys so in this case what i will do is i will not use those difficult techniques i don't want to go through upscaling or up sampling down something i don't want to do all that so in this case i'll use simple maths i will say i will combine three four and five together or else i will say um three and four i will combine together and i will make a new class called 4 5 i'll keep it as it is 6 i'll keep it as it is 7 and 8 i will combine together i will call it is the new class is 8. yeah or else we can do one thing we can say best quality class we can say mid quality class and we can say low quality class even you can do this you can use this method uh my method or this one anything or even you can use target imbalance there are multiple ways using which you can solve it for now i have done the combination method just to show you nothing wrong into this so if you see here what i have done is i have added 3 4 and 5 together i've kept six as it is and i've added seven and eight two so now if you look at it you should show this method only for target in balancing i think oh i showed you only this method oh yeah give me one minute then if we are in the topic let me show you a good example on targeting balance so this is something extra guys this is not included in your you will not find the videos and all on your sessions so yeah so this is the one yeah so what i have here is i have got a very simple logistic regressor problem of a data set of a banking data set where these are the attributes of the customer and the bank is either approving the loan or rejecting them so when i go down so don't worry about the encoding part amount so we'll ignore it for now we're not worried about data cleaning if you see here number of yes's are 422 and number of nodes is only 192. so what is happening is we have got if i plot my targets we have got 422 yes's and we have got 50 percent of them as knows so what would happen biasing is going to happen for sure that means prediction will be better for wise we'll be able to understand more on wise we'll have a low understanding on number of noise yeah so what i have done now for now if you look at the accuracy we have got only 75 percent of the accuracy even the accuracy is not that great and also if you look at it two 61 ones are identified properly out of 62 and only nine zeros are identified properly out of 31. so definitely zeros or no's are not getting identified properly so to avoid this what i have done is we have got two method which we usually use for targeting balance so one is called up sample in up sampling what i will do is up sampling i will the the smaller one i will randomly take rows and i will append here i will randomly take some rows and i will try to append it i'll do the process till i reach here so it's like if it is 422 7 here 420 or 422 should be good so when i have this kind of ratio now i can say that at least there will be no biasing happening yeah but we are kind of see we are randomly choosing the rows and pushing so there are chances that similar rows might be get pushed yeah so there is again unknowingly we are creating a bias anyway but sometimes this method works very nice sometimes it will give you pathetic results so be ready with for this also so in this case other cases opposite of this where we will try to reduce this so randomly start deleting the rows till it comes to the opposite level again here this i feel out of these two i feel this is kind of an okay method but again we are losing data so losing data is also not that great for us so these are the two techniques krishna india this type of technique i think is maybe case to case basis the data set what they are doing i think this is also has got a little bit of bias because uh wrong prediction is more in in terms of you know calling customers as going through the data if i do this method then it will give a wrong prediction for me right uh yes so there is a risk of biasing happening yes you are right but at least uh you can try it out say i'll we cannot uh openly say that okay we'll get a wrong prediction for sure but you can try this out up and down sampling i'll share the code with you you can try it out if you get a better result than this just for like uh practical thinking i'm doing you know out of thousand customers all the thousand customers 500 customers uh got a ton deposit and um 250 or 300 because 250 customers are wrong predicted that means that they are called but uh so many times but they have taken no term deposit in this case again 500 and 250 bias happened right if i do this method exactly what happens at 250 customers again same thing will come to the data no uh see okay so you want to do first of all up sampling up upscaling or up sampling you're talking about this one or this one though yeah sampling okay so what we will do is rather than copying the whole chunk so there are two ways either you can copy the same chunk here which is i feel is not a good method logically what we're going to do is randomly we'll choose a row and we'll append it here okay till it becomes 500 so definitely there is a chance of bias i'm not disagreeing to that but at least it will be better than what was earlier the imbalance so again it depends on data to data so sometimes i will say it works very good sometimes it will it will not work that great so i'll show you both the example so in this case study that is there with me the example is other way so on purpose i have chosen this data set where we are not getting a good result out of this okay so you are right definitely you are creating a bias unknowingly also but that bias is much lesser than what earlier we had that bias okay i hope you got it yeah yeah yeah so if you look at this it's a very simple uh uh we use resampling and in resampling we are saying replace it yes replace it till when till we reach this particular number and randomly choose the how many at a time you want to replace so i will suggest you you can go ahead and give a smaller number here as a random state rather than using a bigger number we use randomly chosen number so don't worry about his numbers yeah so when i did this when i use a triple digit number because if i let us say if i do one okay if i do one by one it's going to take lot of time for me to replace but i will feel this is a better way sometimes so let us say i have chosen some odd variable like this so when i go through i have now number of yes's and number of nodes are exactly equal how i have a control here yeah so now our data set that the new data set that we have is not having a target imbalance issue at least so now i can take my new data set again through my model that is my logistic regression model and i can predict it and i can print the accuracy so when i print accuracy i get an accuracy of 68 but when you look above the original accuracy that i got with bias was 75 so in my case up sampling did not help me much it reduced the accuracy by 68 so it's reduced by around seven to eight percent okay another case is the down sampling one so where i'm saying i do not want to replace it i want to delete yeah and in deleting delete till when till you reach 192 as a number yeah so in this case again if you observe both of them are similar number so there is no imbalance as of such so when i again run same code i take logistic regression i predict it and i give my unique y's and nodes and also i give my accuracies in that case i'm saying again it's around 16. so no matter i am doing up sampling or down sampling for this particular data set this data set both of the techniques are not working that great okay again there are a lot of factors on which it depends it depends on how many number i am taking what is my random state so when i deliver the code to you guys you can try altering these factors it might change a little but i will not you will not see a major change here for sure yeah now when you do this kind of stunt so basically i call this a stunt why because we are playing around with our data it's not that good and in this case if you observe 422 and 192 it's a huge difference almost 100 difference okay so in this case when you do this you are supposed to uh use a very pin a very strict algorithm or a penalizing algorithm and in our case the penalizing algorithm is svc or your svm basically yeah so this is how you can use svm in it and when i used svm i got an accuracy of 69 and 64. so on on an average i would say my accuracy is around 70 so when you do some altercation be ready with your svm also or in this case even you could have gone and used boosting also nothing wrong but you would have got a similar accuracy again so do not use log r with uh something like this it will not give you good results okay got the point i chose this data set purposely because of this issue are we good everybody so this is the original way to use salt target imbalance this is an alternate way to avoid all the problems so now problem with this also is client should agree first of all so whatever we are doing we are saying quality 3 quality 4 and quality 5 is same that means we are saying the current version of android next version of android and the next to next version of android we'll put it into 1k sometimes it's not good business point of view it will not be validated by the client but if in case it is validated you can do this not a problem but so if you see this and if you see this i can say that somehow i have reduced the target imbalance issue okay everybody good with this yeah yeah perfect so now moving ahead uh what i will do is i will split my data set as usual after splitting i will start all of my ensemble techniques i will start with decision tree always decision tree the basic problem is training accuracy is 100 testing accuracy will be pathetic that is 64. so this is a clear case of overfitting problem so again what we'll do we'll do pruning in pruning we'll try to change the max depth let us say to 5. so in this case 63 68 so 68 is my training 63 is my testing so now i will say i have under fit my model this was over fit this is under fit both of them are not working that great so decision trees out of my scope and just to record all the stuff what i am doing is i am putting my accuracy here okay so one thing guys i will try to i'd like to show you one thing let me copy this [Music] it's a good habit to put over give me one minute we'll print recall also so i'll push recall here and we'll print okay or else i'll do one thing i will share the code with you directly let's not waste time coding right away i'll show you the recall factor separately so guys if you see this case study that i have i just i cannot scroll up because of the data issue but for now just look at this if you include a recall factor also last time if you remember i said do not always depend on accuracy try to include your recall also so if i go down and if i show you all of them together and if i go down here and if i show you all of them together something like this so here you have only one option but in this case you have three if you want precision also you can include precision here nothing wrong so when you sorry one question when we are comparing like this you know you have comparison tree to a gradient boost when we are comparing like this sometimes you know uh one person to another person very slight difference will come yes like for example maybe one one digit or two jt is that okay like uh that is or everybody should get the same no no no it is okay it is okay if you get a slight difference see i'll tell you why do we get those differences the first difference comes the way we interpret our data cleaning not all of us will do data clearing similarly yeah secondly is the way we implement our model or else the second way might be your training and test split some of us will use different random state you can be not all of us will use the same random state so if we look at our hyper parameters of train and test split so give me one minute yeah so these are called hyper parameters so if you look at these parameters we can have a different parameter here nothing wrong into that so here if you see here i have used both of them and i have used random series 22. some of you might use two to zero some of you might use two so again it depends on how you are splitting your stuff and what size you are taking and thirdly it depends on these hyper parameters also so if you see under decision tree itself you have got so many hyper parameters but here you are using only one so tomorrow some of you might use minimum samples per split should be two let us say i try to include it here instead of two i will say minimum samples per split should be ten that means if you are splitting your node at least it should have ten samples in there okay okay in that case one team will say like oh you can use knnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn so correct okay so that is the reason after you finish your unsupervised learning we have a separate module called featurization there we will train you guys on how to normalize this how to use these hyper parameters such a way that we will streamline all of your algorithms together got it yeah yeah so this is like we are we are discussing two modules down the line okay but yeah you are you are exactly right it's it's a correct uh thinking process where yeah we were discussing in our workshop you know like some of the guys got uh knns modules all the guys got tradition so that's something i asked you correct so i will give you guys one hint then um from for this project okay uh still you guys are not good with this hyper parameters but try to use them so i'll showcase you today which all you can use it'll be really good yeah so i'll show you how to do this so like let's not get confused for now for now let's follow the same process after we finish this data set i will show you how we can play around with this and get a better accuracy and streamline this basically yeah okay perfect so let's say i uh one minute yeah so i hope you people got try to use two factors so that it'll be easy for you to take a decision so if i show you this table can somebody tell me which looks the best algorithm yeah 89.4 i think this is the the best okay but what about the recall factor give your equal factor it is 21 and the difference between these two is around three correct but the difference between these two is hardly point seven or eight correct in this case can i say bagging will do better correct accuracy wise you are perfectly right boosting it take an average of both of them so that you people remember that not only accuracy can be used for your measuring matrix you can use precision and recall also what we do we focus on neon loss it's not always necessary to focus on this even you can use this as your matrix nothing wrong with it what if i combine the power of all three of them if i use one then you would have chosen this when i use two i have chosen this let us say tomorrow when i use precision also there is a possibility i will choose this also god knows correct so try to use all possible stuff together but accuracy and recall i think should do that's better good um so yeah so in the wine data set that i'm going to show you give me one minute let's scroll up so so we have decision tree this is what we got not good as soon as we complete decision tree random forest follows so when i implement my random forest i get 69. so definitely you will see the difference a huge amount of difference here and these are my testing accuracies okay 63 69 post that i am going to ada boost so under ada boost i got 62. so it's again down so even lesser than uh decision tree but if if your client asks you okay fine give me a stable model then i will recommend always boosting because the boosting both algorithms are having learning rate so they are kind of you know more stable algorithms next is if you look at gradient boost it will be almost similar to ada which is around 63 so you use this or you use this nothing wrong into that next comes bagging so when i implement bagging i get 67.5 so now i have a clear winner which is around random forest but you should keep in mind the training time will be pretty high here because random forest is a time consuming algorithm so if in case training time is a problem for you you can go ahead with bagging baggings training time is pretty lesser as compared to this random fast okay guys so this is how a normal ensemble technique goes on there is no difference in the this code and the industry code we do the same thing on the industry side also the accept thing is we do not hard code like this we have got functions which we directly call we give the we pass the dataset as an input to that function and gives us an output so i will train you guys towards the end of our module or sorry to the end of our certification how you can create your own functions and call every time you need it because if we do it now then every time you will use this only rather than coding so we do not encourage that as of such okay so [Music] from decision tree to bagging all the algorithms yeah all the algorithm yes see if we are if we are if we have a gut feeling that yes random forest will work the best if we have that kind of in-depth knowledge of the data and depth knowledge of random forest in that case i will say yes do the sea i will show you how to form for example if you look at my data set my data set has how many 1 2 3 4 5 6 7 8 9 10 11 i've got 11 ways i can form my decision tree so my entropy calculation information gain calculation in random forest randomly have to choose so definitely you can you can understand that these things are meant for decision tree and random fast we have got a variety of data here to be choosing between correct but in the same case let us say if you have got only this much of data this was your total data if we ignore this part of it in this case you do not have much options so decision tree and random followers will be almost similar to each other agreed so in this case bagging and boosting will take charge i hope you people got it yeah so this is the way you can you know make your own analysis and try to do it but i will always suggest it does not take much time to check so better for checking purpose implement all the algorithms together even on the industry so yeah question one question since these quotes are almost uh pre-written and as you said it can be called through the function what is the actual role of the data analyst in that industry what does he actually supposed to do all this is almost automatic in nature so i'll show you what exactly a data scientist or data analyst will do see the the first thing which we have not like we have not discussed this so far because we are still under learning phase so the first thing what we do is validate our data do people know how do we validate the data has anybody tried doing that so if we talk about if you remember my first session so where we had sandeep also with us so in that case you remember he was continuously asking me okay why it should be normal and why there should not be any correlations or multi-collinearity should not be so there are so many assumptions that we have but we usually don't use it so a data analyst or a data scientist basically will are not data analysts or a data scientist will basically take this kind of approach where i will see that whatever data i have is it fit to be put inside machine learning first check to be done okay so uh second check will be your data cleaning so let us say you make modifications in your data again you have to go and validate it so what i usually do is this is my original data set and this is my change data set so i will say originally how similar it is have we changed the shape of the data have we changed the meaning of the data have we changed the standard deviation of the data have you changed the mean of the data have i changed the the iqrs of the data lot of things we go and check to validate if there is a huge amount of difference between both of them then i will say whatever i have done here no it is garbage basically got it yeah next part of a data scientist is basically to what i say is take a decision on which independent variables are to be used which are to be ignored because we have still not touched unsupervised learning right that's why you guys are a little confused on how i can remove it so that's that's the reason you guys are little little stuck over here that okay what is the difference between this and so once i complete your once we complete complete ml the whole ml i do know then we'll enter featurizes and in that featurization i will show you guys this method not to take anything blindly so right now we are in a learning curve where we are not doing any validation and testing but once we learn it i'll also show you how to validate and test your stuff okay next next part comes in uh stress testing so uh do we know performance testing stress testing have you heard these terms earlier guys the software part we use this so there is a software we will check the maximum limit it can take until it breaks down something like this so in our model also the aim or ml model that we'll put we'll do this kind of tests on that stress tests and we will come to know that if i test this model onto this data what is the lowest accuracy that i will get and what is the highest accuracy that i can get this is again a part of featurization module okay next i go and do validation on my data so let us say i completed my machine learning let us say i completed all of these techniques and the best just now we found out best we got was bagging which was 89.8 something for example is it possible to improve this accuracy in future guys see i got 89.8 on my training data but tomorrow if my data changes let us say client will use its own data every time right we will not use our training data always it will use their own testing data or futuristic data is it possible to increase this yeah yeah possible no so by what percentage we have to do some calculations so this is where a data scientist will come to picture the scaling scaling they do it scaling option they use yeah they do scaling definitely so when we provide them that this model is tuned on the scaling part definitely but that that does not mean that it's going to improvise accuracy yeah i hope you got my point see there are so many patterns inside the data so let us say in training data set if you remember 5 6 7 eight three and four so if you remember it was around forty two percent thirty two percent now tomorrow this might not be a factor within the client data or a futuristic data it might not be a division like this so definitely when there is no biasing at all there are chances that the accuracy is going to go high nothing wrong with that but the client will always ask you by how much so in that case a data scientist will exactly know how to get this thing done using statistics probability statistics and all that we'll use over here so what i can say is data interrogation yes exactly exactly so a data analyst what he will do is i will tell you a very clear cut differences between a data analyst a data engineer and a data scientist so a data scientist is basically a universal i will call it as a universal sect he knows completely what these two people do a data analyst is uh in our if you take the jd's of our uh current market data analyst is something like a reporting specialist or a tableau specialist or a person who's working on cue lick reporting and all if you heard about these tools or a person who's working on purely onto bi business intelligence where his job is to just find out some opportunities from the data and represented in the form of graphs and numbers okay he might be using this techniques no nothing wrong with that but these techniques he will already have it he is not going to make much changes on that because from the statistical point of view he's not that strong okay next is a data engineer so basically these guys come into picture nowadays they call them as hadoop admins if you guys have heard about hadoop so hadoop has its own ecosystem if you know uh a big hive lot of tools they have so on the real world let us say if we are talking about mysql so there is an hadoop version of it present within the hadoop cluster that will be your high for example so in that case these people will be specializing on how to handle a bigger data so a technique called map reducing techniques and data copying data multiplication duplication lot of things going on so this is what these guys will do basically okay and also sometimes you know when your model has to interact with the live database server and also the it has to interact with the client server so in that case these people will definitely come and help you in patching this up but a data scientist is the one who will be having a knowledge on this knowledge on this and also he will be responsible for guiding the whole project from a statistical point of view see i will tell you very frankly um out of 100 deliveries that or out of 100 proposals that a company does to the client 70 of the of them if the client digs down and basically finds out these stuff no definitely there will be some violations for sure because they do not use very good authentication techniques 30 of the ones i can say they are real you know deliveries where if the model says we are predicting something the predictions are really good and accurate okay so this is where a data scientist will kind of come in so i hope i answered your question don't go in detail perfect good so yeah question one more thing here oh during with this course we would be able to cover what from a data scientist perspective or a data analyst perspective data scientist perspective yeah we would be able to cover all this everything see what i do is i do not use direct codes from great learning what i do is i put my flavor so if you see uh let me show you my flavor so this is where my flavor comes in so if you see here i have used validation techniques i am showing that whatever we are doing above is it good or bad a testing kind of environment and if i if you if you still scroll down let us say uh down the line i am i'm also using something like future predictions we are importing servers you know we are writing queries we are predicting something we are exporting back to the server so this is the db part this is the statistical part i will say and sometimes i go deep into eda also that will come the analyst products so some of like once we finish mlo my uh case studies will also include tableau interfaces so where i'll take you guys to tableau we'll put geographical maps we'll put different type of graphs and all using this data so i'll give you a complete in and out exposure as a data scientist okay okay yeah i usually my codes are little bigger just because of this reason okay and later on i will we will also learn how to optimize these things how to package these things so there is another role called packaging expert if you guys have heard about those cherries where there are specialist people who try to package this stuff together as a deployment to client so i will try to show you as much as possible how much we can package it so then in production i do not work like this this is not a good way to work in production you should have ready-made automations where i just have to trigger it it will be called data will be given outputs will be thrown out immediately yeah but that will come once we you know we are at the end of the the aiml course so the last whole month i will dedicate only on to the practical part of it or in the industry implementation part of it okay okay perfect yeah so i'll just uh tell you i just uh ask you guys to hold on uh for two more modules why because currently you're learning curve like it's it's it's it's ongoing right now so once after i think fifth or sixth module once we complete it then it will become like kind of stagnant where you will have the complete overview of machine learning and then we will play around with this stuff okay at that time these concepts uh you know will come in and it will aid your view so this is it so yeah so i will discuss about validation techniques and all once we reach to the stage okay if i do it now it will be too much like information overload for you guys so this is basically this dataset is not meant for the session but since i wanted to show you an implementation i have done this but i'll show you how to do the testing and all once we come to that module okay next what i was wanted to show you was how you can this is not again a part of your ensemble technique so please uh you know just try to understand on a high level view see under random forest for example i am choosing random forest okay what parameters you people know under random forest has anybody gone and explored these parameters for sure you are currently using only this parameter n estimators is equal to 100 200 300 whatever you put but are we using other parameters i will say not for this module you are not it's not mandatory but just in case if somebody wants to say that i want to streamline i want to streamline all my accuracies let us say it's like let us say there is a team that sits in one country and there is a team which sits in another country and you have a mutual understanding that no matter what is your data set your model should be tuned and you know let us say it should have an estimator says say hundred a number of samples is one there's let us say you you baseline yourself to certain values and you communicate with each other and you say from tomorrow if any random forest implementation comes you have to streamline your values and attach only these values in that case i will say your accuracies will match so it's like our macd funda you have a macd in india or you have a macd let us say not i will not compare foreign because the qualities are different there let us say you have a macd manga or macd mumbai for example the quality will be more or less same you're not able to find out the difference something like that you do it when you baseline this stuff so you can use these parameters to do that you can you guys can try this out nothing wrong okay but again this is a part of post ml you are supposed to do all these things so there might be some concepts which you might find a little difficult here let us say you change this from two to three then it'll be a little difficult for you to understand okay why the accuracy changed so in that case you can do a for now you can for the project you can do some trial error methods here to work on okay uh guys fair enough or anybody wants to know in detail i have no problem i can explain you in detail also right away depends on you guys because it will be an information overload again too much of stats that are flowing around let me know if you guys are interested i will i'll definitely give you some of yeah you can you can go through some of the important features in this which perfect so which you often use in your production okay perfect so the most important ones that we use is your nst meters because this determines how many times you want to do your random forest you know how many trees you want basically next is your uh bootstrapping so this is like you are telling that if you remember i said the input given to random forest will be randomly chosen values bootstrapped values the the concept that we use for bagging the same input concept is used for random falls so if you want mandatory the if you do not call this function it will be true itself but if you do not want it you can put false here you you can not acknowledge this i am saying we do not want randomly chosen choose it in a grid kind of fashion but if you are giving bootstrap as true then also you should acknowledge the ob score okay so usually it is given as false but i will say to be fair enough because you people understood ob ob is if you are using bootstrapping yes ob will always occur so in that case you put it as true your accuracy will go down a little but the model will be stable model okay next is your maximum features to be used so if you remember just now we discussed we had 11 independent variables or in data science we call it features now how many features to use it's not possible to use all of them right let us say from five features itself when i use i got my leaf nodes all my leaf nodes are ready from five inches so if you want to control that if you don't want to control that you can give auto if you want to control that there is one more parameter called sqrt that means you take the square root of this and it will limit itself to these many parameters if you take the square root of 11 it will be what will be i can say the nearest value is nine so around two yeah so it will use only two parameters to move on it is a little risky one square root one but if tomorrow you have 1000 features i have done one implementation where we had you know huge amount of data in that case square root function will work the best so you are saying limit your decision tree splits only 200 boxes so if you remember what are these things is nothing but your criteria splitting criteria if yes if not how many you want you don't want more than 100 so this is the way you are controlling your time so these are the important features that i usually use some of you would like to see what is going on in the background because there's a lot of estimators are working so for that you can put verbose verbal is like a live this thing what is the execution so like let me show you a verbose example i usually switch off the verbose because it takes lot of time i think we have those under bootstrapping yeah if you look at here this is what my verbose is so what i've done is i've done a bootstrapping validation where i'm running my stuff for more than 10-15 times if you look at this each step it is showing me what is going around so if you are if you want to pinpoint some problem or if you want to understand step by step you have to activate your workforce function for now so these are some of the important features that i usually use okay but um i have seen some of my batches using all of them together okay initially it's a little difficult because there are a lot of combinations happening but you know once you play around the difficult part this will become very easy for you guys all these things uh christian do you have some document where there are a lot of features in every algorithm the decision tree random quiz so do you have any document where all these features are covered in detail and what what is explaining what so that we can refer it as a go to document later on when we are implementing it okay so i will say the original documentation of these psychic learn or c-bond or matplot will be one option or else if you want to let me show if you want to understand what that function does you can do shift tab when you do shift tab you will get the complete option so it will try to do that but a lot of a lot of time the data is missing is just yes so in that case i will say refer the official documentation this is coming from where i could learn anything only let me check yeah scala so just uh go to scikit learn and put gradient boosting it will give you the complete list so what i did i will tell you frankly when because i started with r my career and then i shifted to python so it was very easy for me to transition so what i did for r was i made an excel file with links so tomorrow if i want to understand gradient boosting for example in in r i'll put the web link over there so i'll go there and refresh my way i know i i got your point it's not easy to you know remember all these hyper parameters correct yes so i usually now what i do is i let me run one minute now what i do is i usually do shift tab i feel this is more handy rather than you making a big document it's difficult sometimes and also the good thing is they have given you all the explanation here so sometimes it helps sometimes it doesn't help i agree okay uh definitely i'll what i do is every month or twice i sit uh on web to refresh my knowledge so if i try it if i find out some blog like a black book or a handbook kind of thing for this i'll share it with you guys okay i doubt because of this open source stuff no not many people are interested to spend the time and help others this way we don't find it okay so this was it ensemble so this is what is ensemble overall okay and uh what i did was here was i have connected again to sql and we are trying to do a lot of stuff but i think you have already seen one of my implementations on sql so i will keep this on the back burner for now i have shown you guys right in the first session itself if you guys remember where we have connected our linear regression regressor to a server and then we have done it yeah uh you guys want me to repeat it i will do this i don't remember actually oh you don't know okay so um okay let me let me do it then just just hold on for a minute i'll try to load our models otherwise do not work give me one minute okay random ada gradient backing okay so we are done no need to do validation we are good stratified model tuning no need grid search on it yeah so now what i have what i've done is uh usually see we are lucky right away to work on static excel file and csv file but not always we expect a data in a very good format like this so usually the data comes in from a server and is thrown back to the server so for now what i've done is let me start my sql server okay so we are done so this starts my and i'm hosting locally so there is no external server in one and for visualizing the data set and using my workbench okay so let me log in here also good so what do i have here is let me check the name of the table the name of the table is wine future db so what do i have here is i have my own node krishna under this node i have got multiple tables and this is the table that i am currently looking for so if you see here i have got some futuristic data which does not have the target variable if you see here this is how the data comes from the client so this is what is the client data looking like client will not be able to have the target variable this is our model is going to predict it basically so so far in your videos you would have seen the professor or the person who's taking the videos ending at what at the accuracy level so as soon as you get accuracy they will end it up right so theoretically it's good we understood now what happens in in industry is we try to connect our model so now our model is ready what is the model we have we have a model called rfcl dot rfcl that's it so your random forest classifier is our model from where it is coming it is coming from our above our data set so if i'll show you quickly there is a trend yeah so if you see this model rfcl that we have built this model is saved now i am using the same model the only thing difference here is i am not using an excel file as an input where is it yeah i am not using my excel file now i'm connecting it to the server now for this you need some libraries so mysql connector we need and also we need my actual cursor so sometimes what we do is if you guys write your query here modify your table and then push it here one way or else we can write our query right in python so to do that you need your cursor yeah so if i go down the line we have to define a connection first of all we have to log into the server virtually so for that i am design defining the name of my connectivity say db we can even use some other name no problem then i am saying python's mysql connect so this is the command in that we have got hyper parameters like host so usually we will put our ip here but for now i am hosting locally so i have a local host next is your username your password and the node that you are looking for now again the problem is you have got multiple nodes multiple tables so for now i am defining that the node that i am looking for is this particular node so once you establish the connection you are going to get a confirmation like this so it doesn't make much sense but yeah you should not get an error here if you get an error some of these stuff is not good or your libraries are not perfect now the problem is it took some time for me to install this on mac so on mac the support is not good but if you are working from windows point of view these this libraries will not at all trouble you anyway next i'm defining a cursor so what i'm doing is i'm writing a query saying select everything from this particular table under what connection under this particular connection yeah so i am saying my cursor is nothing but my connection db and you implement a cursor on that and then i am executing my query so it's a very helpful command sometimes if you want to do querying from python next i'm defining my pandas data frame to import it so earlier we should write pandas dot read underscore excel csv html json whatever you have now i'm saying sql that's it and then i'm putting my query or i can give this to a variable and i can put the name of the variable here nothing wrong with that and then i am saying the connection name is db so now it will execute the connection all right so once i do that you can see my data is imported and the quality the target variable is missing here if you see now what i will do is i will predict it using what using my above model so i am saying rfcl dot predict on what fdb fdb is what my futuristic pandas data frame when i do that i get my target variable like this then what i do i join this particular thing with my original data set so if you see here target was missing if you see here now target is appended correct and just to check if everything is okay you can do the target imbalance stuff if the client is not happy but again client cannot come and question us here because it is his data so it doesn't matter much but for now looks good so this is how you can use your live sql uh live server basically to connect so also i have i am currently working on nosql that is your mongodb because even is you know coming up as a very crucial source of data where the problem is it's not an rdbms system basically so i'm finding some trouble with mac to connect jupiter or python with but i'm still working on it once i am successful on that i will show you guys that also so this was an example of mysql how we can connect it if you refer my first code that i have shared with you guys the first session itself of a linear regression i have done the same thing there so you guys can use my code for any of the verbal it's an open one so there's no restriction yeah so this is what's uh the server part okay guys fine so i'll be good with ensemble now at least i can say 50 60 percent because once you start practicing your start writing your own code then you will get more clarity onto that and again the code that i have will be sharing with you guys these codes are universal codes yeah so if i am if i am giving you a code which is running backing this will work for any technique or any dataset you don't have to worry about that you just have to use it that's it okay perfect so shall i move on now to clustering a brief idea about what is clustering and then it will help you guys in your videos also so what is clustering is it's very simple now we are talking about unsupervised learning so please remember in unsupervised learning target variable is missing target is missing here so whenever you find no target variable immediately you have to jump to unsupervised learning techniques now there are various types of unsupervised learning techniques i will define some of them so we have got clustering k means clustering hierarchical clustering we have pca principal component analysis singular vector decomposition factor analysis natural language processing um i think these are the most important ones that i usually use so there are variety of unsupervised learning techniques so now today whoever asked me how to differentiate between algorithms unsupervised learning will be the best cluster where you can differentiate actually so for example if you have a data set and the class is missing there is no target there is no class variable in that case you can go ahead with the clustering techniques so what does clustering do is very simple you have a independent set of variables now we will use clustering to give them a target variable that means some of them we will say they are in cluster number one some of them are in cluster number zero so it is like a technique to generate a target variable that's it and then you can use this data set to give as an input to supervised learning why if you look at this data set this data set is valid for supervised why because it has independent variable also it has got target variables so later on you can combine this data set under your supervised learning and carry okay so do not have a siloed thinking that okay so unsupervised learning means we can we will end it once we generate the target for it no nothing wrong in giving it to future study okay uh second way of so i'll give you a complete view of clustering so this was just a high level view next is your principal component analysis has anybody heard about pca this is one of the most complicated algorithms in the learning have you heard about this base yeah i think it is used to take care of a multiculturality in the statuses column okay or else i will try to reform your sentence i will say it is used to reduce your dimensions so let us say you have got 100 columns 1000 columns its not possible for us to handle this many so we will downsize this such a way that we retain only important columns we do not retain the columns which are not that important okay so i will have a good session on this and usually all of my batches at least i have to explain them three times three to four times then they get this concept so we will hold on till it starts but this is the overview of pca svd is also similar to dimensional reduction but it is more of a predictive technique where it will predict missing values basically so this we use it in recommendation systems this is your module after feature engineering so once you complete your machine learning models now the next module itself is recommendation model of systems yeah and uh little opposite to pca is our factor analysis again this is used for dimensionality reductions so these both are kind of related i will show you this is not a part of your certification but anyway i will show you separately okay so this is your unsupervised learning so today we will be talking about what is clustering okay and what is the use of clustering so i use clustering for two techniques or two situations let us say there is a data set which has target variable and there is a data set which does not have a time between for both of them i will use clustering how i will show you for this one i hope you people understood the technique of clustering what does it does is it tries to cluster similar stuff together so let us say this is cluster number one this is cluster number zero this cluster number two same values will be appended back to your dataset so whichever points are there under cluster number one all of them will get target variable is one other ones will get target variable zero and whoever that into will get target variables so this is the way we solve a problem or we try to you know visualize a data set another way is let us say tomorrow we find out somehow that the target variables given by the client are not that great like i'll give you example we just saw in the wine data set we had these target variables guys agreed now it is not necessary that what client has given us is optimal we can say the client that we will ignore your target variable we will say we do not want this we will regenerate a target variable for you and this time we will regenerate only three target variables that means we will say quality a quality b quality c so once we get quality a b and c together i will say i will ignore this particular stuff why it would have solved my balance target imbalance problem and also rather than having six targets i have narrowed down to three of them correct guys so this is an unconventional way of using it not many people do or have a thinking like this but try to you know think out of box where if your if target is troubling you ignore it generate your own target nothing wrong into that guys call it a very high level view now we'll dig down how to do this are we good yeah yeah perfect guys one one suggestion or one one what is a request for all of you is if we are not getting the concept no if if even a single stuff you people find we are not understanding of please talk me right away why because we do not i don't want you guys to carry any confusion down the line yeah so we have good amount of time please stop me and you know tell me to reiterate now coming back to the clustering part i'm sorry yeah so i i will give you one example where there is a pizzeria basically and let us say the pigs area is situated somewhere here so this is a google map on purpose i have imported this and uh ignore the colors right now there is a pizza area and all these people so if i change it actually i made a video out of it but i have lost that video now so it was a simulated video which showed us how this thing works and i will try to put some scatters here so it look realistic yeah so let us say there is a bits area here and all the customers and also i will remember the plus signs creates unwanted problems yeah these are the customers of the physique so what happens is the place is pretty far even though the place is far these customers are traveling yeah so what happens is this pizzeria is observing a lot of rush on weekends so these people prefer that on weekends anywhere we have time let's travel but on the weekdays let's not go there because of the commuting issues so now the business problem is this pizzeria has to think about rather than having a larger one here he's telling i will try to open one piece area which will be equidistant for all of them so let us say he opens a bids area over here and uh again he found out that somehow people are not committing that often because of the distance might be so in that case what he does is he assumes that there are three clusters of client what he assumes guys please remember this word we are not sure we are assuming this part of it so let me merge let me make it bigger to center so let's assume we have got three different clusters so randomly i will put a pixel area here randomly i will put one piece area here and randomly say i will open one piece area here for example now if you still look at it i will say this is not so let us say this is one cluster for example let us say this is one cluster and let us say this is one more cluster for example still i will say the places where this person has chosen these pizzerias is not centered at all it's not equidistant for all of them so now the business problem lies try to find out the place which is equidistant from all the points so for that reason what these guys will do is they will start rotating so for the first rotation let us say these points find these particular places so now if you look at the colors i will say the orange one is if i'm not wrong ah i'm changing the text i will say yeah i will say this is one cluster okay then i will say this is one more we'll make it violet okay and remaining ones will make it blue this one right yeah so let's say we have found out that these customers are divided like this so how do we do this is very simple there will be a distance formula involved knn similar concept is kn now once it is done i am now understanding that if this point is nearer to me even this point is kind of nearer to me so in the next run what will happen is this particular color will come and sit with a purple one so it will change its cluster and say now i am with purple why because i am nearer to him so as soon as it changes no the center point will readjust itself as per the distance okay now also if you observe here this particular plus is having a claim on this this also plus is having a claim on this and this plus is also having kind of near to this so i will say this is nearer to this particular point so again i will change this to purple when i change this to purple again my centroid changes when my centroid changes again i will say this particular point is again nearer to me so i will make it purple okay so when this thing is settled i will again change my centroid and put it here when this thing is sent and settled what will happen the centroid will go back and sit here this centroid will go back and sit here now do we have any more changes guys do you feel any more changes will happen here i will say no so now we have seen that this is one cluster this is another cluster and this is the third cluster so i will name this cluster number a cluster number b cluster number c got it this is called clustering and if you predefine your k value let us say k is equal to 3 this becomes k means clustering this is your first topic in the next module so and now when you are looking at the videos it will be easy for you guys to understand k means clustering okay simple straight forward right true yeah yeah now the only challenge is here is very smartly i have kept a good amount of distance between the points okay do not expect this in the live production environment so in production you will have sometimes overlaps so also sometimes the clusters will overlap with each other yeah so in production this is going to go a little difficult it will not be as easy as we see here but for understanding point this is good now there are two problems with this technique the first problem is deciding number of k values so we have techniques for that so don't worry about it we have got techniques which will show us how to decide the k value and all that yeah but this if it is not decided properly it's going to create a lot of chaos so for example what if i would have given k value as 6 for example what would have happened we would have got six different clusters so let us say this cluster which we see as one we could have splitted it as two this cluster which we see as one two is also we can split it as 2. what is going to happen is this is going to create same problem as your decision tree overfitting problem overfitting means as per my training data convenience i am saying six clusters are formed properly good so if i show you visually earlier we had three clusters now i'm splitting these three clusters i'm saying i have got smaller clusters but they are six in number something like that so what's going to happen lot of splits are happening in decision tree if you remember lot of depth was there in the split so because of that training was working good in the testing phase when new data comes in it's not necessary it will be similar type similar issue happens here if k value is not chosen properly second is when the data set is pretty huge here we have hardly 15 i can say 135 9 and 5 around 14 points we have for example what if we have 1 lakh points imagine how many times this cycle will repeat sometimes it goes time it's time consuming and in production environment it will create some issues but if you ignore these two problems then i feel this this is a very powerful and easy technique where you will get optimal you know cluster names so basically you are generating your target variable somehow okay so any questions for me on clustering no good okay so now uh i'll give you an overview on how we decide the k value it's very simple so what do we do is we assume k values let us say from 1 to 10 yeah and also we put mean sum of errors or we call it mscs basically so we will observe that usually we call it as an elbow plot so let us say the plot looks something like this okay so where we are seeing when we are increasing the number of clusters the error rate is coming down yeah that should that is that is a correct analogy but if you have too many clusters also overfitting issue happens which is not displayed by this cluster so now we are supposed to or else one minute let me make it a little better let me use something like this okay so let us say you got an elbow plot like this now how do we determine the optimal value of k as the name says elbow so try to find out a place where you have an elbow kind of structure so if you see here this is a place after which it is continuously it's an elbow structure so we will when we drop this down we are going to get one number which belongs to your k value so we are going to choose this value as your k value usually what i do is i keep an option of plus 1 and minus 1 also so if this is giving me an accuracy i go down one level down i'll try to calculate accuracy here also and here also somehow okay and we'll see which works better because the msc is in reducing even after this point i shouldn't be taking the higher k value yeah so uh it's a good point actually if we take this higher k value know so for the training part and the current testing part it'll work good but tomorrow when you have a newer data set now there might be an overfitting issue because let us say right now we are having 10 clusters why because our data is like that tomorrow let us say i have let us say this data was one lakh rose tomorrow i have a data set which is only say 20 rows so in that case what will happen these clusters will kind of fail the data will be confused okay this clusters will be confused why am i empty here because the centroids of this these clusters whatever it will disturb others when they are empty okay so overfitting under uh clustering i will go through anyway in the code again but keep this point in your mind that if too much of something is also not that good it's like an uh you know a person who is over organized too much of organization also not great this brings us to the end of this tutorial on ensemble learning now let's have a quick recap we started off by learning the concepts of decision tree then we learned about different bagging and boosting techniques finally we had a demo to implement all the ensemble techniques now before you guys sign off i'd like to inform you guys that we have launched a completely free platform called great learning academy where you guys have access to free courses such as ai cloud and digital marketing you can check out the details in the description below so thank you and have a great learning ahead you 