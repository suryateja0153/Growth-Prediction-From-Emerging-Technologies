 alright let's get started good afternoon so last time I started talking about the use of natural language processing to process clinical data and things went a little bit slowly and so we didn't get through a lot of the material I'm gonna try to rush a bit more today and as a result I have a lot of stuff to cover so if you remember last time I started by saying that a lot of the NLP work involves coming up with phrases that one might be interested in to help identify the kinds of data that you want and then just looking for those in text so that's a very simple method but it's one that works reasonably well and then Cattleya was here to talk about some of the applications of that kind of work in in what she's been doing in cohort selection so what I want to talk about today is more sophisticated versions of that and then move on to more contemporary approaches to natural language processing so this is a paper that was given to you as one of the optional readings last time and it's worked from David's on tags lab where they said well how do we make this more sophisticated so they start the same way they say okay dr. Leal let's say give me terms that are very good indicators that I have the right kind of of patient if I find them in the patient's notes so these are things with high predictive value so you don't want to use you know a term like sick because that's gonna find way too many people but you want to find something that is very specific but that has a high predictive value that you're going to find the right person and then what they did is they built a a model that tries to predict the presence of that word in the text from everything else in the medical record so now this is an example of sort of a silver standard way of training a model that says well I I don't have the energy or the time to get doctors to look through thousands and thousands of records but if I select these anchors well enough then I'm going to get a high yield of correct responses from those and then I train a machine learning model that learns to identify those same eye those same terms or those same records that have those terms in them and by the way from that we're going to learn a whole bunch of other terms that are proxies for the one that the ones that we started with so this is a way of enlarging that set of terms automatically and so there are a bunch of technical details that you can find out about by reading the paper they used a relatively simple representation which is essentially a bag of words representation they then sort of masked the three words around the word that actually is the one they're trying to predict just to get rid of sort of short-term syntactic correlations and then they built a an L to regularize logistic regression model that said what are the features that predict the occurrence of this word and then they expanded the search vocabulary to include those features as well and again there are tons of details about how to how to discretize continuous values and things like that that you can find out about so you build a phenotype estimator from the anchors and the chosen predictors they calculated a calibration score for each of these other predictors that told you sort of how well it predicted and then you can build a joint estimator that you uses all of these and the bottom line is that they did very well so they looked at in order to evaluate this they looked at eight different phenotypes for which they had human judgment data and so this tells you that they're getting a you sees of between point eighty three and point ninety five for these different different phenotypes so that's quite good they in fact were estimating not only these eight phenotypes but forty something I don't remember the exact number much larger number but they they didn't have validated data against which to test the others but the expectation is that if it does well on these it probably does well on the others as well so this was a very nice idea and just to illustrate if you start with something like diabetes as a phenotype and you say well I'm gonna look for anchors that are a code to 50 diabetes mellitus or I'm going to look at medication history for diabetic therapy so those are the initial the silver standard goals that I'm looking at and those in fact have a high predictive value for somebody being in the cohort and then they identify all these other features that predict those and therefore in turn predict appropriate selectors for the phenotype that they're interested in and if you look at the paper again you what you see is that this outperforms over time the standard sort of supervised baseline that they're comparing against where you're getting much higher accuracy early in in a patient's visit to be able to identify them as belonging to this to this cohort I'm gonna come back later to look at a another similar attempt to generalize from a core using a different set of techniques so you should see that in about 45 minutes I hope okay well context is important so if you look at a sentence like mr. Huntington was treated for Huntington's disease at Huntington Hospital located on Huntington Avenue each of those mentions of the word Huntington is different and for example if you're interested in eliminating personally identifiable health information from a record like this then certainly you want to get rid of the mr. Huntington part you don't want to get rid of Huntington's disease because that's a medically relevant fact and you probably do want to get rid of Huntington hospital and its location on Huntington Avenue although those are not necessarily something that you're prohibited from retaining so for example if you're trying to do quality studies among different hospitals then it would make sense to retain the name of the hospital which is not considered identifying of the individual so we in fact did a study back in the mid 2000s where we were trying to build and improved the identifier and and and here's the way we went about it this is a kind of kitchen sink approach that says okay take the text tokenize it look at every single token and derive things from it so the words that make up the token the part of speech how it's capitalized whether there's punctuation around it which documents section is it in you know many databases have sort of conventional document structure if you've looked at the mimic discharge summaries for example there's a kind of prototypical way in which that flows from beginning to end and you can use that structural information we then identified a bunch of patterns and thesaurus terms so we looked up in the U MLS words and phrases to see if they match some clinically meaningful term we had patterns that identified things like phone numbers and social security numbers and addresses and so on and then we did parsing of the text so in those days we use something called the link grammar parser which doesn't make a whole lot of difference what parser but you get either a constituent or a constituency or a dependency parse which gives you relationships among the words and so it allows you to include as features the way in which a word that you're looking at relates to other words around it and so what we did is we said okay the lexical context includes all of the above kind of information for all of the words that are either literally adjacent or within an words of the original word that you're focusing on or that are linked by within Kay links through the parse to that word so this gives you a very large set of features and of course parsing is not a solved problem and so this is an example from that that story that I showed you last time and if you see it comes up with 24 ambiguous parses of this sentence right so there are technical problems about how to deal with that today you could use a different parser the Stanford parser for example probably does a better job than the one we were using fourteen years ago and and gives you at least more definitive answers and so you could use that instead and so if you look at what we did we said well here is the text mister and here are all the ways that you can look it up in in the UML s and it turns out to be very ambiguous so Mr Stan's not only for mister but it also stands for magnetic resonance and it stands for a whole bunch of other things and so you get huge amounts of ambiguity blind turns out also to give you various ambiguity so it Maps here to four different concept unique identifiers is is okay 79 year old is okay and then mail again maps to four different concept unique identifiers so there are all these problems of over generation from this database and here's some more but I'm going to skip over that and then the learning model in our case was a support vector machine for this project in which we just said well throw in all the you know it's the kilomole and God will sort them out kind of approach so we just threw in all these features and said Oh support vector machines are really good at picking out exactly what are the best features and so we just relied on that and sure enough so you wind up with with literally millions of features but sure enough it worked pretty well and so stat the ID was our program and you see that on real discharge summaries were getting precision and recall up on pH I up around ninety eight and a half ninety five and a quarter percent which was much better than the previous state-of-the-art which had been based on rules and and dictionaries as a way of the identifying things so this was a successful example of that approach and of course this is usable not only for the identification but it's also usable for entity recognition because instead of selecting entities that are personally identifiable health information you could train it to select entities that are diseases or that are medications or that are various other things and so this was in the 2000s a pretty typical way for people to approach these kinds of problems and it's still used today I mean there are tools around that let you do this and they work reasonably effectively they're not state-of-the-art at the moment but they're simpler than many of today's state-of-the-art methods so here's another approach this was something we published a few years ago where we started working with some psychiatrists and said could we predict 30-day readmission for a psychiatric patient with any degree of reliability that's a hard prediction Willie is currently running an experiment where we're asking psychiatrists to predict that and it turns out they're barely better than chance at that prediction so it's not an easy task and what we did is we said well let's use topic modeling and so we had this cohort of patients about five and close to 5,000 patients about 10% of them were readmitted with a psych diagnosis and almost 3,000 of them were readmitted with other diagnoses so what one thing this tells you right away is that if you're dealing with psychiatric patients they come and go to the hospital frequently and this is not good for the hospital's bottom line because of reimbursement policies of insurance companies and so on so only of the 4700 only 1200 and 40 were not readmitted within 30 days so there's very frequent bounce back so we said well let's try building a baseline model using a support vector machine from baseline clinical features like age gender public health insurances of proxy for socioeconomic status so if if you're on Medicaid you're probably poor and if you're if you have private insurance then you're probably an MIT employee and are better off so that that's a frequently used proxy a comorbidity index that tells you sort of how sick you are from things other than your psychiatric problems and then we said well what if we add to that model common words from notes so we said let's do a tf-idf calculation so this is term frequency divided by log of the document frequency so it's sort of how specific is a term to identify a particular kind of condition and we take the thousand most informative words and so there are a lot of these so if you use a thousand most informative words from these nearly five thousand patients you wind up with something like sixty six thousand words unique words that are informative for some some patient but if you limit yourself to the top ten then it only uses 18,000 words and if you limit yourself to the top one then it uses about 3,000 words and then we said well instead of doing individual words let's do latent the earthly allocation so topic modeling on all of the words as a bag of words so no sequence information just the collection of words and so we calculated 75 topics from using LD a on all these notes so just to remind you the LD a process is a model that says every document consists of a certain mixture of topics and each of those topics probabilistically generates certain words and so you can build a model like this and then solve it using complicated techniques and you wind up with topics in this study as follows I don't know can you read these maybe too small but so these are unsupervised topics and if you look at the first one it says patient alcohol withdrawal depression drinking and ativan EtOH drinks medications clinic in patient diagnosis days hospital substance use treatment program name that's a the identified use abuse problem number and we had our experts look at these topics and they said oh well that topic is related to alcohol abuse which seems reasonable and then you see you know on the bottom psychosis thought features paranoid psychosis paranoia symptoms psychiatric etc and they said okay that's a psychosis topic so in in retrospect you can assign meaning to these topics but in fact they're generated without any a priori notion of what they ought to be they're just a statistical summarization of the common co-occurrences of words in these documents but what you find is that if you use the baseline model which used just the socio demographic and clinical variables and you say what's the difference in in survival in this case in time to readmission between one one set and another in this cohort and the answer is they're pretty similar whereas if you use a model that predicts based on the baseline and seventy five topics the 75 topics that we identified you get a much bigger separation and of course this is statistically significant and it tells you that this technique is useful for being able to improve the prediction of a cohort that's more likely to be readmitted from a cohort that's less likely to be readmitted it's not a terrific prediction so the AUC for this model was only on the order of 0.7 so you know it's not like 0.99 but nevertheless it it provides useful information the same group of psychiatrists that we worked with did also did a study with a much larger cohort but much less rich data so they got the all of the discharges from two Medical Center's over a period of 12 years so they had 845,000 discharges from four hundred fifty-eight thousand unique individuals and they were looking for suicide or or other causes of death in these patients to see if they could predict whether somebody is likely to try to harm themselves or whether they're likely to die accidentally which sometimes can't be distinguished from suicide so the censoring problems that David talked about are very much present in this because you lose track of people it's highly imbalanced data set because out of the eight hundred forty five thousand patients only 235 committed suicide which is of course the probably a good thing from a societal point of view but makes the data analysis hard on the other hand all cause mortality was about eighteen percent during nine years of follow-up so that's not so imbalanced and then what they did is they curated a list of 3,000 terms that correspond to what in the psychiatric literature is called positive valence so this is concepts like joy and happiness and good stuff as opposed to negative valence like depression and sorrow and all that stuff and they said well can can we can use these types of terms in order to help distinguish among these patients and what they found is that if you plot the kaplan-meier curve for different quartiles of of risk by for these patients you see that there's a pretty big difference between the different quartiles and you can certainly identify the people who are more likely to commit suicide from the people who are less likely to do so this curve is for suicide or accidental death so this is a much larger data set and therefore the error bars are smaller but you see the same kind of separation here so these are all useful techniques now switch to another approach this was work by one of my students you on wall who was working with some lymphoma pathologists at Mass General and so the approach they took was to say well if you read a pathology report about somebody with lymphoma can we tell what type of lymphoma they had from the pathology report if we blank out the part of the pathology report that says either pathologists think this person has you know non-hodgkins lymphoma or something so from the rest of the context can we make that prediction now you want took a kind of interesting slightly odd approach to it which is to treat this as an unsupervised learning problem rather than as a supervised learning problem so he literally masked the real answer and said if we just treat everything except what gives away the answer as just data can we essentially cluster that data and some interesting way so that we re identify the type the different types of lymphoma now the reason this turns out to be important is because lymphoma pathologists keep arguing about how to classify lymphomas and every few years they revise the classification rules and so part of his objective was to say let's try to provide a an unbiased data-driven method that may help identify appropriate characteristics by which to classify these different lymphomas so his approach was was a tensor factorization approach so this is you know you often see datasets like this that say you know patient by characteristic so in this case laboratory measurements systolic diastolic blood pressure sodium potassium etc that's a very vanilla matrix encoding of data and then if you add a third dimension to it like this is at the time of admission 30 minutes later 60 minutes later or 90 minutes later now you have a three dimensional tensor and so just like you can do matrix factorization as in the picture above where we say my matrix of data I'm going to assume is generated by a product of two matrices which are smaller in dimension and you can train this by saying I want entries in these two matrices that minimize the reconstruction error so if I multiply these matrices together then I get back my original matrix plus error and I want to minimize that error usually root means square or mean square or something like that well you can play the same game for a tensor by having a so-called core tensor which identifies the subset of of characteristics that subdivide that that dimension of your data and then what you do is the same game you have matrices corresponding to each of the dimensions and if you multiply this core tensor by each of these matrices you reconstruct the original tensor and you can train it again to minimize the reconstruction loss so there are again a few more tricks because this is dealing with language and so this is a typical report from one of these lymphoma pathologists that says immunohistochemical stains show that the follicles blah-blah-blah-blah right so lots and lots of details and so he needed a representation that could be put into this matrix tensor this tensor tensor factorization form and what he did is to say well let's see if we look at a statement like this the immuno stat stains show that large atypical cells are strongly positive for CD 30 negative for these other surface expressions so the sentence tells us relationships among procedures types of cells and immunologic factors and for feature choice we can use words or we can use um LS concepts where we can find various kinds of mappings but he decided that in order to retain the the the syntactic relationships here what he would do is to use a graphical representation that came out of again parsing all of these sentences and so what you get is that this creates one graph that talks about the you know strongly positive for CD 30 large atypical cells etc and then you can factor this into sub graphs and then you also have to identify frequently occurring sub graphs so for example large atypical cells appears here and also appears there and of course will appear in many other places yeah so in this particular study he was using the Stanford parser with some tricks so the Stanford parser doesn't know a lot of the medical words and so he basically marked these things as noun phrases and then the stanford parser also doesn't do well with long lists like the the set of immune features and so he would recognize those as a pattern substitute a single made-up word for them and that made the parser work much better on it so there were a whole bunch of little tricks like that in order to adapt it but it was not a model train specifically on this I think it's trained on Wall Street Journal corpus or something like that so it's general English no he did he did it algorithmically but he didn't learn which algorithms to use he made them up by hand but then of course it's a big corpus and and he ran these programs over it that did those transformations so he calls it two-faced parsing there's a reference to to his paper it on the first slide in the section if you're interested in the details it's it's described there okay so what he wound up with is a tensor that has patients on one axis the words appearing in the text on another axis so he's still using a bag of words representation but the third axis is these language concept sub graphs that we were talking about and then he does tensor factorization on this and what's interesting is that it works much better than I expected so if you look at his technique which is called sa NT F the precision and recall are about 0.72 and 0.85 for macro average and 0.75 for micro average which is much better than the non-negative matrix factorization results which only use patient by word or patient by subgraph or in fact one where you simply do patient and concatenate the sub graphs on the words in one dimension so that means that this is actually taking advantage of the three-way relationship if you read papers from about 15-20 years ago people got very excited about the idea of by clustering which is the in modern terms the equivalent of matrix factorization so it says given two dimensions of data and I want to cluster things but I want to cluster them in such a way that the clustering of one dimension helps the clustering of the other dimension so this is a formal way of doing that relatively efficiently and tensor factorization is essentially try clustering okay so now I'm going to turn to the last of today's big topics which is language modeling and this is really where the action is nowadays in natural language processing in general I would say that the natural language processing on clinical data is somewhat behind the state-of-the-art in natural language processing overall there are fewer corpora that are available there are fewer people working on it and so we're catching up but I'm going to lead into this somewhat gently so what does it mean to model a language I mean you could imagine saying it's coming up with a set of parsing rules that that define the syntactic structure of the language or you could imagine saying as we suggested last time coming up with a corresponding set of semantic rules that say concept or terms in the language correspond to certain concepts and that they are combinatorially functionally combined as the syntax directs in order to give us a semantic representation so we don't know how to do either of those very well and so the current the contemporary idea about language modeling is to say given a sequence of tokens predict the next token okay if you could do that perfectly presumably you would have a good language model so obviously you can't do it perfectly because we don't always say the same word after some sequence of previous words when we speak but probabilistically you can get close to that and there's usually some kind of Markov assumption that says that the probability of emitting a token given the stuff that came before it is ordinarily dependent only on on n previous words rather than on all of history you know on everything you've ever said before in your life and there's a there's a meta there's a a measure called perplexity which is the entropy of the probability distribution over the predicted words and roughly speaking it's the number of likely ways that you could continue the text if all of the all of the possibilities were were equally likely okay so perplexity is is often used for example in speech processing we did a study where we were trying to build a speech system that understood a conversation between a doctor and a patient and we ran into real problems because we were using software that had been developed to interpret dictation by doctors and that was very well trained but it turned out we didn't know this when we started that the language that doctors use in dictating that medical notes is pretty straightforward pretty simple and so it's perplexity is about nine whereas conversations are much more free-flowing and cover many more topics and so it's perplexity is about 73 and so the model that works well for perplexity 9 doesn't work as well for perplexity 73 and so what this tells you about the difficulty of accurately transcribing speech is that it's hard it's much harder and that's still not a solved problem now you probably all know about Zipf's law so if you empirically just take all the words and all the literature of let's say English what you discover is that the the enth word is about 1 over and as probable as the first word okay so there's a long tailed distribution one thing you should realize of course is if you integrate 1 over n from 0 to infinity it's infinite ok and that may not be an inaccurate representation of language because languages is productive and changes and people make up new words all the time and so on so it may actually be infinite but roughly speaking there's a kind of decline like this and interestingly and the brown corpus the top 10 words make up almost 1/4 of the size of the corpus so you write a lot of those of Zan's A's - Zins etc and much less hematemesis right obviously okay so what about Engram models well remember if we make this Markov assumption then all we have to do is pay attention to the last end tokens before the one that we're interested in predicting and so people have generated these large corpora of engrams so for example somebody a couple of decades ago took all of shakespeare's writings I think they were trying to decide whether he had written all his works or whether the earl of somebody or other was actually the guy who wrote Shakespeare you know about this controversy yeah so that that's why they were doing it but anyway they created this corpus and they said so Shakespeare had a vocabulary of about 30,000 words and about 300,000 by grams and out of 844 million possible by grams so ninety nine point ninety six percent of by grams were never seen right so there's a certain regularity to his production of language now Google of course did Shakespeare went better and they said hmm we can take a terabyte corpus this was in 2006 I wouldn't be surprised if it's a petabyte corpus today and they published this they just made it available so there were thirteen point six million unique words that occurred at least two hundred times in this Tara Tara word corpus and there were 1.2 billion five word sequences that occurred at least 40 times so these are the statistics and if you're interested there's a URL and here's a very tiny part of their database so ceramics collectibles collectibles I don't know occurred 55 times in a terabyte of text ceramics collectibles fine ceramics collectibles by pottery cooking comma period end of sentence and at is etcetera different number of times ceramics comes from occurred 660 times which is reasonably large number compared to some of its competitors here if you look at four grams you see things like serve as the incoming blah blah blah 92 times serve as the index 223 times serve as the initial 5300 times okay so you've got all these statistics and now given those statistics we can then build a generator so we can say alright suppose I start with the token which is the beginning of a sentence or the separator between sentences and I say sample a random by grams starting with the beginning of a sentence and a word according to its probability and then sample the next diagram from that word and all the other words according to its probability and keep doing that until you hit the end of sentence marker okay so for example here I'm generating the sentence I starts with I then followed by want followed by - followed by get followed by China followed by food followed by end of sentence so I've just generated I want to get Chinese food which sounds like a perfectly good sentence so here's what's interesting if you look back again at the Shakespeare corpus and saying if we generate a Shakespeare from yoona grams you get stuff like at the top to him swallowed confess here both which of savon trail for our I device and wrote life have doesn't sound terribly good right it's not very grammatical it doesn't have that sort of English you know a Shakespearean English flavor although you do have words like knave and and I and so on that are vaguely reminiscent now if you go to buy grams it starts to sound a little better what means sir I confess she then all sorts he is trim captain right it doesn't make any sense but it starts the sound a little better and with trigrams we get sweet prince Falstaff shall die Harry of Monmouth's grave etc okay so this is beginning to sound a little Shakespearean and if you go to Quadra gram so you get King Henry what I will go seek the trader Gloucester exempt some of the watch a great banquet served in etc you know I mean when I first saw this like 20 years ago or something I was stunned this is actually generating stuff that sounds vaguely Shakespearean and vaguely English like here's an example of generating The Wall Street Journal so from you know grams months the my and issue of year foreign new exchanges September were recession its word salad but if you go to trigrams they also point to ninety-nine point six billion dollars from two hundred 406 three percent of the rates of interest stores as Mexico and Brazil so you imagine that this is some Wall Street Journal writer on acid right writing writing this text because it has a little bit of the right kind of flavor okay so more recently people said well we ought to be able to make use of this in some systematic way to help us with our language analysis tasks so to me the first effort in this direction was work to vac which was Nick loves approach to doing this and he developed two models he said let's build a continuous bag of words model that says what we're going to use is co-occurrence data on a series of tokens in the text that we're trying to model and we're going to use a neural network model to predict the word from the words around it and in that process we're going to use the parameters of that neural network model as a vector and that vector will be the representation of that word and we do and so what we're going to find is that words that tend to appear in the same context will have similar representations in this high dimensional vector and by the way high dimensional people typically use like 300 or 500 dimensional vectors so there's a lot of it's a big space and the words are scattered throughout this but you get this kind of cohesion where words that are used in the same context appear close to each other and the extrapolation of that is that if words are used in the same context maybe they share something about meaning so the other model is a skip gram model where you're doing the prediction in the other direction from a word you're predicting the words that are around it and again you're using a neural network model to do that and you use the parameters of that model in order to represent the word that you're focused on so what came as a surprise to me is this claim that that's in in his original paper which is that not only do you get this effect of locality as as corresponding meaning but that you get relationships that are geometrically represented in the space of these embeddings and so what you see is that if you take the encoding of the word man and the word woman and look at the vector difference between them and then apply that same vector difference to King you get close to Queen and if you apply it to uncle you get close to aunt and so they showed a number of examples and then people have studied this it doesn't hold up perfectly well I mean it's not like we've solved the semantics problem but it is a genuine relationship the place where it doesn't work well is when some of these things are much more frequent than others and so one of the examples that's often cited is if you go you know London is to England as Paris is to France and that one works but then you say as Kuala Lumpur is to Malaysia and that one doesn't work so well and then you go as you know oh good Judah or something is - whatever country it's the capital of and since we don't write about Africa our newspapers there's very little data on that and so that doesn't work so well so there was this other paper later from Van der Martin and Jeff Hinton where they came up with the visualization method to take these high dimensional vectors and visualize them in two dimensions and what you see is that if you take a bunch of concepts that are account concepts so one half 30 15 five four to three several some many etc there is a geometric relationship between them so they in fact do map to the same part of the space similarly you know Minister leader president chairman director spokesman chief had etcetera form a kind of cluster in this space so so there's there's definitely something to this okay I promised you that I would get back to a different attempt to try to take a core of concepts that you want to use for term spotting and develop an automated way of enlarging that set of concepts in order to give you a richer vocabulary by which to try to identify cases that you're interested in so this was by some of my colleagues including cat who you saw on Tuesday and they said well what we'd like is the fully automated and robust unsupervised feature selection method that leverages only publicly available medical knowledge sources instead of EHR data so the the method that David's group had developed which we talked about earlier uses data from electronic health records which means that you move to different hospitals and there may be different conventions and you might imagine that you have to retrain that sort of method whereas here the idea is to derive these surrogate features from knowledge sources so unlike that earlier model here they built a word to vex Kip Graham model from about five million springer articles so these are published medical articles to yield five hundred dimensional vectors for each word and then what they did is they took the concept names that they were interested in and their definitions from the UML s and then they summed the word vectors for each of these words weighted by inverse document frequency so it's sort of a tf-idf like approach to to wait different words and then they went out and they said okay for every disease that's mentioned in Wikipedia Netscape emedicine the merck manuals professional Edition the Mayo Clinic diseases and conditions MedlinePlus Medical Encyclopedia they used named entity recognition techniques to find all the concepts that are related to this phenotype okay so then they said well there's a lot of randomness in these sources and maybe in our extraction techniques but if we insist that some concept appear and at least three of these five sources then we can be pretty confident that it's a relevant concept and so they said okay we'll do that then they chose the top k concepts whose embedding vectors are closest by cosine distance to the embedding of this phenotype that they've calculated and they say okay the phenotype is going to be a linear combination of all these related concepts so again this is a bit similar to what we saw before but here instead of extracting the data from electronic medical records they're extract extracting it from published literature and and these web sources and again what you see is that the expert curated features for these five phenotypes which are coronary artery disease rheumatoid arthritis Crohn's disease ulcerative colitis and pediatric pulmonary arterial hypertension they started with you know 20 to 50 curated features so these were the ones that the doctor said okay these are the the anchors in in David's terminology and then they expanded these to a larger set using the tech that I just described and then selected down to the top and that were effective in finding relevant relevant phenotypes and this is a terrible graph that summarizes the the results but what you're seeing is that the orange lines are based on the expert curated features these this is based on sort of the an earlier version of trying to do this and said feh is the technique that I've just described and what you see is that the the automatic techniques for many of these phenotypes are just about as good as the manually curated ones and of course they require much less manual curation because they're using this automatic learning approach another interesting example to return to the theme of the identification is a couple of my students a few years ago built a new the identifier that has this rather complicated architecture so it starts with a recursive neuro a bi-directional recursive neural network model that is implemented over the character sequences of words in the medical text so why why character sequences why might those be important well consider a misspelled word for example right most of the character sequence is correct there will be a bug in it we're at the misspelling or consider that a lot of medical terms are these compound terms where they're made up of lots of pieces that correspond to Greek or Latin roots right so learning those can actually be very helpful so you start with that model you then concatenate the results from both the left running and the right running recursive neural network and concatenate that with the word Tyvek embedding of of the whole word and you feed that into another bi-directional RNN layer and then for each word you take the output of those RN ends run them through a feed-forward neural network in order to estimate the problem it's like a soft Max and you estimate the probability of this word belonging to a particular category of personally identifiable health information so is it a name is it an address is it a phone number is it or whatever okay and then the top layer is a kind of conditional random field like layer that imposes a sequential probability distribution that says okay if you've seen a name then what's the next most likely thing that you're going to see and so you combine that with the with the probability distributions for each word in order to identify the category of pH I or non pH I work for that word and this did insanely well so optimized by f1 score were up at precision of 99.2% recall of 99.3% optimized by recall were up at about 98% 99% for for each of them so this is doing quite well now there is a non machine learning can't comment to make which is that if you read the HIPAA law the HIPPA regulations they don't say that you must get rid of 99% of the personally identifying information in order to be able to share this data for research it says you have to get rid of all of it so no technique we know is a hundred percent perfect and so there's a kind of practical understanding among people who work on the stuff that nothing's going to be perfect and therefore that you can get away with a little bit but legally you're on thin ice so I remember many years ago my wife was in law school and I asked her at one point so what can people sue you for and she said absolutely anything right that they may not win but they can be a real pain if you have to go defend yourself in court and so this hasn't played out yet we don't know if a the identifier that is 99 percent sensitive and 99 percent specific will pass muster with people who agree to release datasets because they're worried too about winding up in the newspaper or winding up getting sued okay last topic for today so if you read this this interesting blog which by the way has a very good tutorial on on Bert he says the year 2018 has been an inflection point for machine learning models handling text or more accurately NLP our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving and so there are a whole bunch of new ideas that have come about in about the last year or two years including Elmo which learns context-specific embeddings the transformer architecture this Burt approach and then I'll end with just showing you this gigantic GPT model that was developed by the open AI people which does remarkably better than the stuff I showed you before in in generating language alright if you look inside Google Translate at least as of not long ago what you find is a model like this so it's essentially an LS TM model that takes input words and lunges them together into some representation high dimensional vector representation that summarizes everything that the model knows about that sentence that you've just fed it okay obviously it has to be a pretty high dimensional representation because your sentence could be about almost anything and so it's important to be able to capture all that in this representation but basically at this point you start generating the output so if you're translating English to French these are English words coming in and these are French words going out in sort of the way I showed you where we're generating Shakespeare or we're generating Wall Street Journal text but the critical feature here is that in in the initial version of this everything that you learned about this English sentence had to be encoded in this one vector that got passed from the decoder from the encoder into the decoder from the source language into the target language generator so then someone came along and said hmm someone namely these guys came along and said wouldn't it be nice if we could prove find some auxiliary information to the generator that said hey which part of the input sentence should you pay attention to and of course there's no fixed answer to that I mean if I'm translating an arbitrary English sentence into an arbitrary French sentence I can't say in general look at the third word in the English sentence when you're generating the third word and the French sentence because that may or may not be true depending on the particular sentence but on the other hand the intuition is that there is such a positional dependence and the dependence on what the particular English sentence was a English word was that is an important component of generating the French word and so they created this idea that in addition to passing along the this vector that that encodes the meaning of the entire input and the previous word that you had generated in the in the output in addition we pass along this other information that says which of the input words should we pay attention to and how much attention should we pay to them and of course in the style of this of these embeddings these are all represented by high dimensional vectors high dimensional real number vectors that get combined with the other vectors in order to produce the output now a classical linguist would look at this and and and wretch right because this looks nothing like classical linguistics it's just numerology that gets trained by by stochastic gradient descent methods in order to optimize the output but from an engineering point of view it works quite well so then for a while that was the state of the art and then last year these guys of asrani at all came along and said you know we now have this complicated architecture where we are doing the old-style translation where we summarize everything into one vector and then use that to generate a sequence of outputs and we have this attention mechanism that tells us how much of various inputs to use in generating each element of the output is the first of those actually necessary and so they published this lovely paper saying attention is all you need that says hey you know that thing that you guys added to this translation model not only is it a useful addition but in fact it can take the place of the original model and so the transformer is an architecture that is the hottest thing and since sliced bread at the moment that says okay here's what we do we take the inputs we calculate some embedding for them we then want to retain the position because of course the sequence in which the words appear it matters and the positional encoding is this weird thing word it encodes using sine waves so that it's an orthogonal basis and so it has nice characteristics and then we run it into an attention model that is essentially computing self attention so it's saying what it's like word to vac except in a more in a more sophisticated way so it's looking at all the words in the sentence and saying which words is this word most related to and then in order to complicate it some more they say well we don't want just a single notion of attention we want multiple notions of attention so what does that sound like well to me it sounds a bit like what you see in convolutional neural networks where often when you're processing an image with and you're not only applying one filter to the image but you're applying a whole bunch of different filters and because you initialized them randomly you hope that they will converge to things that actually detect different interesting properties of the of the image so the same idea here that what they're doing is they're starting with a bunch of these attention matrices and saying we initialized them randomly they will evolve into something that is most useful for helping us deal with the overall problem so then they run this through a series of I think in Bonnie's paper something like six layers that are just replicated and there are additional things like feeding forward the the input signal in order to add it to the to the output signal of this stage and then normalizing and then re running it and then running it through a feed-forward network that also has a bypass that combines the input with the output of the feed-forward Network and then you do this six times or n times and that then feeds into the generator and the generator then uses a very similar architecture to calculate output probabilities and then it samples from those in order to generate the text so this is sort of the contemporary way that one can do translation using this approach obviously I don't have time to go into all the details of how all this is done and I'd probably do it wrong anyway but you can look at the paper which gives a good explanation and that blog that I pointed to also has a pointer to another blog post by the same guy that does a pretty good job of explaining the the transformer architecture it's complicated so what you get out of the multi hat attention mechanism is that here is one one attention machine and for example the colors here indicate the degree to which the encoding of the word it depends on the other words in the sentence and you see that it's focused on the animal which makes sense because it in fact is referring to the animal in the sentence here they introduce another encoding and this one focuses on was too tired which is also good because it again refers to the thing that was too tired and of course by multi-headed they mean that it's doing this many times and so you're identifying all kinds of different relationships in the in the input sentence well along the same lines is this encoding called Elmo people seem to like Sesame Street characters so Elmo is based on a bi-directional lsdm so it's an older technology but what it does is unlike word Tyvek which built an encoding an embedding for each type so every time the word junk appears it gets the same embedding here what they're saying is hey take contact seriously and we're going to calculate a different embedding for each occurrence in context of a token right and this turns out to be very good because it goes part of the way to solving the word sense disambiguation problem so this is just an example if you look at the word play in glove which is a slightly more sophisticated variant of the word Tyvek approach you get playing game games played players plays player play football multiplayer this all seems to be about games because probably from the literary sure that they got this from that's the most common usage of the word play whereas using this bi-directional language model they can separate out something like kefir the only jr. and the group was commended for his ability to hit in the clutch as well as his all-around excellent play so this is presumably the baseball player and here is they were actors who had been at handed fat rolls in a successful play so this is a different meaning of the word play and so this embedding also has made really important contributions to improving the quality of natural language processing by being able to deal with the fact that single words have multiple meanings not only in English but in other languages so after Elmo comes Bert which is this bi-directional encoder representations from transformers so rather than using the lsdm kind of model that Elmo used these guys say well let's hop on the bandwagon use the transformer based architecture and then they introduced some interesting tricks so one of the problems with transformers is if you stack them on top of each other there are many paths from any of the inputs to any of the intermediate nodes and the outputs and so if you're doing self attention you're trying to figure out where the output should pay attention to the input the answer of course is like if you're trying to reconstruct the input if the input is present in your model what you will learn is that the corresponding word is the right word for your output so they have to prevent that from happening and so the way they do it is by masking off at each level some fraction of the words or of the inputs at that level so what this is doing is it's a little bit like the Skip Gramm model in word to vac it's trying to predict the likelihood of some word except it doesn't know what a significant fraction of the words are and so it can't over fit in the way that I was just suggesting so this turned out to be a good idea it's more complicated again for the details you have to read the paper I gave both the Transformer paper and the Berg paper as optional readings for today I meant to give them as required readings but I didn't do it in time so they're optional but there are a whole bunch of other tricks so instead of using words they actually use word pieces so think about syllables and and you know dolt becomes do and and apostrophe T and so on and then they discovered that about 15% of the tokens to be masked seems to work better than other percentages so that's those are the hidden tokens that that prevent over-fitting and then they do some other weird stuff like they instead of masking a token they will inject random other words from the vocabulary into its place again to prevent overfitting and then they look at different tasks like can I predict the next sentence in a corpus so I read a sentence and the translation is not into another language but it's predicting what the next sentence is going to be so they trained it on 800 million words from something called the books tokus token books corpus and about two and a half million word Wikipedia corpus and what they found was that there is an enormous improvement on a lot of classical tasks so this is a listing of some of the standard tasks for natural language processing mostly not in the medical world but in in the general NLP domain and you see that you get things like an improvement from you know 80% or even the GPT model that I'll talk about in a minute does at 86 any 2% they're up to about 86 percent so four percent improvement in this domain is really huge I mean very often people publish papers showing you know 1% improvement and if their corpus is big enough then it's statistically significant and therefore publishable but it's not significant in the ordinary meaning of the term significant if you're doing 1% better but doing 4 percent better is pretty good here we're going you know from like 66 percent to 72 percent from the earlier state-of-the-art 82 to 91 93 to 94 35 to 60 in the cola task corpus of linguistic acceptability so this is asking I think Mechanical Turk people for generated sentences is the sentence a valid sentence of English and so it's it's a an interesting benchmark so it's it's producing really significant improvements all over the place they train two models of it the base model is a smaller one the large model is just strained on larger data sets enormous amount of computation in doing this training so you know I've forgotten it took them like a month on some gigantic cluster of GPU machines and so it's daunting because you can't just crank this up on your laptop and expect it to finish in your lifetime so okay the last thing I want to tell you about is this GPT - so this is from the open AI Institute which is one of these philanthropically funded I think this one by Elon Musk Research Institute - advanced AI and what they said is well this is all cool but so that they were not using Burt they were using the transformer architecture but without the same training style as Burt and they said the secret is going to be that we're going to apply this not only to one problem but to a whole bunch of problems so it's a multi task learning approach that says we're gonna build a better model by trying to solve a bunch of different tasks simultaneously and so they built enormous models by the way the task itself is given as a sequence of tokens so for example they might have a task that says translate to French English text French text okay or answer the question document question answer and so the system not only learns how to do whatever it's supposed to do but it even learned something about the tasks that it's being asked to work on by encoding these and using them as part of its model so they built four different models take a look at the bottom 11.5 million 1.5 billion parameters okay this is a large model this is a very large model and so it's a byte level model so they they just said forget words because we're trying to do this multilingual e and so for Chinese you want characters and for English you you might as well take characters also and the system will in its 1.5 billion parameters learn all about the sequences of characters that make up words and it'll be cool and so then they look at a whole bunch of different challenges and what you see is that the state-of-the-art before they did this on for example the Lambada dataset was that the perplexity of its predictions was about a hundred and with this large model the perplexity of its predictions is about nine so that means that it's reduced the uncertainty of what to predict next ridiculously much I mean by more than an order of magnitude and you get similar gains accuracy going from fifty nine percent to 64 three percent accuracy on a this is the children's something-or-other challenge from eighty five to ninety three percent so dramatic improvements almost across the board in except for this particular data set where they did not do well and what really blew me away is here's an application of this 1.5 billion word model that they built so they said okay I give you a prompt like the opening paragraph of a Wall Street Journal article or a Wikipedia article and you complete the article by using that generator idea that I showed you before that just uses the language model and picks the most likely word to come next and emits that as the next word so here's a prompt that says a train carriage containing controlled nuclear materials was stolen in Cincinnati today its whereabouts are unknown by the way this is made up I mean this is not a real news article and the system comes back with the completion that says the incident occurred on the downtown train line which runs from Covington and Ashland stations in an email to Ohio news outlets the US Department of Energy said it's working with the Federal Railroad Administration to find the thief etc okay this looks astoundingly good now the paper from which this comes this is actually from a blog but they've also published a paper about it claims that these examples are not even cherry-picked if you go to that page and pick sample 1 2 3 4 5 6 etc you get different examples that they claim are not cherry picked and every one of them is really good I mean you could imagine this being an actual article about this actual event so somehow or other in this enormous model and with the this transformer technology and with the multitask training that they've done they have managed to capture so much of the regularity of the English language that they can generate these fake news articles based on a prompt and make them look unbelievably realistic now interestingly they have chosen not to release that train model because they're worried that people will in fact do this F that they will generate fake news articles all the time they've released a much smaller model that is not nearly as good in terms of its of its realism so that's the state of the art in language modeling at the moment and as I say the the general domain is ahead of the medical domain but you can bet that there are tons of people who are sitting around looking at exactly these results and saying well we ought to be able to take advantage of this to build much better language models for the medical domain and to exploit them in order to do phenotyping in order to do entity recognition in order to do inference in order to do question answering in order to do any of these these kinds of topics now I was talking to Patrick Winston who is one of the good old-fashioned AI people as he characterizes himself and the thing that's a little troublesome about this is that this technology has virtually nothing to do with anything that we understand about language or about inference or about question answering or about anything and so one is left with this queasy feeling that here is a wonderful engineering solution to a whole set of problems but it's unclear how it relates to the original goal of artificial intelligence which is to understand something about human intelligence by simulating it in a computer maybe our bcs friends will discover that there are in fact transformer mechanisms deeply buried in our brain but I would be surprised if that turned out to be exactly the case but perhaps there is something like that going on and so this leaves an interesting scientific conundrum of exactly what have we learned from this type of very very successful model building okay thank you [Applause] you 