 hello i'm tanda from vgg oxford recently contrastive learning is a very effective and popular method for self-supervised learning in the image domain moco and simclair are showing great classification performance on imagenet even approaching to the fully supervised networks these works use a type of contrastive loss called influenza e loss and in essence their task is instance discrimination however a key problem of instance discrimination is that there's no learning signals between visually dissimilar instances regardless of their semantics for example two different golf swing videos are always pushed apart this is not an ideal representation and we want to improve the representation in this aspect this work focuses on video representation learning video involves multiple modalities like appearance motion audio and narration the key idea of this work is that for the same semantic class dissimilar instances in one modality might be naturally similar in other modalities take this golf swing action for example in rgb space clothing and backgrounds are different but in the optical flow space they are naturally very similar it's also possible in the other way around that in optical flow space some samples are not similar but they are similar in rgb space this finding is universal in other modalities like rgb versus audio text or different color space in this work we choose rgb and optical flow to utilize this property we propose cochlear we co-chain two networks by alternatively using one space to help the other space in detail we prepare one rgb network and one optical flow network they are both pre-trained with influence e in their own space for one video we go to optical flow space use its flow feature to compare with other flow features and find the top k similar instances in the flow space then in rgb space we can use the top k instances as a positive set to train our congestive loss to let the contrastive loss support multiple positives we use multi-instance contrastive loss in this way optical flow space helps rgb space to find hard positives we can also swap the role of rgb and optical flow by using rgb space to find harder positives and change the optical flow network we can alternatively do this to improve both networks cochlear is trained on kinetics 400 and evaluated on the action classification task on ucf 101 it outperforms all other methods that only use visual information cochlea is also comparable with other methods that use audio or text and trend with much larger amount of data please check our paper for more details and results thank you very much 