 robert garros is someone who i believe needs no further introduction and despite loving a grad student um i've i feel very lucky i must have met robert in one of the vss probably two or three years ago right when you started working on your work with felix and matthias and robert is uh presenting from germany uh university of tubinging and uh he's going to be speaking about on the supervised similarities of super on the surprising similarities of supervised and self-supervised models robert please take it away thanks arturo and also a big shout out to all of the organizers of this workshop it's a fantastic lineup i'm super excited so yes i'll be talking about the similarities of supervised and self-supervised models and i'll start by explaining some of the motivation behind the excitement that surrounds uh self-supervised learning right now and i think part of that story is that supervised learning has quite a few problems so we know it's not particularly robust it suffers from shortcut learning it's clearly showing many aspects of non-human behavior it has a textured bias and it's incredibly label and data hungry so just standard plain um feed forward supervised learning has quite a few issues and the big hope is of course that sub-supervised learning might overcome some of these issues and make progress so the situation uh to draw scheme here is sort of um standard supervised cnns they're not particularly human-like they're not particularly robust humans on the other end they're quite robust and they're arguably also fairly human-like so the question really is how far is self-supervised learning going to get us into this intriguing direction and there are already quite a few super exciting studies that looked into some of these aspects we decided to look at this question from those behavioral perspectives so we'll be comparing human and sub-supervised and supervised behavior so the methods that we used was um essentially we took a bunch of existing data sets um carefully collected in the lab lots of servers we compared that to quite a few supervised torch vision models and also to a range of self-supervised contrastive models within those contrastive models we include sim clear for the purpose of plotting we're using a different color here because skin clears sometimes stands out but this is also just a self-supervised contrastive model and the paradigm that we used for collecting the human behavioral data was was quite simple we presented them with an image for brief uh presentation time we then had a one over f noise mask to limit the influence of recurrent processing and then humans had to choose one of those uh 16 categories over there and the same images were shown to cnn's and we also asked them to rate to pick one of these categories so um let's let's see some some results here the first and in total we looked at three different questions and the first one was uh noise robustness so just a question when we change certain aspects of an image how fast does recognition accuracy degrade with noise both for humans and for different types of cnns and just to give one example here that's just standard blurring when we increase the level of blur then at some point obviously the classification accuracy drops and it drops much faster much faster for supervised models than it drops for human observers so the big question now is obviously well where are some supervised models uh in in this range are we making any progress here and actually for this particular type of noise blurring we don't really see any um striking differences here um but obviously we're interested in more than just a single type of noise so we looked at many of these and not as a slide with lots of results i'm not going to go into many details here but essentially the pattern that we can see is that self-supervised models and standard supervised models they on most of those cases they agree fairly well except for those three that i'm highlighting here and and this is where the blue mole sim clear really seems to stand out and this is interesting because um these are some types of noise like uniform noise contrast high pass the one part of the training data augmentation is used for for sim clear training so this is some sort of um immersion finding here um so for moisture robustness it's kind of a mix story some are quite quite interesting others it's for other cases like low pass filtering it's rather more disappointing i would say if you're looking to increase robustness um the second aspect that we looked at was uh trying to go at a deeper level and go beyond aggregated scores like accuracy and that was error patterns so essentially two observers they can have 50 accuracy each but we're really interested in well are they finding the same images hard and and um the same image is difficult and this is what we um can look at here the intuition again is if two observers or a human observer in a cnn if they use the same strategy they should also make sarah make errors on the same individual images and this is what we can quantify using the error consistency metric essentially we're going to see some overlap in terms of making errors just by chance so this would be a corrected for chance would be zero if a value is zero that means um just random overlap and then if there's some systematic agreement here in terms of which images are easy and difficult then we would see higher values here and now we compare different groups for example human observers one human observer against other human observers and what we can see here is that that's actually quite quite a consistent agreement um in terms of finding the same images easy or difficult so one human observer agrees with other human observers also a supervised model agrees with most of the other supervised models and self-supervised models agree with other self-supervised mods now the interesting question again is well what about across groups how about humans versus other models for example so humans versus some standard supervised models actually don't don't really show much of an agreement here also humans versus self-supervised models not much of an agreement but and this is super interesting um and something that we haven't quite fully understood where this comes from supervised models and self-supervised models despite being trained in a completely different way they show an extremely high agreement they completely agree which images are easy and difficult and this might indicate that they might be using similar strategies uh despite being uh trained in a completely different way so this is um something i'm quite curious about and haven't fully understood yet and last but not least um we decided to look at shape versus texture bias this is the the last of the three experiments that we looked at are self-supervised models now that they are not trained with labels um are they going to be biased towards texture or shape and the idea is that in standard images you could use either feature to um and get high accuracies cats have cut shape cut texture so we can just go the other way around for the purpose of the experiment and create images that have conflicting shape and texture information so with this paradigm we now investigated humans supervised models and self-supervised models and if the response was cat in this example here this would be counted towards shapelines if it was texture would be counted towards texture bias and we know already that humans have a strong shape bias no surprise here and we also know already that all those uh 24 supervised models that they're on the texture bias side and now again the question is where where are these exciting new self-supervised contrastive models and actually they all seem to be on the texture bias side as well this includes self-supervised sinclair it's a bit more in the direction of a shape bias but not completely there so this is also an instance where we see a surprising similarity between supervised and self-sufficient learning and yeah just to to wrap this up what we've seen is that um self-supervised and supervised models they agree in the sense that they have a similar lack of robustness with the exception of sinclair which has some emerging benefits here of um of subsequence learning or perhaps of the particular data augmentations used during training we don't exactly know yet we see that um these groups make highly consistent errors much more than what can be expected but by a pure chance agreement alone and finally that they're all biased towards texture so right now we don't really see good models of human behavior at least for these particular types of data sets that we used here but we also think that this is just the very start of this um exciting self-supervised revolution that machine learning is currently undergoing so um the hope was that um self-suffice learning is going to get us closer to human perception and also closer to robust models and well empirically our first results seem to indicate that it's actually more more more similar to standard supervised models than we would have expected um again this is just for a particular type of sub-supervised models which is contrastive models and there's much more to come so this is just a snapshot in time not a definite conclusion and we already see some exciting uh trends for simply here which is more robust not particularly more more human-like even for those cases where sinclair showed superior noise robustness it was even way beyond human robustness so there wasn't wasn't um much progress in the direction of human-like but it's certainly an improvement in terms of noise robustness and and i guess there are many uh other interesting aspects that that one could look at and currently i would say we we have um many more questions than we have answers um there are still many things that we don't really understand about this so um why is it the case that supervised models and self-supervised models end up in such a similar space at least that's what our data seems to suggest there's no principal reason why this should be the case um or at least none that's that's apparent to me at the moment so there's definitely a lot that remains to be understood about this but um also we're just at the very start of this exciting supervised revolution and i believe there's much more to come and investigate and with that i'd like to give a big shout out to um my my colleagues collaborators and mentors here that's kandaracho benjamin matthias felix and willand and well thanks 