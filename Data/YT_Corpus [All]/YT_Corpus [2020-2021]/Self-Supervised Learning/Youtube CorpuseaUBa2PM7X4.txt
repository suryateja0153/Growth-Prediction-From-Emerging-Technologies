 yes so my name is Johan and together with my colleagues we have worked on uncertainty modeling and uh its application in diarization okay so why we are interested in this uncertainty modeling is that if we think of the typical diurisation process we take some recording and we split it into segments and then we extract the embeddings for each of them and we try to cluster them bending so that segments from the same speaker ends up in the same cluster and the embeddings they that we extract they depend on speaker identity but they also depend on factors such as noise duration or phonetic contact so it is a speaker embedding but it also has some additional randomness which we can call the uncertainty so we argue that this uncertainty is something that we should try to model and accurately try to incorporate in in our models so for example in direction that means that some uncertain embeddings um uh will affect the speaker model that results in the diversation process less than than the the certain embeddings so before we have already worked on this topic a bit and we tried to estimate uncertainty in embeddings based on a neural network and this neural network it took us input the the statistics from pooling layer in in embedding extractor and also the embedding itself we were for this for this task we we needed a kind of new training objective because we need something that is discriminative because the model is not a complete generative model and also we need something that trains the the model for uh open set speaker kind of recognition so we cannot really use multi-class cross-entropy or something like that so the the objective that we used was this that we can call the tuple-based objective where a training example are segments so i show there eight segments u1 to u8 and the task of the model is to cluster this so for example it may say that all the first three are from one speaker and the following five are from another speaker or it may say that uh segment four and six are from one speaker segment one and five is from another and so on so basically there are many many partitions like that from eight speakers you from from eight segments you can form around four thousand partitions so the task of the model is to say which of the partition is the correct one so this is the objective that we used in the past and in some of the work here as well and we use the embedding we cluster numbering just based on agglomerative hierarchical clustering so in the workshop we worked on incorporate the embedding uncertainty also in the vbx recipe or model and we try to improve the estimation of uncertainty and we also worked on a new kind of diversity method based on transformers so first regarding the incorporation of uncertainty in vbx so pbx is a probabilistic model based on hmm so you have for example now you are in the that you have some embedding and it's uh has been assigned to speaker one then we have enough probability that the next segment will be from another speaker or we can stay it could be the same speaker and so on right and the the speakers we have hidden variables in this model which represent the assignment of segments to speakers called said in that little equation there and this is what we are trying to find and we we do that by variational base in france uh so this is a model that has we have been used for quite some time and what we added here now is to eat it before we used it among other stuff for modeling x vectors as observations but now we want to model x vectors or some embedding along with some uncertainty so the the observations on our kind of probability distributions instead so we extended that and we did some experiment with x vectors where the uncertainty is estimated as in our previous work and we also did on i vectors where you have kind of from the model we have already a natural um you you obtain directly uh estimate of the uncertainty of the i vector so it's clear here first of all if we look at the upper part of the results x vector so these are without uncertainty used in the bbx and here we see that x vectors is quite a bit better than i vectors and this results here is actually quite good compared to some some results we have seen before so actually vbx can be is quite well performed quite well and we should maybe also notice that here is not this this method is not really designed to deal with uh overlapped speech as in in the moment although we we consider to to extend it for that maybe in future then if we look at the eye vector result for eye victor we see also that using the uncertainty helps quite a bit we can compare the two orange lines or the two blue lines so it does help however we could not get at this stage any improvement of by using the uncertainty of uh x vectors which is a bit surprising it's uh or i mean probably we have done some mistake or overlooked something because we had quite good improvements of using uncertainty estimation for ahc so this is still something that we would have to look more into later on so now uh yeah a second uh experiment uh with vb was to so the thing is that the the the result of the vbx depends on how you initialize the initialization you give to it so attempting approach there will be to do something like okay we select for example 20 random clustering hypothesis and we apply vb to each of them and then we check the score because the vb x also outputs some kind of score how how good the the results is let's say and we could select some based on this score the one that gives the best score but this never worked for us in the past but we see now that when we do include uncertainty estimation in in the embeddings we this approach actually works and this is nice because it suggests that we now have a better kind of more accurate modeling of the embeddings so now i'm leaving the bbx and just talk a little bit briefly on the how to estimate the uncertainty because i i said that we we estimated the uncertainty using a neural network which took as input the statistics from the pooling so that's and also the counts the zero order statistics and this is quite reasonable um in um in if because the the what comes out of the pooling is basically sample mean and sample standard deviation and the variance as probably we most know the variance of sample mean depends on the standard deviation of the input and the number of samples if we assume identical independent distributed data uh so actually the uncertainty and and then the embedding is just and and the same goes actually for the variance of sample variance it's just a little bit more complicated formula so in principle the since the embedding is just a fine transformation of this pooling output we know really the uncertainty related to duration based on this so it's actually quite reasonable to do what we did in order to estimate the uncertainty coming from from the fact that we are using finite duration segments however uh it's maybe not so reasonable to assume that uh uncertainty coming from noise or phonetic content and so on uh could be estimated based on this input because for example very noisy data tends to make mfcc more identical which would uh make the the second order statistics in pooling layer um quite small which would actually suggest a low uncertainty in so here instead we started to look to do something to to estimate uncertainty using something similar to how we actually produce the embedding so we have some convolutional neural network and the framewise output is then eventually forced to be positive because we think that it should represent the amount of useful information in the frame and finally we do some summation over these frames um to to get what we call the precision which would be the inverse of the uncertainty um this we actually unfortunately we don't have this to give any improvement we didn't try it in combination with the previous method but on its own it could we haven't managed to improve on what we had before but uh i still think that we should get something from it with a bit more experimentation uh we'd actually just started this so and finally let me go also to the direction we did using transformers and this is a a work inspired by a self-fundation mechanism used in diarization in some some other diarization works and also some self-supervised works so we use a self-supervised method called parser to extract features basically and these are features that represent basically it's a very generic representation of speech data and then we applied a diversation we did the air station by using kind of multi-layer self attention mechanism so for the passer that one process uh quite uh um like high like very short segments and uh we will you know normally and we in addition applied uh on the output of that one uh convolutional net neural network with some reasonably high strides to kind of uh get a little bit fewer frames or fewer segments let's say representations per second so in the end we had something like five uh frames per second which will be the inputs to the transformer network and this is a transformer network where we at least in the final version shared the projections between the keys and queries and the similarity between the keys and queries was measured by cosine distance and finally uh this network um let me go to the next one so finally the transformer on the final layer we compare all the embeddings with dot product and therefore we get a kind of similarity matrix um for each pair of segments and this one we will apply binary cross entropy as objective in training where we say pro it should output the probability one if it's the same if the same speaker exists in the two segments uh there could be more than one speaker but if the same speaker is there it should be one otherwise zero and in testing we do clustering based on this infinity matrix and we also had yeah a multi-head version of this so this here in these experiments we were looking as baseline as using the output from the um passer plus this convolutional network for for re reducing the frame rate and that alone gave something like 46 percent uh or 46 with overlap of 41 with no overlap and with the best self-attention or transform based model we get around 29 or 20 percent uh therazation error rate so basically this means that what the transformer does it's that it's kind of refines the embeddings in each layer so each layer it becomes a little bit more representative for the speaker identity uh yeah and what we will continue with is more analysis and and work on these methods because a lot of these things have just started uh and we are also thinking much about other training objectives it's something that we kind of studied or discussed quite a lot in the during the workshop but we didn't yet conclude what what really is the best thing to do there and we studied until now uh mainly methods that do not deal with overlap speech it has not been our primary focus but we will extend also our methods to deal with this in future work so that was all from me thank you for that for the talk uh i think given the time i think if people uh audience have time uh probably they can ask in the chat and you can answer uh from uh for the chat yes of course yes so johan there's a question in the chat from prachi she wants a bit more detail about don merch if you could answer it then that'd be great uh sorry yeah i thought that's basically so i mentioned that one thing was the using let's say 20 random initializations down burning merging we also have several initializations but they are done in the way uh if i say it's wrong so i hope that someone corrects me but we do the ahc in the normal way and then when we at some point stop the clustering because of some threshold we will continue to check uh that will be our first initialization and then we check all the following uh clusterings if we do one more clustering step take that as output another one another one until everything is is clustered and that so so we use each of those outputs as as initialization please continue in the q a box yeah yeah yeah okay then let's welcome and thanks thanks again for the great talk and let's we'll come now to our last uh presentation from uh roshan shama 