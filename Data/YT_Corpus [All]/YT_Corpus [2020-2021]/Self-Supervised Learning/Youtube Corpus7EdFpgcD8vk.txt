 good morning good evening good afternoon depending on where you're joining us in the world welcome to nano explorations our twice weekly celebration and conversation and engagement with the phenomenal research of our students that's happening at mit as part of mit nano and across the campus so it's my great pleasure to introduce dianna wolfe who just completed her master's in the department of electrical engineering and computer science she's graciously joined us to share her work in in fast and energy efficient monocular depth estimation on embedded systems diana thank you for your time today and i look forward to hearing about your work and i'll i'll turn it over to you and participants please save your questions and on where you can raise your hand or put your questions into the chat with that diana it's all yours hello everyone my name is diana and today i'll be presenting about my work on fast and energy efficient monocular depth estimation on embedded systems this work was supervised by my advisor professor vivian z and was done in collaboration with feng chung ma daenery yang and professor sertas karman i'd like to start out by highlighting several different applications of depth sensing and active area of research in computer vision this is a key step in autonomous navigation for example of drones or small robots that could be deployed for search and rescue or disaster relief missions depth sensing is similarly important in navigation systems within self-driving cars more recently depth sensing has also seen applications in biometric security with three-dimensional depth scans of facial features being used to unlock devices like phones and computers there are several different methods of depth sensing and they can be classified in these groups we have stereo vision where depth information is extracted by comparing object locations and a pair of images taken by two cameras that are separated by a fixed length we also have time of flight cameras where depth estimates are measured based on the round trip time of an emitted and reflected signal pulse an example is lidar commonly used in self-driving car technology this sensor scans environments with laser signals lastly we have structured light cameras that project light patterns onto surfaces to produce three-dimensional maps of them many of these sensors face limitations in their range and resolution and tend to be bulky with high power consumption this has motivated research into monocular depth estimation where a dense depth map is determined from just a single camera image this is appealing due to the ubiquity and the low cost and size of rgb cameras let's narrow down now to one particular application of depth sensing autonomous navigation of low power miniature robots a camera onboard one of these robots would take an image and from this image a depth map is generated the pixel-wise depth estimates can then be used for obstacle detection algorithms and motion planning the challenge here is ensuring that depth estimation is accurate and energy efficient if the algorithm is inaccurate or too slow the robot will misinterpret its environment and crash into an obstacle if the algorithm is too power hungry the robot may simply run out of energy before getting to its destination firework investigates deep learning-based methods for monocular depth estimation as many state-of-the-art approaches are learning based these algorithms use deep neural networks to estimate depth for every pixel in an input image depth estimation neural networks typically consist of an encoder decoder type structure that i will describe in more detail shortly when deploying our depth estimation neural network we target several embedded platforms including the nvidia jetson tx2 cpu and gpu the apple iphone and an embedded fpga part of the ultra 96 system on chip let us now explore the structure of a depth estimation neural network in more detail the network consists of two smaller sub-networks an encoder and a decoder the job of the encoder is to extract information from the input rgb image for instance features like corners and edges this is very similar to what is done by image classification networks the low resolution features from the encoder are then directly fed into the decoder where they are gradually up sampled and merged together to perform a higher resolution depth map output the encoder and the decoder may be linked even further through skip connections these pass additional feature details from the encoder to help improve the clarity of the feature maps that are processed within the decoder this often leads to a sharper depth output there are different ways in which features passed along these skip connections are merged within the decoder for example the features may be concatenated resulting in a larger feature map or they may be added pixel wise maintaining the feature map size in existing depth estimation neural networks the encoder is often based on an image classification network that may be deep with numerous layers decoders are often custom designed and tend to consist of complex building blocks for instance an individual building block may contain multiple convolutional layers which will increase the decoder size and complexity as more of these building blocks are added a larger number of layers in the encoder in the decoder allows state-of-the-art models to achieve high accuracy rates in depth estimation however these models are also too large and too computationally heavy for real-time inference on devices with resource constraints the most common type of layer found in depth estimation neural networks is a convolutional layer that performs two-dimensional convolution on an input feature map each such layer contains a number of filters dependent on the number of feature map channels processed by that layer in a standard convolutional layer if there are c input channels and m output channels the layer computation involves c times m filters the height and width dimensions of the filters are typically a small odd number such as 3x3 filter or a 5x5 filter efforts to design more compact deep neural networks have included developing computationally simpler convolutional layers one example is the depthwise separable layer which splits the standard convolution into two a depth-wise convolution that operates on a channel by channel basis this convolution uses filters that have a height and a width of k and are a single channel each and a point wise convolution that scales and sums pixels values across channels this convolution uses filters that are multi-channel but they have a height and a width of 1. the net effect of this depth wise decomposition is the reduction in parameters due to there being less filters to train the resulting deep neural network is less computationally costly the depth-wise separable convolutional layer was introduced as part of the mobilenet family of image classification networks and this layer is heavily incorporated throughout our own depth estimation neural network our depth estimation dnn is shown here the input here is a three channel red green and blue image of resolution 224 by 224 pixels the output is a single channel dense depth map of the same resolution to 24x224 we adopt an encoder decoder structure with low latency designs for both our encoder shown in blue is based on a compact and computationally simple mobile net network that utilizes depthwise separable layers our decoder shown here in yellow consists of five cascading up sample layers each of these up sample layers performs a single 5x5 convolution that is followed by nearest neighbor's interpolation this decoder design differs from complex decoder designs in previous works that may incorporate multiple convolutions per up sample layer furthermore we perform convolution before interpolation as opposed to after this allows us to reduce computation in each upsample layer by a factor of four the bar chart on the bottom shows the progression of runtime improvement that we observe in our design the runtimes shown here are in milli seconds on the jetson tx2 gpu our baseline here was selected to be a model achieving state-of-the-art accuracy rates on monocular adapt estimation at the time of this works publication this baseline model consists of a deeper encoder network based on resnet50 paired with a more complex decoder using three convolutions for up sample layer the decoder dominates this model's runtime and does not allow for real-time inference in comparison our model shown to the right achieves a reduction in both encoder and decoder runtime we further modified the decoder to use to utilize depth by separable convolutions in every upsample layer this allows us to roughly half the model runtime and in addition the network is now balanced in that neither the decoder nor the encoder dominated the runtime to the bottom left we show a visualization of depth estimation performed with our model the rgb input samples shown here come from the nyu.v2 dataset which we used to train our fast depth dnn this dataset is comprised of video sequences of indoor scenes with depth measurements being on the order of several meters we incorporate skip connections between the encoder and the decoder to help sharpen the depth map output and improve our model accuracy we use additive skip connections where we add the connected feature map channels instead of concatenating them this lets us avoid incurring a computation overhead keeping our model model runtime the same as we now see on the bottom left with skip connections our model produces sharper depth maps that covers the neural network design of fast depth there are two more steps that we take in deploying fast depth on the jetson tx2 one of these is hardware specific compilation this is motivated by inefficient performance of depth twice separable layers on embedded devices which is due to poorly optimized low-level operators in deep learning frameworks this has spurred considerable research in compilers for dnns that tune operators for specific hardware targets in our work we use the tvm compiler stack to optimize fast depth for the jetson tx2 the compiler can be understood in two parts the front end is what interfaces directly with the dnn model definition and it transforms the model into a computation graph this graph simply represents dependencies between operations and is hardware agnostic the compiler backend takes this computation graph and performs hardware specific optimizations these can aim to reduce memory access hide memory latency increase parallelism all with the overall goal of reducing model latency at this stage empirical measurements done on actual hardware may be taken into account to help guide the optimization process compilation outputs an executable that is tuned specifically for fast runtime on a given hardware target in our case that target is the tx2 gpu or cpu on the bottom right we now see that with hardware specific compilation our model runtime is half even further the last step in our design process is dnn simplification through pruning this is motivated by trained deep learning models being often parameter over parametrized and containing operations that may be redundant pruning algorithms identify these redundant operations and remove them without noticeably impacting model accuracy in our work we applied the net adapting algorithm as a final simplification of our fast depth network and the chart shown mid slide the gray outline depicts the original network shape while the colored bars show the effect of pruning on individual layers in the network we note that the size of the layers denoted by the number of input channels to them is greatly reduced for many of the layers in the middle of the network this means that the filter size for these layers is also shrunk resulting in less computation and lower runtime overall as a result of this simplification our model runtime is further reduced by 1.5 times and our total inference runtime is just 5.6 milliseconds per image on the tx2 gpu pruning has a negligible impact on our model's accuracy and the visual clarity of the output is maintained if not improved when deploying on jetson tx2 we compare against several prior works that are listed in the accuracy versus frame rate plot here the quantitative accuracy metric used here is the delta 1 metric which measures the percentage of pixels in the predicted depth output where the relative depth error falls within 25 percent of brown truth the higher the delta 1 accuracy the better the y-axis here reports delta-1 accuracy the x-axis reports frame rate on the tx2 gpu the desired region is the upper right hand corner denoting high accuracy and higher frame rate which directly implies low latency since we run inference on just one image at a time our model located far to the right of all other listed models achieves comparable accuracy with over an order magnitude speed up in inference runtime here we show a video sequence of depth estimations performed by our model in comparison with ground truth and with baseline this baseline corresponds to the model denoted by the pink dot in the upper left hand corner of the accuracy versus trade-off claw lastly we report the power consumption measured on the tx2 we measure power at inference time on both the gpu as well as on the cpu in isolation on both platforms total power consumption ranges around 10 to 12 watts with active power consumption of 7 to 8 watts this trim demonstrates that with our fast depth dnn we successfully achieve real-time depth inference at under 10 watts of active power we additionally deploy our model on an apple iphone we target two iphone devices the iphone 6s and the iphone 10 that features dedicated neural network hardware called the apple neural engine on both of these devices our fast depth model achieves real-time inference rates with frame rates of up to 25 frames per second on the older iphone 6s and up to 40 frames per second on the newer iphone 10. we ran our app with the fasttop model live at the conference menu at ecru 2019 where we originally presented this work a clip of the recorded video is shown here the conference venue featured several indoor scenes with barriers walls and people all serving as obstacles a longer demo video is available at our faststepped project website we're additionally interested in deploying fast step on an embedded fpga one advantage of this step is the flexibility to design specialized hardware to run layers in the fast depth dnn specialized hardware is more likely to achieve higher energy efficiency when compared to more general purpose hardware like cpus another advantage of this step is that it allows for joint co-design and optimization of algorithm and hardware in our approach when designing an accelerator for fafsa we have the ability to revisit and modify the original dnn to better utilize our custom hardware in designing an accelerator for the fast depth neural network a prominent design consideration is that of the recon it was that of reconfirm reconfigurability versus performance for instance consider again the structure of fast depth the encoder primarily uses filters of size 3x3 while the decoder uses filters of size five by five if we design an accelerator that is dedicated for three by three convolutions how do we run the five by five ones on it making the hardware reconfigurable would incur area and energy costs designing a separate accelerator for five by five convolutions would also incur area and energy costs in our approach we actually revisit the fast step dnn model definition itself to see if we can make any modifications there to address this design challenge we'll recall that in the decoder every up-sample layer consists of a 5x5 convolution followed by nearest neighbor interpolation however this means that the five by five convolution in each subsequent upsample error will have its 5x5 convolution following the previous interpolation as a result there is some structure to this convolution that we can exploit suppose an interpolated feature map is fed into the 5x5 convolution of a subsequent layer as shown here if we superimpose a 5x5 filter from the convolution over the interpolated feature map we know that the four filter values in the upper right hand corner all apply to the same pixel value in this case a green pixel value this means that in the convolution all four of these filter values would get multiplied by the same pixel value instead of performing formal multiplications and then summing them we could first add the filter values and multiply once by the green pixel value we can apply a similar step to the rest of the 5x5 filter the windows of filter values that could be pre-added are shown in red here by pre-adding these filter values we can actually generate a smaller 3x3 kernel in the end we discovered that the 5x5 convolution can be decomposed into a set of smaller 3x3 convolutions this insight allows us to slightly alter our original fast depth dnn so that this decomposition can be better exploited in our resulting model every layer in the encoder and the decoder can run natively on an accelerator that is dedicated just for 3x3 convolutions this was one example of a design consideration about the trade-offs between reconfigurability and performance another example concerns how the depth-wise separable layers are themselves run on the accelerator recall that a depthwise separable layer consists of two operations a depth-wise convolution and a pointwise convolution these two convolutions have different computation characteristics again we must decide whether implementing reconfigurability to handle this is advantageous or not while i do not have enough time to discuss this more in today's talk if you're interested in learning more we go into greater detail about the design process in my thesis on this work i'd like to conclude by highlighting two key takeaways from this talk and from our work on fasta one takeaway is that in depth estimation dnn design complexity is not necessarily better simplicity is not necessarily worse we observe in our efforts to design a low latency depth estimation dnn then deeper networks with complex building blocks may achieve high accuracy but at the cost of high model agency simplifying network designs either through simpler layers or through pruning or through both will result in faster models while lowering accuracy but the accuracy degradation may be far outweighed by speed up gains we observe with fast depth that we are able to achieve an order of magnitude speed up with accuracy still close to state-of-the-art rates another takeaway is that hardware and loop optimizations are critical in meeting target goals examples include latency power consumption and utilization we observe this with hardware-specific compilation that incorporates empirical measurements to guide its low-level optimization process we observe this again when designing hardware for our fast accelerator where our hardware design choices inform the modifications that we make to our uh fast step algorithm optimizations that take hardware into account therefore prove critical for successful deployment on our embedded platforms the evaluation code and the trained models from our fafsa fork are made available at our project website listed here i would like to thank analog devices intel and the national science foundation for funding this work and thank you to all for attending and listening to my talk diana thank you very much i sent you the the the physical and the the virtual uh virtual clapping so thank you for the the very interesting talk uh so participants please raise your hand or excuse me sunday chat daniel let me let me lead off with a question though um first a point of clarification um i think you made the statement that you're doing your estimation on a per frame basis yes okay so what can you comment a little bit on what is the opportunity to use frame adjacency information so that if you have depth information from frame one and then you get frame two how do you use that frame to is there a way to efficiently use that frame frame one as a prior to accelerate even further the estimation of depth and frame two uh so there is research on this uh we have not incorporated this in our work there is research on using adjacent frames and a video sequence uh to not only uh increase like how much frames you're processing in the neural network at once like increasing the batch size for instance but also to improve the accuracy of the depth output um there's also a matter of temporal consistency where you want depth in every frame to kind of correlate with some sort of scale to maintain scale between different frames so there is work on this uh i have not incorporated this in in the fast of the work that we published at the time though okay thank you for that uh participants please feel free to send me a chat to public or to raise your hand um i do have a follow-up question to my last one as as participants um sort of get their heads wrapped around what they would like to ask um related maybe to the sort of video sequences um and recognizing that this isn't what you had focused on um yeah i think you said you looked at up to 140 or 170 frames per second yes um so it what is the sort of the balance between the energy consumption associated with higher frame rates um and the computation that to keep up with those frame rates so there's energy required to record fast imagery you know is there a trade-off if you will where um you know by the increasing power of your higher speed acquisition starts to become a detriment um to overall efficiency versus using that time sequence sorry guys uh yeah i understand your question so the the um consideration here is that do you actually need 178 frames per second and if there's an application that requires that the model will support that high frame rate and it will run at the power consumption that we observed so around 8 watts of active power consumption if you do not need 178 frames per second well you're not going to be constantly running the model then you don't necessarily need to you can run the model uh for a smaller amount of time necessary to support whatever frame rate you're targeting and then you're saving on on power and energy consumption that you can then allocate to other tasks that are being performed on the computing platform so if this would be part of a navigation system there would also be other algorithms running alongside depth estimation and so by supporting a higher frame rate this enables us to lower to use a lower frame rate if we need to and instead of offload i'll reallocate the power and energy resources that we have for other algorithms very good thank you um there's a question here from alex um have you quantified the cost saving between using a stereo setup with a maybe a simpler cnn or dnn or a monocular setup with your current architecture we have not explored stereo stereo vision in comparison with monocular for purposes of our work we narrowed down into the monocular domain space and compared against work in that field there have been works done on uh stereo approaches but we have not directly compared against those very good uh from santosh could you comment on the maximum range of accurate depth measurements observed with the model uh yes so with our particular model we train it on the nyu data set where all the scenes have depth measurements that are on the range of several meters so as the model was trained it will work pretty much on indoor scenes where depth is on the order of let's say 10 meters um it does not really work well outside because your depth range is outside or on the order of tens maybe hundreds of meters uh in that case you could explore either retraining the network on a different data set that's more representative of that environment uh or use some transfer learning to again uh bring in like re have the have the model kind of learn more about different different scales of depth but as it is this model was trained and works on indoor scenes thank you uh there's a question uh chatted to me directly i just saw one coming in from cedric but let me ask this one first um if you why not apply these energy efficient techniques on systems that don't require the energy efficiency so if you were to look at a non-mobile system is there something to be learned in terms of of maybe having multiple instances of a very energy efficient uh algorithm in situations where you're not as power constrained yeah i'm sorry i don't i didn't i'm just fully understand the question so so if you take an energy efficient algorithm that's designed to work on a mobile platform and instead run it on a non-mobile platform or is there a benefit to looking at why isn't everything energy efficient how can we sort of extend the energy efficiency to cloud-based or other server-based instances where you're trying to learn but why not be energy efficient in those cases well for for large like data centers um i mean there's there are benefits of energy efficient design because that allows you like you mentioned to increase the number of instances you you run in parallel which increases parallelism and and throughput and at the end of the day lets you compute more um i guess in our work we we kind of focused on the reverse position where it's much harder to take an algorithm that was let's say designed to run in real time on a server on a large-scale server but it's much more difficult to fit that and run that in real time on a small device now if you have something that you've designed for energy efficiency on a small device if you were to run that on a larger scale machine that just naturally gives you an increase in how much like if you if you have the ability to instantiate it multiple times if that answers the question i i think so and i think we have some related questions also looking at implementation i guess um cedric asks could you estimate how hard or long it will be to optimize your algorithms so they run on smaller microcontrollers such as in the openmv project cortex m4 or other platforms i guess would be the broader question uh yes so i mean you would always start out with training your own neural network on some representative data set uh and the steps you take into into deploying it on a particular hardware target uh or i guess also depend on on what that hardware is if it's uh like a standard defined cpu or gpu architecture i mean there is ongoing work on different compilers that are supporting an increasing number of these hardware targets and so you would want to compile specifically for whatever hardware target you you you're using and that can take on the order of i mean in our in our project it took on the order of several hours to maybe a day um but that's also dependent on the size of your network and how many layers there are in your network if you if you are targeting more custom hardware well now you have the freedom to design your own custom accelerator and that design process might take much longer like the order of weeks or months to develop and implement thank you i'll ask the the last related question here from uh anarid um thanks for the impressive presentation they say how does the the fast depth network work with domain data set transfer apart from the nyu data set keeping in mind the trade-off between network generalizability and hardware reconfigurability right so network generalizability is is the challenge um it was the challenge that we kind of it was something we left faster on and subsequent work on this project might address this uh we have tried running fast depth on images from other data sets but obviously the performance is is a part because if you if you apply transfer learning and retrain it on those data sets you you'll have much better model accuracy results uh we have not actually tried um like we have not tried transfer learning with fast step that would be one of the next steps to make that steps more generalizable very good thank you diana let me just double check make sure i'm not missing somebody with a hand raised no i'm not okay so i think with that diamond thank you very much uh for the wonderful presentation and work and for the the questions and answers i think the participants really enjoyed probing a little bit deeper here so thank you again and look forward to hearing more about what what your next steps uh are for your career and your in your work um so again everybody thank you for your attendance uh and participation and questions uh do join us next week same time same channel on tuesday at 11 a.m we'll hear from michael walsh um on solid state platform for boston quantum network again nano exploration is an opportunity to engage with our phenomenal students and we look forward to your continued participation and diana again thank you for your time and questions today and everybody stay safe and see you next time thank you thank you 