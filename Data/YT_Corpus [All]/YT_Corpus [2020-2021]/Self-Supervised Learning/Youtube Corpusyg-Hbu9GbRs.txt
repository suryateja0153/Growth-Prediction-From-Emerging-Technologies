 you [Music] all right thank you everybody for coming that for those that are online I'll be monitoring the the websites of you have any questions please feel free to ask it there it's my pleasure to introduce our Wyndham JD he is from University of Southern California from school of engineering he joined us this summer for his three-month internship and this is his final talk on audio retrieval Thank You Dimitra for introducing so I'm more in demand as my today's topic is supervised de Fasching for efficient audio retrieval but before we start the meter already introduced me but just to let you know that I am from this signal analysis and interpretation laboratory laboratory sail and my advisors name is Professor shreekant Narellan or in short three and so this is our today's agenda so I will start with the basic definition of audio event detection and classification and kind of tell you some some really nice applications then I will move to audio retrieval and ranking and I will say why that is important then we will do some literature survey about retrieval and ranking systems then I will move to efficient audio retrieval with hashing where I will introduce some algorithms some of them are supervised some of them are unsupervised then we will see the experimental setting the results and the conclusions so let's start with audio event detection and classification generally it is defined as the human-like ability to identify and relate sounds from audio generally audio events are annotated by humans and when during annotation process the annotators provide semantic level for the sound events whether it's a violin whether it's a bark or whether it's a sound of an engine and it is a common practice to follow an ontology or hierarchy information during the annotation because it's kind of easier to annotate the course class first then the fine grained class for example if I have a sound of things I can first identify okay it sounds like some engine then I can go into like okay this is a particular motor or something now this is a very high-level overview of of an audio event classification system using deep neural network so generally we have some a pool of audio events and and their labels for example here we have three audio files with bark alarm and car engine and we extract some kind of features like spectrogram and then feed them into a neural network and try to predict the human animated levels this is the this is the simple overview of a training or learning process now after the training is done we save the model and we can use the pre trained model for some inference tasks or some testing one of the very simple testing would be we haven't query audio and we try to predict its level for example in for this audio we can predict some some confidence scores like wait it's like 95% confidence it's a bark and 4% confident it's a cough like that we can also do another kind of inference task and which is relevant to our applications here and this is called this is something that we call extracting feature or extracting embedding for a new audio file or any audio file so after the model is trained we can have any audio file and we can pass it to the pre trained model and we can extract output of a of an intermediate layer and use that as a embedding or distinctive feature representation of that audio file since this model was trained to classify different audio events this embedding should have features which are distinctive to different audio events so there are some open source models like that one of them is Google's vgg SH model and we can use this pretend model for our specific applications okay so this is the basic pipeline now we can go over some really some real-life applications of audio event detection and classification and one of them is definitely the accessibility feature it gives and we all know that Microsoft soundscape is a product from our team and also there is a project going on called hearing AI Mariah is working on that so the basic idea is to provide some kind of accessibility and listen to the environment and try to identify different audio events and somehow let the user know that what audio events are going on in the environment another application would be autonomous driving where the self-driving car can detect different sounds and for example some emergency vehicle and kind of takes some decisions based on that and also smart cities crime prevention smart homes we have applications of odd-even detection there and this is like more relevant to our applique and it's like audio content understanding and retrieval this might have like sub applications and one of them is like browsing so for example in Bing search we have this search by image where we can upload an image of coffee and we can get two things one is it's it take text levels that it says that it's a coffee and that's like image recognition and another is like a bunch of other pictures which are similar so we can kind of think about similar applications with audio where we can have a query audio and try to find which audio it is and try to find other similar audio or sound one more applications of retrieval is creating multimedia for example games or movies so this is like one scene of a grain and now when the game is designed the that particular scene might have requirement of some audio for example here we have we might have sounds from birds some some footsteps and when sounding engineers add these sounds they generally understand the scene and search in an audio database and put a listen to the audio and put a put a suitable sound into the content now this searching can be can be partially or fully made automatic if we have a good audio retrieval system with using text query or even audio query we can make this process partially or fully automatic so this kind of tells us that audio retrieval is important and making it fast for search like real-time fast is is also really important and that's the whole motivation of this work so before like starting audio retrieval I would just like to give a overview of the research areas in audio event detection so that we can see why we lie Before we jump into that so one of the important research area is finding better feature representations of the audio and with powerful machine learning models especially DNS and this is kind of well explored it's still being explored but we we have some open source models like VG jesh or all the steel wick that can give you powerful audio deep presentations another research area is hierarchical audio events so as I said earlier audio events generally lie in a hierarchy and it would be really nice if the model can back off to a coarser class when it fails because or because models sometimes fail to detect the very fine-grained class then it might have the option to back off so there are some past work on this and those are on by level hierarchy but the like for any arbitrary hierarchy it has not been explored yet the third applications which is relevant here is efficient retrieval and ranking and to the best of our knowledge it has not been well explored so let's see what is what are the challenges for efficient retrieval first of all there are millions of sounds almost like infinite amount infinite number of audio events available in the internet so possibly our database will have millions of sounds as well and there is no established meaning of distance between two sound files so what people generally do they get some kind of high dimensional feature representations of the of the audio events using some up using any of these like vgg SH or TL week and then try to compute distance between audio events now the problem is that since these are high dimensionals and since we have millions of sound this distance computation become very expensive so that's where the that's where we it I mean the need arises to create a very efficient system for retrieval okay so so let's let's jump on that audio retrieval and ranking and this is a high-level view of a pipeline of audio retrieval system and we have a query audio and what we get we get some kind of fixed dimensional representation like vgg SH and we also have a database which might have millions of audio events but we store the fixed dimensional embeddings for the database now before the query audio we what we do we basically compute distance between this query audio and all of these audio files in the database so we do some kind of exhaustive distance computation and then based on the distance values we can rank the retrieval results so if we kind of want to see where where the maximum amount of computation is being done that's basically in the in the exhaustive distance computation part so what we will do we will try to optimize this part as like as much as we can and this can be done using some kind of hashing or in in more specifically some kind of quantization so the basic idea about hashing or quantization is that instead of storing the floating-point audio embeddings we stores some kind of hash codes some kind of binary codes or in quantizes in term some kind of discrete values that will help us fast to make this process fast the distance computation okay so based on this overall pipeline we can kind of formulate our problem so our goal is to come up with a efficient distance computation and instead of like searching exhaustively in the database so that we can get very accurate result what we will do we will do a trade off so we will not get the exact neighbors we will get the approximate neighbors but because of doing because of that we will we will be able to do it very fast so this whole thing is called approximate nearest neighbor search and a very simple example will be some kind of quantization so let's say we have a bunch of samples here and what we can do we can just discretize them so let's say we discretize them to six centroids or something some kind of clustering so since we have like after we have discrete values we can make the distance computation very fast what we can do when a query Y comes we first assign them to a closest centroid and then when we assign them to the closest centroid the exhaustive distance computation becomes computing distance between these six centroids so since everything is discretized quantized so this exhaust this like floating-point distance computation can be done now by a lookup table because this centroid is fixed during the training process and we can compute the PRI pairwise distances and save them in a lookup table so d-ring query we can just find that from the lookup table okay now this is a very simple example of doing that via quantization but we will use some sophisticated algorithms that I will come to those algorithms later but in general we can divide the algorithms into two clusters one of them are unsupervised that means they don't need any labels of the audio human annotated levels they don't need so that's the that's the that's a good part but the con is that since it cannot incorporate human knowledge it it might not retain the data similarity or the data pattern in the hash codes so there is another types of algorithm that super versed so it needs data data labels audio levels annotated by humans so so the benefit is that it can preserve the data pattern or the similarity between similar audio events and distribu lera T between these different audio events in the hash codes eventually which which will help us to get better retrieval accuracy but the problem is that we need annotators yeah so based on these like two types of algorithms we can do little overview of the literature that is available so the first one is unsupervised one and it's called product quantization and it's one of the state-of-the-art quantization or hashing algorithm so it is inspired by vector quantization and it is it basically improves vector quantization by a simple divide and conquer approach the rest of the three algorithms there super fast algorithms the first one is called CN n H and as you can see it's it is it kind of does the hash coding and the feature learning things in two different stages and then combines them together later the next one is called D nn H and it basically improved version of C n NH where they they do the feature learning and hash coding together and jointly learn the model last one is the one we will use and they have two different variants they are called deep quantization Network D queuing and they they also jointly learn the levels the the data pattern and the hash codes but they also have a formal control over the quantization error they like they literally minimize the quantization error during the training process so we will use this method and the product quantization and I will discuss them in detail later ok so let's move to efficient audio retrieval with hashing and kind of introduced the unsupervised algorithm so let let's discuss about some preliminary so why the exhaustive distance computation is so expensive so we want to compute nearest neighbor using Euclidean distance and let's let's assume we have n samples in the database and D is our feature dimension and what we want to do we want to find we have a query X and we want to find its nearest neighbors so what we have to do we have to calculate distance of X between all these samples and sort them so that's basically the minimum distance of x and y I so if we take an example so before going to that so the complexity is order of nd because we have D dimensional feature and we have n samples in the database now if we see an example where n is the number of samples which can be a million and D can be thousand that's very standard so the complexity becomes order of a billion because it's n D and that's that's a problematic thing so that's where we want to focus we want to reduce this complexity of searching and we will see so this phenomenon is also known as like carts of dimensionality in machine learning community and after the hashing algorithm we will see that we can achieve around order of 9 million computation so from 1 billion to order of 9 million computation ok so let's do some more preliminary since product quantization which is the algorithm we will use it comes it's kind of inspired from vector quantization so we just want to kind of review vector quantization first so in vector quantization our core goal is to convert every feature into some kind of code word some kind of hash codes and the method we use is very simple it's basically k-means algorithm in the step 1 so we have d dimensional features and we do k-means clustering here let's say K is eight so we have eight clusters after the algorithm in the second step what we will do we will quantize every sample so we'll assign every sample or will approximate every sample with their closest centroid so all the samples will be approximated by Tate centroids now so if we have K centroids for example eight centroids we can just represent those eight centroids by like number zero to seven so for example sample one might belong to the fourth cluster so we will denote sample 1 by 4 that's 1 0 0 that needs 3 bits so if we have K centroids in vector quantization we needs log of K base 2 bits to represent the hash codes so our hex codes will be log of K bits okay so based on this simple idea of vector quantization we can move to product quantization the main problem of vector quantization is that it cannot support exponentially large number of code words so what I mean is that since since we need like log of K bits so if we need 64 bits codes K will become 2 to the power 64 and that that's a that's a very large number and K means algorithm can become intractable in that situation so because of this problem of not supporting a large number of centroids what we do we do simple divide and conquer so we have original feature D and we divided them into M sub spaces here M is 3 so just linearly divide them into M sub spaces and so each of this sub space has d by M dimensional feature and we do independent vector quantization in each of the sub space and then we will somehow combine the combined the results together so we have d dimensional feature we first divide and do independent vector quantization now when your query audio comes here after the product quantitation algorithm is done so it it go it first it comes here then it kind of gets divided into different subspaces and in each subspaces that might get a particular cluster so if I have eight clusters here eight clusters here eight clusters here's we we can like choose any of them from from each of the subspace so in general the total number of code words or centroids it can be 8 cross 8 cross 8 so it's like Cartesian product of the all the code words that's why it's known as product quantization so just to kind of keep in mind so we have M subspaces here and each of dimension D subspace which is basically D by M and K a subspace is the number of centroids in each subspace typically is it's like 256 or even lesser than that 128 so these are the two terms that will come D subspace and K subspace so the effective code boots code book size becomes a Cartesian product of all the code books because we can choose one from there and one from there and one from there so the size of the effective code book become K subspace to the power M and that's where it supports exponential number of exponentially large number of code words so just after like keeping this in mind we can kind of see the bigger picture where we are moving from Euclidean to VQ vector quantization to product quantization and then eventually in DQ n so we can get like see them column column wise so first in Euclidean for Euclidean computation we already saw that the it's like the expenses like order of nd and the process it's the most accurate because it does exact issues computation but the con is that it expensive in VQ we saw since we discretize everything and ultimately distance computation becomes computing distance between the query and the center's so it's big becomes order of KD where the number of centers is number of code words is K so it's very simple to implement it's very simple algorithm but the problem is it cannot support exponentially large number of code words and if it does not support lots of code words eventually the quantization error will be high so we moved to product quantization and it can be proved or it can be seen in the paper that the exhaustive distance computation is order of n m+ k subspace d n is the number of samples in the database M is number of subspaces k is number of centers in every subspace like 256 and D is the feature dimension so if we can see we can see that n is not multiplied with D here so that's why we are gaining and if we want to compare with VQ it can support a lot lot of like like lot more code words than VQ now just to go to yeah first what is the number of effective words okay to the end still a.m. yeah yes so that's a good question because so here let's say this is the full dimension and we divide it into three parts now in every subspace let's say I have eight centroids so the number of so I have eight centroids here eight centroids here and eight centroids here so for each centroid from the first subspace I have the option to choose eight of them and eight of them so it becomes 8 x 8 x 8 that will be number of possible choices yes yeah so that for example the query audio might have the first cluster from here the fourth cluster from here and that 8th cluster from here this is the effective number of code words so you can basically combine effective code book size yeah you can combine any cluster like any centroid within each one of and partitions yes yes so that's the effective number of code words so the good thing is that so this is very high let's say it's 256 to the power 8 right it's very high the good thing is that we don't have to store all of them because we only store 8 of them 8 of them and 8 of them but the number of choices becomes 8 cross 8 cross 8 so that's where it is gaining thing yeah yeah the contacts Li 4pq will say it depends on your deal side set size right it depends on the feature dimension you have also also done a number of samples here yeah it's it's it's it's bad but this is exhaustive distance computation I will also there are algorithms for non exhaustive distance computation so what I'm showing here it's basically it's still computing exhaustive distance so basically this computing distance between query and all the samples in the database but there are algorithms I will come to I will talk about it in future works which where we can incorporate this product quantization algorithms and D queuing algorithm with the non exhaustive distance computation there is a of the Shelf algorithm there but yeah so even with exhaustive distance computer then we are gaining like this much that's the benefit yeah because M is very small in maximum value of M is number of subspaces generally people use like 8 at most yeah yeah if the demon dimension is small we don't have cars of dimensionality so we don't need this yeah so this is mainly for mainly for high dimensional features here okay so let's come back to the example so we had this this talk to lines so 1 million samples in our database and thousand is our feature dimension now just keep in mind these two more things here now so let's say the number of subspaces M is 8 and number of centroid power subspace K subspace is 256 now this just this for now based on this if we want to calculate the complexity as we saw earlier Euclidean becomes order of a billion now VQ if we want to like have the same number of code words as as PQ it becomes intractable because the total number of code words is becomes 256 to the power 8 and if we want to have similar number of code words vector quantization becomes interactable but PQ is not because it does divide and conquer and it can be calculated that it becomes order of 8 million and so that's a that's a that's a big gap and we will see what will happen in d queuing yeah so this is the overall system again that we kind of discussed earlier and we told that we have to optimize the distance computation part so now we have an algorithm that we can use so we have product quantization that we can use on the full database to Train and after the training is done it this is done offline after the training is done we can store the code words and when it when it comes instead of doing Euclidean distance computation we can use product quantization algorithm to compute the distance with the query and these code words so we save some computation here and the rest of the ranking system is similar okay so now let's move to supervised D fashion so just to recap the reason for moving from unsupervised to supervised walls we want to incorporate human knowledge into the hash codes so we need some level data but after incorporating those knowledge we can have hash codes that can have similarity data similarity in the in the hash codes embedded into them and so the overall picture is similar we just replace replace this offline potion so we have an audio embedding database and all of the audio in this database might not be manually labeled so we can take a part of the audio database that is manually leveled and do supervised hash code learning but after it is trained we can generate hash codes for the full database as much as data as much as level data we have it will be better in general but after the training we can generate hash codes for the full audio database and store them in the in our code or database so the good thing is that this database can keep increasing because we can have more audio and we can we want to put them into the database and this might have label might not have level but in general this database can keep increasing and in the distance computation part we can use some supervirus hashing algorithm instead of the product quantization algorithm okay so the algorithm we use is called deep quantization network and as I said earlier the main contribution was to do hashing and feature learning together and to have a formal control of the quantization error and if we want to see from from the left we will have an audio pair so for example they can be from the same class or different class and we passed them through the through some kind of feature extraction module either vide dish or some TL week and then after extracting the features we can pass them through a neural network and in this case it's a it's a simple multi-layer perceptron and after the forward rough at the outermost layer we we compute the loss and we have two losses here the upper one is pairwise similarity loss and it basically increases the similarity cosine similarity if this two audio are from the same class or decreases the similarity if they are from two different classes so it kinds of retains the data similarity the second part is basically the hashing part so it also does product quantization so as a hashing algorithm it uses product quantization and it calculates the quantization error or quantization loss so for some specified bits for example I want to train this model for 16 bits 16 bit is by by that I mean after training every feature can be represented by 16 bits so for 16 bit I can do the forward path I can compute the cosine loss I can also do a product quantization for 16 bits and calculate the quantization error and then I can add these two losses with some weight parameter and minimize that through back propagation now so if this model is able to decrease the quantization error so we can see it kind of increase the quantized ability of this these green embeddings so that's the whole process now if we come back to that comparison slide so we left this blank the of the dqn and we can see that so this D queuing it has a forward pass over this network and it can be a specified size according to applications and after that it does product quantization so during testing time it has to do forward pass and product quantization for inference so the the complexity it becomes equal to product quantization plus an added term which is the d ln x and DNN x is basically the complexity of doing one forward pass for one sample and so it depends on the size of the DN n and number of hidden layers you have because it has to do some matrix multiplications and if we come back to our original example so and put them there and using use our DN in size then we can come up with more or less this figure so we can do it in order of nine million operations now now this forward pass of DNA nor DN + x this is basically matrix multiplication and here I use the brute force matrix multiplication but there are more efficient matrix multiplication techniques and also we can use GPU if needed and this can be reduced more if if we use sophisticated algorithms [Music] besides on your side yes it depends on the feature dimension and the size the structure of the DNN yeah number of layers because the more layers we have the more matrix multiplication we have to do that's the next question okay so it's kind of similar to PQ but it cannot be generally lower than PQ but it like depends on the DNN architecture okay so yes we do raise so this is the most accurate yeah the least accurate so V Q becomes intractable in if we have a large number of code words so if we want to compare like PQ and DQ n DQ n will be more accurate because it's super biased because we use the human human knowledge there so we will see that DQ n will boost up the performance even if we have a very small amount of training to the whole size of the yeah yeah yeah so so now this becomes our input to the algorithm so so this becomes the input we do and calculate the after and that's also there yeah yes that helps to retain the data similarity but the whole hashing part is done by product foundation so if we don't have a cosign loss that's basically doing P Q after some linear non linear transformation so I mean it will work but I am not sure whether I mean will have any application for that because P Q in general is very efficient and state of that algorithm so why not apply P Q directly on the larger features so you mean if we remove this part and just have the quantization laughs right yeah so the the thing is that this quantization loss this does not need any level so only supervision that is like useful is in this cosine loss yeah the quantization algorithm itself does not need levels he's basically k-means clustering yeah okay okay the supervised oh you mean like instead of cosine laws we can have some prediction of the level of something that no no just training with holiday laws okay yeah maybe yeah maybe we will discuss this offline better yeah it will be better yeah I'm not getting exactly the architecture you were thinking yeah yeah okay any other question we will move to the experimental results okay so yeah so we have two data sets for evaluation one is decays 2018 task to and which has like 1600 - two audio files and around 10,000 training audio files and it has 41 classes so here are some examples of the audio that I took from the decals dataset so let's see some of those and try to predict whether so this is an acoustic guitar [Music] this is a cello this is scissors and can anyone guess that oh wow I thought I thought that's the most difficult one that's why I put it in the end okay we have we don't need machines then so we also tested on ESD 50 data set that is kind of a little bit smaller than the decays data set and for features as we discussed earlier I use vgg SH and I use mean over the temporal dimension so it's 128 dimension and I also used TL week so this is not a crow named STL week in the paper there is no acronym but you can just see the paper here and it's dimension is thousand twenty-four and the evaluation metric is called mean average precision and at R so this R is basically the number of retrieved items and do you like different applications might have different kind of different requirement of this R someone might be interested in the top three results some browsing might be interested in the top ten results or some applications might be interested in all the retrieve results this is the art and the average precision is basically number of positive retrievals number of hits that are correct and mean average precision is mean over all the tests queries so this map at R it it varies between 0 to 1 and most importantly it depends on the ranking or order of the retrieved results so if all the hits are correct and all the hits are like ranked at the beginning of the list the map will be 1 so let's move to results and so we calculated map over the full database size as our metric so the reason is that this is a very standard metric first of all calculating it at the full database size the reason is that if the if some retrieved hit is at the very end it generally penalizes it very much but we want to see even then we want to see where it lies so if we calculate map at 3 it will just discard all the retrieval results after 3 so we use 2 features as you can see and all the unsupervised algorithm they are trained on the full database because they don't need need levels and the supervisor algorithm DQ when it is trained on 10% of the database and more specifically I selected 20 samples from each class that kind of gave around 800 samples and it's around less than 10% of the full database size so we are assuming we don't have levels for the 90% of the database even then we will see improvement so first among the unsupervised models we can see PQ is better than some baselines and some of them are like spectral hashing iterative quantization these are the we apply like we applied like multiple baselines and these are the like best performing baselines so even then we can see PQ is better mostly for lower number of bits and when we go to supervised model we can see that even having a small amount of labeled data we can increase the map performance by a by by a significant margin we also applied we also like got same findings in the esc 50 data so i'm skipping it for the sake of time so here it's a it's a very busy plot but let's first focus on the red lines which is basically VG geesh 128 features so what we are doing is we are in x-axis we are increasing the amount of labeled training data we have so we start from like 10% less than 10% and we eventually increase the amount of labeled training data and this y-axis is basically map at the full database size so we can see when we increase the level data the general trend is to get better retrieval accuracy for both the VG geesh features and also the GL make features another observation is that so if we only consider one type of feature let's say the red one when you move from 8-bit to 16-bit there is a very high gap and this is because based on our parameter values 8-bit is basically having one subspace so it's essentially it's basically vector quantization and when we move to two subspaces we gain a lot but after that it's kind of getting saturated yeah [Music] yeah there is not much improvement for the wage easier for the visually and so I also I was also looking into that and what I kind of my hypothesis is that our model is rich really tiny it's basically two layers 256 and 512 yeah so this tiny model is like good for this amount of data like 800 samples but when we reach here we have like 10,000 samples so we might like having a bigger model actually might help in that case but having a bigger model here will might result in overfitting so if we really have all the data level we can just use as big model as we can here is another AB lesson study and we can what we did we calculated map at different number of retrieved results because as I said people might be interested in the top three results or top five results or top ten results so x-axis is basically number of results that we consider for calculating the map and y-axis is map value and if we again concentrate on one feature let us say the thousand twenty four dimensional feature we can see so this triangle this is for this is with ten percent of data and we can see map attain if we consider about being search like so we are interested in ten retrievals so we can achieve around 65 percent map but if we have full data we can achieve around 77 percent map value so yeah so conclusions so the main contributions of the work is that this is like the first attempt to even try efficient audio retrieval technique for audio for audio events so and it definitely saves a millions of operations in nearest neighbor search through some kind of quantization technique and we as we saw that even is even having a small amount of labeled data can can be very beneficial and it can boost the performance by even an absolute amount of 30% from the best unsupervised algorithm and moreover we validated our dataset in multiple validated our model in multiple data sets and we found similar performance we also validated the different feature rajesh and TL weak so in future this is this is the question that Raymond asked so this is still exhaustive search because we compare with all the elements but there is algorithm there is an algorithm called the inverted multi index where we can actually do some kind of non exhaustive search and that can be the son of the shelf algorithm that can be incorporated with product quantization that can be incorporated with D queuing another future direction and I already started some work and have some preliminary results is doing the hashing or quantization part for cross modal retrieval cross modal between text and audio been already had this paper and so that would be very beneficial because people might start with text or they also might start with audio and the last direction would be to do this efficient retrieval for hierarchical audio so that the hash codes can also save the hierarchy information somehow so that when you retrieve we can have the option to back off to a course class yeah so yeah this is it thank you and please ask me questions any questions yeah so if you had a couple of variables there had no very public data cities yes I'll make your future dimension businesses and then a model complexity also provide versus so if you combine all of those together would you say that this algorithm actually helps in one case when you have big cleaners there Oh actually helps yeah we only have a big data set like hundred millions and when the feature dimension is high at least hundreds yeah because euclidean distance is fast if we have like give the diamonds and it's two or three then it's fast but when we have hundreds dimensions then it becomes slow so it will be very effective if we have a very big data set and our embedding is high dimensional and that's it that is that is generally true because all this DNN embeddings are high dimensional yeah these are the two requirements yeah that it will be really nice to incorporate the inverted multi index algorithm for non exhaustive search yeah so that we don't search the whole database we just search a bucket yeah yeah so I mean the number of spaces that you use was eight so I I it was the maximum I started with one two so yeah one two three four and eight yeah so one means like depending on the parameters I have M M equals to 1 means like it's it's similar to VQ vector quantization now when I use two it's a two sub space for subspace yeah yeah it helps yeah it's a very good question because there was a relation between this there is a relation between this number of bits and M so I didn't go through that because it becomes too mathy and very detailed so bit 8 means M is 1 bit 16 means I mean like that so we can see when M increases the the performance increases yeah it's not like strictly increasing it might I think it strictly in yeah so but it might also oscillate because the data is not perfect it is noisy yeah these are like cross validated twenty times so because I have ten percent of the data I chose like ten percents multiple time like I did the training multiple time and also validation right evaluation multiple time and also the parameters like lambda and those parameters they are also cross validated using 10-fold cross-validation try to tune the parameters on the training set yeah okay so you mentioned that these technique might help you attain with data sometimes yes and I assume that comes from cosine or sine cos we are at you so it really makes sense you're learning and training using it but is there a way you can visualize it and understand I will be actually learning yes I I suppose varies so since we are kind of increasing the similarity of the same classes what visualization would be one technique so we can basically visualize the last layer and plot some kind of like clustering scatter plot in a lower dimension using some PCA or t-sne some kind of dimensional reduction technique you can visualize and see how the clusters are formed between the original feature space and the learned feature space another way would be to calculate some kind of clustering loss like cluster purity since we have the labels like theoretically so we can kind of calculate some kind of clustering loss and see how the purity how pure the clusters are yeah my question i was when i said removing the cosine cosine net put a zero weight for that i've just trained based on connotation yeah so so if we train based on quantization loss what do we do with the labels that I'm not very so because this quantization loss doesn't need letter so we have to formulate we have to come up with a quantization loss that needs level well the was the was this how you cover the loss it is in this so this loss yeah this loss is basically the one transition error if you run product quantization on your on these embeddings so in other words this loss just think about the k-means clustering so if I do k-means clustering on this space I will have a mean squared error right that mean squared error is basically becomes a loss so solo solo only power uses the ground truth variable so I so that's why I was thinking like so that's why I was telling that we might have we might really replace this with a different system so for example there is another paper by the solder by the same authors it's actually a paper from Microsoft Research Asya where they replace this by a triplet loss so a trivial kind of extension supervised which one like for the unsupervised approach I didn't get the question properly so you have one supervise and another yeah this is the upper one is unsupervised the lower one is unsupervised yeah I mean these are just two losses the upper one is the these needs levels to calculate the loss the lower one doesn't need level to calculate the loss yeah yeah so basically the P Q is the P Q is basically product quantization that is done directly on the features without the levels so that so so if we like really want to compare the then we should compare PQ + DQ ends so if if we only use PQ that this is the performance and when you use the similar errors this is the boost and this boost comes from the because we use levels so basically this we can think this as a way like so the the cosine similarity lot it's kind of creating better clusters because they are bringing them closer so that in term that that that in turn helping the product quantization algorithm that is inherently embedded into the model so if a better feature space PQ will be good any other question okay [Applause] 