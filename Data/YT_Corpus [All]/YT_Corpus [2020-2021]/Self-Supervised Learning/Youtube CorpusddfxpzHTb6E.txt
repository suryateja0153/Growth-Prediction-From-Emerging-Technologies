 Machine learning has revolutionized a lot of industries today. It can help us recognize objects , rank things, even understand what you are saying. But it is still in its early stage and faces a lot of challenges. Here I will list three major challenges in machine learning nowadays from research to industry and tell you what is our current solution to each of them. The first one is the lack of training data. Data is at the core of any Machine Learning project. However, it is usually very hard and expensive to obtain labeled data. How to train a model without large amount of data is very hot topic today. Transfer learning is one of the methods to solve this problem. It enables the model to utilize knowledge from previously learned tasks and applies them to the new related ones. For example, in the image classification task, very few people will train an entire convolutional neural network from scratch, which usually needs millions of labelled images. Instead it is common to use the pre-trained models and fine-tuning them on the target domain.  Self-supervised representation learning is another way to solve the lack of data problem. It opens up a huge opportunity for better utilizing large amount of unlabelled data. Here is a very famous example in language modeling. If there is a sentence, but there are some missing words in it. How can you fill in those missing words. Well, the idea is to predict the missing words by learning from the past and future knowledge. The representation of the language is also learned during this process. Google have used this technique to understand searches better than ever before, which brings one of the biggest leaps forward in the history of search. The second challenge is that there are usually some discrepancies between your training data and production data. Sometimes the model works well in your prototyping environment, but fails to generalize in the real world cases. Let’s see a few examples here. The model may work well in one country but fail in another due to geographical differences. The model may work in winter but fail in summer due to seasonal differences. The model may work well on mobile but fail on desktop due to user behaviour differences So when you are developing your model. You need to be very careful when you collect your training data. To make it as close to your target domain as possible. And keep updating your model once it is outdated. The last but not the least challenge is about model scalability. It is a big issue for a lot of projects in industry. As a machine learning scientist, you need to make sure that the model can run fast enough and the model size is small enough, which is usually a big challenge for a lot of tasks. One of the possible solutions is to use Post-training quantization. It is a conversion technique that can reduce model size. At the same time, it can also improve CPU and hardware accelerator latency, with little degradation in model accuracy. Thanks for watching, don't forget to ring the notification bell and subscribe us. 