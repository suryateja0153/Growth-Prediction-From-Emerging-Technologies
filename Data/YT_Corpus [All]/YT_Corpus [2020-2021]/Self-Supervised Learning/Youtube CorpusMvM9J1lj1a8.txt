 I'm saying in the research scientist at Google research his research interest lies at the intersection of generalization can optimization with emphasis on people earning prior to joining Google in 2016 he was a for social researcher in the computer science and artificial intelligence lab at MIT while with us at to that night he was also he collaborated it with a virtual phone joke and published a paper in Nick's 2015 he obtained his PhD in computer science from the University of Illinois at urbana-champaign and he's a actual member of the machine learning community serving as a nature of ICML in Europe and I say our thanks everyone for attending my talk today I'll be talking about self training and self distillation and why they can improve generalization this is joint work with my collaborators method fresh sabor from deep mind and Peter Bartlett from Google research and also you see birth so I was asked by the organizers to keep this talk high-level as a result of that I'll skip some of the details during the talk but I wanted to let you know that all the details are available in this preprint which i share on the screen so you can find it on archives and all the details are there all right so let me first start with self training so it's actually a quite old technique used in unsupervised and semi-supervised learning in machine learning so let me first give you an example in case of semi-supervised suppose you have some label beta and some unlabeled data what they do is that a first training model on the labeled data you get a model and then use that model to make predictions on the unlabeled data so at this point you're only will they have some labels attached to it these are not real labels it's generated by the model predicted by the model that's why people sometimes call it pseudo labels but now you can train another classifier on the food set including these pseudo labels and you know the original label set and get a second classifier and then the second craft classifier is now like doing the Bell job you can also use this in a fully unsupervised setting the idea is since all the data is unlabeled in this setting you need to have access to some other data set that is some somewhat similar or it has some commonality with your data so you train a classifier on that data which which is labeled and you get a classifier then you take that class right apply to your unlabeled data and again generate pseudo labels for them and use those through the labels now to train a model using that in a supervised fashion and there are slightly tweaks and variations of this whole concept for example in the unsupervised case sometimes people not like use all the unlabeled data that's now absolute labels they only use a portion that the model is more confident introduce that into the next round of training and once they get the second model then again they generate pseudo labels on the remaining unlabeled data and it can only take a portion of more confident subsets and you know but the overall idea is this sort of self training that you have a model that learns from its own predictions now there is a more restricted version of this more special case of this which in deep learning community people call it self distillation so it's now used in a completely supervised setup so the the idea is as the following I'm just illustrating the figure on the top so we have some input-output pairs X 0 y 0 we train a model we get a classifier or if it's regression we get some continuous predictions F zero and what we do is now we pretend these predictions are new target values so we replace labels with those while maintaining the inputs the same and then this creates a second data set right we train the model from scratch with this new data get a second model and we can repeat this process because this gives its own prediction again you can treat this prediction of the next day what people in deep learning community observed and you see one reference here born again neural network was one of the main references join this was that the test performance of these models actually can improve over this iteration and that's way in my opinion at the first one it was very weird and surprising because there is no external information coming from anywhere it's the same network architecture is the same training procedure that I'm using in this block and it's the same data set but this the input part is the same and somehow this internal loop is able to you know generate models that perform better and that was at the first glance again it was surprising to me why this happens but once we studied this problem more closely we realize that actually this is a more profound phenomena it's not unique to deep learning although we should be thankful to deep learning community because they in to the best of my knowledge they first observe this I do at least the name self-isolation is exactly attached to deep learning community when they observe this but now we know that this also happens in more basic regression centered like even simpler setups you don't need the fancy deep learning food for observing this phenomenon and actually I will use this regression simple regression setup today to present my analysis which explain what's what's happening why self-isolation can improve generalization and the reason is obvious because now we have a that is mathematically easier to analyze and we can you know keep concrete answers to some of the questions about this phenomena so in this slide you see the regression setup what we do is to given a training set where I show as XK YK is that input-output pairs you have capital K of these points and I'm using simple l2 loss I'm just demanding the total loss to be smaller than some tolerance epsilon 4 for this set up of course there are many functions in the function space that can you know satisfy either completely pass through this point or passed close by these points to maintain this epsilon accuracy so in order to single out the solutions that we care about we have to use regularization which basically is our bias introduced into this regression which which solutions we prefer and here you see a specific form of the regularization that I am using I think because of the broad audience if you haven't seen this sort of regularization it may look a little bit complex but you really don't need to worry about because I have some slides soon that will this provides some intuition what's going on with this forum for now just think of it as a regularizer takes a function outputs a number and this number basically assigned some sort of complexity to this function or some sort of smoothness of that so we can use Lagrange multiplier very straightforward to convert this into an unconstrained optimization so this will be what I will focus on is easier to present we have these coefficients C which you know will the trade-off between fitting accuracy and regularization and actually this form of regression problem is very well studied in machine learning literature in fact Tommy has been a pioneer in this area from the machine learning perspective he and Federico 0c in the 90s published a series of interesting papers on problems of this sort and I think those are great references if anybody is interested to learn more about some of the detailed aspects of you know how this regularization framework works these are great papers here that just have one of them with more than four four thousand citations as an example okay so I promised that I provide some intuition about what this regularization doing and I think eigendecomposition will be a very very appropriate tool for for achieving that goal so here the ribbon is the regularization was characterized by a kernel you in the earlier slides you can think of this as a linear operator that this integral you times function as a linear operator that you know takes a function and returns a function right because the the dummy variable extract eliminates and you get another function interest so in the same in a very like similar way that we think about matrices and vectors and matrices having eigenvectors we can have operators similar to matrices and functions similar to vectors and then for matrices or in this case operators we can have I can function and eigen values and they basically satisfy very similar property so if Phi I is an eigenfunction of this operator it means that when i apply to this named operator i get the same function phi i times a constant which is the the eigenvalue of this I am metric now without loss of generality I can represent my F using this de spaces because this this gives me a complete orthonormal basis so now my goal is to just you know identify the coefficients AI the bases are given by the regularizer to to fit my to find my solution so now this helped me to reduce the variation on problem that you saw earlier to something that okay I just have a quick question because there is this portion that's blocking part of the formula is it just on my screen or you also have it maybe I can wish it a full thing oh you see great great so yeah what happens is that for the regularization now it greatly simplifies you can see that the regularization part is now some of the squared of this coefficient although each coefficients is penalized by this eigenvalue right so essentially it says okay if my regularization is like introducing some eigenvectors so that you can or eigenfunctions so that you can build your solution by weighted some of those i also assign a cost for you to pick each of these eigenfunctions and that the cost is determined by by this operator so here is a very simple illustrative example suppose I'm considering functions that map the interval of zero to one two real numbers and suppose my regularization is just penalizing the second-order derivative of my function over the entire domain of zero to one so i squared and integrate it of course because I introduced this differential operator are also need to talk about what happens out the boundaries but they're really those are the details you don't need to worry about those okay so I just need to get my parrot getting upset so he will be listening throughout the talk as well so now for this operator for this second-order derivative I have plotted the eigenfunctions so the blue one is the first eigen function then you have orange green and red and then on the right you see the eigen values so you can see that the red one is actually the largest and the blue one is I think if you read the scales the blue ones maybe less than ten why the larger one the red one is about 120 so it means that if I want to use the red one to contribute to my solution I have to pay the cost of like 120 times more than using the area so that's how its biases us to you know use some basis versus others all right now this is a key intuition we will use it across across the talk and one more thing I should say before we move on is that this kind of regularization problem that we are studying as a closed form solution again the details how we get this closed form are not like crucial for this talk I can just present it the final form but if you're interested you can either refer to our paper or if you want more details as I said Tommy has a series of papers on this which you can refer to but what what like if I just want to give you the gist of it so associated with this kernel or regular kernel of the regularization operator we can identify a another like kernel which is called greens function and it satisfies that this identity that you see just you know applying that to the power operator should produce the Delta function so once you have the greens function then you need to form two quantities one a matrix capital G and one vector small G so the capital G is just taking all pairs of training points evaluate them with the screens function that's why you get a matrix the other one uses greens function but only use one argument for training points the other argument is free so you get a vector but each component is not a function of text and now if you put these matrices and your labels why I also arranged or all the labels as a vertical as a column vector then I can express my solution in this form so this is a very well-known result I don't need to like emphasize too much or we just take it as granite but we will use this form for our for our analysis okay now again before we move on up to this point I want to make some connections that are interesting and important one of them is that you can you can clearly see there is a close connection here between this kind of regularization problem and kernel regression problem essentially G is a kernel function so you can think of this G as kernel in the sense of you know kernels that we use in SVM so it's the same thing so this is essentially like this regularization problem can be written equivalently as a kernel regression problem so that's one point and now that we see this as a kernel regression problem it provides another connection interesting connection that's too wide neural networks which operate in ntk neural tangent kernel regime because for these neural networks it's been shown that the problem moves down to just a kernel regression so all the biased fancy biases you have in a deep architecture like a convolution fooling around her representation all of that is at the end of the day encoded into a single function and that's a kernel function for the case of like white neural networks so we'll get back to these intuitions ok now that we have the regression problem set up let's talk about self distillation so again some notation here as I said the vector Y is just stacking all the training scalar points why I have K training point so I have y 1 2 YK and suppose also if I have it model training this data I get predictions I can again form a vector of predictions over the training pants writes from x1 to XK so this is the bold F alright so now as I showed earlier the solution form for this regression will has this form so for the first round F 0 the solution has this form as you see where y 0 is the initial ground for slave now what we do in self isolation is now pretending that this F 0 is going to be a label so we set our next round of the label for the next round which I denote by Y 1 equal to F 0 and then I write the formula for F 1 which is exactly the same because we are not telling anything other than changing the from y 0 to y 1 and then I replace the definition of Y 1 so you can actually keep repeating this and see that at the end after 3 steps your solution actually evolves according to this formula at the bottom the important part is that product so you get the product of T plus 1 terms everything is the same except as the coefficient the regularization coefficient CI which varies in each round of self distillation so at the first glance this may look a very simple form it's like a power iteration but actually it's not a conventional power iteration why because in power iteration this thing this linear operators are I'm busy grouping everything after this PI as a operator so the matrix or the operator a is G times this practices inverted take that as a linear operator so we are applying this linear operator over and over to y 0 T times but this is not really a standard probability region because this linear operator is changing dynamically over time because this depends on this like iteration dependent coefficient CI and this actually greatly complicates the analysis because this CI itself has you know very complicated dependency of the norm of the solution in the previous round and creates a messy recurrence relationship which doesn't have a close form but you know in the main paper we we basically provide bounds here and there to get control over this thing but for the sake of this talk you can just pretend that this coefficient C T is constant over time because it still enables us to make our point although it's not exactly correct but it's enough for making the point and as I said the full exposition is in the paper okay so if this coefficient C is constant it doesn't change over time now things greatly simplify because now I and say okay is this product thing inside this pie it's just essentially raising the same thing to power T plus one right and that's what I'm doing and on top of that I can just use eigen decomposition of the matrix G because everything you see is either g or identity matrix or some matrix inversion and none of these changes the eigenvectors so i can push the eigen vectors out and write the thing they meet that is in the middle just in terms of diagonals okay now let's look at this diagonal now things are very clear now you can see that as T goes larger and larger what happens that these smaller components of this diagonal shrink faster and faster to the point that maybe after like some iterations you can completely consider the small initially small ones as negligible now because they shrunk so much relative to you know the magnitude of the larger values that you can actually consider them as non-existent so as actually T goes to infinity all these components will die and become like super negligible relative to the largest one and so you can only you can think of it as only having like one nonzero component on on the diagonals everything else and here I'm not talking about absolute numbers absolute value numbers I'm talking about the relative scale of this thing so compared to the largest one the smaller ones I becoming smaller and smaller so that's when T goes to infinity you end up with one significant component but of course during this path as you increase T you get gradual you know elimination or weakening of these small values so it kind of like progressive these sparse if I this diagonal matrix okay so why this is important because this is exactly how it's imposing this capacity control that can improve generalization here I just do as simple renaming - I think make it slightly more clear let's name this rotated labels by 0 times the eigenvector which is a rotation matrix just call it Z 0 these are the labels and a solution on the left hand side of Z 0 doesn't depend on labels at all except possibly true scalar C but none of the vectors or matrices depend on label so this is saying that all the information at this end in the vector form of the labels goes into this Z 0 and now when this middle term is very sparse it means that I'm only allowed to use a few number of these rotated labels to construct my solution so initially I didn't have this this restriction right as it's possible more and more I'm losing my degrees of freedom to make use of more points in my representation so this is exactly how the capacity of this model is being controlled and the sparsity level of this matrix is basically determine on determining the effective number of basis function of using to represent your solution ok so actually you can use this for C pattern to even come up with generalization guarantees because I want to keep the top high level I will not get into the details of this part but the high level idea is that you can bound the so-called Rademacher complexity which is a complexity measure of function classes of models of this type depending on their sporty level and then once you have a bound on the Rademacher complexity then you can use standard Germanization bouncer or standard theorem set based on a bound on Rademacher complexity you can get a transition all right so now let's revisit our toy example that I showed earlier although this time we want to study it within the self distillation do see what happens there so the first part of this slide is the same as before we are penalizing the second order derivative I will have the same boundary condition but here I also provided the analytical form of the greens function although you really don't need to have it like it's just for completeness of the presentation you don't need to know how to derive the greens function from the regulation operator so for the kind of results we present in stock but I have it just for completeness okay so for this example the first figure a is showing just the shape of the greens function and the middle figure is showing our in setup regression setup so the orange curve in the middle is the sinusoid that we are trying to see it's it's like the underlying ground root function that we don't see instead we see some noisy samples from this function and those noisy samples are shown by these small blue dots I hope you can see them and it's still in the middle figure I'm talking I have eleven of those points if I'm not mistaken and the goal is to you know use these points to recover a function as as close as possible to the sinusoid all right so if you just go ahead with the first round of training you get this blue curve in the middle which is clearly overfitting to the data right you don't get anything close to the sinusoid but on the right I'm showing what happens if you do additional rounds of training by just taking the predictions of this original training and you know doing another round of training and repeating this loop so you can see that the functions you go from blue to orange to green and eventually red are becoming smoother and smoother actually I think the orange one which is just one round of self solutions already very close to to a sinusoid so you can stop there but one thing you can see is the further round doing additional smoothing plus shrinking the size of the function so you see that the function is becoming also closer to zero so this is basically confirming what we were discussing with in a way example alright now let's look at these diagonal components also how they evolve so because we have a 11 training points in this example the matrix that we have it's K by K if you remember the notation so it's 11 by 11 it's diagonal so we have only 11 components initially these components are distributed as shown on the left so you have some larger ones some smaller ones but after one round of self-possession you see that the smaller ones quickly shrink and only the larger ones are significant relative to others and if you keep doing this of course this process exaggerated and at the end I think in fairly say it's only 2 or 3 bases that remains for you to represent your function so one thing that perhaps you saw in the figure but I didn't explain much was that as you increase the self-isolation rounds because it as we showed it amplifies this regularization effect it shrinks the function also like the value of the function is shrinking towards zero and at some point the process will collapse you will just get zero function and from that point on there's no more self decision just know it's a fixed point you just produce the zero function from that point on and the reason that this happens is very obvious because they label because the predictions are shrinking right at some point you can easily satisfy the error tolerance being smaller than epsilon because the labels are very small so you can choose very small functions and at this point zero function may even be within your error tolerance so zero function is a solution and it also minimizes your regularization so at that point your solution the solution minimizing the regularization is still valid for your constraint is zero function so you get zero function and let's collapse and then nothing interesting going on but actually we can bound the number of rounds that you can get meaningful self-isolation before the solution collapse so again the derivation I won't I will not enter I'll just presenting the end result so K is the number of examples epsilon is your training error like in each training round at what point I stop frame based on the training error reaching epsilon and Kappa is the condition number of your gram matrix that you have from the kernel like the the big matrix G the condition number of that so that is that is one I think interesting result that we can show first of all it collapses and second we can bound the number of meaning for the iterations another thing is the advantage of having small epsilon which allows these models to move near interpolation regime so that means the models are attaining very close error to zero but not perfect zero so we can actually show that by reducing epsilon the error tolerance you can increase the sparsity level of the solution that you ultimately get after like repeating this for whatever number of iterations that we have here that there is no no collapse at the end before the collapse happens you get some solution and that solution this for sea level is affected by by the error tolerance that you choose for these self-isolation intermediate problems and smaller is better so it suggests that in order to get the highest is partly it's best to choose a smaller epsilon of course this is in theory in practice you cannot make it one too small because there is numerical issues right but in theory the smaller gives you the sparse already another thing I want to discuss is comparing this with early stopping because okay everybody knows that every stopping is providing some kind of regularization and here we are saying softest vengeance also providing some kind of regularization is there a connection between the two are they similar so although people use the name of early stopping in the field a lot but I don't think there is a concrete increased definition for it so here maybe I first give a general definition for early stopping I call it any procedure that cuts convergence short of the optimal solution and then there are different instances of this for example if you're using a numerical optimizer to minimize your loss such as SGD you could for example limit the number of iterations say okay after this many iterations done or you can do early stopping by increasing your training error loss tolerance so instead of like getting to near zero training error you can stop at some Epsilon that's slightly bigger and that's also another way of doing it yourself but the first definition is not applicable for an or analysis because you know here there is no numerical optimization we're looking at the closed form and our analysis is independent of how you privatize the function so there's no numerical optimization so we can only look at that the second definition so under the second definition let's see what happened in the yellow box what I'm doing is again listing the solution after the first round of self distillation we are not doing self distillation for early suffering so it's just a first round so you you stop after getting this solution and if you play with the error tolerance Epsilon it's going to affect you know the Lorenz multiplier the c0 so but nothing else in this form of solution so if we know what happens for like range of c0 from very small to very large then early stopping will be somewhere in there so it cannot be outside of this range right and we can actually see that for both cases of very large and very small c0 this never is able to specify the matrix that we have in between for example if c0 is very large then you get this approximation the first bullet point which gives the matrix D which you know is the initial sparsity pattern we have in T and if c0 is very small then actually this whole thing becomes close to identity which is a full rank matrix so actually you go against sparsity so this is saying that with the early stopping at best you can maintain the original sparsity pattern you cannot make it more sparse but you can make it more dense depending on you know how you choose this epsilon so it's not it's not actually doing anything similar to self installation and I one thing also I think I can say about this is when you're early stopping means that okay you choose a bigger epsilon so that means that you choose a smaller c0 and for smaller c0 you get close to identity here so that's actually showing that it's doing the opposite of sedition so if you do it early stopping you end up with with something that's even like less force than your than your initial solution so you're moving toward the identity matrix alright so now let's let's move on to some experiments on deep learning I should say that our theory so far was only about this specific regression setup okay so I'm not claiming that like the results that we have here clearly carry over to deep learning but there are some hope there that you know could at least make us feel ok maybe this is providing some approximation to what's happening in deep learning and that connection was through again wide neural networks and you're times and kernels because in the antique a regime we know that the problem of deep learning is equivalent to a kernel regression and kernel regression is what we study so at least in that regime these are related very close of course as you move away from you know wide networks then this becomes a noisier and noisier approximation the second thing I want to say is the beauty of self isolation so throughout the talk we have to think about this regularization underline regularization and it's greens function and all that but in fact you don't need to know any of those in order to like use this kind of regularization to get the effect of this regularization all of that you can think of it as a black box so all the as I said all these biases of a deep neural network are now encoding into that kernel Abyssinian T carrying and we don't need to even know the kernel because what we show is that whatever that kernel or greens function is then we don't know we just provide you know input read the predictions and then feeding these predictions again as new target values to the system and its fortified that underline like regularizer which we don't know and we don't see and we don't need to know ok so I think that's very beautiful because we can claim that we are specifying you know the representation that is induced by that regularizer without even needing to know that regulator so is that it is this clear because I think that's very interesting point that I want to make sure or is there any question so far in general because we have time so if not I can move to some experimental results on deep learning so we have experiment we both we teach the architecture and resident architecture I only show the slides for ResNet but for vdg it's similar and we have uses for C for 10 and C for 100 okay imagine it was was not really an option for us because we wanted to do multiple rounds of socialization it's not just one one-time training it's like for example here it's like 12 times retraining the model from scratch on top of that we wanted to get the variances so we repeated each experiments ten times so we couldn't really go beyond C for 100 so what are these plots saying so the left two plots are the Train and has 44:10 the right to our 44100 the leftmost one okay maybe I start from the okay maybe I start from the second plot from the left so that is C for 10 but the training accuracy so this is the training accuracy with respect to the original labels y 0 we see that the accuracy training accuracy is going consistent it down and this is very consistent with a regularization viewpoint because as we increased the regularization if they amplify the regularization effect we know that it's limiting the way that you can fit your trainer that so we expect the training to you know get the training accuracy to get worse now let's look at the leftmost plot which is the test accuracy we see that up to I think three rounds of self desolation in each round we are able to benefit from this regularization each time it amplifies it a little bit but after about three rounds it becomes too much we start to over regular because this process just keeps amplifying the regularization forever right so at some point you start to over regularize and then you see a decline of performance in the test accuracy and then on the two right plots it's very similar a trend it's see for 100 it is there but the trends are quite similar you can see that the training accuracy which is on the rightmost plot is declining which is not surprising them we're well aligned either and also the test accuracy is increasing so I extend you get the peak about like maybe after ten iterations but after ten iterations it starts to decline now one thing I need to say here is that people had observed that self distillation impurity that observe the self-isolation improves test performance by running it for one or two rounds but here I think we did it for a longer window and we can even see this decline thing because I think you based on previous result where to just show you okay you improve for the first one or two rounds you don't know what happens after that this is saturate you get a flatter after that or it starts to decline but here we show that it declined and we know now white declines because it's a over regularization right so we are getting close to the end of the talk so maybe I can talk about some of the open problems here that can be pursued by anyone interested I categorize to apply them theoretical so from applied side there is actually a very important direction to pursue and that is whether we can use this understanding that now we have based on this theory we know that what happens we know how its regular how surface relation is regularizing and what is exactly that regularization form it's you know gradually specifying the matrix that we discussed so whether we can use this understanding to develop construct new efficient algorithms that can achieve similar regularization effect but you know more efficiently because it's it's like insane you want to do such a solution for ten rounds on a big model I mean that that's not possible right it's not interesting people will not use it but the hope is that maybe by just understanding this organization you can now use it more directly maybe somehow change your train loop and just in the first cycle of training you somehow enforce this regulation effects simultaneously so you don't need to run the model multiple times you don't need to run training multiple times from scratch so I think that's very important direction and then from theoretical viewpoint there are some I think interesting directions one of them is extending this analysis to cross-entropy laws so the plots I showed for deep learning were based on a true loss but in the main paper we have plus also for cross entropy and crossing to be show similar trend like if you use a cross entropy la strain the models with self distillation and cross-entropy loss you see similar trend that the Train accuracy goes down test accuracy going up but we don't have a theory why that happens so extending the analysis tool from l2 to process entropy is a very important because the person who is what people use more often the other question is whether we can take this analysis beyond the arc AHS which was the key set up so the regularization that we study here you know things ended up being like a kernel regression problem so it's a like a problem at the end of you know studying reproducing kernel Hilbert space that was the setup that we could prove all things but whether we can relax Sicily that go beyond this and still show what happens yourself the solution that's another interesting direction for future study with that I conclude the talk and thank you again everyone for being here and attending the talk if there are questions we have I think some time to discuss and answer questions 