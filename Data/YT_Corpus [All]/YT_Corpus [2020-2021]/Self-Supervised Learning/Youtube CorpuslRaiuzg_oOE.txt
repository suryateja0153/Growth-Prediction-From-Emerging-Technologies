 Welcome Cloud Gurus to another episode of the Future of Tech. This time, I'd like to expand your horizon using the question: what is machine learning? It's one of those terms that are thrown around in tech conversation and used to say that the future will make more sense. We'll look at the background of machine learning before delving into the nitty-gritty of improving everyday life with automation, how you can get started and what the future promise of machine learning is. This is the Future of Tech, the show that has the largest data set of normalized average jumping distances of drop bears. So what is machine learning? This is often not fully understood when the term is used to, you know, for example, describe how technology will steal our jobs. Ooh, one of the best definitions is "Machine learning is an application of artificial intelligence or AI, as we probably heard of, um, that provides systems, the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.". Machine learning is a subset of AI, as we've talked about just a minute ago, and this is where much of the fear usually stems. However, machine learning has a much more scientific background. So how did we get to this SkyNet inspired self-learning computer frenzy? Well, if we go far enough back, the roots of machine learning starts with Thomas Byers, who in 1763 - I know we are going a bit far back, but you know. He created the Bayes theorem that describes the probability of an event based on prior knowledge of conditions that might be related to that event. This is still considered to be one of the founding pillars of machine learning today. Now, between then and 1950, not much happens apart from statistics and statistical analysis and, you know, it's continually being refined. Uh, but in 1950, Alan Turing creates the Turing Test to determine if a computer has real intelligence, no real, um, to pass the test, the computer must be able to fool a human into believing it is also a human. In 1952 so, you know, shortly after, Arthur Samuel wrote the first computer learning program. The program was the game of checkers, and the IBM computer improved at the game the more it played, studying, which moves made up winning strategies and incorporating those moves into its program. And then we get to 1957 where Frank Rosenblatt designed the first neural network for computers. The perceptron sounds pretty cool, uh, which simulated the thought process of the human brain. And in 1967, the nearest neighbor algorithm was written, allowing computers begin using very basic pattern recognition. This could be used to map a route for a traveling salesman, you know, starting at a random city, but ensuring they visit all cities during a short tour. Look it up if you don't know that theory. In 1979, students at Stanford University invent the Stanford Cart, which can navigate obstacles in a room on its own. And then in the 1990s, work on machine learning shifts from a knowledge-driven approach to a data-driven approach. So scientists begin creating programs for computers to analyze large amounts of data and draw conclusions or learn from the results. 1997, we're getting closer to today. IBM's Deep Blue beats the world's champion at chess. And then in 2006, Geoffrey Hinton coins, the term deep learning to explain how algorithms that let computers see and distinguish objects and texts and images and videos, and then much more closer to our time in 2010, the Microsoft Kinect actually comes out and can track 20 human features at a rate of 30 times per second, allowing people to interact with the computer via movements and gestures. And that was done. A lot of that research was done with machine learning in mind. So the process of learning begins with observations or data such as examples, direct experiences or instructions in order to look for patterns in data and make better decisions in the future based on the examples that we provide. That makes sense? You know, you put some data in and you get some rules out right now. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly. So how does it work? There are various ways of using machine learning algorithms to achieve a range of, outcomes. So we can talk about decision tree learning. Remember the game, guess who each player has a, a number of faces, usually 24 and each player has one card with one of the faces on it. And then the opponent then has to guess which card you have by asking questions. It's a method of sort of alternating yes, no questions to ultimately determine which face the opponent has. And that's a decision tree. You can think of that set of questions as a decision tree that guide you to more specific questions and ultimately the answer. Then we have something called rule based learning. Uh, the defining characteristic of a rule rule based learning machine learning system is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. The rule based machine learning applies some form of learning algorithm to automatically identify useful rules rather than a human needing to apply private domain knowledge, to manually construct the rules and, and curate a rule set. Artificial neural networks are one of the main tools used in machine learning as the neural part of this name suggests they are brain-inspired system, which, which are intended to replicate the way that we humans learn. In general there are two types of learning for neural networks. First, we have supervised learning and it's the machine learning tasks of learning a function that maps that input to an output based on example, input and output pairs. So you put data in that already makes sense. A supervised learning algorithm analyzes the training data and produces an inferred function which can be used for mapping new examples. The supervision part is providing questions and answers for the algorithm to learn from now second, and you may have guessed, this is unsupervised machine learning, where the machine learning task of inferring a function to describe hidden structures from unlabeled data, where unlabeled is a classification or categorization that is not included in the observations. So you don't know what you're looking for. You let the machine learning, figure out the patterns itself, which in some cases can reveal surprising results. And then we have something called deep learning, and you may have heard of deep learning because it sounds very Star Trekky and whoo, you know, star sci-fi and whatnot. And it is like the solution to some great questions. In fact, it is another name used for describing artificial neural networks in particular, very large neural networks. The theory is that as networks increase in size, so does their performance. Other types of machine learning techniques tend to plateau and taper off, you know, in performance as size of the data set increases. So when you heard, hear, the term deep learning, just think of a large deep neural network. Deep refers to the number of layers typically and so there's kind of a popular term that's been adopted in the press. I think of them as deep neural networks generally, because that makes most sense. Then we have genetic algorithms and you use genetic algorithms, not when you have a complex problem, but when you have a complex problem of problems, we're getting deep. Now one such problem could be teaching a robot to walk like a human on two legs and feet, really difficult problem to solve. Genetic algorithms can also simulate natural processes like natural selection. The basic approach to genetic algorithms is to generate a bunch of answer candidates and use some sort of feedback to figure out how close the candidate is to optimal such as your natural selection theory. Far-from-optimal candidates literally die (sounds a bit severe, but they do) and are never seen again, close to optimal candidates combined with other, you know, with others and maybe mutate slightly. And this is an attempt to modify the candidates from time to time and see if they get closer to what we consider to be optimal, or further, from it. Now, at this point, I should probably mention that. Machine learning is sometimes linked to some dark and fearsome theories. For example, Elon Musk very famously has stated "With artificial intelligencen we are summoning the demon." Yes. He did say that "In all those stories where there's the guy with the pentagram and the holy water, it's like, yeah, he's sure he can control the demon. Didn't work out." And while there is some truth to that statement from Elon, most researchers agree it a long way off. And we have certainly have time to put measures in place to manage this. How would this does raise some questions around the ethical side of machine learning. And I find this particularly interesting, um, in particular, how about how we teach and train the models in machine learning. So while machine learning implementations can, of course make its own conclusions, you know, that's kinda the point, right? Um, they do require human input to get started. And what happens if the data going into the model is skewed? For example, if we disproportionately feed data about crimes committed by African Americans into a crime prediction model, well then of course our model's prediction will be based, biased against Black communities. And that's just not fair. According to machine learning experts, there are three different ways that we can kind of address this issue of skewed data. Post-processing in terms of calibration of the model. So what this means is that we calibrate parameters such that it has the same acceptance ratio for all subgroups, uh, of sensitive features. You know, it could be race, sex, et cetera. Second data reassembling to remove skewed samples. However, for a lot of reasons, collecting more data is not very easy and sometimes causes problems for the individual sample set. And then the third way is casual reasoning. So we capture different paths in a casual graph that can lead to the same observational data. And this basically means that to model possible factors such as the sex and race and other sensitive features, um, to make sure that their impact is captured and does not directly affect the result variable. Now that last point is not my words. They're rather, there are Dr. Hanie Sedghi from Google Brain, which is Google's machine learning research team. So I thought that was kind of cool that they're on top of it as well. Another side of the ethics question for machine learning is this concept of fairness. And let's put fairness because it kind of hard models need to be fair to work appropriately. But fairness is a concept which many with many definitions and even more opinions from researchers, for example, some researchers will come from a social angle and some from an AI going a AI angle and so on. And until we can agree on a definition of fairness, the ethics of machine learning are going to be vague and varied just like my left leg. So let's talk about how machine learning is making a difference today. Let's start with financial services. Machine learning helps financial services, firms, track customer happiness. And you might think finance happiness, what? But by analyzing user activity, smart machines can spot a potential account closure before it even occurs. They can also track spending patterns and customer behavior to offer tailored financial advice. Machine learning can analyze a large number of disparate data sets, you know, credit scores, spending patterns, financial data, so on, um, to accurately assess a risk in both insurance underwriting and loan assessment, telling them to specific customer profiles. Personalized health monitoring is another area like smartwatches and other variable devices have health telemetry, a reality, but machine learning is taking things once that further allowing doctors and relative to monitor the health of elderly family members. And the more personal data these algorithms are fed the better they understand a user's profile, um, enabling healthcare professionals to spot potential anomalies earlier than otherwise. Now retail machine learning algorithms are probably behind some of your favorite online retailers, as well as, you know, as we'll discuss in more detail in just a minute. Companies such as Amazon use this technology to offer a highly personalized service. You know, the endless ads for chainsaws and kid's iPad cases you keep seeing. Yeah, that's part of machine learning, learning about you. So in large companies where response time is limited by staff resources, machine learning can help ease some of the burden, smart machines can decipher the intent and, and meaning behind emails and delivery notes to prioritize tasks and ensure sustained satisfaction. It makes a lot of sense if you're a big corporation. And then of course we see machine learning in tech as well. Uh, you probably heard of Alexa from Amazon, you probably have for better or worse, but they use a ton of machine learning to constantly get better at understanding and responding to users. So the more data Alexa or Amazon gets the better of the voice recognition and perceived cognition, it exhibits. Sometimes it's even creepy how much she understands. She, I mean, it, it. Traffic, reducing commute times is no simple problem to solve. A single trip may involve multiple modes of transportation. Like you might be driving to the train station, then riding the train to a stop and then walking or using a ride service like Uber or something like that, you know, before we get to a final destination. So not to mention the expected and the unexpected construction accidents, uh, road or track maintenance weather conditions, um, you know, it can all restrict traffic flow with little to no notice. And some of the companies like Google is using the Google maps division to solve that very problem right now using anonymous data from your smartphone. Hmmm, they are. You know, and you know how Uber sometimes says that your ride will be more expensive. This is also down to using similar machine learning models to determine where the slowdown is in traffic and something as ancient, at least in tech terms, as email has also benefited from machine learning. One of the most important features of modern email platforms is... no it's the spam filter that keeps your sanity. And because that it spam filters like the block messages, like your "Viagra offer", in them are very easy to circumvent. Machine learning is used to continue to learn from all those messages and evolve the whole system. Consequently, the spam that hits our inboxes, Google, for example, claim to fill it out 99.9% of spam email by using this approach. True? Who knows? It started on using the goodness of machines teaching themselves to dance. I'm glad you asked. There are a few companies that will provide a platform that you can start using, because you don't necessarily want to have to set up, you know, your own system. Well, at least not initially. Anyway. So Google cloud has their AutoML, which is their suite of machine learning products. And that'll allow you to get high quality models fast, and they even have a very comprehensive, free training course. It's huge actually on how the tooling works. Use the link. Okay. Amazon has a machine learning platform, of course, and it is part of Amazon Web Services. It also has a great tooling and it supports common ML frameworks, such as TensorFlow Caffe2 and Apache MXNet use the link, here. Microsoft Azure offers the Machine Learning Studio and that's a graphical user interface or GUI that will walk you through how to create new models and manage the large data sets. It has hundreds of built in packages to quickly get going with the most common scenarios. Use this. Like, this is a good one like this. Now let's just delve into the platforms a little bit more. Now all of them have similar offerings, and structure and all this, but in this case, let's just have a look at AWS. Amazon has a product called SageMaker, and that enables data sciences and developers to quickly and easily build train and deploy machine learning models with high performance machine learning algorithms and has brought framework support and one-click training and tuning. So you can, you know, are we talking about for your tune, your models and interference, it really is quite simple in relation or in relative terms to get started. They also offer pre-trained services. This is cool, which you can hook into through simple APIs. For common tasks, such as audio and video recognition, you can use these services instead of having to write your own every time, you even get language comprehension services to get meaning and context out of conversations. Machine Learning does require a fairly large amount of computing power, but all three platforms that we've just talked about, offer various ways of delivering just this, whether it is a large memory usage, like hundreds of gigabytes, massive computing needs, or even field programmable gate arrays, or FPGAs that you can program to create custom hardware accelerations for your machine learning applications. Now there are other offerings as well, and often developers will use the R language like the letter R to create machine learning models and interact with them and manipulate them. Yeah. Of course. We're going to talk about the future of tech or the future of machine learning as well. Otherwise this show would be false advertising, right? So in the last couple of shows, I talked about specific industries that could take advantage of both augmented reality and quantum computing. And of course it was, you know, not an exhaustive list, but you could almost visualize how those technologies would impact the future. With machine learning it can be much more subtle, as you know, it is a powerful technology lurking in the background. Machine learning is a technology that data scientists primarily use to explore new computing frontiers. It is what make Alexa better and understanding your questions and dialect. Thus saying that machine learning will impact consumers with shiny buttons and furry unicorns is not really that accurate. So instead the products and services that we use every day, will most likely get even better and more accurate and faster and cheaper to use. We just maybe won't realize initially, but we will get more appropriate ads from websites. Yeah, I know ads, but we will see some technologies advanced, faster and faster. And we can predict patterns from very large data sets. You know, think all the traffic data in New York, like we were talking about before with Google Maps for, you know, a year, large, like we can get more efficient solutions to everyday problems. So before we end, I would love to hear more from you on what do you like about the show? What do you want to see more of? What don't you like? What do you want to see more of my legs? Whatever. How can I give you more of, you know, how can I find that last piece you're missing of your, My Little Pony collection. Now you can find me on Twitter @larsklint or through the A Cloud Guru Facebook and Twitter accounts of course, as well. So thanks for watching the Future of Tech, the show that holds the largest data set of My Little Pony accessories. [Inaudible]. 