 hi i am simal li from mit csail i will talk about our recent work on differentiable vector graphics rasterization this is a collaboration between mit c-cell and adobe vector graphic is a compact and resolution independent mathematical representation of shapes that is commonly used in user interface web web design logos phones art google map etc to show a vector graphics on the display we need to rasterize it to a raster image despite its popularity there are not many previous works on editing vector graphics most of them focus on the geometric properties of the vector graphics like how do we represent the parallel lines or how do we represent repetitive patterns more recently there are works of machine learning where people want to teach machine to draw vector graphics this is a hot direction but so far most methods either require vector supervision or brute force reinforcement learning one problem is that we can't apply the powerful convolutional neural networks to compactor graphics because they lack the structure of the restore images on the other hand raster image processing and learning has advanced a lot in the last decades in addition to the more traditional computational photography methods like retargeting or texture synthesis we can now learn very complex structure from lesser images using convolutional neural networks our goal is to bridge the gap between the vector address or graphics by connecting them using a differential vector graphics rasterizer once we have artificial parasitizer we can unify editing and learning of vector graphics propagating learning the editing and learning uh propagating the editing and learning of our wrestler images back to the vector graphics using gradient decent update in this talk i will discuss two questions first how do we compute the gradients of vector graphics rasterization second what can we do with our differentiable rasterizer i'll start with the first one to compute the gradients we borrow the idea from our recent 3d differential rendering work our key observation is that anti-aliasing makes rasterization differentiable even though a point sample might hit or miss a shape after anti-aliasing the pixel filter integral changes continuously as the shape deforms differentiable rasterization leads to some requirements of our rasterization algorithm firstly some rasterization organisms need to convert shape formats in non-differentiable ways we don't want to do that next because we are using the differential rasterizer in some optimization we want minimal approximation since the optimization process can produce arbitrary shapes finally as we mentioned earlier we need to support anti-aliasing and differentiate it based on these requirements i will discuss our different discussions of rasterization organism i'll start from the representation we follow the scalable vector graphics representation or svg which is the standard representation of vector graphics using practice in svg we can define some closed curves or open curves and we can feel curves with colors or draw strokes with the curves these curves are usually defined as polynomials or rational polynomials like bayesia curves or ellipses they're usually defined in parametric forms so we have a function p that maps a scalar t to two d positions the most popular curves are quadratic or cubic basis curves which are correct of cubic polynomials over t for efficiency and convenience many previous rasterizer would convert strokes into field shapes unfortunately this process is not differentiable because number of segments of the field shapes depends on the stroke parameters we do not want to do this instead we will render the shapes directly without conversion to render the shapes we rely on the inside outside tests inspired by knee hub and hopi's work at 2008. given the point of the curve we test if the point is inside the curve for strokes we need to find the distance d between the point of the curve and if this distance d is smaller than half the stroke width w then we are inside the stroke for field shapes we trace a ray from the point and see how many times we intersect the shape if it is odd number of times the point is inside the shape otherwise it is outside for computing the distance d for strokes knee hub and hopi relied on approximating the distance unfortunately this approximation fails when the stroke width is large this is fine for them because they assume the stroke width is small in practice however since we're using this as an optimize since we're using this in that optimization we may want to have large strokes instead we need to compute exact distance to curve without approximation the separation is not very well discussed in the literature for qubit curves to compute the exact distance between the point q and the curve p we want to find the closest point p of t star such that the square of the l2 distance is minimized so this is a 1d optimization problem to find the minimum of this function we take the derivative and set it to 0. for qubit curves this becomes a fifth order polynomial equation we know that there is no clause form for solution for the fifth order polynomial roots so we have to rely on iterative solvers we propose an iterative solver that will find all the rules of this fifth folder polynomial by separating all the blues into regions and do neutral iteration in those regions see the paper for details as i mentioned earlier we need to support anti-aliasing and differentiate through it we provide two anti-aliasing options that have different characteristics our first option is based on monte carlo sampling where we discrete where we discretize the anti-aliasing integral and into discrete sum by sampling the domain our second option is based on approximating shapes using the closest point to form a half space this allows us to analytically approximate the integrals the monte carlo solution is higher quality but slower and the half space approximation is faster but can produce some artifacts for example what notorious artifacts of the hash-based approximation is something people call the conflation artifacts if you have two shapes that overlap on part of the boundaries you'll see something like the image on the right modern gpu rasterizer don't use half space approximation because of this artifact still many cpu rasterizer are using it because it is fast unfortunately we cannot apply automatic differentiation to either of these two options if you apply it to the monte carlo sampling method we get incorrect derivatives if we apply it to half space approximation we get the correct derivatives but it will use an enormous amount of memory i will discuss why this is the case for multiple sampling first and how we deal with it the problem with automatic differentiating when in color sampling is that the molecular samples do not really land on places where interesting events happen suppose we are deforming the yellow shape all changes are happening at the boundaries and multicolored samples have a zero probability of detecting it if you ask one of these multicolored samples they will just say the derivative is zero so what we do is to explicitly sample the boundary to detect the change when computing the derivatives if we ask if we ask one of these boundary samples what is the derivatives they will not be able to say after say that after the deformation there will be more yellow and less white or red so we can compute the derivatives the math of the derivative the math of the derivative can be derived from this reynolds transform theorem we have another presentation at sig graph asia related to this check out cyber rules talk about unbiased warp area sampling for differential rendering it's also similar to our previous 3d differential rendering work but we handle polynomial curves instead of triangle meshes so we resolved the incorrect automatic differentiation of monocular sampling by sampling boundaries now i will explain the problem of differentiating the half-space approximation automatically differentiating the half-space approximation does give us the correct result because we simplify the integral into a differentiable expression however this means that we need to differentiate through the distance to curves and for cupid curves or any curves that doesn't have closed form distance we need to differentiate the iterative solvers remember that we can turn the distance into some optimization problem where we find the rules of the derivatives let's call the derivative function r and it depends on some curve parameters alpha we want to find the t star such that the derivative function evaluates to 0 and we want to differentiate this process if you have an iterative solver for r and if we apply automatic differentiation to it automatic differentiation will create a tape that records all the intermediate values so that we can back propagate through it later this creates a huge demand on memory because we have to do this for each pixel we do not want this instead our solution is to apply implicit function theorem given an implicit function like r implicit function theorem tells us tells us directly what is the derivative between the parameters of r we can use we can then use this information to back propagate to curve parameters alpha this works because we know what we know we are at the root of the function but autodiv does not know about this using this approach we don't need any tape to sort the intermediate values so the memory usage is constant to recap i showed that automatic differentiating at the aliasing gives undesirable results and we need specialized solution for correctness and efficiency let's review our requirements of our rasterization we don't rely on any non-differential conversion because we use inside outside tests furthermore we avoid approximating distance because they can fail for some bad parameters we support two different kinds of differentiable anti-aliasing we cannot rely on automatic differentiation because they are wrong and inefficient instead for monte carlo we apply boundary sampling and for half space approximation we apply implicit function theorem to avoid storing tapes now we have our differentiable rasterizer we will answer the second question what can we do with it turns out we can do quite a bit many of them are new vector graphics editing and learning operations i will discuss them one by one one by one one of the first thing we build using our differential brass riser is an interactive brush space editor given the vector graphics and a brush we do gradient descent to increase or decrease the opacity within the brush this allows us to sculpt our vector graphics to a desired shape we can also combine this editor with geometric constraints like saying we want some of the lines to be parallel etc we can solve these constraints using standard optimization tools like projected gradient descent or lagrange multipliers as a fancier example we can also use a secret a secret shaped brush for editing application we found to be useful is to refine image recognization results given the raster image we want to convert it to a vector image there are many tools out there we can use to achieve this for example adobe illustrator has this feature called image trace what we can do is to take adobe image trace result and further optimize it using gradient descent we found that we get we get significant significantly more accurate vectorization by doing this here's a video showing the optimization process and here's an arrow map we obtain much lower arrow both around the edges and textures we tried out a few images and found that on average were better than adobe image trays for about three to four decibel in terms of psnr another cool application is that we can simulate what an image look like if it is composed of many strokes given the target raster image we start from some random distribution of strokes and then we optimize for the parameters and this gives us a painterly looking because of the strokes we can fit the strokes using different matrix we found that l2 gives a more faithful look and the deep perceptual metric gives a more stylish look we can also apply image processing operators on the raster render and propagate the change back to the vector graphics one example of this is sim carving which is an image processing operator that changes the aspect ratio of the image by removing pixels we take an input vector graphics when we render it and then we apply one step of seam carving and then we propagate the change back to the input using great and descent and then we repeat this process so here's a video showing the optimization process finally we also did some early experiments with deep learning we want to take wrestler training data and learn the vector structure inside them we train this vector auto encoder so the network takes a raster training image and produces a vector graphics we then take the produced vector graphics and render it with our differential rasterizer we then compare the raster rendering and the input and propagate the gradients to the network weights in this way we can learn the network that produce vector graphics given raster images we can also sample from the network to generate new vector graphics by tweaking the latent variable in the network so given the target resonant images on mnist our network can output vector graphics like the one on the right these factor graphics are infinite resolution compared to the target raster images alternatively we can generate a generated virtual network so instead of taking the raster image as an input we train a classifier to classify between the rendering and the raster training data so here's some sample from our back again generated by sampling the latent variable these results are fine but gains are more difficult to trend so we don't get results as good as a variational autoencoder because now we can sample digits in latent space we can also interpolate them if you ever wonder what the integer between 0 and 1 looks like here's it one major limitation of our work is that we do not have anything we do not do anything to the vector topology this is both a blessing and a curse real vector graphics has complex topological structure like the one we show on the right we preserve this structure during the optimization but we do not synthesize more maybe some language processing models like rnn or transformer can help generating the topology so that's it in conclusion we show that there are promising applications we can do with differential rasterization technical wise differentiable rasterization requires us to rethink factor rasterization and automatic differentiation the code is available at our project site thanks 