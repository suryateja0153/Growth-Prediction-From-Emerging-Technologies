 [Music] neural network is not a plus but a effective way to lose the interest of the audience because everybody knows it so i promise you guys i'm not going to add more evidence to the success of big data and uh this computational paradigm uh especially for speech coding uh we're talking about this um scenario uh for the hardware uh on device application where the computational resource is at a premium so we cannot really afford a very large model that's why we're talking about efficient scalable solutions with neural network um i guess most of you have already uh worked on speech coding audio coding for years for some of you maybe for decades just to make my presentation self-contained i still want to use one slide to introduce speech coding from a very high level let's say right now i'm speaking and the signal is transmitted to the listener side whether in seattle or in germany and the listener you can make sense of the talk the speech itself is not directly sent through the internet it will be first compressed into this bit string which is transmitted to the other side the decoder will take the bit string and reconstruct the speech signal the hope is to achieve as much compression as possible at the same time we also want the decoded signal to be perceptually similar to the reference signal uh let's say the sample rate is 16 k hertz which means for each second there will be 16 000 samples to represent the signal if each sample is represented by a 16 feet fixed point that means the beat rate is approximately to be 256 kbps that's a very large bit rate usually with a advanced codec we could bring down the number to 24 or even 9kbps let me play some other samples i know it's not a perfect way to do this but you see my point i mean even at 9kbps a very a great compression uh we may not um feel the difference i mean for 9kbps you could but for 24kbps the quality is almost transparent iguanas and alligators are tropical reptiles iguanas and alligators are tropical reptiles a non-contributory plan usually won't pay off for the worker until he retires a non-contributory plan usually won't pay off for the worker until he retires so the design of this codec is not an easy task because we have to make many trade-offs to make this happen a good code back is expected to be scalable we don't want to code that that can only function at a specific setting for instance when we don't have enough computational resource it should perform reasonably well at for example 9kbps but given more resources the performance should be correspondingly increased that's the scalability of course lightweight uh when we talk about edge applications um if the model is too big it just cannot be handled by the hardware this is people do use deep learning uh for speech coding for instance a wavenet based model is a one of the methodologies but the model itself is just too large it has more than 20 million parameters which is usually not visible of course we can conduct network compression or some other techniques to make it smaller or we can even use an integer to represent each modal weight still uh why don't we start from a simpler model topology so uh our work here is to not uh further push the capacity of neural network we've seen that and i personally don't have the intention to do so our methodology is to bridge the gap between this conventional dsp methods and deep learning as a computational more than computational paradigm to make this new models more efficient um this is not really a straightforward way because there are multiple challenges for instance if we use a neural network to compress something maybe an autoencoder is a default answer as the first step but as you know this is not sufficient because the bit rate is a multi-product it's a product between two uh items the first one is the amount of samples uh to be transmitted per second the second item is the amount of b to represent each sample with this all encoder so all encoder is this type of neural network uh which tries to reconstruct the input because of this middle layer this very narrow bottleneck in the middle it can learn a compact representation of the input however it could reduce the size of the feature map or the first item here the amount of samples per second but each feature is still represented by a 6 16 bit fixed point so it doesn't benefit the big depth reduction that's why only using an auto encoder is not sufficient for speech coding or audio coding here if only we could for instance use one out of 32 kernels to represent each feature in that case it's sufficient to just use five beats instead of 16 to represent each sample i mean we have to include quantization to make this neural network work this is one challenge how to make a autoencoder neural codec another challenge is that uh when we talk about end to end speech coding the network learns everything from scratch this is not an efficient way to uh model speech because we know that according to source builder model a speech generation process include sources and filter if only we could outsource certain sub-problem to an efficient conventional artisan maybe we don't need a very large neural network to deal with the rest part but how to do so is still an open uh problem by the way i really think this is end to end is a trending term but it also can be an abuse of dnn because for speech coding as you will see later we don't really have to start from scratch third the third challenge is the discrepancy between the loss function and the actual measure used by human listeners to evaluate the sound quality when we train a network we have to resort to some type of objective functions like mean square error or things like that but eventually um the decoded samples are measured by a task or for audio samples we need to find human listeners to collect subjective scores if we use a sub-optimal loss function to train the model usually the model is not efficient enough to compress the data so in this presentation i will start from a trainable quantitation scheme that can convert an autoencoder to a neural codec from there i want to talk about cascaded cross module residual learning scheme to achieve scalability without uh making the model go deeper then i will introduce a concept called collaborative quantization where we borrow a highly mature technique from dsp to make the model even more efficient with even better quality performance last i will talk about a better loss function for audio coding where we employ psychoacoustic calibration to train the model let's start from the quantization part let's say now we have this output from an encoder which is uh vector h the hope or the goal here is to map each value in h to uh one kernel in this kernel vector b transpose uh to do that let's first calculate this pointwise distance uh which gives me the matrix d usually um with this soft max function um d can be transformed into a probability matrix a uh it could be much uh easier if i could use the pointer here i'm not sure if i can see uh where my where the pointer is let me try one more time okay here let's take a look at this value in point two great this is corresponding to the second row uh here and the hope is to pick the one with the highest the probability here which means point one in this kernel vector that means eventually point will be replaced by 0.1 in this kernel vector during quantization so there are multiple transformations here we first calculate the pointwise distance which gives me the matrix d with soft max function d becomes a probability matrix a if i multiply this d by a negative scalar alpha then we could simply pick the one with the largest the probability here of course to make this happen this matrix a will need to be binarized um so that if i multiply a hard with b we're picking the corresponding kernel in this b transpose vector so if i'm interested in point two here eventually if the a hard matrix is this uh for the second row only one value is uh with the probability of one and the others are with the probability of zero so um this value point one will be selected now the problem of this operation is that it's not differentiable if i include the operation like this into a model trading procedure the propagation organism will be disabled one way to get around this is to stick to a soft version of a during model training only in this case um this value is not one and unique kernel in b transpose but a mixture of all these four kernels um at the test time we will convert this a soft matrix to the corresponding a hard matrix to make sure the quantitation is conducted you may wonder will the performance uh drop a lot at the test time if you replace a soft by a hard yes that's why there's another maneuver long story short uh if we could make this soft version as close to the hard version as possible at the test time the performance drop will be minimized one way to do so is to try is to increase the absolute value of this scalar alpha as you can see if the alpha is a negative one the probability is this if i choose the one with the probability .31 over point 26 it is correct but the model is confused it's not so deterministic when it makes the decision but if i try a larger alpha in terms of the absolute value we could see that the greater the greatest the probability uh becomes even larger while the others are getting closer to zero in practice you could start by setting the alpha to be negative 100 or even negative 300 so that uh for each role there will be one value that is almost one and the others are almost zero okay so this is a uh the introduction about the scalar quantization remember this is trainable because the b transpose is a variable in your neural network and your encoder will also be optimized to generate this h vector this output that can be favored by your quantizer if i place this quantizer into that out encoder this can be a neural codec during encoding um the size of the feature map is reduced during quantization we could also reduce the beat depth so both two items that are related to the bit rate can be reduced that's how we reduce the bit rate uh i want to add one more thing uh this quantizer can be coupled with entropy coating as well remember if we have 32 kernels it's sufficient to use five beats to represent each sample but if you try huffman coating on top of that the average b can be even uh smaller to represent each sample and the decoder usually is another part of the neural network with the mirror topology this solution is end-to-end the input is just the waveform of the speech and the model is trained to recover the waveform this is a very straightforward way um actually uh if we implement this concept in convolutional neural network the design can be compact um it only contains 0.45 million parameters one of the topologies can be this um let's say each frame contains 512 samples the input will go through some arrestnet blocks followed by a down sampling cn layer by setting the stride value to b2 and this is where we reduce the size of the feature map this is followed by two more uh resnet blocks eventually we have the code vector this is where the quantization kicks in the one that i just introduced followed by huffman coding to give the bit stream the bit screen is transmitted to the other side where the decoder recovers the input and there if you calculate the loss you could update the parameters in between to optimize the problem this is a very straightforward design uh by using an autoencoder and a trainable quantizer um the topology is also summarized in this table i don't want to go through the details of the table but from this table you can calculate the total amount of parameters for that network which is about 0.45 million uh it's still much much bigger than a conventional dsp based uh codec but uh uh when we were talking about deep learning this is one tiny model because of the usage of rest net we could well the original model like this has 1.6 million parameters because of resnet we could bring the number down to 0.45 so i guess so far you're already familiarized with the idea of using a trainable quantizer and straightforward cnn based neural auto encoder to make a codec now the question is what if the hardware can handle a larger model say it can support 0.9 million parameters not 0.45 how can we achieve a better performance or scalability here the default way is to enlarge the kernel or making the model uh go deeper which is not fun uh we have a better uh solution which is called cross-module residual learning uh i will just use this um diagram to illustrate the data flow uh as i just talked about the input is x it goes through the encoder and the code layer which gives me the code vector h and the decoder predicts the input from there we will make the model larger and deeper we just stack another auto encoder with the same topology the second auto encoder takes the residual as the input the residual means uh uh what is not reconstructed from the first encoder so uh the input of the first encoder is x and the prediction is x hat the input of the second one is um is this which is the residual part we could stack even more um so and so forth let me do this one more time we start by first all encoder make the prediction calculate the loss to update parameters only for the first autoencoder and then the second one was its own last term and we could also conduct a optimization just for the second auto encoder eventually the prediction will be the sum of the output from all encoders and during fine-tuning we could use this loss to update all parameters simultaneously um we compared the snr and the task for the validation data set i will just uh tell you this for our cmrl model it has two auto encoders which means uh the prime there are 0.9 million parameters this is compared against a single model which is with a similar topology but just larger kernels and everything which gives about 1.6 million parameters in both snr and task cmril has better numbers the major observation here is that the solution or simrl can organize a bunch of lightweight auto encoders that can outperform a big one big autoencoder even with the combined complexity the idea the idea is really not new um conventionally people to this concept as multi-stage vector quantization but we bring this concept to deep learning and show that this could achieve a higher motor efficiency for instance if your hardware cannot handle a gigantic neural network if we could decompose a big one into several smaller one with sumo l maybe we don't even need that amount of parameters to achieve even better performance this will the that is the first part of the talk um i combined the self-default quantitation with this uh conventional concept multi-state vector quantization into this cmrl concept um this the the method here is still based on end-to-end uh speech coding uh methodology um this is not optimal because like i said at the beginning of the talk we don't have to model everything from scratch that brings me to the third topic of the presentation collaborative quantization i really like the way i don't throw dsp out with the bathwater um so in this source through the model um the speech production process i think multiple sources and filter so the glottal air flow is being filtered by the mouth and before being generated from the lips it's not known exactly how to use a computational model for the sources but we do know that the response of the vocal tract the filter can be efficiently modeled by a conventional dsp organism here in this figure the response of the vocal tract is just the spectral envelope that organism is called lpc linear predictive coding with this levison derby narcissism we could learn this envelope with a very few amount of beats usually one or two kbps will be sufficient for this task of course the estimation is not perfect it's okay the error is called lpc residual we could use more powerful models to deal with that part uh this is not new again because usually for a codec it uses lpc as a preprocessor and for the residual part depending on the scenario it could be a recorder it could be a lossless coding orgasm of course here we're focusing on waveform coding so uh we argue we could just replace our cmrl model here to estimate the residual and the estimated residual will be used along with the lpc coefficients to synthesize the speech and this is how to say this method is not really sophisticated enough because that means we simply shoehorn lpc to a neural network we just follow the lpc artisan from the shelf say uh from mr whiteband to cmrl the lpc part is a deterministic origin so there's no interaction between lpc and cmrl now the goal for collaborative quantization here is to make lpc a trainable block in summary l why do we have to do so think about this if we assign too many beats to the lpc part the residual will be very soft and noisy because the goal for lpc orgasm is to minimize the energy of the residual signal um another uh edge case is uh when we don't locate any b to lpc in that case it's just an end-to-end solution because that's where we model everything from scratch between these two edges uh maybe there's a sweet spot to find that sweet spot we have to make these two parts interactive with each other that's why we propose collaborative quantization if the lpc part is trainable it can decompose the input into the residual and the coefficient the residual will be the input of the neural network and the estimated residual along with the coefficients uh will be used to generate the reconstructed speech if i zoom in this part i want to illustrate why this lpc analyzer can be a trainable component again the input i will go through the high-pass filter pre-emphasis filter as usual and we use lemons and derming orgasm to calculate the coefficient here the quantizer for the coefficient is soft to hard quantization remember that part is trainable so if we combine this trainable lpc analyzer with mrl by setting a target heat rate for instance 24 kbps the model can find the right amount of beats to be allocated to lpc and the rest part for quantizing the residual this is achieved by collaborative quantization in fact in the past score we could see that cq has a noticeable margin against lpc crl where we just um add lpc as a deterministic preprocessor to the neural network uh see from here the passive score is hugely improved uh for 9kbps and 6 kbps uh 16 kbps although in terms of the task score um they're not as good as opus however in a mutual test um at 24 kbps both cq and lpc simrl outperforms outperform opus at the same b rate setting although at 9 kbps well the lpc summer l model is not as good as mr whiteband cq is kind of in a par with mr white band again uh i need to emphasize this our goal is to not really outperform this conventional codex with a large margin because we intentionally choose a very compact neural network for this problem comparing with weight net and its own predecessor k-net cmrl only has 0.9 million parameters so it's a tiny new network let me play some other samples here iguanas and alligators are tropical reptiles iguanas and alligators are tropical reptiles iguanas and alligators are tropical reptiles i guess the reason why the listeners are in favor of our model is because um the dakota sample from opus sounds a little bit norfolk because the energy um at high frequencies are kind of lost um but at 24 kbps i guess they all perform uh very good and i don't think we could tell the difference without a good headset let's see a non-contributory plan usually won't pay off for the worker until he retires a non-contributory plan usually won't pay off for the worker until he retires a non-contributory plan usually won't pay off for the worker until he retires a non-contributory plan usually won't pay off for the worker until he retires a non-contributory plan usually won't pay off for the worker until he retires yeah it's just for me it's just hard to tell i guess uh it can be even worse for the listener side given that this signal will also be compressed and then decoded anyway so far we've talked about two uh of our previous work cmrl and the cpu collaborative quantization that brings a conventional lpc organism into a neural codec let's move on to the last section of the presentation here i extend the neural codec to audio signals because first speech there have been effort to implement a story like loss function or task like loss function but for audio quality i don't know if there is a good objective measure that is equivalent to human auditory perception for instance if i employ the previous neural codec directly to audio coding the performance is far from um satisfactory actually i recorded this uh um auto click by myself in a studio because i don't want to offend the singer if i mess up the audio quality so i performed by myself this is at 168 kbps which is a really high bit rate setting but the loss function is just based on mean square error it's just not perceptually salient enough as you will see and the performance is not good [Music] um we have this um decoded samples online but i hope that uh you already figured out that the decoded sample from c m r l uh is not comparable uh for now sorry that's where we resort to uh psychoacoustics because traditionally it helps many audio codecs such as mp3 to achieve coding efficiency basically not all sound is treated equal by our human ears say some sounds are too soft at certain frequencies so it's inaudible even though the sound does exist so this one is inaudible i guess this one is uh this one is still audible though um given a certain um frequency uh the loudness of the sound will determine if the sound itself can be audible or not um if i play uh two tones together sometimes uh you could tell yeah there are two tones but sometimes when uh the frequency of these two tones are too close together uh the interfering heap is masked by the major tone your ear will barely distinguish one tone from the other let's see uh i guess you could tell that there are two tones but uh let me see yes there are two tones not sure i i guess some people will say uh there's only one tone if i visualize the spectrogram right you see that if this frequency of this interfering beep is very close to the major tone it will be masked by that tone this masking effect is studied in psychoacoustics basically if the tone is too soft at a certain frequency it can be inaudible if the tone right next to a loud tone is also kind of soft it will also be masked in autocoding what i mean is this for for this for this tone we don't have to uh assign too many beats because it's masked of course this can lead to some quantization error or this artifact as long as the artifact is also masked by this curve or good it won't really hurt the subjective experience inspired by this psychoacoustics we implement the cyclocrystal model in python the model uh detects uh the total massacre and the noisemasters the corresponding masking curves along with the absolute hearing threshold which is this red dashed line we could get a global masking threshold which is this uh red heart line with this global masking threshold long story short we could inform the model to pay more attention at certain frequency beams while being easy otherwise so the beats are located in a wiser way now uh if i play the reference again [Music] and even at a lower b rate case of 112 kbps [Music] so this is near transparent according to the subjective score uh collected from uh mushua test so sorry anyway the difference is also explained in these two figures um so uh the power spectrum of this artifact uh i'm sorry the power spectral density of the artifact is well below the global masking threshold which explains why the artifact is less notable that's less audible in this new model i'm not sure but i tried to make this presentation aligned well with some ongoing project and the group and the company um but i don't know if i can do that i do know that at microsoft uh people consider input from multiple uh modalities even for speech uh we have asr model and some other models for uh speech enhancement and stuff these models are already implemented in the neural network i'm not sure what's going on for coding but if a coding orgasm is also implemented in this relative method at least we could fine-tune all these components simultaneously hopefully to achieve a better global solution although i have no comfort tool that this would be a possible assumption but yeah this is my presentation uh i remember the goal is to bridge the gap in between to improve the model efficiency for ad applications thank you correct very interesting talk so questions [Music] no questions then i'll start with a question um so going back um to your um the network that tried to learn a linear prediction um was that really a linear function that you tried to learn or was it also um could it also be a londoner function and would that make sense and in that context yes i probably should have uh clarified that a little bit more um right now when i say the lpc part is trainable only the quantization part is trainable i didn't really change any other components here the high pass filter the pre-emphasis filter they are just borrowed from mr whiteband [Music] so i i think it's just a linear model if i understand that correctly and so so what what did you your network predict and did it predict kind of the linear prediction filter or or did it simply output like the residual or something like this right uh yes the output of the cmrl is lpc residual the residual which is this very soft uh this uh very soft whey and orange uh color uh this along with the coefficients will be used to synthesize the speech so the output of the cmrl part is residual [Music] to be frank uh quantizing the residual is even more challenging because i guess the structure for this noisy residual is uh much harder to be learned as opposed to the raw pcm waveform so if i calculate the snr it's much much lower but when we synthesize the speech although the passive score is also lower it just sounds better but that's why uh you you could tell that there's a discrepancy between uh the comparison in terms of the task and the comparison in terms of the subjective score the passive score is not as good but listeners are in favor of our model more questions oh uh i have some questions on the training on the uh cmrl uh yes uh i'm just curious no so have you tried to train end to end maybe potentially we might have the lower the learning rate for the deeper layers so i'm just just curious have you tried to train this all these networks uh end to end uh yes uh first uh this cmrl uh is trained in an intune setting second you are right uh the learning rate for the second and the rest all encoders uh is much smaller than the one for the first auto encoder and it's even for even smaller uh during fine tuning for instance if i if the learner is one e minus three for the first one you could use one e minus four for the second one and the one e minus five for the fine tuning because when during fine-tuning this parameters are already initialized in a decent way if it's tuned with a very large uh parameter the system can be collapsed or it's not stable that's why empirically which shows a much much smaller learning rate sorry i might miss something so i joined the meeting in the middle so uh when you train uh the second module for example did you freeze gear the parameters on the first module or you just uh change the running rate when i optimize the second auto encoder the first one is freeze uh so uh there are two rounds uh the first round conducts the greedy training scheme so i take turns to train each of the auto encoder from left to right during the second run we conduct fine tuning where all parameters are updated simultaneously okay thanks right more questions and i have another one okay um so you talked about this integrating this psychoacoustic model and in the end and this was all kind of um there was no um training involved right that was all um basically a hard-coded model so did you have any ideas if there would be a possibility to kind of learn such such uh psychoacoustic effects in a neural network model i yes actually i i thought about that although this is just a highly immature idea so first the calculation of this global masking threshold is deterministic but it is integrated in model training so there are some lost terms to liberate this but uh can we really learn this masking curve i think there is a topology called a clone network um and there is a learning mechanism which is called uh self-supervised learning so let's say if the input or samples decoded from different codecs opus if we could a hidden representation from this clone network the input are different because they're from different codecs the the header representation should be equivalent because perceptually they sound almost uh perfect i'm not sure that makes sense to you uh let me uh put it in another way so if we could uh define a distance or a loss in a hidden space from a clone network maybe that could be um comparable to this um psychoacoustic curve i guess oh i i just uh yes it's uh not the mature idea but i i thought about it a few days ago yeah yeah yeah that is kind of an interesting thing so it could also be something like uh with reinforcement learning um but uh the it's it's probably a very difficult problem because you can only get in the end you want to get uh feedback from humans right if if you want to adapt those masking thresholds online for example but it's it's probably a very terrific tricky problem to solve but yeah it could be something to think about right great more questions okay let me see if i include my email there but i uh uh if you uh later find any questions you want to ask me feel free to send me an email i'll be more than happy to answer the question yeah i really enjoyed the talk it's really nice to see someone not building larger networks but actually trying to increase the efficiency and that's actually getting more and more important and us also these networks become more practical and more tractable to be used and in edge devices and stuff like this so yeah really really interesting work and a nice perspective um so if there are no other questions i think i'll stop the recording how do i do that and 