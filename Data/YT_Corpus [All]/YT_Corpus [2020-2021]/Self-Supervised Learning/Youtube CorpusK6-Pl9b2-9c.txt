 Adrien Scheuer: Yes. Okay. Zhilong Fang: Yeah, well, we can see your screen. Bas Peters: Oh, you see my mouse. Zhilong Fang: Pointer. Yeah, yeah. Bas Peters: All right. Okay, well thanks for inviting me. It's always Bas Peters: Nice to get the opportunity to share some of my work and maybe have some nice discussion with some Bas Peters: People work on different applications. So today we'll talk about constraints optimization for weeklies to provide deep learning in case you a few data and fewer labels. Bas Peters: So essentially, I work on the question, what do you do if you don't have enough label data to train deep neural networks can you use something else then annotations that were manually created Bas Peters: So, Bas Peters: Learning from a single example is possible or a few examples and a few categories that are Bas Peters: often mentioned are like where you work with partial annotations. Bas Peters: But if you don't have that you can work with weekend notations more like higher level information like image captions. For example, like a text description of what's in the image. Bas Peters: And another alternative Stewart formulating says constrained optimization and I will discuss mainly this category. And I'll show some applications in remote sensing seismic interpretation and image processing. Bas Peters: So the basic idea of supervised learning for semantic segmentation is that we have just an image, and we want to train and network that out could say a semantic segmentation. So every object of the same type, gets one color. Bas Peters: Of it seismic data, you can do the same thing where you have a seismic image and you every sort of geological type of the same class gets the same color doesn't matter where in the, what we are. You are just if you are have a every pixel with the same class gets the same color. Bas Peters: So you started some notation. What are they tried to do Bas Peters: In the is trading network. So I Bas Peters: Indicated by just a nonlinear function g, that depends on okay that are the network parameter Seiko delusional kernels in a golf delusional neural network and some input day that the So D would be say to the image that you plug in Bas Peters: And then training a no network in this notation just means minimizing the difference between the output of the network. So what comes out of your function g and compared to the label. See, so these would be the label Seta Bas Peters: The goal. What you want to try to match an L is data loss function. For example, cross entropy for these type of applications. Bas Peters: So semantic segmentation, then Bas Peters: There's the size of the network. The mapping is if you have an pixels and your image, you would go from and by the number of channels in two Bas Peters: And times number of classes out or if you factor is it you just take the product of these two. So for example shows what will happen. So you have one input image that could be three RGB channels, if they're still classes, you get two outputs. Bas Peters: Images of your network, each of them, showing the probability that Bas Peters: This class occurs at this pixel. So this would be the presence of an object class and this would be the breath and self background. Bas Peters: And then the knife sharp images are typically is the result of trash holding just post processing and looking at each pixel and deciding what's the most likely class at that pixel. So in this case you just take the maximum of the two Bas Peters: For this example, where you have RGB input, you would have three channels input and pixels and the output would be in this case 12 years 12 classes. Bas Peters: And, of course, also for an pixels. Bas Peters: So there'll be a map from three and 212 and Bas Peters: Of course, this Bas Peters: Well, everybody can kind of annotate these these images. There's not very difficult. Of course, very time consuming and therefore costly. Bas Peters: And indecisive case where the data is not facial but experts have to look at it, it would also be potentially a reliable because it's not it's not obvious why this what in these types of Bas Peters: Non visual data types. Bas Peters: So what you can do to reduce the cost. There's a couple of options. One of them is, for example, point annotations that's shown in the middle here where I just asked people to click once on every object type. Bas Peters: That will give you a few pixels at very low cost. It's very fast. Bas Peters: And other alternative using squiggles, so he just real simple line true an object and you get a lot more pixels. And it's also quite quick Bas Peters: This is of course also easy. Most of the time, unless you're very narrow curvy object works. You have to still trace accurately. Bas Peters: So it is sort of works in general image recognition classification segmentation type work. Bas Peters: If you have like hundreds or thousands of Bas Peters: Images with one point or a few points, they can think of this will work because you've got into total amount of labor information you have is still a lot. So the question is, does this still work. If you have just a single or maybe a couple of images with some point and notate it Bas Peters: So let's take a look at a simple example where we have this is Bas Peters: Most of the state of Arizona and land few. So this is just a topographic map of, say, Arizona, and then on the same map. You've got just gravitational magnetic gravitational data, but also discrete data like rock agent types. Bas Peters: So in the network, you would split these maps out into one map per rock type of rock age because otherwise this is not like permutation invariant with respect to the color map so you cannot use it directly. Bas Peters: It is would be if every large to the data case with 56 channels. So there's like 20 or so in each of these ones. Bas Peters: And Bas Peters: The true aquifer map. What I have here is really what people have constructed so that's not ground through this. So people have worked on over years or decades to construct that Bas Peters: And suppose you have some point annotations say information from boreholes for people have trialed or investigated what aquifer type is there or what is not there. So it is a one image with some points. It's a large image dope. Bas Peters: And the result prediction here is you can see here the difference between the map that is constructed by people, and by the machine. Bas Peters: So this is really an offense interpolation extrapolation task and they match quite well. So it's a it just shows that even with one image and just partial notation, you can construct something that Bas Peters: Is is a good segmentation. Bas Peters: So you don't need hundreds or thousands of images. Bas Peters: Now there's another example where you have a video. So that's a 3D array. And suppose we're just giving a few times slices of the annotation. Bas Peters: Then it is still possible to just training on that video, so no large data set just trained on a single video you can train a neural network to segment. Every time slice Bas Peters: So this was a 3D convolution on network that we trained Bas Peters: And with seismic data you have similar situations where you don't have full annotations, you often have just in one slice. If you're lucky to bore holes that gave you some information. So these colors will be again the little logical units that you're interested in. Bas Peters: So well supervised learning is, of course, learning from example. So we can actually learn without ever seeing even one fully annotated example as it Bas Peters: But you still need sufficiently many points, lines or patches annotated Bas Peters: So there's no time to discuss really why why you can do this, but Bas Peters: I can just mention a few words that this is actually very similar to what you're doing to physical in first problems like seismic for way from a version. Bas Peters: Where you got the one single model and you sample sparsely at the surface say but receivers. So two receivers are mathematically equivalent to the point annotations. Bas Peters: And the reason why you can segment. Everything is that, just like the way field goes everywhere, regardless of how sparsely you sample it. The same is true for the states in the network that propagate from the input data to everywhere to your image. If you have a deep enough network. Bas Peters: And the back propagation is also the same digest originates from those Bas Peters: Receivers and for equivalency from to point annotations. Bas Peters: So learning on one single example follows all the same rules as into physical universe problems. Bas Peters: So, so that was about Bas Peters: sparser annotations to speed up annotation processes or just to work with limited crowd through, but even that's not always possible. For example, if the ground truth has to come or the labels have to come from ground truth and it can be too expensive for sometimes Bas Peters: Like medical data seismic data you have, not everybody can do it, you need to get experts to annotate and another problem is, of course, in a Bas Peters: historical data and time ferrying settings you can go back in time and collect crowd through, for example, time lapse monitoring and hyper spectral data sets, you can go back and figure out what was at a certain place on Earth. Bas Peters: So then you just have to work with what you have. You cannot get more in rotation. Bas Peters: One examples, of course, geology, if you want to create geological maps, you have to collect crowd through it. You cannot just look at say electromagnetic data magnetic data and typography and just put point of the map and decide what whatsoever rock is underneath the surface there. Bas Peters: And and this is also a case where to be very expensive to collect extra credit through Bas Peters: Seismic case. We also know that, of course, drilling extra boreholes is very expensive and you cannot do that quickly. Anyway, so you just have to work with what you have usually Bas Peters: So what do we do if we don't have more annotation between needed Bas Peters: Well, we can resort to using higher level information so not annotations, but Bas Peters: For example, image level tags. So this is a nice overview picture where we have an image. Bas Peters: And the full annotation that we'd like to obtain a something like this. So just the horse and the person annotate it Bas Peters: So deployments point annotation what we discussed before. Bas Peters: We can work with it. But suppose we don't even have that then image level. Bas Peters: Information is something you can use. So you would just train on pairs of images and tags, like what's present in the image. So here it just, it has horse and person. Bas Peters: So if you have lots and lots of images, then a network can start to discriminate and learn why certain images have these tags and get a reasonable accurate pixel level segmentation and it's you, scientists. This is not very useful because if you look at Bas Peters: Like seismic images and you ask, is this Bas Peters: Geological unit present. It's pretty much any images have. Yes, it's not like a distinct objects. We're looking for and also in Bas Peters: mineral exploration. If you look at those airborne electromagnetic maps or something. You ask, is there gold somewhere here, then the answer is always yes it's not Bas Peters: We really care about where things are. And it's not it's not really the the data types are not like this is not the background plus object. So that's this is not terribly helpful. And did you science in general. Bas Peters: And alternative is to use quantitative like high level information or prior knowledge as we would often call it and the geosciences Bas Peters: And an example is say natural image just the image of a bear. And suppose you don't know exactly where an object is or how big it is and you cannot annotate Bas Peters: This case you could, but we just for illustrative purposes, saving can't Bas Peters: So, but you often are still able to just roll a bounding box. So you say, Okay, I don't know what's, what were the thing is, in this box. I just know there might be something in here. Bas Peters: So if you drill. If you're able to draw the bounding box, you get implies a maximum area or size of the object or cardinality of the factories image like the number of non zero in that image. Bas Peters: So you because you know I could drill, drill a bounding box. So the object is not bigger than what's the bounding box itself. Bas Peters: This, this is still not enough information to train and network because you also need to lower bound on what's in this box so that often is just a small number. Bas Peters: That says Bas Peters: There is something so it's not an empty box. So there is an object. I didn't know where or how big, but it's at least there's something Bas Peters: And the reason why that would maybe work is that everything is image outside of the box, there's a level of information that you're get you can say, I'm not interested in that. Bas Peters: So if you have to highlight something in this bounding box, then it's going to highlight all pixels that are like that. So the bear, because that's the only thing that is really different from anything outside of the box. Bas Peters: So what I Bas Peters: Like to discuss in the rest of the talk is how do you do prior knowledge into training a neural network different types. I just showed an example with the bounding box and like information about the size of an object and presence. Bas Peters: And how does this work in the single image case where you maybe not have objects background structure and I have some examples of various to science and image processing applications. Bas Peters: So when we look again at the rotation of these Bas Peters: Ideas that we going to minimize loss function say cross entropy that minimize it. The output of the neural network G and labels see Bas Peters: So the input data would be D and the network parameters are k. So, it is what we standard Lee do if we have f labels. Bas Peters: And note that if you have the size of a class or surface area information like that would be like minimum minimum, maximum size of that object you're looking for. Bas Peters: That could be loose bound to don't have to be very tight. That is information that relates to the output of the network at the last layer. Bas Peters: So it had to be a state the network statewide at the last layer and which is just the output of the neural network. Bas Peters: So that's a bit different than an inverse browser, we typically care about properties of the model parameters, but in this case we care about. Bas Peters: The final state. So in geophysics equivalent, that would be we care about the properties of the sad seismic Wakefield at the less time step broader than the model parameters of the earth. Bas Peters: Is the equivalence here. Bas Peters: So there has been some work in this area for sure and Bas Peters: People have exclusively focused on size of objects. I will also discuss some other types of prior knowledge you can incorporate and Bas Peters: The fact that Bas Peters: The output of the network is what we want to Bas Peters: Describe in terms of prior knowledge rather than the mobile parameters itself is causing some problems in optimization wise it's I'll show some details later. Bas Peters: That some people mainly proposed introducing oxy other variables and working with like adaptive penalties or yeah alternating optimization and ferryboat protection schemes. Bas Peters: And or approximations of Bas Peters: The size of some object five like penalties on the sense of network outputs. So there went to the soft max and they're just zero and one between zero and one positive values. Bas Peters: So these things are may be motivated by people trying to decouple the optimization over network parameters versus regularization of the output and these schemes that introduced a lot of complications that are probably not necessary. Bas Peters: So I'll just show what what my proposal is to do Bas Peters: And first we can think of the area of certain class of class one would be that bear and class two would be the background, if you have some bounce on the size of an object, then if you have two classes, the balance of course relate in one minus the other bound for the second class. Bas Peters: And a one, a two, or just cater upper lower bounds on the size of the object. Bas Peters: My Bas Peters: Idea is to just directly translate this fire some cardinality constraints. Bas Peters: So what that means is we we form constraints at of all network outputs of which Bas Peters: Say channel one has maximum, the number of a to non zero entries so cardinality is just a number of moles heroes in that factor and for the second channel of the network output you do the same thing. You just have a different bound. We don't need lower bounce here because in the Bas Peters: Normalized network outputs. They always some to one. So that's why we can get away with just a pronounce. Aime Fournier: What are, what did you mean here. Bas Peters: That's just the output channels of the network. So channel one would be the object class output probability map and number two would be the background class ability map. Aime Fournier: Thank you. Bas Peters: So training and last function for labels. Bas Peters: Subject to constraints on the output you can write it down as this in this form. So the said he would be the intersection of all constraints that you formulated. Bas Peters: And in case there's no annotation at all, then the first, the whole loss function drops out, and the problem reduces to just a feasibility problem. Bas Peters: So, Q is just a selection matrix that selects the point and notation. So from the full network output you select locations where you have the points annotated. If you have any other advice. Of course, this altar, like in the previous slide drops out Bas Peters: Yet stochastic gradient descent. This course or variants are really the standard method to to train neural networks to find mobile parameters. K. So now we could of course ask ourselves can be just used to gastric projected gradient descent to deal with the constraints. Bas Peters: So those iterations are written down here where the first one is just a regular gradient step. So the fourth step. And the second one is the projection. So that's the backward step. Bas Peters: If d is not going fast. And of course, this is the northern northern equal sign. That's why if this one. Bas Peters: And it's just a projection of the gradient step onto the constraints at to Bas Peters: It is hyper actually not really much easier than the original problem because we still have this network function here. It's the output that is constraint and not the parameters, over which we optimize Bas Peters: So, so these are the same two equations as in the previous slides. And the interesting thing is, of course, that Bas Peters: This is for fabric splitting that normally makes your problem break up into two simpler pieces, but in this case it doesn't really work because the second part is still almost as complicated as in the beginning. Bas Peters: So the question is why it's not working. I mean, you get iterations that are phallic but it doesn't really make it easy like it should be. Bas Peters: Or at least, so we're used to in most problems. So if you compare this to more common inverse problems for like image deep blurring or in painting, we have almost the same iterations accepted. Bas Peters: What's in the constraints that are the mobile parameters itself. So if you're deploying an image, the mobile parameters are the image. Bas Peters: And in our case the image is routed to network output. So that's indicated on this slide. So this just a subtle problem of Bas Peters: Regular rising, the mobile parameters versus regular rising, the output and the output in this case comes from a neural network. So it's a, it's just a more complicated problem. Bas Peters: So one what I came up with to deal with this issue is to use the point is set distance function. Bas Peters: So it's given a point why you want to find a point x that's closest but in the set the and the way to proceed for this promise to rewrite this using the projection operator Euclidean projection onto the set D. Bas Peters: Which this expression will be zero if you're into set and non zero sure outside of that set Bas Peters: So that's a known tool and but as I, as far as I know, it hasn't been used in combination with neural networks or regularization of neural networks. Bas Peters: So what you can do is Bas Peters: You basically rep any constraints set D into a differential penalty. It turns out you can differentiate this one. Bas Peters: And the good thing is that the value of your penalty would relate directly to the distance to feasibility. Bas Peters: And Bas Peters: It doesn't penalize anything. Once you once you're in the set deed and the expression become zero. Bas Peters: This can also become an exact penalty if you drop the square and it will still be differentiable with the squares of course non exec. But there's some Bas Peters: Slightly different numerical properties and the other good thing about this is that however many constraints you have because you project onto an intersection here you've got one penalty function to to with one penalty parameter to select Bas Peters: So just to show some examples. These are the distance function slaughtered in just two dimensions. So here we have the distance function related to the set of cardinality Bas Peters: Smaller equal to one. So that's that is just the axes of your coordinate system and going away from those juicy to penalty increase Bas Peters: And this is a box constraint. So you have a cereal box here. That's the constraint set an outside the value increases. Bas Peters: So yeah, these are just two examples of one conflicts and one non conflicts. Bas Peters: Distance function corresponding to these two sets. Bas Peters: So the interesting thing about this why I chose this function is that the gradient is known in a closed form solution just y minus two projection itself. So even though you need to compute the gradient Bas Peters: You don't need to do anything with the projection of breaker. It just occurs again. Bas Peters: And there's a few reasons how you can derive this result or understand it. Bas Peters: One intuitive. One is to just look at the picture where, for example, this would be intersection of have spaces, you have a point x and you project it Bas Peters: Director Euclidean orthogonal projection. You see that that direction is orthogonal to the level sets of the distance function. So, it is a negative gradient Bas Peters: The other way to see this is Bas Peters: The gradient can be is directly a fixed point iteration on the appropriate choice of operator. So in that case, you don't need an objective function like this. This is just to monitor the progress, this would really be directly be a fixed point iteration. Bas Peters: So to to put the pieces together I minimize the loss function with labels. If there are any. And then I add the distance function instead of the constraint. Bas Peters: So this is the distance to the constraint set and Bas Peters: One way how I usually like to write the network is just one equality constraint for time step or for layer into network. Those are the same things and Bas Peters: So what you see here, just one called strength per network layer or it's a time dependent discouraged national food differential equation. Bas Peters: At the beginning, if the input data that would be just the state network statewide at the first layer and then all you just have Bas Peters: At the end, the last layer with the state y n and that is what you use to compare to the labels and do. There's the property, you want to constraint. This is the output image in a way Bas Peters: Of course, there's one nonlinear activation function and every layer. Bas Peters: And what you see here the structure of the network is actually the rest nets, but it's worth this this whole structure fits most neural networks and I use some hyperbolic networks index samples. I don't actually use the rest net Bas Peters: So the next few slides are just showing how to get the gradient and that is sort of a known stuff in the BD constrained optimization or neural network literature should be as well. So I'll go very quickly. I'll just want to mention a few things. Bas Peters: So first, if you write down like ranch and you just have your objective your penalty, and then you add every equality constraint using a multiplayer p Bas Peters: So dead. There's nothing different than what we're used to in PDA constrained optimization Bas Peters: If he can look at derivatives, of course, we need to revert this the only thing I want to point out is that the projection operator occurs only once in or partial derivatives. Bas Peters: And that is the derivative with respect to the less state. So that's, of course, the one gets you an expression for the last like wrench in multiplayer. Bas Peters: I think in seismic literature is also called to enjoin source. Bas Peters: But the important thing is to project breakthrough occurs only once, but that's it doesn't make things more complicated. Bas Peters: And then the final algorithm, of course, is that joint safe method or known as reduced like Russian or back propagation Bas Peters: Where there's little stuff here. But just to show that it's really the same as what we know in geophysics thank you for propagate. Bas Peters: You compute your final Lagrangian multiplier that you bet propagate to get your all your multipliers in reverse order. And along the way, you can update your mobile parameters. Bas Peters: That are just based on the Bas Peters: States. Why, and that multipliers P for single layer. Bas Peters: There's one more piece that we need to add and that's of course the selection of the penalty parameter to enforce the ghost trained by the point to set distance Bas Peters: And in this case, we can just look at it for making progress towards feasibility, then we don't need to do anything. But if we're not, then we can increase the bounty parameter little bit Bas Peters: So that's a simple strategy that works in this case. Bas Peters: So a few examples, some image processing and Bas Peters: Some to science applications. Bas Peters: Where I show some of them have like bounding boxes. Some of them have annotation from one of the classes and some of them have no no labels at all. Bas Peters: So going back to that image with that bear. So this is a joint image. Bas Peters: In painting and segmentation. We have a corrupted image with 50% missing pixels and we want to get segmentation. So the whole data set is just this image. Bas Peters: And we can still train a deep network. So what we're going to ask is, find me anything in this box that is Bas Peters: Something, but it's different from anything outside of the box. So we have labels that say there's nothing here and the constraints say there has to be something in here that's at least 10% of the size of the image, but not bigger than the box. Bas Peters: And this is the result. And it worked really well on this case. Bas Peters: So, it is also shows you can train deep networks and single images if you have some regularization Zhilong Fang: I asked, What do you mean by an object is 10% of image. Bas Peters: OK, so the lower bout of Bas Peters: The class that yellow here is 10% of the size of the image. So it basically says there is something in this box. Bas Peters: Because otherwise, the you fit all labels by say setting everything to zero, basically. Zhilong Fang: So it's a number Zhilong Fang: The number of points is 10% of the email. Bas Peters: No, no, there there's there's no, there's only labels outside of the box. Bas Peters: Okay so into books. There are no labels. It just, it's just a constraint on the number of non zeros in the final segmentation. Okay. So, just as in this box there have to be 10% of them have to be a yellow pixel Zhilong Fang: Okay. Bas Peters: So treat a lower bound on the size of the object that you hope to find Okay. Bas Peters: So yeah, that's essentially the problem statement is find something Bas Peters: In the box that is different from anything that's outside of the box. Bas Peters: Precise with data, you can think about doing the same thing, but it turns out it's a bit more difficult than these images where you have a background in object. Bas Peters: Of course, you see here that there's no obviously something there's local stuff that's all very similar Bas Peters: For example, suppose this is a bounding box for a geological layer or sequence of layers. Bas Peters: Everything inside or outside. Almost looks the same. There's nothing distinct in here. There might be for geologists, but not it's not facially obvious. Bas Peters: So consider Bas Peters: A unit of interest. This is what you'd like to find Bas Peters: So we are we, we can draw bounding books. Suppose we know that. And suppose we can set a minimum size. We have a rough idea of how big this thing is Bas Peters: If you would have a different segmentation. Suppose you get to yellow segmentation that's completely wrong. Bas Peters: But it does fit all old knowledge you have, it's still within the box, it has the same size and it follows boundaries that are in the seismic image. Bas Peters: And it also had the same sort of geometry, the same curvature. So every property is essentially the same. So it's really this is not going to be easy. Bas Peters: So yeah, I won't show you examples of this, where this doesn't work. But if you try to save us from the previous slides is not going to work for these cases. Bas Peters: So an alternative maybe slightly less ambitious goal would be to work with a couple of annotations. Bas Peters: So some labels and then use the prior knowledge about the size and maybe a bounding box as regularization strategy to just improve your results. Bas Peters: So here's a very simple example of what that could do. So if we have 20 slices. So that's a pretty small seismic data set. And if you got eight boreholes that you can form. Bas Peters: Yeah, about that again for quite a few slides that have two columns and notated here we just look at two classes. So one boundary Bas Peters: So there's 20 pairs of these type of images. Bas Peters: If you train a deep network, just do it with just those point annotations, you get results. Bas Peters: So these are the two channel outputs. Bas Peters: So one of them is one minus the other Bas Peters: And the results of course don't really make a lot of sense judo every stripe the artifacts from that results from the fact that we have only columns annotated Bas Peters: Had yeah you can put the prediction. Then on to the data and you see it doesn't really follow any boundary, particularly, so this is not use it pretty useless result. Bas Peters: And it's just really simply retrain a deep network on a very few data, but even fewer label. So it's just not working. Bas Peters: The truth and notation. Bas Peters: This is from Bas Peters: A data said that I think SharePoint. Bas Peters: notated for length test. Bas Peters: So this is some of the training data. So this supposed to be. Bas Peters: Well, true or what they think is true. Bas Peters: So one thing we can Bas Peters: Yeah, this overview. So this is the final segmentation with the true label and then the every sheets. Bas Peters: Yeah. Not good, even though it's a simple problem, right, the horizon is pretty, it's not. There's no big jumps or steep curves. So it should be simple, but if you don't have enough notation. It's just not going to really work to train and network. Bas Peters: So what we can do instead is you some very simple prior knowledge. And one thing I did is, suppose you know your training on data that has just two layers. Bas Peters: That you know that output of the network should be monotonic increasing or decreasing vertically like one of them should be monotonic Lee increasing and the other around should be monotonic the decreasing. That is the probability that of the presence of one of the two layers. Bas Peters: And by looking at the seismic image, you can just see that all these lines have a limited curvature, so you can put some constraints on the slope into lateral direction without knowing anything off your labels. If you Bas Peters: Just know you're looking for something in here. You can instantly derive the maximum curvature Bas Peters: So if I do dose to constraints. So this time no bounding box or size constraints on the minimum, maximum size just multiplicity constraints of the probability functions. Bas Peters: In the output and about on the lateral variation. So that's just point twice bound constraints on the say lateral finite difference Bas Peters: That's also called the so called strain and these are actually the outputs of the network and death case. So they are much better. Bas Peters: So now we compare it to the true model here you see spelled exactly the same but it's almost the same. And at least it's a lot better. Bas Peters: So here's the final result. So the prediction. Bas Peters: You see, compared to the true one. There's still some differences, especially at the end. It's always you know boundary effects at these Bas Peters: Constitutional networks. It's just something you have to take into account. But in general, it's, it's a little better. So even though it's a very simple problem. It shows that Bas Peters: If you don't have enough data you need something else and and constraints on the output can be something very simple, that can help you also in a seismic case. Bas Peters: So this is one of the last applications. I worked on. So I don't have much more exciting seismic examples, but this is just a start to show Bas Peters: That things can be done in seismic case also, but it is more complicated than object background type data. Bas Peters: Let's see. I mentioned a few things. There's other types of prior knowledge. For example, here I segment this image. This time without a bounding box. Bas Peters: And they use the same ideas in the previous seismic case, except it's not a layer so I can issue monotone a city in one direction, but I can assume Bas Peters: That the object, the segmentation bill be constructed out of the sum of monotonic increasing and decreasing Bas Peters: Functions in both vertical and that thrill direction that is like inspired by techniques I cartoon texture decomposition where you decompose images in terms of simpler images. Bas Peters: So the intermediate steps are not shown here. This is the final result. So there's no bounding box here, but we can still get a segmentation of an object by using ideas about monitor necessity and the composition of complicated images in simpler ones. Bas Peters: Out just skipped some details here. Yeah, there's, you can do it with other images as well. Bas Peters: So again, no bounding box. No Labels just to show them that the object here can be written as a symbol of monotonic increasing and decreasing images in that role and horizontal directions. Bas Peters: That you can not pre present things fairly accurately, but you can still construct segmentation like that. So it's a really strong regularization approach. Bas Peters: That can give you approximate outlines to have images. Bas Peters: So yes to note that you need typically for these Bas Peters: Constraints. You need sufficiently large data inputs, because the prior knowledge that you might have typically doesn't hold for small patches. Bas Peters: If you know the size of an object somewhere, roughly, you have to have the whole object in your few and you into full image. If you take a small patch, then Bas Peters: It could be all the object or old the background so sizes important here. Bas Peters: So yeah, you often run them into memory issues if you use automatic differentiation, because they like standard packages like Bas Peters: TensorFlow or bike towards it's all safe old the network states, but you can avoid as if you use reversible network architectures, where you re compute forward states, while back propagating so you can afford storing all states. Bas Peters: So it just like Bas Peters: If you reverse wave per application and recover your original tank steps. That's essentially what you're doing these type of reversible approaches. Bas Peters: So to conclude, I think point to set distance functions are convenient way to merge labels and output constraints on the network. You don't need to alternate thing expensive alternating optimization schemes or introduce auxiliary variables. Bas Peters: And if you change your constraint. You just have to change the projection operator, it doesn't Bas Peters: Propagate in all your derivations or all your algorithm. So the software like implementation of the things is pretty simple, because the penalty function or degraded are always the same. It's just a projection that you Bas Peters: Plug in a slightly different Bas Peters: And it yet by construction includes intersections of set. So if you have more different types of properties you know something about you can directly include that and Bas Peters: Yeah. Again, nothing changes into implementation. Bas Peters: And Bas Peters: Yeah, I've shown a few examples. I got some more like other types of examples of additional time for that today. But yeah, it does work. If you have a single image or a few a few slices in seismic case. Bas Peters: And while it's a maybe most suitable for data of an object and background structure. You can also use it for problems right you don't have an object background so good I showed it for some the seismic stuff that you can also get nice results for things like accurate for mapping Bas Peters: And geological mapping and hyper spectral time lapse monitoring, for example, I've also worked on that. Bas Peters: So these are a couple of papers that. Yeah. This presentation was based on Bas Peters: And that's, that's my last slide. Zhilong Fang: Okay, thanks a lot for for passport interest such an interesting talk. Anybody have any questions. Thomas A Herring: Hi this is Tom hearing good question on your first one where you bounded the box around the bed that video right if you took that neural network and then applied it to later frames in that video, would it successfully grab the bear again or something go wrong, possibly Bas Peters: For I haven't really tried to this in the video starting to him. I just focused on just images using this approach. Bas Peters: But yeah, we took the image as a frame from a video to Bas Peters: Yet to do exactly what you propose to see if you can use this for Bas Peters: Other other Bas Peters: For the full video as well. Bas Peters: Yeah, we did. I did some tests, but I didn't really write it down but Bas Peters: Yeah, it sort of works just like Bas Peters: Yeah, just the original video that I segments that you're using the labels. If you can segment A slice in this way, then that also works for the full video Bas Peters: Especially for those videos that are every frame is different, but it's the same. There's no object that appear out of nowhere. For example, those are difficult ones. Aime Fournier: I have a question which I don't know how to ask exactly but I'm recalling, you know, not being an expert in seismic interpretation. The first time I see a seismic image. Aime Fournier: On my brain. I'm trying just to see what am I looking at and I start to work out well. I can see there are layers, I can see that some layers this mood and some layers are curvy, I can see the orientation of the layers, um, is it possible to have learning that sort of Aime Fournier: Starts from, you know, just a grid of pixel values and notices patterns like that at a very high level. Aime Fournier: That this isn't just a bunch of pixels, but the pixels have patterns like layers and some of the layers of different other layers and sort of intuitively tries to interpret the image based on some basically geometric concepts. Bas Peters: Yeah, you can do that because, in, in this case, I use simple geometry for one of the classes, but of course you can. Bas Peters: Trade network for more than two, you can train it for, say, five different classes and then for every class you add different geometrical cross train. So for one of them, you could say this, this has to be things that have very high curvature. It cannot be flat, for example. Bas Peters: Right, or other Bas Peters: Classes. You can enforce different geometrical property so that the network automatically Bas Peters: Segments the image based on those assumptions into different classes. Aime Fournier: And it helped a lot right when you as soon as you added that constraint, but you could see with your eyes, then the network was able to train much better. Bas Peters: Yeah, yeah, yeah, definitely. Aime Fournier: So so Kevin network sort of have like a lookup table of G. What should I try, you know, and so that instead of you having to the side for it. We give it like a dictionary or resource. So these are the things to try for constraints and see which one works something like that. Bas Peters: That's a no. Bas Peters: Maybe that's difficult because Aime Fournier: It's an infinite number. But, you know, get it sort of Aime Fournier: Discover, which one will work. Somehow, Aime Fournier: I guess it's transfer learning right you want you wanted to learn from what worked in other cases. That's how human works. Bas Peters: Yeah, I mean, you could of course taken network that you trained on some similar data set that you know if you know it looks very similar and it had good results. You can start with that. Bas Peters: Yeah, but then sort of always the question, how do you get that one. Aime Fournier: Yeah. Aime Fournier: It's a vague question, but I just think the constraints seem really key. You know that that prior information. Aime Fournier: The human intervention in sort of clothing. The, the most useful prior is still do you think Bas Peters: Yeah, if you don't have Bas Peters: Labels, then the information has to come from somewhere. So Bas Peters: Yeah, and this is one way to to do this and that. Yeah, I think that applies to many cases in many, many examples. Thomas A Herring: As Curious Case whether you could use the absolute value of the loss function to give you a sense of which of those potential Thomas A Herring: Different categories that were chosen was the best one. Bas Peters: I met you can you can you ask that again the question are Thomas A Herring: Very think, and I was talking about having potentially different choices that you would apply Thomas A Herring: And you try to decide which one of these is the most appropriate and I was just asking whether the absolute value of the loss function associated with each of the choices. Thomas A Herring: Rather than just minimizing the loss function you actually compare the absolute van to make a decision that this is the one that fits best Aime Fournier: Or the rate of decreases along. So something like that. Bas Peters: Yeah. Well, I mean, the thing to remember is if you have Bas Peters: If you enforce some properties on on your network output that it cannot satisfy then until the last will stop to decrease, you'll get stuck somewhere. Bas Peters: Because you basically formed a inconsistent problem. Yeah. Bas Peters: That yeah that can happen but Bas Peters: I typically assume you you select your prior knowledge that there exists a solution. Zhilong Fang: So your constraints on the output of the net. Right. Yeah. Zhilong Fang: So it's just on the Zhilong Fang: On the each image or on the Zhilong Fang: whole set of Bas Peters: You apply different constraints to each output. Bas Peters: So each channel output has a different constraint. Oh. Bas Peters: Yeah, that they cannot be the same because otherwise they would be in conflict. So they have to be different for every channel. Zhilong Fang: So I know under the end after training through I mean after the training. Why, why you do a prediction to you to do the projection after the end of the network. Bas Peters: No, no, because your network is just your train network at that point and constraints, do not Bas Peters: Are not part of that. Okay. Bas Peters: Yeah, that's why this approaches. Bas Peters: I think nice because the distance function. Bas Peters: Really does not Bas Peters: Interfere with your network itself. Zhilong Fang: That's no guarantee that's a set, who will be combat. Bas Peters: Know if you choose conflicts test. And yes, but those cardinality constraints on the size or no context. Bas Peters: For a few you select slopes that have to be larger than or smaller than zero. Bas Peters: Like without zero included, then it will become non conflicts as well. Bas Peters: So, Zhilong Fang: So, so the shift from results, you, you, you, you show it's a it's a chef from great for open test or what Bas Peters: Yeah, these are they put that out for an SVG workshop where you they had like last week or so they had or they put it out a bit earlier, but they gave data and you could just work on it and then Bas Peters: Submit some results on a some test data and they would show the results at some workshop Bas Peters: I just, I didn't take part in that. I just took the data and labels, they gave to do some experiments like this. Okay. Bas Peters: So you had the quality of their segmentation. What will we assume it's true. Yeah, I don't know, but it looks reasonable. There, although it's you see some weird pixels, a little bit. Bas Peters: Yeah, you got to work with something Zhilong Fang: If I'd have any fashions. Philippe Ricoux: There is one question. So could I share my screen, please. Bas Peters: Yeah, I can stop sharing Thanks. Philippe Ricoux: She was Queen Yeah. Philippe Ricoux: So, Philippe Ricoux: The speed of all the while he your presentation. You have only two layers. Philippe Ricoux: And working with your learning Philippe Ricoux: For training for that this is a real Philippe Ricoux: You science underground in Jordan. And so my question is very simple. He sees a single additional this the world are coming from a few reasons to speak interpretation. My question is how many agents images you need for training your interpretation system for this kind of Philippe Ricoux: Complexity of the real into wrong. Bas Peters: Yeah, that's always a difficult question, but Bas Peters: Yeah, even Though Bas Peters: Yes. So the first thing you always have to decide is, do you want to train one network to segment into a 10 different layers or are you going to train different networks to segment. Bas Peters: Takes a train a few different networks that distinguish between two units that that's effectively the same output. And if you have a couple of networks with two layers or one network with 10 layers output. Bas Peters: But yeah, those deep folds. I haven't Bas Peters: Tried these type of approaches. Bas Peters: Is still have to do some work on that. So, so far, mainly focused on remote sensing an image processing. Philippe Ricoux: You at my main question is the following. You see, this is through your Philippe Ricoux: VC of his affair between these three are the series, the same Philippe Ricoux: Time they are they fail as into slot. In fact, well, my question. My question was learning is a Philippe Ricoux: Oh, is it possible, and which kind of difficulties which kind of he fought, we have to do to select the right part of z, which will ensure to incorporate that this layer on this scene was the scene. Bas Peters: Yeah, for those cases, you need enough Bas Peters: Label information to Bas Peters: Make sure to network learns that those two are the same thing. And they're connected through that fold Bas Peters: So you need it big enough images that show the whole area, but also enough label data to just indicate that where there's people you train networks on Bas Peters: seismic data would pick faults. It just needs typically needs more training. Bas Peters: More training labels. Aime Fournier: This, this, what what Phillip is showing reminds me of another question that I often have one big constraint on Aime Fournier: Do geological data. Aime Fournier: Um, no matter what else is complex, most of the time. If there's not, overturning the sequence of the layers is the same from point to point in space. So how can that fact that most of the time that what layer of funnel is what other layer is preserved be applied as a constraint. Bas Peters: That just point to ice inequalities. Bas Peters: That you say Bas Peters: The value of the probability of this layer always has to be Bas Peters: Lower than this one say have to yeah you construct the sequence of inequalities that give you point twice information. Bas Peters: So districts always has to be above this pixel basically Aime Fournier: So the class to Bas Peters: Be above clustering. Bas Peters: For example, that is a Bas Peters: Way to that. Aime Fournier: Is that sufficient just to have any qualities or is there more information in the in the sequence being preserved that it's truly a global constraint somehow Aime Fournier: The same Bas Peters: Points lighting the qualities will is sufficient for to preserve the global order. Bas Peters: Okay, you have to have an anchor point you have to say what's at the top, for example, but Right. Aime Fournier: Thank you. Zhilong Fang: Oh, anybody have any questions. Zhilong Fang: So if there's no questions, I will first to stop the recording and if you have anything you want to talk with us. You can still Zhilong Fang: Saying, a zoom here and just feel free to chat. Okay. Zhilong Fang: Thanks again bus. Zhilong Fang: Very Philippe Ricoux: Good wine. 