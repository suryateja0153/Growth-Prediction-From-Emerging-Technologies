 you I wanted to start with this cute little video  what I really study is algorithmic methods to   make robot manipulation generalizable and and  why I really like this video is this is how I   got inspired to work in robotics this is science  fiction and as a researcher you are always chasing   science fiction and in trying to make some of  it reality and and really to think about this   if you think of something like this if I were to  have a system like this in my home I would want   it to do a variety of things maybe clean cook do  laundry perhaps help me sweep or other stuff and   not only that I would probably want it to work  outside of my lap I would want it to work in   a variety of settings maybe my home your home or  maybe in settings which are much more complex then   you can show it perhaps in the door and and the  idea really is how do we enable such complexity   and learning in such sort of general purpose  diversity of skills which interaction in the   real world requires and this is where I argue that  a lot of my research agenda lies we are trying to   build these systems physical agents particularly  that can really extend our ability and when I use   extending augment it's both in cognitive and  physical sense but there's nothing new about   that dr. Cox already talked about the the talk  of emergence of AI that happened in 1956 and   soon after we were dreaming of robot assistants  in the kitchen this is the first industrial robot   I don't know how many of you are aware of  this robot anybody in the room know the name interesting the robot is called unimate  it is as big as the kitchen but this is   this is actually 50 years to date in the  time since I would argue people have done   variety of tremendous stuff this is not my  work this is boss robotics but this is one   particularly good example robot can walk  on ice lift heavy boxes and whatnot cut to   this year in CES this is a video from Sony  doing pretty much the same thing this is   a concept video of a robot cooking circa  20 2015 2 years after the original video what what is what is very striking is despite  the last 50 years when you put real robots in   real world it doesn't really work work it turns  out doing long term planning real time perception   in real robots is really hard so what gives  what gives from from 1950 1960 to today I argue   that we need algorithms that can generalize to  unstructured settings that a robot is going to   encounter both in terms of perception in terms  of dynamics perhaps task definition and I will   only given you examples of kitchen but there's  nothing particularly about special about kitchen   it this sort of lack of generalization happens in  all sorts of robots application from manufacturing   to healthcare to person and service robotics  so this is where I argue that to tackle this   problem what we really need is to inject some  sort of structured inductive bias and priors   to achieve the generalization in simpler terms  you can really think about this that we need   algorithms to learn from specifications of tasks  where specifications can be easy language video or   let's say kinesthetic demonstrations and then  we need mechanisms where the system can self   practice to generalize to new but similar scenes  but often imitation gets a bad rep imitation   is just copying but it actually is not let's  let's think about this very simple example of   of scooping some leaves in your yard if you have  a two-year-old looking at you trying to imitate   they may just move them up around they're trying  to just basically try to get the motion right but   nothing really happens so they get what you'd call  movement skills as they grow up probably they can   do a bit better they they can get some sort of  planning they can do some sort of generalization   some of the tasks actually works and they even  grow to a point where now they understand the   concept of limitation is really not the motion  but it is actually semantics you need to do the   task not an exact not exactly always the how of  it the what matters so they may actually use a   completely different set of tools to do the exact  same task and this is precisely what we want in   algorithmic let's say equivalence so today what  I'm going to talk about is at all three levels   of these let's say imitation at level of control  planning and perception how can we get this kind   of generalization through structured priors and  inductive biases so let's start with control so   I started with these simple skills in the house  let's take one of these skills and think about   what we can do with this what we are really  after is algorithms which would be general so I   don't have to code up new algorithms for let's say  sweeping versus cleaning or or create a completely   new set up for I don't know let's say cutting so  let's think about that this is where precisely   learning based algorithms come into play but one  of the things that is very important is let's   say we take the example of cleaning cleaning is  something that is very very common in everyday   household you can argue wiping is a motion that  is required across the cross board not very many   people clean radios though but still the concept  is generalization perhaps cleaning a harder stain   would require you to push hard you have some  sort of reward function where you wipe until it   is clean it's just that the classifier of what  is clean is not really given to you explicitly   or maybe you know the concept or context where if  you're wiping glass do not push too hard you might   describe it how do we build the algorithms that  can get this sort of generalization one way can   be this recent wave of what you would call machine  learning and reinforcement honey you get magical   input of images some sort of torque or output  in actions and this is actually done very well   in robotics we have seen some very interesting  results for long-standing problems being able to   open closed doors which is handling some sort of  fluids or at least deformable media but and this   is actually surprising and very impressive but one  of the things that strikes out is these methods   are actually very very sample inefficient it may  take days if not weeks to do very simple things   with specification and even then these methods  would be very let's say iffy very unstable you   change one thing and then the whole thing comes  shattering down you have to start all over again   now the alternate to do this might be something  that is more classical let's say look at controls   you are given a robot model the robot model maybe  specification of the dynamics it may include the   environment if the task is too complicated given  something like this what would you do you would   come up with some sort of task structure T I  need to I need to do this particular step maybe   go to the table do wiping and when it is wipe  then have a comma so this this has worked for   a long time actually when you have particular  tasks but there is a problem generalization is   very hard because I need to create this tree for  every task to perception is very hard because I   have to build a perception for particular task  wiping this table may be very different from   wiping the whiteboard because I need to build a  classifier to detect when it is white so one of   the one of the algorithms we started working on is  like can we take advantages of or the best of both   worlds argument in this case so the idea is the  enforcement learning or learning in general can   allow you to be general-purpose but it's very  sample inefficient on the contrary model-based   methods allow you to rely on priors things that  you know about the robot but require you to code   up a lot of the stuff about the task so what we  thought is maybe the way to look at this problem   is break up this problem in modular settings where  you take the action space of the learning do not   be in the robot space but in the task base itself  what do we really want to do is think about a   modular method where the output of a policy learn  the policy that is taking in images is actually   not at the level of what what joint angles do  you want to change but really think about the   pause and the velocity of the end effector but  also the gains or the impedances how hard or   stiff dude because the end effector needs to  be at this why does why is it important it   is important because this enables you to manage  different stiffnesses when you are in different   stages of the task this basically obviates the  need for you to create a task structure tree   so the system can learn when it's free when it  needs to be stiff and in what dimensions it needs   to be stiff and this is very important now the  policy is essentially outputting this stiffness   parameters which can then be put into a robot  model that we already know the robot model can   be nonlinear rather complicated why bother wasting  time spending learning effort to do this and this   is the only sort of best of both worlds where you  are using the model however much you you can but   you are still keeping the environment which is  general to be to be unmodeled what benefit does   this sort of give you so what we do here what  you see here is image input to the agent and   this is environment behavior we model this as  a reinforcement learning problem with a fairly   simple objective the objectives up top basically  clean up all of the tiles do not apply any forces   that would kill the robot that is basically and  what you really need to see is we tested this   against a bunch of different actions basis so  actions faces the prior here that you're using   and the only thing that you should sort of take  away is at the bottom is the favorite image to   talk and at the top is basically impedance  controlled or variable impedance directly   provided through the policy and this is basically  the difference between failure and success in both   terms of sample efficiency and all the smoothness  and control because you're using known mechanisms   of doing control at high frequency where you can  actually safeguard the system without wetting   about what the important learning algorithm  can do to your system interestingly what you   can do with this is because you have a decoupled  system now you can train purely in simulation F   theme is can be a model in the simulation you can  replace this model with a real robot model on the   fly you do not need fine tuning because again  the policy is outputting end-effector spaces in   end effector spaces so you can replace this model  and even though there might be finite differences   in parameters at least in these cases we found  that the policy generalizes very well without   any sort of let's say computational loss we did  not have to do simulation based randomization   we did not have to do techniques on either  fine-tuning the policy when you do go to the   real world so this is kind of zero short transfer  in this case which is pretty interesting basically   identifying the right prior enables you to do this  generalization very efficiently and also gets you   sample efficiency learning so moving on let's  let's talk about reinforcement learning again   we always want reinforcement learning to do any  sort of interesting tasks which can do let's say   image to control kind of tasks this is yet again  an example from Google but but often when you want   to do very complicated things it is it can be  frustrating because the amount data required on   realistic systems can actually be very big so when  tasks get slightly harder the enforcement learning   starts to what you would call stutter so you you  want to do longer time longer term things which   it can be very hard and interestingly something  that came out last year this is not my work but   friends from a company actually showed that even  though reinforcement learning is doing fancy stuff   you can code that up in about 20 minutes and you  still do better so what was interesting is that in   these cases at least from a practical perspective  you could code up a simple solution much faster   than a learn solution which basically gave us  the idea what is going on here what we really   want to do is exploration in reinforcement  learning or in these sort of learn systems   is very slow often in these cases you already  know part of the solution you know a lot about   the problem as as a designer of the system the  system isn't actually working ab initio anyways   so the question we were asking is how can human  intuition guide exploration how how can we do   simple stuff which can help learning faster so the  intuition here was let's say you are given a task   the task requires you some sort of reasoning it is  basically move the block to a particular point if   you can reach the block you will move the block  directly if you cannot reach the block then you   use the tool so we thought that what we can do  easily is instead of writing a policy it is very   easy to write sub parts of the policy basically  specify what you know but don't specify the full   solution so provide whatever you can but you don't  actually have to provide the full solution so this   basically results in you get a bunch of teachers  which are essentially blackbox controllers most   of them are some suboptimal in fact they may be  incomplete so in this case you can basically say   that I can provide a teacher which only goes to a  particular point it doesn't know how to solve the   task there is no notion or concept of how to  complete the task so you can start with these   teachers and then the idea would be you want to  complete you want you still want a full policy   that is both faster than the teachers in terms  of learning and a test time doesn't necessarily   use the teachers because teacher may have access  to privileged information that a policy may not   have but the idea even though it's simple the  specify is actually non-trivial think about this   if you have teachers multiple of them some of the  teachers can actually be partial you might need to   sequence them together maybe they are partial  and they are not even complete in the sense   that there is no single sequence that even will  complete the task because there is no requirement   we put in on on these teachers sometimes the  teachers may actually be contradictory you   did not say that they are all helpful they can  be adversarial independently they are useful   because they provide information but when you  try to put them together let's say go back and   and move forward and you can keep them you keep  using them without making progress in the task so how do we do this so let's review some  basics in reinforcement learning I believe   you went through a lecture in reinforcement  learning this is an off policy reinforcement   learning algorithm called DD PG what DD PG does  is you start with some sort of state that an   environment of rates you run a policy a policy is  let's say your current iterate of your system when   you operate with the policy the policy gives you  the next state the reward and you put that pupil   in a database that is called experience replay  buffer this is a standard trick in modern deep   reinforcement learning algorithms now what you  do is you sample mini batches in the same way   you would do let's say any sort of deep learning  algorithm from this database that is constantly   updating to compute two gradients one gradient  is the value of what you call a function critic   which is value function which is basically  telling what is the value of this state the   value of this state can be thought of as  how far would the goal be from my current   state and then you use the policy gradient in this  particular case something called deterministic pol   policy gradient that is what the name is deep  deterministic policy gradient so the you have   these two gradients one to update the critic and  one to update the policy and you can do them in   a synchronous manner offline offline in the sense  that the data is not being generated by the same   policy so you can have these two update process  and the and the rollout process separated that   is why it's called off policy so now let's assume  you have some RL agent whether it's DDP G or not   doesn't really matter you have an environment  you get in state and you can run the policy but   then now the problem is not only the policy you  actually have a bunch of other teachers which   are giving you advice so they can all basically  tell you what to do now you have to not only   decide how the agent should behave you also need  to figure out if I should trust the teacher or   not how do I do this one way to do this is think  about how Bandit algorithms work I can basically   at any point of time think about the value of  any of these teachers in a particular state I   can basically think of an outer loop running RL  in the outer loop or well of the policy learning   and I basically state if the problem that I was  solving was selecting which agent to use or which   one of these teachers to use then I just need  to know which will result in the best outcome   in the current state this formalism can basically  be stated you learn a critic or a value function   where it is basically choosing which which of the  actions you should you should pick simultaneously   as you run this thing then the policy which is  called the behavioral policy basically runs that   agent whether it's your own agent the learned  agent or one of the teachers but then now the   trick is regardless of who operates whether it's  the teacher or your agent the data goes back to   the replay buffer and then my agent can still  learn from that data so basically I am online   running some teachers so using supervisors and  using the data to to train my agent so whenever   a supervisor is useful the agent will learn  from it if the supervisor is not used then   standard reinforcement learning will happen  so what does this result in so we go back to   the same task we provide for teachers they look  something like grab position push pull but we do   not provide any sort of mechanism to complete  the task so the first question we were asking   is if we give one full teacher the method should  basically be able to copy the teacher and that's   what we refined that in terms of baseline we are  basically able to do something that is Porsche   at least copy one teacher when the teacher is  near optimum a more interesting thing happens   when you actually get multiple teachers so  when you get multiple teachers the problem   gets a bit more complicated because you have  to decide which one to use in and if you use   a suboptimal one you waste time this is where  we basically see that using our method results   in sample efficiency even more interestingly  what happens is if you provide teachers which   are let's say incomplete I only provide teachers  for part of the task and the other part needs to   be filled in this is where all of the other  methods kind of fail but the fact that you   are essentially using reinforcement learning with  imitation you still maintain the sample efficient so just taking taking a breather here what  we really learned from this line of work   is understanding domain specific action  representations even though even though   they're domain-specific they're fairly general  manipulation is fairly general in that sense and   using weekly supervised systems so in this case as  we are using weekly supervised teacher suboptimal   teachers provides enough structure to promote both  sample efficiency in learning and generalization   to variations of distance so let's go back to  those notes setup we started with low-level   skills let's graduate to a bit more complicated  skills so we started with simple skills like   grasping pushing what happens when you need to do  sequential skills things that you need to reason   for a bit longer so we started by studying this  problem which is fairly sort of interesting let's   say you have a task to do maybe it's sweeping  or hammering and you're given an object but the   identity of the object is not given to you you  basically have given a random object whether it's   a pen or a bottle how would you go about this  one way to go about this is look at the task   look at the object predict some sort of optimal  grasp for this object and then try to predict   the optimal task policy but it I I can argue that  optimally grasping the hammer near the center of   mass is suboptimal for both of these tasks what is  more interesting is you actually have to grab the   hammer only in a manner that you'll still succeed  but not optimally because what the goal standard   that you're really after is the task success  not the grasping success nobody grabs stuff for   the purpose of grabbing them so how do we go about  this problem you have some sort of input some sort   of task and the problem is we need to understand  and evaluate there there can be many ways to grasp   an object and we still need to optimize for the  policy so there it is a very large discrete safe   space where you are basically grafting in objects  in different ways each of those ways will result   in a policy some of those policies will succeed  some of those will not but the intuition or   at least the realization that that enables the  problem to be computationally tractable is the   fact that whenever the task succeeds grasp must  succeed but the other way round is not actually   true you can grab the object and still fail at the  task but you can never succeed at the task without   grabbing the object this enables us to factorize  this value function into two parts which is a   condition grass or grass model and a grass model  itself just an independent gas model and this sort   of factorization enables us to create a model with  three last terms one is are you able to grab the   object whenever you grab the object does the task  succeed and a standard policy grading locks this   model then can be jointly trained in simulation  where you have a lot of these simulated objects   in a simulator trying to do the tasks where the  reward function is parse you're basically only   told do you succeed in the task or not there is no  other reward function going on so a test time what   you get is you get some object a real object that  is not in the test set you get the RGB D image of   that what you do is you generate a lot of these  grasps samples the interesting part is what you're   doing here is you're ranking grasps based on the  task so this ranking is generated by your belief   of task success and then given this ranking you  can pick a grasp and evaluate the task so you can   actually go out and do the task and this is what  the errors from here is back dropped in to picking   this ranking the way this problem is set up you  can generalize to arbitrary new objects because   nothing about object category is given to you so  in this particular case you are seeing simulation   for the hammering task and for pushing task and  we evaluated this against a couple of base lines   basically a very simple graph in base line then  you have this sort of two-stage pipeline where   you optimally graph the object and then optimally  try to do the task and then our method where you   are jointly optimizing the system what we find  is in this case end-to-end optimization basically   gets you more than double the performance there's  nothing special about simulation because we are   using depth images we can directly go to real  world without without any fine-tuning because   input is depth so in this case it is basically  doing the same task but in real world and pretty   much the trend for the performance still holds  so we are much more than double the performance   then let's say a two-stage biplane where you're  optimally grasping the object so moving forward   in the same setup we wanted to now ask the  question can we do more interesting sequential   tasks which require reasoning as dr. Clarke was  mentioning earlier that can we do something that   is requiring you to do but discrete and continuous  planning simultaneously so think of these kind of   spaces where the task is to roll the pin to  you but if you keep rolling the pin will roll   off the table hence you need a support and you  may have objects that are blocking the task and   there can be variants of this service set up  what it requires you to do is basically both   discrete and continuous reasoning the discrete  reason is which object to push and in the scene   and continuous reasoning is how much to push  it by or what is the actual sort of mechanism   of control so basically the kind of question  we are asking is can a robot efficiently learn   to perform these sort of multi-step tasks under  various both physical and semantic constraints   these are usually kind of things that people use  to let's say test animal intelligence behaviors   so we attempted to study this question in a  manipulation setting in a simple manipulation   setting where the robot is asked to move a  particular object to a particular position   the interesting thing is that there is constraints  so in this particular setup the constraint can be   the object can only move along a given path in  this case along let's say gray tiles and there   can be other objects along the way so in this  particular case in presence an obstacle multiple   decisions need to be made you cannot just push  the can to the yellow square you actually need   to push this object out of the way first and then  you can do a greedy decision making so you have   to think about this at different levels of time  scale so now doing something like this you would   argue can be done with a model-based approach  you can learn a model of the dynamics in the   system you can roll the model out and use this use  this model to come up with some sort of optimal   action sequence to do this and one would argue  that in in recent times we have seen the number   of these papers well such model can be learned  in pure image spaces so you are basically doing   some sort of pushing in pure image space then the  question we were asking is since this is such a   general solution basically its visuals are going  it seems natural that that these sort of models   will do it and and we're really surprised that  that even though the solution is fairly general   and there's nothing new about these papers from  the perspective of the solution it's basically   learning a model and then do optimally control  these particular classes of models do not scale   to more complicated setups so you cannot ask these  complicated questions of doing hybrid reasoning   with these simple geometric models the reason is  to be able to learn a complicated model that can   do long term planning or long term prediction  the amount of data that you would need skills   super linearly so to be able to do this something  like this would require many many robots and many   many months of data even then we do not know if  it'll work on the contrary what insight we had is   there is hierarchical nature to this action space  basically there is some sort of long-term symbolic   effects rather than the actual space of the US and  then there is a local motion if you can learn both   of these things simultaneously then perhaps you  can generalize to do an action sequence that can   that can achieve this reasoning task so what we we  propose is basically a latent variable model where   you are learning both long-term effects as what  you would call the effect curve and local motions   so what this does is essentially you can think of  the long term planner doesn't really tell you how   to get to the airport but it only gets what would  be the milestones when you get to the airport when   you do that then the local local planner can tell  you how to get to each of these milestones as you   go along so think of it like this that you can  sample a meta dynamics model which generates these   multiple trajectories multiple ways to get to the  airport you select one of those depending on your   cost function given the sequence of subtasks now  you can actually generate actions in a manner that   would give you a distribution of those actions  for for going forward let's say from milestone   to milestone and you would check it against a  learnt low-level dynamics model the value any   of that action sequence so you are basically  saying that the action sequence generated by   module is that going to be valid based on the  data that I've seen so far and then you can wait   these action sequences based on cost functions so  essentially what you are doing is you're trying to   train a model of dynamics for multiple levels but  you're training all of this purely in simulation   without any task label so you are not actually  trying to go to the airport only you are basically   just pushing around the other thing is you do not  actually get labels for let's say milestones which   is equal into saying you don't get labels for  latent variables so motion codes and effect codes   are essentially related so you set this up as a  variational inference problem so you see these   modules at the bottom these are these are used  to infer latent codes without explicitly boots so   the set up overall looks something like this you  have a robot the robot input image is parsed into   objects represented object centric representations  then this representation is passed into the   planner the planner outputs a sequence of states  that can basically be now fed into the system and   you can basically look through it I gave you this  example of a simple task of moving the object but   we did other tasks as well where you are basically  trying to move the object a particular goal in a   field of obstacles or trying to clear a space with  all of the multiple objects needs to be pushed   out so what we found is comparing this map model  with a bunch of baselines which were using simpler   models that having a more complicated model works  better especially when you have dense reward   function but when you have sparse about functions  which is basically saying oh you only get reward   when the task completes no intermediate reward  then the performance gap is bigger and again   the way this is set up you can go to a real world  system without any fine-tuning pretty much get the   same performance okay the the trick is input is  depth images okay so just to give you qualitative   examples of how the system works in this case the  jello box needs to go to the yellow yellow bin and   there are multiple obstacles along the way and the  system is basically doing discrete and continuous   planning simultaneously without doing any sort  of modeling for us to do sort of discrete and   continuous models or-or-or specifically designed  or task specific design models so the interesting   thing is there is only single model that we  learned for all of these tasks it is not separate   yet another example is the system basically  takes a longer path rather than pushing through   the greedy greedy path to get this bag of chips  in this particular case the system figures out   that the object needs to create a path by pushing  some other object along the way or out of the way   so in both of these projects what we learned is  the power of self supervision in in robotics is   very strong you can actually do compositional  fires with latent variable models using purely   self supervision both the both the task where we  are doing hammering and in this case we basically   had models which were doing pure self supervised  data in simulated setups and we were able to get   real-world performance out of these so moving on  the next thing I wanted to study was what happens   when tasks grow a bit more complex so we live that  simple let's say to stage tasks what happens when   you are graph structured that's when you actually  have to reason about longer tasks let's say towers   of Hanoi problem so we talked about these problems  clearly RL would be much harder to do imitation   even even imitation in these cases starts to fail  because specification of imitation let's say to do   a very long multistage task whether it's building  Legos or block worlds is actually very hard what   we nearly want is meta mutation journey so meta  mutation learning can really be thought of as you   have our environment the environment is bounded  but it can be in many final states which can be   thought of as a task so you get many examples  of reconfigurations of that environment this   can be thought of as examples of tasks that you  are seeing in train distribution in a test time   you are given a specification of one final task  that can be new most likely and you still need   to be able to do this how do we do these kind  of tasks in the current solutions actually let   me skip this so the way we do this right now is  write programs these programs essentially enable   you to reason about long term tasks even at a  very granular scale this is how you would code   up a robot to put two blocks on top of each other  now if you were to do this slightly differently   you need to write a new program so this basically  gave us an idea that perhaps instead of doing the   enforcement learning we can pose this problem as  a program induction or noodle program induction   it's essentially reducing an enforcement learning  problem or decision making problem to a supervised   learning problem yet in very large phase so you  get an input video a meta learning model which is   basically taking current state and outputting what  is the next program you should output not only the   next program but of course also the arguments that  you need to pass using that API it's essentially   equivalent to saying that if you give the robot  an API can the system use that API itself so   what you need is a data set of demos video  demonstrations and let's say a planner that is   giving you what what sub programs were called in  that execution and the loss basically looks very   much like a surprise learning loss where you have  a prediction and you compare it with your planet   Earth what does this look like okay should not be  like this okay so you can really think of this as   a high-level neuro symbolic manner well you at  the start out put something like block stacking   no let's see this should be better you start with  block stacking the block stacking unpacked to pick   in place pick in place can unpack to pick and  once you are unpacked to pick you can basically   say the robot will actually execute the API level  command as the API level command execute the the   executor goes back to pick moves forward with the  pick and and then actually does the pick itself   once the pick is complete the pick in place moves  forward to the place aspect of it and then goes   on to pick up the object by grabbing the object  and picking it up in in sequence and once place   is complete the executor basically goes back  to pick in place to block stacking and you can   continue doing this so this is just an example of  one pick in place but this can actually continue   to multiple blocks we tested with over 40 of  these examples sorry the videos didn't play as   well so what does this enable what this enables  is you can now input the specification of the   task through let's say doing VR execution what you  see in the inset and then the robot can actually   look at the video to try to do this task what is  important is to understand what is happening in   this sequence of block executions the system is  not just parsing the video because that would be   easy the system is actually creating a policy out  of out of this sequence so one way to test this   is let's say if there is an adversarial human  in the system that will break the model so if   you have done the task half way through the world  is stochastic the world goes back it should not   continue doing the same thing that you saw it  should actually be a reactive policy so it is   actually state dependent in terms of numbers  when you look at this basically what we find   is that if you have a flat policy or a deep RL  style policy it does not work on test tasks but   this sort of task programming or programming  action works very well and it actually works   even with vision so you have pure visual input no  specification of where the tasks where the objects   are so you get generalization with visual input  without specifying particular domain specific   visual design but again none of this thing works  perfectly it would not be robotics if it worked so   so so what feels often what happens is because we  are using API if the API doesn't declare when the   failure happens let's say you're trying to grab  something but the grab action did not succeed the   high-level planner does not know any continuance  so we went back to the model and said what is what   is it that is actually causing it to fail so we  found that even though we used the output as the   program we were able to inject structure the model  itself was still a black box it was basically   an illustration we thought perhaps we can open  the black box and actually put a compositional   problem what does this compositional prior look  like you basically say think of graph neural   networks so graph can basically be this idea of  executing the task in a planner you know you know   in a PDF style planner where nodes are active  States edges are graphs and you plant through   these things so this can actually still result  in a two-stage model where you are learning the   graph of the task itself rather than a black box  LST and predicting this but there is a problem   the problem is in these kind of setups the number  of states can actually be combinatorial millions   maybe and the number of actions are finite so the  concept of learning this graph in a neural sense   was to understand that the graph will not be in  this set up but actually a conjugate graph the   conjugate graph flips the model by saying nodes  are now actions and edges are States so you can   really think of these are nodes are telling you  what to do and edges are pre and post conditions   and how does this model work now you can actually  have an observation model which tells you where   to go in any particular state where what action  was executed and each action tells you what state   you end up in which tells you what it would  be the next action to do because this graph   is learned your base you're basically getting the  policy for free and the training is very similar   to the program induction except you do not need  full execution with the lowest level of actors or   lowest level of actions you are sufficiently this  that would be sufficient and what that basically   gives us is stronger generalization in both videos  and states with much less supervision again so we   have fewer data points or weaker supervision but  we get better generalization so the big picture   key insight again is compositional priors such  as let's say new programs or new tasks graphs   enable us a modular structure that is needed to  achieve one-shot generalization in these long   term sequential plans in the one or two minutes  that I have left I want to leave you with this   question often we are looking at robotics as azzam  as the sort of ultimate challenge for a high and   we compare the performance of Robotics with our  colleagues in vision and language where we have   done a lot of progress but if you notice one  of the things what happens is as the tasks grow   smaller the datasets grow very small very quickly  in robotics but to do more interesting tasks   more complicated tasks we still need to keep the  datasets large enough for us to be able to sort of   leverage powers of these algorithms if you look at  robotics in recent times the data that have been   essentially miniscule about 30 minutes of data  that can be collected by a single person this is   a chart of large data sets in robotics this is  not very old actually this is coral 20 19 20 80   just to compare this with NLP and and envisioned  a license we are about three orders of magnitudes   offs so we were basically asking the task why  is it the problem the problem is both vision   and language have Mechanical Turk they can get  a lot of labels but in robotics labeling doesn't   work you actually to show so we spent a lot of  time to create the system which is very very   similar to Mechanical Turk we call it Robo turk  well you can use essentially commodity devices   like a phone to get large-scale data sets which  are actual demonstrations full 3d demonstrations   and this this can now enable us to get data both  on real systems and simulated systems at scale   so you can be in places where ever you want  collect data from crowdsource workers at very   large scale we did some pilots we were able to  collect hundreds of hours of data just to give   you a sense of how this compare that's 13 hours  of data collected in about seven months we were   able to collect about 140 hours of data in six  days now the next question would be why is this   data useful so we did reinforcement learning what  we find is if you do pure RL with no data even on   three days of of doing this with multiple machines  you get no progress as you keep injecting data   in the system the performance keeps improving so  there is actually value in collecting data so the   take-home lesson was basically that the that more  data with structure and semantics supervision can   fuel robot learning in increasingly complex tasks  and scalable crowdsourcing methods such as robotic   are really sort of enable us to access this  treasure trove so going back what I really want   to leave you with is we talked about a variety of  methods in different levels of abstraction from   controls to planning to perception and then we  talked about how to collect data but if there's   one thing that I want to leave you today with is  if you want to do learning in in complex tasks   and complex domains such as robotics it is very  important to understand the value of injecting   structured priors and and inductive biases in your  models generic models from deep learning that have   worked for may or may not work for you that is one  - the use of modular components in modularization   of your problem well you use domain dependent  expertise with data-driven problems can enable you   to actually do practice build practical systems  for much more diverse and complex applications   that I would like to thank you all for being such  a patient audience and happy to take questions 