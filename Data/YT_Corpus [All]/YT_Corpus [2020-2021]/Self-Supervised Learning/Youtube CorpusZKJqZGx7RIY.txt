 to lucas's tutorial to the to the demo but uh sorry to the agenda so people should refresh there in about uh 30 seconds or so but okay uh with that go ahead lucas yeah thanks a lot and just to say remind people that we're asking questions on slido and if you open the chat and zoom you will get the link there in case you forgot right sorry for the introductions lucas off you go that's fine um okay yeah so um i was asked to give a little bit of an introduction to automatic differentiation and so i also tried to put it into a very long notebook and so not sure about the timing but let's try to get through this um okay so um you might have heard about uh automatic differentiation it's basically a way to um uh get derivatives of uh kind of uh computer programs and uh so this says um it's a pretty old idea but it's been kind of come to the forefront with the rise of machine learning and especially gradient-based methods in machine learning so everybody who's ever trained the neural network has probably trained it with gradient descent and so the gradient where does the key word there is so if you want to train a neural network you need to be able to compute the gradients uh with respect of the loss with respect to the weights um of the neural network efficiently so that you can minimize uh basically your loss function find the weight configuration of the neural network that is kind of optimal and so um okay but so the machine learning is not the only um um kind of situation where you need gradients obviously um you know like standard statistical analysis and high energy physics are likelihood fitting hypothesis testing also require gradients and uncertainty propagation is kind of uh multiplication of jacobians which are also gradients so gradients kind of come up a lot and so a lot of what we do essentially involves optimization like cut optimization and uh all these things and so every type of optimization requires gradients so um automatic differentiation is a very specific way to compute derivatives but before we delve into that we might want to kind of review what other approaches we could do so um the one that uh is kind of uh probably uh best known is the finite differences approach that we all know from our kind of pre-calculus class um you know how to uh approximate gradients um so you just basically take um you know you evaluate the function at two nearby points and then you divide through the distance uh of the points that they're apart and then basically the ratio of the differences is a approximation of the gradient and so that becomes a good approximation of x or this difference between the point or spacing is sufficiently small and so we can uh try to do this so i have like a small black box function so it's not black box here because you kind of see what it is it's x cubed right and then the true gradient function of course we know is 3x squared and so we can kind of do these finite difference gradients in a very simple way and so numpy has this gradient function where um basically computes finite difference gradients and we can compare to the true gradients and so what we see here is if the spacing is pretty coarse the approximation can become bad at certain points and so this is the x cube that's a core spacing and then you have the um true gradient and green and then you have the finite difference gradients and orange and but if you make this so you know maybe that's not the most accurate gradient but uh if you make the spacing a lot uh um finer then the approximation becomes better so that kind of gives you um kind of like um you know like a feel of what uh the situation is here so the um pro of this is basically that uh friends gradients um you uh the only thing that you need to do is you need to be able to evaluate your program that you want to kind of take the gradient with respect to and nothing else neither there's no framework maybe you just need to be able to um kind of compute the function and but then the drawback is of course that it's inaccurate unless you do a lot of evaluations and then so this is a gradient in one like one direction right so you have dfdx if you have [Music] imagine you have a multivariate function where you have a function that is dependent on a lot of parameters every time you want to go into a different direction and compute the gradient that is another set of two evaluations so it's a scaled sorry with dimensions and if you recall neural networks can have the number of weights in the neural network can easily reach millions and so obviously if you wanted to train a neural network finding different gradients you'll not get very far and so finite differences is very robust but it also has a lot of drawbacks and then the other thing that you might be thinking of is symbolic differentiation so in a computer algebra system so uh the main one here is mathematica or uh in the python world that's senpai and here you can basically manipulate um kind of expressions and then the computer algebra system knows about the differentiation rules and uh can apply those in a pretty road way and then just produce expressions that correspond to the exact gradient so that's much better so let's try that so i'm going to import some pi here and so you can basically uh you still have these functions so you have like x cubed and then three so that's the true derivative of 3x squared and so if you apply this function to like a symbol so this is not anymore like a specific value like a normal python but it's just like a symbolic placeholder and um basically what you get out here if you pass this through the function you get like a new expression a new symbolic expression which uh gives you x cubed and then um this is something that you can then evaluate so you can use this lambda phi to kind of evaluate the function so that's nice but the real um kind of neat thing is that you can differentiate the symbolic function using your compute algebra system and then you can basically get the symbolic derivative and so you can see here okay i differentiated the function and i get uh this exact gradient of the function which is 3x squared and then i can evaluate that and so if you um kind of compare this to the finite differences situation this is now much better so you can uh kind of compute uh compare here we have the same comparison with the pretty coarse grit and uh again uh the green is the symbolic uh derivative and the orange would be the true derivative and um and blue is the normal function and then you can see okay you can hardly make out any difference between true derivative and the symbolic derivative the reason is because the computer algebra system applied to the differentiation rules correctly and then just came up with the exact uh real function and then that's basically uh correct or the true derivative to machine precision and it doesn't really matter how much you evaluate or where you evaluate it it's just always 100 correct so even if you go to a super course grid uh i mean the function will not look nice but the gradient values will be exactly right okay so that's a symbolic derivatives so um okay then function composition is also easy so if you wanted to do um kind of the composition of two functions uh uh that operate on a symbolic value um you can basically get this composition so here's it's like cosine of x squared and then um the computer algebra system is also uh smart enough to do the chain rule and then you can do this um okay so um great so you might think okay problem solved so we get very exact gradients and uh what's the issue so why do we need anything else um so the first uh kind of uh drawback is that you need to implement uh whatever program your um you're intending to run you need to implement that in the language of the computer algebra system and so you all know our reconstruction and um you know analysis software is not um implemented in a computer algebra system but it's like a bunch of c plus or c or fortran or python code and so usually it's not in the um kind of general purpose language but okay um so i use senpai here and like the selling point of senpai unlike mathematica which is a pretty hermetically closed world unto itself the appeal of senpai is that it kind of uses um native python language constructs to kind of over overload their meaning and you can basically get away with programs that look very similar no matter if you use the symbolic version or the um kind of non-symbolic version so you might only kind of differentiate itself by using a port statement oops what's going on right so you uh like this section here like f1 and f2 is exactly the same source code the only uh difference is that it's um you know the import uh for the cosine function is different and then so that that's already nice um so um but the issue is of course that not all functions are so super simple and um so uh one thing that can happen is something that is called expression swell um which kind of happens if you have um kind of standard programming practices like recursion or you know like looping constructs and whatever so we have a pretty let's take a look at the pretty simple function which is x squared plus three x plus four and so we then uh so that's uh pretty banal and then we have like a function that just loops like six times and then kind of updates the value itself uh kind of um onto itself so you can replace the x value with um just a quadratic map and then you run the six times and then if you look at kind of what the expression looks like it's uh kind of horrible so i mean the notebook doesn't really do it justice because you need to scroll a bit as well and so it's like a super complicated expression that basically everything gets unrolled so the notation here is super tidy and like you know well structured but then uh like it kind of explodes into this uh horrible mess and so um that's because um like once you run the symbolic uh the function on the symbolic value everything gets unrolled so uh that even gets worse if you try to differentiate this so if you differentiate this um okay this is kind of what it looks like and again it looks even more horrible and so you have like all these expressions and then uh like you have to scroll a bit okay and then uh you have a lot of these expressions and so um so if we would now just go ahead so the way that we would evaluate it kind of naively is that so every time there's so this is a function of a single variable so there's like a bunch of x's here and so if you would uh just evaluate it um simply we would actually end up computing a lot of things multiple times so you might notice that like this x squared plus three x plus four is uh kind of present at a lot of different locations so you have this this this uh here right and so um that is basically so why does this happen so this basically happens because of this kind of looping construct so it's kind of like a remnant of the source code structure but uh like the senpai or the computer algebra system doesn't really know anything about uh this kind of uh original structure of the program it just kind of unrolls everything and then um kind of uh flattens it out into all the single uh everything until everything is a single expression and uh then a few we just kind of replace x with a numeric value and then evaluate everything you would end up kind of computing for example x squared plus three x plus four you know a bunch of times and that's super wasteful and um so it's not super efficient and so one thing that modern computer algebra systems can uh can do is that they can actually recover some of these uh common sub expressions and so you can in some pi you can call this a common sub-expression function and then kind of try to recover some of the structure and lo and behold do we see this three x plus x plus four um you know that we've seen above um and um so this kind of you know recover some of the structure and then this allows you to be a little bit more efficient in how you evaluate these things but it's not automatic because it doesn't really know anything about the structure of the original program and then you have to do a lot of work to kind of backtrack and kind of regain the structure which is um a little bit tedious and then so it's not as automatic and it really depends on what this algorithm is to find these common sub expressions and so that's the problem you put one request in the chat to increase your zoom the text size oh yeah sure right oops does this work better does this work better that looks better to me well that's okay okay so so this is kind of one annoyance with computer algebra systems but okay maybe you might uh learn to deal with that um but another kind of basic uh problem is that okay a lot of our functions that we tend to um want to evaluate actually like our programs they tend to have control flow statements so they've like if else constructs and looping statements and you know break out from a loop and continue and uh maybe we even have go-to's and so um the computer algebra systems can't really kind of deal with this in a graceful way so let's have a pretty benign function that is kind of branched in two ways so like if x is greater than two it's x squared and otherwise it's x uh x cubed so this is a perfectly respectable function which is uh very much uh differentiable almost everywhere except for a single point so you might hope to be able to differentiate this but if you try to do this in senpai like you get an error because simply cannot really handle this branching statement in the way that it's written here and so because it says okay i cannot determine the truth value of this construct and so uh it can't really do this but if we go back to the um to the original function this is what it looks like it's very smooth it's a pretty nice function actually right but only that has like this one point where it's um kind of it doesn't have a well-defined derivative but everywhere else it's uh has a derivative and so if we went back to the finite difference this way which is kind of the dump version of computing derivatives but the robust one you know the gradients compute just fine so um so this is like on the left hand side you have like this uh section which is xq and then the right-hand side you have the section which is x squared right and then here you get um here you get again x cube section that's kind of a quadratic derivative and then in the uh from the quadratic original function it becomes the linear derivative so as you would expect and then as you would expect also uh uh close to this boundary uh okay it becomes undefined but most of the time you will not hit exactly the points where it's undefined and everything everywhere else you have a pretty well defined derivative um okay so uh let's uh go on so so just to recap so we have uh two um methods that we might think of first uh if we wanted to the uh compute derivatives we have the symbolic uh differentiation and the finite differences one and the defining differences one is a little bit brute force and not efficient and the symbolic differentiation alleviates some of that so you have exact gradients and um but kind of depends or it restricts a little bit of what you can actually compute and you need to implement that in the computer algebra system and computer algebra systems are not easy easily available in all like programming languages so some python is one exception where you know you have a computer hydraulic system that tries to like overload the natural construct of the language to um kind of uh co-opt as a computer algebra system but for them in mathematics it's a completely different uh programming language and there are no computer algebra systems uh maybe like in golang or java and whatever and so um and then additionally you cannot really handle control flow structures in a super easy way okay so um so this is the situation what we have and so both are not um super um um satisfying uh because they both have drawbacks so uh like what we would really want is like to have a third approach which is uh one that where that is exact um efficient uh can handle arbitrary programs and then it's easy to implement in many languages and this third approach is automatic differentiation um so before uh and before i'm gonna talk about automatic differentiation i want to uh make a short detour into kind of linear transformations um because in the end that's what derivatives are and so if you uh just kind of consider to have like a linear transformation which is kind of um you know you have some uh you map from uh some source space which is m-dimensional into some target space which is n-dimensional and then you have uh some operator some linear operator here so this linear operator can be described as a kind of rectangular matrix right and then so how do you compute this program so you might imagine you have a computer program which is basically a linear transformation and then how do you compute this so one way to do it is to kind of go into the linear transformation figure out kind of what exactly the coefficients are for every kind of component and the source vector to map to the target vector and then you kind of can actually literally uh construct this matrix right and then you do like a full matrix computation and then you have a very kind of a generic way to evaluate any type of linear transformation by just using a matrix multiplication um function of any uh language that you want and so in this case it's numpy okay you have some matrix that you somehow derive and then you have some input vector and then you can compute the result so that's nice uh except when the matrix becomes very large and again um consider that the neural networks can be like a million dimensional and so obviously then not you know a lot of these connections might be um kind of actually not active and so uh if a lot of these matrix elements are zero uh might actually be wasteful to store the four matrix which is n and squared elements and so you might actually use a sparse matrix and then so that stores only the non-zero elements and then has some kind of lookup table to say where in the matrix these non-zero elements are situated and then you can basically do as far as matrix multiply uh take the sp sparse matrix and then compute the results and this is still um super generic so you can just put in any sparse matrix and then uh you you get the result and so in this case the program is actually like a data structure so in both cases the matrix is like a data structure that is stored somewhere separate from the algorithm to evaluate it and then the third way is to have a matrix free computations uh this is basically where um where we kind of hard code the linear transformations this is usually the case in which we have most of our programs so even if we have programs where like in principle we could have the made some kind of coefficients oftentimes we kind of actually write it out as code and then the only thing that we we don't really have access to the individual matrix elements but we can only compute these matrix vector products uh where you put in the vector and then uh kind of implicitly does the linear transformation and then outputs an output vector um okay so uh so if you don't have really access to these um matrix elements uh can the question is can we recover them and then the answer is yes so if you kind of want to figure out like what this matrix is you can basically just feed this machine that just takes vectors and splits out other vectors um like kind of well-crafted input vectors and then with these well-crafted input vectors you can actually you can recover the full matrix so if you would put in like the different basis vectors you can actually kind of reconstruct the various columns of the matrix and then once you have your various columns so you need to basically run this machine three times and then you get the full matrix and so that's kind of nice because basically um we can recover the lineage a full linear transformation by just running this kind of opaque black box machine which is computer code um a bunch of times and then we can get the full um kind of uh matrix so this is uh so this is a matrix and vectors and this is called like a matrix vector product obviously uh but actually it turns out you can actually uh also go the other way so instead of so you notice this is doing one column at a time and then it gets assembled into this full matrix but you can also go one row at a time and this is because you can basically kind of feed in the um the basis vectors from the other side of the um of the matrix and then if you run the vector matrix product instead of the matrix vector product you can recover uh one row at a time and so in this case we need to run the um kind of the machine twice to get the two rows and then we also recover the full matrix and so this is a matrix vector product and um basically uh what is here is so this is basically a computer code that implements this vector matrix product uh accordingly the same way that uh this code up here um implemented a matrix vector product and so in this case so you'll notice uh what kind of happened and so in this case we needed to run the machine three times and to get the full matrix because it was column at a time and up down here we needed to run it only twice because it was row at a time so depending on what the shape of this matrix is you might want to either use this matrix vector product version or this vector matrix product version and so but in any case you can recover the same matrix so you can uh see up here so you have uh this matrix here if we compute it one way and then we have the other matrix here which would come a different way and this is exactly the same result and doesn't really matter it's just a matter of computation efficiency uh which direction kind of uh requires the fewer evaluations and so if you uh recap this uh basically um so we kind of have some kind of linear transformation that is characterized maybe implicitly by such a matrix but even if we can only evaluate the program at different inputs we can still recover the full matrix sooner or later by kind of feeding in specially constructed inputs and um yeah if you go to function compositions the linear compositions of linear functions are still linear functions and so the idea is still the same that you kind of feed in uh kind of special especially constructed input vectors but then instead of going through one matrix they go through a series of matrices and so if you feed it in from the right hand side you basically run forward so you kind of evaluate these matrices one at a time and then so you kind of eat away uh on this chain of matrices and then so this runs forward and then in the end you get your results so this is the column in your kind of linear transformation that you want to recover or you go the other way around and then you eat away from the other side which so you feed in like this row vector and then you kind of do these matrix multiplications and then so this is going backwards and so this is propagating so to say a computation into the forward direction this is so to say back propagating this this input vector through this composition of functions and then you arrive at your row of the kind of linear transformation that you're interested in okay so this is um all well nice but of course not all our kind of computations are kind of just matrices that we're multiplying together and so instead of so you can also kind of view um kind of linear transformation [Music] you can view it as a series of matrix multiplication but you can also view it as a series of graphs so if you have kind of a map that goes from a three-dimensional to a four-dimensional to a three-dimensional to a two-dimensional space so it's either like a set of matrices here or you can also uh construct it as a directed asset graph like here and then every edge here basically um corresponds to one of these matrix elements and uh so if you write down basically what the values are of the intermediate values if you're going to start evaluating these matrix multiplications this is kind of the normal matrix multiplication formula so you have y i is i i k x k so you have so you do the series of um scalar products or if you go like multiply from the other side you have the vector here and the matrix here and so the order is reversed but you still um sum over the indices that are common and so but you can also kind of um translate this to this graph language where so either uh if you go the matrix vector product route you kind of sum over all the input values uh in the graph so if you want to compute this you need to sum over all of the incoming edges or if you want to go the other way around and compute the vector matrix product you go the other way around so you in order to compute this you need to compute all the values from the other side which are kind of along the outgoing edges and so the nice thing here is basically that this allows you to kind of go into a more generic picture where you have a computational graph which is linear but uh kind of has a kind of a more complex structure and so this is a computational graph um as you would normally kind of see it and if you run some kind of uh you know series of statements in your computer program right so you have like some value that might get computed from other uh incoming values and then uh that might become the input to the next value and so on and then at some point your computer program is finished um okay so uh so this is just to say that you can basically do the same thing so if you um basically take the same idea as the matrix vector product in the vector matrix product but kind of computed using these formulas where it's more kind of in the language of edges of um a direct adhesive a graph you can still kind of inspire it do like this uh matrix multiplication even though you're not actually uh using let's say numpy matrix multiply and you still recover the same uh matrices in the same way that we recovered them before okay so this is this detour into a linear transformation so let's talk about derivatives and so uh you might wonder okay why did i spend so much time to talk about linear transformation the reason is because this is basically the entire trick of the automatic differentiation so even of our normal functions so obviously not all our functions are linear functions but a lot of our functions are non-linear in very complicated ways but the entire point of derivatives is that it's a linear approximation of that function and so that is always linear so uh learning above about a linear transformation was kind of set up for this and so then crucially the composition or the derivative of a composition of nonlinear functions is still the composition of a kind of linear derivative so this is nothing but the chain rule and so if you have a complex series of um kind of composition function compositions then in the end the only thing that you need to do is to multiply together the jacobians and so you now if you have like the series of jacobians you can then um basically do the same trick so you have like the series of matrices but we don't know exactly what this matrix is and so we're exactly in the situation that i explained above so we have some kind of matrix that we only know kind of implicitly and so we want to kind of find out what this matrix is and then we can do the same kind of trick that we did before so before i call that matrix vector product and vector matrix product but if the matrix in question is a jacobian matrix people usually use these terms jacobian vector products and vector jacobian products and uh this is basically uh so there's really nothing fancy so sometimes i think in the machine learning uh literature it's uh to my eyes are kind of uh presented in a convoluted way but basically only is the decision okay do you start multiplying these matrices together from the right hand side or do you start multiplying these together from the left hand side and then you basically get these uh um various uh so either the coping vector pros or vector jacobian products and then this is exactly what is uh meant by automatic differentiation so either in the forward propagation or reverse propagation mode and so in machine learning most of the time it's reverse propagation because um so this is the case where the jacobian is kind of wide where it's only one row because you have a scalar output but many many many many inputs like millions of inputs and so the efficient way to derive this is to use this reverse mode uh or back propagation algorithm which basically computes a row at a time and so since the entire jacobian only has one row you're basically done um okay so let's try to work this out in a super simple example and so let's say you have like uh this kind of computation a graph where you have put values and then um some kind of intermediate output value and um then you um kind of have a third value which kind of takes this input this intermediate variable and one of the input variables so you can either kind of compute this as a kind of a matrix as matrix picture uh which uh kind of requires you to add some kind of dummy edge just in order to not jump over uh the graph in a way because in a matrix multiplication you only kind of can have um edges across adjacent uh layers uh or you can also represent us just in this directed aztec graph and so you basically only compute this product of two jacobians and uh this is basically the derivatives and so um so either so you so in this simple example you of course know the full jacobian matrices but recall the automatic differentiation is relevant if you don't really have access to this full jacobian yet and you just have a machine through which you can push values and you get some values out and so we can try to do this by doing the forward propagation mode so here basically the forward propagation mode is pushing well kind of prepared input vectors through these jacobians and then you get the derivatives and in this case you need to evaluate it twice and then you do a bunch of math which is basically here for your offline viewing um but in the end what you get is you get the full derivatives by doing this uh twice right and so this is uh how you get the uh you know derivatives by following this computational graph and in the backward propagation mode it's similar so in the backward propagation mode it's basically this vector matrix version of it where instead of kind of going from kind of evaluating a node by kind of taking on values from its parents and pushing it through the graph you go the other way around where you evaluate the node and uh but then pick up values from downstream of the node from its children and then propagated upwards and so you uh kind of push a vector from the left hand side on this through these two jacobians and then you get your matrix multiplication and then so if you do this carefully um by basically taking on these edges that are in this computational graph you arrive for the um at an expression like this and so the nice thing here is since you only have a one-dimensional output uh so it's uh the entire jacobian is one row and so again uh so you just need to do this um this operation of pushing a value through these jacobians um once and then you get the entire all the gradients and so this is the case again in machine learning where you have a scalar output which is the loss function and then you have millions of weights and you still want the gradients with respect to all the weights but all you need to do is to just basically take this computational graph and traverse it backwards once and then you get basically all the um gradients for free um okay so let's uh kind of look at time it's running a wee bit but i i suggest that you take five minutes more and and then maybe try and reach the conclusion and then we can take a couple of questions yeah thank you that sounds good yeah so basically so i don't have uh that much left so it's uh basically okay i'm taking the same graph structure here which is basically uh okay you have to these two values an intermediate value in the output value but i just um kind of um give it live by um having uh like an actual um mathematical operation so this intermediate value is the product of the two input values and then uh that gets added to one of these input values again and then that's the final output value and so the idea here is that you can have python code that runs this function and then that's great so you get some function result out of it and so if you wanted to take a derivative so what we've learned is basically that we need to create equivalent functions for every operation that we want to do so in this case we have two operations so we have a multiplication and a summation and we need to somehow implement kind of helper functions that implement this matrix multiplication and so this is the case here so uh if you want to take a kind of differentials from your inputs and push them forward to the differential of the outputs in the multiplication this uh ends up being uh the chain rule so uh if you want to get d y given x one and d x two you get um x one times dx2 plus uh x2 times dx1 so you have this familiar chain ruler for multiplication and then uh you kind of push values through this jacobian vector product but this is this gvp and then for a sum it's uh even uh easier so so here it's basically on the differential from the uh summation they see just the sum of the differential so this is also just um the differentiation rules so basically in order to kind of make this function differentiable you need to uh kind of uh for every operation implement the function that kind of applies this differentiation rule and in the end then the kind of function has this same structure where you basically have an input and an output but now it's kind of expanded to have kind of this tangent uh value and this tangent output and but the structure of the program looks uh very uh much the same and so that's uh one of the nice things about uh automated differentiation so you might recall in the symbolic differentiation uh the structure was completely lost of the program and then meant that we had a hard time kind of figuring out what intermediate values we could compute ahead of time and not kind of repeat ourselves and in this way if you kind of construct your the uh um kind of code like this uh it's uh kind of automatic and keeps the structure and okay i'm not going to go through this in detail but this is now um kind of the same thing but in the other direction which is this backward propagation um kind of picture and so here it's a little bit different and so basically here it's uh so you still run your normal function forward and you know taking your multiplication and your uh sum but then you kind of once you've run through a function you kind of know what the computational graph is and then you can run it backwards and then so you can run these vector jacobian products backwards and then kind of you get both of these gradients at one time and so this is uh kind of the usual case that is happening in neural networks so you evaluate your network uh and then prepare uh to evaluate the gradients and then run this backward propagation step and um okay so this is um basically yeah so just to recap uh basically we have these two ways to compute these derivatives so it's not uh so this is just a very peculiar way to kind of structure your derivative computation and so you have these two modes the forward mode patient and the backward mode uh differentiation and um uh basically whoops where am i basically this allows you to kind of reconstruct the full jacobian of your operation um you know in a very efficient and automated way and it kind of it circumvents a lot of these issues that symbolic differentiation has and so what we gain is basically we get exact derivatives and the complexity of the derivation computation is exactly the same order as the complexity of the original program and so you saw the structure of the program uh is still the same even if you do the derivatives just that you do you know for every operation that you do you do one additional operation so you basically spend twice the time and oftentimes only a single pass through this program is necessary and so you basically you know can get the derivatives of a program in a very efficient way and then it can kind of handle our arbitrary controls uh flows and uh this is something obviously you will not do this by hand but it's something a computer or programmer can do very quickly to kind of implement the library and so this means that it's easy to make the suitable in a different programming language and in fact it is available in different programming languages um okay so obviously okay so here there's so this was more kind of uh educational uh in order to kind of uh kind of show uh what's under the hood in a differentiation system but obviously you don't really want to implement this yourself uh and you might wonder okay aren't there libraries for this and so the entire point is that there are a lot of libraries and there are a lot of libraries in many languages so you so this is a table from a paper uh on where you have all of these different um automatic differentiation libraries and you can see the scheme of python and you have multiple ones in each language you have matlab uh julia haskell and um you know like all kinds of languages and um so this kind of shows that this is much easier to implement than like a full computer algebra system where you want to do symbol manipulation but it's enough to kind of get derivatives out in a very efficient way and so since this is kind of pihab i didn't want to focus on python and so in python you might know all these machine learning libraries and at the core what they are are basically kind of automatic differentiation libraries um and so they all are able to kind of run the um kind of more or less remote python program and then give you derivatives for this and so just to kind of show this so you can have like a function which is um kind of x squared and so we import jacks and you have your kind of x square function which is like a standard python code right and then um or um you have like all the numpy uh functionality so uh jax as a library kind of implements all uh like a drop-in replacement for numpy so you can any type of array calculation you can uh do now in a way that is differentiable so you can kind of add arrays you can multiply areas you can take the logarithm you can take the exponent and uh but um if you use these um jack's jacksified version stuff in numpy all of this becomes differentiable so just to show that this is um uh kind of actually doing uh you know the gradients you can have like x cubed and then you can have not only the gradient you can have like second order gradients third order gradients fourth order radians any type of order that you want and it still kind of gives you the exact result and this is very different to uh what you would get with finite differences um so um so if you want to kind of evaluate this so this is x cubed and if you just uh kind of won't want to evaluate in a broadcasted way this will fail intentionally but you can kind of teach jags to broadcast this correctly and then you can basically get so you have x cubed and then you have kind of x squared and your linear um your third derivative and then uh the third derivative which is then um a constant right and so but you can so everything is exact as you see so you can get in any type of order of differentiation uh correctly and you can also do control flow so this is the same example that we had before which is x cubed and x uh uh x squared and then so here it's um again like a comparison of autodiff and the finite differences one and so here you can see you know what you gain so the black one is the true derivative and then so the autodesk one is all exactly on point and then the finite difference is really uh starts to become bad especially because it kind of depends on exactly where you are and your derivative uh with respect to this discontinuity and becomes very sensitive and so this becomes even worse if you go to like second order derivative so where this is a parabola this is the line the next if you differentiate that again you get like a line and a constant value and then you see that the finite difference is one just really has nothing to do anymore with the true derivative and so but if you use automatic differentiation uh it stays quite exact and so uh you can basically when you use these automatic differentiation libraries to get any type of um in any order of derivation you can get hessians and all kinds of different derivatives um okay i'm going to skip this this which is uh operations which um kind of discusses things where you know we have um kind of constructs inside of our code which are not per se differentiable so you can so i'll leave this out for a sake of time but i did want to quickly maybe segue into a nathan's talk where it's uh talking about how we can use this in high energy physics and so as i said so we can of course use automatic differentiation for neural networks but it's kind of limiting artificially of where we can use this tool because we need derivatives and minimization optimization in many other respects in high energy physics too and so one kind of marquee example of statistical analysis we want to do maximum likelihood fits uh where it's you know minimizing the log likelihood or negative log likelihood and so there obviously uh knowing the such a mini minimization procedure knowing the exact gradients is uh super useful and so um phf is basically uh one uh tool that we've written in order to kind of make use of these automatic and machine precision exact gradients in order to help the statistical analysis or minim minimization of the log likelihood and so kind of see that we can we've basically implemented you know some kind of statistical model and using uh jacks or any other machine learning work that is available in python and then we can run this maximum likelihood estimate fit and then this will give us a best fit value and so in this case um okay it's a very simple best fit value and but you can kind of see here that basically so this is the likelihood so this is the likelihood of two variables and um so this is the full likelihood length landscape and then the best that value is shown so it was one one when we did the maximum likelihood estimate and so this is the best fit value and so if you plot the full contour here and on top of the gradient so you can see that the gradients are all exactly all kind of point in the right direction and then having access to these super exact gradients are this actually uh you know makes the fit go uh you know orders of magnitudes faster than if you use something like minority which um in the standard configuration would use um finite differences um okay so uh but i'll leave this for nathan uh to go through and so um thanks for joining the tutorials i think was a little bit too much of material uh so i didn't time this perfectly but um i hope it's a good resource to kind of go through this offline and by hand and so i did want to just finish with this one paperwork from the like early 90s so this is not a new idea in the um in certain circles that you kind of want to make a lot of your workflow differentiable and so this is from young who kind of invented the lstm and so there's like this notion that we can kind of reformulate a lot of our workloads in a differentiable way and then make use of these kind of automatic optimizations efficiently and yeah so i think i'll stop with that 