 [Music] my name is Chanchal Chatterjee I work for the Google cloud I am a cloud of AI program manager I'll be accompanied by Adam Hammond who is from quantify on one of the use cases that we are going to demonstrate here so today I'm going to talk about model explain ability for financial services industry from the perspective of regulations so we are we have been doing model explain ability for a long time as a community but there has been a bit of a confusion regarding who the audience is a lot of us who are developers are very focused on doing model explain ability from standpoint of understanding tomorrow so today I'm going to take the perspective away from that we are going to talk a bit about how the financial industry regulators view model explain ability which is the most critical application of model explain ability in the financial industry and also one of the most critical user model explain ability period so moving forward we are going to learn how some salt financial how financial problems business critical problems are solved in the area of risk lending risk and insurance loss to use cases we are going to look at well will be explaining the models from a regulators perspective the all the solutions are built on the Google cloud platform on the cloud machine learning engine with complex deep learning models will also demonstrate how we improve the accuracy but yet we explain the models for regulators and finally one of the key pieces is we are going to convert some of these models to a rule-based solution in order to augment existing rules so if you already have a rule-based solution that is completely approved by the regulator then you may not want to add a completely new model to the rule you'd see lot of your financial services customers pass for additional rules in order to augment the rules and make it better so that regulation becomes regulatory approval becomes much easier so we'll start with a few use cases of model explain ability in the financial services industry we'll also talk about why we want to do model explanations we want to talk briefly about some methods some model explanation I think I'm going to squeeze the slides because I don't think that is very very relevant for sake of time then we are going to talk about types of explanations that regulators typically want and will give two examples one from lending risk the other from insurance loss and we'll also talk about a method that Google created in order to augment existing rules okay awesome so financial services use cases there are many use cases in financial services which is of high category high use for model high importance for model explain ability purposes so the first one is credit lending this is a use case we will cover the main purpose is to comply to regulation and it's very high importance transaction fraud this is where model explain ability is needed in order to create transparency is there are many reasons for it but I just highlighted one or two key reasons anti money laundering also transparency claims processing this is where we want to automate the explanations and the decisions so that it's all supported in one transaction and we also want to create transaction transparency investment management automated trading are also very high degree of use for model explain ability and for automated decisions so the two use cases we will consider one is lending risk the other is insurance loss prevention and Adam will present insurance loss prevention use case which you worked on why do model explanation so when developers look at model explanation we are typically looking at model explanation from the perspective of understanding the model whereas when the regulator's look at model explanation they are trying to increase their trust in the model these are completely different goals and this is why I made this table in order to make it clear what the two goals are they developers look at model explanation to improve performance of the models whereas regulators look at model explanation in order to reduce bias and increase transparency most of our software engineers should look at model explanation in order to create better algorithms in order to create a better model is this a six-layer ResNet or ten layer ResNet do we do it this way that way all these differences are going to come up in model explanation but regulation regulators want to do model explanation to comply to regulations and the output for software engineers and developers for model explanation is to produce a better model a better software but regulators want to produce a better report so clearly the two audiences are very different and therefore when you look at the regulator's perspective the entire scope of model explain ability changes and how you do the explain ability and how you report are very different so this is a funny picture which shows why we want to do model explanation because we want to create transparency we want to create trust we want to create a better understanding and in the u.s. we have to guidances one is the SR 11/7 and the OSI 2011-12 which are basically guidance for model risk management but when everybody uses them then it almost becomes mandatory gtp are in Europe is very important and there's two articles that are written one by a CC and the other by a large number of academic and Industry people have written the article together as to how to do the governance of machine learning so types and methods of machine learning considering the time I'm going to go through fast on this so model interpretation has two variants one is the global here that is local those of you who are practitioners in machine learning know this difference global is when we take the entire aggregate of data and we want to give the explanation for the model on the entire collection of data local is for a given transaction like if we are trying to detect fraud for one transaction that was considered fraudulent you want to know why you identified that transaction as fraud or no fraud and it can be a single or a small group of transactions but local explain ability is basically for a limited number of data so in terms of methods we saw that there are over thirty methods that are used in the industry which cover a wide scope of model explained ability we have only I have only identified the key ones that are most commonly used in the industry and that are most common and highly prevalent so the non-google ones that are very common are line ok locally interpretable models it's mostly a local model I have a brief description the second is anchors which is by the same authors and shaft values which is very common in the industry in terms of the Google methods there are two that I want to highlight one is called integrated gradients and that one is very common and that has been used very widely within I'll describe that in a little bit of detail and the second is called tika quantitative testing and concept activation vector which is also another model explain ability tool that Google has created so going over these lime stands for locally interpretable model agnostic explanations and this is primarily a local interpretation modern methods although there is an extension to global cholesky lime the key attributes here is that it generates data in the vicinity of the transaction or the particular data and then it fits the linear model and explains the linear model it treats the features as uncorrelated and the main issue with lime lime is excellent in explaining local models they doing a local interpretation of models but the problem here is that it has to go through the process of generating synthetic data and fitting the results to that so it creates what we call a surrogate model which is problematic in some cases some regulators will not accept that and secondly it takes time to do that so if you have a large number of transactions happening a million transactions per second it will be very difficult to apply this type of a complex technique on a transaction-by-transaction basis the second one is an innovation over line which is called anchors which is also a model agnostic explanation tool so they basically generates logic and rules around transactions and these high precision rules are called anchors and present local sufficient conditions for these predictions I have linked the paper and the github is all open source it's by the same authors that wrote the line this third one that is publicly available is called shaft values Shapley by it basically does the model by comparing features with and without it you take the feature out it basically measures the sensitivity of a feature in producing a certain outcome what is great about Shapley is that it produces the positive and negative effects on outcomes for different features and we have some results on that and because you have to take all combinations of features if you have let's say a model in which you have 2000 features then taking different combinations of twos and threes and four and five makes millions of combinations and therefore it is very difficult to go through all those combinations so they make some approximations called Sabah's methods and other methods and what it does is that it finally creates an approximate solution nevertheless Shapley is an awesome method of doing model explanation and is widely used in the industry and it is also very fast and accurate in real time transactions so coming to the google methods the first one is called integrated gradient switch axiomatic attribution so deep networks now this one is also a sensitivity type measure it is also a local interpretation tool and not geared towards a global interpretation although you can take a whole bunch of local interpretations averaged them together and call it global so the basic idea is you perturb the features and inspect the change of the features on the network's prediction this method has been used in over 20 Google products internally so let's take a visual example of how this method works so here's a picture of a fireboat in front of the bridge so the model says there's a fireboard so if you look at the far right picture at this end you'll see this is the picture the fireboat and these are the gradients so the problem with that is that the gradients doesn't tell you anything because it covers the entire the bridge and the boat and the ocean and everything so what we do in this method is we take a baseline picture which is a near dark picture at the far end and then we look at the gradients that come out of that picture and we see some remnants of the bridge then as we change the picture intensity from almost dark to the full intensity different gradients emerge and as you progress through this different intensity of the picture and as you see these different gradients as you integrate through these gradients then voila the explanation comes out and you get a beautiful picture of the gradients that are relevant to the firebot itself and that's how the integrated gradient method works and we also found that the gradients are most relevant during the transition phase rather than the full feature dimension and this is an internal paper published and available so next one is called decaf this is a innovation over the axiomatic methods this creates boundaries of different features in order to judge the sensitivity of the features effects on the outputs so these are some of the methods we have for model explanation that are very common so next let's look at the types of explanations that regulators will be looking for the first one that is most common is the importance of the different features how are the different features important so if you label the features one through nine or ten then you see that certain features have a higher importance the second thing you also want to look at the standard deviations of those features that is it's not important just useful to take the average importance but also how does that important is vary from sample to sample like if you have an importance which is point let's say 31 but the standard deviation is Twain tea which means that the importance of that feature varied widely over the samples so you want to also look at the standard deviations the second is the positive and negative contributions or features that is if certain inputs had a positive effect on the outcome and certain inputs had a negative effect on the outcome so those are the two important things that a regulator would want the other two important things are joint contributions of features or inputs so let's say you had two inputs one was let's say the loan amount and the second one was the income of the person how does the loan amount and income relate to each other in terms of the risk of credit lending so this is in those two examples we'll take some examples of how that happens and the fourth one is and also we want to see how does an input increase the outcome and how does an input decrease the outcome and then fourthly how does the different samples affect the outcome the different inputs vary from sample to sample in increasing and decreasing the outcome so let's go through the first example and then I'll turn over to head Adam for the second example loan risk prediction this is a credit risk and customer lifecycle value prediction problem the data came from Freddie Mac single-family home loan the ML best solution is using user behavior patterns and we are predicting to use a risk which is the loan delinquency risk so this is an example that is very relevant to cases like student loan delinquencies which has been announced as gone 29.1% in some cases us is facing student loan repayment crisis it can be repurposed to market loans student loans auto loans credit car loans personal loans the partner on this jet engine Pluto seven so so there is a the we basically considered four classes of outcomes and one is safe where the we say that the creditor has been very safe lender or has been very safe to land to the the person second is moderate safe and the percentages is shows that how much of the data was in each of these category as you can see very risky we have very very little data so this is a very unbalanced situation where we have large amount of very safe lenders and a lot of these financial services examples we have very unbalanced data that is one is one outcome has a very small amount of data so we took three sets of data and we built multiple models on that and it is also very good we use deep learning networks we use extra boost we use random forests and we use nine place based methods so by using all these different algorithms we came up with a very good result in which the case one is safe and case two is moderate say if the third one is model risky and the fourth one is very risky obviously you want to catch the last two very accurately and we have 99% accuracy in the last two the class two and three which is model risky and risky and this was a fantastic result regard so going to model explanation on this problem we had I took the top six inputs that were that I'm going to use to explain the models the first one is recency which is the number of months ago the customer had a recorded delinquency I stands for number of months frequency how often the customer had a recorded loan delinquency value of which category like 1 2 3 recency is the category delinquency value with the 0 1 2 3 0 is more safe and 3 is most risky credit score we all know what that is loan to value which loan to value means what is the value of the asset and what is the amount alone they took on the answer let's say your home is worth 200,000 and you borrowed a hundred thousand and your loan to value is 50 percent so the original unpaid balance of the mortgage and the original interest rate on the market again I can take any one of these four values safe modern safe more risky and risky so then going through those four examples that I gave for the regulator's look first one is the feature important the second is the positive and negative contributions of inputs so we found out that resets the score of two was one of the highest important features which means that if you were moderately risky before you have a high propensity of becoming risky and then recency score of safe means also was a very high predictor of safety and therefore recency score of zero and one were also very high play unpaid balance was also one of the high predict a high important inputs in predicting the outcome but that doesn't tell you the whole story like I told you that recency of 2 is probably a predictor of risky recency of 0 is probably a predictor of safe so we also want to know which one of these inputs had a positive contribution on the outcome and a negative contribution which should increase the outcome which decrease the outcome right which predicted risky which predictor safe so that's important to know so we found that find early increase the risk so the red line shows that if that particular input increase the risk and if any other input decrease the risk so you want to know which inputs had what degree of contribution towards a positive outcome or a negative outcome and also to what degree like signed early had a huge contribution in increasing the risk and same as the discount whereas the quantity how many loans the Turk had a very small contribution in decreasing risk the second thing is to look at the correlations of features they on the Left chart the vertical bars are 15 euro 15 year mortgage 20 year 25 and 30 year and you can see 15 year Dawn's have higher risks than 30-year loans is a deduction you can make of this original loan terms versus the original loan if you look at the term alone and the riskiness the green the blue means it lowered risk whereas the red means it increased the risk so 15-year had a high degree of risk lowering and high degree of risk increase so it had a wide dispersion of a wide range of contribution towards the risk on the right side you can see California lon I can I have results on many other cities so whenever I go make presentation I can switch to Illinois to New York to other places but here we are talking in California so you can say California properties of a lower risk because even though it makes a negative contribution Andres which means that it increases the risk but the increases of a smaller amount compared the range of values the intensity is smaller whereas any other state the risk increase of a much higher intensity and the decreases of a smaller value so clearly there is a bigger risk any other state but California and I don't know if that is true or not but this data shows that but so that's that's a great example so what come Adam he's going to talk about the second use case insurance loss prevention please thank you so the use case said I wanted to speak about today to all was insurance loss prevention so here what we had done with a particular property and casualty character was a casualty carrier is they wanted to take a look at the loss likelihood of their particular policies so there are really three elements that you can consider when you're factoring losses so that's the the frequency that happens so whether it will occur not the severity of it so how much will that loss be and the timing of it for this particular carrier in line of business in that carrier the crucial thing for them was whether or not a loss occurred because the majority of those losses were greater than the premium so they didn't necessarily care about the severity anytime you had a loss it would be an issue to recreate in this analysis so the data be a little bit different here I took some open source data that's available through Google data set search and it was Iowa property and casualty losses data set it also contains a little bit information from the surrounding states so whereas John child chose a you know active state that were in California I chose one that don't visit too often for conferences in Iowa but our solution was email based and that's from policy information and then one component that we added to that was crime score details so based on the location of a particular policy the crime score could be different and what we wanted to do is to be able to predict that policies likelihood of resulting in a loss some examples in terms of why frequency is important just linked a couple articles in the top right and then different examples of insurance that are out there where some similar analysis can be done is in property casualty which are really focused on here but also we've seen it in auto professional lines and health insurance as well so first getting into the data preparation and results so here we didn't quite have the issues with class imbalance to the same level as John Charles use case but what we saw was that the majority of policies result not having a loss and then about 40% resulted in having a loss from that data set some additions that we made were a loss flag so in the data set if you look this up online it will just contain the value of a particular loss so we wanted to create a 1 0 since we're really just looking for the likelihood and not the severity we added some crime score index information so again based on the location which we randomized based on the state that they're in we just added crime score index data to that the results that we were able to generate by running just a pretty simple straightforward random forest here was an AZ of 0.9 1 and the way that we compared it was with the false positive rate and true positive rate basically at smaller levels of that false positive rate you want to have a higher true positive rate meaning those that you forecast or predict as positive are actually going to be positive those that we say will lose that will have a loss actually do have a loss so when we look at that at individual threshold levels if we look at the false positive rate of 1% we can see that the true positive rate there that we have is 40% and we expect that to increase over time so at 10% of the false positives we'd have 78% of the true positives so anything really above an equal false positive and true positive rate would indicate a well performing model when we go through this process there are a couple of different ways that we make it interpretable some of these we've seen before so I'll gloss over them at a high level but the first thing we do often before we even start modeling is look at the feature correlations so understand whether or not there may be a positive or negative x negative effect on the outcome as well as if any features have correlations with each other as your independent features so what this will do is shed light on the different relationships that we see so you can easily see a plot of the data if I go down the diagonal axis from top right to bottom right and then we could see the scatter plots which would be between two fields so if I look at the second row first column basically that showcasing the my scatter plot for premiums written versus taxes paid so we expect some alignment or correlation there and then on the right hand side of it so if I look at the first row second column I'm seeing the correlation between those two between premiums written in taxes paid so this is often how we begin to wrap our minds around the data that we take a look at the second thing that we do is once we perform the model and we want to understand a little bit more on that global interpretability front as we look at the feature importance so here what we're just looking at is the mean decrease in Gini and we're able to see that the important features here are crime index premiums written and line of insurance now some child pointed out earlier a great thing to add to this is going to be your variance so you can tell sample by sample how this changes we didn't really have a high variance in this one so didn't necessarily add them to the visual but this also needs to be combined with some sort of exploratory analysis to understand the direction so just because we know a feature is important so if my crime index is important to be able to see if I'll have a loss or not you know from logic here I think we can tell which direction it is but you'd also want some exploratory data analysis to say a higher crime score would indicate a higher likelihood of loss so this in isolation doesn't do well but in combination with some exploratory data analysis can act as a way to interpret models the next one that we wanted to look at was again more on that local interpretability front and the ones that we use most frequently when we're looking at problems that are in tabular structures are going to be lime and going to be chef as judge y'all already mentioned what those two things are but here rather than looking at it at an overall level one thing that we've found useful and displaying information to customers is looking at it at the observation level so typically we may see this in customer churn prediction problems where they're trying to identify for a particular customer the specific reasons why that customer might churn so rather than looking at all data points across all features we look at one specific observation across the features so it might be for this individual person that the written premium and crime index are in creasing their likelihood of churn the tax is paid for whatever reason might be decreasing their likelihood of churn versus their industry that the Property and Casualty loan was assigned to increasing it in the location decreasing it but understanding that on an individual level and that it might change from observation to observation when I'm looking at both Lyman shap is extraordinarily helpful all right thanks Jan Schall now I'll hand it back to you to convert to deep learning Thank You Adam such an excellent result the two use cases so once again in each of these use cases we identified four things that are very important to regulators one is which features sorry which inputs are important and what is the intensity and the variance of that secondly how do the inputs correlate to the outcomes contribute to the outcomes in a positive way in a negative way that is which one increases the outcome which decreases the outcome in in Adams case it was lost prediction in the example I took with Brutus Evan was in delinquency loan delinquency prediction that's the outcome so which inputs make it larger which inputs make it smaller the third thing how do different inputs correlate with each other in contributing to the outcome so for example you might find that a certain amount of loan may not matter if it is taken from a certain banks so maybe the large banks control the loan industry we didn't find that result but say let's we could find a result like that it does not matter what the amount is as long as you take it from these banks right so sometimes you see significant results like that and sometimes you see how relationships between you and the banking makes difference and we sometimes find those that's important criteria so the correlation between different inputs like the person like the lender and the person who is lending to the relationship matters so those are the correlations demonstrate that and lastly is how do these different inputs change from sample to sample very important things and finally I'm going to show a very strong and innovative research that came out of Google research by two folks Nicolas frost and Jeff Geoffrey Hinton Geoffrey Hinton is obviously very very very well known one day Turing Award just a few days ago Nicolas is a student and we thought that this is a very significant result in order to do what is called a deep a augmenting of an existing rule by creating a soft decision tree let's say you built a complex model is my audience still working let's say you built a complex model a deep learning model but a lot of the time your regulators will not allow you to use that model they will say distill it down to a few rules that you can add to an existing solution that already there which has past regulation we want to add more rules to that that's what we do so a typical method to solve this problem would be to take that deep learning network here's the input here's the output and take that same input and output and create a random forest or a decision tree what happens is that the results drastically drop and let's say you're this complex deep learning model did 99 percent all of a sudden the decision tree does 80 percent so they've got to be a solution in between where you take that deep learning model and convert it to rules but you do not lose the performance to that drastic a degree that you've come up with something in between and this result is that that it produces a result somewhere in between it basically takes the deep learning model and tries to understand what the inner workings of the models are the gradients at each layer so that you can take that and make it into a decision tree this particular picture on the right is from a game called Connect four and so the main idea of the soft decision tree is to augment existing rules find features that have significant influence on the input on the outcome and create a human interpretable rule so a binary decision tree that has learned filters at each of the nodes okay is basically a multinomial distribution function and then when you do a stochastic gradient on that which is the back propagation algorithm by which you learn a deep learning network then you use a equation like that when you sum a multinomial distribution over all layers of the network and then you're running a stochastic gradient descent so what we do is that we took the Mint's data m-miss data sorry and then we learned the filters can be understood by a person and then errors can be easily understood and then the branch numbers indicate the potential for the classification this is classification of digits okay then the leaf number indicates the final classification so what they did in their research they took this complex decision to the convolution neural network and to learned layer by layer what the decision was and they converted that to a decision tree by that they got a 96% accuracy in solving it that way so this chart sort of demonstrates on three examples one is M nests the other is connect four and the letters so the deep network gave very high accuracy like 99 percent letters 95 connect four is 87 to 0.7 but when you fit a decision tree then the performance draw drastically you can see eighty-seven went to 78% yeah so anywhere from 5 to 20% in some instances as you can see 95% dropped to 78 but using this decision tree approach to disturb the soft decision tree approach we got a result in between so we got created a human-readable rule which you can use to augment your existing rules satisfy the regulations but yet you don't have as bad a performance as fitting in a simple decision tree so that's all I have thank you very much [Music] 