 all right everyone thank you for joining here we're gonna get going in just a minute or two wait for everyone else to join but meanwhile the chat is open here and you should be able to chat amongst yourselves in us and let us know where are you all joining from for example uh myself my name is Denny Lee and I'm actually from the Seattle Washington area so where are you guys from love to see it in the panel on the chat here and I'm sorry and I'm Douglas more I'm outside of the Boston area it's quite warm here today and this is your host Ryan Boyd and I am in Burlingame California right now and preparing for a move over to Boulder Boulder all right we've got some people from Austria New York City Poland oh that's awesome for Polat we're in Poland Medicare ste and then San Diego I love that town that's pretty awesome London India J / Valley Netherlands hey yeah that's awesome cool so good to see everybody jumping onboard this is awesome ah Warsaw excellent I've actually really loved my visits to Warsaw I've done talks there a couple of times and it's a very enthusiastic audience which is great awesome let's hope we can get those co19 thing figured out so that way we can start doing that again let's hope yeah all right I think we should be ready to get going now the the pace of people joining is slowed a little bit so we're here today everyone to talk about slowly changing dimensions sed type 2 which is a fundamental from the data warehousing world and we're to talk about how that applies in data lakes and in particular Delta Lake so again my name is Ryan Boyd I'm your host today substituting for Karen filling in some big shoes here for those of you who have attended past meetups but really looking forward to this and talking with Douglas and Denny so as a reminder the upcoming online events are available on our meetup site so the data AAI online meetup and you can also subscribe to our YouTube channel where we're launching a lot of new live content plus pre-recorded content and be sure to turn on your notifications in order to get alerted when we have launched a new online or sorry new live or recorded video and just a side note there is we're almost at 30,000 subscribers on YouTube just a few more subscribers to go so please join and push us over that line and if you enjoy the content here I am sure you'll enjoy the spark plus AI summit that is coming up here in just a couple weeks or a few weeks and so that's June 22nd to 26th and both Douglas and Denny will be presenting during the summit on a variety of different topics but we have a few hundred other sessions full 5 days full week of content we have quadrupled the pre-conference training and here's a discount code for you to get 25% off the training the conference itself is free so you only really have to pay if you want to go to the training or there's a special VIP pass which gives you some special access and different types of content as well but even that is only I believe $99 so register today and we'll have some great sessions as well as some keynotes our upcoming products from data bricks but also a lot of things from the open source community academic community and some some great thought leaders that are going to be keynote so for today however I wanted to introduce you to the two people speaking here so Douglas more solutions architect here at data bricks and I chose a very quick summary of his bio came for data visualization and stayed for computation and data Douglas do you want to just introduce yourself quickly yeah hi everyone I'm Douglas Moore I've been working at day bricks for just over a year but I've been in data databases first OLTP databases and then more data warehouses data Mart's for analytics over the last 20 years or so and I've been working in big data specifically for about eight years and I kind of I kind of smiled that that you actually got caught that I actually wrote that that quote in my bio it's kind of funny one of my spark AI tracks is is a portable data visualization so I hope you come and see that as well oh cool I actually want to go see that so looking forward to that Douglas great well thank you for the introduction here Douglas and then he Douglas is going to be interviewed and in discussion with Denny developer advocate here at data bricks and I took his past introductions to me both internally as well as on some of these meetups to say his parents forced him into pre-med premed and he went into bioinformatics instead trying to trying to get a little bit that technology in there and then he went full boat against the the wish of his wishes of his parents by going into data warehousing data science and eventually developer relations Denny want to introduce yourself a little bit more beyond beyond that line thanks very much right that's a little probably suspiciously specific but nevertheless I've been part of data bricks I guess I'm going about three years with a break in between that's a little long story was actually part of the sequel server team at Microsoft and help at our enterprise customers working with data warehousing and OLTP Enterprise projects with sequel server and I was also prior to data bricks also on the team that helped build what is now known as HD insight we were the nine person incubation team so I've been working with Apache spark for quite some time still likes to intermix data warehousing and then every so often you'll still see me talk a little bit from the biological side so that way I don't disappoint my parents just this ride called out all right yeah we don't want to have you disappointed you no no no let's not do that yes yeah all right so let's get into it now that you've met everyone I'm going to turn it over here to Douglas and let Douglas present his screen and he's going to be presenting his notebook here on slowly changing dimensions so Douglas go ahead and take it away okay thank you Ryan let's see I assume you can see my notebook now yes we can we can all right very good so I just want to set some context initially so we you know a lot of companies you know part of that eight years in Big Data is been about building data lakes and and data looks our way to bring in all these different data sources and integrate it and prepare it for a IML reporting creating data products and doing stuff downstream so Sebring bring your data in and RAW format whether it's CSV or or just binary so and whatnot you clean it up and and you prepare a goal a lot of companies are also moving their data warehouses that they may have acquired actually 20 or so data warehouses over there over their last 40 years or so and they're bringing all that data in there a lot of structured data and they want to build a another data warehouse in the cloud and and run that off to the data lake and technologies or move that to a cloud data warehouse and you know what and and and I saw this on on-premise and Hadoop and and and what you need to do that to support that is you do all the ETL on your data Lake because it's cost efficient and scalable and and then you push it into your data warehouse so what what you need to do there is you know integrate all these data sources you need to build your fact tables you need to build your dimension tables because you know the techniques the kimple techniques for analyzing dimensional data still live on and will live on for many more years and probably decades to follow so doing this in Hadoop was building dimensional tables was was really hard because common technique there is that is for a dimensional table is to build a slowly changing dimension type to now why type - what happened to type one you might ask well type one is where you overwrite say that customer record with new information type - creates a new record with the new attributes of the customer and and tatoos very important in that it preserves all the facts that you've accumulated up to this point in time you may be running you know I'm building this data warehouse every every month or every week or you know as you get more horsepower and more efficiency you can be rebuilding this or updating it every four hours or so you know giving your consumers fresh data now you don't want to rebuild that fact table every time a customer and devise product store so and so forth changes so you want to add a new customer record and then any new facts that come in you know sales or orders related to that customer they just get added to and and link to that new customer record that turn active customer record so that's why you know you you do this in the data like why you ant use a type 2 dimension here and then you know as you need to you your kind of analyte you have some users analyzing the data directly off the gold tables or you know you're shipping this for your high concurrency reports and dashboards into your cloud data warehouse Jenny did you have something well I mean you've talked about type 1 and type 2 which is awesome but I guess the question for me is like alright I believe you had mentioned in the past of like there's actually like six different types of slowly changing dimensions right yeah yeah it's it's kind of crazy and by no no means I am I going to go into those but Wikipedia would list sense Kimball would really only list three right and so and I would go with Kimball and he you know that's where you're adding additional columns like you know what was your first address what was your second address so what was your first market segment or second market segment no problem so let's dive into that so you've been bringing up the concept of Kimball's so for the folks that actually have a data warehousing background they're probably gonna understand invariably understand Kimball quite well but how about for those folks that are coming more from the distributor side of the plan what what what are what is in reference to Kimball in this case well so so Kay Kimball's traded methodologies for building analytics in add a warehouse and and thought through all the unique data problems and how structure and all the processes and kind of codified it and he has hundreds of tips and best practices available now and he's written a whole whole bunch of books and a whole team forum and and so he coulda fired all these techniques and the slowly changing dimension type to is a best practice and and the key aspect of this allows you to do ads of analytics you know what what did my sales order look like as of last month last week three years ago and and and and and that's really important if if you just use the type one dimension then you can only do as is view of the world you know what does the world look like today got it and and it doesn't really let you understand what the world looked like according all your data you know last quarter so if you want to do quarter-over-quarter analysis week over week analysis it's it's impossible to do got it so Kimmel codified on all this now it with Hadoop oh sorry a one clarification point is there then an implication on the type of schemas that are being built when you when you work with type two slows and changing dimensions yeah yeah so a good point maybe I skipped over that context setting so there's a dimensional or star schema that you see here you have the fact table here and you have foreign keys from from your customer devise a product or store so on dimensions there's usually a time dimension as well in here and and those are surrogate keys say on the customer table that lend themselves to this fact and the fact might be generated off of a purchase order or sales transaction an e-commerce order so on and so forth so so these facts there are generally append-only but but the the dimensions that represent a customer customers change that the preferences change the market segment that you might want to classify them in that we're going to use for this example will change over time a device or a product you might have different revisions of a product that you're selling you might not want to or device a device might get moved or change ownership or so these things change over time you need to capture that history you don't want to lose that history because you want to understand like you know what why do my a classic example is why did my let's say I have a wine store I'm selling wines and I want understand why my sales have dropped well you know you can look at the facts all day long but you know without that context that maybe some of your customers have moved out of the region around your store you know that would be lost if if you use the type 1 dimension now building this in in a data Lake with with a you know technology say Hadoop or you know other technologies where your data like is append-only you know the underlying substrate is that's 3 or HD pass yeah you know it's a pend only file system that gives it the amazing scale that that you get petabytes of data you can store in your daylight and integrate at all but you know you can't update it directly because I would require locking so Delta Lake creates this abstraction over s3 or a TLS gen to over your cloud storage and allows you to get these updates these deletes these up certs or merges and so the that absurd emerge is is key to doing a slowly changing dimension type to because what you do is you're going to update the previous record so in in this example here this previous record you know the supplier state is California that previous record that was given the end date you know when was a supplier in this example when was the supplier last in the state of California well 2004 12 22 prior to that they were in California now after that date they're considered to be Illinois so they're given a new start date end date is unknown so that's no so this you know as you can see this is an additive process but you have to go back and update that pre-exists that prior record here and that's what's been challenging with Hadoop and big data is doing updates until Delta loop comes along and so now we can do that we have updates and in inserts but it's important to note that you want to do these two things add the new record and update the previous one in one transaction because you don't want any kind of inconsistency and Delta it gives you transactions and ask the compliance on that updates and the merge operator you know across each operator the merge operator allows you to combine both the update and insert so database people know this as up cert the kind of the Hadoop Big Data folks know this is emerge so and in in in Delta like it's called the merge into operator and so we're going to get into that but before we do that let's explore the data a bit so I have some data from a TPC data set the customer data I you know I drop into a bash I do an LS I see it's a hundred seventeen megabytes you know a sample data set I see it's got 750 thousand lines in it and I can see that it's pipe separated and I see that has no header to it so these are important things that that I need to know when it comes to loading the data so I'm gonna split this you know for demo purposes only normally you can just ingest that one go and I'm going to use another feature of Delta that we're not going to get into too much but it's copied into copy into will track like which files you've read and which files have not read but what I'm doing is I'm reading from the cloud storage you know in your AWS account or or as your blonde store but I'm reading from your story from our storage account the data I'm mapping it based on types and names and I'm putting that into this Delta table here and I'm telling the the copy into that it's a CSV a separator it's pipe and further schema no header I learned this because my data exploration and because I've run this as a demo I'm going to force this to reload you know otherwise what will copy into will do is ensure that you only load it once it keeps track of all the files that you've loaded so and it simplifies your ingestion but anyways so for the demo we're going to force that the truth so that runs then I'm gonna create a table over it over that location now this is accessible this customer dim table is now accessible via sequel so I can work with in sequel I can look at and all the different types of it the fields I can look at the physical location of where that data is in I can browse the data and see that I've imported now one thing I didn't talk about I don't let me go back you know if you're an ETL shop you're going to add a lot of audit columns here so I put this in here because you know I used to do this for a living build ETL sisters but you know the input file name what was the source file this gives you the provenance the user that ran this job the job date if you see my notebook I added some parameters here the job date my user name the the target the req database there as parameters so anyways going back I'm loading I'm loading this sorry so I loaded it all that date a little that they have the audit comes here source user createdate when I I created this record an update date so I I think I mentioned I designed some data warehouses in the past so it's hard for me to build a demo without some of these production type things that you would expect anyways let's look at the customer dimension in in more of a summary point of view so what I see is we have five market segments in the TP CH data set and you know this is a synthetic data set and and your first clue there is that this is evenly balanced you won't normally expect to see this in real data set but you know because we clicked here with a bar chart you know we could basically see that we have five segments and and they're evenly balanced and so with Delta lik you can look at the transaction history and you can see that I did a load I did this yesterday and in the top and the copy operation and you can scroll over here you can see a bunch of operational metrics you know I loaded a little bunch of records it makes sense any any questions as far as so I took this through the overview and loading the data and and a little bit of analyzing the data that we loaded sure sure actually we've got some great questions that have come in so why would take a pause here to start diving into those questions actually but before you would even do that I did want to call that one thing that people may not have noticed that Douglass is doing a great job showing you notice that this notebook is a sequel notebook right so very much in line with how our data warehousing folks are typically running so on the top will sequel and all the commander in sequel up to this point most of our Delta Lake sessions have actually been on Scala or on Python but Douglas has done a great great call at the fact that whoa yes for those folks who are very much in that data warehousing database world I'll a sequel there you go let's go ahead and build this stuff in sequel so so hey Douglas we've got some good questions and in fact I went ahead and answer some of them privately but I did something I think it's good for the entire audience so let's go ahead and start with the first question is um can Delta tables generate certain keys I like basically identity calls in the sequel world and I Basin answers like no you actually have to automatically create them yourself but I just figured since you've done this many times any other suggestions on how you would approach that particular problem yeah Danny that's an interesting question that came up yesterday actually with a customer perfect and and and there there's like three approaches to do that at least three approaches and we could devote a whole actually session to generating surrogate kings that scale would spark so you know we we might want to say that there's there's like generating a hash on on the key values there's a monotonically increasing ID a lot of people come from you know Kimball said this themself and it was a lot easier and in smaller systems but you have just a sequence number now a sequence number doesn't work too well in a massively parallel system right it doesn't scale yeah so you know you gotta break it down into partitions and do things like that and and you get except that you know sooner or later your your your ID numbers aren't going to be sequential and compacted so we could actually devote a whole session just to this I've gone through this a number of times with big customers and it's a meeting subject so it's an excellent question so so maybe we can set up an item for a future date absolutely because of that alone right that alone just the idea of parallelization I had some customers in the past try to go ahead and like have a service that would just generate the idea in sequence and compaction and of course basically what end up happening is that the system got throttled when they're trying to get you get throttled down the for yeah you know that the hashes is is super parallel yeah you you do have to deal with collisions exactly yeah exactly to your point Douglas we would literally have one entire Sisson just on hash collisions yeah but we're doing pretty good so let's let's ask another question and there's some other questions that are coming in the Q&A panel let's ask the one more question I'm gonna go ahead and answer them privately while you go ahead and continue on and then if we have time we'll do the other ones but let's at least ask this other question Agron other-- great question I thought was that we should share with everybody is keeping in consideration about time to market right do you still recommend the dimensional modeling that you're designing and my response to him was that was that you could potentially do that one approach that some of the customers I work with is basically they would have the at least when it comes to the medallion model the bronze and silver tables are not dimensional modeling and the gold tables are so because the gold tables would end up being where you apply the dimensional model but you didn't necessarily apply it to bronze silver so that way you can actually have faster time-to-market but I'm just wondering if you can but is that the right approach to it yeah yes that's definitely my been my experience as well that that you know and it's not like all your data actually meets makes it to the gold layer right you know we draw this this diagram but you know the bronze layer should probably be ten times larger than the silver layer and and that's multiple larger than what's in the gold layer the diagram you know but we draw these kind of nice and even cuz it shows better yes you know what what companies do is they'll and the raw data because you don't want store in filtering because they might drop some you might need today or you might need in three months from now so you you know you have the theme touch it take it you know if if you touch a system we'll bring it all in you know you keep the catalog and the provenance of that but then you allow people power users even to go in and explore the state and figure out what's there figure out if that's even worth further structure you know is this a useful value useful metric here in the bronze layer before we put together an IT project to structure it and create reports and dashboards you know we might think like the number of windows on a house or whatever that metric is is a useful number you know we might just send out you know a scouting party say well is that really useful before we build a whole IT project and wait so in terms of time to market by by having these layers of curation you know you'll get there faster you'll get to the crux of it the important stuff a lot faster because you're weeding out a lot of stuff that you probably don't need down here wait way out here where your executives and and your airline folks are looking at this on a hour-by-hour basis that I you know that's been my experience it sounds like lines up with yours as well yeah seems like you know this is we've got common approaches in the even though we're different sides of the country common approaches of the sequel's our world yeah and you know as you can see you can access all this data with sequel or Python or Scala but you notice you can access it with sequel you open it up to a broader audience with enterprise but you can get to it quickly and you don't have to wait for that long IT project and budgeting and justification before it gets into your reporting system engineer say your data warehouse so you actually in a way get there faster because you have this have this data Lake and the flexibility of that and Delta like makes doing this so so much easier with the updates inserts the merge into the copy into rock on well okay let's continue on since we've got about I'd say about 15-20 minutes left and so the hotel will use this up and then we we will we do have a bunch of other questions I'll be answering some of those questions online so just in case we don't have time to answer them live but I'll answer the questions of the Q&A panel while you go ahead and talk about the really awesome context of the change set here so the change set this comes from imagine you get a load from a source system or a whole bunch of source systems these are changes that are going into your gold tables into your dimensional model you have this in your data warehouse you have this in your own OLTP system but these are the updates so you need to process this through the silver layer and standardize it and and now you have a chain set now you need to apply it to your dimension okay so I'm simulating one Here I am actually sampling some 10% of this customer dimension and I'm saying that in my case I'm changing the market segment from whatever it is to sports now you can make one of up whatever business scenario where you would reclassify some of your customers from five segments to say six segments but we're adding a sixth segment to some 10% of our customers here and you know we're doing this for demo purposes but they're real-life scenarios where where you resegmented your customers and using data science and machine learning for that so that's what that's a change set and then I go and evaluate it you want to make sure you have no duplicates in your chain set we're gonna display it so out of some 700,000 records that ended up in the in the customer dimension that 10% is 70k there and and here here's the grand so now you know here's the here's a big big thing that upset emerge so we you know we talked about what we're going to do we're gonna do a merge into a merge into the customer dimension right we're gonna do a match on the customer key you know from our data set so to do that we create a set so here's our chain set in here now do to do the SCD type to dimension the very eighth the variant that we picked was between the I I and III type but what we have to do is we select the chain set and we we mapped the customer key as the merge key on this and you can do this with multi-part keys and other things like that and then we go to Union that we're gonna start matlynn old has a merge key the first one is used for updates the second one is used for inserts now by putting this all under to merge into both of these that update and insert or the absurd is done under one transaction at scale you know whether it's 750,000 records or or 750 million records you know it's all done under one transaction in the Penn State Delta so now what are we gonna merge on well the customer key is that you know this update is that equal of the merge key so win match and you can have additional qualifiers win match and you have a current record you know is current equals one and if none of the oh sorry and if any of the attributes have changed because if you're change said somehow for some reason doesn't have any changes reflected so sometimes you'll get a load from a source system and there's absolutely no changes in there or 99% of those records actually haven't changed so that's what this clause is in for also this causes in here to make this this running this idempotent so if I run this and then run it again I don't want to you know create unnecessary churn and add add more records than I need to so I'm adding this to make sure that it's adding both on there now however all right so then you do your update you set your is current to zero and you end date that record so I've just done the update there you know and that's when match so when your keys match you do an update I says now when not matched let's say you have a brand new record right or your brand new record because you know some of the attributes of chains you do an insert and we have this little shortcut here insert stock which means you don't have to list out all the fields in your chain set you know as long as your change that schema lies matches or your your target table you can just put the star in there and makes things a lot easier so you run that that's a lot of code to look through and you know I went through and here you can see in the transaction log all that's captured and you know operational statistics about what happened that's all captured I'm you know I I like to I like to test my code so I left some of that in here but you can see the quote unquote older original version there all is current had seven hundred thousand of them in the new version about seventy thousand of those have been in dated in in the in the quote unquote new version 63 630,000 went untouched and and and about seventy thousand were inserted no and so those are all statistics do a final check of all current records you know we're just resegmented customer base so you expect 700,000 there and again big data stuff so you know visualizing it is important to tell the story so I think this tells it better than than my explanation query the the the regional version old version here in orange you can see looks like our previous chart now I took 10% away from that original set and I create a new market segment sports so this Delta or this this difference here basically I spilled it and imported all into the sports segment is that does that make sense yeah definitely I mean I think what you're doing here is absolutely providing that context and then so related to that right you're noting this the switch over the sports right that this segment is sports and so that environment leads me to get asked a completely random question here which is what's your favorite sport well and we we recently discovered this that we're both into Rocklin that's right and Denny's told me some amazing things about rock climbing around Seattle and and one of my favorite places out here in the East is Acadia Park climbing over to see cliffs and whatnot so you know what once this kovat thing is is is over with we hope to visit each other and go climbing outside so yeah directly but but what you're also saying is that based on this particular customer scenario it's it's a very apparent that we should actually create a sports store maybe specializing in climbing that's absolutely okay perfect I just want to make sure that we got that covered that's all and and and let me show you so you know the press the ads of aspect of what we created yeah is I'm doing a union hall between two views this first one is like okay I said an arbitrarily large date here because I didn't want to update this every time the demo was given but at some point in the future you know what does a market segment look like well that's where you get the new or blue in here and I wanted Union all that with some some date in the past so because we did this we can do a you know a quarter over quarter week over a week comparison I'm doing kind of a you know because a date said it's decades but I with this I'm able to trade a before and after view of the data that's now in my customer dimension right and so this is what you would normally do in in a data warehouse you know doing that dimensional analysis and and doing that time based analysis so so this is this is a typical data warehouse query in here and this is the visual now with Delta Lake and time travel you can see that I can look at transaction one that we showed a little bit before and and I look at the original transaction the load of the data and and and I map that to old and new and you get the same results does that make sense so we can look at this two ways verify the transaction now as you run this daily or hourly or whatnot on into the future you're ultimately going to vacuum this this table and remove all those previous transactions continued up and so that's an administrative command and you know and so it's best to use this square here the real data warehouse query in there now for a short period time under that administrative theory you can certainly use this to understand what changed and say last night's load all right you can see what happened if you want to know from operational point of view you can use the transaction log and the version numbers on it to understand but for analytics point you tell your consumers you use the data warehouse code that makes sense absolutely so actually let me chime in bet there's actually a question which I'm in the halfway answering so as to call out that Douglas is doing a awesome job calling out the fact that we have this ability to do dimensional modeling within data Lakes because of Delta Lake because Delta Lake has acid transactions which is pretty sweet but the key context is that you know we can do it but obviously we don't have all of the advantages of the enterprise data warehousing world in Delta Lake yet but that's okay so one things that we would definitely suggest all of you is that go to the Delta Lake github create an issue we as the Delta Lake community actively look at all the the the requests coming in so we can go ahead and understand better what you do or do not want to work with right and so if we're seeing a lot more people saying hey we love dimensional modeling we really want some of the features of dimensional modeling within Delta Lake we get enough of the community that's saying yes that we want to do that for that matter you can even put some PRS in to help us with that right so it's a it's an open source project so that's what's great about this so I just wanted to call that out real quick yeah and you know coming from a Hadoop background last eight years and helping customers optimize or spend on their data warehouse and actually move all the ETL out of their data warehouse and move it onto the daily doing it with Delta Lake and the merge and the transactions is so much simpler so much easier and faster as well so so with Delta leg you can you can or actually was the Duke you have to shut down access to your customer dimension to your tables while you don't loads with Delta Lake you can lead those queries open to run because it's a commit and and your users will always see a consistent view on the data so again overall like coming from the Hadoop background this is so much easier and I know from you know Exeter dear colleagues yeah you know everything is not rosy in the data warehouse either you have to counter it staging tables and and change views and and and things like that so but yeah I would would be extremely excited to learn about what what improvements we can make to Delta Lake technology and make it even easier to use bye bye folks especially folks coming from the data warehouse world they want to optimize their spin or scale up or they kind of want to avoid all the hops because you know this data this this or this clean gold customer dimension you're gonna need it for reporting and dashboards you're also gonna need it for your AI and ml to do that because that's what's gonna boost the next round of profits and enterprises is that AR and ml does that looks into the future the dashboards and reporting looks backwards you know which is still important to do but you know the the next wave of mega profits are going to come from looking into the future anticipating you know what customers are going to do how that's going to affect the sales orders perfect so I believe we have time for about I'd say about five to ten minutes worth of questions look right so let's dive into that because we want and so I answered a bunch of them online but there's a bunch of that I have not yet had a chance to type because if I was typing right now you'd hear my my mechanical keyboard clicking away and then nobody hear anything so let's start we got a great question uh I'm pretty sure you have your perspective and I have my perspective so let's make sure we both dive in on this one because it's a great question is there a performance impact on using time travel compared to coding SCD - on my own on my own and tape set of tables like this is similar to sequel server temporal tables compared to manually maintained sed to tables whoa why don't you go first and you can absolutely okay so I had a little more time to actually look at this so fair enough so let me dive right into it so yes there actually is a potential performance impact because when what you're thinking about is that time travel is great for a short term like segment okay in other words like 30 days 90 days things of that nature and so when you're looking exactly as Douglas remark here you definitely want to be able to use time travel because it's great for debugging it's great to see market changes that happen recently things of that nature right but what ends up happening is that don't forget time travel what's going on underneath the covers and we actually have a bunch of other YouTube videos specifically on this especially the one that says diving into Delta Lake unpacking the transaction log what happens with time travel is that there basically is a duplication of all the records for all intents and purposes okay so every time there's a new version of data there's a new set of data okay that's sort it's it's a little bit more complicated that but for all intents and purposes the ideas are more files underneath the covers because you have more files of the native covers you don't want time travel to be running too long because it runs too long you end up actually having far too many files hence the impact and performance okay so there are definitely some advantages to basic saying yeah well maybe I will physically go ahead and manually recode the sed to myself okay and it's not to say that you don't want to use time travel like I said the four specific scenarios time travel is absolutely a massively good idea where it comes a player's more like if you want s EDD to but you want to be able to look at the changes that happen - let's just say geographic information customers don't change geographies often but they will change over like a 10-year 20-year period right if I want to go back 20 years in time you probably don't want to do that as a time travel trick you probably want to do this as a manual coding - sed - so it really depends on your scenario and often I see customers where they'll do things slightly differently depending on the type of formation because some of its also very temporal in other words you need to look at history but you only need to look at that history for short period of time time travel is great for that versus no I need to look at those history for a very long period of time okay ICD - coding it manually how it makes a lot more sense so at least that's my context Doug go for it yeah I'm sure some of you guys some of you might even disagree so which is really cool I I like it I like what you said there a lot so this is yeah so in some ways this is a shorter path to to doing a query with this if you look at this query we're looking at at the start date end date across the whole table across the current version of the data and we're looking at both active and inactive so all the changes that we reported in the customer dimension we're looking at and unless we've kind of zo ordered this on start date and end date we're really you know data skipping homes but we're really looking at all these days probably the whole table here and and and we're doing that twice with this type of query now we wouldn't have some optimizations that kick in here but we're basically looking at the whole table and all of its twice in this query if you look at this what it should be doing on the cover is go to the Delta log transaction table find the zero zero zero zero dot JSON that Denny mentioned that was covered in the previous Chuck Jack get the list of all those part key finals and then pull that back and the same with this so in a way we're just scanning two versions of the table yeah I guess actually you know as I talk to myself in this this query here is really scanning version one of the entire table this query here is scanning both versions of the table so this is this is ultimately gonna read twice as much data right yeah so so that that's that's a one downside so I use this for more operational concerns like like if if you had a bad if customers complaining that numbers are wrong and they're like what happened last night on the load go look at this you know if you're doing a business analysis your business users are gonna probably want to look at this right so that's my it's a very good question you know should you kind of you know use a use a assembly code or versions and and you know before before what we used to do or what some customers of mine used to do is they they look at using age base to do these merges and and if you remember the HBase know sequel it would simulate updates as well but it also will keep additional versions of each value again that was that was sensitive to administrative commands wiping out the previous versions there was a nice trick to do updates and you know even though HBase generally had you know because of water was stealing maybe four times less i/o for a whole table scan you know for certain cases it was faster so again it it it depends but you know if you're going to add more of a business question you know definitely use this take advantage of all the query optimizations available in in SPARC and Delta like as well if you're doing definitely look at the versions cool so hey we've got time probably well for one or two more questions so I'll so all I'll ask a provider loved context and then and then we time in what's any color coding ideas like as well like so one question is how did to do the incremental load into Delta Lake and so we this the best way to answer by the way because I want to give a short answer but the best way to answers actually look also into the our data bricks YouTube channel where we actually talk about the series is called diving into Delta Lake so we actually have a diving into Delta like I'm packing the transaction log evolving the schema involving a forcing schema and also we have DML internals or how does update sleet's emerges work okay these will give you the technical context behind a lot of what Douglass's has been talking about today but in addition to that specific to the question of an incremental load all of them use the examples of incremental processing a la structure streaming so the idea of when you're processing data and loading it into Delta Lake the idea is that you can even if it's not traditionally a streaming system for example it is a file that gets dropped in every few minutes or whatever else you can then run structure streaming with trigger once that trigger once it allows you to go ahead and then still basically look for the file streaming in load it into Delta Lake anyways and with all the asset transactions compliance all those other cool features that are included so that's the most typical context of how you do incremental loads anything else set up I may have missed Douglas that you'd like to collect yeah those those are great points you know cuz because a underlying engine it has this unified API between batch and stream you you had the same functionality the same experience and either way you can run sequel over those streaming spark data frames as well I'd like to add that I I was working with a customer this this morning and they were getting a dump from a data provider never getting a dump of essentially a business dimension table and and rather than overwrite everything they want to figure out what change from one month to another and so so there's a couple things to do there one is to analyze that you can do the minus operator and see what records have changed when when it comes to adding it to your your your copy of this business dimension table you can use the merge operator merge into table you have your key which is your primary key which is going to be your key you know your business identifier your start date end date because again they're sending you snapshot so they're dimension to it but there's they're sending these full snapshots full dumps what billion rows multi-building rows right now you don't want to necessarily reprocess all the billions of rows if say only a hundred thousand of those chains so you want to figure out what's different now you can't do a you can use the merge into and then when not matched you know and you said set the primary key to be that you know your matching key here would be your your your business identify we start that in date right and so when not match to you and insert now you have just the changes that you've received you know changes based on imagine women by McKee but which is awesome and you can go back to this query with the version numbers and you can see exactly which records have changed and then you can run you can update all your downstream analytics that are based on that that business dimension table and what might have changed you know if you need to rerun your your scores on the business but only on the businesses they've changed you can find out what that is through this query you don't have to rerun another - query and and you get that and and now you can update the rest of your downstream pipeline with structured streaming and and some other features that are coming out that can kind of be all stitched together on a magic you know that that Delta you can write to a Delta table you can have a stream reading off that Delta table it'll pick up only the changes so you you know you know on that on that second table there you don't even have to run the query like this your yours your structured streaming reading from Delta as the Stars will automatically pick up all these changes which you know again can greatly simplify your Indian pipeline speed it up so some amazing amazing things that you know stitching together these perfect so hey well time is up for us we want to be cognizant every time we have a lot of other questions so I apologize that we could not answer all of them so I encourage you to go to the data bricks YouTube channel where this video is actually sitting right now because you're sitting there then you're able to go ahead and we actually do answer questions so take your questions whether we already answered them not doesn't matter prop them right onto the YouTube channel we'd love to go ahead and answer them live so um thank you very much I do want to wrap it up and send it back to Ryan great Thank You Denny well thank you everyone for attending today it was really exciting to hear Douglas share all his knowledge on on these topics as well as you know Denny's great questions and sharing his own knowledge as well but if you want to hear more feel free to like like then he said subscribe at the YouTube channel and also don't forget to register for the spark and AI summit where they'll both be speaking and there is a discount code for 25% off training as part of the spark AI summit so thanks very much and looking forward to the next session and all of your fantastic questions and have a great rest of the week thanks very much have a good time climb on that glide 