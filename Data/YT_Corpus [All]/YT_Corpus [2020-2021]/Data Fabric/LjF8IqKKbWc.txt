 hey everyone thanks for joining welcome to today's presentation we're going to talk about building a hybrid data pipeline with Kafka and confluent my name is Josh Michael I lead partner solutions engineering at confluent I'm excited today to be joined by sarthak from GCP both are thick and GCP are have been great partners to us over the years so start thick please introduce yourself and take it away awesome thanks Josh and thanks everyone for joining as Josh mentioned my name is Arthur I'm a partner engineer for Google but GCD for North America and I worked but it closely with Josh on designing this architectures and solving discussed robotics so first let me spend a few minutes understanding the agenda so we are going to first talk about what's the exact problem statement which we are trying to solve today what is a budget offer and what is constant we are hoping everyone in this audience is familiar with capita but this is just for today we are going to talk about how to great hydrojet of pipelines using kaufman compliment and finally we are going to look at some of the common use cases from a design patterns which our customers have implemented in production and having great success going to this idea about that one business problem which you also seen her personal lines which probably all of you can relate to is the total volume of data which is getting produced and consumed completely both on an individual level as well as an enterprise level is increasing thankful and and also the window of opportunity when you can actually take the data and take business insights helpful visuals and size is reducing trusted one example of that is happening all around us with covalent coronavirus and and all the aftermath of that from economic political business competitive landscape which is changing every day every week now with a tremendous increase in a volume of data from business standpoint if you have to make a decision on how to best adjust your business both at a at a larger organization level as well as a lot of smaller are tactical project clear the land every delicious lemon your window opportunities really low it's strong a few hours a few days when you have to actually process the data and understand how best to take actions based on data if you look at over the last decade or so how this industry has kind of a bond it is primarily on batch deduct where a large volume of data gets is collected over the day only a small amount of data is gets used during the day other data is kind of uploaded for FTP or other technologies for nightly job or weekly job and then it is kind of process of life oftentimes what happens is because of because of the batch nature of that cleaner the business decisions what he will take kind of lags by a huge module we have seen for example the fraud detection risk scenarios we have seen many times it takes it takes weeks even months for them to determine whether reporting the transaction or set of transactions were a fraud or whether a set of transactions at pollination to determine whether it was a fraudulent transaction a lot in today's world more and more organizations are trying to take these decisions in more near real-time so they are trying to capture opportunities from things like market trend analysis or user behavior tracking again a user were a retail is running on their website and apps you are doing and if I do predict their behavior and provide mode you know provide more relevant suggestions and recommendations or so on and so forth with as near real-time as possible and not based on what they use our data weak back and too much time and similarly even in case a reduction of risk other than fraud detection from bad after identification in a whole bunch of other use cases lot more and more enterprises needs to make this take these decisions in near real-time so with that challenge in mind a lot of times the customers come to cloud one to GCP and try to leverage some of the capabilities and the idea we'll have abilities Imus our abilities the GC beam is awesome act and try to see how can we leverage those Google capabilities to actually solve those from him secondary attack and but when they look at their on-premise system one thing which kind of stands out in a lot of times is they might be heavily invested in copper and hopefully all of you are rating of your February captor Kafka is is a messaging platform it is for pub/sub if the pop server is extremely scalable messaging platform the whole idea of messaging and publish/subscribe is is nothing new this has been there from the from the good ol days of you know JMS to MP we have seen many incarnations coming out of link daily I think in 2011 have you got a few smart software engineers over in LinkedIn what Kafka did it kind of reinvented the whole world of pockets of time and created the system which is which is infinitely scalable extremely fault tolerant which can scale across multiple servers while the media centers on evil chakra piece and it had this way to store all the data infinitely in the form of logs so you could have created this brand new architecture and how pops-up should work and which lend an excu you been as a volume of data kept increasing so a lot of enterprises I have actually standardized over the last decade or so under Kafka especially today with backed by the Apache it has kind of become the backbone or how all the discrete systems are kind of coming together in the enterprise data center so what we thought we'll try to talk about today is how can you leverage your existing investment in Kafka and and try to use that to do this near real-time you know near real-time use cases on GCV so this is exactly what hybrids hybrid streaming is trying to solve that using GCP + conflict and the consequent cloud specifically which we have not talked of a little later how can we actually get up from there on French systems from your existing compound without changing anything on minimally anything and move the data DCP and start leveraging the world-class surveillance no logs extremely scalable an extremely powerful analytics platform which Google offers across products like bigquery to data flow to to looker and everything else so I'm going to hand it over to Josh who is going to kind of walk you through about what is a budget Kafka and what is the distributed event streaming platform and and how does the whole thing and a face set into this vision of so with that or do you know SH thank sarthak I think sarthak did a really good job describing the need for real-time data and it's a background about Kafka and I think I'm expecting that many of you have at least heard or know about Kafka at a high level I'd like to just take a little bit of time to talk about some of the key features about Kafka that we think really differentiated and make it a streaming platform rather than just you know pub/sub messaging so Kafka is most widely known as kind of the de facto pub/sub messaging platform it is highly highly distributed and has a ton of different use cases throughout organizations what people don't know is that or might not know is that Kafka also stores data so similar to HGF HDFS Kafka's stores data replicated in a durable way and you can store as much debt as you want so you can sort an infinite amount of data on a Kafka cluster Kafka also allows you to process data so via Kafka streams case equal or even single matches transformations in the cough Connect framework you can interact with the data in real time as it's moving through on that last slide I mentioned Kafka was an event streaming platform I think a lot of people aren't familiar with the term event streaming or probably think like why did you create a new term we already have kind of existing terms for data infrastructure and we think that kind of with this new age of data and how businesses are starting to build their business around streaming in real-time that it that it really it does matter and there is really this new paradigm shift towards real-time event streaming and so we created this this new phrase in in in this it really has four requirements to it so there it's built for real-time its scalable to all types of data its persistent and durable and it's capable of enrichment so you can interact with the data as it's moving through the system in real time and there are other existing data infrastructure tools out there that provide you know a lot of these but they don't only provide all of them so you could put together a number of different systems or you could use you know Apache Kafka and compliment and get the best Best of Breed event streaming platform some of the key pieces of event streaming and specifically for this hybrid streaming data pipelining we're talking about is being able to connect systems together and in kafka the way you connect systems is via aptly named the cockpit Connect framework so Kafka Connect provides a framework to easily boot connectors the ecosystem is built you know over a hundred different connectors to different systems so I can pull data from pretty much anywhere and I can push data to pretty much anywhere and well that data is in Kafka I can interact with it view something called Kafka streams so Kafka streams a Java library to do stream processing within the coffee ecosystem on top of Kafka streams we've built case equal case equal is a sequel syntax that's an abstraction layer on top of Kafka stream so for non developers you're able to interact with these streams of data in real time so with Kafka connecting Kafka streams were able to really simplify data architectures so instead of having um you know tens or hundreds of systems that need to talk to tens or hundreds of systems we can move data through Kafka we can interact with that data with Kafka streams we can transform it we could merge streams and then we can push those streams to the systems that really want like bigquery get pulled that pull the data from data to data flow push it to GCS now let's think about this more from a customer's perspective right so what would this look like in the real world so if we look on the left we have a very simplified view of some data infrastructure right we have a bunch of applications they look like micro-services applications that are communicating via Kafka an event-driven way some of those applications are directly communicating with some data sources we also have data lakes and other databases that are connecting to Kafka pushing and pulling data they may be creating materialized views in Kafka for those applications to read and that's on Prem or hidden it's in some other cloud right and then we have where people want to go right like the world's moving towards the cloud GCP has world-class analytics so we want to get we want it as a customer take part in the streaming analytics so I'm gonna let start that kind of talk about how we can connect these these two worlds thanks Josh when you talk a lot of time or in different customers often telling their journey wants to they want to start with Google's world of world-class analytics from their desire might be to start with TCP to use bigquery because you cannot process massive amount of data are much faster because you might want to use big sheets to connect the data into into machines and then processing a business-friendly way or you might do also visualizations using looker and so on and so forth now what do you see on the right-hand side this is a pretty representative architecture of what the data pipeline on a DCP might look like first of the left you have observed which is Google's own abortion of pops of a messaging platform this is server less extremely fault tolerant extreme which you're distributed Google medic pop server which is very heavily used it's at Google and a lot of customers using and this is typically what you're starting quietens so there were jumped arena in and then the data can get picked up on another subscription pop surf topic and then flows through data flow bad there is a processing store the data in bigquery which is Google's extremely scalable no ops and then finally might do some visualizations is the data studio on the card on the other hand you might also want to take the same data processing through data flow and then store it you in in GCS in storage you say you know in final form is like after or party or whatnot keep in mind this of any group does not've architecture there have a lot of variations from instead of data and even observable languages yes or instead of just doing visualizations you wait on the trigger actions using music functions based on read and bigquery or you might want to you know use AI ml you speak UML for that part of matter the query ml or the other energy now as as Josh mentioned that this is always kind of the desire state right this is the high level data pipeline which you want to go towards but your reality is what you'll eyes on and left and especially if you have invested in concur so what we were trying to see is is how can you actually leverage those existing infrastructure in pushing the date honey so the way customers typically do it today means pub/sub has an SDK parcel has REST API is using those they can actually push the data from the different applications and the Divas frozen and main tricks and whatnot to pops up and they have no cups of it can start all these data contracts though this works the challenge with this is depending on how complex that on prep ecosystem is you might need to have included SD game to so many different applications and deployed in production you know and that itself can be a daunting and a pretty complicated project especially if you have technologies as well as mainframe as/400 so honest so what we started to look at is if that enterprise are already invested in Kafka how can we actually leverage that to make that journey a little better manner possible so confluent cloud which is available as like a first party service on GC p it is available directly to marketplace with direct integration of Billings and would and integrating with I am networking all the kind of stuff they have this awesome capability called concern indicator so if you're using if you are using Kafka you can then leverage that replicated capability feature from conference on top of that and replicate the data pretty easily from on-prem to GCD now all your existing systems are already pushing their to cover so you hardly have to do anything and then the data once it comes to GCP then the same get a pipeline which it alter when earlier it can be triggered on and then the data can go into all this different parts from visualization to function so Venus to D and so on and so forth I'm going to hand it over to Josh now who is our cast iron content expert to walk you through what exactly spawn strength plow and replicated in case equal all these cool technologies that you see in this room Josh what do you cool yeah I think so I think I did a pretty good job of describing compo cloud and it's tight immigration to GCP the idea here is really that for existing kafka users we're trying to simplify and ease their adoption journey to the cloud and so I started to mention there's a bunch of different ways you can do this there's a number of like he could go on point-to-point from a database directly to bigquery but we think that for coffee users since Kafka is oftentimes kind of the way that those systems connect that tapping into that existing koepcke infrastructure getting that creating that persistent bridge that real-time bridge to the cloud and then being able to integrate directly to GCD for starting services start building applications with the data from on-prem in the cloud is really compelling and so confluent cloud a sarthak mentioned it's a fully managed service and so this this makes the the journey a bit easier right so you're not having to manage Kafka on Prem and then manage Kafka in the cloud we take that burden away from you I wanted to talk a little bit about the two components on that last slide confluent cloud and replicator so a confluent replicator on that slide it looked like it was actually got part of both of sides of that right so it was part on pram and part in the cloud it's actually a you know part of the connect framework or uses the connect framework and is a connector it's a complicated connector that has a bunch of cool stuff that makes it super resilient but it's a connector that either runs on the origin cluster or at the destination so it's sort of pulling data across a network or it's pushing data across a network in typically we have it pull data but you can run it in either pattern and so this conflict replicator allows allows us a really nice clean story to tap into existing copy infrastructure without really having to touch anything right so we don't have to go touch those databases we're only I'm hooking into the existing Kafka installation on Prem a confluent cloud as sarthak mentioned it's a it's a fully managed service if you're if you've ever managed copy before you know it can be challenging right so it's a distributed system of Kafka brokers it's a distributed system of zookeepers Kafka Connect is a distributed system and so there's a lot of complexity here that that we hide from you so you don't know how many brokers there are you don't know about zookeeper when you run in the connect framework you you know this is from a UI where you just point and click so we really we think that this is a whole new way to run Kafka kind of a cloud native Kafka that's the super compelling and it's not just Kafka that we're running we're running schema registry we're running a sequel to mention we're running connectors for you so if you want to connect to bigquery you don't have to you know figure out how much data you're moving there you just go to the UI so you want to connect this topic to bigquery and it starts moving and then we allow you to store as much data you want us on that Kafka cluster as you need right so you can store an infinite amount of data in those topics so adding these two features you know if we take a step down right from the the more market sector slide if we if we think about existing customers that have on-premise copy infrastructure whether it's open source Apache Kafka confluent Enterprise right so they can they can use complan replicator they can run it on Prem they can run it in GC P and now they can create this real-time stream of data to complement cloud once the data's in conflict a CAD Vantage of those fully managed connectors and just start pumping data into the services that their mo interested in the COS also have applications that start directly integrating with with comfortable cloud right so building micro-services and an event-driven way and using Kafka as the mechanism to do that this super common pattern so now you can have confluent cloud there as your event ribbon microservices coffee let's talk about a couple use cases here to start right just wanted to talk about like the fact that this is a super a kind of simple horizontal use case that can be used practically for anything as sarthak mentioned earlier any use case where you need real-time data that doesn't live on GCP this is pretty well suited for we're taking advantage of the fact that something like you know it's something like 70 75 % of Fortune 500 companies already run Kafka so the Assumption here is that that can be a vast source of data that we can easily tap into and start creating this real-time data pipeline once we have this flow of real-time data we can start building solutions with it right so if the flow is from a mainframe and you have financial data now you can start building some fraud analytics on that if you're a telecom you know you probably want to think about do I have anomalous activity at my antennas right like that might be something you're looking at but effectively the pattern we're talking about is the building the data infrastructure to get data to GCP and then you customer can build the business solution that you want to build one really cool pattern that we see a lot is using this for multi cloud so the you know a lot of customers have already you know they live in one cloud but they they started to think like we need to find the best of breed technologies across the clouds and use that and in the data analytics world that's GCP so a lot of customers want to move to GCP for bigquery but the data they want to use in bigquery might not live there so if the data if that's a Kafka user we can use that exact same pattern right hook up replicator confluent cloud and GCP and just start feeding bigquery so your system of record may live in a different cloud but you can actually use that data and pretty much near real-time in GCP in bigquery we actually have custom they're kind of doing this right so you need technologies who GCP and confluent a joint webinar with moved from another cloud to GCP by building this bridge and they moved to GCP because they wanted data flow and bigquery and they were a Kafka customer so this is the pattern they used so sir thick maybe you can just walk us through like how customers might use this for AI n mm yeah no Thank You Josh so this particular architecture diagram which you are seeing this is actual customer implementation for a customer in the banking space who wanted to handle a better fraud detection more near real depth fraud detection using G Series capabilities but they had tremendous data gravity on Prem because they are got a lot of data in mainframes in in Hadoop in Oracle here at a net set relying on Prem Plus they had a whole bunch of data from different from different stores to like mobile devices to web which is constantly coming in and all the systems are set up to receive the data thankfully they are already using Kafka to communicate across all these disparate systems as a message broker on trim and and what we did was we deployed constant cloud and BS function replicator like what we have been talking about to move some of the data to DCP now as a first step what they are doing is they are actually taking of fully data which is something every day rather in near real-time as the more data is coming in they're actually flowing is reading a flow for doing some processing storing the full data source our data are then a stream in bigquery because in bigquery now you can do a lot faster analysis across those terabytes or petabytes of data then what you could have done on Prem especially across all those discrete systems and then they are doing a visualizations of the data using local but probably more interesting is they are actually doing in a constant cloud they are actually sorting out some specific data streams and those data streams they are they are training some fraud models and they are actually passing it to those fraud they are deploying those fraud models and they are sending those data are to run those fraud models to determine what are those transactions have any fraudulent behavior in them and then finally they are using actually sending the data to our to account for topic on running unconstrained cloud so that they can then send it back that feedback downstream to their to their own Prem system so that based on on the transaction being determined to be fraudulent or not they can actually take actions on the systems it is not it is a second and you guys will will appreciate and understand and relate that this is though this particular case is part of fraud detection application but a very similar pattern can be applied to almost every other use case from consumer behavior analysis to market analysis every other business scenario which you can think of if it if it satisfy the criterias of like hailing you have you are already invested in Kafka on Prem and you want to start leveraging the GCP services like AI ml or visualizations or bigquery those things immediately without having to move all that it are neither have to wait for you to move all the later to cloud so this kind of sums up the core messaging with how can a hybrid data pipeline can solve those business problems so thank you very much for attending a session I want to hand it over to Josh who have a call to action for all of you so Josh please take it away Thanks yeah I think you know what I would love is if your if this sounds interesting to you please check out confluent cloud it's you can spin up a cop the cluster in about 10 seconds and confluent covers the first $200 for the first three months for everybody so it's effectively free to try check it out play around some k sequel and hopefully you love it thanks for joining 