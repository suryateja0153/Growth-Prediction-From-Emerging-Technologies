 (peaceful music) Hi, my name is Matt Maccaux, and I'm gonna talk about environments and the workloads that run in them as it relates to big data and analytical applications. So when we talk about big data and analytics, we think about Hadoop. And here's a traditional Hadoop environment that you probably have deployed in your organization, you've got an underlying storage and persistence layer, you've got a processing layer, you have a multi-threaded workload management layer, and you have a number of tools and databases that sit on top of it for access. Off to the side, we have the management tools, whether this is for ingesting data, discovering, curating, or moving that data around, that's again, very traditional in most big data environments. And then lastly we have the ecosystem of data science tools and applications that access these various tables to take advantage of these to perform those types of workloads. Now let's talk about the various environments that exist in most organizations. Here we have a lab, dev, UAT/integration environments, and a production environment. Yeah, you probably have a DR environment as well that's not pictured. And there may be a number of other environments that exist not pictured here as well. But this is fairly representative for what we see in the enterprise. So what happens in lab? Well, in the lab environment, we tend to have two different kinds of users: we have data scientists and data engineers doing work in this lab environment. And what sort of work are they doing? Well, they're doing experimentation regarding the tools that they wanna use to do their job. So in this case for a data engineer, they're probably picking the software packages and libraries that they're gonna use to ultimately move through the software development life cycle to operationalize their code into production. For data scientists, what they're doing here is they're picking the libraries and tools that they wanna use to ultimately solve for their task as well. As you can imagine, this is extremely high churn because of the iterative nature of the development process. I'm gonna try this tool and experiment with it. It may fail. I'm gonna try again. I'm gonna try again and again, so the churn is very high, and from an operational perspective, the SLA's are relatively low. If I break something in this environment, it may take a day or two to rebuild it and that's okay. We also don't have any real data in here because we don't need data in this environment. All we really need again is access to tools and libraries with relative freedom to do that exploration. And this environment tends to run on last generation or older infrastructure. And that's because we don't need the performance that we would potentially need later on in the software development life cycle. And so here we have underlying infrastructure that's gonna support these types of workloads. Once we've completed our work in the lab environment; we've picked the tools and libraries we want to use, we move to the development environment. Now, the development environment's a little different than the lab because we're gonna use some sort of representative or sample data, whether that's anonymized data or publicly available data, we are now going to take the libraries and tools that we selected, and we're gonna start building our application, writing our code, and building our predictive analytical models from a data science perspective. Here the churn is still relatively high. I'm still experimenting as a data scientist. I'm still writing code and iterating over that code as a data engineer. So my SLA's are gonna be relatively low again, so if I break something, it's part of that iterative process. It's okay. It may take a little while to rebuild because that churn is high. But again, remember, we don't have any sort of representative data in this environment. So the security levels are still relatively low as well. And just like the lab environment, because we're not running anything high performance at this point, we're probably running on infrastructure or environments that are smaller than we're gonna see later in the software development cycle potentially last gen infrastructure here as well. Once we finished our work in the development environment, we promote our code and our models to UAT/Integration. From a data engineering perspective, this is where we are gonna start hooking into our CICD Dev Ops pipelines. From a data science perspective, this is where we're gonna be training our models. And so in order to train our predictive models, we need access to real historical data in order to do so. We wanna get as much data as possible, and that may mean that our UAT/integration environment is gonna be mirror of the production environment. We may need just as much data that exists in prod sitting in UAT. And so because we're training our model and we're pushing into CICD pipelines, the churn should be much much lower. Yeah, the jobs may fail, or the model training may fail, so we may have to restart it. But the churn should be relatively lower than the SLA's because we're getting close to production should be quite a bit higher. But the security levels in this environment are what makes it different from DEV or lab. Because we have access to real production data in our UAT environment, the security levels require us to treat this as just like production or nearly like that. And because we're doing model training that may run over a long period of time, and we may be doing integration or performance testing in this environment, we want the underlying infrastructure to look a lot like production. So we need high memory boxes, high density or fast storage depending on how production is set up. And we may even have GPUs in this environment on that underlying infrastructure. Once we've finished with our work in the UAT/integration environment, we are now gonna promote our code and deploy our models into production. We all know what production is like. Production is very, very low churn. There shouldn't be very many changes in production. We should be pushing to prod only when the code has been tested and is stable. We've got our Dev Ops pipeline doing that potentially in an automated basis. But again, the churn is relatively low in prod, and the SLAs are extremely high. So with our SLA's, if something goes down in prod, we generally have a resolution in hours rather than in days. Of course we have access to real production data in production so the security requirements are the highest as well. Which means that the only real users that have access to production are system users running system jobs or our reporting users that are using known tools that generate well-formed sequel to access that production environment for reporting purposes. And of course in our production environment, very similar to our UAT where we are running the latest generation infrastructure. We probably have GPUs deployed to do the model processing that those require. We have high memory, high CPU disks as well as fast memory in this environment. Now that we've defined these different environments and the underlying infrastructure within them, we can start thinking about the workloads that sit in them. Now, we talked about SLA's, we talked about churn, we talked about security requirements, we talked about the users that access those and the underlying data in there. And if we started thinking about, rather than the environments, but the workloads or the jobs that run in them, we can start treating what we deploy in these environments a little differently. And so we've got a term that we use called RET. RET is a definition of the different types of workloads that we can run across these various environments. Let's define RET now. The first is restartable. A restartable job is anything that, if it fails, we can restart it without affecting the user or the system. An example here is a nightly batch job or potentially even model training. If that fails, it's okay. We can restart it, we can go gather the data again, we can program the steps in, and we can restart it with the only thing being lost is time. An anti example here is something that's real time processing. A real time processing system, if it fails, we lose the data, it gets dropped as part of that process. The next term is ephemeral. Ephemeral means anything that is created or destroyed on demand and that is relatively short lived. An example here would be a development environment or potentially automated tests that are part of a DEV ops process or even exploratory data science. These environments are created and destroyed on demand. Think about spinning up and spinning down these environments. An anti example here would be a pub sub feed through something like Kafka. The last term is temporal. Temporal means it's a job or a workload that has a finite run time. An example here would be a potential spark cluster that is used to process large amounts of data during the month end close for a particular business. That is again a well-defined time period it's gonna run for hours or minutes, and then it's going to end. An anti example would be an environment serving ad hoc or reporting for the BI team. That environment always needs to be on. It only gets shut down for regular maintenance because we can't predict when users are going to be using it. So that would be an anti example of temporal. Now that we've defined what restartable, ephemeral, and temporal means for these RET environments, we no longer need the terms for these various environments as we have here. So we no longer need to have a lab environment specifically dedicated to those purposes. We don't need a DEV environment specifically allocated to those purposes either, nor a UAT environment because if we're able to treat these workloads based on the SLA's, churn and security requirements, we can start deploying these workloads a little differently. And yes, that may even mean deploying some of these workloads that are production-like in a new and different manner. So rather than environments, we now have workloads that are defined as restartable, ephemeral, and temporal across that. And if we start looking at the underlying infrastructure to support these types of workloads, well, what we used to consider lab absolutely, DEV, and UAT no doubt, and even some of the production workloads that we had deployed in the past. And so if we now ring fence all of this infrastructure and make it available for these RET type workloads, we can now pool all of this into an internal cloud to get more agility across the infrastructure and drive more productivity to our users. So let's use an example. Let's say we have a data scientist that was previously going to work in the UAT environment to train an analytical model. Ordinarily this training process would take many many hours. And because the UAT environment may not have the latest and greatest GPU's, it would take even longer. Well, what if, overnight, we could schedule this workload or this job and leverage a couple of the under-utilized or otherwise idle GPUs that came in production. What if we came in here and grabbed some of those underlying resources for that particular workload to ensure that it completed in a faster period of time, ultimately driving more productivity for these users. Now we can only do that if we release that infrastructure when that work is done. If we don't release that infrastructure, well, we're gonna be in real trouble because the next day, production workloads take over. Perhaps we have badge processing that occurs first thing in the morning, or all of the reporting users come in and start hitting on this production environment. So we need to make sure that we can release these workloads and this underlying infrastructure when we're done with it. Well, now things start to get interesting if that's the case. If we can spin up and spin down environments based on the principles of RET, we start to get- we can start thinking about the infrastructure a little differently. We can think about separating compute from storage. If we now have all of our storage pooled on potentially cheap and deep infrastructure, and other storage or other data located on really thin and fast infrastructure, we can use intelligent software such as BlueData by HPE to deploy the applications and connect to the data, spin these environments up, these ephemeral environments and then spin them back down temporally when we're done with them, thus resulting in higher productivity for the various users that access these environments. And greater infrastructure utilization across this estate. Now we can buy underlying infrastructure that's better suited to the task at hand. Maybe we have a high demand for GPUs so we can buy those GPU enabled servers and deploy them where necessary and access that infrastructure on demand when we need it, as well as investing in that potentially cheap and deep storage as our data estate continues to grow. And so if we start thinking about our infrastructure that way, in terms of workloads, restartable, ephemeral, and temporal workloads, now we can get greater utilization of that infrastructure, we can keep up with the demands from the business, and the business will get the sort of SLA's and productivity that they expect from IT. And all of that is powered and provided by HPE BlueData. Learn more about HPE BlueData here. 