 okay so my name is michael moore i'm a executive director at Ernst & Young I'm responsible for our relationship with neo4j we've been working with neo4j closely now for about three years and I lead our national practice and knowledge graphs and AI and I'm a huge proponent of graph based technology I think this is the this is the wave of the future I bet that probably 50% of all sequel workloads are going to move to graphs in the next ten years and so you guys are absolutely at the right event this is this is a real conversation that's happening if your organizations are thinking about this this is a great place to start and this isn't you know this isn't like a you know a fad or a flash and the plan graphs are here to stay there's lots of investment in them there's a real market it's a competitive market neo4j is the most established player in this market and we're pleased to be able to work with them closely so today what I'm going to talk about is how you begin the graph conversation at your company and I want to introduce this notion with a couple of things that are probably you've probably been observing these in the background but essentially we're in the middle of a paradigm shift between going from kind of a traditional single data model single vendor approach for management of data where you take your data you transform it so it fits into a sequel style database whether it's Oracle Terra day a sequel server now in the last ten years we've seen the emergence of fit for purpose databases where the design of the database is actually designed around the shape of your data and that's why we have these proliferation of no sequel databases you know where there's document stores there's key value stores columnar databases time series databases and in the new paradigm the idea is you pick the data platform that fits the shape of your data and if your data has a lot of relationships in it and the relationships are the subject of interest then you would pick a graph database and then these databases are now at this stage very interoperable they're part of modern data management stacks and then they're typically surrounded by a bunch of ancillary services to bring those stacks to life so why are graphs a story so graphs have been around for a long time but what's making graphs interesting is that the hardware that's available now allows you to literally take an entire data warehouse and put it on a single server and I live in Seattle which is the middle of cloud country and so I get to see these wars going back and forth and so a few years ago AWS made a big splash when they started releasing their x1 line of virtual machines with 4 terabytes of memory that's a lot of memory and everyone said wow that's really cool and so then of course Microsoft is 13 miles away said you know what we're gonna do that too and so then they released the next year 12 terabyte machines and Amazon Mona said well we'll do you know we'll go up to the next next level and we'll meet that and then now if you go out onto a sure you'll see some fine print where if you pick up the phone and talk to your friendly as your representative you can actually get a 24 terabyte VM so think about that for a second 24 terabytes is a huge amount of RAM and in fact if even if that's not enough you can go out and talk to IBM and they will gladly sell you a piece of hardware where you can have you know up to 500 terabytes of RAM so I think that you know in the next 5-10 years we're gonna literally see petabyte scale VMS and so when you have that much memory at your disposal why not put your data into a graph where you can materialize all the relationships between your data points and have a whole fabric that you can query instantly right and so so scale up is is you know so big hardware is kind of the story behind a lot of graft popularity there's a website out there called DB engines comm and DB engines comm basically tracks the developer conversation around new technologies and so they look at things like you stackoverflow posts they look at pull requests on github and a bunch of other other things and one of the really consistent stories is that as far as the data development community is concerned graphs are hot and graph the popularity of graph continues to be the fastest increasing area of interest among developers so what is a graph database so a graph database derives its structure and what it does from the basics of a graph and so for those of you who have never seen a graph you've probably actually you've probably actually written many of them so anytime you're holding a dry erase marker and you're standing in front of a whiteboard and you begin to draw circles and bubbles to explain a business process you're building a graph and so what a graph really is is it's a visual representation of how data is connected and so and it's typical to describe business processes in terms of a graph and so this is a very you know basic example you know so what's you know ecommerce we send email to people we want them to go to our website and buy our product that's sold on the website if I was to graph that business process it would look like something on the left and so I'd have a bunch of emails I'd be sending those to people I want those people to visit my website and I want them to do something on the website like purchase a product that's in stock on that website and so what's a graph database so a graph database is a database that's optimized for the storage querying and management of data in graph structure and so in a graph database you would have millions of records potentially being your emails all your outbound emails each one of those emails that have been sent to an individual person so you might have hundreds of thousands of people there would be a relationship a hard-coded relationship represented as a data point that's stored on disk in memory and because it's a real object in the database you can actually put data on that relationship those relationships will typically be typed in the property graph model which means that they have a name and so then they become part of the semantics of graph you can also see that they have things like directionality so I can say that an email was sent to a person and that makes sense that's that's how it works in the real world and so graphs have good semantic fidelity to business processes which is one of the reasons why they're very easy to be able to you know have a very good discussion with your business stakeholders do I understand your business have I represented it properly have I built a structure that we can query and get good answers out of now the query language for neo4j is called cipher and cipher is a declarative language like sequel but instead of having to declare what tables I want to join with what other tape tables and having knowledge of the keys that I need to drive those joins in a graph database query what you're actually doing is you're declaring a traversal path through the graph and so here's a little query here and you can see that we're saying find all the emails that were sent to this person named Steve and so there's a single Steve record and that record is part of the person's set of nodes now where I'm going to find all the emails that are connected to Steve and we're also going to require that Steve is a has visited the website and he's purchased something that's been sold on the website and if that path is true I'll receive back a row of data from the database and in fact I'll receive back a row of day of data from the graph database in every instance where that path is true on all possible traversals that fit this pattern and that's how you query a graph database okay so one of the first things you want to do when thinking about your graph projects is you've got to get your graph goggles on and so does everyone see the graph here right anything that's a network is a graph how about here does anyone see the graph anything that has complex hierarchy is a graph you want to see the graph here anything that has flows from point A to point B whether its supply chain or materials is a graph so you want to see the graph here any type of transactions involving people and objects like say products or bank accounts right is a graph okay and so so you want to think about your you know the you know getting your graph call goes on thinking about data in in a newer way and of course the other thing I want to point out is that any sequel database is actually a graph and the very first thing that you do when you design a warehouse is you sit down and you build the concept diagram for your data warehouse and that is a graph and I and I believe that if if Larry Ellison had as much computing power back in nineteen seventy or eighty when he was putting together the first Oracle database he probably would have done something that would be would look a lot like neo4j but he didn't and so everything you see in sequel is basically a workaround designed to handle limited compute and limited memory that's why the users pay the cost of running a query what do I mean by that so when you run a query in sequel the end user is the one who's waiting for those joins to be computed in memory every single time every single query in a graph database those joins are computed at write time and so the trade-off there is it a graph is a little slower to write to but it's way faster to read from and you usually have far more reads than you do writes okay all right so graph use cases lots of use cases most of our customers start with a customer 360 view or an asset 360 view and usually the reason for that is that most of the data that goes into these kinds of views is coming from a variety of legacy systems and maybe they've landed the data in a data Lake but they're still having trouble unifying the data because there's so many disparate entities and so customer 360 is a good place to start thinking about your graph problem but there's many other use cases oftentimes the customer 360 work will quickly turn into some kind of a wreck Commendation engine or a detection engine because pattern recognition is very easy to do in graphs and so you know common patterns become recommendations rare patterns are usually some evidence of something going awry so maybe it's a fraud ring or some point of failure and it's in a network system things like that generally speaking the way you want to think about your graph is we feel that their best deployed on top of data links this this is a very common thread through most of our graph engagements and usually what's happening is there'll be some team and you can look at those buckets across the top there that is dissatisfied with the level of reporting and analytics that they're getting out of the data like and they want to do something that's that's more powerful so we might start maybe it's over on the marketing side or we maybe we're starting on the sales side or somewhere in between and operations but generally speaking you know there's some need it's either an application it's better bi or it's empowering the data science team to be able to look at a lot more data that's sitting in the data lake and then this gives you a kind of a nice stack so conceptually what you're really doing is you're saying you know we're going to pull we're gonna pull our important data not all of the data that's in the lake but the important data that's in the lake up into the graph we're going to use the graph as a vehicle for creating a common business language we're going to use the graph to impose some standards around taxonomy and concordance and then and then we're going to hook our applications up to that that level or that layer that that connected layer of data and that's what's going to drive our next generation applications and bi and advanced analytics and so this this is and then I'll also point out once you handle one or two use cases it's very common for the business to come back to you and say all right now that you've solved this use case can you bring in this other this other data set and with this kind of a general approach it's actually very easy to do okay so let's talk about this roadmap for enterprise graph strategy and so the scenario here is you know one of you in this room has decided you know what I want to bring graphs into my business how do I do it how do I win at this conversation how do I move the business and so generally speaking you want to begin with identifying your graphi problem you'll build a localhost just on your laptop a small POC you'll move that POC into a cloud pilot and then you'll eventually graduate that into a production build and like most data work you know you need to understand your use cases so you've gathered your stakeholder input you'll spend some time designing your graph and the thing that I want to point out is when you do graph design it's a very iterative process graphs are very easy to manipulate they're easy to change the design and schemas involved in the graph and so you don't have to be right the very first time it's not like building a data warehouse so you can come in with a theory of how the data should be connected and if the query loads or your business stakeholder input tells you something different it's actually easy to change the graph and adapt it very flexibly and so even in bigger graphs we always sometimes we always revisit the design of the graph data work is is inescapable you always have to work on data and one of the things that we often see when we do these kind of unification projects across multiple data sources that are in a data Lake is that it's it's in the graph where you actually begin to detect some of the real issues with data because you're actually slamming data together that has maybe never been joined together and then finally you know there's a lot of great ways of interacting with the with the graph has seen lots of visualizations and things like that generally speaking most graph stacks will have some kind of a middleware layer that is supporting the api's that are talking to your applications or your BI stack and then from there on it proceeds like any other IT project you don't need a large team to build a graph most of the teams that we that we bring in or our clients stand up are generally around four to five people and usually there's somebody who is a champion and so I'm hoping one of you in this room will become a champion for your company but you don't need it you don't need a huge team and so typically you'd have a graph architect you might have a data engineer who understands the environment maybe a developer of some sort depending on what use case you wanted you want to demonstrate and that you know might and then maybe some data scientist from maybe some report developers so general you know small agile team works quite well okay so what is a graph you problem so this is just a hit list I'll go through it super quickly so anything requiring a lot of entities anything requiring and recursion where I'm joining the same data to itself anything where I've got complex hierarchy scenarios where it's the relationships that are important not necess anything that requires very fast query results so neo4j as an in-memory graph is super performant you know you should expect queries to be coming back particularly like singleton queries where i'm passing in a customer ID and i want to interrogate all the nodes around that customer you know 200 milliseconds 100 milliseconds or faster you know and then if you can't figure one out on your own go talk to your business don't be shy anybody who's who is running some form of business in your company will have a very long backlog of analytical questions that they've never been able to ask an answer of the data that's under management you can go to this great website called neo4j dot-com slash download and you can actually download neo4j onto your laptop and the version of neo4j that you're downloading is the enterprise version and it's got a really nice desktop application wrapper around it that lets you stand up multiple small databases on your laptop and so the first thing I would do is encourage you to go there download it and then if you use there's a really nice package of utilities called a puck and when you download the desktop there's a button there and where you can install a POC with a single click and then there's a little configuration page and if you type in these configurations here you can actually load data using a POC very easily and so you can begin with some CSV files there's a couple of ways you can do it you can there's a neo4j procedure there called load CSV and you can see I've got some local file path and I'm dumping my CSV files into my graph or I can use the apoc one and I can load from a URL and that URL cannot be a file path or it can be a remote web location and any reasonable laptop you know like a like an 8 or 16 gig machine you can build a graph that has millions of nodes in it graphs are very very compact and so your your localhost demo can actually be a very powerful demo in terms of the amount of data you can push into it yep some of the things that you might want to think about when you do your POC graph is you know just start with a handful of data sources don't don't attempt to boil the ocean use common sense business naming for all your graph components so that when you show your graph to your business they immediately recognize what it is you're trying to represent and it can give you the kind of feedback you can handle things like recursion and your graph design so there's a little query there you know an employee reports to an employee right that's totally legitimate way of designing a graph right and so you can handle things like complex recursion and things like that at this stage you don't have to get too hung up on the design of your graph you know might think about well should this be a property should this be a node you know those kinds of things you can deal with later but at this stage you're just trying to get you know is there a data domain a set of business problems that I can put some data together in a graph and run a new set of queries that our business has never seen be executed before if you want to show your graph to other people there's a great website it's HTTP APC Jones have to put a WWE from that to get it to work that URL but there you can basically create schematics of your design your and there's a couple of other ways to do that but the main thing to do is get your graph together on your laptop do it yourself get comfortable with the neo4j platform and then write some queries against its simple queries that can test your graph design and then when you get to that point then you can have a really nice conversation with your business stakeholders and they can probably suggest additional questions for you so I'll give you some examples of graphs schemas to get you thinking so we talked a little bit about customer 360 so one of the reasons why customer 360 is an interesting problem is because most businesses have customers and the more you know about your customer you know the better off you are customer data has many different shapes and sources and typically like this is a b2c example you know I have a I have got my customer node up there at the top so that you know I might have hundreds or millions of customers and then related to the every one of those individual customers I might have data about their demographics and how they've been segmented I might have a whole variety of marketing touches a crop coming across different channels they have probably executed transactions so if they're a customer that so that means that they have a product history they have individual orders they've paid for those orders with different forms of tender they may have come through either my physical stores or my or my ecommerce stores or maybe through a partner and then I have a whole bunch of identity information for that customer all of these concepts you can pull together and relate in a single graph which then gives you incredible analytical power here's another type of scenario so this is a little bit more operational so here the question on the table is what is my procurement process look like and and the main entity here is a store location and so this store so imagine it is like a grocery store for example is has a whole product supply chain so they're basically sourcing product you know perishables and non perishables from a variety of distributors those are being those are being you know provided to the store this they're getting invoiced for those etc for example the perishables and and the non perishables might be being generated in specific Lots they all have SKUs so maybe I have some tracking and traceability that I that I want to be able to to impose on that data every one of my invoices has detailed line items and then I have a whole bunch of service providers who are doing things like cleaning the store or doing the landscaping etc and and all of and all of these business entities whether they're a supplier or a service provider are operating under a set of contracts those contracts are with entities those entities that roll up to maybe parent entities and then if I'm the chief procurement officer I can look at a graph like this and I can say how much am i spending through this network of entities and could I do a consolidation and get better pricing leverage and so and the point here is that with a graph like this in a single data fabric you can traverse from a master services agreement all the way down to the individual line items in an invoice and so you have so you have a very wide span of hierarchy plus a bunch of transactional data that you can use analytically here's another type of graph so this is master data management in b2b and so the concept here is that my customer in a b2b scenario is an account it's another company right and the print the the perennial problem with b2b data is I need to know what contacts I can I can go talk to in that account and and you know in a large enterprise that could be a difficult problem to manage one of the things that you can do in a graph is that you can actually use the graph to compute a best or golden record from a variety of different data sources and so that and the idea here is that for every one of the contexts that I have I can explode their contact information into a phone and email a physical address and I can use the graph to understand the data lineage so what source did this phone number come from so was it a phone number that was on an original contract from three years ago or was it a phone number that was provided to me in an in-person event that we had last week right two totally different sources for a phone number and you could basically create a rule that says you know what I'm going to give more authority to the phone number that I saw last week right but now for a physical address I might do the opposite I might say you know what we're gonna use the physical address that's on a contract or that's on a ship that's a ship to destination rather than a physical address that I got you know off of you know some form associated with a webinar right and so what you can do is you can take all of the sources you can ship you can trace the individual contact core data elements back to those sources you can put some rules on how much authority you want to ascribe to each of those sources and then you can compute a probability of accuracy and hang that probability on the relationship and so now when I query my contact not only am i querying for the for the details of the contact but I can actually write my queries such that I can say give me the highest probability phone number give me the highest probability physical address and email and so that you can have basically have a very dynamic master data management approach that will that recognizes the provenance of all of the individual details all right so just to finish up this point so so you're still in laptop mode you've got your data designed together and so you want to think about things like what are you what are the breakthrough queries there's opportunities to apply advanced analytics there's some really nice algorithms that ship with neo4j and you know you want to be use case specific you know feel free there's projects you can go out onto github and clone and you can see how full stacks interact with neo4j you can use there's some there's a nice new interface called graph graph QL which is a new type of API and and the main thing is you know you want to focus on what is the value that you're being able to drive by putting the data in the graph so now we know we can make you know now that we know this we can make better decisions we can now do these kinds of things for our customers we can get these new kinds of experiences off the ground at this stage you'll begin to think about all right how do i scale it and how do i actually expose my graph design to a totality of data and this is this is the point where you start thinking about a cloud pilot a lot of our customers like to do pilots in clouds but you can also do it on Prem and all of the cloud platforms have marketplace images or a.m. eyes where you can basically with a few clicks install a neo4j server single server or even a cluster and generally speaking what you want to do is set up those servers so that they have attached drive so you put your data on the attached drive and that way you can scale up or down your compute depending on how big your graph gets and if you're talking about sizing your machines generally speaking rule of thumb you'll need a VM that's about 50% of the size of the equivalent sequel database on disk Python is very handy there's a high-speed loader for slamming in all of your initial data and and then usually at this point you'll start thinking about you know what kind of data cleansing you need and and if you get stuck on this stage you can always reach out to your friendly system integrator like my team a DUI or neo4j which has a very strong services offering as well and you know we can help you get over any difficulties you might have as you approach your cut your cloud pilot I want to share with you the model that we use and so the idea here is that you want to accelerate to discrete and user experiences so as much as you might be interested in data and the inner workings of different types of databases and data management your management doesn't really care how you do that what they really care about is how are you going to make that how are you gonna help the company drive more revenue drive better customer engagement or be a more efficient that's basically it right and so as you think about your pilot you want to think about ok so I've got this great data environment now what can I illuminate with that what are the new use cases that I can drive and so my general recommendation here is begin with you know the Minimum Viable set of data domains to three data domains that are important put neo4j on top of those data domains figure out what is the minimum amount of middleware you need to build and then take it all the way out to the edge so build a new website build a new widget that can be deployed on the website build a new suite of bi reports the show the power of connected data and if you don't do this part you run a strong risk of having a very interesting graph POC they that just gets put on the Shelf right and so you know your job is to basically work with your business stakeholders and figure out what is that what is going to be this new experience that will show the value of connected data whether it's D analytics faster performance etc once you've got that done it's actually usually pretty easy to add additional experiences and then you can circle back around and begin to add new data domains but you don't want to do is you know go down that boil the ocean approach where you say we're going to take everything that's in our data warehouse we're gonna build some massive enterprise data model we're gonna dump all that into the graph and then we're gonna go looking for use cases that's a recipe for failure all right so here's I'll build these out really quickly here's some example architectures these are pointing a little bit towards sales and marketing use cases this is a sure and you know the theme here is that we're is that you're putting neo4j on top of a data Lake and maybe some of the other polyglot persistence databases that we've we talked about earlier usually you're these you know these environments are surrounded by a set of additional value-added services that ship with the platform and then usually you're going to be you've got some kind of execution layer whether that's a website or you know your data science team that is connecting to the graph through either drivers or api's so this is the azure example here's the similar one for AWS and then this is just to get you thinking about you know depending on what cloud you might use I'll point out Ketel has a has a neo4j plugin that allows you to set up ETL data pipelines that can write directly to neo4j neo4j also has connectors to systems like snowflake 3 JDBC or ODBC and also has connectors into things like Couchbase and MongoDB right so you can put you so you can you have a lot of flexibility all of your applications neo4j supports drivers for dotnet Java JavaScript and Python and then there's a bunch of community drivers as well alright so as you begin to build your bigger graph this is a this is a peek at what you can do with the neo4j loader so there's a high speed loader and generally what you'll do is you'll take your data out of your data Lake you'll you'll create what are known as graph form tables so table deep tables of nodes deep tables of mapped relationships between those nodes so think of like maybe it's just the primary and foreign keys that relate your data you'll zip those you'll export those files as CSV s and zip them up and then you can load those compressed files and this is an example from a couple years ago so this graph had on initial load four hundred and fifty eight million nodes 2.2 billion relationships and nine billion properties and it loaded that in an hour and a half so very fast with a lot of data so I also want to walk you through a quick example from oil and gas that brings to life a little bit of this kind of stack so here is there's an open source data set out there that it's called the vole field data set and what it consists of is about 10 years of data that was pulled from an oil field in the North Sea and it has exploration data and and production data in it and in the oil and gas industry probably 95 percent of the data that's used is actually unstructured data and so as a great example to show how you can use neo4j to do data unification and so in this example we're using Azure Couchbase neo4j and power bi and so here's the basic data flow and what I want you to take away from this is the degree of compression you get using a metadata management approach so we start off with about five terabytes of data all of the semi structured data there's lots of xml's that are coming off of these monitoring applications etc those are all converted into jason's there's a bunch of reports that are being generated also presented in XML those are also converted into documents and Couchbase in southern Couchbase becomes the document repository and then metadata and selected commentary is then pushed into neo4j along with all the structured data and along with every single pointer to every single blob that is sitting in Azure and all of that data is then pulled together in the graph and so what you're looking at here is essentially a mixed use graph where it's got some structured data it has pointers and extracts from the semi structured data and it has pointers to all of the unstructured data and so here in a in a relatively small graph this graph is about a gigabyte in size we actually have real-time access to five terabytes of data and this is just an example of some of the analytics that we can drive so in the world of oil and gas when something falls down the hole and you're doing drilling that's called that results in a phase of exploration called fishing where you're trying to pull that broken bit up out of that hole that's a mile deep or so and that's a real bad place to be because you're wasting a lot of money and everything has stopped and tell that that bit of broken equipment is fished out of the hole and so of course the obvious question is is who was who was operating the drill rig when the thing fell down the hole right and so here's an example neo4j query offered this off of this database and so we can basically pull up every single instance out of this five terabyte of data where time was lost due to fishing we can map that back to the well that had occurred on we can map it back to a set of reports and ultimately the contractor that was on duty while that was occurring and then you can have a different kind of conversation with that contractor so anyway some screenshots so this is Asher's blobstore this is typical data so this is a seismic profile and so you know and you can't really do anything with this data except pull the file and then it has to be visualized by some specialized application but in neo4j we can at least find it access it search it and and present it this is Couchbase using full text search and so here we're searching for injuries and then this is a graph QL demo where we're using graph QL on top of neo4j and this is a react page and there's an Apollo driver that allows you to access data through the graph QL API and here we're running full text search and if I wanted to I could click on any one of those little blue little blue clouds and I could pull up the the raw PDF so again that's an application where we're accessing this five terabytes of data and then this is a little bit on how graph QL works so graph QL is a structured as a structured API and it works on a slightly different premise so essentially you're posting a a graph type query and then there's a and then the neo4j graph QL driver and API is basically converting that into cipher and then handing back the result and so this is essentially a post request and then as such you can actually integrate it directly into tools like power bi and so anyway so as you think about you know broadening your view around analytics and really leveraging neo4j so recognize that you can use neo4j not only for direct management of data but indirect management of data and if you construct your modern data management stack so you have some of these other fit for purpose capabilities you can use neo4j to knit all of that together and I think that's exceptionally powerful so finally as you go to production all the regular IT stuff kicks in right so you have to think about your best practices your security model you can integrate neo4j you know with with Active Directory groups through LDAP or Kerberos you'll typically want some kind of a set of set of environments where you have you know dev test stage and production clusters you want to be able to have scripted deployments using your favorite DevOps package like Jensen our Jenkins or ansible so that you can have consistent deployments etc etc one of the things I'll point out is that you want to think about monitoring your graph so you understand the kinds of queries that are hitting your graph and then maybe periodically addressing the structure of the graph to see if it's really optimized to handle those queries and I made that made that comment earlier and then and then finally you know leverage that iframe model so once you're in production on a set of use cases you know now you can then go back to the business and say look you know we we stood up this new we stood up this new experience it's working really well and now we have an opportunity to see if we can drive additional business value by adding new use cases this is a screenshot of an application that has been in production now for almost a year it's a Royal Caribbean and so if you go download the Royal Caribbean app we have a recommendation engine that provides suggested activities for passengers and so this is just you know another form factor so you can push push out to your mobile devices not just websites and so forth so finally I want to touch on these key conversations so as you go in to your thinking about how you're going to stand up graph at your company so figure out you know what problems of this graph going to solve what questions can we now ask an answer of the graph but we weren't able to before how much did what kind of data do we need to connect what are we trying to do with the new experience what's the feedback from the business on these new experiences and so think about it as an agile iterative conversation with your business all right just to give you a sense of timing we've done lots and lots of these graphs deployments in a variety of sectors the thing that I want to call out here is that you should have the expectation of being able to build quite large graphs in a relatively short timeframe and by short time frame I mean getting to like a working prototype using real data and typically you know we're able with with one of those small teams to get a working graph together and somewhere you know between you know 16 12 to 16 weeks this is a very fast process it's way faster than building a data warehouse all right and that is the end of my talk if you have any questions feel free to ask [Applause] 