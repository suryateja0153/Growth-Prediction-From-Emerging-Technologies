 it's 11 30 we think you can get started um  i just want to thank you all for joining   us for this concurrent session on the theme of  managing data and research products effectively   we'll have three presentations today each for a  total of 15 minutes that includes about 12 minutes   of talk time followed by about three minutes for  q a session after all the three talks are complete   there'll be more time for questions and then  we can also have some general discussion for   the final 15 minutes of the tsar audience  you're welcome to post your questions in the   zoom chat i will look for those questions  and then share them with the presenters   and presenters i when you've reached about 11  minutes in your talk i will interrupt your talk   and i'll let you for the time um so the first  presentation today is titled hubzero hub drive   prototype for projects and publications  it is a pre-recorded uh presentation but   the presenter james graves is here for any  live q a to follow after the presentation if you want to start the presentation hello  i'm james brian graves a computer scientist   researcher educator and community organizer  i've been working as a contractor for hub zero   off and on since 2015. hub zero is based in san  diego at the san diego super computer center   sdsc at ucsd university of california at san  diego but i'm currently based in amsterdam   and welcome to my office in  the netherlands what is subzero i'm glad you asked the hub zero platform is an  open source cyber infrastructure where research   teams can host analytical tools publish data  share resources collaborate and build communities   the hubzero platform originated from the nanohub  project in 2002 and 18 years later the platform   now collaborates directly with over 35 hubs  or science gateways using hubzero's framework   all hosting research projects these projects can  range from hard sciences such as civil engineering   differential equation modeling geospatial modeling   and even soft sciences such as intellectual  intercultural learning and k-12 education today i would like to tell you about hub drive  a prototype collaboration tool for teams which   facilitates the content management in the hub zero  cms for example around projects and publications   in 2018 michael zettner of hub zero and  gerhardt clinic of nanohub mentioned the   interest in a desktop tool which fits more into  the reach the researchers workflow not unlike   google drive with dropbox i had already  been working with the hypercore protocol   formerly the dat protocol and immediately saw  the potential for implementing such a feature   with this new peer-to-peer based technology  what is peer-to-peer well i'm glad you asked the traditional internet is built on  a client server architecture this puts the   server in a central role that serves data  basically to end users so if i have two   people who want to collaborate let's say brian  sitting over here in james sitting over here and   james wants to share a file with brian he first  has to upload that file to the main server as a   client and then brian's client needs to make  a request to that server to get the file back   this has worked great it's scaled this is how  hubzero's infrastructure has worked for years and   probably will continue to do so in the short term  but it has one minor flaw which is if the server   ever disappears there's no result redundancy to  the content requested by brian and shared by james   the other side there's peer technology in this  case you can have james here and brian here and brian can make a request to  james for a certain piece of content   and james can send that to brian directly  similarly you can have a server acting also   as a peer interested in content shared by james  and also interested in content shared by brian but   not so relevant in this particular use case and if  that server disappears then this sharing can still   happen that data is still available there's a  redundancy there similarly the ability of hubzero   to sit as a peer in that system allows it to pin  content shared by james and this is at the core of   how pub drive works uh content is pinned on that  server and while it's pinned on that server it   becomes available to the traditional cms system  and allows it to be easily integrated into our   projects and publications components again  long sense part of the hub zero uh offering data is a protocol for sharing data between  computers dad's strengths are that the data   is hosted and distributed on many computers  on the network and that it can work offline   or with fortnite poor connectivity and that  the original uploader can add or modify   data while keeping full history all the while  handling even large amounts of data is it secure i'm glad you asked dad transfers files over  an encrypted connection using state of the art   cryptography only users with your unique read key  can access your files allowing them to download   view or re-share the files to update the contents  of debt users must have must have the right key   by verifying hashes during transfer that makes  sure that no data is altered or corrupted as long   as you don't share the read key outside of your  organization then the content will remain private here's a quick demonstration of  dat we're going to make a directory   called our amazing research and we're going to put  some data there some trivial data just going to   echo alice into bob now we can just type dat share  and then that's going to generate for us a special   cryptographic url which james  can then share with brian hi i'm brian brian can then get that  url quickly open up his terminal   and then type dat clone and paste the url not  unlike git and just like that with an open dot   getting a finder window you can see that the  folder was easily transferred containing the   data from james yep got that data looks amazing  now let's see how hubdrive works i'm doing some   amazing research on homomorphic encryption with cl  with my collaborator brian we would like to bundle   that research into a hub zero collaborative  project and ultimately publish the results   brian is a bit better at writing than myself so  i'll just drop the results in my hub drive folder   so that he can get to work on that publication  the first thing i have to do is generate some   data for the publication hubdrive fits seamlessly  into my workflow and i can just simply use my   terminal create some output put it in a folder  and now by just dragging that folder into hub   drive i can create a hubzero based project with  files that i can share with brian looks like i   got a message from james indeed yo brian i added  some output from the quantum encryption research   ah i guess i should start that publication  uh yeah indeed i do see in my hub drive   the qe output project it's also synced  with my finder the output's sitting there   fantastic i'm gonna just open up hubzero here  go to projects uh ideally it's gonna be there   yeah indeed i see the quantum output  of project let's start that publication   click publications start publication boom  done scrolling down let's add that output   uh here yeah there it is output one whoa whoa  let's uh rewind a little bit and and have a look   at what just happened there not only was james  able to create a scenario where brian had those   uh output files synced to his local file system  but they were also available as a project with   files attached in the hub zero cms system  pretty amazing now we probably need a little   bit more output than just that i'm going to  ask james for a little bit more output here i think we need more outs put more  outputs ah well he's not wrong just let me   jump over here again my normal workflow so i'll  just run my script output that into output to   text and then i can let brian know that there  is some new data there just added some okay james added more data great let's have a look  in the project already synced yep it is but something's off here   what the data's no good all right i guess  i'll just generate some new output here okay uh yeah remove the last data let's uh  kill that uh rm yeah yep output two done   uh should be deleted boom is it gone yeah looks gone  to me hey yep great thanks ah ah yeah make more data of course all right  so let me uh run my script uh one more time   yeah okay uh should be all synced up now ah done great all right let's see uh is it  there indeed it is let's select one uh three   uh looks great i think our publication  is ready to go let's ship it man thanks so i guess that was the end of the  presentation james is here for any questions sorry um i have a question yes go  ahead okay um hi james this is lee um   this is probably a silly question but uh i  just be clear does that mean that the james   created the data i think you said and brian was  reading the data did i get that right the person   reading the data there could not delete it it's  only a read only or is there a possibility that   that person could also edit the data uh yeah so  i think in in this case there's a lot uh that   can be done from a user experience standpoint  but the intention of uh showing it in this way   was all the changes that james was making as  the data creator were then instantly being   translated to both brian's local file system and  the file system that was sitting on the gateway   so um yeah it was a bit of a contrived use case  that uh that james deleted it but uh but i did   want to show that um that brian was getting those  sinks okay thank you i liked it oh thank you um sorry go ahead um hi james um this is carol  song i'm wondering so you showed the command line   so for people who don't know how to use command  line how would they uh use that do they just   create folders in the finder or how does that work  yeah the the command line is not necessary at all   in this case um what i was trying to show with  the utilization of the command line and both the   and also the finder so the the folder window uh  was that the intention here is just by modifying   the file system so whether it's with a traditional  sort of file explorer or the command line or maybe   even saved as as long as that file is changing in  that folder then it's syncing uh to the cms system   and to the collaborators uh local file systems so  it's not necessary uh that's i probably just chose   that tool as an example because i'm uh more of  the nerdy programmer type but it could have been   literally any tool so it sounds like  it seems similar to things like dropbox google drive uh the client application so   um then do people need to download and  install a hub drive app or something like that   yeah the idea would be that this would be a piece  of standalone software that sits on your computer   i think the differentiation between this  technology and something like dropbox and google   is uh well it's just the the information stays in  your organization as opposed to going to a third   party and being hosted there and then also if  for whatever reason uh your hub or your gateway   falls over you can actually still request  that data directly from other peers in the   system so it can go from researcher to researcher  without ever making a stop on a central server   thank you looking forward to trying that out yeah  cool thanks for the question and nice to see you   any additional questions for james hi i'm sorry my  name is lan zhao um i have a question is there a   size limit on the amount of data that can  be used this way like if i have a very large   modeling output would it cause a problem to sync  with the other side yeah i think that's super   interesting question uh we've always had size  limits uh on the cms system um kind of based on   kind of usage tiers uh but there's nothing from  a protocol standpoint that would prevent you   from transferring large data from researcher  to researcher so it may be that the there may   be limitations on the cms side um but again the  protocol will let you send terabytes if you want   so you mentioned a little bit earlier like   this technology also allows a peer-to-peer type  of transfer without going through the hub cms so   is this uh automatically configured to do that  to it to do it that way you don't need to the   user don't need to do anything right it's already  set that way exactly yeah no special configuration   you don't have to be a hacker or something  to figure it out it just works okay thank   you great thank you so we we can continue with the  questions right after the three presentations um   so we can move the next one the next presentation  is uh titled an open source managed file transfer   framework for science gateways and it  is presented by dimutto vani puraga um can you hear me now yes yes yeah um hello um so uh i'm dimita  vandepara gay and i'm a research engineer at   cyber infrastructure integration research center  at indiana university and i'm going to uh talk   about a product called ira out mft which is an  open source managed file transfer framework for   science gateways so before going into the product details i i i  would like to um go to some of the challenges we   have seen in specifically in the file transfer  scenarios in gateways so the first one is uh   the close storage transfers so basically you  might have different type of storage types   and then how do we do the protocol matching and  the protocol translation between different storage   types when if you are trying i did translate  to a gateway the other one is the performance   as always um but the performance might go in  two ways in for example let's say if you have   a private dedicated link across two endpoints  then you can you utilize the maximum boundary   but in some cases you are transferring data  to a public link in that case you have to be   much fair and give the opportunity  to the other transfer sensory   and the other one is the data path optimization  so this comes into the the picture in the cases   where you have end points having multiple  network links and the transfer solution should   be smart enough to pick the right network link  to get the maximum throughput or in some cases   even you have to reduce the network  cost especially in the cloud endpoints   the security is also very important in the  cases where you have sensitive data sets to   transfer so in that case you have to go through  more constrained and secure paths but in some   cases you have public data sets which doesn't  need that much of security but in that cases   you can focus more on the performance of the data  transfer and in either case you have to be very   careful about the integrity and and the integrator  should be preserved at the end of the transfer   and this is like a very recent use cases and like  very like famous these days people are bringing   their own storages into the gateways especially  with the the advancement of the cloud endpoints   then cloud resources so the in that case the  gateway should be able to plug new storage   and points easily and it should be accessible  for the end users to the gateway apis of the   uis so these are some of the data solutions that  we have observed we categorize them based on their   um open source um so like community  adoption or the deployment mode   so so we have like discussed about  this on in detail in the paper   so then i would like to go back to like  what's irat and nifty so i don't think   mfp is you know is not itself a product why  we call it like a framework for people to   deploy their own managed transfer frame platforms  so it's uh it's designed in a way like it's fully   pluggable you can configure your own credential  backend and the resource backend to the   mft framework uh so then you don't have to  manually copy or clone the credentials of your   existing gateway into the inventory framework  so the mft framework can automatically call to   your existing backend and fetch the data and  it has easily extendable protocol interfaces   for example you can write your own protocol  implementation for the data transfer or you can   even improve the existing protocol implementation  like sfpp and you can easily plug the new version   and so the ira the mft doesn't come with  a fixed deployment pattern so it could be   vary based on the quality of service you want  and even the security and even the maintenance   complexity you want so i will cover this  topic in more detail in the later slides   and and the other aspect is that uh sometimes uh  it it will be a concerning matter when the data   flows through your gateway back-ends so if  you if if you use uh like the mfd platform   like this uh like there are ways you can isolate  the data paths and the control paths from the   gateway brackets so in addition to that it has  uh active metadata capturing of the transfer   of the the files that have been transferred and  it do it's doing the replica management essay   so so um so um and more importantly so the idea  of mft is an open source framework and it's   apache to license and it's currently managed  under the purchase of the foundation so anyone can   contribute and watch the code and see what's going  on so this is a very top-level architecture of the   iraq mft uh it has uh at the top play  it has external apis for the users to   submit transfer request and it has the  framework extensions as i mentioned earlier   you can plug your own credential backends and  the resource backends of your existing gateway   backends then it has the metadata collection  data transfer and data browsing components   in middle and underneath it has the mft  agents we call it which are the actual agent   uh which are doing the communication across the  different type of storage endpoints so on the   outside we have different kind of cloud endpoints  even hpc or even you might have a databases so   the mft agents are smart enough to talk to these  endpoints according to the protocol they support   so i would like to go to a simple scenario of how  the file files have been transferred using the mfp   framework uh so the use case is uh you have a data  store one uh it's having some file and you need   to transfer the file to the data store too and  then you install mft agent we call this pattern   as the end-to-end transfer um first uh the so this  is the mft uh framework that i mentioned first the   user sends a request to the api service so the api  service gets a request and put it to the console   uh the event driven message bus and the message  is forwarded to the mft controller which is the   coordinator of the all the services so then it it  reads a message and validates whether this is a   write request coming from the right user and  then it sends to the right mfd agent so the agent   then these additionalizes the message and talk  to the resource service uh to get the resource   metadata of this particular file and talk to  the secret service to get the credentials to   talk to the data store too so once it get once  it has like they received all the information   then it starts to get the data from the local file  system and push it to the data store too using the   credentials it got from the secret service so this  is a uh this is like a very simple diagram which   shows the previous use case we call it end-to-end  transfer so to do the end-to-end transfer you   should have agent installed on either side or you  can have agent installed on both side uh it's your   preference and so the mft platform is sitting  outside and sending the control signals and the   data path is only between the two endpoints so  this is recommended and the cases where you have   sensitive data transfer requirements so  even like if you need some hydro food data   transfers you can use that because there is no  any third party entity involved in the transfer   so so so the second use case is uh we call it zero  intervention because in some cases uh it's not   practical to install agents or some binaries on  the storage endpoints because due to the security   or the policies enforced by the endpoint so in  that case you can use an extra agent to pull   the data with according to supporter protocol of  the source um resource and push it back to the   destination endpoint according to their supporter  protocol so uh here also you can put the agent uh   inside the network boundaries and you can control  those agent from the inventory platform from the   outside here also the data path and the control  paths are separated and this this could be useful   in the cases as i mentioned earlier in the cases  where you don't have any authority and uh you you   might and the data transfer is not like the the  sensitivity of the data is not that much critical   so and this is another extension of the zero  intervention transfers you might you might   be able to define dmz boundaries inside the  machine platform so so the metric platform is   smart enough to keep the data inside these dmc  boundaries and select the right agent to the   uh to transfer data in between the storage  endpoints and this is another use case um so   so you might you might have access to multiple  cloud endpoints by different vendors and if you   need to transfer files in between different  vendors you can use public cloud agents   to do the transfer and use the mft platform to  signal the agents to do the transfer operations   so this is a little bit advanced use case so for  example you your laptop have contained some um   research information research data and you need  to expose them into the public uh public endpoints   and in that case you can use the mft platform  to expose them using uh even through the netwo   firewall provided by your service provider so um  so that's all about the use cases and so now i'm   going to briefly go through like let's say you  have a gateway and how how do you integrate the   uh the idaho mft platform uh so first thing as  i mentioned like early slide you need to have   plugins for your credentials and resource back  games once you have configured that one uh   then obviously you have to change uh some code  fragments of your gateway instead of doing the   transfer manually you can talk to the mfd apis  and it provides all the transfer and monitoring   endpoints so if you are more interested you  can do some advanced integrations like you can   look at all the storage endpoints as  a one single global file browsing view   or even you can do the searching and indexing  through different endpoints and you can do the   data grouping or repository management if you  want so all are provided through the mft api   uh so so these are like um so because because  the framework is very extendable so we are   looking through new innovations and um see  what are the advancement of the technologies   in the in the area where the file transfer happens  so the first one is we are currently looking at   how quick protocol which is a um udp based  file transfer protocol developed by google   uh can be used for our agent region transfer  scenarios and then we are looking at rdma based   uh data transfers specifically in the clusters  where you have we are they contain the rdma   support and network cards and in addition to  that uh we are actively looking at how we can   improve the data encryption uh for and usually  data encryption uh doing on the cpu could be   like a very expensive operation and because and  there are some technologies where you can do the   encryption at the chip level and we are looking at  those opportunities and see whether we can improve   our overall transfer performance and um then um so  that's all about the the iraq mft uh for now and   um please uh go ahead and check out  our code from our youtube page and   it's deployed under the apache umbrella  and you can go and look at the issues and maybe like you can give your feedback as well  and because the project is still at the very   early stages and we are actively working on some  some fuji improvements and you can also contribute   so these are some interesting ideas that um we  are like planning to work in the future but we   haven't planned yet a plan about the development  iterations so you can maybe like contact us and   give you feedback as well for example we  are doing some research on smart network   detection across agents and so and we are doing  some work on detect transfer time detection   before initiating the transfer based on the  historical data and machine learning modules   and then we are planning to uh see some  automatic fault work of operations across   agents which is a distributor system problem and  these are all open problems and we are always   open to get the feedback from you all thank  you you dima do there any questions for him if not maybe i can ask one so in at least in the  you know for when they're in the hpc resources   when people are trying to transfer files usually  they have like you know gazillion like 10 000   files that they want to transfer when they're  doing that is there any like data management   practice like in the sense you want to like maybe  compress them put them as a zip file to transfer   just to improve the network latency is something  that you thought about uh yes it's so it's on   our pipeline um but right now we don't support  that one we are current we are transferring file   as it is but it's it's yes of course yeah  there's a uh it's a very valid use case um   the only thing is that uh it can be done  only on the uh the file system on the like   hpc file system we can't do that on probably on  the cloud endpoints uh so we might we might be   considering that in future but uh not try and  right now we don't do that one okay thank you um so we can move on to the next talk that's uh  the third presentation the final presentation   is titled fair data and secret gateway a  research data alliance adoption project   and the presenter is rob quick hello  and let me share my screen here share and review let's see and let me know if you're seeing my  screen correctly yeah we can see it okay great   so somewhat inspired by the plenary talk this  morning i'm going to start off with a statement   here that i hope can lure everybody in i think  it's a statement that actually everybody will   agree to but it's easy to get lost in a conference  like this where we're talking about gateways but   the statement i want to make is that the research  enabled by a gateway is much more important than   the gateway itself right so we all have got  jobs that relate to gateways so it's hard to   think about that but i think it's important to  realize that the research the gateway is a tool   to accomplish the research and the research  is really the goal here and research that uh   allows some decision making or some advancement in  wisdom so um i wanted to start with that statement   because i think the gateways themselves are  somewhat transient they they come they go   um the research though should be permanent um  and one way to think about that is um is by   saying that we want the the data and in fact even  the workflows to move towards the fair principles   and at this point everybody's probably heard  fair so many times that they're they're tired   of it and they know what the acronym means they  know findable accessible interoperable reusable   uh like the back of their hand um though they may  not have put much thought into what those words   mean beyond that and we'll talk a little bit about  that and in fact the project that we got funded by   the research data alliance to do was to look at a  specific gateway in this case the secret gateway   the science engineering application grid gateway  and a data infrastructure called rapid which is   the robust persistent identification of data and  i'll talk more about both of those a little bit   going forward but to see if we could take what are  the core fair principles at least the technical   part portions of the the fair principles not  so much the social portions and apply them to a   gateway environment and what that really looks  like so um to set some possible scenarios um   we as i said these gateways are tools for  researchers the researcher may be concerned   um that the digital objects they use or produce  are governed by fair principles they may have   heard the buzzwords they may have put it in  their proposal they may have agreed with what   the word's meaning was though didn't think so  much about what how to actually enable fairness   they may want their research to be reproducible  outside of the science gateway environment   as i said the gateways themselves are tools that  that will come and go over time um and i don't   think we should expect that the gateway will be  a permanent place that the research will live   obviously the data lives outside of it the  publications live outside of that and in   fact you want your research to be reproducible  even if the science gateway is unavailable or   possibly retired at that point after some  number of years or some amount of effort   they may want to share the steps or the workflow  not just the results so they may want to have   some record of the workflow again outside of  the gateway to share with collaborators or   to put in a publication and they may want to reuse  that workflow with some amount of tweaks say a new   version of software comes out i just want to drop  in a new new version of software i don't want to   have to point all of my new data at it or all  of the intermediate steps just drop in a a new   piece of software so those are some scenarios um  but first a reminder of what the fair principles   are again everybody knows the words um but i  don't know that so many people have written read   exactly what fair means and fair means uh several  things and in this slide produced by luis benino   shows the difference between the the green is  what can be uh implemented on a technical side or   through a uh through software whereas the the  black or blue uh whatever you may be seeing   uh text is more of what is a social contract  right so looking at f1 you can technically assign   a globally unique identifier but there's some  social contract in saying that the systems that   record that identifier will be will be persistent  um the same thing with f2 data can be described   with rich metadata and you can make a mechanism  to do that um however the rich portion of that is   up to the social contract and the researcher to uh  or the community to implement the other ones here   and again we centered on the green things the  things that were technically implementable and   i don't want to go through all this because  this is a relatively short presentation but   we focused on on those things in rapid rapid  the robust persistent identification of data is   basically a handle service or a pid issuing  service and that holds the metadata resolves   to the global system the same global system  that is used for dois and many of the other   data site uses it uses it also so i'm  going to move in secret here is basically a   a relatively general grid for science and  engineering applications is built on apache area   area vada and the adoption that we did here was  centered on small molecules and their fluorescent   properties i will show a bit of that if i have  time here because i want to show you how you   take what you've done in a gateway and actually  find out information about it externally so the   rapid testbed is a data cyber infrastructure  it's a something i've been funded by the nsf to   to work on it is entering year three right now  but basically it has the cyber infrastructure   for minting pids resolving them to get metadata  a data type registry so you know the meaning of   that data and what type it's actually in and  then a protocol and in this case it's called   the digital object interface protocol that allows  operations on digital objects and it's basically   the cyber infrastructure that can it's  a cyber infrastructure it's not the only   cyber infrastructure that does it but can  implement those technical components of the   fair principles that we looked at a few slides ago  and it leverages several things from rda and then   some existing software including a a persistent  identifier identifier kernel information strawman   profile basically a profile of the metadata and  the data types the data type registry and then   it does the group within rda called the data  fabric working group theorized a bunch of this   we took it to the nsf got funded to work on it  and there's more for this at the link here below   and here's the idea of using pids pids are cheap  inexpensive easy to issue and so what if we   assigned pids on every step of the workflow so the  raw data either in the file or the raw data file   are in other formats can be assigned  a pid for for each piece of that   the any preparation software or pre-preparation  can be given a pid the same with intermediate data   products these the final analysis application of  software and the final data products they all get   persistent identifiers that are resolvable  with metadata about the workflow steps and so the the simple example of  the phosphorus molecules are the   fluorescent molecules sorry that i mentioned  earlier is shown in this tree it's really   um this is a very simple but basically the  workflow itself is given a pid so when you hit   uh um submit a job it's given a pid for the  um for a group of other pieces of metadata   it's split into input software and output pids and  these have various different things the input has   the the actual molecule in this case the  coordinates of each of the atoms in the   molecule the charges on those it also has some  of the things that are used in input for the   in this case gaussian software the software pid  has the name the version and other bits and pieces   in fact the software should probably be split into  a software and environmental pid because some of   the things we captured in the software pid are  more environmental type variables and then the   output id has the the error the output and and the  log information so these could all be assigned to   pid these are all retrievable externally so if  two months later i want to go back and see what   version i'm writing the paper two months  after i did the initial run i want to know   what the exact version of the pid or i'm sorry the  exact version of the software i used i can do that and i will show that here real quickly   i have this set up so this is the area vada  and let me get it on a bigger view here   um the the workflow you can see this is a workflow  i ran a few months ago again i'm writing a paper   today i have a um and uh and uh for example if  this gateway is gone i still can get this pid i   hate this pid i take it to the handle.net which is  the global resolution system it sends me back to   to the rapid services i'm here you can see that  i have pids that are the the input data the   output data and the software data in this case i'm  looking for the version of the gaussian software   as i write the paper to make sure that it's  not a or that it's correct when i publish this   i can follow this pid down here to find the  app module version i can grab this from right   here again i have i don't need any access  to the gateway to do this and i found my   version i can put that in my paper if i want  to change that version in the workflow i can   put in a pid that is you know a new  version or a different version or   something along those lines um and i think  i'm running out of time so i will get to my   conclusions here as soon as i can exit full  screen mode and get back to my presentation okay um so we did this integration uh  secret is working with this it's available   at rapid.super.org if anybody wants to play  with it um and it implements the issuing of   pids the gathering of metadata that is resolvable  externally and we it does use actually the digital   object interface protocol that i talked about and  these describe basically the workflow components   you can use this to recreate the  entire computational workflow   without having any access to the gateway  so if if a researcher picks up your paper   and says i want to recreate this workflow but i  don't want to do it in the gateway environment   they can do that with by getting all  of the metadata information about the   about the uh workflow that was used within the  gateway again this uh um oh well not again but   this directly impacts the uh um mostly the fa  and i parts of fair so the the fae the r is   is much it has much more to do with the reusable  with the social contracts and the metadata that is   required to be gathered um so it's much more of a  a discipline or a community enforcing that to make   those reusable and nicely this is is extendable to  other area botta-based gateways pretty easily in   fact we have a client so it could be moved to any  gateway not necessarily just area database science   gateways um and it leverages uh the rda community  and in fact it was funded by the rda so we have   some future work document the client deployment  if there is documentation it needs to be tested   and maybe reworked to be more user friendly  we did this on a very simple workflow we'll   have to do more complex workflows so something  like the common workflow language may need to be   integrated i showed it in the tree structure  it's probably more of a a dag structure a a directed acyclic graph type  structure of what workflows really   work in we don't we wouldn't want people to have  to uh copy and paste through various different um   things like i showed through various  different web pages to get the information so   it needs some sort of workflow visualization  that is representable in uh in something a   researcher could read quickly without having to  copy and paste many pids to get the information   we're looking at the possibility of then  using the data to populate public repositories   in this case the fluorofluoroforbs.org because  this was fluorescent materials and the other ones   rapid itself right now is a in the test bed  service it was funded in the eager award um so   it would have to operationalize to be used uh in  general it's a very stable very um a very robust   service and running on jet stream right now but it  it would mean some operation optimus operational   uh components um and then um we have some testing  that's been done on the performance of how many   pids can be both resolved and issued and and  they're in the hundreds per second however   that comes from the developer of that service so  we would want to do some independent performance   analysis before we uh release this widely  into the community and with that i will   see if i can escape and stop sharing and take  questions hopefully uh uh suba that was in   uh a reasonable time i think so i think  so i just went one minute or that's fine   so thank you rob and any questions for rob  when i see one in chat let me quickly see um   so the question is have you implemented this on  any non-apache ayravata gateway to today so no   to date we were only funded to do this on the one  gateway it was a a small short award rand ran from   january to june of this year um it funded about  uh about 20 of a person so this is the only   implementation we've done however the client  is independent of apache area vada and can be picked up from github reused again the the  instructions probably need to be gone through   but we would love someone to test  it and see if the instructions were   reusable outside of the  people who have developed it   but there's no reason that it can't be done  it just hasn't yet any other questions for ron uh maybe i can ask one so um what was the  level of effort that was needed to change   you know to incorporate this into secret because  secret was like existing gateway right so what   what is it maybe you can give us an idea of the  effort involved so the uh i can give you a so for   those six month months it funded approximately 0.2  fte however that was the initial interaction and   i don't think that it would take that long again  because now we have the client again independent   um however you know doing it the first time takes  some some amount of effort and some integration   and i would see that future it would it should not  take nearly that much if it takes that much then   obviously we don't have a product that can be  reused widely okay i'm going to open the floor   for questions for other speakers too there is  one question in the chat do you have any secret   or other users experience with using this is there  a use case documentation yet that can be presented so there is not a use case a documentation  as of yet though that is a really good idea   what we've what we did is the proposal of  the idea to rda and then the implementation um the sudakur who was the who is the pi for um  the c grid project was the one who brought the   use case of the uh the fluorescent molecules  and some work that's being done here at iu so   i think we had we had kind of a use case to  be directed at but uh seductor has is hap in   a gateway community also and what we need to do i  think at some point in the near future is go back   to the actual researchers one of the things  they did bring up was that it would be good   to publish information to the floraforbes.org or  some public repository of fluorescent materials any questions for um all our  three uh presenters today um i do have a question for the mft  presentation i wonder if you could talk a   little bit more about the way the accounts  are authenticated how that works in your system so um yeah it's a good question um so  if you can remember um so i mentioned that   um so you have you you have the full functionality  to plug in your own uh credential back ends   um so we from the from the mfd side  we we act as like a stateless uh   entity so what happens is that  if you get a request request in   any request can contain a token so what happens  that the token is routed all the way through the   credentials backend of your implementation so  then you can uh so the advantage uh advantage   you get is that you don't have to think about the  mft specific security uh implementation it's like   you get the token and you can validate from you  are in and give the access to get the credentials   of the resource information so uh yeah that's  the idea so you have the full control over the   authentication and not the authentication you have  the full control over the authorization of the   entities because we we we assume that we get an  authenticated request from the api okay thank you are there other questions um for james i have a question so in um in the  drive is there i mean i missed that part maybe   is there a weight you capturing metadata along  with the uh with the files and everything yeah   uh luckily i mean i think dat which is now called  the hyper core protocol was a a deliberate choice   because of a lot of the really nice functionality  that already exists there so for instance it has   a full uh change history as well so it's  quite similar to what you see with git and   other kind of version control systems and there's  also a very vibrant community community there   but i mean specifically about metadata yeah  seeing who changed the files when they changed   the files i mean what you would normally see with  version control i see okay yeah but the yeah again   the community is great and what you can get what  the community has managed to do i like the beaker   browser project uh for instance i would recommend  people to check out uh to build on the metadata   that's that's on that protocol and there's  some really cool work happening there i see   um and is there like you know other gateways or  not part of the hub zero can they take advantage   of it or is it just you know for projects that  are built with hubzero for now uh the protocol   is open source uh and it's uh it's funded by other  mechanisms um yeah so i think anybody can can it's   just a protocol right so anybody can look at that  anybody can build upon it um i think what hubzero   has has done and what we are doing because this is  very much a prototype as i mentioned in the talk   um is how we are able to leverage that protocol  and then integrate it into components that we have   in the gateway already so publications have been  there forever projects have been there forever   it's just the overhead because uh those components  have kind of been there with the traditional page   loading clicking web kind of interfaces feels  quite heavy from a user experience standpoint   and what we really wanted to do was let people  hey i'm collaborating i want to create a project   it should literally just be dragging this  folder into this uh special location and   boom the projects there uh and then with one  click start a publication and all the files   that are part of that project are part of that  publication so it's it's something that we've   been doing for a long time projects publications  we just want to streamline that user interface   okay okay thank you i also saw a  comment about my lighting i apologize   since i'm in amsterdam it's very dark  so i'm a bit redshifted i don't know and james i was wondering if you could talk a  little bit about i'm sorry if i've missed it about   the performance and sort of the reliability of  this protocol this transfer of data yeah i mean it   is a peer-to-peer protocol uh so it's kind of the  redundancy between peers can be a problem um but   the way that that we can solve this or get  around it is by letting our centralized   cms server also act as a peer or pinning  service so not only can users say   i'm interested in getting this data but the server  itself says i'm interested in that data so it pins   that data and then acts like an everlasting peer  so even if peers leave the network they're no   longer discoverable as long as that centralized  server's there it's available as a peer   and this centralized server is what you also  have on the hub zero is this part of what   you're offering is that correct uh yeah it will  be yeah okay thank you there's one more question   james do you have a sense when this feature  will be available for other hubzero gateways   uh yeah i mean it's uh again in the  prototype uh phase and i think one   of our uh main drivers going out and speaking  about it at the conference uh conferences um   here uh this fall is to try to get a sense if  people are really uh finding it interesting uh   it feels like it feels like people are um so i  would say uh it's it's on the roadmap and and it   will be there uh probably not in weeks probably  not in months but uh yeah it'll be there soon   thank you any other questions for our speakers i think we have two more minutes but  we can you know take the two minutes   back um if not then let's thank  our speakers you know we have uh   and uh thank you all so much for joining us  and thank you for being the technical host um   there is a break now and after the break  um learning labs will begin in the mark   session rooms topics are posted at the welcome  help desk and on the schedule and with the list   of rooms displayed along the left side of the  keycode chat but thank you everybody for joining today 