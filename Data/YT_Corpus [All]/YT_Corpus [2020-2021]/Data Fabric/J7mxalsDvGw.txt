 hello everyone thank you for tuning in my name is sergey and i'm the founder of chemin data in my presentation today um i would like to take a step back from the advanced machine learning algorithms and things like petabyte scale clusters and focus more on fundamentals of how we manage our data and how data is being exchanged and processed and most importantly not focused just on one company or organization but take more of a holistic view and see how data is managed worldwide how we collaborate and improve data so i'd like to introduce this topic uh with an example a lot of you have probably heard about the drug called hydroxychloroquine um it's been all over the news for all the wrong reasons but you probably missed this bit of news so way back in may there was a study published in the influential journal called the lancet and this was purely data science study uh it included data of more than 96 thousand coveted patients worldwide for data source it was using a proprietary database called sergiosphere so the finding on the study was interesting it claimed that the use of this drug was actually increasing the risk of inhospital mortality for the patients that were taken so obviously this kind of massive study very early into pandemic was treated very seriously by everyone and global trials throughout the world were completely halted as a result so just a little bit after that the publication was actually retracted after the journal was flooded uh with numerous complaints that uh about data inconsistencies and the provenance of data that was used for the study could not be established search sphere database basically refused to open up the data to be analyzed by other researchers and uh they basically decided to shut the database down so what we have here as a result it was a database manager data management issue that was ignored for years and derailed potentially life-saving efforts in a very critical time and uh this example is just one of many a lot of other studies also related to khalid were using this database and this database existed for many years prior to that so what we have here the reproducibility the the guarantee of reproducibility was uh violated reproducibility is the foundation of the scientific method but if you look at the results of this uh survey published way back in 2016 by the nature magazine 90 out of 1500 researchers said that there is a reproducibility crisis in the in the field so as i will show my presentation today i think the problem originates in how we manage data and i will show that with current practices reproducibility is nearly impossible to achieve in practice so let's take a quick look of how the reproducibility and verifiability look in modern data to give it this a bit more structure i'm going to differentiate two kinds of data so on one hand we'll talk about source data this data originates or generated by some system or originates as a result of observations so simplest kind you can think of as a weather data right as a rule of thumb this data cannot be reproduced if it was lost so publishers in this case have full of authority of the over this data and are responsible for its validity on another hand derivative data is data that is produced by some other data through some transformations so this uh encompasses all the aggregates summaries uh even though learned machine learning models are everything i think are part of this category so this data usually can be reconstructed if it was lost if we still know the transformations that were applied so what validity in this case depends on both the source data that it was used and the quality of those transformations what's our expectations when dealing with the source data i think it can be actually summarized in just one sentence two unrelated parties at different times should be able to access the same data and validate that this data comes unaltered from the trusted source so in technical terms this means stable references and also a validation mechanism so you can think of something like checksums or digital signatures in reality however a lot of data publishers these days are are still publishing data sets in non-temporal formats so this is extremely prevalent in gis data publishers which currently provide most of them currently provide just the current state of the domain so when you go download the data you all you see is the snapshot of what they think the the domain looks like at this current time when the data is updated it's usually updated destructively and in place so what we have here is a constant loss of history so we cannot even begin to talk about stable references because even the history is being lost um temporal data sets on the other hand are better because uh so they store the events and observations and typically they are append only uh but they are still those publishers are still doing the in place updates so the new data replaces the old one so the same url will yield you different data so again due to this and some other more technical problems stable reference are nearly impossible to achieve for derivative data my expectations would be following repeating on transformation step produces same results as the original and the reciprocal of that is that if you have the data you should be able to verify that declared transformations were in fact truthful and that there was no accidental or malicious alterations made to the data so this means transparency knowing which transformations were applied and this means determinism redoing those transformations yields the same result in reality however reproducibility is a very manual prop process and it currently is a burden on shoulders of data scientists and achieved through the various workflows and compliance routines if you were to try to implement the reproducibility in our project you would have to start with stable references to source data which is we talked very hard to achieve you would also need to create reproducible environments so versioning your code versioning all the libraries that you're using and their transitive dependencies all the frameworks and use and ideally going down to the level of operating systems and hardware you would also make your need to make your project self-contained so this means no external api calls if your project is using something like google api for uh for geo location for example you cannot rely on them not changing this api and returning different data like the next day after you're finished with the project or shutting down that api all together compromising and leaving your projects stranded and non-reproducible so what i'm getting here is uh that one time is pressing your producibility goes out of the window because how it's how hard it is to achieve so here are some examples right um i see more and more prevalent uh more popular for people to share data sets on platforms like google and github so the idea here is that a person goes to a trusted data publisher they don't know the data they see that a lot of that data is probably not very clean so they clean it up repackage it into a more usable format and they share it with other people on kaggle or github so the goal here is to collaborate on data cleaning worldwide but reality is that nobody will use this data set in a serious project because they have no facilities to verify that there was no malicious or accidental alterations made to this data so data cannot be trusted same goes for data hubs and portals which have a noble goal of improving the discoverability and breaking down silos but in fact they just re-upload the publisher data without this mechanism of establishing the validity therefore most of these hubs end up more like data graveyards so copy inversion approach is how enterprise data science is deals with the problem of reproducibility in source data so this involves a person from a company fetching the data from a trusted publisher slapping a version on it and re-uploading into some persistent storage where it will never be changed so something like s3 but this of course works within only the enterprise bubble it does this approach cannot work when data is needs to be exchanged between different parties so in summary i think we're mismanaging data in the most fundamental way and collaboration on data is currently impossible we're constantly losing the forward progress because every data science project wants to begin with a trusted data from a publisher but it produces the data that cannot be trusted and i think we reached the limit of workflows a long time ago and we need a technical solution so is there a better way this what we set out to find out and design decided to design a new kind of data supply chain that would be built with reproducibility and verifiability at its core and some other traits like low latency of propagating data through the system complete provenance something that would encourage data reuse and collaboration so the result of it is called open data fabric and this is the project that we recently open sourced and this is the first time i'm presenting about it in public open data fabric is a protocol specification so you can go into github and see the protocol spec for yourself this protocol specifies how to reliably exchange data between parties and transform it along the way to summarize it in one sentence i would call it the first world's first peer-to-peer data pipeline so before i talk about details of open data fabric i need to clarify what we mean by data here we decided that we cannot build this system uh for data as it uh as this term is currently used because it's very wide and it's very broad and all-encompassing and usually brings all the negative properties of data that we would like to avoid um so what we decided is to narrow down the definition of what data means in odf so here are the steps that we took to do this so first of all we say that there is no such thing as current state so current state is this infinitely small time interval and it's i find it's quite interesting that a lot of software engineering like ltp and lab system systems are focusing on this infinitely small time interval instead we decide to focus on history so we store history therefore data needs to be temporal history doesn't change therefore our data is immutable future decision making relies on history so the more history we preserve the better decision making we can have in future so data must have infinite retention also time is relative and especially in data system systems it takes time to propagate data through many steps and there is a delay to propagation so we use it by temporality to reflect that so overall our data model looks as following a data set is potentially infinite stream of events and observations and every event has two timestamps associated with it event time which is the time when this event occurred in the outside world and system time when this event was first observed by the system because system time is monotonically increasing all you need to do to get a stable reference is to just decide on this right most bound of system time so this one timestamp completely already completely achieves the goal of stable references you just truncate the data using it and you get a stable reference validity can be established by taking the same system time and just hashing the data preceding it this is how data flows in open data fabric so adf is basically a graph of data sets where every data sets can have their own maintainer like a person who is responsible for good quality of that data set the data flows from left to right from publishers to consumers on the publisher side if the publisher is odf compliant meaning that they provide all the they comply with our data model and all the qualities that we like to achieve from the source data that means that odf can extend to them and simply use data from their so-called root data sets so this is where we store source data and all the derivative data sets for derivative data can be built directly from that data if publishers is non-compliant that means 100 percent of publishers right at the moment we can employ different change data capture techniques to give retroactively give the data the properties that we need to look for so to keep this more grounded this is how would you define a data set in odf so it's just a yam file and for root data set here we're specifying the url of external data publisher in this case open vancouver open data for portal of where the data will be fetched how to parse it and most importantly as we pull the data over and over how to merge the data we already saw with data that is appearing in the data set on the derivative data set side all you specify is input data sets ids and some transformation in this case we have an scale query that uh does a join over these two dataset datasets um so one important pillar of the odf is that our metadata itself is temporal as well so here you can see the transformation between data set a and data set b and the metadata related to both the most common metadata event is of course the data has been added into the data set so we have those on both sides but here as you can see in the middle of data set a's lifetime there was a schema changing event so this is what we call lifecycle metadata events some important things that can affect schema or how the query is executed these chain uh lifecycle events are chained together uh between metadatas of derivative and the root data sets so here the maintainer of dataset b has a chance to react to the schema change in data set a so this is something that enables us to accommodate the data set evolution over time so without it if we did not support it you would have to reconstruct the entire graph over and over when the schema change occurs in a root data set this is also a key enabler of reproducibility and variability and as you will see further it gives some great properties for data sharing and what we're currently working on is uh achieving the fine-grained provenance using this data metadata model so what we're aiming for is being able to take any data cell and derivative data set and say exactly where it came from what was its ultimate source and which transformations were applied so something you don't see in any of the enterprise pipelines today under the hood this is how metadata chain looks like so it's very blockchain-like so a lot of blocks combined chained together blocks are individually cryptographically secured and they're also cryptographically linked to the data that associates with them this format is very extensible so in the future we see extending it to semantics andology governance and licensing all the security concerns so the idea here is again taking all the workflows and compliance routines that are currently very manual and uh codifying them so removing the burden from the person of maintaining it this is how dataset looks on disk and very git like and structure so every metadata block is its individual file we have references pointers to these blocks there's a checkpoints that we're going to talk about later and the data is stored in individual part files in rk format so no surprises here a lot of time we were thinking how to describe the transformations in a derivative data sets and the conclusion we arrived at is that batch processing is simply unfit for purpose so if you look at the picture on the right here imagine that you're joining two living data sets so the data is being periodically still added to those data sets and if data set b is updated on a cadence of once a year and that data set i as updated on a cadence once a day you would need to accommodate for all these temporal differences in your batch transformation code so this would be extremely error prone uh especially with the more complex scenarios like backfills and out of order arrivals and corrections so basically the batch transformations how they deal with these problems is by simply ignoring them so they wait it out until temporal problems disappear so they rely on the simplicity of non-temporal data instead of facing problems um if you write out this dash transformations it would be extremely error prone and it would be impossible to audit so what's the use of having transparency and knowing what transformation takes place if it's so complex so you cannot understand it so this is where we looked at stream processing stream processing is essentially designed with uh for the temporal data problems so we have things like runtime processing which is basically the by temporality i described earlier other great mechanisms like watermarks windowing different kinds of joints there's this misconception that stream processing is only useful for a new real-time processing when data arrives live from like mobile devices but i think it's not correct we actually found it very useful even for data sets that are being updated on a cadence like once a year so we decided to make the stream processing our primary transformation method and one of the biggest benefits of it is that it makes transformations agnostic of how and how often data arrives so the data can arrive in uh on periods like once a year and gigabyte batches or it can arrive every minute in smaller batches it doesn't affect the way you would write the transformation so this is very powerful um it's also declarative and expressive so it's easier to audit and it's better for determinism and reproducibility and of course the it gives us minimal possible latency because you write your query you run it basically forever with no human moment on the negative side of course it's unfamiliarity because it's pretty new and the limited framework support so speaking about how these transformations are performed what we have here is a coordinator component coordinator is responsible an idea for handling everything related to metadata so during the transform when you initiate it it reads the metadata it figures out which slice of the input data set was not processed yet was not yet seen by the output it takes the slice and passes it down to the engine so odf is actually a framework agnostic so we have several implementations of the engines our main workhorse is the apache spark framework but we also extended the support to apache flink so the engine can be any data processing framework focused on the stream processing mainly that just has some odf adapter very slim adapter on top of it so the framework does the processing it can use the checkpoint stored alongside the output data set read and write into it and it produces the output slice that coordinator it and stores an output data set the engines are what's very important they run in a so-called sandbox environment in a docker container this achieves two properties so first of all we strictly version every container this means that any transformation for repeatability is going to be performed by exactly the same version of the engine as it was performed originally secondly the sandboxing ensures that engine cannot access any external resources so this is how we disallow any kind of api calls for reproducibility purposes this approach has interesting implications on data sharing so for the data storage for data sets there is no surprises here since this data is on risk cannot be reconstructed if it's lost we recommend durable and highly available storage for derivative data sets however you can use any cheap and iron viable storage or no storage at all if you would like because uh because all transformations are reproducible derivative data sets are basically a form of caching it doesn't matter how deep of a transformation graph you have you can always reconstruct data by only having the root data and metadata associated with it so metadata here acts as a kind of a digital past part of data it's used to verify data's integrity once you download the data but it can also be used for validating work of another peer so let's say you grab metadata from someone else and you see the peer doing the transformation and sharing the data you can rerun those transformations and compare the data hashes that you computed with what they're publishing and you can spot by doing this you can spot malicious activity so publisher that incorrectly declares the transformation that they performed or tries to spoof the data so um now i'm going to talk about chemiseoli chemiseoli is actually the tool where open data fabric originated from so in idea of terms this is a coordinator component it's just a single binary app written in rust that handles tasks like ingesting the data transforming and sharing chariot it comes with two prototype engines as i mentioned based on apache spark and apache link has some convenience features and currently it's alpha quality but it's being actively developed so it has a very good-like interface here you can see me adding a dataset definition into my workspace from a young file pulling the data set so this is where the tool would ingest the data from external publisher i can list my data sets in the workspace and see that some records were added and i can also use camelock to see the metadata blocks just like git log shows you the git blocks associated with your [Music] source code so it also has the sql shell as you would expect for exploring the data uh it's also based on apache spark scale so no surprises here um we also integrate jupyter notebooks so with one command you can start a jupiter server that is also linked to apache spark using the apache levy framework and this gives you the convenience of using the sql first or even apache spark code to massage the data in the format you that you would like and then transfer it locally into your notebook and use any visualization library that you like to actually display the data so in conclusion here's how i would summarize the open data fabric this is a data pipeline designed around the properties that we deemed to be essential for for collaboration and introduces the decades of stagnation and data management it uh admittedly runs a much stricter model so preventing you from doing any sort of api calls and like box transformations so it requires a bit of a mindset shift and how we define what data is and how we treat this data it also encourages embracing temporal problems head on instead of trying to avoid them so overall i think this strict model this definitely pays off if you look at the properties that this framework gives you so taking a more of a broad view i think open data fabric the way i see it is becoming one of the pillars of digital democracy and if you're familiar with the term the next generation decentralized 80 popularized by protocols like that and ipfs and blockchain um so the way i see it is becoming this factual data supply for something like blockchain contracts so this is a network where trusted data cannot be can be stored with no central authority but yet is still resistant to any kind of malicious behavior so this can also serve as a foundation for things like data monetization so providing incentives to the publishers for publishing very high quality and recent data and it can be a data provider for the next generation of web and applications so these are the references of where you can find more information so i definitely recommend you to check out commu cli it has a lot of examples that i didn't have time to get into today um so definitely take a look you can also read this pack it's very brief and i think it's interesting to read or you can check out our blog or just if you have any questions shoot me an email or just post a question on any of the repositories um so open data fabric originated as a response for uh of to call for action from the scientific community to address these problems of reproducibility but i have a call for action of my own in response so i think we need to treat data as our modern age history book so we should stop modifying and copying it around i think versioning data is uh not the right solution the part by temporal data modeling addresses the problems uh here in a much better way i think data publishers will have to take ownership of reproducibility because uh reproducibility has to start from the source data so we need to provide them with good standards and good tools where open data fabric can help with that um i think non-temporal data is really a huge local optima so temporal data should be definitely your default choice and uh in a similar way stream processing is a local sorry batch processing is a local optima and stream processing i think will very soon displace patch so i think we should collaborate together on improving tools and not over simplifying problems so please check open data fabric out and let us know what you think and i will ask you to not forget to rate this session and i'm looking forward to your questions 