 Hi there, I'm Ted Dunning. I'm CTO of Data Fabric at HP, and what I'd like to talk to you today about is how to size a data fabric cluster when you use the HP Ezmeral data fabric. Some people have a bit of problems, partly because they're so experienced and they have expectations about the way systems should behave. But in many cases, the HP Ezmeral Data Fabric doesn't conform with those expectations. And frankly, in many cases it's much, much simpler. So let's roll up our sleeves and get to work about how to size one of these clusters. So the first big thing about this is there's a difference between the number of bits that we think we're storing (I'm going to call that data kind of a logical view) and how many bits have to reside on physical devices in order to store that data to effectively have that work for you. And they're different because the data fabric works to preserve your data even under some pretty radical failure scenarios. In order to do that, it has to store some redundant data so your logical data will usually almost always be some multiple smaller than the physical disks or SSD or Flash or NVMe that you actually use to store the data. So let's walk through that sort of idea. Now, as a spoiler, I'm going to just tell you the answer right now, and that is you either multiply by four or you multiply by two. You multiply by four if you're using an expensive high-performance encodings which basically triplicate your data, and you multiply by two if you're using more advanced erasure coding techniques. And those that's the short description of how you how you can do this. But there's still some degrees of freedom and we should work through why it's that way. Also, I should mention that I'm not going to consider compression here. The reason for that is in very large data systems. If you've got images or videos or Parquet tabular data or a number of other sorts of sources for your data, you're probably going to have compressed data. And so for the largest part of your data the compression probably won't matter. On the other hand, if you have Avro or JSON or text, then compression could matter. Anyway, that's out of scope for this video. You should measure it and then just apply that multiplicative factor to the sort of considerations that we are going to talk about today. So let's consider first this idea. What I've got here is a simple five node cluster. And we're just going to pretend that each one of these nodes in this cluster is represented by these boxes. Let's also assume that each one of these nodes can hold five containers. Containers are a logical apparition inside HP Ezmeral Data Fabric. And so we can see right here, for instance, a little yellow hexagon and just below it, a red one. We're gonna call these containers. They are units of storage. Typically, there's far more than 25 of these in a system that's typically millions of them in a large system. We're going to use a small number so we can count them here. And so what we've got is five nodes. They can each hold five containers. So the total we can hold is twenty five. And we're sitting here with 18 of these containers. You can count them. I already have. And that means our data system is 72 percent full. Seems like there's a lot of headroom there. But as you can see, if one of these machines fails, say this one here, then the three containers on that machine have to be re-replicated. They're going to have to go somewhere. We're gonna have to store that data and we're gonna have to replicate it to the level it was before. And so what we see here is that this node here and this other node here both get some extra containers. And the bottom one there is now at five and the top one is at four. We still have the same number, 18 containers as we had before. But because we only have four nodes, we can now only store a total of 20. Eighteen out of twenty is a much higher percentage fill. It's 90 percent of our capacity as opposed to 18 out of 25. And so our cluster, because of this one failure, has become a lot more full. This illustrates how we need to keep a bit of headroom in the system. About 25 percent is a good rule of thumb that that should cover failures. It should also cover a little bit of working disk for temporary storage. Things like that. It should keep the file system a little bit of slack as well, because the file system can do a better job of arranging bits on disk if it has that slack. So we're going to keep twenty five percent headroom in order to handle all of these things and also to allow us to have time to provision more of the cluster if we think that in the future our data is going to grow. Now, this depends on your budget cycle and acquisition cycles. Acquisition could be pretty quick if it's in the cloud but your budget cycle may still take some time to get more hardware approved and to talk to all the stakeholders. And so you generally have to look out several months at least. The 25% headroom helps with that as well. So there's lots of rooms reasons for that headroom. But twenty five percent's a reasonable starting point for that. Very large clusters might be a little less, but their budget cycles are a little longer. So if we look at this, the situation here, we've got this whole kind of stack which represents all of our data. I've arranged it here so that we've got 25 percent held free at the top. And we've got our data in the middle with replicas on either side, data in pink, the replicas in blue. We're using 3x replication here and you can see that the total physical disk space needs to be about four times our logical size. I've got our data here. We've got two replicas and our headroom, which happens to be the same size as the actual raw data. Now. That's where this 4x rule of thumb comes from, is that if you're replicating and you have that headroom, you should multiply your logical size times four. Now the situation is different if we use erasure coding. With erasure coding, the situation is a little bit more complex. Our data here, the original raw data is split up into pieces. Now, it's not the whole data is split into pieces. It's each file portion is split into pieces. And then we add parity bits, (that is, redundancy) which are represented here in blue. Again, we always add two of them so we can have two failures and still persist and have durable data. But now the original parts of the data are divided into four pieces. And then we have these two redundancy parts and then we still have the headroom, which is twenty five percent of it. So now our original data four parts parity two parts and headroom equivalent to two parts, we have a two to one ratio from logical to physical. This encoding, because it has four pieces plus two parity blocks, is called for plus two. And we can view, frankly, the replication case as one plus two. Most people don't think of it as erasure code, but it is actually the trivial erasure code. And so what we can do then? We can think of this as changing the value of n with n+2 encoding from a value of one to larger values, and we can look at how the the overhead changes. So it goes from kind of an extreme where we have one plus two encoding (replication). That's where we have 4x amplification. At around four plus two encoding, as we saw earlier (that's here) we see that it's at 2x expansion and then it continues to decrease, but it only decreases a little bit. It doesn't ever go very much below two. And that's because of the twenty five percent overhead that the headroom that we have in there. It's hard to get some redundancy at reasonable levels of encoding and get much better than two X on that logical to physical ratio. So that's where that other number comes from. So we either multiply by four or we multiply by two. Now, a very interesting thing is happening here. The data in an HP Ezmeral Data Fabric is scattered in small units all across the cluster. Because of this, we're able to use the law of large numbers, an emergent phenomenon across this entire cluster to get estimates about reliability, but also estimates about size, which are robust with respect to the size of different nodes, they could be different sizes, with respect to speeds, with respect to the internal arrangement of data on any given one of these machines. All of these details average away. This is a little bit like physics where we get very, very simple laws out of averages of large thermodynamic ensembles of things. And so here it is again. We have a very simple physics like rule 4x for triplication and 2x for erasure coding. It's pretty easy to estimate sizing based on this. Thank you very much. I'm Ted Dunning, CTO for Data Fabric. And we've brought a little bit of the physics of data fabric to you today. Thank you. 