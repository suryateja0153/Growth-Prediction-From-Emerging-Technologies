 well everyone thank you for joining us we're building a better day too late with talent and data bricks my name is Michael this team I'm the director of technology Alliance as a talent and with me is Cameron Davey a solution architect in the technology alliances group today we're going to go through talking about how to build a better data Lake and I will walk through kind of the concepts and Cameron will give you some demonstrations first let me talk about Talan just give you a quick overview who we are we are a leader in the data integration and data integrity market we're well placed in the Gartner and Forrester analysts reports we have over 4,000 customers across all different verticals all over the world and we have deep partnerships with the key cloud vendors like AWS and Microsoft as well as with deta breaks we have offices around the world and we support key cob locations both in AWS and Azure now Talent the main product that we provide is something called talent data fabric and when you think about the the data world it's a bit it's a bit of chaos out there there's hundreds of different source systems and applications where data might be coming from and so how do you really manage that chaos well first you have to collect the data into one central repository namely a data Lake then you need to govern that information to make sure that that only the right people are having access to the data and also that you're applying that this the enterprise standards on how to form that data and how this how the data should be collected and published you might need to transform that data into a common data model and then share it with other users talent data fabric provides the comprehensive toolset to do that and it supports all different roles within the data team the data architects integration specialists they've scientists stewards citizen integrators and business analysts and for multi cloud on-prem we can support whatever deployment option you like now let's think about what a data like a delta lake is so building the delta lake as the reference architecture really specifies there's kind of three key areas in which you would store your data there's the ingestion tables which Delta Lake calls the bronze tables there's the refine tables knowing the silver and then feature engineering and the aggregated data store called gold so utilizing this reference architecture the key question then becomes how do you get data into those various items and get them prepared and ready to go the first one is sourcing the data from streams and batch and applications and so the first thing talent we'll be able to do is help you ingest that data into a raw zone so that you can get those bronze tables the second step is how to refine that data into those Silver's tables how do you curate the data with common schema and standardized quality and lastly taking that data to the machine learning world requires another level of transformations and this is called feature engineering so we'll talk about how talent can do all three of these let's first talk about bronze two options to ingest data into your Delta Lake the first is our SAS based option and this is ideal for sales of marketing SAS applications this is a fully exhaust environment available through the data ingestion network on the partner gallery menu in your data VIX console the platform as I mentioned is fully SAS based so you there's nothing to install nothing to run it takes care of it for you you just set up your schedule and the orchestration of how you want the data to come where you wanted to come from and how often do you want it to be loaded in now that platform is soft to HIPAA and gdb are compliant so you can be assured that you're gonna follow all of your enterprise standards it'll Auto scales and has Enterprise SLA now what it's really famous for is being able to connect to those SAS applications as I mentioned it has over a hundred different connectors and also has an open-source connector kit that allows the user to build any of their own and customers that you select our enterprise contract can add a custom connector as part of that agreement but as you can see it's applications like workday Salesforce Facebook Ads SendGrid square el agua and so forth now that covers all the SAS applications but many of you also have legacy systems whether those be you know old IBM systems or you know traditional enterprise ERP applications and and other on-premise applications and systems for that we would use our ETL option which which provides through something called town studio that can be an on-prem or a cloud-based environment it offers the bulk extract and bulk load capabilities in this case they're usually read and write connector so you can do you know multi-directional integration and it has support for Kafka and queues again it's sock to HIPAA and gdpr compliant so it'll meet your security regulations now here are the applications and the connectors that we connect to are your more traditional enterprise applications ASAP Oracle e-business suite as well as key on-premise systems like Kafka my sequel and Tara data and other databases like M sequel so so this provides you two different options to load the data into your Delta Lake depending on what the data is coming from Cameron can you give us a little overview of that from a demonstration perspective right so let's take a look at what that looks like to integrate data from the SAS application side so here we have within the talent Stitch day loader environment and here I be I want to be able to set up what it looks like to connect into Delta like on data bricks in this case I don't really need a ton of configuration for this in fact very little really all I need to do is select the fact that I'm going to select a destination in other words my data is going to be extracted and landed into Delta Lake and then I'm gonna set up the JDBC URL endpoint of where I'm going to connect into and my bucket name along with my access token and that is essentially all I need to be able to provide for connecting into Delta like on data bricks through talents ditch data loader I do have to do some setup on the AWS side is from a security standpoint but that's all of the configuration that I need to do from there I will have that integration ready to go and in our particular example we have one that loads from an azure data set and lands that into Delta Lake and we also have an example that loads from Salesforce the classic SAS application and lands that data into Delta Lake as well so we've talked about the SAS side of things now let's look at what it looks like when it looks like to integrate data from the on-premise or legacy side of things so as Mike mentioned earlier we would use talents to do to connect into those on-premise or legacy data sources so in this particular example I have a very simple example because landing data into Delta Lake at that Bronze level is very simple I have my source and I'm gonna map it into a target that's it so in this case my source is an on-premise Oracle data warehouse that I'm connecting into via JDBC I have my my my asher data bricks environment set up and establish down below and all I want to do is be able to drag that in to start with I'm gonna start on the right hand side by grabbing those components that I'm gonna drag and drop onto my canvas so I found the Delta Lake output component I dragged it onto my canvas I've configured my data bricks system my cluster down below and that's all I need in order to run that particular job extracting data out of my on-premise data warehouse and landing that into Delta Lake Mike so now we've solved our bronze tables we've loaded those tables up with all the raw information and that's really your landing zone for the data now what we need to do is combine multiple data sources together and to do that we'll be coming back into town studio which provides that graphical layout and data flow environment to design the data flow connecting the data together now that's going to provide a lot of transformation capabilities to structure the data from the source applications into a target system but then you're going to look at how's the data content of structure is it meeting the data quality rules is it standardized do we have duplicates and so forth and so for that we provide the profiling tools to be able to understand what data is available and how is it formatted as well as all the data quality functions to transform that into a common content now often that work needs to be done by a business analyst rather than a data engineer because who's better at deciding what the good data quality is rather it's the business analyst that's very familiar with the data so for that user we provided a web-based spreadsheet based UI to be able to understand what is the current state of the data and then provide an easy way for them to apply rules to configure that data to meet their enterprise standards so with that UI that user is going to be able to define what's called a recipe let's say this is how I want the data quality applied and then they submit that recipe back to the data engineer who puts it in there to the data flow for enterprise operations now sometimes there's going to be situations where the the data engineer and is not going to have a flow that's going to handle the correct information and even the business analysts might not have anticipated certain problems and so when those types of errors occur we route the data set out to the data steward who review the data potentially correct it and then resubmit it back into the data flow and so we provided a specialized UI for that data steward as well let's take a look at at the data integration side of it for the data flow as well as the data preparation side for the business analyst yeah great thanks again Mike so in our particular example we've moved off of simple ingestion that was bronze and now we're working into transformation that really is what starts to define that this these silver tables and we're starting with the use of the various data privacy data quality survivorship standardization rules and components that are available within the talent studio palette so highlighted in red on the right hand side are all of these various components and this is just a short sampling of the various components that we have once I've done that I can add those drag and drop them directly into my existing workflow in this particular case I'm extracting out of Shopify and some sfd C data and I'm landing this a nun using sparks equal to be able to extract that data out and now I have that those different rules in this case I've got a verified email component and a survivorship rule component that are added directly on to my job workflow here the my sources have been brought in as you can see on the left hand side and I'm gonna run those through those various data quality components to further clean my data now as Mike mentioned earlier we also have this interface to be able to allow the business analyst or data analyst somebody who is much closer to the end result of that data to be able to interact with that data as well that user logs into talent cloud as we've seen here and that user is going to come up to the menu and is going to choose data preparation as Mike mentioned we want to be able to allow this business analyst or data analyst a interface to be able to prepare data or to define a set of rules for how this data should be cleaned verified quality check etc so in this case I have my list of functions if you will on the left hand side the ability to define the connection into my data source the ability to define what data set I'm going to extract and then the ability to now prepare a theater preparation or a recipe and that's what we're going to do now so I'm going to add a preparation and launch into that now with this data set that we see here this is the raw data it has not been transformed yet and I'm going to take this tool and use it to be able to prepare that definition or that set of rules a recipe if you will now what you'll notice first of all is that there are some orange highlighted on this and that is indicating that that data is either incorrect or malformed added and so that's what we're going to use as our guideline to be able to change how this data is going to look now I can start on the right hand side I've selected a particular column this job industry and I can use the suggestions or the this ever-growing list of preparation functions that are available for me to use to change this data to check the quality of the data etc so again I've selected the job industry as my first column but I can go through any and all of these columns to change that data now we see the result of my finished preparation if you will now on the left hand side is where I have developed my whole set of instructions or my recipe for how I want this data to change so it's not just simply change one column I have an in fact 10 different steps to this column that are all baked into this recipe and now as you can see here on the top of my bar it's actually showing no more orange because based on how this preparation will transform this data I've got that data down to looking clean solid any of the gray lines indicate simply null or blank data and that's okay we don't we what we want is clean data that does exist and that's what the green bars indicate so we have identified that we have cleaned this data up quite significantly based on what it used to look like so really what we have at that point is data that's really good for business intelligence we've combined the data into a common schema and we improve the quality of the patient standardize that and made it ready for consumption for reporting and business intelligence the next step then is let's look at what is a machine learning need machine learning is going to want the data in slightly different formats so much like data quality is to business intelligence feature engineering is to machine learning with machine learning you're going to want to transform the data to make it optimized for those algorithms so one one common example we see is but you know we saw those job classification codes in the prior table now often in a machine learning world it's not you don't want to look at one column that has all the different job classifications but rather you're going to want to have a separate column for each classification there so we had you know financials as one of them and so you would have have a 0 or 1 to indicate if it's financials second one might be retail and and so forth so what what we can do is use the binarize err function in that case to transform the data into those various columns now this is just one example as you can see here there's dozens of functions available to make the data ready for machine learning through the feature engineering process Cameron why don't you give us another example of how you've applied that to the data flow that you've been creating absolutely so if we take a look at sort of the the further build out of this sample job and I think Mike touched on this at the beginning so we have data already available if you will are ready for the BI or the business intelligence use case and that's the top top track of this job you can see that we've actually spliced this job the data for this job into two different areas so I don't need to do anything as far as extra for my BI folks but for my data engineering folks in my data science folks I want to I want that additional capability of adding in with us with additional feature engineering more machine learning capabilities etc so that's what the bottom track here is if you will the first thing that you'll notice is that I've added in that data preparation run that I talked that we showed in the last step but now I've added in additional capabilities as well so again on the right side in my palette I have all of these different data quality data privacy machine learning functions or components that are available for me to simply drag and drop on to my canvas that's what we've done here and I the one in particular that we focused on is the T model and Cote encoder component which I've highlighted down below so that's just one example of that might talked about the binarize ER before that's another example with these different transformations so I've selected in this particular example one column it's gonna output the custom the customer number and I can use that and use some transformations directly within that data set so I can tokenize it indexof you know based on it if it was a string or not a whole host of different components and those are all available directly in that palette that we see on the right hand side so what we've done is we've built a data flow so you know the data engineer has walked through how to get data into the bronze tables how to refine the data in the silver and how to optimize it for machine learning into the gold now the question becomes how do you deploy and run that so what Cameron showed you so far was really step one in this process using the the design surface to develop that data flow and what happens next is that when you go to deploy that we're going to convert that data flow that graphical flow of information into native job native SPARC code and so we're gonna use our jar code generator and take that code and put it up into the town cloud into the repository as stored as an object ready to run and then we can set a schedule for how often and when you want to run that when it comes time to run that job we will push that code out to your data bricks cluster it could be an interactive cluster or it could be a data engineering cluster you get to decide and then that job is gonna run as native as native data bricks code within that cluster Cameron you're gonna show them how to set that out absolutely very easy to do which is the good part here so we've we've developed our job or workflow as mike has mentioned and now we want to be able to deploy that so very easy to do I first of all have defined the data Brooks cluster on which I want this to run very simply we've talked about this earlier endpoint cluster ID and a token is really all I need in order to define where I want this job to ultimately run in order to do that I'm going to simply right-click on the jobs name in my talents to do and hit publish to cloud that will use the credentials that I've added into talent studio which is basically my username and password to talent cloud to my talent cloud account and that will publish this job to my talent cloud account in order for me to be able to run it in the cloud so how do I access that well then I come back to talent cloud we saw this earlier with data preparation we're logging in now we want to get into the next phase of running this job or publishing this job so same interface as we saw before but now I'm going to go ahead and click on the management console for talent cloud and that is going to give me access to all of the different management capabilities of talent cloud all focused around the running and management of those jobs so I've got my operations my management all of the projects etc I want to be able to view tasks and plans because that's where I'm going to go ahead and run a job so on here I have all of my various jobs that I have loaded in the past the top one is the one that I just uploaded to talent cloud and I want to be able to add a new task or essentially a run cycle for this job so I'm gonna go ahead and hit add and that will run me into adding a new task for this particular job in here very little configuration necessary in order to run this job remember that the definition of the job tells the job to go and send this native SPARC code to that particular data bricks cluster when it runs so I don't even need to worry about setting that up in here because that's already defined in the job here I just define what I want what I want to run where I wanted to run so this is my personal workspace within talent cloud with that artifact name and I go ahead and run it from here I just I define where I want it to run so I want it to run in my cloud runtime and then I define if I need to do any of logging as and then finally is when or how often do I want this job to run so if I do it manually it just means okay go ahead and run it now or once and I define it a later time or based on any of those others I can set a recurring schedule for when I want this job to run including on a web hook which means I get external web based input as to when I want this job to run and again no matter what the option is what talent cloud is doing is compiling the code spark code and sending it to data bricks for it to run whenever I decide I want that job to run so to summarize you know we support data bricks on AWS and as your data breaks so regardless of where your public cloud is we can support you that native code spark generator provides for very high performance and very tight linkage to your data breaks environment and more so we support you know have full support for Delta and the time travel Cameron might have showed this earlier when we touched on a little bit we do support spark sequel so you can you know mix and match your your data what your cloud data warehouse in there and then as I mentioned earlier we support both interactive and data engineering clusters and from a performance and management perspective supporting data sets and data frames so to summarize talent provides are ready to run in integration platform as a service so all of that capability is cloud-based and ready ready to use at a moment's notice it's delivered as a data fabric and what we mean by that is that one platform to support all those different functions whether it's ingestion whether it's data integration data quality data preparation data stewardship we didn't even have time to talk about our data catalog capabilities but those are they're supportive as well and all this is ready for you to try so go to in photon comm slash request talon cloudy HTML and you can get started on your cloud journey with town and data breaks today from Cameron and I both we thank you very much for your attendance 