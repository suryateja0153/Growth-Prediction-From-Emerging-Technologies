 (upbeat music) Hi, I'm Matt Maccaux and I'm here to talk about how HPE BlueData can help accelerate the time to value for the various analytic users in your organization. And before we get into the design of what good looks like, let's talk about the different personas that exist in the organization that are operating on those analytics. The first persona we want to talk about is the community of data scientists. Now, what makes data scientists unique? Well, data scientists are often folks that have a more of a math or a quant background. They tend to come out of university with masters or PhDs in math, specializations in either verticals or specific techniques related to data science. They happen to do some programming, they know how to code a little bit, but that's really not their forte. And what's interesting about this community is they are working to solve the future and very complex problems that are going to change your organization. So they're doing exploratory analytics. They're asking and trying to answer very open ended questions like, how do we reduce churn by six percent in our organization? Or how do we increase a value from our sales organization by 16 percent of the organization? These are very open ended questions and because they're open ended questions the requirements to solve them are not always very well known. So if you were to ask a data scientist, in order to solve the churn reduction problem, what data do you need to do this, they're going to say, "Well, I'm not exactly sure but I'll take all of it. "I'll take all of the data that you have "in the organization "and I'm probably going to bring some data in from outside." And that usually gets the IT team to go, "Oh, ah, "I don't know about bringing in data from outside, "but okay, let's figure that out, "but what about what tools do you need to do your job?" And they're like, "Well, I'm not sure. "Probably what I'd really like to do "is go out to GitHub and download the latest library "and set of packages that my professor in college "recommended I use." And again IT teams usually pause there and say, "You want to bring in tools that we haven't certified? "I'm not sure if we can allow that, "but how good are you at programming?" They're like, "Well, I'm really much better at math "than programming, so I may end up crashing the system "from time to time." And the core of the IT teams go, "Well, I'm going to have "to create a sort of sandbox for you "so I can probably create, you know, a couple boxes. "I can mash them together to give you this environment. "Is that going to be sufficient?" And the data scientist usually goes, "Well, I'm going to be churning over hundreds "of terabytes, maybe petabytes scale. "I really need everything you've got. "I'll take all the servers, all the GPUs, "all the CPU and memory." And usually this is when IT throws up their hands and say, "I can't support you." But again, these are the folks that are going to change your organization. They are the ones that are going to do that exploratory analytics to give you a differentiated approach within the marketplace. Now, they make up a very small percentage of most organizations. When I pull most enterprises and ask what percentage data scientists make up, the answer's usually less than 10 percent. So who makes up the other 90 percent of the analytical community within most organizations? Well, it's the folks that we call data analysts. These are more of the traditional BI type reporting users. They're not asking what is likely to happen, they're answering questions about what happened in the past. This is operational analytics. This is the analytics that your organization uses to run your business. How many of this did we sell last month? How many stores closed within the last quarter? Very sort of structured reports that tell us what has happened. And so you can sort of imagine now the conflict between the operational team and the exploratory team. They both need access to the same data, but the way they're going about doing that is very, very different. This team tends to be somewhat destructive in nature, whereas this team tends to be more predictable in nature. The next set of personas are the engineers. We call then data engineers. The data engineers are the teams that take the models that the data scientists develop or the reports that need to be formalized and operationalize them. Operationalization means taking the sort of development test ad hoc work and formalizing it into a repeatable operational system. So data engineers have a very different set of requirements too. They don't use the same tools the data scientists do. They are developing code and often times taking the static models that data scientists have built and making them live or streaming on production systems and they're using different code and they have different underlying infrastructure requirements. All of those requirements are provided by the data operations team. The data operations team are the folks that are giving the data scientists access to the data. They're providing and provisioning machines and making sure tools are installed and supporting them as they go through that software development lifecycle to ultimately get that predictive model put into production. So, when you ask your data scientists, your analysts, and your engineers what does good look like? How would you like to interact with us to do this provisioning process? And if you go and ask them that, they're going to say, "Well, I want a cloud-like experience. "I want to be able to select the tools "that I'm going to use to do my job." But again, every different user has a different set of tools. My data scientists are going to want things like RStudio, and differences in libraries and packages to do that data scientist. Where the data analyst may use things like Tableau and SAS, whereas your data engineer are going to be programming against the operational system so they may be using Spark or Kafka to do their job. Okay, well that's a different set of tools but sure, tell us what tools you need to use to do your job. Next, tell us, well how do you actually do your development? What is the interface in which you do it, or what is the IDE? So if I'm a data scientist I probably want to use something like a notebook. I want a Jupyter or Zeppelin notebook to do my job, whereas our data engineer may use a different coding language like Ruby or Java. They may need IntelliJ provision for them so they can take the tools and then write code against them in the IDE, but of course that assumes that we have access to a code repository. Or in this case, for data scientists, models. Most data scientists don't build models from scratch. They often times leverage either publicly available models or models that have already been developed within the Enterprise. So it would be great if these users could all go from a single repository or set of repositories and pull their code, check their code out, load it in the IDE, and then attach the tools to it. But, we can't really do anything, especially if we're a data scientist or a data analyst without data. So as a data scientist I, of course, requested give me everything. But they don't really want everything. They want data that's been well curated, well described, that has high quality. Your data analyst need specific sets of tables or ranges of data to run their reports and so that should all be done through a common browsing interface. Think of it as a shopping experience with a data catalog. I browse that information. I select the data packages or ranges of data that I wish to use so that I can generate my models, load it in my IDE, and then hook my tools up against it to begin work. So, that's what these users are going to request. What the data operations team needs to know, there's a couple of other things on top of that. They want to know what the duration is. So, if you're a data scientist and you're just trying to select your tools, I'm going to spin you up an environment to do that as an operations team member. But how long do you need this environment for? How long are you going to be experimenting? Is it a day, a week, a month? I need to know that so I can appropriately make sure that the infrastructure's provisioned. I also need to know the performance characteristics of that environment. So if I'm in the early phase of software development as a data scientist and I'm just selecting tools and libraries, I probably don't need a high performance environment, but if I'm in the period where I'm training my model, where I'm going against hundreds of terabytes or maybe even petabyte scale, doing image recognition using TensorFlow, well then I probably need GPUs and I probably want a lot of them and I need high memory boxes. And this may run for days at a time to do that model training and I'm probably going to iterate over that process a number of times and so I need a high performance environment for a long period of time with access to my data so that my code can run with the tools I've specified. Now, nothing comes for free, whether your organization is doing true chargeback, we should at least have some format of showback, whether we do chargeback or showback, the operations team needs to know that because if you're training your model and you've consumed 24 GPUs, 3 terabytes of memory across a 60 node cluster, well, your department should be aware of that and whether we charge you back or show you back for that, that's really important information. So, this is what the request interface looks like. Think of this as a request portal or an interface where these users all come together to request these sets of information. Now, this shouldn't be a static interface though. This should all be meta data driven. Meta data should be used to feed the information that these users select. So, imagine that data shopping cart experience. Hopefully there's a data catalog that the data stewards, who are not pictured as part of this set of personas but is a really important group as part of the overall analytics workflow, have gone through the process of working with the lines of business to discover the data as it exists in the organization, curate the data, and provide relevant information so that this shopping cart experience can be the most optimal for these users. Additionally, the code and the models should be coming from code repositories. So all of this should be meta data driven and dynamic, of course, based on the security profiles that these users have, so that we're not just opening manual tickets that are submitted to the data operations team. Additionally, we should be using meta data for the most commonly defined sets of environments that these users have and use templates to drive process automation. So this is our process automation layer in the design. This is my request interface, this is my process automation layer. Some may call it CICD, some may call it DevOps, but whatever term you want to use, this is the automated processes that is going to be used to create environments based on those templates that we are then going to spin up in that third layer. So, I've got my automated process. Maybe if there's an exception, it gets kicked off to a manager or something along those lines, but let's just assume it goes along the happy path. What's happening on the next layer? What are we actually doing to get the rubber to meet the road? Well, here is where we have our multitenancy cloud-like environment. So as my data scientist, I may be doing exploratory analytics. That's that very destructive type work. I may write some code, execute it, write some more code, execute it, pass, fail, over and over again and so what the operations team wants to do is isolate these users from one another in these environments. So this is my exploratory analytics environment. Perhaps I have another data science tenant that is more predictable, so this is my data science predictable environment. And I can isolate that from everyone else. I may have a data engineering tenant where I've got guys and gals that are really good at writing code that are going to be isolated from someone else. And so behind the scenes we need to use software defined networking to physically and logically isolate these environments and then provision the tools using monitoring technology such as containers, we're going to spin up these environments and through the power of containerization we can isolate this so that if this container fails, well it only effects this environment. Additionally, if I need to add more capacity let's say I'm doing model training and I need more nodes in my Spark cluster, well I can simply spin up more containers and add them to the cluster on the fly within my predefined tenant. And this is all done across a heterogenous set of infrastructure, whether we're running against bare metal on premises, on virtual machine on premises, or using virtual infrastructure in the cloud in a tenant such as EC2 where I can use containers to spin up and spin down environments on demand, all managed by the data operations team from a single pane of glass based on the request portal driven by the automation in this layer. Now, we've covered just about everything except the data. So, I've spun up my tools, I've created these containers, they're all isolated from one another, everybody's got their IDE. What about the data? Well, in most organizations we have the concept of a data lake. Something running Hadoop or maybe you've got files sitting on an NFS or S3 or somewhere else. We've got, at this bottom layer, our data lake. And to be realistic, most organizations have many data lakes. So this is a logical construct of the data lake. And the data lake is being fed by jobs created by the data engineering team. These are traditional ETL jobs, ingestion jobs that are streaming, but all of the data that flows into this lake has been curated and engineered by the data engineering team. Of course these jobs are producing meta data that should be flowing into the catalog and they're flowing into the part of the data lake, again this is a logical representation, that we consider read-only. Why read-only? Well, our data scientists and data analysts requested data when they were browsing the catalog, and so within this part of the data lake, based on the information that these users have selected here, we want to be able to tap into parts of that data lake in a read-only format. So that potentially there's no opportunity to corrupt that data. But that means we need a read-write portion of the data lake. The read-write portion of the data lake is where the data scientists and data engineers get sandboxes, where they can write information. And so from these containerized environments, so let's say that my data engineer needs a full-blown cloud-error distribution in order for them to do their job. We will spin up that cloud-error cluster in EC2 or on-premises or wherever it makes the most sense, and then tap into the sandbox so that they can connect that to a repository. Our data scientists, as we talked about earlier on, maybe want to bring their own data. Maybe they're doing some sort of predictive forecasting for revenue on a logistics company and want to bring in external weather data. Well, again, we said the operations team said, "You're not bringing data from outside "into my data lake. "There's no way that's going to happen." Well, in this portion of the data lake, we can give them access to the sandbox where they can bring data in from outside. And so now, I've connected into my read-write portion, my shared sandbox, I've connected into my read-only portion and it's over here that I'm doing my joins and creating my insights for my predictive models. Now, you're probably asking yourselves, "Well, why would you do this on your data lake? "Why wouldn't you just do this in a sandbox area "on a laptop?" It's really important to note that if I am over here, potentially bringing data from outside, I may be creating new insight or personally identifiable information that the organization is going to want to audit and track and have some sort of paper trail or digital trail so that if there's potentially a breach or compliance issues we can report on that over here. Remember this? This was the duration. How long did this environment stay up for? So let's say that my data scientist was doing some exploration and that lasted two weeks. The power of containerization, against this technology is that when this timer's up and everyone agrees that the work is done we can take the insights that were generated in my read-write environment and archive it off, maybe feed that into the meta data so it can be discovered later on, but then we can destroy this environment. We remove these connections and free up these resources for other users in the organization. This gives the operations team the agility to now flex the infrastructure that they have whether it's on-premises or in the cloud, keep track of it and potentially give charge back, give these users the right performance infrastructure for the job that they're doing at the moment in time they are in the software development lifecycle and that's all through an intuitive interface. So again, these various users, the data scientist, the data analyst, the data engineers are going to come into a common request interface, put the information they need in terms of the tools, the IDEs, code models, and data, they specify the duration and the performance characteristics. That's all meta data driven, as well as the templates that are used to generate an automated process which will deploy those tools and IDEs and add the code in there and then provide access to the data where it resides so these users can be more productive in their work and they can iterate again and again. And so this is what good looks like, and HPE BlueData allows this to happen within the Enterprise. (upbeat music) Learn more about HPE BlueData here. 