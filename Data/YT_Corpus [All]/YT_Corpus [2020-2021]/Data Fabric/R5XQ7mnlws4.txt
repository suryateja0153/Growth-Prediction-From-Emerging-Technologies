 so welcome everyone to our spark AI summit and session on encryption and masking of sensitive data for spark analytics with a particular focus on California Consumer Protection Act compliance and governance so presenting today we're going to have myself less mcmoneagle chief security strategist at SEC UI with over 25 years experience in information security data privacy and regulatory compliance and we'll have a demo by our CEO of the company and founder along rosenthal who has also been almost 20 years and the information security space he had his own company active base that he built up from scratch invented dynamic data masking and sold it to informatica which became their DDM product and then started SEC QP about five years ago with you know the idea to build what you know we felt was the next-generation security solution so on the agenda then first we'll be talking about you know achieving that balance between data protection you know and analytics value you know you've got two very competing forces that are opposing each other it's it's how to you know spend a lot of time talking about how to achieve that balance and and satisfy both requirements satisfactorily and then looking a little bit about the difference between you know hold your own key encryption versus you know bring your own key or cloud provided encryption yeah so first you know there's a no thorah of different data privacy regulations around the world you know what you see on the map here is just a sampling or you know representative sample of some of those regulations you know clearly we're going to talk specifically about California consumer protection act you know in this particular session but the one common thread you're going to see across all of these is that these are all generally converging on a internationally accepted set of standard privacy principles you know things like you know you only collect the data that you need to provide the service that you're you know contracted are offering your you know customers or consumers of the data subjects that you only keep the data for as long as necessary to provide that service that you you know honor that data subjects you know rights to to have their data deleted or removed when they want that you honor their consent or preference management you know what they've opted into and what they've you know not opted into or opted out of that you honor that and you do it in a consistent way so these kinds of privacy principles are going to apply everywhere and if you really keep those in mind and you go into this with this you know privacy by design from the beginning and build privacy compliance into each new initiative it'll be a lot more streamlined you'll have much better compliance with a lot less effort and you'll do it in a you know a more cost-effective way than you would if you try to bolt it on afterwards so let's look at a few common use cases you know you know first you're gonna have situations where you're gonna have to have special controls restrictions around you know VIP or celebrity customers you know when Beyonce checks into a hospital that everyone at working at the hospital wants to know what she's in for you know when I check into a hospital the only person that merely cares is the doctor treating me in the x-ray tech that's gonna do the x-ray so the same thing happens with casinos with high rollers or you know banks with high net-worth individuals especially that's an investment bank operation so every organization is typically going to have you know those records that require special treatment or special consideration another one is cross border data flows you know where different privacy regulations have restrictions on where data is stored physically which is becoming more and more of a challenge especially when the organization's move to the cloud as nowadays you become you have less and less control over where the data is physically or it's just more abstracted from what's going on most organizations they really need to focus more on the logical location of the data who gets to see what data under what circumstances depending on where they're accessing the data from as opposed to where the data is physically stored and when you move to the cloud you might not even have any control over where the data is physically located that might be completely within the span of control of the cloud provider themselves unless you you know have special you know requirements in your hosting contract with them and then the consent and preference management as we mentioned earlier this is a big part of providing you know any privacy compliance is making sure that you're only using the data for what you've gotten permission to use it for and you're only keeping it for as long as you should you're not sharing it with anyone you haven't specifically you know stated and got permission that you're going to share it with et cetera in the u.s. especially most organizations that they've ever suffered any financial penalties for privacy violations it's usually been a contractual violation of their own privacy policies where they clearly stated you know we will not share your data with anyone under any circumstances and then they went and did it anyway and you found that out in almost all cases in the US it tends to be somebody just didn't do what they said they were going to do so another very important thing to keep in mind is say what you're gonna do and only and do what you said when it comes to your privacy policy for your company and then you know the last one the real-time behavior and analytics monitoring this becomes very important to be able to establish what is normal access or use of the data with any organization it's going to be hard from the right up front to know exactly how a data scientist or data analyst is going to have to use that data so being able to collect the information that lets you see who's accessing what data under what circumstances what applications their use you know what columns and they need to see how many records are typical access and so on and let's you establish that baseline or profile for that particular user or peer group and let's you then be in a much better position to detect anomalous or suspicious activity you know for that group or a particular user so now we come to this you know the balance between these two opposing forces you know in the one end you've got you know the data protection and privacy compliance rules no personally identifiable information or private health information has to be protected you know an access strictly controlled under need-to-know basis there's you know quite a few laws like CCPA that now are enforcing this hey you've also got to you know manage all that consent and preference management that we talked about well all of this kind of flies in the face of you know the advanced analytics and monetization of the data where you know you really want to have completely unlimited access to all the data at all times using any application for any one of your data analysts or data scientists you want to have complete flexibility on the mobility of the data and where you host it and why you know you might you know if a ws is offering a deal over as your one month you know you might want to move it over you've got elastic consumption of storage and compute capabilities that you spin-up and spin-down and it's very important that you know you maintain that freedom to leverage any and all analytic tools but at the same time still be able to apply consistent you know privacy compliance so when you do this and there's a few different specific things that you need to make sure that you do first you know that data loss prevention sort of category being able to prevent abuse being able to detect when you know a trusted insider is doing something malicious or unauthorized or abusing the privilege that you've granted them or worse when their credentials somehow have been compromised and you've got an external hacker or other rogue employee that's gotten a hold of someone else's credentials and their accessing you know data you know through an account that would otherwise appear to be you know authorized to see that data in the clear so being able to provide very fine-grained access control being able to lock down and establish a profile for how one user uses the data versus another puts you in a much better position to be able to detect then anytime you see suspicious or anomalous activity you know being able to to control you know the cross-border data flows or you know the geofencing of access to the data and then on the governance side of course you know you're going to have you know very specific or prescriptive rules that have to be enforced within the different regulations you know like supporting someone's right of erasure that's a really clear definitive functionality that has to be a requirement that we've built into you know any data set or data repository or application that is accessing you know personally identifiable information you know being able to support someone's you know right of erasure etc and then on the encryption side most privacy regulations don't mandate or require particular data to be encrypted but encryption when used appropriately is a very powerful tool to prevent unauthorized you know access or you know unauthorized exposure to any sensitive data and for particularly sensitive fields like Social Security numbers credit card numbers you know a few others date of birth and so on these are quite often selected them for adding an additional layer of security where you apply column level encryption or field level encryption you know or tokenization to those particular fields and in the case of outsourcing to the cloud to be able to use encryption to anonymize records before they're copied to the cloud so that you know if i've you know change the first name and the last name and the date of birth email address phone number you know social security number i've encrypted each of those before i host it at a cloud provider i don't to worry about that cloud provider ever being able to see any of that data in the clear or I'm not gonna suffer any damages and neither will my customers if there's you know some inadvertent data breach you know at that cloud provider so when you look at the two different ways that this can be done on the Left we have you know the normal cloud service provider encryption this couldn't be applied at a disc level at a file level you know they encrypt entire database for example or where they apply it at a column level so you either have situations where the cloud provider gives you a solution for doing that it'll be unique or different for each cloud provider so if you're using you know a hybrid cloud environment or a mixture of on-prem and in the cloud it's going to be challenging if you have a separate encryption solutions on each platform and a lot of cloud providers then their response to this was will you give us the keys you know you bring your own key you give us the keys you want us to use to encrypt the data so you control the keys but ultimately you're not controlling the keys because you're really just handing over the keys to somebody else for them and you're trusting them in the same way to only use them the way they're supposed to be used so any of these bring your own key solutions or cloud provided solutions have limited applicability when it comes to very sensitive or regulated data and for most organizations they simply don't fit the trust model that their internal compliance or information security group is trying to maintain now on the other hand the hold your own key concept you have the the same where you can check the box that the data is encrypted at rest but you can have the exact same keys used to protect the data whether it's on Prem or in the cloud and the keys remain on-premise you encrypt those particularly sensitive fields enough to anonymize the records before they're ever leave your organization and they're only decrypted on the fly at runtime for authorized users on consumption so now there is no a breach that could happen at a cloud provider or at the data layer that can expose any of that sensitive data in the clear so it's a much better approach it's a much better trust model for a lot of organizations and quickly becoming sort of the standard for you know cloud implementations and even organizations playing at Gartner and that are you know recognizing this and making recommendations to that effect so when you look at this then you start right from the data flow from end-to-end protection of the data you know one single pane of glass access control and protection solution so that you want to have the data and that column level encryption applied right at the ingestion point so you know whatever ETL tool you might be using to load the data into your you know cloud or on-premise data repository you want to apply the encryption keys before the data is loaded into the data repository then it remains in its encrypted form any time it's hosted in the cloud and the computer data layers regardless of your platform whether it's Park or anything else and then to decrypt at runtime you know based on you know various user or data attributes you know locations the application someone's connecting from you know their job title what role memberships they have that they would only decrypt whatever columns you know are needed on a need-to-know basis for that particular user at runtime and and having this where it's a centralized solution that's managed from one point is much more cost-effective and more consistently applied rules than having you know individual proprietary solutions that are unique to each platform so a common customer or a common situation that we have at one of our customers one of the BlueCross BlueShield organizations they were using primarily talent and a little bit of knife I for loading data into the cloud they're using this year they would selectively encrypt individual columns before loading them so even though they might only be encrypting seven or eight columns within you know customer and transaction tables and so on that are loaded into the cloud they're still controlling access at a fine-grained level to hundreds and hundreds of columns across all different tables independent of the ones that had that extra layer of security that encryption at rest so then on the is your cloud of course you've got only anonymize records then regardless of whatever tool someone's using to access that data whether it's you know spark or anything else on the consumption side you're unprotected the data at runtime for the authorized users and you know this satisfied all of Blue Cross Blue Shield's requirements you know they wanted the patient ID and several other particularly sensitive fields like social security number to be encrypted at rest they wanted to have a complete data discovery and data flow mapping capability that would allow them to gain insight into how the data is being used by who and under what circumstances and that helped them to fine-tune their fine-grained access control so they were putting in place and they were insistent that it had to be a holder own key concept they weren't allowed to go to the cloud unless they could show that these records were going to be adequately anonymized or at least sued anonymized before being loaded to the cloud and they needed to support a wide range of different platforms they were obviously a heavy spark user but also had other platforms that that they had to use at the same time so what we're going to do now is take a few minutes to have a long run a demo on spark letting you see how even though you've got columns that are encrypted at rest within a data repository you can still access the data you can still perform joins on those protected fields you can still do searches etc that pretty much all of your analytics can be done in a way that's transparent to the end-user and we'll show you a few different examples on spark and and Kafka and then we'll come back to continue the session welcome to this demonstration on how sacrified protects Park and similar systems here is a Jupiter notebook which I will use to interact with our spark system this particular spark system includes a special ingredient namely occupy which is currently embedded into the spark application it is now watching everything going on in spark and it will react if and when necessary spark is getting customer data from our bigquery system this is how that data is stored in bigquery we also have this details table associated with the customers table which we usually join with the customers table on the SSN column the join looks like this now back to Jupiter I will now initialize my sparks session by running the first cell I'm connecting to spark using a user called yarn which in sacrify we set to be an analyst that must work with encrypted data now I will initialize my data frames that will read from the two tables in bigquery and if I run this command that will show the data the data is presented to me like this the sensitive fields namely these four have been encrypted in my view but despite that I can still run a join with the other table because that other table also has a field as its n which is encrypted in the exact same way as the SSN field here so when I run the cell I get this switching gears a bit let's now consider a scenario where I upload my data into bigquery already in encrypted form I have done that and uploaded the data into this customers Bank table and it looks like this I've done the same to the customers details Inc table then in Sega PI I'm changing the role of yarn so that I get to see data in decrypted form now back in Jupiter I run these cells and this is what I see the encrypted sensitive fields are now shown to me in clear text as if I were working with the table that was not encrypted at all I can also run joints like this one again I am able to receive data because the SSN field in both of these tables were automatically decrypted for me in a consistent manner which allows the joints to work in our setup we embedded sakya pie in the kafka producer so that when we streamed the data into the calculus er we implemented a rule that will encrypt one of the fields here namely the first name so when the data landed in the cluster they looked like this with the first name fields encrypted now on the other side we will query the data here using case equal now in case equal we also embedded sacrifi in it so that it will implement the rules that we implemented or defined in sacrifi on the left hand side i will login as user adam and on the right i will login as user tom in our sacrifi rule we specify that adam is authorized meaning he can see data in clear-text while tom is not authorized now i'm going to run these queries on both sides and as you can see they don't see the same thing Tom continues to see the data in encrypted form while Adam is now able to see the data in clear text and we can go beyond that I can run as Adam a query like this where I'm using a clear text value in a where condition in the query and I get a result even though that in the Kafka cluster we don't have a row where the first-name is equal to this value so even no matter how complicated the query is that Adam is executing he can rest assured that the table that he's actually carrying will be assumed to contain clear text value even though in reality it doesn't so now that you've seen a few quick examples of how this can work in SPARC we'd like to go into a quick poll question for the audience and if you could you know please answer just this one question how important is it for your organization to apply you know column level encryption or hold your own key prior to data being hosted in the cloud you may find that this is essential to your organization and it's it's critical you know like Blue Cross Blue Shield did for example maybe just a nice-to-have but you don't think it's a requirement or it's not required you know perhaps because your organization doesn't host any sensitive regulated data in the cloud or you're not even doing anything in the cloud and it's not a question you even have to think about or are the cloud hosting providers you know file level encryption you know in key management you know features of functionality that they offer are those you know considered adequate by your information security and privacy people as you know fully trusting the outsourcing of that to your cloud provider so you could just pick whatever one of these you think is the the most appropriate for your organization that would be most appreciate it thanks everyone for your responses most appreciated will certainly you know make those available if they're not available before the end of the session the results that we got back so this is clearly something that you know a pretty wide range of customers that we see in the marketplace you know really care about this you know being able to provide you know this privacy compliance and-and-and very granular data protection is a very high priority for any organizations you know processing sensitive or regulated data regardless of what country you're operating in and you know the data protection is a very complex global issue it's not you know as simple as most organizations think it might be initially you can always your gloat your business may be global but regulatory compliance is the local and we'd like to stop now and good you know spend the rest of the time just for an open Q&A we've got several people on line you know a lot of myself and a few others that we'll be able to respond to any questions you have in the chat window and you know we'll try and answer as many of them as we can you know in the time that we have left in the session and then just while we're providing answers to those questions if we could have everyone also complete your your feedback using the normal channels you know provided by the event and look forward to seeing you hopefully in person next year rather than just over the videoconference thanks for taking the time and we'll stay online dance or you know questions till we're out thank you very much you 