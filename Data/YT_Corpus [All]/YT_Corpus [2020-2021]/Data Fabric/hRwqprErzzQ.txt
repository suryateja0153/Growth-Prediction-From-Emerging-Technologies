 I think I got it Can you see my screen? - Yes - Yeah I can see it now, yeah All right My name is Juan Ore, I'm an architect I work for Bits In Glass I'm based on Toronto offices  I'm Peruvian I've lived in Canada for 30 years The reason for my presentation is regarding the installation of Runtime Fabric in Amazon and Huawei and I'm putting it in that order for a reason I'll explain why just a regulation What is Bits In Glass? Bits In Glass is a Canadian company which head office is located in Edmonton we have offices located in Edmonton, Calgary, Toronto, Montreal and also offices in the US and the history of Bits In Glass can be seen here it started by being an Appian partner in 2008, then on 2013 started a partnership with MuleSoft in 2017 with Blue Prism during years 2017, 2018 and 2019, we've been named partner of the year by Appian and MuleSoft and on 2020, we've acquired a partnership with Vantiq I think I have it blocked there it is I don't know why I had it blocked and Solace is the service for... a Message Broker. We also have a partnership with them. The agenda, as we can see, is the installation of Runtime Fabric in Amazon and Huawei Why? What happened? In March In March of this year, 2020 a client in Latin America, specifically in Peru is very interested in using MuleSoft but their interest is also to modernize their solutions and have shown interest in seeing how is MuleSoft's behavior on premise or in the cloud and they have a lot of interest in knowing how would it be MuleSoft's Runtime Fabric in Huawei's cloud because they were very interested in having some other services in Huawei so we based on all these requirements from this client and we established that the best alternative for them was to have Runtime Fabric in Amazon and in Huawei and the idea was to start with Runtime Fabric's installation in Amazon on our side, to see how is the configuration to see how MuleSoft makes this installation in Amazon and from there, make a manual installation in Huawei's cloud we define the clusters, we define the VPCs and then we make the final installation of Runtime Fabric in what is called the "Manual way", according to the documentation. for the installation in Amazon we use what is called the Production's configuration - minimum configuration according to MuleSoft's documentation, which consists in 3 controllers and 3 workers and then for the manual installation in Huawei's cloud we use the minimum configuration for development in which we have just 1 controller and 2 workers, although in our case we used 3 workers now we start with our installation of Amazon or AWS these first steps are common, it doesn't matter if it's manual or with Amazon we create a Runtime Fabric installation, in this case the name we chose is big-webinar this is a new installation that I did 2 weeks ago, approximately just to see if there was any variation from the installation we did in March ok, so, two very important steps step 2 is to download the configuration files that are compressed and the second thing is to save the information, or the activation data once you download the installation file you will find several folders. One folder in regard to Amazon another folder in regard to the manual installation another folder in regard to the installation of... Azure in Windows, and the common script Inside of Amazon, there are two types of files one that ends with .tf and other that is run by shell "tf" means TerraForm MuleSoft... chose to do the installation for Amazon by using TerraForm, which is a very useful tool to automate installations made by code from these 4 tf files the one that you can normally modify is the first one, the fabric.tf the rest should remain as they are there's no need to change those and for the shell scripts, those will be used during the installation that TerraForm will perform in the manual case, you just have one script shell, which is to generate the environments' configuration and afterwards the init file runs ok, let's see what updates have we done for the fabric.tf file with respect to the installation we did in March in March we didn't modify it that much, but in this second time we started to see what was best to do or how could we take more advantage of Terraform what we added were 3 variables one for the access key for Amazon, the second one is for the secret key, and the region then we add these 3 lines over here the second change was because of this note that you'll see in the installation manual it's with respect to the access for the OpsCenter once the Runtime Fabric installation is complete the access to OpsCenter is not exposed but if you want to access through the internet to OpsCenter you have to do two things: first, activate the public IP, and second create a rule for the security group that allows the public access from the internet to port 32009. In the first installation we did in March, this access to port 32009 through the internet, we did it manually we accessed Amazon's console and created this access but now we said, ok, if this is going to be an option that we'll be using frequently it's better to have a predefined value in the Terraform this variable already exists inside the fabric.tf file the only thing we modified was the CIDR 0.0.0.0 and this variable is used in the Network_ingress.tf file, in which, we are able to access through the public internet to the OpsCenter this is one of the changes we did in this last installation ok, now with these changes the first step is to run the Terraform init with which Terraform loads all the plugins that are necessary to work with Amazon it verifies that you have the latest version, it verifies that you have everything authorized with respect to what is necessary for Terraform and finish with the execution of the creation services from Amazon the second part is the Terraform apply. It is here where we start to see the execution of the installation in Amazon the moment you call terraform apply, we can also pass some variables in this case, one of the variables, as you can see in the example that shows the MuleSoft's documentation the cluster_name is the same name that we used to create the Runtime Fabric installation you'll see next how the Terraform script uses this cluster_name as a prefix to tag all the resources that it's creating in Amazon the key_pair is what gives you access to Amazon's resources we selected 3 controllers, according to the minimum setup for the production environment we also have 3 workers here we activate the public IP we specify that we want to activate it according to the note that was on the documentation we got the activation_data at the moment of creating the Runtime Fabric installation, also the mule_license and the most important thing, this state that is where all the configuration made by Terraform where is all that information going to be saved here you can see that it's going to be creating a folder tf-data and the file with all the configuration's results is going to be rtf.tfstate it's very important that you take note of this information ok, once the Terraform apply is executed the last section of the messages that it shows in console, is the results of what it created so, we can see here, that it created the private IPs for the controller they're 3 the public IPs for the controller same thing for the workers, the public IPs and the private IPs when you set enable_public_ips = true it creates them for both the controller and the worker, that's how the script is designed what commands should you have ready for Terraform? well, I recommend these 3: one is the terraform show and you get the state of the terraform this command shows you everything that it has created in Amazon the Terraform output, contains information from the terraform script why is it necessary? in this case, you can see this output so you can see again the same information and the last one, is Terraform destroy.  You execute this when you want to uninstall Runtime Fabric please note you have to be careful with this command because it deletes everything well, once that the Terraform apply is finished we can now see that looking at the log this instruction that uploads the init log you can find this in the installation document for MuleSoft it gives us the URL for the OPS Center the user and the password to use this IP address, is the address for the private IP, which you can't access that's why you have to use this address: 35.183.62.191 which would be the public IP address here you access with the public address and you'll be able to access the OPS Center. Here I already accessed the OPS Center and when I click the Servers button, I can see all the workers and controllers that have been created and how you can see, I have one, two, three controllers and three workers and each containing its IP address ok, now, let's see in detail, everything that was created in Amazon the region that we used was ca-central-1 this is where we did the last installation the VPC that was created we used this address this CIDR 172.32.0.0/16 as you can see the Terraform script uses two tags: the name and the role. As you can see in the name, it generates 'big-webminar', that's the name of the cluster and a VPC. This is the standard that is used by Terraform it's very good, because we can search for all the resources we have defined for for example this rtf-big-webminar simply by looking for it with this and the Terraform itself searches for which availability zone is available and, basing on the controllers requirements and workers requirements, it creates the distribution of both. In this case, it used 3 availability zones central-1a, 1b and 1d it also creates a gateway, which gives internet access to the worker and now the subnets there's a subnet in each availability zone and each subnets has its own CIDR in the case of 1a, it's 72.0.0.24 the 1b is 32.1.0.24 and 1d is 32.2.0.24 all of this is done by the Terraform algorithm you don't have to be assigning this it's enough to know the value for the VPC and the script will start assigning everything in each subnet it's creating a worker and a controller as we can see here then it comes the security group where we have two rules: one for inbound and one for outbound. These are the ports that are documented in the installation of Runtime Fabric so, all these ports have been assigned, and how you can see here, inside this purple circle is the access we're giving through public internet to port 32009 to be able to access the OPS Center if we hadn't done that change in fabric.tf we would've had to access the console, access the Security Group, and add this rule in the inbound but there's no need to do it with this last installation we continue with the inbound rules and the outbound rules all of this is according to the documentation the Terraform already has all this defined ok, with respect to the EC2 that were created as we can see, there are three controllers and three workers all of them with the 'big-webminar' prefix the controllers 0, 1, and workers 0, 1, 2 this is how the Terraform script assigns the names the controller has an instance type of M5 large and for the worker it's R5 large this is what the Terraform script defines. However, you can change it it depends a lot on the configuration or the requirements that you can have. But, in this case, we didn't change it we continued with the default values that Terraform assigns and as you can see, these are also distributed in the three availability zones: 1a, 1b, 1d for both the controllers and the workers ok, now, let's see what is the controller we already saw that they're assigned to each subnet the only thing that we have to see here is the assigned disks there are two assigned disks for the controller one is called the gravity volume and the other one is the etcd, under the defined conventions this, for the controller the worker only has the gravity volume and it's 150 GB it doesn't have etcd, only the controller includes an etcd device name the worker doesn't have it once the installation is done you can go back to the Runtime Manager and we can see that it's active this Runtime Fabric big-webminar the version we have is 1.6.8 that's the actual version 3 weeks ago, 1.6.8 here we can see the state everything is working as expected it found the 3 new controllers and 3 workers with all this information that we get from the Amazon installation is the knowledge that we need to start the installation in Huawei's cloud with this information it was easier for us to plan how we were going to create all these resources in Huawei Cloud. Right now we're in Huawei's console as you know now, we are following the standard that Terraform has with this we can set the name for our Runtime Fabric as rtf-poc-hw that's the name of the Runtime Fabric installation we created we chose this CIDR 172.31.0.0/16 and we created 2 subnets by the way, as you can see here, in the top here are the 3 workers and below that, the rtf-poc-hw controller. This one here is a controller these two elastic cloud server is for the Solace installation the client wanted to see too how this Message Broker worked so we made an installation for a Solace client here and we also installed a web server to be able to display this information and create the demo for the client so this is all we created for the Huawei Cloud as you can see, the VPC contains 2 subnets 172.31.0.0/24 for the controller 172.31.1.0/24 for the worker we had a controller in a subnet and the rest for the workers in another subnet. as you can see, there are a lot of ways to do this Runtime Fabric installation we have to experiment in order to see the advantages of one and another we created the Route Tables for the VPC in this case, we also created a NAT gateway here's a bit different between what we have in Huawei and what we have in Amazon, but the functionality remains the same. This NAT gateway will have access to both subnets as you can see here, we already have our controller defined and the 3 workers that was the result of the installation in Huawei Cloud the controller the controller, as before, has a public IP address and a private IP address it's the only one this is an instance type of C3 large that has 2 virtual CPUs and 8 GB this is an instance type in Huawei's terms and the operating system in this case was the CentOS 7.6 the disks we continue with the standard, it's Docker we can see here the gravity device just like Terraform did in the manual form it's called RTF Docker it has 250 GB and the etcd. As you can see, the controller always has to have all these disks. Meanwhile, the worker only has to contain the RTF Docker same capacity of 250 GB, and an instance type same as the controller and same operating system as well in this case, the worker does not contain a public IP address. Here it is the summary of the configuration here you can see the controller and the workers we used the Santiago region instead of using the Lima region just to be able to see what's the behavior or if there was a difference, but there wasn't any. The only availability zone that we had available for the created account was the zone 1 instance type was the same the number of CPUs is higher, and contains a higher memory because those are the ones that handle the workload. Once you have all these services defined in Huawei we can start with the manual installation in this manual installation you have to specify what is the IP address that will be assigned for the controller and the IP addresses to install the workers we define the RTF Docker Device according to the same standards in CentOS it's dev/vdb in Linux it's dev/xvdb it's the same standard for the etcd device select the RTF Activation Data, which you get when you configure the Runtime Fabric installation and the license, of course.  And you run the shell script. What does it generate? it generates the information that is exactly what you have to create in each server. In this case, for the worker, you can copy all this, then access the worker and create this env file with these values that's the first step you need to do. You'll see that for every IP address that was specified over here, 0.66 is the controller there's also an entry for the 1.5 for the 1.241 and for the 135 here we can see the 1.5, which is the worker as you can see, it's the one that has just one Docker Device of 250 once you run the init shell and the process is finished you can go to the .state folder in in the controller, in which you can see that all your files are there and there wasn't any problem here is the part of the activation of Runtime Fabric for Huawei we need to be using TLS for which we'll be using the Secrets Manager, which is the recommendation that MuleSoft gives you we create this group and create this context this will be used for both Amazon and Huawei in this case, the big-webminar once the installation is done it ends up with version 1.6.8 with this number of cores and this number of available memory. At the moment of defining the inbound traffic, the TLS context defines the same that it's using here. In March, this is what we ended up building the hw-poc-latam this is the Runtime Fabric that we created in Amazon the rtf-poc-hw was the runtime that we created in Huawei Cloud we have the two Runtime Fabric installed one in Amazon and one in Huawei and the idea was to create an application that allows us to access or synchronize data from Salesforce, from contacts, with a MySQL Database and this MySQL database existed in both Amazon and in Huawei so we performed several tests and, continuing with that, after performing the installation of Solace too here is the Solace configuration and we automatically have access to the console that allows you to create queues, topics that were necessary to create the Proof of Concept for the data synchronization between Salesforce and MySQL here we can see that the challenge was to create it in Huawei it had 11 cores, 45 GB no problem. This is the project we did for deployment, both for Huawei and AWS it's the same project the same project that we did deployment for Huawei Cloud and Amazon and by managing the properties we could see if it was Amazon Cloud or Huawei Cloud in respect to that we would access the database in this case here we can do a deployment in Amazon and we can tell it to access the database that is running in Huawei or... if we're in Amazon's Runtime Fabric we could tell it to access the MySQL that is in Amazon or in local we could do different combinations with the idea of showing the client the flexibility that Runtime Fabric has. You can use the external resources that are in Amazon, but also use the external resources that are in Huawei and also show that we can have a Runtime Fabric running in one Cloud system, like Amazon, and use the resources that exist in Huawei, that's what the client wanted these are the two definitions of MySQL database: the one created in Amazon and the one created in Huawei. the web server that I mentioned from Solace, also let us see the messages that were being created and we could see them from both our laptop and the cellphone that was an improvement what other commands should you know how to use? even though everything is going well first of all, we have the gravity status, which is what we use in our Amazon installation which shows you what it created with all the details another command that you can use is the rtfctl status you can use it for both Amazon and Manual once the installation is done, you can run this rtfctl status in order to see the status of your nodes in case you have network issues, you can use rtfctl test outbound-network and it will check if you have any issue with the network or not from all this experience that we had with the client what did we have as feedback? in the case of the Terraform use there's also another option in Terraform, it's called workspace this workspace lets us, you have a predefined script and all configured, and you can create the workspace using Terraform creating a workspace for development, another workspace for testing and another one for however you want to distribute your project's lifecycle for MuleSoft and so you have workspace where you have the same configuration for so you have several commands for this workspace, like show that will indicate your current workspace list - to show you all the created workspaces select - to choose in which workspace do you want to work right now new - when you want to create a new workspace and delete to remove the workspace and this through this workspace, you don't have to remember that much where is the the Terraform file 'state' you can control this file through workspace Examples: here you're creating a new workspace that's called development and here you select the development workspace to start working and, of course, if you're going to work with this remember to create a variable in which you can assign the value of the workspace you want to work on that's one. The other one, I don't have it here in the slide which is the use of Terraform Terraform is a very flexible tool that works with different Cloud systems it even works with Huawei so, one of the tasks that I have is to work with MuleSoft to create the Terraform script for Huawei because right now, the only way you have of installing Runtime Fabric in Huawei Cloud is the manual option it's not like installing in Amazon, where you have the Terraform script and it's bulletproof in which you're more confident on what you're installing. That's all I have for this installation of MuleSoft's Runtime Fabric. 