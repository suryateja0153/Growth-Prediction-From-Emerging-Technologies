  Hi, everybody, thank you, I want to welcome everyone to our second distinguished lecture of this academic year. It's an honor to be able to introduce Dave Patterson to you. Dave received his bachelor's, master's, and doctoral degrees, all from UCLA, and joined the Berkeley faculty in 1976. He's renowned for a number of a long sequence really of just amazing collaborative research projects at Berkeley, RISK, SOAR, SPUR, RAID, NOW, IRAM, ROC, RAD Lab, and PAR Lab. I think that's probably going to be it, we'll find out soon. The contributions of these include, of course, research results, also, a huge amount of commercial impact, a great sequence of students, just phenomenal students graduated from those projects. And one of the things I've always admired about Dave and the Berkeley projects is there is enough meat in each of those projects for every student to have a piece that allows him or her to reach their potential, so it's really remarkable how those projects are run and the quality of students they produce. Also, these projects have really created a signature for UC Berkeley-- a reputation that makes it absolutely preeminent. Dave is a member of the National Academy of Engineering, National Academy of Sciences, he's been president of ACM, he's received the Turing Award, the Eckert-Mauchly Award, the ACM Distinguished Service Award. He's now a one-day-a-week emeritus faculty member at Berkeley and a distinguished engineer at Google. And today with whatever time remains, he's going to talk to us about domain-specific architectures for Deep Neural Networks. Please welcome Dave Patterson. [APPLAUSE] Thanks, this is one of my favorite places to visit and give talks. I visit once every five or 10 years here and catch up and all the amazing things going on. I assume you are all appreciative of having the greatest rainmaker in computer science with Ed Lazowska here. The fact that we're sitting in the second building that he has helped raise, where many of us are looking forward to the first building. But it's really a fabulous space and it's fun to see here. So what I'm going to talk to you about today is a very-- we're in a very exciting time in computer architecture-- is that what computer architects do is try and make things go faster for less money for some workload. And this ending of Moore's Law and Dennard Scaling has made the general-purpose microprocessors not so fruitful, so we're switching over to domain-specific architectures. And basically, they do just a few things really well, but they can't run arbitrary C++ programs which was the goal for the last 40 years. And so the things we've learned and the experiences may not translate to this domain-specific architectures. So that makes it fun for researchers and engineers, scary if you're at a company. So I guess six years ago, Google started doing domain-specific architectures for what was then a new idea of Deep Neural Networks and the Deep Neural Network model that transforms inputs via calculations of weights to produce the desired output. And the two big things are training where you develop a model, and then inference of serving where it uses that training model. The-- the resulting Tensor Processing Units, or TPUs, were first deployed in data centers four years ago. If you use Google, you are using TPUs for searching, or for ads, or for translation. So this talk is going to talk about the first three generations of TPUs. So we've talked a lot about the first generation of TPU. I'll talk a little bit more about it today, but the next two TPUs haven't been published yet, so this is brand new information. But when I'm talking about is the cloud, not the edge. I mean, the edge is a very exciting area for machine learning, that's just not what I've been working on at Google and that's not what I'm reporting on. So it's based on two talks that are both going to appear in Communications ACM. The one that's already appeared that was on the cover more than a year ago, and this next one that's going to appear sometime in Communications ACM, and this one-- this is the part that's not been published yet, so you'll hear about it first. So I think Jeff probably came-- when Jeff Dean was the guy here two years ago who founded Google Brain, where I'm working. He's the guy who made this observation. He said I think Deep Neural Network is going to start at work. If people started using Google Translate on their phone three minutes a day, and let's say 100 million users did that, Google would have to double their data centers. And even Google can't afford to do that, and even if it could afford it, it would have taken years to double the number of data centers because you've got to build buildings and stuff like that. So they started this emergency project, and I don't know if they arbitrarily picked the improvement in total cost of ownership for this inference phase by a factor of 10, but that was the charter. So they started this emergency project thinking the DNNs were going to work, and in 15 months they came up with everything; the architecture, the software stack, the hardware, and deployed it in just 15 months. And this is TPUv1. This is what it looks like. It's a-- here's the chip and it's on a board that's this big and you can put four of them into a server-- that's the accelerator. And the heart of the CPU is the matrix multiplying unit. Amazingly, 65,000 multiply-accumulate units on this chip. So why is that amazing? Let's see if this works now-- all right-- that-- that's like 25 times as many as you'd see in a GPU of the time, so many more arithmetic units there. It also has a lot of memory on chip, and that is four times as much memory on the chip. So here is it-- that's the big pieces of why it's a domain-specific accelerator. And then it also had a modest amount of slow DRAM that was connected-- that's where the weights were kept on this design. So that's the big ideas there. That's what it looks like on the inside. And here's what the floor-plan of the chip looks like, and more than half of it is for the memory and for the multiply-accumulates unit. So most of it is in the-- is in the parts that are delivering on the performance that it wants. And then one of the ideas that they resuscitated from the past was 40 years ago Leiserson and H.T. Kung-- who were at Carnegie Mellon at that time as part of Leiserson's dissertation-- invented this idea they called Systolic Arrays. That if you're going to do a big matrix multiply, normally what you do is write the results in the registers and read it back out, and write it-- you'd do lots of register axes. Their idea, you can choreograph the partial products if you're doing a lot of them so that one would just show up at the inputs just in time, and you would save all those register reads and writes to do that, and save a lot of energy. So that idea got brought back from the past and used and used in this systolic array. What it led to is a really amazing resulting in performance. So this is for products, not research papers-- but the performance per watt-- and with the end of Dennard Scaling performance per watt is a very important figure of merit-- this is compared to CPUs in red and GPUs in orange there-- this is factors of 80 and 30 times better performance per watt by doing something domain-specific rather than general purpose. And not only that, as part of this first paper we said, well, this was done in a rush, suppose we had more time, suppose we could have made the clock write faster, or made the matrix multiply unit bigger, or made the memory faster. Well that was the key, it was the memory was pretty slow that had that weights and it was a standard memory. Suppose we'd use the graphics DRAM like GPUs use, how much faster would have gone? The memory bandwidth would've gone up by a factor of five and a lot of the applications were compute-- were memory-bound-- limited by the memory. And so it shot up again, so it would be a factor-- 70-- if we had built that one, it would be 70 times the performance per watt versus GPUs or 200 times, so that's amazing numbers here. So why? Well, this-- probably that giant multiplier with-- well, whatever if it was-- 25 times as many multiply units there. And not only was it a giant multiplier, it was doing things in 8-bit-integer arithmetic rather than 32-bit floating points, so that's much more efficient use of the memory and the computation. The systolic array idea brought back from the past that probably cut the energy costs in half for the multiplier which is a lot of it. And then it was domain-specific, so a lot of the things that are complicated and area consuming on CPUs like branch predictors and caches-- CPUs are covered with caches-- they're not there, because they're not helpful for this application. It was better to use more multiple units and dedicated memory units, so that saves energy and you could reuse the resources. So that was a big success. So then, while they were getting it rolling into the data center, they said, what are we going to do next? The original plan was let's do the next inference, so that's what they seriously considered-- "We know how to make it better we've done this other one."-- But they decided instead to do for the harder training project. And so I'm going explain what it-- why is training harder. But they said, we've got inference working, let's go for the training step next because a lot of what-- Google spends a lot of its resources on training. So classically there's-- a neural network has an input in these stacks of layers, right? And so the way this works, amazingly enough, is you're going to do the supervised model where you start off with the weights set to random initial values-- random data-- and then you have this labeled data-- millions or billions of these pairs of input results-- like here's an image and it's a cat, or here's a frenum-- here's a waveform and the frenum it represents. So they got started off with this labeled data. Now, what you do is to learn the weights you use this old idea of stochastic gradient descents, and you do these three steps repeatedly. You grab an input result pair, and you run it forward through those layers, and see what the result is. Well, since it was garbage to begin with-- random [INAUDIBLE],, the first time through it's going to be a crazy result. But then, you look and see what the error was and then you read it backwards through those layers to try and adjust the weights so you can get the right answer-- so you calculate loss function and then you update the weights. So a key part that'll come later are these updates are pretty small, right? They'll be-- you're trying to adjust the weights eventually so you get the right answer. And then you just keep doing this millions or billions of times to be able to adjust those weights. And remarkably enough, you start off with random date, you do these sets, you do these enough times and it can be smarter than a human or something, what amazing result this is. So the inference part that the serving is really just the first step-- that's going forward through it-- you've already got the way to go forward. So the inference is one-third-- or less than one third of the work, training has got to do everything else. So why, what else is? So a lot more compute, you've got to do the back-propagation, and you probably should do derivatives, the activation functions that you do after when those layers is transcendental, and you have to do multiplications by transpose matrices, so lots more computation for training. It's more memory, you've got the weight updates, and then you have to talk-- you have to keep track of information between the forward pass and the backward pass and that can be 10 times bigger than the weight. Because the training algorithms change, it's got to be a little bit more programmable, because you want to make sure you can handle changes in weighting at training changes and it still works on your computer. Tiny integers aren't going to work for training, it's got to be floating point, and it's going to need to be wider than eight bits. And then the parallelization between the chips is harder because for inference they're all independent-- you've trained it, you can just run it through, but you've got to coordinate the weights to keep the weights consistent as you update them, so it's a more difficult computing problem. So 2014 when this started, what did the world look like? Well, people were doing all this stuff on CPUs and they'd divide the CPUs into two pieces. There were the workers that would do the stochastic gradient descent and the parameter servers would do the weight. So they would-- it would be a big cluster, the stuff, it could be flying back and forth, and the parameter servers would handle the updates, and that's the way that was done. It was called asynchronous stochastic gradient descent-- because you're dealing with clusters and long latency, that's the way it was done. So then became the question for the Google engineers, what are they going to do? Are they going to just do what they did for inference was just build this cluster-- cluster of servers that have these accelerators stuck inside them and use data center networking to connect them together, or are you going to build a supercomputer? And so part of the calculation was, well, how long do these things run? So the Google production applications to train would take between two months and 16 months on one chip-- 16 months to train. So if you're a computer architect, you love people like this, right? You hate people who say, why are you building faster-- my laptop's fast enough. You hate people like that. But people who are like, it takes me months on the computer train it, so you like those-- so-- so they want it-- clearly you don't want to wait months, you want-- so you want to use hundreds of chips. And the other machine-learning conventional wisdom then and now is the bigger machines, the more data, the bigger ML breakthroughs. So the Google engineers decided we're going to build a supercomputer, we're not going to build chips that you stick into clusters in somebody else's network, and we're going to build a supercomputer. And that was a really great decision in 2014. This paper that was done-- a blog post by people at OpenAI in San Francisco-- calculated over the next eight years how-- for the state of the art-- the bleeding state the art-- how much did the compute go up? And they said it was doubling every 3 1/2 months and if you multiply that out, that's factors of 10 per year. So if you wanted to be at the bleeding state of the art there, you would be 10 times as much compute every year so which is amazing. So yeah, so people really wanted supercomputers for training. So the critical feature in the supercomputer is the way they talk to each other-- the chips talk to each other. So what's called-- Google invented something called ICI, it's a custom Inter-Core Interconnect that they did custom for this design-- it's not some standard that you buy off the shelf. It runs it about 500 gigabits per section in both directions per link and there's four of those links. And then, rather than having a separate switch, they all through-- through the switches distributed through all the chips is the standard way to build a supercomputer. And it turns out one of the nice features of building a supercomputer for domain-specific architecture is you know the communication patterns. The challenge for normal supercomputers is, well, there's all kinds of ways people might communicate, how do you balance that? But you're doing this weight update thing, and you can map that really well on a 2D torus and that's a nice simple topology for things to connect to. So we can do a domain-specific topology for a domain-specific supercomputer and that's what a 2D core is like. It's just a matrix and you connect the ends, that's pretty easy to build and deploy, and it's very economical. So you do direct wire connections between the TPU chips, on the dye size of which I'll show you in the-- it's less than the-- it's about one-eights of the chip is all you need for doing those links-- that's the extra cost. And it saves power and makes it easy to build these things. That's the TPUv2 there and those colored wires are the wires that connect, connect them together. But you can-- they're basically adjacent links except for the [INAUDIBLE] ones-- the ones at the ends. And then the link bandwidth, is-- this is 500 gigabits per second, that's five times the link bandwidth over what was available in 2014. And it's probably, let's say, one-tenth of the cost because you don't have to have the network interface cards, you don't have to have the switches, and you don't have to have a little protocol engine that runs that. So it's a much simpler design and so faster and cheaper. And then this thing happened once the hardware people said we're going to do this really fast supercomputer interconnect that led to innovations in the algorithm. Because as a dedicated network there would be no variability in the latency like they were on clusters so you could do synchronous parallel training. And so instead of having parameter servers, and workers and you have to handle asynchrony, you could just get rid of the parameter servers, they have the parameters as well as in the SGD and the short-- they'd be long short-tail latencies so you keep everything synchronous which led to a better designs. And it means-- as we'll see later-- given that you could do all your work synchronously, you get really tremendous speedup as you-- as you add chips to it. OK, then next question is-- OK, so we did the network-- that's the big question for supercomputers is network. What about the chips themselves? Well, then one of the questions was how many cores per chip? On TPUv1 we had one core per chip so that's attractive. But as Google went to a more aggressive technology for TPUv2 with a shrinking feature size, the wire delay increases relatively. So what's the right number of cores per chip? Well, we know training can use a lot of computers-- a lot of processors because it takes months to run. But so how many cores should we do for chips? So we're building a supercomputer, we went to Mr. Supercomputer, Seymour Cray, for advice. And this was something Seymour Cray said if you're plowing a field, which would rather use, two strong oxen or 1,024 chickens? So we followed his advice and went with strong oxen, so there's two TensorCores per chip and a big reason was we could-- that was pretty efficient and we could-- it was easier to program it for two cores per chip than lots of little cores per chip, so that's the way we went. Here's what the block diagram looks for the TPUv2. It does have a matrix multiply, it's a smaller matrix multiplier unit, so it's one per TPU, or for TPUv2 there's also a unit that does transposes, permutes, and reductions, so special hardware for those matrix operations. There's a vector processing unit, which does a two-dimensional vector processing and it has a vector memory as well. The-- and its peak performance, this thing is where it gets the highest performance given that you have 32,000 multiply operations per-- multiply-accumulate operations per second, whereas this one has about one-tenth of that peak performance. This thing fetches instructions from memory. And then the ICI-- I already told you about-- that connects the chips together. And then one of the weaknesses of the TPUv1 was that memory system-- the standard-- using standard DRAM instead of graphics DRAM. Well, if they didn't want to make the mistake this time they went to something that's called high bandwidth memory. Now, what high bandwidth memory is a more novel packaging solution where you have a substrate that's called an interposer that you put your TPUv2 dye on, and then near it are stack-- short stacks of DRAM at very wide interconnections. It has, amazingly enough, 32 64-bit buses per one of these TensorCores so there's two, so they actually have 64-- 64-bit buses per chip, whereas a server might have four or six of those 64-bit buses, so it has 20 times the memory bandwidth of TPUv1, so they weren't going to make that mistake again. And then this is an accelerator it talks to the host you over a standard bus, and, again, that's either typically about four of them per server. This is what the floor plan looks like I said the ICI links which are in these corners use up a small fraction of the chip, there's also some ICI switch stuff in there-- but it's about an eighth of the chip-- and even the matrix multiply units you see aren't all that big. Now, when you're doing domain-specific architecture is the scary part about it and what people worry about who don't-- especially people who would rather use FPGAs than ASIC chips is what happens if the algorithms change then the thing doesn't work? So this happened during the design of the TPUv2 is in 2015 when they were building it this new algorithm appeared called batch normalization and it could speed up training and make it more accurate by factors of 10 or 14. So essentially, what it does is it subtracts out the mean and divides it by a standard deviation to make it look like samples of a normal distribution-- that led to this big advance. So TPU has-- is an instruction set, you have software, so you could do it in software, but it ended up with a lot more vector operations. And our vector unit at the time when we were doing it wasn't all that powerful, so they made the vector unit eight times as powerful-- so eight times as many, what they call, vector lanes-- to be able to do a lot more competition of the vector unit, which would be needed now because of batch normalization, so that's an example of something that's done. In terms of the instruction set itself-- instruction set itself, it decided to go with what's called, very long instruction word model, so that you can specify instruction word parallelism on an x86, they try and guess by looking at the instruction stream-- the very long instruction word model is you do it statically. You have this is a 320-sum bit instruction that has actually eight slots for operation. So in one instruction, it could do two scalars ALU operations, two vector ALU operations, one vector load, and one vector store, and then queue something for the matrix multiply unit and queue something transfer unit. So you can do eight things potentially in this one very long instruction-- so that's going to be a compiler challenge to be able to figure out how to use it. And then here's the amount of memory that's the state per TensorCore, and one of these things-- the big part of this is the vector memory which is 16 megabytes per TensorCore, so there's two of them on the chip. OK, now, I showed you in the beginning for TPUv1 we said, well, here's this emergency design, what about if we did some variations, what would happen? Well, we're going to do better than that this time. Rather than a hypothetical TPUv2 with different types of memory multiply units and speeds and stuff like that, Google actually built two chips out of this very same technology. So the TPUv3 has basically a third faster-- the clock is a third faster, the ICI interconnect bandwidth is a third faster, and the HBM memory bandwidth as a third faster, but instead of having one multiply unit per core it had two multiply units per core so there's four in the chip. So it has-- when you multiply that together it has 2.7 times the peak operation bandwidth on this design. How did they-- how were they able to do that? Well, it burns a lot more power. Well, how could they get it-- how could they handle that? Well, they went from air-cooled design to liquid-cooled design, so-- and I'll show you that in the picture-- so instead of just dumping the heat in the air and having air-conditioning units suck that other air, you heat up a liquid which can be more efficient. It's got more-- it has more DRAMs in that HBM stack so it has more capacity, and they decided to scale up the size of the supercomputer by a factor of four that they're going to build. And then what-- how-- it's two chips in the same technology, how much bigger was it? Well, because they already had a good idea with the layout challenges were from TPUv2, it was only 6% larger because it was a much more efficient floor plan, so do all of that. And here's what they look like, and Jeff Dean probably showed these. So this is TPUv2, and you can almost see that this is a heat sink and that heat sink is like this tall, right, on that board? And here's-- this one has 256 chips and plus some room for I/O devices. Here it is with the liquid cooling and you see it doesn't have a-- this is very flat, but because it-- and these pipes, liquid are going through these pipes and this is copper to help cool it down, so the water-- or whatever that liquid is-- flows through that and comes back out-- cold in and heat out, and that's how they cooled it. And now, we can go eight of these racks which has 1,024 chips in it. And so this is-- to keep up with the OpenAI challenge a factor of 10 in one year-- this is a factor of 10 in one year. So OpenAI, I guess would say, OK, but it was-- it's a pretty amazing thing to do. That's the peak performance, OK, so now, what are some of the features of these things? This is TPUv1 and, again, this clock rate is-- that's the same the clock rate as the v2, and it was a third faster for TPUv3. And then here's the Volta GPU-- which is the latest GPU-- which has much better technology and a much faster clock rate. They have-- they're in a much better technology-- the so-called 12 nanometer-- Google doesn't reveal its partner's technology, but it's a much older technology. And v1 was in an even older technology-- it was less than half of the competition then. This is less than three-fourths of the competition, this is 6% larger. So it's a smaller dye and an older technology. And then, architecturally-- I'll talk more about that later-- but you see this is a modest number of cores per chip in the-- the GPU is in the many cores per chip model there. Now, this is-- I'm genuinely surprised as part of my career that I'm going to talk about somebody who doesn't use IEEE floating-point arithmetic. So Berkeley is very proud of the IEEE floating point, Velvel Kahan won the Turing Award for his contribution to IEEE floating point. And I never thought I would ever talk about a chip that doesn't follow IEEE floating point, so let's talk about that. So TPUv2-- the systolic array-- 8-bit integers, that was a big win, so could we do it again? But 32-bit floating point ALUs are tremendously bigger areas so that seemed unaffordable. And by the way, 16-bit floating point is much faster. If you were looking at that last chart, 16-bit floating point is eight times faster for both the GPU and the TPU than 32-bit. The multipliers are much smaller in 16 bit than 32. So they started playing around with the floating-point format to see if this narrower floating point would work for-- work for training, right? That was the test. So what they found immediately is the outputs of the matrix multiply unit, and the internal sums need to be remaining 32-bit floating point otherwise things went to hell pretty quickly. But if they followed the IEEE standard which is shown here in this slide, as you see, the exponent-- it's only 16 bits, so the exponent is only 5 bits. If they did that, quickly accuracy went down. And for-- if you're a machine learning worshiper for accuracy-- which is how good your model does a job of translate-- recognizing images and labeling them, that's everything-- they care passionately about accuracy, and if you cut accuracy half a percent they're going to yell at you, right? So if you're going to 16-bit floating point it's much faster but wow it hurt accuracy a lot. If you think about it, five bits of exponent can't even represent-- can't even represent a 32-bit integer number. If you took a 32-bit integer number and you convert it, it wouldn't fit, all right? It can only-- a five-bit integer can only fit something like-- something about 60,000 in integer. But if they cut the mantissa down from 23 bits to seven bits, it still worked. So this is one of the claims about-- as I learned about machine learning is the saying it's low-precision linear algebra, and it's like, well, OK, how low can it be? This is an example of how low could be. They could cut the mantissa from 23 bits to five bits, but the range was really important. So Google invented this format called Brain 16 format that is not IEEE, and the saving grace was that by keeping the exponent the same size they'd have exactly the same software behavior, right? If you know what nans are and the infinities, they would occur at the same place because the exponent was the same side and coded that way. And this was enough precision to be able to do a machine floating point. So why is that a big deal? Because you think of it you'd think it would be the 16 bits is the area, but it's not, it's the mantissa-- it's the fraction piece of it that you're doing the multiplies. So the smaller the fraction, the smaller the area, the less energy and it's much more economical to do that. So this shows that if it was 32-bit floating point, it would be nine times the area and energy of Brain Float format and the IEEE standard would be twice as big. So these factors of two is a big deal for something when you have 65,000 multipliers. And then it was easier for software. It was easier for hardware, but then because of this narrow range, if you're trying to make 16-bit floating point work for this narrow range, you've got to modify the software so you don't lose-- you don't turn things into the underflow. So there's this idea that Google-- I mean, that InvIT uses called loss scaling, where you have to scale the software to make sure you don't lose those bits so it's extra work. Intel did a recent study that corroborates that benefits. They might not have mentioned that Google invented Brain Float but they said it's-- but they still call it Brain Float to do that. And then, so it's this rare thing in hardware where here's something that's easier for software and saves energy, usually, you have this trade-off. And so this is an idea that has caught on is that Google has had it in data centers for two years, and people have now found out about Brain Float-- it was in TensorFlow so it wasn't like you didn't-- couldn't learn about it. But so now ARM, Intel, and lots of start-ups have announced that in the future they're going to use this non-IEEE format going forward. OK, so that was the supercomputer inter-connect and the supercomputer nodes, and the supercomputer floating point. What about the compiler? So this is all about performance, so you would think-- or I would have thought-- that something based on Python like TensorFlow-- which is a-- it's a domain-specific library on top of Python, that's got to be slow, right? It's Python, right? Python's famously slow. C++ is fast, right? You'd think this was-- how is this going to work? Well, TensorFlow has it's a program that creates these graphs, right? But for these programs, multi-dimensional rays are first-class citizens. So what this means is, first, you can operate a multi-dimensional arrays explicitly in TensorFlow, unlike in C++ you've got these nested loops that are trying-- that are representing some-- and you have to reverse engineer that, and compilers are not good at figuring that out. Secondly, they have explicit analyzable and bounded data access, so you know exactly what's memory being accessed, and that's really hard to figure out in C++, which has pointers and stuff like that. And there's no aliasing, unlike C++. So ironically-- or to my surprise-- TensorFlow is much easier to get performance out for these multi-dimensional arrays-- for low-precision linear algebra than C++ is-- this language that people have used for performance. So they allow the compiler that the supercomputer uses to do all kinds of transformations that would be very hard to do with C++ traditionally. So this compiler is called XLA for accelerated Linear Algebra, and it really-- everything happens in the compiler in this design. It exploits parallelism. There's up to 1,024 chips-- two cores per chip-- it handles that kind of parallelism. They have these 2D vector units which is the new challenge-- so two-dimensional in data, and memory, and in registers, and you operate them and stick them back. And then there's this instruction-level parallelism eight operations, 300 bits, lots of parallelism that it has to be able to exploit. The 2D thing is a new challenge. We've had since Seymour Cray, we've had one-dimensional vector units, we understand that one pretty well. These 2D things are a new challenge, we haven't had computers that have had this before and so it's harder to do. And on top of that, the compiler has to manage all the data transfer. There's no caches. There's no instruction caches, no data cache, so it's up to the compiler to schedule all the operations and to move the data properly in the design, so it's all up to the compiler. Now, there's two-- there's a classic operation in compilers called operator fusion that would spin around forever. And what you try to do is you're trying to-- if you can do two operations back-to-back rather than write it to memory and read it back, let's see if you can put the operations together and skip that. But now you want to do a two-dimensional version of that, that's operator fusion. And then how fast are these things going to improve, right? Is this TensorFlow thing based on Python? Is it going to improve fast? Or It's newer technology and compilers are move faster, and I"ll talk about both of those. So this is the benefit of going from no fusion to fusion for-- benchmarks I'll tell you about later. So it's factors of two or three, and the two exceptions here are this convolutional neural network and ResNet-50 which is also a convolutional network. And why did they stand out? It's because there's an idea in these deep neural network models of what's called skip connections where they skip over a layer, and that works out really well if you do fusion. The other thing is how fast are they getting better. So there's this benchmarking effort called MLPerf that I'll tell you about, and this is the voltage EPU in green for their benchmarks, and here is the-- the TPU's in red, and this is just six months apart. How much faster did it get? In the GPUs improve-- they're a third faster, and the TPUs are a factor two faster, so that's-- that's amazing progress from a compilers perspective. If you tip-- for C++ compilers if you made it 20% faster in the year you'd be a hero, right? So this is tremendous progress because these are much newer stacks. And we found this for the production applications as well, that over time that wasn't as big of numbers but over time they were getting a lot faster. So because it's a newer stack and a newer compiler technology, that looks like there's opportunities-- big opportunities to make improvements. So now that we've finally explained to people what TPUv2 and v3 is, and you've always been able to know what InvIT has been doing the GPUs, we can compare them architecturally-- these architectural philosophies here. So what multi-chip support? Now, that's a first-class citizen. We were building a supercomputer, so the interconnected network is on the chip and they plumbed the ops all through the compiler and the TensorFlow. For NVIDIA, they do it two layers, right? They're accelerators that go into hosts, and then they use standard networking to connect those hosts together like InfiniBands-- and InfiniBands, which are-- so it's two different technologies to put them together. The multiplier are these big systolic arrays for these per chip where they have many small-- they do use systolic arrays in the voltage GPU they're just tiny ones. And then because they use this IEEE floating point they have to compensate with loss scaling and software. We do the two big oxen-- two cores per chip-- and XLA compiler overlay-- handles all the overlays and software. With GPUs they have 80-- on the case of the Volta-- of these multi-threaded cores. They have so many threads they have to have 20 megabytes of registers to keep all the threads alive. And then they've got threading hardware and CUDA coding conventions to have those overlap. And then the memory hierarchy, there's one big software controlled scratch-pad memory for the TPUs that the compiler schedules. And then when it goes to DRAM it sets up what are called the DMA access as the access to DRAM, where it's a mixed-up scheme over for the GPUs, they have both caches and scratch-pad memories, and then they have all this multi-threading stuff, and then they have hardware to unwrap the multi-threading to get sequential performance-- D accesses. OK, so that's high-level comparison of the two. So just quoting some architects at Purdue, they did an op-ed earlier this year, and they said GPUs incur high overhead in performance area energy do the heavy multi-threading which is unnecessary for DNNs which have prefetchable sequential memory accesses. The systolic organization of TPUs captures the data reuse while being simple by avoiding multi-threading, and so I don't disagree with that. I think it's a fine observation. So besides these architecture differences, the technologies that the new companies are using are quite different. You know, Google-- I mean, InvIT has been building chips for a long time, Google is new to the game and they're behind. So it's a huge chip-- what's called the full reticle-- you can't build a bigger chip and it's in a much newer technology. So I tried to adjust if it was in the same technology, how big would the chips would be? It would be less than half the size in the same technology. So that's-- I'm trying to come up with a cost metric so that's the blue bars relative to Volta, then blue and red bars or two generations. Then what about the cost-- the power? And for a 16-chip system, it's a third or a fifth less power. And power is correlated with total cost of ownership so that matters. And then you can rent these things in the cloud at Google and here's the different prices here. So it's not trying to say this is exactly the number, but these-- here these three metrics or costs related are all less for the TPU than it is for the GPU, so I would-- it's certainly a cheaper chip and an older technology. OK, now, computer-- what about performance? This is the bottom line for a lot of architects, that's what I'm trying to do. So computer architects grade on a curve, right? They don't grade on an absolute scale. It's not like OK, if you get 90% of this, it's an eight. No, we-- it's, what's the competition? Because if it's the same era, people have about the same opportunities, so it's not a perfect grading system but that's the way we do it. So I'm going to split this grading on the curve into two pieces; this per chip performance and then the supercomputer performance against Volta. And why do we care about the per chip? Because of it's-- if it's wimpy chips, it's like the oxen, right? If instead of oxen we have cats or something, right? I don't care if 1,024 of them that they're not that fast, so how fast are the chips? And then another question we're a training chip, but what about that-- how well does it do inference? That's for the chip, and then how about the supercomputer? We can put 1,024 together, do we get perfect linear speed-up? Is 1,024 chips 1,024 times faster than one chip? How close? That's the ultimate. And then another measure that the high-performance computing people typically use will multiply the scale-up to how efficiently do you use a chip? And that's a metric that you use because you know these things cost millions of dollars. OK, that's the setup. What are we going to run? On-- we have to have fair benchmarks for this. And so the TPUv1 used Google production Apps which was very realistic, but no one else can see with Google production apps are so that's not very helpful. So I and people from other places started meeting so these companies, Baidu, Google, and Harvard-- and no, Harvard, Stanford, and Berkeley, and those two companies started meeting I guess a year and a half ago to try and come up with some benchmarks. And that has now expanded to 50 companies and eight universities do that, and they did training benchmarks first. They've had two deadlines-- I mentioned those two-- and now the inference deadline was just a few weeks ago and the results will be announced next Wednesday, so that will be interesting to see what happens here. So this is now-- this is now the-- probably the standard way that machine learning is going to be measured. And if you're interested in participating, MLPerf is open and we'd love more people to join and help. To keep the training costs low, they don't take very long to train, you know? Instead of months, it's a day or less to train. So to measure the supercomputers we have to include Google production applications so that's one that's why that's included there. So in terms of the types of applications similar to TPUv1 we have multi-layer perceptions. This is the classic neural network with Texas input it's fully connected, all the inputs go to the next layer. The Convolutional Neural Networks, another classic design for which you know LeCun won the Turing Award, probably a big part for this that's CNN. So this takes images as input, it does this localized calculation for the next layer. And then the Recurrent Neural Networks is where they bring in sequentialities that they used the previous state as well as the inputs to calculate the next one and this is typically used for translation and has text as input here. So those are the-- there's two of each of those for the production applications. So in terms of the single-chip performance at the top is the normal perfect benchmarks this is relative to TPUv2, and if you look, they're pretty comparable. So green is TPUv is the Volta, and red is the TPUv3, so they're both about the same, 1.8 to 1.9 times as fast. Now, for the production applications, it's a very different story, is again the TPUv3 is about 1 times as fast, that's nice, consistent, but the Volta is much slower. So why is that? A big reason is the Google Apps we know now that we do measurements nobody at Google uses fp16 on the production applications, they don't try to use it. And so what I would love to be able to do-- and I'm pretty sure InvIT video won't do that-- is get performance counters in all the Volta GPUs and find how many people are using fp16. You can do it for benchmarks and it is highly motivated to that in benchmarks because they're eight times faster, but in the real world do people use it? Do I-- are there people who are using Voltas here? No, OK, I'd love to know the answer to that because is it really just a really great benchmarking technology, or do people use it in the field? And I don't know the answer to that, but Google certainly doesn't use it. OK, what about the Inference Speed? So this is a response time versus inferences per second so you want to be over here we're low response time in that. So here it is we start off for this particular CNN, example and this is its response time in milliseconds and how many inferences for second was about 50. When we move it over to TPUv2, well, that's you know five times faster in almost one-third of the latency. Now, it's so much faster that this is faster response time than this application would need, so you can increase what's called the batch side which is how many operations-- how many data sets you do at once. And you can make the process more images at the same time so you can kind of double performance again, and the latency goes up a little bit but it's still inside the limit there. And then if you do TPUv3, it's the same inferences per second but you drop down the response time. So inference is the first phase-- the forward through-- so if you do a training chip you can also do inference pretty well. OK, now the supercomputer stuff. So here's the number of chips and here's the response time, and this would be perfect right here. So this is for resonant 50 which is the only one of the MLPerf benchmarks that you can run at 1,000 CPUs-- or GPUs or TPUs. So there's three results. This first one-- the gold line-- is what was published by Google for ResNet50 before the MLPerf competition. The blue line is part of the MLPerf competition so it goes from 3/4 of the chips to half of the chips. So why is that? And it's basically the rules of MLPerf. There's something called evaluation where you want to see what the accurate score is so you run this hold out training set-- most researchers don't include it-- and then MLPerf to deter cheating makes you do an evaluation at the end of it's called evaluation/epoch every time you go through the data once-- and you go through that 10 or 100 times. So nobody would do that but it's a fair thing to do and in the green line is for you for GPUs and they can do 1500 chips at about 40%. So that's it, OK, that's MLPerf so I don't know how important that is. What about the production apps? These are the two, the MLPs, the classic ones, and they're limited by something that's called embeddings. This is about 40% which is not that bad for a supercomputer and this is more like 20%. So embeddings are-- was a kind of, one of the big uses of translation is where you take these words you put them in the vector space and see how close they are and do your calculus there. It's basically very sparse data-- converted dense data-- not every DNN use it but these two use it, and it's a much harder calculation because of doing table look-ups and stuff like that so that was a limit for them. But what about the other four applications? This is to me the startling one. This is the best supercomputer scaling I've ever seen, right? CNN, zero is-- this is AlphaZero-- the thing that was on the cover of Nature magazine-- it won at chess, and Go, and Shoji, and that runs at 96%. And these three which are-- this is another CNN and these are two translate ones, they run it 99%, well, wow, that's amazing. What about super-- traditional supercomputers? At the last MLPerf, the Fujitsu people submitted a result on this supercomputer which has this-- that's how fast it ran, so smaller is better for doing the training. This is how many GPUs and CPUs they use. Now, what they did is to get that result they did some changes that you're not allowed to do and what MLPerf calls the close division. There's a rigid set of rules. There's this other thing called the open division that's more flexible. So this result is in the open division. Had both InvIT and TPU done those same optimizations, they would have run about 10% or 15% faster too. OK now, the next thing I did was well what about traditional supercomputers is something called the Top500, that many-- has been around for decades fastest at the world at running Linpack. Now, what Linpack does is uses what's called weak scaling that is the more chips you have they synthetically create more data to keep all the chips busy. And-- and Linpack's been criticized for that not being very realistic. They can just synthetically make your things bigger, and it does its work in 64-bit and 32-bit floating point. We're going to compare that to using alpha zero on the TPUs. Now, it's using 16 and 32-bit floating point. And then here is the top number four on the Top500, that-- that one. And then here's the number one in the Green500. The Green500 is you divide the FLOPS by the number of watts you use to see who is the most efficient. And then-- and this one's pretty good. Yeah, both of them, the Fujitsu one-- and by the way, Google's not just building one of these TPUs-- these supercomputers, they built a lot of these supercomputers. So if we could actually submit these there would be a lot of spots. So what's the percentage of peak that they're delivering? And here is the TPUv2 and V3 and they're at 70% or 80%. So amazingly, when alpha is-- or for this production application with real-world data, they're at 70% to 80% of their peak which is even higher than running this synthetic Linpack thing which does about 60%. In terms of the top 500, that's 87 PetaFLOPS that was done on TPUv3. And then for the Green500 it's-- factors are 5 or 10 in terms of the performance per watt for these kind of supercomputers. OK. AUDIENCE: [INAUDIBLE] DAVE PATTERSON: The what? Yeah, the legend, OK, blue is TPUv2, red is v3, and then the rest of these are-- this is a classic supercomputer-- the Tianhe-- that was number four there at 61 PetaFLOPS, and then-- and the performance per watt goes way down for that one. And this used to be the record-- or this is the record for Green500. These are much higher in the performance per watt. AUDIENCE: So these FLOPS here are different. DAVE PATTERSON: Right, yeah, this-- yeah, so this is a 64-bit FLOPS, but it's on synthetic data, right? It's weak scaling with sis-- so it's not-- it's not-- it's not-- it's not-- it's nowhere near as a perfect comparison, but it was-- I was interested to see where it would end up on this thing. And to me 84% of peak is amazing at 1,024 chips and-- and that I was just interested in the other ones. So now, let's put in perspective this announcement. So this is Sundar Pichai, the Google CEO, and two-- three and a-- I guess, 3 and 1/2 years ago now, we had this kind of, oh, by the way, I thought you might be interested, we've been running in our data centers this thing called the TPU and it's got an order of magnitude better, you see, this is TPU and this is other, right? And this is at least a factor of 10 bigger than that. And by the way, we did that-- we did that and that's the announcement, right? You just throw that one-slide announcement there. So that-- this was like an earthquake in Silicon Valley. So the way I think of it now is if you know the-- if you know Helen of Troy, she was described as-- a poet described her as the face that launched 1,000 ships. Helen of Troy was stolen from Sparta and that led to the Trojan War and stuff like that, right? So this is the TPUv1 is the launching of 1,000 ships, right? So what happens next is DNA-- Intel starts acquiring companies, Nervana, half a-- almost a half a million dollars there, and then Movidius, another half a billion, and then $15 billion, and these are months apart you know? And then Alibaba and Amazon have both-- within the last 12 months or so-- announced their own versions of TPUv1, they announced their own custom inference chips. And then there's at least 100 start-ups who are taking venture capital money and investing in this space. And all the ideas that didn't work for general-purpose computers are being resuscitated. So an idea called Dataflow architecture-- and not the classic Von Neumann one-- there's companies that's doing that. Coarse-Grained Reconfigurable, that's the next generation of FPGAs, there's a company that started doing that. Asynchronous, "why do you spend all your time with a clock, that's wasting power, let's get rid of the clock," there's a company doing that. Analog computing, "why are you limited to digital?" "Well, let's do the multiplies and analog and convert them back to forces a company." And I think in every of these cases there's multiple companies doing these things, but those are the ones I know of. And then the most way out there one-- and I'm rooting for them to be successful-- is called Wafer Scale Computing. So a wafer is the size of a big Frisbee. Today in conventional wisdom, you chop it up into dye, you put it into packages, put them on print circuit boards, and there's this question, why do you do that? Why don't you just make the computer out of the wafer? That's what they're doing. So they talked about the physical-- so they have a wafer it gets very hot so it expands, so they have to have a cooling system with water or some liquid that sucks the-- [INAUDIBLE] out and imagine all the things that might go wrong with that. You're a startup company and you're betting that you can do wafer-scale integration which many bold people have failed at, and they're going to do that. So this is the most-- this is surely the most diverse time in computer architecture-- there's been no era where at the same time, there's all these people trying commercially to make these technologies work because of the excitement of machine learning. Only five-- most people are doing inference. Most of those start-ups will do that, only five of them that-- I think there's only five that are doing training that I know of, and none of them have submitted performance results yet. So it's a very exciting time. We don't know quite what's going to happen. If none of them-- if they never submit MLPerf results we'll be suspicious but. All right, so now I got my last two slides here. So TPUv1, as far as I can tell, had Google not made that announcement or if they made the announcement a year later, everything would be a year later, all right? If everybody who did a start-- people would use that to recruit people, people used that announcement to get funding, it was-- I don't know why they made the decision, but it was-- it changed history. This chip which does 90 tera operations at 75 watts is still pretty impressive, and it's four years old, and it's still being used at Google. Probably it's about-- TPUv3 is about the same speed of Volta per chip, but it's got this much better scaling. I can't-- still can't believe the thing's at 99% for production applications. Much-- different floating point arithmetic but pretty high performance per watt. I think it was the-- the things I tell you about-- the interconnect, the systolic arrays, the bfloat16 and being dedicated for DNNs, and it's an older technology. Smaller dyes and a little technology which makes it look cluttered. So thinking about-- and this is my last slide before I-- oh my, last slide-- it's-- these DNN models are improving really fast, long short-term memory's been around for a while, then two years ago it was Transformer that everybody was doing and now it's BERT, a version of Transformer that's got 1700 citations in 12 months, all right? What an amazing fast field. Running these things well not only is interesting, it actually has huge commercial impact. There's more-- clearly more open problems with DSAs than general-purpose computing-- we-- it's really-- I don't know what the ideas are to make general-purpose computing run faster, but there's all kinds of things that might work for DSAs, and you can have bigger impacts with that way. What do you-- to work-- to be a person like me to work in this area, you've got to learn more-- you got to-- it used to be not just some simulator inspect benchmarks, you got to know much more than stack which it's interesting to learn that stuff. And so if you like stuff like this, send me email, all right? With that, I'll open for questions. [APPLAUSE] OK, Mike, Michael, I know the name of everybody here so I'll call by name, Michael. AUDIENCE: So you mentioned early in your talk the 12 months of training time and that-- DAVE PATTERSON: Oh, 12 on one chip. AUDIENCE: Yeah, and alluded to how you had to scale it. I was wondering how do you partition training [INAUDIBLE] 1,000 chips? DAVE PATTERSON: Yeah, so there's-- the two ways are called data parallelism-- ED LAZOWSKA: Repeat the question. DAVE PATTERSON: How do you-- how do you partition over 1,024 chips? So the two ways of doing it are called data parallelism and model parallelism. This is just data parallelism, so there's just enough data that you can just keep all those chips busy. We haven't had to go to a model parallel so much more classical supercomputer parallelism, where we have to divide up the-- divide up the computation on another axis. AUDIENCE: Do you have to repeat the batch size to do-- to get that parallelism? DAVE PATTERSON: Yeah, so what batch size is is the number of the things that you operate at the same time. And since you-- if you ask, I have-- I knew I had a long talk, but if you ask about batch size-- here you go-- So my colleagues at Google did this paper, Shallue et al, and they looked at how much benefit you could get from increasing the batch size and so you get good accuracy scores because there was questions about that. And it's a 50-page paper where they did it. But they found basically three regions, perfect scaling where you double the batch size the number of steps go in half, diminishing returns where you double the batch size and the number of steps go down but it doesn't go-- it's not by a factor of two, and then no more benefit. And the batch sizes are really big, so if you-- if you want to do what Google is doing, you won't go-- and you want to have really big batch sizes to keep your machine full, you like this idea that, wow, batch sizes of 8K to 256 is perfect, and you still can get benefits at 4K to 65K. So this is-- this is one of the nice things about being at Google is that people do work research like that. Yeah. AUDIENCE: Yeah, so I have a follow-up question then another question. So the follow-up here is that if you decrease the-- if you want to have a smaller batch size because then you have better statistical efficiency, right? DAVE PATTERSON: Then you have better? AUDIENCE: Better statistical efficiency, just to get better training with smaller batches. DAVE PATTERSON: No, these results are-- well, these results show you can get exact-- without hurting the quality score you can have bigger batch size, that's what their results. AUDIENCE: So I guess the question is how do you keep up with network bandwidth, right? Because eventually this problem-- aren't you going to be limited in other bandwidth [INAUDIBLE] the models and then send the updates for all of the other nodes? DAVE PATTERSON: Yeah, so the way-- I think the way it works is the bigger the batch size the less you have to communicate, and the smaller the batch size the more you have to communicate. So for the Cerberus thing-- which has no DRAM on it-- they have-- there are small-batch size signs, so they're going to do a lot more communication but everything's on the wafer, that's their bet. AUDIENCE: Right, OK, well here's the question I really wanted to ask, was-- OK, so you talked about-- DAVE PATTERSON: Nope, no, no, that is not OK. AUDIENCE: Well, you-- ED LAZOWSKA: Yeah, go ahead. AUDIENCE: You've been teaching architecture for a long time, right? And now we know architecture is just getting popular again because-- DAVE PATTERSON: So I missed-- I missed my chance or no? AUDIENCE: So how do you think we should deal with needing more and more people to understand hardware and design domain-specific architectures? Because it seems to be the way you're going to rely on-- the way you're going to rely on to get better and better performance. So how do we-- DAVE PATTERSON: I think we should hire more computer architecture professors. Yeah, I would say. AUDIENCE: It's not sufficient, right? So-- DAVE PATTERSON: I was talking to Magda earlier, and what an exciting field we are in, is that it seems like every couple of years you go, wow, things are really exciting now. I think that is not uniform across all disciplines, right? I think within computer science, there are fields that-- without naming names-- boy, things are polishing a pretty smooth stone, right? And I think we can say there were periods in computer architecture that were-- so there were decades in there like, wow, this is pretty boring stuff here. Why did I get-- why did I want to work on this area? This is not one of those times. Right now this is a very exciting time in architecture. So I think in computer science, my guess would be we haven't had a decade that was boring for everybody, I don't think, but maybe-- maybe there was and that's when computer science enrolments plummeted, but this is not one of those times. Machine learning is highly exciting. The ending of Moore's Law and the-- these ravenous ML colleagues who need every cycle they can get. What-- they need Moore's Law to continue and it's not, and so this is an exciting time for us. So my guess is, within computer science, there are these things that go up and down in excitement and we've got-- it is a very exciting era right now. I would-- yeah, I think it's great that computer architecture is not everybody doing branch predictors, right? I think this is-- I think this great. AUDIENCE: [INAUDIBLE] designing hardware. I mean, so the tools despite the need, they're super primitive, we can't teach Verilog to everyone because it's too boring. DAVE PATTERSON: Verilog is awful. Some of us-- some of us have worked on-- some of us have had colleagues that have made more-- more modern-- taking advantage of more modern programming languages ideas to improve that. I can tell you the people at Berkeley who have learned Chisel, which is an example that, they're never going back. They're never going to start writing Verilog again. There's practical reasons to write Verilog because of all of these hardware engineers that don't know modern programming language ideas but we're researchers, right? So I think it will-- I think it's going to be hard to get [INAUDIBLE] students excited if what they have to do is do things in Verilog. I think-- so I think-- I don't know what the right solution but something that-- they're comfortable with more modern programming language concepts having Python as their first language, and so we need to do something to raise that level of innovation. But and I think we also-- this idea that you guys are doing is building chips is exciting, right? And it's the fact that you can get chips back. That's the thing that we have as architects over software people. Their stuff is all in their head, right? Show me your program, well, its bits somewhere, right? Or here is a listing, right? Oh well, that's very exciting. We can-- there's chips-- you can give-- you can give earrings to your mom, and bracelets, and it's exciting. The chip comes back, does it work? Power and stuff, it's-- we're creating physical objects, so we've got something going for us too and we need innovation now. I mean, I think how much of the economy is based on computers getting faster every year, right? People are counting on that and where is that going to come from, right? It's going to it's going to be heterogeneity of design, and so we're going to-- it's-- it's-- this is going to be upon us too. We're being called on and we're going to have to deliver. It's a great time. ED LAZOWSKA: I wanted to ask you a final question, and there is a reception outside. AUDIENCE: OK, well, I'm going to-- I'm going to-- ED LAZOWSKA: Wait, here's my final question. You flashed by this Life Lesson slide. DAVE PATTERSON: OK, I'll do that. ED LAZOWSKA: And close a talk with-- as the oldest person in the room I think you should give us life lessons. DAVE PATTERSON: Well, one of the things as an academic is some of my colleagues have been reluctant to give advice and I love it. I love giving advice. I'd love the title advisor, I'll tell you what you should do with your life, sure. And now that I've-- now that I've-- I've had a 40-year career, and maybe because now that I have the Turing Award, people care what I think now. So all right so I'm-- ED LAZOWSKA: You're even more dangerous. DAVE PATTERSON: All right, yes, yes, so they might listen to me now. So for me, what worked well for me was I maximized personal happiness versus personal wealth. I think you grow up in America, you think those are the same thing, they're not. There are very wealthy unhappy people in this country like our president. So and I don't feel-- and looking back I'm like, here, well, what's wrong with personal happiness as a goal as opposed to? I can't figure out what's wrong with that but think about that, those aren't the same thing. Family first. I think, especially today, but it was even true when Ed and I were young. As a professor, there's all these uses demands on your time it's very easy to put your family as lower priority because you want to make other people happy so I didn't do that. I had time to think about it, it was going to be family first so I was-- there was-- I have two sons so it was Indian Guides, and Cub Scout leader, and soccer coach, field trips, and all that stuff. And so when your adorable young sons become rebellious teenagers, "dad, you were never here." "Indian Guides, Cub Scouts," it was very satisfying to me-- all right, all right, And you really-- you really-- I had a colleague who told me this late one night, you really don't want to say what he said to me, "if I had it to do over again, I wish I had spent more time with my kids." You do not want to ever have that thought in your time. For those of us-- for Ed Lazowska, at a certain age there's a song that went out that would just send shudders through the heart of fathers and it's called Cat's in the Cradle. And it's a talk-- if you want to-- if you want-- it was actually a poem that his wife gave him to slap him around and he put it to music. But it talks about how you miss these opportunities with your son, and well, I'll-- I'll do it later, I'll do it later. And then you're older and then he-- and you say, hey, son, let's get together, and he said, well, I'll see if I can find time for you dad, and it's like, whoa, this could happen, right? So passion and courage. I like to think I'm an intellectual, logical person but I'm not. I have to really believe what we're doing and swing for the fences. We try and hit home runs, we don't bunt for singles. The courage part of it is-- as I told you guys last time I was here-- I was a wrestler in high school and college, so ironically, the physical courage from wrestling is intellectual courage somehow in me, and I feel like if something's wrong I have stand up. So during the George W. Bush administration, there was a guy in charge of DARPA who I thought was screwing up and Ed thought was screwing up, and wasn't funding computer science like DARPA used to at the time. So he and I thought we should stand up to this guy and no one else wanted to do that. But at that-- so we, I think we're both glad we did it, but there were some costs to that. This is advice I got from a senior colleague. [LAUGHTER] And I think he was-- given. I was passionate and courage, I think he was warning me to be careful. And I've had a career to look back on and this is true. Friends come and go like people you went to school with, you lose touch. Enemies never forget-- they don't forget they're your enemy. So I think-- as far as I know-- I have two enemies. One of them is that guy who-- at DARPA and then another guy in my field. But-- but I feel comfortable with the decisions that made them unhappy with me. But think about it, right? When you make an enemy it's an enemy for life. I've in my DNA believe in winning as a team versus as an individual. I really want my team to win. Quoting Fred Brooks, one of our heroes, says, there's no losers on a winning team and no winners on a losing team. So no matter what you do in a winning team you're a winner. And I just-- I believe in that. I think it came from for me for wrestling. I had wrestling coaches in high school and college who believed that if we bonded as a team, we'd move successfully, even though it's an individual sport, and that's-- I absolutely think that's the way the world works. But I think if you had other analogies like drama or orchestra, working as a team is good. I think I'm good at seeking honest feedback and learning from it, especially if it's an enemy, you send them your paper, they're going to tell you what they think, and you want to hear that early on. It's easy to avoid feedback, it's easy to avoid. So there's a corollary to that as I reflect on my career is-- is somebody who thinks they're the smartest person in the room. And in my career, there's a guy in my field who thinks he's the smartest person in the room and he's not a very popular guy, there was an Ivy League president who thought he's the smartest person room and he was fired, there's the founders of Enron, they're convicted felons. And amazingly enough, my ex-brother-in-law's brother is a convicted felon. And all these guys thought they were the smartest in the room. Well, was the flaw they weren't that smart? No, I think they were smart. I think the flaw is if you're thinking you're the smartest person in the room and you have an idea, why ask anybody else what your idea is because you're smarter than everybody else, so I'll just go ahead and do it, I know what I'm doing, right? And I think you make these terrible mistakes where you go to jail or you get fired. But in any case, for younger people, it just-- if somebody-- you hear somebody thinks they're the smartest in the room, just run away. One big thing at a time I still can remember waking up on a bright morning and it was as if God spoke to me maybe 10 or so years into my career, and what he said was, it's not how many projects you start, it's how many you finish. And it seems obvious but I was just struck with that. As a professor you kind of volunteer for a lot of things and it's up to you to control it, and after that hit me, I did one big thing at a time. So when Hennessy and I wrote a textbook that was the one big thing. I do other little things but that was the major thing you did. And so to get those done. And when I was president of ECM, that was the big thing I did. When I was the chair of the Richard Tapia conference that helped that get going, that was a big thing. So I did one thing at a time. But if you-- if you do one big thing at a time for 30 years you get a lot of things done, right? And for a lot of people, they just try to do too much and they can't get them done, so I would think about that one. And then the final thing is I'm a natural-born optimist. And this next-- I have one more slide, and this is the most important slide of the day. You-- this is something that you will remember-- the next slide. But to set this up-- I'm a natural-born optimist-- an example is when I was 16, I was dating an attractive girl and at the time I asked her to be exclusive. And we'd only gotten a couple of dates and she dated guys older guys and she was only 16. And she said to me-- I asked her, would you go steady? Is the phrase we used, and she said, well, Dave, I don't know how to say no. So me as an optimist and a logical person, I said let's go, great, and I hugged her. And so she said-- in her mind she said, well I'll let this guy down later, but we've been married 52 years. She may still let me down but that hasn't happened yet. But if you've been married 52 years people ask, what's your secret for a long marriage, and so I'm going to tell you the secret. So this is the nine magic words, and it's not that hard, right? It's nine words, it starts I, you, I, all right. And this is for both partners here, OK? So the nine magic words, I was wrong, you were right, I love you, right? And those are the nine magic words for a long happy relationship. There's no substitutions. I-- I was wrong. You're a jerk, I hate you, that doesn't work. You've got to say all the words. And it's not just for one gender here, not just for one sex, it's for both of these and both have to say it. OK, with that I'm done. [APPLAUSE] ED LAZOWSKA: Join us outside at a reception. 