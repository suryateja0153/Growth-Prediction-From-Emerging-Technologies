 (logo whooshing) (chiming music) (gentle music) - So today I'll talk about the development of implantable interfaces to restore motor function. And in the first part, I just want to provide grounded in sort of work that's been going on for a long. You know, some of this sounds very modern concepts, things that may have only happened this decade, but in reality these ideas and the research underlying this goes back many, many decades. And, for example, here's a magazine cover from 1957, where there's already talk about radio to control thoughts, and so there's a long history of people thinking about implantable devices that might control movement, control disease states. And this type of research has really been very successful for clinical diseases as well, and a couple examples include cochlear implants. Some of this work was actually done at UCSF, some of the seminal work that resulted in translational devices. And this, as you probably well know, allows both adults and children with certain hearing loss to have restoration of hearing. There's also deep brain stimulation which has been remarkably effective for Parkinson's disease, for movement disorders. It's now being tested in psychiatric illnesses. There's epilepsy devices, so there's a long research history as well as successful translation. And what I'll talk about today are brain-computer interfaces, another term is brain-machine interfaces. And the the hope is to use this technology to improve motor function, and in some ways it is a natural transition from a lot of these previous work, and at a high level, a big part of this where these are devices that stimulate, you know, what we'd call open looping, just stimulating without responsive to the environment, and a big part of the brain-machine interfaces are to record lots of neural activity. And as I'll show you, be able to really decode intention and allow complex control. And so I have this broken up in four sections, so I'll just talk about the, kinda the clinical problem that's being addressed through this research, and then talking about some of the basic science and clinical work, and then just talk towards the end about the work we're doing at UCSF, and talk about next steps. And so in terms of the overall goal, it is to address this really large level of motor disability in the U.S., and this is really an unmet need. And this graph was actually produced by the Christopher and Dana Reeve Foundation, and it's one of these really impressive surveys where they were, you know, surveyed tens of thousands of people, and estimated that in the U.S. alone, there's about 5.5 million patients with paralysis any given time. And from a rehab perspective, these are, you know, not disease, these are disease independent, right? So if you look at this pie chart, it can be stroke, it can be cerebral palsy, things at birth, TBI, spinal cord injury. So regardless of the disorder, there's a very high level of prevalence, and at this stage there's really not a treatment to improve their, to restore, let's say, reaching, or grasping, or restore basic functions. There's, of course, things that one can do to improve their quality of life, and to exist in your rehabilitation. But this hopes to try to give more complex control, to try, at least in the longer term, to restore some functionality. And an important part to consider, you know, as I'll go through this, you'll probably seem you know, I'll show you some videos, and it's worth thinking about towards the end, about thinking about the future steps, which is the customization, right? And no two patients are the same, no two diseases are the same, their deficits aren't the same, and I just wanted to highlight this at the beginning, that important part of the next stage of development is the customization. And this relative slide I just wanted to highlight some of the potential needs of patients, right? So if you look at this upper category, if their patients are locked-in, right? So these are patients from a variety of disorders, ALS or Lou Gehrig's disease, a brain stem stroke, or a severe TBI, these may be patients who are completely unable to move, and often they are unable to communicate, and they have some residual maybe movements of eyes, maybe blink, so very limited ability to really communicate. And so for them, a communication interface is of paramount, and as I'll show you, some of this is, some of the work right now is moving towards trying to allow very fast control of a computer cursor, or a computer through mind control, right? So this allows someone who's in a locked-in state to interact with their environment. And then if you move down this relative scale here, there are a fair number of patients. We take care of them, take care of them at UCSF. They may be tetraplegic or quadriplegic, and, you know, depending their exact level, there's a very high need for, let's say, upper limb function and then maybe below there's different needs. And then someone with, for example, stroke, who may have unilateral limb paralysis would require a very different approach, right? So this is just to make a statement that as I present, this is worth keeping this mind, this picture in your mind of the need for customization to patient's needs in the long term. And just along this line, I wanted to present work that has been done, that tries to assess patients goals and needs, and this has been an important driver of this field. So for example, this was a paper from Kim Anderson, et al, who looked at spinal cord injury patients, and these patients were tetraplegic, so they had the TIA spinal cord injury at upper level C5 or above, so they couldn't move their arms or legs. And, for example, for them, in this survey, about 50% implied that arm and hand function's really critical, so that's, as you'll see, a big part of the field is trying to look at upper limb, restoration of upper limb dexterity based on these patient needs. Another part that you might see if you lump all the next three categories, it's really about bowel and bladder function. So there is, I won't talk about it today, but there is ongoing research in allowing independent bladder control, and so forth. And so, and this has been an important evolution of the field, to try to take into account patient goals and needs, not just to build fancy gadgets, but trying to customize this and make them work for individuals. So then, you know, moving towards more, okay, what is a brain-computer interface, and what are the essential goals? And this is just a high level conceptual view of, like, you know, why might this work, right? So if you look at this, here's a cartoon view of someone with a spinal cord injury, and the main problem here is, of course, that the signals from the brain aren't able to descend, right? So if you look at the anatomy, the brain has motor areas, and I'll go into a little bit more, there's, you know, numerous motor areas that help you look at the environment, know exactly what type of grasp or grip to pick in order to interact with an object. And so someone with spinal cord injury, all of that's intact, right, the vision's intact, they can think about it, they can, in their mind's eye, think about movement, it's just those signals don't descend properly because the nerves are cut off. And there's also, of course, a loss of sensory feedback, so they can't feel things, or many of them can't feel, some of them may have motor deficits, but feeling is intact. And so the idea here is that if the brain can, if the individual can think about this, imagine this, perhaps there's a way to tap into that, and to restore function. And here's a more detailed version of this, right? So these are the, you know, some of the components that are really important for a brain-computer interface. And in this schematic, probably the most important thing is the neural signal recordings, right? And so here, the goal would be for someone to imagine moving left, or think about moving left, and the goal would be to record signals that would reflect that intention, and these neural signals could be non-invasive, and I'll talk about that a little bit, so it could be EEG or scalp recordings. But, you know, most of the work, as I'll talk about here, about invasive recording, so these are electrodes that are sitting in the brain that can record action potentials, or I'll talk about a little bit more, field potential. So this would be, for example, the forest, this would be the more the forest, this is the trees, individual trees within this entire brain network. And so the idea is to record these signals and then decode this intention to move, and then allow assistive device control. So an important part of this is this, the real-time analysis. So this module, if you will, would take the neural signals and I'll talk a little bit about the algorithms, we'll create, using algorithms, we'll take these neural signals and convert them to control signals so they can control assistive devices. And, you know, in line of what I was describing about patient needs, a focus has been on computer cursor controls. This would allow someone who can't move to control a cursor, in order to interact with the computer, and that opens up the entire internet, and that obviously has benefits for communication. It can also be for artificial limbs, to allow restoration of reaching or grasping. And I want to highlight that the feedback is a real key part of this, and I'll talk about this in a little bit more detail. And this feedback is proving to be really important for stable control for learning. If you think about, you know, if you were really not able to see your hand move, you'd probably introduce a lot of errors, you'd have a hard time correcting, and so most of the things I'll show you are about, are based on visual feedback. So subject is able to see the intended output, right? So if the individual thought about moving left, the question is, did the cursor go to their left, or did they make an error? And that feedback over time allows for correction and for learning. And I won't talk about this, but there's also research into potentially creating artificial touch, and these would be, you know, and that might be really important for fine dexterity, right. If you wanted to pick up a penny, we know that it's, that the ability to detect that contact and to really manipulate, it's really important. And so most of what I'll talk about is, are the neural signals the analysis some aspects of the device control, and then visual feedback to illustrate proof of concepts of where this, you know, the state of the art now, and then try to highlight what are potential translational pads. For those who may not be as familiar, I had also there's this slide here to explain a little bit more about recording of neural activity from the brain, and to explain the two types of signals. So here, it's just a hypothetical, you know, part of the brain with a lot of neurons, and, you know, most of the research that I'll describe are in cortex, so in this rim of tissue around the brain, and so these tend to have high density of neurons, and the two approaches I'll talk about are either sharp electrodes, so these would be tiny, almost like hair-like 35 microns that are inserted into the tissue, and they tend to settle around neuron dense areas, just they're lured towards areas with lots of neurons. And so for example, when electrode is placed here, after the tissue heals a bit, maybe a week or so, and that, you know, if this green neuron started to fire, these are what action potentials look like. So these are literally a neuron firing an action potential. These are, you know, and these are almost, you can think of 'em as binary signals. You know, obviously, neurons communicate through both electrical and chemicals, right, but brain-machine interfaces are only based on electrical signals. That's what we can detect. And it's worth pointing out that these are tiny, tiny signals. So we have to amplify them about 100,000 times. So these are very, very tiny. And so we spend a lot of time in the algorithms, denoising, trying to track these neurons, and ultimately, you know, this is an example of the exact signals that comes out of the brain. And in this case, these are six neurons that were identified, right. So each of these color-coded signals actually came from a neuron through a recording, and just gives you a sense of the signal-to-noise, right, but good high-quality recordings. You can really just, it, you know, it's like a binary, it's like a digital signal from nothing to something, so it's very high signal-to-noise, and it can give you really fine resolution, right. A given cell may be five microns, maybe ten microns, depending on the neuron type, and so you're able to record through these, you know, large networks, and then, you know, get to a single neuron. The alternate approach is something called electrocorticography, and I just tried to draw it to scale where imagine it's a bigger disc, right. So this disc, because it's so big relative to neurons, end up averaging a lot of this. So, you know, and so what we record here are field potentials. So this envelope, this red envelope would be a field potential that would, you know, kind of typify what a field potential looks like, and it's really recording, you know, and I tried to draw this here, which is the, this is all the neurons firing together. So this is really an aggregate measure. And so one way to think about this, this is really the forest, right, and these are the trees, so you can either record at the single high resolution point, or you can record at this more aggregate view, and I'll talk about this later, but it's, you know, it's amazing how much information's here. It is also amazing how much information's at the field potential level. And both may be useful, and then to have strengths and benefits from an interface perspective. And here's an example of the signals, right? So again, you know, it's much more noisy, and this is, you know, the thick line is a processed version of the signal, right? So when we apply signal processing, we can extract what looks like a much cleaner signal. The underlying signal is all the wiggles around that, right, so it is a much noisier signal, but with everything we know about the nervous system, we'll extract features that are meaningful. And so just gives you a sense of the exact neurons. And so when I talk about spike-based recordings, it's really these. When I talk about field potentials or ECoG, it's really these types of signals, just to keep in your mind's eye if, you know, what type of, you know, signal to noise and why there's certainly more information here, but maybe this is more stable, right, because it's trying to record from lots of neurons at the same time. And just to kind of dive into a little bit more, and to show you the scales of this, as I mentioned that, you know, neurons are very tiny, right, maybe 10 microns, five microns, and this, and usually when we put a wire in the brain, and so these are just the types of it, so this is what the microwire looks like, about to go into the brain, and so it's really recording from a small volume, whereas ECoG tend to record from, you know, about 10 times as big an area, maybe bigger, and then EEG, and so the EEG is, of course, above the scalp. And so anything that's, you know, and so you could imagine between ECoG and EEG there's a skull, there's a lot of layers, and so there's, it's a much more filtered signal, much more spatially diffuse. And, you know, and that's partly, I won't really talk about this here, but just to mention that the signals that one gets from electrodes sitting in the brain are necessarily going to be much higher bandwidth, there's a lot more signal and much smaller resolution compared to something like a EEG. And I just, I had this one last slide, just to give you a flavor of these different resolutions. So an important part of neuroscience research, generally, is to understand this, how do different scales of activity really coordinate together to result in neural functions? And here's a nice example from the Buzsaki Lab, showing simultaneously EEG, so this is from a central EEG scalp recording, and these are the field potentials, and these other hashes here are single neurons. And so you can sort of get a sense of the EEG, you know, you have a much more, sort of a low-pass filtered, small amplitude, right? So you have small wiggles that really reflect these much richer signals. And so, for what I'll talk about from the remaining part are these invasive approaches, but I wanted to highlight that there are, there is ongoing research using EEG non-invasive approaches, and this is still at the research level, but the hypothesis, and what I'll show you is that the amount of information you can get out of this is a lot more, and this, so far, has resulted in more sophisticated control and more reliable control compared to EEG. So that section was just more about like neural signals, and really trying to distinguish spiking activity from ECoG. This next section goes into describing what underlies brain-computer, BCI control. And, you know, this is probably recognized in the field as the seminal paper, and it's, you know, if you look at the date, 1969 was the, sort of the paper that really launched the field in retrospect. And this was work done by Ed Fetz, who's still a active professor at University of Washington in Seattle, and, you know, back then, and this is, it's an amazing solo author paper, just himself doing the research, publishing by himself. And he put electrodes into a monkey motor cortex in M1. This is the part of the brain that really coordinates all the downstream, the muscles, the basal ganglia to help you move in a reliable coordinated way, and these are, again, are the single neurons recorded from M1. And what he showed was that these animals can learn to control their brain activity without moving. And, you know, back in those days there was this notion that motor cortex was just this labeled line. Motor cortex would burst, and then the muscles would move. And it really showed that there's these rich dynamics that the brain can be very, very active, the animals could learn to control their brain activity without moving a muscle. And it suggested this idea that the brain, and the muscles, and movement are completely dissociable, which in the modern world doesn't seem surprising, but, you know, if you trace back to history that this was really a profound idea. And this really just sort of set the motion of the notion that, you know, someone with paralysis who may not be able to move a muscle may be still able to change brain states and control states. And, you know, this is just a slide to pay homage to all the basic work that underlies the work that we do now in brain-computer interface. And this is work that's been done for many, over 50 years, trying to map out the networks for skilled control, right? So if you think about moving, there's a lot of processes that go into that, especially like visually-guided movement. If you see a cup, you know, your brain instantly recognizes you have to pick it up this way. If it's a jar of milk, you pick it up with a different grip. So there's this very sophisticated circuitry that helps you look at your environment and rapidly select between what you need to do actions and so forth. And I just wanted to highlight that, you know, even though most of the research in brain-computer interfaces are focused on primary motor cortex, which is a spinal brain output that goes to the rest of the subcortical areas, or the downstream areas, it's really part of this very vast network that's helping it shape and understand its environment, and know when to react. And it's worth just remembering that even though, you know, we are recording from a few neurons, and a lot of the sophisticated control that I show you are based on maybe 20 to 100 neurons, but it works only because it's part of this vast network that's supporting it, allowing it to shape activity very, very reliably. And so it's just to point out that this basic research was very, very critical to these developments. And then I'll kinda go through some of these, the papers that helped shape this work in brain-computer interfaces. And about, in 2000, about 20 years ago now, was another important paper that really highlighted the ability to record multiple areas, and in real time decode activity. And so prior to this, just the computational power wasn't there, you know, the folks are recording a few neurons at a time, and what the Nicolelis Lab here did, was really record from multiple areas within the motor network, and showed the ability to record these in real time, and really process them in real time. And part of this was just the ability of, you know, for a computational, and the methodology to record all these signals, amplify 'em with millisecond precision, all sort of arrived at the same time, but also was a tour de force work, where, that animals making movements, and they were able to decode the reaching and grasping in very fine resolution. And so for example, you know, and I just want to show, and show this figure with raw data. So here's the arm you can imagine moving up and down, and you can, you know, if you look by eye, these are neural activity, and maybe you could say sometimes I see something that tracks, but it's really hard to, by eye, find the patterns in there, right? But then using either, you know, very, very simple algorithms or more complicated algorithms, they were able to show that they could predict activity. So this is the animal's hand moving up and down. This is the animal's arm and hand moving in 3D space, doing a dexterous task. And they could track the arm, right? So that's a important principle to show that by recording from these parts of the network, this is possible, and I think another important principle is relatively simple algorithms can do this. So I won't go into the details, but essentially what this linear prediction is just a regression model. Fairly, you know, that's been around for, you know, many, many decades, idea of a linear regression. And so, and a lot of the subsequent work in brain-computer interfaces have probably benefit from the fact that simple transforms can allow animals, and then also humans to control these assistive devices. And not to say that more complicated algorithms can't work, but I think from an initial phase, the fact that simple decoders can work has also been a powerful way to make progress. And the next part, so I'll just show you some aspects of what does it actually mean to decode, and what is the types of signals. And here's an example of, you know, what the channels look like. So this is a recording from multiple brain areas, motor cortex, premotor cortex, the other hemisphere, and this is, again, things that I've described. These are single neurons, right? These are firing. And the algorithms that we have can detect whether it's neuron I or neuron II, right. So this was automatically labeled green and yellow, right. So there's a lot of algorithm that goes into tracking these neurons to make sure that we can tell which neuron from another, in this case there are three. And these are all recorded simultaneously. And here's an example of the animal making movements and then these are the neural recordings, and in this case, this is work that I did a number of years ago, where you could say, okay, with a relatively small number of neurons you can start to predict the hand movements. And so here is an example, an animal making center-out movement. So the actual white is the animals arm making these movements, and it's just, it's trained to make these center-out movements to eight targets. And the red is a prediction from their brain activity. And you might say, "Well, that's pretty good." And I think I wanted to point out that there's an error, right, there's a consistent offset. It's good, but not perfect. And that's been an important part of BCI that, you know, that there is a error that the animal has to learn something to get better, and to get proficient. And so this is a statement that, you know, and I want to make this, this is open-loop, right, this is the animal making movements. It has no idea that we are taking this cursor and plotting it, right? This is just a offline analysis of their brain activity. What the brain-computer interface is about is closed-loop online, meaning all of a sudden it's no longer moving its hand, it's trying to move this red cursor. So here, it was the arm moving, and us saying, well let's record activity, let's predict what the arm is doing, versus here, now the brain activity directly controls the the red cursor. And that gets to this idea of closed loop control, right? So this is where the animal has feedback. It's that error, that little error between what the white was and the red really would matter, right, because that would mean the cursor wouldn't get to the goal. And these are, and this is just to highlight that again, where we can record brain signals, whether it's these, you know, bed of nails, which are these arrays that are sitting in cortex, or these electrocorticography, these disc electrodes, these signals are processed, and these algorithms go into these actuators I've described, but importantly there's this feedback, right? So subject has to learn and compensate for these errors, and, as I'll show you, this is an important limitation of current approaches, and some of the research we're doing is trying to understand how to really improve feedback, and how to use long, how to enable long term plasticity in order for reliable control. And I wanted to highlight this. This is, again, work that I did a number of years ago, trying to highlight this idea of maps, right? So this is again the animal, the same animal who's doing the control. Now imagine this, you know, this is white, but imagine this is a red cursor, now the animal's no longer moving, and has to control this cursor with brain activity. And, you know, and so what this is is 19 days of control. So every day the animal would come back and try to do the center-out task without moving, only by moving it's brain, changing its brain states in order to make these movements. And in the first few days, there's a lot of mistakes. And what I'm trying to show you is here, the first five minutes of plugging the animal in, right? So the first five minutes, there's a lot of adjustments, and then, you know, over the hour session you can see it kinda gets better. On day four he gets plugged in, makes mistakes, and gets better. What's interesting is, if you look at day six, eight, and 10, he's ready to go from the outset, right? And this is something that we call, essentially, a plug-and-play, right? So with learning and plasticity, that this type of control can become, basically it can, like a natural movement, there's no need for training anymore. And I'll show you, towards the very end, how this principle has been important for what we're doing at UCSF, we're translating neuroprosthetics, or BCIs to patients to make it more reliable. And, you know, of course what's an important part of this is to really understand why does this happen, right? So why is initially the animal making lots of mistakes and has to learn every day, whereas with time, it becomes really the system in which the moment, for example, here, day six onwards, the moment he's plugged in is quite good. And it really goes towards brain plasticity, right? So in this system we're able to track the neurons with learning, and what's interesting is if we look at these early periods, the brain map is just really quite unstable. Even though these are the same neurons, the animal hasn't really figured out a strategy to reliably do this. Whereas, later in the time, there's a very fixed transform. So there's essentially what we would call a fixed map, where the moment we plug him in, these neurons have certain tuning properties that just get locked right in. And so it's essentially a form of, you know, adult plasticity, in which these neurons can change their properties and tuning in order to become really stable. And here's another view of that, right? So this is the performance curve, so this is the animal over 19 days, learning the control, and these red curves are the actual changes in the map properties. And what this shows is that during unstable performance, the map is stable. When the performance gets really rock solid and stable across days, the map is really crystallized and stable, it's just to highlight, and again, I'm, you know, I'm spending a little bit of time on this because this probably will be important for long-term stable control across days and weeks, something, of course, that's desirable in a brain-computer interface in a patient. And just to make one last point, which is that this, we found, only worked when their signals were stable across time. And so we showed in controls that if the signals are unstable, so for example, if every day the signals change, that this doesn't happen. So something about the animal practicing this task with the same transform, and the same set of neurons, and again, and I just want to foreshadow towards the end to say why this may be important for translational purposes. So this next section is really more of a show and tell. And it shows videos of most of the other folks who work in the field, showing translational work from the early days towards, you know, state-of-the-art papers that are published, like a year or two ago, showing the development in the field. And then the last section, I'll talk about more of the work we're doing at UCSF in this regard. I'll sort of skip this, but, you know, I just wanted to highlight that, you know, these fields, there's just a lot of people involved in this, and these are some of the papers that are really important for the development of this. And I want to just highlight a few of them. And in one it was a paper by Niels Birbaumer in 1999, and it was among the first papers that showed the potential power of a brain-computer interface. And this is a EEG interface, with non-invasive scalp electrodes, no surgery required, right? They are just electrodes put on the scalp with a conductive gel. And he had a long interest working with ALS patients, patients with Lou Gehrig's disease, and he had two patients who had advanced ALS, who were, this is in Germany, who were chose to remain on the ventilator. So they're ventilated, they were artificially fed, but they couldn't really communicate. So they could barely move their eyes, so they're really in this really severe locked-in state, and they had no voluntary movements. And what he fashioned was a EEG-based device to help them communicate. And so, you know, over a several month period, these subjects learned to control their EEG signals in order to say yes or no. So you can imagine, they would learn to, you know, create a state in their EEG that would be decoded as one, and then the other state would be zero. This was slow, it took them a whole day to type this message, but for them, this was the first ability to communicate in a long time. And so this paragraph, it's in German, and I had to use Google Translate, but it basically says, "Thank you Dr. Birbaumer, "for allowing us to communicate." And it, you know, it was, and remember, I was a graduate student at the time, it really captures your imagination, because these are patients who are unable, they really have a bottleneck for communication, and something at the early days was slow, could be quite impactful for them. And so this is, you know, 20 years ago, then, just to kind of go through the progress. And this is again an EEG, so it's still using a band-limited set up, meaning it has relatively low information content. But just to show an example how these things can be very important, this is just a case report published in the "Journal ALS" that I really found kind of just very poignant, because this is of a 51-year-old research scientist in the Southern U.S. with ALS, and he learned to use a EEG interface in order to type and do emails. And, in fact, they reported that this subject had actually wrote an RO1, meaning an NIH grant, and got it funded, and says, you know, is typing here, saying, "No problem, "I couldn't run my lab without BCI. "I do molecular neuroscience research "and my grant pays for three people." So as an example which this technology, even in it's early days, you know, it was potentially usable, and here's a simple single example, but a poignant example of someone using it to do, to improve their function. And then, you know, there's been a lot more, not, I wouldn't say a lot more, but there's been separate research in using invasive approaches in humans, and these are all under the guise of FDA clearance, so they're experimental devices, and they're pilot trials, and I wanted to show you three examples of them, and this is one from Leigh Hochberg in Harvard MGH system. And this is one of the first invasive-based approaches in humans. And so it's still early days, and so, you know, and so this is someone who's a spinal cord injury tetraplegic. And so this, again, is completely under brain control, right, so he's not moving, and the task here is to take that green cursor, and to touch the targets. And I want to point out that this used, you know, about a hundred channels, and a subset of that had neurons, and this is using a simple linear filter, not very sophisticated, and I think that's the power of this, that even a simple transfer through some plasticity allowed this patient to kind of, you know, do this task reasonably well. And then I'll show you a couple more. So this is from last year, or 2017, so a couple years ago, and, you know, the performance has gotten better, as you'd hope, right? This field has been progressing. And this is from the Shenoy Lab at Stanford. And this individual is, again, someone with ALS, and is unable to move, but you can see the much better control, right? And so she's, I believe, in this video, asked to type about how she got her kids to play piano, or something of that sort. And, you know, it's, yeah, so this is, I would say, by far the best control demonstrated to date, and this is, again, based on spikes. And so these are moving quite well, she's clicking and selecting, right? The next one is of, and this is a few years earlier, but from a group at Pittsburgh, the Andy Schwartz group, and they, again, had a tetraplegic. In this case it was someone with a neuromuscular disease and couldn't move. And here, they're also using spike-based technology, so the arrays. And they are trying to, so the subject learnt over several weeks to manipulate these objects, and what was an important milestone for this research was that they used a clinically viable, or a clinically important scale, and so this is a ARAT test, which is used often in spinal cord injury and stroke research to understand someone's functional recovery. And so you can say for this session it's quite remarkable, right? And this is only with visual feedback, there's no proprioception, there's no tactile sensation, right. So this is all vision, and it shows you the potential, right? And then this, so this, the one before, with the cursor control, and this with the robotic arm are by far the the best to date. And, yeah, and so, so this gives you a sense of what's possible. And the next part I wanted to just talk a little bit more about translational challenges, and talk about things we're doing at UCSF, to try to bridge some of these. So hopefully, from the what I've shown you, and based on the videos, I think there should be no doubt that this can work, right? There's examples, and there's more and more in the literature showing that BCI's can work, that paralyzed patients can learn to control their brain activity in order to control complex devices. And then the question is, well why isn't this in many more people, because the need is quite great? And it ends up being a couple things. One is that the signals are not stable, and so this Utah Array, so that's the basis for the, you know, all the work that I've shown you earlier, and so these are arrays that are silicon probes that actually go into the brain, and are recording activities I showed you. The important point is these are very, very stiff. It turns out the brain is like a ball of jello, so it's constantly moving, it's not very stiff, and this is a very stiff device. And we now know that when the brain is moving and this device is not moving quite as fast, that this can lead to degradation of signals. There's gliosis, there's loss of the signals. So this is an important problem with long-term longevity. The other is that, you know these rely on single action potentials, and if you think back to one of the earlier slides, those arrays, these arrays are recording from neurons that are really, really close, you know, they have to be within a few microns of it. And so every day it turns out that we lose the neurons. You can't really track the neurons. And so it turns out that the type of control isn't always reliable. Some days it works, some days it doesn't. It requires a lot of training. And so that, so those are two limitations about the signal instability, and the lack of long-term reliable control. And the other parts that are, you know, things could be solved if these two, if the signals were stable, the training were more stable, and required less training, you could imagine that this would also reduce the complexity, but right now it requires every individual lab requires a research engineer at the home in order to really set this up. So this is not ready for fully autonomous control. And currently, they're not wireless, but this, you know, this, you could imagine, is the easiest to solve. And there are commercial companies and research universities that are solving this already. And so one of the main things is signal stability and daily training. And so this last part, I'll talk about a ongoing trial at UCSF that's just we've currently enrolled one subject. We're just trying to use technology that is hopefully more stable, and that can leverage more of this plasticity that I showed you was important. And the last part I'll talk about. you know, the other part of our lab that works on trying to use this type of technology for stroke rehabilitation, for recovery, and I'll just highlight why that's different from this. So we have a ongoing trial that's also FDA, under the FDA clearance, and it's this experimental device, and so unlike the array that I talked about, the spikes that go into the brain, so these are electrocorticography, and so these are disks sitting on the brain surface, and so there's potentially two benefits. One is because it's averaging a lot more tissue that perhaps this is more stable, that we, you know, that micromotion is gonna be less devastating on a day-to-day basis. And this might allow us to do more, you know, daily training and engage plasticity, and I'll show you, this is immediately what we find is that we can make patients learn a skill that they don't forget. So it's basically, you know, approaching plug-and-play for our subjects here, and, you know, and we are currently enrolling additional subjects. So our hope is to demonstrate the longevity of ECoG, as well as the training and the stability. And I wanted to also point out that ECoG has been used in other medical devices, so we know that these signals can last for over a decade. And so, you know, if one could show that ECoG was, and because it's a much more average signal, it remains unclear if it can result in more complex control, so that's an important part of our trial. And so the two goals of our trial are to look at a typing interface, and then to look at complex robotic arm control. And, you know, to date we have enrolled one patient, and this is, indeed, where the grid is placed over brain area, and so this is the central sulcus. So this is motor cortex, this is the sensory areas, and these are pre-motor, motor areas. And so, you know, if you were to look at the UTARI that I've been describing, it would be like one of those dots. So just to put it in scale, these are much larger scale, and, you know, what we have found to date is that with training, our subject can really achieve a plug-and-play state. So this, for us, was very exciting because the, you know, with about a month of training, the subject, now, we just plug him and he's good to go. And so that offers the opportunity, now, to say that, you know, and this is what we're doing right now, is to, this is completely with just cursor control. And he is, he's a 36-year-old gentleman who's quadriparetic, and he's also very dysarthric, so he can't speak. And so for him, he's not moving, this is fully under brain control, and with learning, he's able to do this point-and-click. So he goes to each of the targets, and does a brain click in order to turn the cursor red. So this is akin to your mouse click. So this is him going to a target on the screen and clicking. And then, for us, it's exciting to also look at the neural plasticity, right? So what this is, is the patient's neural responses over a many day period. So each of this is a daily map analysis. And so it's essentially this grid, but in a matrix form. When something is red, it implies that channel's really active. When it's yellow, it's not very active. And so what's interesting is that in the early days when he's training, you can see his brain activity is quite unstable, that he hasn't converged on a solution, and then all of a sudden that, you know, he sort of kinda gets it, his performance gets stable, and then all of a sudden this brain map has been stable for several months now. So he's able to recall it, and, you know, it acts like a memory, right? He's able to tap into it very reliably. And so we're, you know, this is still early days for this trial, and we are, you know, and we are now measuring the bit rates in terms of its communication output, and trying to push the limits of performance. But to us it's exciting to at least say that even this lower resolution, because it's much more averaging, can result in a stable signal, and can engage these brain plasticity mechanisms. And so this is just to show that, you know, this is, would be an alternate approach to spike based approaches, and to be determined which of them will end up being, you know, the most clinically meaningful, and most stable. but just to highlight that at UCSF, this is something that's currently ongoing, and currently recruiting for subjects two and three. And so the last part I wanted to end with, which is more at a early translational stage, and something that we are thinking about from a translational perspective, is how to use this brain-computer interface technology for stroke. And, you know, stroke is extremely common, it's the number one diagnosed at UCSF for neurology, probably, you know, it's very, very common. And about 50% of those patients are left with upper arm impairments that limit independence. And I wanted to contrast what I've been showing you with brain-computer interfaces for stroke versus someone with spinal cord injury, right? So in spinal cord injury, the brain would be intact, whereas, of course, in stroke there's a lesion, there's actually damage to the brain. And so the type of research and the interface would be very different, right? So here we'd have to try to create, recreate brain dynamics. So the hope is can we create a artificial systems that can rest within this area, and try to recreate brain dynamics, improve function. And so this is still at a basic science level, but I'll show you some of our progress in animal models of moving towards potentially recreating brain dynamics and improving function. And just to give you, you know, kind of a vision of what this would look like, you know, unlike a neural interface which I was showing you before, which is decoding user intention and allowing cursor control, or robotic control, here, it's trying to augment the arm function in this case, right? So the subject, for example, might be thinking about moving, and maybe this intention's really weak because of damage. We would decode that in real time and try to augment function through stimulation, so we try to recreate brain dynamics. And this is, I'll show you something that we've had success with, trying to recreate how neurons are cofiring together in order to improve function. And, yeah, and so what are some of the questions that are important for this, is, well, what would a electrophysiological target look like, right, if we're recording brain activity and trying to decode intention, what is the exact signature in the lesioned brain relative to normal brain? And then how might this be a therapeutic target? And I wanted to contrast with the other part of our lab, really, and try to understand normal skill movement. This is in rodent models, to try to figure out signatures that might be a target. And I won't go into a lot of details, but I want to show you, like, you know, two parts of the motor network, and why synchronous activity is really important for control. So this is a, we're recording for motor cortex, which I've been talking to you about, and we're recording from one part of the basal ganglia, the striatum, which is one synapse below motor cortex, so these are heavily connected. And here's a rat learning control. Early days, you know, that when the animal's learning, this cloud of dots implies the animal's very variable, isn't able to do it well with training, it becomes a skill where the animal's been very accurate and very fast, so it's fast and accurate, which are hallmarks of a skill. And if we record their brain activity, what we see is this characteristic change in the synchronicity. So if you look at early days, where, you know, if you look here you might see some squiggles, and so it's interesting that the neural activity early in learning, similar to what I showed you in the ECoG, is very variable from trial to trial, it's not reliable. When the brain or the animal finds a solution, there's these bursts that emerge, and so hopefully you can see these, you know, oscillations here, compared to there. These cycles there, there, there, even the muscles. And so what we have found is a hallmark of skilled movements of these reverberating patterns of activity that are throughout the motor network. And as I'll show you, these are an important signature that helps us improve function after stroke. And this is another view of that, if you were to look, recorded at a single trial with animal reaching and grasping. We see, you know, what this is helping do is drive coordinated activity across brain structures, and if you think of what motor cortex has to do, its integrating all the cognitive cues, and then driving many, many subcortical nodes, the muscles, the spinal cord, brainstem nuclei, and this basal ganglia. So it's really important to synchronize. And here's another view of that, with training, when the animal, you know, goes from this early, slow, and not accurate, to more accurate, and green, and fast, that these yellow streaks reflect the system really reverberating in this oscillatory mode. And so for us it's, you know, it's a potential therapeutic target to look at injury. And for the aficionados, we can do all sorts of statistical analysis and show it's a very robust effect when the animal transitions from unskilled to skilled, this transition point, the hallmark is this oscillatory activity. And so we've been studying this in in a recovering animal, and we have found that basically by recording in the residual tissue, that we can segregate good recovery versus poor recovery based on his oscillatory dynamics. And so, for example, this animal recovered well, and so here all of a sudden, you know, it became very accurate, so this was not accurate. And these yellow streaks, again, are the oscillatory dynamics, and you can see, with recovery, when this pattern starts to emerge, that the animal becomes much more accurate And so this, and if you were to look at another view of this, part of that what's going on is that it's the tissue here reengaging and having patterned activity that's not able to drive muscle, whereas, early on, the patterns are unreliable, very weak, and so the common theme is that maybe after injury, if there's this poor patterning, function is not reliable. With recovery of this, it becomes more reliable. And the important question is what about animals that don't recover? And this is analogous to what we see in stroke patients, right? Somewhat around 50%t of our patients have a stroke, they just don't recover. So they would resemble animals that remain in this state and don't transition to this state. And the question, of course, is can we do something artificially to recreate this type of patterning to improve function? And that's essentially what we have found in our rodent models. And what we have done is a closed-loop system that's recording activity, and then stimulating to create artificial patterning. And this is the type of effects we can see, where here's an animal with a chronic deficit, so can't really reach and grasp very well, and when we turn on our device, which is recording and stimulating, the performance goes up, we turn off the device, it goes back, and so forth. So you get a sense that this device is, when it's turned on and able to generate these artificial patterning, can actually improve function. And so this is kind of our working model where the intact nervous system is really about these, you know, brief oscillatory dynamics that help reliable function. With injury, there's a loss of this, poor behavior, and then when we induce these artificial oscillatory dynamics, we can go back to this. And this gives us potentially a therapeutic target to potentially create a closed-loop system that's using this brain-computer interface technology I've been talking about, as a way to potentially improve function in patients with poor recovery. So there's only two more slides left, and then I wanted to show that, you know, and so obviously rodents are a model system and, you know, there's a lot of failures that don't translate to humans. And so we've been taking these early strides of trying to find innovative ways to see if these biomarkers hold up in human subjects. Still early days, but this is work done with a colleague here, Eddie Chang, who is a neurosurgeon, and, you know, he's a epilepsy surgeon who takes care of patient with epilepsy who needs surgical treatment. And so this happens to be a young individual who was 42 at the time, who had a stroke, and had, you know, poor recovery, and he also unfortunately had seizures, and so he, through, you know, institutional review through the UCSF approval, we were able to record his brain activity when he was trying to make movements, to try to understand, is what we are seeing in our rodent model at all reflective of what's going on in human stroke subjects? And just to point out, this is where his grid was, this is where his stroke is. And, you know, for us, it's exciting. So early days, where we found that this oscillatory dynamics I was talking about is actually something that seems to be preserved in human subjects. And why do I say that, is that so these are two, you know, all three of these subjects have seizures, so they all underwent the surgery, mainly for their epilepsy. And what we did with, you know, having them move around and record brain activity was purely for research, and that's not why they had surgery. And if you look at the two healthy subjects, these yellow dots here, are these waves of low frequency oscillation, those oscillatory bursts I was describing, and we have a lot more data on this where really these are these small bursts of activity that are propagating within the motor network, that are present in intact subjects doing reaching. Whereas in the stroke, in this one stroke subject, and we have others now, where this is not present. And so it shows us that potentially this biomarker that we're finding in rodents may be translatable, and, you know, for us it's, you know, we're in this early phase of thinking about the translational approach, and this may be, there may be a non-invasive ways to boost these oscillations, and for us it's at least an exciting step to say, if you look at, you know, recovery, versus poor recovery, versus not, you see this yellow dot there which is a quantification of these oscillations that we see. And this is exactly what we see in our human stroke patients versus intact human subjects making movements, suggesting that maybe these oscillatory dynamics is something that's important for skilled control, for fast reliable control. And, you know, and that's a big part of our research goal is to try and think about testing this in other animals, models, to move towards something that may be clinically testable, whether it's non-invasive or invasive. So that's the end of the data. And I just wanted to end with so what's next, right? So what I have shown you, at least with the brain-computer interface work, is a lot of early stage pilot studies. And obviously, you know, impressive proof of principle. And now as a field, we're, you know, there is an opportunity to conduct larger studies, and I think important for that is to think a lot about clinical efficacy, outcome measures, longevity. Of course this requires a surgery, and so an important part of next stages will be to definitively show that this surgical-based control is definitely better than the EEG, right? So just to make a clear, you know, unequivocal need for these type of invasive procedures. And to date it's roughly the case, but of course this is something that has to be shown through clinical trials. And then as I mentioned, customization to patient's needs. And there's a long history in the rehabilitation field of, you know, gadgets that sit on shelves. And so trying to really, you know, understand patient needs, for example, with our subject, the first subject we've enrolled, you know, pre-enrolled, that we really made sure that we knew the patient's goals, right? One was cursor control for typing and so forth. The other was self-feeding. And so it's, you know, if we're meeting those needs, it would certainly get more engagement, and for us, say, if we're gonna meet potentially NF1, each individual's needs. And then, of course, probably one of the most important is commercialization of technology. And this is something that seems to be really kind of steamrolling in a positive way recently. There's a slew of companies, some in the Bay Area, that are tackling the challenge of commercialization, and it's still obviously early days, because these are all wrapped together, right? Like, you have to, like, this technology may depend on patient needs, right? So if you don't exactly know what a patient wants, it's hard to say let's lock in the exact technology to that specification. So it's sort of, it is a larger closed-loop to understand what does a patient need, what's necessary and sufficient from the types of electrodes, from the longevity, from its ability to do sophisticated control, and then, you know, to lock that into devices that are fully embedded, and wireless, and so forth. So that's the last slide. Thank you for your attention, and I'll happy take any questions. (audience applauds) And so the question was that, you know, this oscillatory dynamic seems to be important for segregating good versus poor recovery, and what may be a mechanism for that? (man speaking faintly) Yeah, and (laughs) yeah. So, you know, we, at this point, have not been utilizing human subjects, but what's exciting is one of, you know, just a professional colleague friend who's at the NIH has done this EEG-based study in patients that just came out. And they, in fact, have found this, you know, and so what we have shown is mainly in rodents, and then in a limited patient. So they have shown it, just a publication came out a couple weeks ago, showing that it's indeed the case, these oscillations aren't there in poor, in the animals that, or in human subjects that don't recover dexterous version versus subjects that do. And of course the question is why. And, you know, I can't speak to the humans, but certainly in our animal models we're starting to figure that out. And it seems to be an important part of it is post-stroke, there is a loss of tissue and a loss of connectivity. And that these, what the connectivity allows is a recurrent network to generate patterning. And so, for example, you know, in a healthy brain, if there's a impulse of activity coming from another area, it would generate these patterns. So essentially it's like a ping to an area, the recurrent network sort of starts to oscillate. And we have evidence that in a injured network there's just not enough recurrent connectivity to generate that. And so we've been digging a lot into, well, what is our artificial stimulating doing, stimulation doing. What we think we're doing is just providing a low level set of excitability to our neurons, you know, just in a, it's kind of not task, it may not be important for task performance explicitly, it's just helpful for network function. And a more nuanced version of that is that, you know, the motor network is really living within a really large network, whether in a rodent or in a primate. It has to receive input from, you know, upstream areas, it has to drive downstream areas, and they all need to oscillate. And you might imagine that you had lost an area, lost connectivity, it just doesn't have the infrastructure to do that, doesn't have the connections. And so our operating hypothesis is that those who have good recovery, you know, for a variety of reasons, that goes maybe to genetics, age, comorbidities, may not be able to generate the axon outgrowth and the sprouting required to regenerate. - [Man] Interesting is level of damage (speaking faintly). - You see, it's interesting, level of damage doesn't seem to predict as much. It's more, it's the, you know, sorry, so the question was, does the level of damage matter? To some extent yes, but maybe where their damage is. So for example, you know, the motor cortex is sitting, is a very privileged site, because it has these really thick pipelines to basal ganglia, to spinal cord, and, in fact, we find this, the volume may be very different. So a strategically placed lesion at that nexus point is really devastating. Something that may be big, but not quite there, may be less devastating. So it's about, you know, real estate is certainly a big part of that, and related to that is, as a clinical neurologist, we see a whole range of strokes. And, you know, sometimes the strokes that happen in the brain stem, which is, you know, deep down in the brain, where there's a lot of fibers and tight density, and you lose all your fibers all of a sudden, and so some subcortical strokes can be very devastating, even though they're small. so it's probably more about where it is and the type of fibers that are damaged are probably the most important, yeah. And then, and then way to recover from that is probably also important, and there's work from, for example, Tom Carmichael at UCLA, showing that the genetics, whether it's in a rat or in human, has a bearing on whether there's the ability to regenerate developmental programs to help with sprouting and so forth. - [Man] Thank you. (audience applauds) - Thank you. (gentle music) 