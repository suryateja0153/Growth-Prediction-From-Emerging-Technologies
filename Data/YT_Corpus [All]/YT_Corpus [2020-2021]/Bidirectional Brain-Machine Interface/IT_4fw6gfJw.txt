 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu hi good afternoon everyone so today we're going to be talking about graph optimizations and as a reminder on Thursday we're gonna have a guest lecture by Professor Johnson of the MIT math department and he'll be talk about performance of high-level languages so please be sure to attend the guest lecture on Thursday so here's an outline of what I'm gonna be talking about today so we're first going to remind ourselves what a graph is and then we're talking you're gonna talk about various ways to represent a graph in memory and then we'll talk about how to implement an efficient breadth-first search algorithm both serially and also in parallel and then i'll talk about how to use graph compression and graph reordering to improve the locality of graph algorithms so first of all what is a graph so a graph contains vertices and edges where vertices represent certain objects of interest and add just between objects model relationships between the two objects for example you can have a social network where the people are represented as vertices and edges between people means that they're friends with each other the edges in this graph don't have to be bi-directional so you could have a one-way relationship for example if you're looking at the Twitter network Alice could follow Bob but Bob doesn't necessarily have to follow Alice back the graph also doesn't have to be connected so here this graph here is connected but for example there could be some people who don't like to talk to other people and then they're just off in their own component you can also use graphs to model protein networks where the vertices are proteins and edges between vertices means that there's some sort of interaction between the proteins so this is useful in computational biology as I said edges can be directed so the relationship can go one way or both ways in this graph here we have some directed edges and then also some edges that are directed in both directions so here John follows Alice Alice follows Peter and then Alice Falls Bob and Bob also follows Alice if you use a graph to represent the worldwide web then the vertices would be websites and then the edges would denote that there's a hyperlink from one website to another and again the edges here don't have to be bi-directional because website a could have a link to website B but website B doesn't necessarily have to have a link back I just can also be weighted so you can have a weight on the edge that denotes the strength of the relationship or some sort of distance measure corresponding to that relationship so here I have an example where I'm using a graph to represent cities and the edges between cities have a weight that corresponds to the distance between the two cities and if I want to find the quickest way to get from City a to city B then I would be interested in finding the shortest path from A to B in this graph here here's another example where the edge weights now are the cost of a direct flight from City a to city B and here the edges are directed so for example this says that there's a flight from San Francisco to LA for 45 dollars and if I want to find the cheapest way to get from one city to another city then again I would try to find the shortest path in this graph from City a to city B vertices and edges can also have metadata on them and they can also have types so for example here's the Google knowledge graph which represents all the knowledge on the internet that Google knows about and here the nodes have metadata on them so for example the node corresponding to da Vinci is labeled with his date of birth and date of death and the vertices also have a color corresponding to the type of knowledge that they refer to so you can see that some of these nodes are blue some of them are red some of them are green and some of them have other things on them so in general crafts can have types and metadata on both the vertices as well as the edges let's look at some more applications of graphs so graphs are very useful for implementing queries on social networks so here are some examples of queries that you might want to ask on a social network so for example you might be interested in finding all of your friends who went to the same high school as you on Facebook so that can be implemented using a graph algorithm you might also be interested in finding all the common friends you have with somebody else again a graph algorithm in a social network service might run a graph algorithm to recommend people that you might know and want to become friends with and they might use a graph algorithm to recommend certain products that you might be interested in so these are all examples of social network fries and there are many other queries that you might be interested in running on a social network and many of them can be implemented using graph algorithms another important application is clustering so here the goal is to find groups of vertices in a graph that are well connected internally and poorly connected externally so in this image here each blob of vertices of the same color correspond to a cluster and you can see that inside a cluster there are a lot of edges going among the vertices and between clusters there are relatively fewer edges and some applications of clustering include community detection and social networks so here you might be interested in finding groups of people with similar interests or hobbies you can also use clustering to detect fraudulent websites on your internet you can use it for clustering documents so you would cluster documents that have similar text together and clustering is often used for unsupervised learning in machine learning applications another application is connect omics so connectomics is the study of the structure of the the network structure of the brain and here the vertices correspond to neurons and edges between two vertices means that there's some sort of interaction between the two neurons and recently there's been a lot of work on trying to do high-performance connect omics and some of this work has been going on here at MIT by professor charles leiserson and professor near Shahid's research groups so recently this has been a very hot area graphs also used in computer vision for example an image segmentation so here you want to segment your image into the distinct objects that appear in the image and you can construct a graph by representing the pixels as vertices and then you would place an edge between every pair of neighboring pixels with a weight that corresponds to their similarity and then you would run some sort of minimum cost cut algorithm to partition your graph into the different objects that appear in the in the image so there are many other applications and I'm not gonna have time to go through all of them today but here's just a flavor of some of the applications of graphs so any questions so far okay so next let's look at how we can represent a graph in memory so for the rest of this lecture I'm gonna assume that my vertices are labeled in the range from 0 to Adam and minus 1 so they have an integer in this range sometimes your graph might be given to you where the vertices are already labeled in this range sometimes not but you can always get these labels by mapping each of the identifiers to unique integer in this range so for the rest of the lecture I'm just going to assume that we have these labels from 0 to n minus 1 for the vertices one way to represent a graph is to use an adjacency matrix so this is going to be an N by n matrix and there's a 1 bit in the eighth row and J column if there's an edge that goes from vertex I to vertex J and 0 otherwise another way to represent a graph is the edge list representation well we just store a list of the edges that appear in the graph so we have one pair for each edge where the pair contains the two coordinates of that edge so what is a space requirement for each of these two representations in terms of the number of edges M in the number of vertices n in the graph so it should be pretty easy yes yeah so the space for the AJC matrix is order N squared because you have N squared cells in this matrix and you have one bit for each of the cells for the edge list it's going to be order M because you have M edges and for each edge you're storing a constant amount of data in the edge list so here's another way to represent a graph this is known as the adjacency list format and the idea here is that we're going to have an array of pointers one per vertex and each pointer points to a linked list storing the edges for that vertex and the linked list is unordered in this example so what's the space requirement of this representation yes so it's going to be order n plus M and this is because we have n pointers and the sum of the number of entries across all the linked lists is just equal to the number of edges in the graph which is M what's one potential issue with this sort of representation if you think in terms of cash performance does anyone see a potential performance issue here yeah right so the so the issue here is that if you're trying to loop over all the neighbors of a vertex you're going to have to dereference the pointer in every linked list node because these are not two contiguous in memory and every time you dereference linked lists note that's going to be a random access into memory so that can be bad for cache performance one way you can improve cache performance is instead of using linked lists for each of these neighbor lists you can use an array so now you can store the neighbors just in this array and they'll be contiguous in memory one drawback of this approach is that it becomes more expensive if you're trying to update the graph and we'll talk more about that later so I knew questions so far so what's another way to represent the graph that we've seen in a previous lecture what's a more compressed or compact way to represent a graph especially a sparse graph says anybody remember the compressed sparse row format so we looked at this in one of the early lectures and in that lecture we used it to store a sparse matrix but you can also use it to store a sparse graph and as a reminder we have two arrays in the compressed sparse row or CSR format we have two offsets array and the edges array the offsets array stores an offset for each vertex into the edges array telling us where the edges for that particular vertex begins in the edges array so offsets of I stores the offset of where vertex i's edges start in the edges array so in this example vertex zero has an offset of zero so it's edges start at position zero in the edges array vertex one has an offset of four so it starts at index 4 in this offsets array so with this representation how can we get the degree of a vertex so we're not storing the degree explicitly here can we get the degree efficiently yes yes so you can get the degree of a vertex just by looking at the difference between the next offset and its own offset so for vertex zero you can see that it's degree is 4 because vertex one's offset is 4 and vertex zero is offset as zero and similarly you can do that for all the other vertices so what's the space usage of this representation sorry yeah so again it's going to be order n plus n because you need order and space for the offsets array in order M space for the edges array you can also store values or weights on the edges one way to do this is to create an additional array of size m and then for edge I you just store the weight or the value in this in the ithe index of this additional array that you created if you're always accessing to wait when you acts as an edge then it's actually better for a cache locality to interleave the weights with the edge targets so instead of creating two arrays of size m you have one array of size 2m and every other entry is the weight and this improves casual county because every time you access an edge its weight is going to be right next to it in memory and it's going to likely be on the same cache line so that's one way to improve cache locality any questions so far so let's look at some of the trade-offs in these different graph representations that we've looked at so far so here I'm listing the storage cost for each of these representations which we already discussed this is also the cost for just scanning the whole graph in one of these representations what's the cost of adding an edge in each of these representations so for adjacency matrix what's the cost of adding an edge yeah so for adjacency matrix it's just order 1 to add an edge because you have random access into this matrix so you just have to access the I comma J entry and flip the bit from a 0 to 1 what about for the edge list so we're assuming that the edgeless is unordered so you don't have to keep the list in any sorted order yeah yes so again it's just oh of one because you can just add it to the end of the edge list so that's constant time what about for that jcu list so actually this depends on whether we're using linked lists or a race for the neighbor list of the vertices if we're using a linked list adding an edge just takes constant time because we can just put it at the beginning of the linked list if we're using an array then we actually need to create a new array to make space for this edge that we add and that's going to cost us degree of V work to do that because we have to copy all the existing edges over to this new array and then add this new edge to the end of that array of course you could amortize this cost across multiple update so if you run out of memory you can double the size your array so you don't have to create these new arrays too often but the cost for any individual addition is still relatively expensive compared to say an edge list or @jc matrix and then finally for the compressed sparse row format if you add an edge in the worst case it's going to cost us order m+ and work because we're going to have to reconstruct the entire offsets array and the entire edges rating in the worst case because we have to put something in and then shift in the edges the way you have to put something in and it shift all of the values to the right of that over by one location and then for the offsets array we have to modify the offset for the particular vertex we're adding an edge to and then the offsets for all the vertices after that so the compressed bars row representation is not particularly friendly to edge updates what about for deleting an edge from some vertex V so for our JC matrix it's again it's going to be constant time because you just randomly access the correct entry and flipped a bit from a 1 to 0 what about for an edge list yeah so for an edge lights in the worst case it's going to cost us order at work because the edges are not in any sorted order so we have to scan through the whole thing in the worst case to find the edge that we're trying to delete for a JCL list it's going to take order degree of V work because the neighbors are not sorted so we have to scan through the whole thing to find this edge that we're trying to delete and then finally for a compressed Barse row it's going to be order n plus and because we're gonna have to reconstruct the whole thing in the worst case what about finding all the neighbors of a particular vertex V what's the cost of doing this in the adjacency matrix yes so it's going to cause this order n work to find all the neighbors of a particular vertex because we just scanned the correct row in this matrix the row corresponding to vertex V for the edge list we're going to have to scan the entire edge list because it's not sorted so in the worst case that's going to be order M for adjacency list that's going to take order degree of V because we can just find the pointer to the linked list for that vertex in constant time and then we just traverse over the linked list and that takes order degree of V time and then finally for the compress bars row format it's also ordered degree of V because we have constant time access into the appropriate location and the edges array and then we can just read off the edges which are consecutive in memory so what about finding if a vertex W is a neighbor of V so I'll just give you the answers so for that Jacy matrix is going to take constant time because again we just have to check the Vieth row and the W with column and check of the bit is set there for edge list we have to traverse the entire list to see if the edges there and then for a JCL list and compress bars row is going to be ordered degree of V because we just have to scan the neighbor list for that vertex so these are some graph representations but there are actually many other graph representations including variants of the ones that I've talked about here so for example for that jcu list I said you can either use a linked list or an array to store the neighbor list but you can actually use a hybrid approach where you solve a linked list but each linked list node actually stores more than one vertex so you can store maybe 16 vertices in each linked list node and that gives us better cache locality so for the rest of this lecture I'm going to talk about algorithms that are best implemented using the compressed sparse row format and this is because we're going to be dealing with sparse graphs we're going to be looking at static algorithms where we don't have to update the graph if we do have to update the graph then CSR isn't a good choice but we're just gonna be looking at static algorithms today and then for all the algorithms that we'll be looking at we're going to need to scan over all the neighbors of a vertex that we that we visit and CSR is very good for that because all the neighbors for a particular vertex are stored contiguously in memory so I need questions so far okay I do want to talk about some properties of real-world graphs so first we're seeing graphs that are quite large today but actually they're not too large so here are the sizes of some of the real-world graphs out there so there's a Twitter Network this is actually a snapshot of the Twitter network from a couple years ago has 41 million vertices and 1.5 billion edges and you can store this graph in about six point three gigabytes of memories you can probably store it in the main memory of your laptop the largest publicly available graph out there now is this common crawl web graph it has 3.5 billion vertices and 128 billion edges so storing this graph requires a little over half a terabyte of memory says it is quite a bit of memory but it's actually not too big because there are machines out there with main memory sizes in the order of terabytes of memory nowadays so for example you can rent a 2 terabyte or 4 terabyte memory instance on AWS which you're using for your homework assignments see if you have any leftover credits at the end of the semester you can and you want to play around with this graph you can rent one of these terabyte machines just to remember to turn it off when you're done because it's kind of expensive another property of reward graphs is that they're quite sparse so M tends to be much less than N squared so most of the edges most of the possible edges are not actually there and finally the degree distributions of the vertices can be highly skewed in many real-world graphs see here I'm plotting the degree on the x-axis and the number of vertices with that particular degree on the y-axis and we can see that it's highly skewed and for example in a social network most of the people would be on the left hand side so their degree is not that high and then we have some very popular people on the right hand side where their degree is very high but we don't have too many of those so this is what's known as a power law degree distribution and there have been various studies that have shown that many real-world graphs have approximately a power law degree distribution and mathematically this means that the number of vertices with degree D is proportional to D to the negative P so negative P is the exponent and for many graphs the value of P lies between 2 and 3 and this power law degree distribution does have implications when we're trying to implement parallel algorithms to process these graphs because with graphs that have a skewed degree distribution you could run into load imbalance issues if you just paralyze across the vertices the number of edges they have can vary significantly any questions ok so now let's talk about how we can implement a graph algorithm and I'm going to talk about the breadth first search algorithm so how many of you have seen Bradford search before ok so about half of you I did talk about breadth-first search in the previous lecture so I was hoping everybody would raise their hands okay so as a reminder in the BFS algorithm were given a source vertex s and we want to visit the vertices in order of their distance from the source s and there are many possible outputs that we might care about one possible output is we just want to report the vertices in the order that they were visited by the breadth first search traversal so let's say we have this graph here and our source vertex is D so what's one possible order in which we can traverse these vertices now I should specify that we should traverse this graph in a breadth-first search manner so what's the first vertex we're going to explore D so we're first going to look at deep because that's our source vertex the second vertex we can actually choose between B C and E because all we care about is that we're visiting these vertices in order of their distance from the source but these three vertices all have the same distance so let's just pick B C and then e and then finally I'm going to visit vertex a which has a distance of two from the source so this is one possible solution there are other possible solutions because we could have visited e before we visited B and so on another possible output that we might care about is two we might want to report the distance from each vertex to the source vertex s so in this example here are the distances so D as a distance of zero B C and E all have a distance of one and a as the distance of two we might also want to generate a breadth first search tree where each vertex in the tree has a parent who which is a neighbor in the previous level of the breadth-first search or in other words the parent should have a distance of one less than that vertex itself so here's an example of a breadth-first search tree and we can see that each of the vertices has a parent whose breadth-first search distance is 1 less than itself so the algorithm said I'm going to be talking about today will generate the distances as well as the BFS tree and BFS actually has many applications so it's used as a subroutine in between a centrality which has a very popular graph mining algorithm used to rank the importance of nodes in a network and the importance of nodes here corresponds to how many shortest paths go through that node other applications include a centricity estimation maximum flows are some max flow algorithms use BFS as routine you can use BFS to crawl the web do cycle detection garbage collection and so on so let's now look at a serial BFS algorithm and here I'm just going to show the pseudocode so first we're going to initialize the distances to all infinity and we're gonna initialize the parents to be nil and then we're gonna create a queue data structure we're going to set the distance of the route to be zero because the route has a distance of zero to itself and then we're going to place the route on to this queue and then while the queue is not empty we're going to DQ the first thing in the queue we're going to look at all the neighbors of the current vertex that we DQ'd and for each neighbor we're going to check if its distance is infinity if the distance is infinity that means we haven't explored that neighbor yet so we're going to go ahead and explore it and we do so by setting its distance value to be the current vertex is distance plus one we're going to set the parent of that neighbor to be the current vertex and it will place the neighbor onto the queue so some pretty simple algorithm and we're just according to keep iterating and this while loop until there are no more vertices left in the queue so what's the work of this algorithm in terms of N and M so how much work are we doing per edge [Music] yes yes so assuming that the NQ and EQ operators operators are constant time then we're doing cost amount of work per edge so sum across all edges that's going to be order M and then we're also doing a cost amount of work per vertex because we have to basically place it onto the queue and then take it off the queue and then also initialize their values so the overall work is going to be order M plus n ok so let's now look at some actual code to implement this serial BFS algorithm using the compressed sparse row format so first I'm going to initialize two arrays parent and Q and these are going to be integer arrays of size N I'm going to initialize all of the parent entries to be negative 1 I'm going to place a source vertex on to the queue so it's going to appear at Q of 0 that's the beginning of the queue and then I'll set the parent of the source vertex to be the source itself and then I also have two integers that point to the front in the back of the queues initially the front of the queue is at position 0 and the back is at position 1 and then while the queue is not empty and I can check that by checking if queue front is not equal to Q back then I'm going to take DQ the first vertex in my queue I'm gonna set current to be that vertex and then I'll increment Q front and then I'll compute the degree of that vertex which are going to do by looking at the difference between consecutive offsets and I also assume that offsets of Adhan is equal to em just to deal with the last vertex and then I'm going to loop through all of the neighbors for the current vertex and to access each neighbor what I do is I go into the address array and I know that my neighbors start at offsets of current and therefore to get the ithe I just do offsets of current plus I that's my index into the address array now I'm gonna check if my neighbor has been explored yet and I can check that by checking a parent of neighbors equal to one if it is that means I haven't explored it yet and then I'll set a parent of neighbour to be current and then I'll place the neighbor onto the back of the queue and increment queue back and I'm just gonna keep repeating this while loop until it becomes empty and here I'm only generating the parent pointers but I could also generate the distances if I wanted to with just a slight modification of this code so any questions on how this code works okay so here's a question what's the most expensive part of the code can you point to one particular line here that is the most expensive yes place sir okay so actually turns out that that's not the most expensive part of this code but you're close does anyone have any other ideas yes yes it turns out that this line here where we're accessing parent of neighbor that turns out to be the most expensive because whenever we access this parent array the neighbor can appear anywhere in memory so that's going to be a random access and if the parent array doesn't fit in our cache then that's going to cost us a cache miss almost every time this address array is actually mostly accessed sequentially because for each vertex all of its edges are stored contiguously in memory we do have one random access into the edges array per vertex because we have to look up the starting location for that vertex but it's not one per edge unlike this check of the parent array that that occurs for every edge does that makes sense so so let's do a back-of-the-envelope calculation to figure out how many cache misses we would incur assuming that we started with a cold cache and we also assume that n is much larger than the size of a cache so we can't fit any of these arrays into cache we'll assume that a cache line has 64 bytes and integers are 4 bytes each so let's try to analyze this so the initialization will cost us and over 16 cache misses and the reason here is that we're initializing this array sequentially so we're accessing contiguous locations and this can take advantage of spatial locality on each cache line we can fit 16 of the integers so overall we're going to need n over 16 cache misses just to initialize this array we also need n over 16 cache misses across the entire algorithm to DQ the vertex from the front of the queue because again this is going to be a sequential access into this queue array and across all vertices that's going to be n over 16 cache misses because we can fit 16 virtus 16 integers on a cache line to compute the degree here that's going to take n cache misses over all because each of these accesses the offsets array is going to be a random access because we we have no idea what the value of current here is it could be anything so across the entire algorithm we're going to need n cache misses to access this offsets array and then to access this edges array I claim that we're going to need at most two n plus M over 16 cache misses so does anyone see where that bound comes from so where does the M over 16 come from Yeah right so you have to pay I'm over 16 because you're accessing every edge once and you're accessing the edges contiguously so therefore across all edges that's going to take em over 16 cache misses but we also have to add to n because whenever we access the edges for a particular vertex the first cache line might not only contain that vertex as edges and similarly the last cache line that we access might also not just contain that vertex as edges so therefore we're going to waste the first cache line in the last cache line in the worst case for each vertex and summed across all vertices that's going to be two ends so this is an upper bound to n plus M over 16 accessing this parent array that's going to be a random access every time so we're gonna incur a cache miss in the worst case every time so summed across all edge accesses that's going to be M cache misses and then finally we're going to pay an over 16 cache misses 2 and cue the neighbor onto the queue because these are sequential accesses so in total we're going to incur at most 51 over 16 and plus 17 over 16 cache misses and if M is greater than 3n then the second term here is going to dominate and M is usually greater than 3n in most real-world graphs and the second term here is dominated by this random access into the parent array so let's see if we can optimize this code so that we get better cache performance so let's say we could fit a bit vector of size n into cache but we couldn't fit the entire parent array in the cache what do we do to reduce the number of cache misses so anyone have any ideas yeah [Music] yeah so that's exactly correct so we're going to use a bit vector to store whether the vertex has been explored yet or not so we only need one bit for that we're not storing the parent idea in this bit vector we're just storing a bit to say whether that vertex has been explored yet or not and then before we check this parent array we're going to first check the bit vector to see if that vertex has been explored yet and if it has been explored yet we don't even need to access this parent array if it hasn't been explored then we won't go ahead and access the parent entry of the neighbor but we only have to do this one time for each vertex in the graph because we can only visit each vertex once and therefore we can reduce a number of cache misses from em down to n so overall this might improve the number of cache misses in fact it does if if the number of edges is large enough relative to the number of vertices however you do have to do a little bit more computation because you know you have to do bit vector manipulation to check this bit vector and then also to set the bit vector when you explore a neighbor so here's the code using the bit vector optimization so here I'm initializing this bit factor called visited it's of size approximately an over 32 and then I'm setting off the bits to 0 except for the source vertex where I'm going to set its bit to 1 and I'm doing this bit calculation here to figure out the bit for the source vertex and then now when I'm trying to visit a neighbor I'm first going to check if the neighbor is visited by checking this bit array and I can do this using this computation here so I I and visited of neighbor over 32 by this mask one left should that shifted by a neighbor mod 32 and that's false that means the neighbor hasn't been visited yet so I'll go in inside this if clause and then I'll set the visited bit to be true using this statement here and then I do the same operations as I did before it turns out that this version is faster for large enough values of M relative to n because you reduce the number of cache misses overall you still have to do this extra computation here this bit manipulation but if M is large enough then the reduction in number of cache misses outweighs the additional computation that you have to do any questions okay so that was a serial implementation of preference search now let's look at a parallel implementation so I'm first going to do an animation of how a parallel breadth first search algorithm would work the parallel breadth-first search algorithm is going to operate on frontiers where the initial frontier contains just the source vertex and on every iteration i'm going to explore all of the vertices on the frontier and then place any unexplored neighbors on to the next frontier and then i move on to the next frontier so the first iteration i'm going to mark the source vertex i's explored sight its distance to be 0 and then place the neighbors of that source vertex onto the next frontier the next iteration i'm going to do the same thing set these distances to one i also am going to generate a parent pointer for each of these vertices and this parent should come from the previous front here and there should be a neighbor of the vertex and here there's only one option which is the source for a text so i'll just pick that as the parent and then i'm gonna place the neighbors on to the next frontier again mark those as explored set their distances and generate a parent pointer again and notice here when i'm generating these parent pointers there's actually more than one choice for some of these vertices and this is because there are multiple vertices on the previous frontier and some of them explore the same neighbor on the current frontier so a parallel implementation has to be aware of this potential race here I'm just picking an arbitrary parent so as we see here you can process each of these frontiers in parallel so you can paralyze over all of the vertices on the frontier as well as all of their outgoing edges however you do need to process one frontier before you move on to the next one in this BFS algorithm and a parallel implementation has to be aware of potential races so as I said earlier we could have multiple vertices on the frontier trying to visit the same neighbor so somehow that has to be resolved and also the amount of work on each frontier is changing changing throughout the course of the algorithm so you have to be careful with load balancing because you have to make sure that the amount of work each processor has has to do is about the same if you use silk to implement this then load balancing doesn't really become a problem so any questions on the BFS algorithm before I go over the code okay so here's the actual code and here I'm gonna initialize these four arrays so the parent array which is the same as before I'm gonna have an array called frontier which stores the current frontier and then I'm gonna have a rate called frontier next which is a temporary array that I use to store the next front here of the BFS and then also have an array called degrees I'm going to initialize all of the parent entries to be negative one or do that using a silk for loop I'm going to place the source vertex at the 0th index of the front here also at the front here size b1 and then I set the parent of the source to be the source itself while the frontier size is greater than zero that means I still have more work to do I'm going to first iterate over all of the vertices on my frontier in parallel using a cell for loop and then I'll set the the ithe entry of the degrees array to be the degree of the vertex on the frontier and I can do this just using the difference between consecutive offs and then I'm going to perform a prix fixe some on this degrees array and we'll see in a minute why I'm doing this prefix um but first of all does anybody recall what prefix autumn is so who knows what prefix on this do you want to tell us what it is [Music] yeah so prefix some so here I'm gonna demonstrate this with an example so let's say this this is our input array the output of this array would store for each location the sum of everything before that location in the input array so here we see that the first position has a value of zero because the sum of everything before it is zero there's nothing before it in the input the second position has a value of two because the sum of everything before it is just the first location the third location has a value six because of some of everything before it is 2 plus 4 which is 6 and so on so I believe this was on one of your homework assignments so hopefully everyone knows what prefix sum is and later on we'll see how we use this to do the parallel reference search ok so I'm gonna do a prefix sum on this degrees array and then I'm going to loop over my front here again in parallel I'm gonna let B be the Eifert X on the frontier index is going to be equal to degrees of I and then my degree is going to be offsets of V plus 1 minus offsets of V now I'm gonna loop through all these neighbors and here I just have a serial for loop but you could actually paralyze this for loop turns out that if the number of iterations in the for loop is small enough there's additional overhead to making this parallel so I just made it serial for now but you could make it parallel to get the neighbor I just index into this edges array I look at offsets of V plus J then now I'm going to check if the neighbor has been explored yet and I can check if parent of neighbor is equal to negative 1 if so that means it hasn't been explored yet so I'm gonna try to explore it and I do so using a compare and swap I'm gonna try to swap in the value of V with the original value of negative 1 in parent of neighbor and the compare and swap is going to return true if it was successful in false otherwise and if it returns true that means this vertex becomes the parent of this neighbor and then I'll place the neighbor onto frontier next at this particular index index plus J and otherwise I'll set a negative one at that location okay so let's see why I'm using index plus J here so here's how frontier next is organized so each vertex on the frontier owns a subset of these locations in the frontier next array and these are all contiguous memory locations and it turns out that the starting location for each of these vertices in this frontier next to race exactly the value in this prefix some DeRay up here so vertex one has its first location at index zero vertex two has its first location at index two vertex three has its first location index six and so on so by using a prefix um I can guarantee that all these vertices have a disjoint sub array in this frontier next array and then they can all right to this frontier next story in parallel without any races an index plus J just gives us the right location of right two in this array so index is the starting location and then J is for the J's neighbor so here's one potential output after we write to this frontier next array so we have some non-negative values and these are vertices that we explored in this iteration we also have some negative one values and the negative one here means that either the vertex has already been explored in a previous iteration or we tried to explore it in the current iteration but somebody else got there before us because somebody else is doing the compare and swap at the same time and they could have finished before we did so we failed on the compare and swap so we don't actually want these negative one values so we're gonna filter them out and we can filter them out using a prefix sum again and this is going to give us a new frontier and we'll set the frontier size equal to the sides of this new frontier and then we repeat this while loop until there are no more vertices on the frontier so any questions on this parallel BFS algorithm do you mean the filter out yeah so what you can do is you can create another array which stores a one in location i if that location is not a negative one in the zero if it is a negative one then you do a prefix um on that array which gives us unique offsets into an output array so then everybody just looks at the prefix some array there and then it writes to the output array so I might be easier if I try to draw this on the board okay so let's say we have an array of size five here so what I'm gonna do is I'm gonna generate another array which stores a one if the positive a lis in the corresponding location is not a negative one and zero otherwise and then I do a prefix um on this array here and this gives me 0 0 1 1 2 and 2 and now each of these non these values that are not negative 1 they can just look up the corresponding index in this output array and this gives us a unique index into an output array so this this element we're right to position 0 this helmet would write to position 1 and this element would write to position 2 in my final output so this would be my final frontier does that make sense okay so let's now analyze the work and span of this parallel BFS algorithm so a number of iterations required by the BFS algorithm is upper bounded by the diameter D of the graph in the diameter of a graph is just the maximum shortest path between any pair of vertices in the graph and that's an upper bound on the number of iterations we need to do each iteration is going to take log M span for the soak for loops the prefix sum in the filter and this is also assuming that the inner loop is paralyzed the inner loop over the neighbors of a vertex so to get the span we just multiply these two terms so we get theta of D times log M span what about the work so compute the work we have to figure out how much work we're doing per vertex and per edge so first notice that the sum of the frontier sizes across entire algorithm is going to be n because each vertex can be on the front here at most once also each edge is going to be traversed exactly once so that leads to M total edge visits on each iteration of the algorithm we're doing a prefix sum and the cost of this prefix sum is going to be proportional to the front here sighs so summed across all iterations the cost of the prefix sum is going to be theta and we also do this filter but the work of the filter is proportional to the number of edges traversed in that iteration and summed across all iterations that's going to give us theta of M total so overall the work is going to be theta of n plus M for this parallel BFS algorithm so this is our work efficient algorithm the work matches that of the serial algorithm any questions on the analysis okay so let's look at how this parallel BFS algorithm runs in practice so here I ran some experiments on a random graph with ten million vertices in a hundred million edges and the edges were randomly generated and I made sure that each vertex had ten inches every experiments on a forty core machine with two-way hyper-threading does anyone know what hyper threading is yeah what is it yeah so that's a great answers so hyper-threading is an Intel technology where for each physical core the operating system actually ceases as two logical cores they share many of the same resources but they have their own registers so if one of the logical cores stalls on a long latency operation the other logical core can use the shared resources and hide some of the latency okay so here I'm plotting to speed up over the single threaded time of the parallel algorithm versus the number of threads so we see that on 40 threads we get a speed-up of about 22 or 23 X and when we turn on hyper threading and use all 80 threads the speed-up is about 32 times on 40 cores and this is actually pretty good for a parallel graph algorithm it's very hard to get good very good speed ups on these irregular graph algorithms so 32 X on 40 cores is pretty good I also compare this to the serial BFS algorithm because that's what we ultimately want to compare against so we see that on 80 threads the speed-up over the serial BFS is about 21 22 X and the serial BFS is 54 percent faster than the parallel BFS on one thread this is because it's doing less work than the parallel version the parallel version has to do extra work with the prefix um in the filter whereas the serial version doesn't have to do that but overall the parallel implementation is still pretty good any questions so a couple lectures ago we saw this slide here such whorls told us never to write non-deterministic parallel programs because it's very hard to debug these programs and hard to reason about them so is there non determinism in this BFS code that we looked at yeah so there's non determinism in the compare-and-swap so let's go back to the code so this compare-and-swap here there's a race there because we can have multiple vertices trying to write to the parent entry of the neighbor at the same time and the one that wins is non-deterministic so the BFS tree that you get at the end is non-deterministic okay so let's see how we can try to fix this non determinism okay so as we said this is the line that cause done causes the non determinism it turns out that we can actually make the output BFS tree be deterministic by going over the outgoing edges in each iteration in two phases so how this works is that in the first phase the vertices on the frontier are not actually going to write to the parent array of the or they are gonna write but they're using going to be using this write min operator and the right min operator is an atomic operation that guarantees that we have concurrent writes to the same location the smallest value gets written there so the value that gets written there is going to be deterministic it's always going to be the smallest one that tries to write there then in the second phase each vertex is going to check for each neighbor whether a parent of neighbor is equal to V if it is that means it was the vertex that successfully wrote to parent of neighbor in the first phase and therefore it's going to be responsible for placing this neighbor on to the next frontier and we're also according to SAP parent of neighbor to be negative v this is just a minor detail and this is because when we're doing this right min operator we could have a future iteration where a lower vertex tries to visit the same vertex that we already explored but if we set this to a negative value we're only going to be writing non negative values to this location so the write min on a neighbor that has already makes been explored would never succeed okay so the final BFS that's final BFS tree that's generated by this code is always going to be the same every time you run it I want to point out that this code is still not deterministic with respect to the order in which individual memory locations get updated so you still have a determinacy race here in the right min operator but it's still better than a non-deterministic code in that you always get the same BFS tree so how do you actually implement the right min operation so it turns out you can implement this using a loop with a compare and swap so reitman takes as input two arguments to memory address that we're trying to update and the new value that we want to write to that address we're first going to set old Val equal to the value at that memory address and we're gonna check of new vows less in old Bell if it is then we're gonna attempt to do a compare and swap out that location writing new vowel into that address if its initial value was old Val and if that succeeds then we return otherwise we failed and that means that somebody else came in in the meantime and changed the value there and therefore we have to reread the old value at the memory address and then we repeat in there are two ways that this write min operator could finish one is if the compare and swap was successful other the other one is if new Val is greater than or equal to old Val in that case we no longer have to try to write anymore because the value that let's there is already smaller than what we're trying to write so I implemented so I implemented an optimized version of this deterministic parallel BFS code and compared it to the non-deterministic version and it turns out on 32 cores it's only a little bit slower than the non-deterministic version so it's about 5 to 20 percent slower on a range of different input graphs so this is a pretty small price to pay for determinism and you get many nice benefits such as ease of debugging and ease of reason and reasoning about the performance of your code any questions ok so let me talk about another optimization for breadth-first search and this is called the direction optimization and ideas motivated by how the sizes of the frontiers change in a typical BFS algorithm over time see here I'm plotting the frontier size on the y-axis in log scale and the x-axis is the iteration number and on the left we have a random graph on the right we have a power-law graph and we see that the the frontier size actually grows pretty rapidly especially for the power law graph and then it drops pretty rapidly so this is true for many of the real-world graphs that we see because many of them look like power law graphs and in the BFS algorithm most of the work is done when the frontier is relatively large so most of the work is going to be done in these middle iterations where the frontier is very large and turns out that there are two ways to do broth first search one way is the traditional way which I'm going to refer to that as the top-down method and this is just what we did before we look at the frontier vertices and explore all of their output neighbors and mark any of the unexplored ones as explored and place them onto the next frontier but there's actually another way to do breadth-first search and this is known as a bottom-up method and in the bottom-up method I'm gonna look at all of the vertices in the graph that haven't been explored yet and I'm gonna look at their incoming edges and if I find an incoming edge that's on the current frontier I can just say that that incoming neighbor is my parent and I don't even need to look at the rest of my incoming neighbors so in this example here vertices 9 through 12 when they loop through their incoming edges they found incoming neighbor on the frontier and they chose that neighbor as their parent and they get marked as explored and we can actually save some matched reversals here because for example if you look at vertex 9 and you imagine the edges being traversed in a top-to-bottom manner then vertex 9 is only going to look at its first incoming edge and find that the incoming neighbors on the frontier so it doesn't even need to inspect the rest of the incoming edges because all we care about finding is just one parent in the BFS tree we don't need to find all of the possible parents in this example here vertices 13 through 15 actually ended up wasting work because they looked at all their incoming edges and none of the incoming neighbors are on the frontier so they don't actually find a neighbor so the bottom-up approach turns out to work pretty well when the frontier is large and many vertices have been already explored because in this case you don't have to look at many vertices and for the ones that you do look at when you scan over their incoming edges it's very likely that early on you'll find a neighbor that is on the current frontier and you can skip a bunch of edge traversals in the top-down approach is better when the frontier is relatively small and in a paper by scott beamer in 2012 he actually studied the performance of these two approaches in BFS and this was this plot here plots the running time versus to iteration number for a power-law graph and compares the performance of the top-down and bottom-up approach so we see that for the first two steps the top-down approach is faster than the bottom-up approach but then for the next couple steps the bottom-up approach is faster than the top-down approach and then when we get to the end the top-down approach becomes faster again so the top-down approach as I said is more efficient for small frontiers whereas the bottom-up approach is more efficient for large frontiers also I want to point out that in the top-down approach when we update the parent alright that actually has to be atomic because we could have multiple vertices trying to update the same neighbor but in the bottom-up approach to update to the parent or it doesn't have to be atomic because we're scanning over the incoming neighbors of any particular vertex V serially and therefore there there can only be one processor that's writing to parent of V so we choose between these two approaches based on the size of the frontier we found that a threshold of a frontier size of about n over 20 works pretty well in practice so if the frontier has more than n over 20 vertices we use the bottom-up approach and otherwise we use the top-down approach you can also use more sophisticated thresholds such as also considering the sum of out degrees since the actual work is dependent on the sum of out degrees of the vertices on the frontier you can also use different thresholds for going from top to bottom to top down to bottom up and then another threshold for going from bottom up back to top top down and in fact that's what the original paper did they had two different thresholds we also need to generate the inverse graph where the transpose graph if we're using this method if the graph is directed because if the graph is directed in the bottom-up approach we actually need to look at the incoming neighbors not the outgoing neighbors so if the graph wasn't already symmetrized and we have to generate both incoming neighbors and outgoing neighbors for each vertex so we can do that as a pre-processing step any questions okay so how do we actually represent the front here so one way to represent the frontier is just to use a sparse integer array which is what we did before another way to do this is to use a dense array so for example here I have an array of bytes the array is of size n where as number vertices and then I have a 1 in position I if vertex eyes on the frontier and 0 otherwise I can also use a bit vector to further compress this and then use additional bit level operations to access it so for the top-down approach a sparse representation is better because the top-down approach usually it deals with small frontiers and if we use a sparse array we only have to do work proportional to the number of vertices on the frontier and then in the bottom-up approach it turns out the dense representation is better because we're looking at most of the vertices anyways and then we need to switch between these two methods based on the based on the approach that we're using so here are some performance numbers comparing the three different modes of traversal so we have bottom-up top-down and then the direction optimizing approach using a threshold of n over 20 first of all we see that the bottom-up approach is a small slowest for both of these graphs and this is because it's doing a lot of wasted work in the early iterations we also see that the direction optimising approach is always faster than both the top-down in a bottom-up approach this is because if we switch to the bottom-up approach at an appropriate time then we can save a lot of edge traversals and for example you can see for the paragraph the direction optimizing approach is almost three times faster than the top-down approach the benefits of this approach is highly dependent on the input graph so it works very well for power law power law graphs and random graphs but if you have graphs where the frontier size is always small such as a grid graph or a road network then you would never use the bottom-up approach so this wouldn't actually give you any performance gains any questions so it turns out that this direction optimizing idea is more general than just breadth-first search so a couple years ago I developed this framework called my Grove where I generalized the direction optimising idea to other graph algorithms such as betweenness centrality connected components sparse PageRank shortest paths and so on and in the library work we have an edge map operator that chooses between a sparse implementation and in dense implementation based on the size of the frontier so sparse here corresponds to the top-down approach and dense corresponds to the bottom-up approach and it turns out that using this direction optimizing idea for these other applications also gives you performance gains in practice ok so let me now talk about another optimization which is graph compression and the goal here is to reduce the amount of memory usage in the graph algorithm so recall this was our CSR representation and in the edges array we just stored the values of the target edges in sort instead of storing the actual targets we can actually do better by first sorting the edges so that they appear in non decreasing order and then just storing the differences between consecutive edges and then for the first edge for any particular vertex will store the difference between the target and the source of that edge so for example here for vertex 0 the first edge is going to have a value of 2 because we're going to take the difference between the target and the source so 2 minus 0 is 2 then for the next edge we're going to take the difference between the second edge and the first edge so that's 7-5 7-5 and there similarly we do that for all the remaining edges notice that there are some negative values here and this is because the target is smaller than the source so in this example 1 is smaller than 2 so if you do 1 minus 2 you get a negative negative 1 and this can only happen for the first edge for any particular vertex because for all of the other edges were encoding the difference between that edge in the previous edge and we already sorted these edges so that they appear in non decreasing order okay so this compress edges re will typically contain smaller values than this original edges array so now we want to be able to use fewer bits to represent these values we don't want to use 32 or 64 bits likely like we did before otherwise we wouldn't be saving any space so one way to reduce the space usage is to store these values using what's called a variable length code or a K bit code and idea is to encode each value in chunks of K bits where for each chunk we use K minus 1 bits for the data and one bit as the continue bit so for example let's encode the integer 401 using 8-bit or byte codes so first we're gonna write this value out in binary and then we're gonna take the bottom 7 bits and we're going to place that into the data field of the first chunk and then in the last bit of this Chuck we're gonna check if we still have any more bits that we need to encode and if we do then we're gonna set a 1 in the continue bit position and then we create another chunk where we place the next 7 bits into the data field of that chunk and then now we're actually done encoding this integer value so we can place a 0 in the continue bit so that's how the encoding works and decoding is just doing this process backwards so you read chunks until you find a chunk with a 0 continue bit and then you shift all of the data values left accordingly and sum them together to reconstruct the integer value that you encoded one performance issue that might occur here is that you have when you're decoding you have to check this continue bit for every chunk and decide what to do based on that continue bit and this is actually unpredictable branch so you can suffer from branch mispredictions from checking this continue bit so one way you can optimize this is to get rid of these continued bids in the idea here is to first figure out how many bytes you need to encode each integer in the sequence and then you group together integers that require the same number of bytes to encode use a run length encoding idea to encode all of these integers together by using a header byte where in the header byte use a lower 6 6 bits to store the size of the group and the high highest two bits to store the number of bytes each of these integers needs to decode and now all the integers will just in this group will just be stored after this header byte and we know exactly how many bytes they need to decode so we don't need to a story continue bit in these chunks this does slightly increase a space usage but it makes decoding cheaper because we no longer have to suffer from branch mispredictions from checking this continue bit okay so now we have to decode these edge lists on the fly as we're running our algorithm if we decoded everything at the beginning we wouldn't actually be saving any space we need to decode these edges as we access them in our algorithm since we encoded all of these edge lists separately for each vertex we can decode all of them in parallel the and each vertex just decodes as edgeless sequentially but what about high degree vertices if you have a high degree vertex you swap to decode it's actually sequentially and if you're running this in parallel this could lead to load imbalance so one way to fix this is instead of just encoding the whole thing sequentially you can chunk it up into chunks of size T and then for each chunk you encode it like you did before where you store the first route value relative to the source vertex and then all the other values relative to the previous edge and now you can actually decode the first value here for each of these chunks all in parallel without having to wait for the previous edge to be decoded and then this gives us much more parallelism because all of these chunks can be decoded in parallel and we found that a value of T where T is a chunk size between a hundred and ten thousand works pretty well in practice okay so not gonna have time to go over the experiments but at a high level experiments show that the compression schemes do save space and see really it's only slightly slower than the uncompressed version but surprisingly when you run it in parallel it actually becomes faster than the uncompressed version and this is because these graph algorithms are memory bound and if you're using less memory you can alleviate this memory subsystem bottleneck and get better scalability and the decoding part of these compressed algorithms actually get very good parallel speed-up because they're just doing local operations okay okay so let me summarize now so we saw some properties of real world graphs we saw that they're quite large but they can still fit on a multi-core server and they're relatively sparse they also have a parallel degree distribution many graph algorithms are irregular in that they involve many random memory accesses so that becomes the bottleneck of the performance of these algorithms and you can improve performance with algorithmic algorithmic optimizations such as using this Direction optimization and also by creating an exploiting locality for example by using this bit vector optimization and finally optimizations for graphs might work well for certain graphs but they might might not work well for other graphs for example the direction optimization idea works well for paralog routes but not for road graphs when you're trying to optimize your graph algorithm we should definitely test it on different types of graphs and see where it works well and where it doesn't work so that's all I have if you have any additional questions please feel free to ask me after class and as a reminder we have a guest lecture on Thursday by Professor Johnson of the MIT math department on and he'll be talked about high level languages so please be sure to attend you 