 So I'm a computational biologist. And my group develops algorithms for a wide range of problems that arise in computational biology. If there's central theme though, is we've looked at a lot of methods for looking at biological networks both from the point of view of prediction and also from the point of view of analysis. And I thought would I tell you about today our methods that we've been developing to help think about disease and disease networks. And this is largely the work of Shilpa Kobren and Borislav Hristov, two recent grad students in my group. And also some of this is in collaboration with my colleague, Bernard Chazelle, all right? So we're now in an era where high throughput experimental methods abound in biology and they've resulted in these really large and diverse and growing data sets. The types of data that arise from them are really pretty varied. And I'm really talking about computational molecular biology here. So they can range for example from sequence data, representing genomic or genetic information to a three-dimensional shape data of some of the molecules that are key in molecular biology, to high dimensional measurements of cell state, and to what I'll talk about today, various types of biological networks. So because of this vast amount of data, the very different qualities of this data, the fact that the data can be noisy, it's really important that we develop interesting innovative methods to really be able to analyze this data in order to uncover biological insights. Now depending on the applications, there are different challenges that can be dealing with scale or they can be dealing with trying to extract weak signals. And I'll tell you a little bit about some of the things that we've been thinking about along these lines. So I'm going to tell you as I said about one type of data primarily and that's biological networks. And I want to give you a little bit of introduction of biology. I know many of you already know some biology and some of you may not. And so just in the interest of time, I'm going to make some simplifications. And so I apologize in advance if you already have detailed knowledge of this and know some of the complexities that I'm just skipping past. So the networks that I'm going to be talking about are molecular networks. And in these networks, you have nodes, which correspond to molecules within the cell. Here I'll refer to nodes that correspond to proteins or genes. And I'll mainly refer to proteins and I'll use these terms interchangeably. And nodes within these networks have edges between them if those proteins have been found to interact with each other. Now these networks are useful abstraction for what happens within in each of our cells. And then of course, our ourselves are made up of many tens of trillions of these cells. So if we have an understanding of us and how we work both in a healthy state as well as in what might go wrong in the case of disease, it's really helpful to have an understanding of what's going on at the network level. OK, so one key thing though about networks and what can happen is that sometimes these networks vary or they are altered. So for example, if one of the nodes in this network corresponding to protein p3 is mutated, this can have an effect on the network. This could knock out the protein or maybe it just removes an edge. And these mutations can have major consequences. So for example, they can underline underlie diseases. And one disease where this kind of process takes place it's cancer. So cancer is a disease where our own cells accumulate mutations that allow them to grow uncontrollably. And so these mutations that occur are changes that happen to ourselves within cells. So given that we're now in a regime where we're being able to figure out what these mutations are that occurred within these cells, one thought process has been, well, maybe we can just look at these cells within a person's tumor, figure out what proteins are mutated, and maybe that'll tell us something about what are the causes of the disease. What initiated the tumor process or what contributed to the growth or progression of this disease. But there's a problem with this. And that problem is when you look at any one tumor and you look to see what proteins are mutated, you can see many proteins that have mutations. Now those of you who know something about cancer know that, of course, other types of alterations can occur as well. But here I'm really going to focus on mutations that occur within proteins. So even though you have many possible mutations that maybe you observe, most of these mutations aren't really involved with the process of cancer initiation or where the process of cancer progression. So just a small number are disease-relevant. So the reason, of course, that we care about this is because of its impact. Of course, not just understanding the basic biology of cancer cells or understanding cancer, but also from a treatment point of view. So if we were able to identify proteins that when mutated are causing cancer or contributing to cancer progression, then that provides something that a drug company can maybe target to try to inhibit this process. And if there is such a drug, you get this personalized treatment. And if not, you would get radiation and chemotherapy. Now through a lot of painstaking experimental work over the years, we actually know a lot about the processes that underlie cancer. And we also know a lot about the players or proteins that are cancer-relevant. But very importantly, this knowledge of cancer relevant proteins and mutations is incomplete. So what I'm going to talk about today is going to be looking at taking a network point of view and thinking about cancer and other diseases and see if we can use this information to try to uncover cancer-relevant proteins. So even though we figure out which proteins are mutated within a single person's tumor or cancer, actually now we have mutations data for tens of thousands of tumor samples. This is the last 5 to 10 years that this has happened. And each individual whose tumor has been sequenced has different proteins mutated even if we're looking at the same cancer type. And as I said earlier, only some of these mutations are involved in cancer. And so one base level idea is that those mutations that are actually cancer-relevant should recur. And so a very basic type of analysis you could do is look at all the data for all these individuals together and look at these individual proteins. And you say how many times is this protein mutated across the tumors of these individuals. And if it's mutated more than some threshold, would say maybe that's a protein that's cancer-relevant. And the trick here actually that I'm obscuring and won't really talk about much more in this context is this is the hard part is figuring out how to set up this background probability, OK? So these methods certainly have some power but they have weaknesses as well. So if you order proteins by how frequently they're mutated across tumors and then you show their mutation frequency, you'll see a distribution where you see that there are some proteins that are mutated with great frequency but there are many more and many proteins that are mutated at lower frequency. Now some known cancer-relevant proteins are apparent in this top list. But even in this long tail, there are still proteins that are cancer-relevant that appear. And moreover, some people's cancers will only have things that come from here. So you can't just look at things as simple as frequency of mutation. And so what we want to do is try to discover from this long tail, these genuine cancer-relevant proteins. Closer. You said the tail has relevant protein? Yes. Is there another one [INAUDIBLE]? Yes, it does. And so some of the things that you think about with significance analysis, you try to think about background models that could maybe get rid of some of those things but definitely if you just look at mutation frequency, there are many things that are going to be irrelevant as well. For example, a protein can accumulate mutations just at a very long protein, OK? So what I'm going to be talking about are network-based methods to identify proteins that are relevant for cancer initiation or cancer progression. And I'm really going to tell you about two types of problems. These are very complementary approaches. The first is mutation patterns that change interactions. So that change these networks and maybe that's what underlies the formation. And the second of the same goal that I'm going to look at mutations in the context of networks, OK? So as I said, we have our biological network where our nodes are proteins and edges if they interact. But what I didn't tell you yet is these proteins are molecules that can be represented as strings over 20-letter alphabets. So protein 3 maybe is represented by this string. So when we talk about protein 3 being mutated, it can be mutated anywhere along the string. So maybe we have one mutation here shown by this orange triangle and maybe that one causes an edge to appear. We might have another mutation and that can cause an edge to disappear. And we may have another mutation and that actually may have no effect. So one very important point is that when you look at this one-dimensional representation of a protein, it's only certain positions that are important for these network interactions. And I want to say when you look at a protein and the functions, proteins do basically everything important in a cell but their functions depend on the interactions that they make. So if you add interactions or you change interactions or you break the protein down altogether, you're affecting the protein network. OK, so certain positions are more important for interactions. And we assume for now, for a little bit anyway, that let's say we have these positions, we know which ones are important. Then our idea is to not treat all mutations the same but we're going to try to predict cancer-relevant proteins as those that accumulate more mutations than expected in positions that affect interactions. So that can be an even if this protein doesn't have that many mutations overall across a whole bunch of tumor samples, if it has more in the positions that affect interactions then that's an evidence that there's a functional change that's going on, OK? And the analysis segment I described for this first part, we're going to end up doing this analysis per protein. So roughly 20,000 proteins in human. We're going to look across 10,000 tumor samples with a total of 1.5 million mutations. So some of the issues that come up here are the difficulties or choices we have to make with respect to the modeling but also issues of scale and getting things to run in a fast enough time, OK? So we started our modeling like this. So we have our protein sequence, as I said it's a string. And I said, assume that we know which positions within them take part interactions. Well, that was a bit of a lie. So instead, we don't have that for every position across the protein maybe we have that for subregions. And in some cases, we'll really know binary. Yes or no this participates in an interaction or not because you looked and seen a 3D structure. But in other cases, we've done some inference and we figured out these scores are these weights between zero and one, which reflect how likely mutations in those positions are to be involved in interactions. So we have our protein, we have a modeled region which is a subset of these positions. And for each position in this model region, we have this weight in zero and one. So there's our preposition interaction weights. And we're only talking about one part of the printing sequence. So now we're going to look at our data. So we're going to look across all these samples and see where did the mutations fall within this protein. So in this case, we had three mutations across our samples and they all fell in this one position that has weight 1. And we compute a score, which is the sum of the weights where the mutations land. So more generally, we would have n mutations across the individuals that fall into this model region. If z sub i is the interaction weight of the position where the mutation lands, then the score is just the sum of these weights. So in this case, the score is just three. Now is that a big number or a small number, OK? The thing you do in computational biology to determine this is based on doing permutations. So what we would do is say, well, you can take those mutations and just shuffle them across this region. And you do it multiple times. And by doing that, you build a distribution. And you could ask where the score is with respect to this distribution. You can compute a mean and a variance and you can either computer p value or you can compute the z-score. And so that's the standard thing that one would do and want something that's done all the time across many different applications in computational biology. But we observe that this permutation model, the shuffling model actually assumes that these mutations are all independent of each other. So that means the zi, which correspond to the weight to the positions that they land are independent and identically distributed. So it turns out that when mutations, when you think about where mutations arise, they don't fall in each position equally likely. But let's say we have some background probability that the mutation affects a particular position, this can come from a variety of ways, biological knowledge. Then we can compute the expected value of this score. As usual is a sum of the expected value of the individual z sub i. But because these mutations are independent, we can also use the same trick too, we can split up the variance into the variance of the individual components, OK? So what that means now is that instead of doing these permutation calculations, you can just compute these values analytically, all right? Yeah. What are the zis? Again, they have nothing to do with the z-scores? No. I'm sorry. They don't. They're just the weight of the position where the mutation landed. So in this case, each of these is one because the weight here is one. So weights are divided by lambda? OK, the lambdas is just the probability. You can ignore that if you want. So if you want to think about it, their each positions. And you have these mutations and you're throwing them into the positions, right? And they're all independent of each other. Now you can assume that they're going to fall uniformly across each position but it turns out that they don't exactly. Some positions, just a bit more chance of getting mutated than others. And these lambdas reflect that. So it turns out that we have not just one measure of positional importance. So that's what I just showed you. And so what I showed you is that said, you can figure out if you have more mutations that fold these high scoring regions than you would expect based on these calculations. But we actually have multiple of these regions. So maybe we have this region and it has its own scores, these weights. And we have a third region which is that has its own weights and we want to know when we look at the mutations that fall into this region, do we tend to fall into high scoring, high weight positions? If we look at this region, do we tend to fall in height weight positions? Et cetera. And those are sometimes what we call interaction tracks. These are places that are involved in interactions. But we also have other information we have that tell us about positional importance and how likely mutations in those positions would be to disrupt functioning and network functioning. So maybe we just have some regions that we say, this is just an important region and we just have binary weights for each thing. You either fall within this region or not. And when I compute, when I look at the mutations across the length of the protein sequence to have an enrichment that fall in this one area. Then you can use the same formulation I just talked about before. And then we also can have these positional importance not really based on interactions but based on other type of data where now you have a weight for each position along the protein sequence. And for those of you who comes bioinformatics side, these would come, for example, from conservation scores. And you would say, when I look at mutations, do they tend to fall into positions that are more highly conserved and thus under evolutionary constraint? And that's more likely to be functionally important. So we have all these different lines of things we want to consider but the question is, how do we consider this information together? So we can't treat each one separately because these regions can overlap with each other. So we have to do something else. OK, so let's look at two at a time. For example, these two regions. Now we have our protein sequence as usual. Now we have two modeled regions, again, shown here is contiguous but they need not be. For each of these regions, we have interaction weights, v sub i for region V and w sub i for region W. Let's say that m mutations fell in V and n mutations fell in W. This time we'll say that yi are the weight to the positions where m mutations and v fall. So we're just reading off these bars when we look at the mutations. And the zi are the weights to the positions where the n mutations in W fall. The scores are defined as before. And we're just summing up these weights. And the expected values and the variances are also computed as before, OK? But we need to take into account that these two regions overlap so the mutations that fall in these two regions aren't completely independent. So we're going to consider what happens when you look at these two regions. We look at their intersection and that's this region x. And we can break apart the scores for each of these regions based on the part that comes from the overlap part and from the remaining. So this is S sub v. This is the part that comes from the overlap. And this is the part of the score that arises from mutations that hit elsewhere. Is that because of linear inverse notation? You don't need that. This is just talk about what the score is. So the score is the sum of the weights where the mutations land. Now I'm saying, here you can actually to split it in two pieces, the part that's in the non-overlapping part and the part that's in the overlapping part, OK? And so we're going to consider the permutations. So when we do the permutations here, in our model, we're going to consider the case where we do permutations. That's not what we have to do but we're doing permutations, It's our back row model, where the number of mutations that fall in the overlap region is what's actually observed, and that's q. So then when we look at the background model of what the covariance between these two scores would be if we had these randomizations, well, the covariance of these two scores is equal to the covariance of the portions that arise in the overlap region, everything else is independent. And then you can do some calculations and this whole thing simplifies to really simple term, OK? So now we have a way of figuring out whether you have an enrichment in these different lines of positional evidence and we also have a way of evaluating significance when we consider these different lines together. So putting it all together, we're going to have these different lines where we have these background models. We can analytically compute this covariance matrix. And this suggests that our background model is a standardized multivariate Gaussian. We're going to look to see what these scores are with respect to each line of evidence. And then we can see based on our calculations where this overall falls on this distribution, OK? So when you say that permutation test-- [INAUDIBLE], is it the total number of regions that you have? Yes. It really varies a lot per protein. So it can be 100, it can be just a handful. So it's like for some proteins, we know a lot of information and for other proteins we know less information, OK? So one important thing is by doing these calculations in this way, the calculations actually yield really significant speed-ups. So here we use just a single measure of importance, actually, the one that use the whole sequence length. And we look to see the improvement, the fold improvement and runtime over a baseline empirical permutation test where we just do one permutation. And so you can see as we consider, so whether we consider all the tumor samples or we consider a smaller number, we see roughly a seven-fold speed-up by doing these calculations, OK? Now the important thing though is you typically don't just do one permutation of this test to get these significant values if you weren't doing the analytical calculations as you would perform 100 or 1,000 permutations. And so you actually get a 7,000 times speed-up, all right? But actually, if we have n correlated measures of importance then you actually have order n squared sets of permutations that you're avoiding here. And so I'd say that permutations are completely infeasible when you have multiple measures of importance in the protein. You really need to do these calculations. So doing these calculations makes the complications that I'm telling you about possible whereas it wouldn't be possible before. So the overall approach which we call pertinent that comes from perturbed and interactions and hopefully also pertinent for cancer. So for each protein, we have multiple, perhaps, overlapping regions that are modeled. Each position or a model region has a weight associated with it reflecting its importance. And we're really asking our mutations falling into high weight positions. In practice, actually, the frameworks extended to consider mutations weighted in some way. So we consider the score of mutations falling into each model region and determine enrichment by comparing it to a distribution of scores if they fell into the region at random. And then finally, we compute a per protein score that considers these per region enrichment scores together and this gives us a combined z-score, OK? So in our work so we have this kind of modeling that we did, we have this method that we developed, but in the end, we want to know, does this method really work on real data? So what we're going to do is rank the proteins from highest to lowest and say, what do we see? Well, here we're going to look as we consider a larger number of proteins. So just working our way down the list. What is the enrichment of known cancer-relevant proteins? I told you we already have some knowledge. Do these proteins that we know were cancer-relevant, do they fall near the top of our list? So I didn't tell you about the different kinds of information we have. But for all of these different types of information we have separately, all of them see a clear enrichment in our list. So starting off at 25-fold and still at five-fold as you get down to 200. And importantly, for each type of information, you also have multiple lines that you're considering. But if you put all this together, you can see that you get a benefit from combining very different sources of information, OK? So this whole approach took just 10 minutes to process but 10,000 tumor samples. And as I said before this would be infeasible to do if you did the standard permutation-based test. Now it's a little hard to figure out the back of the envelope calculations but a rough idea for us would be more than a year if you just did these calculations just relying on these permutation tests, OK? So to have this framework is actually really essential, OK? And I'm not going to talk about this too much. But everything that when you rank your list, there's some things that are what we already know and there are other things which are new. And these could be false positives, things that aren't cancer-relevant, or these could also be novel predictions. And so in our case, we get a bunch of novel, what we call novel predictions. And one thing that I didn't have time to really tell you about is that for each of these regions, we actually have functions or what types of interactions associated with them. And so we can go back at our list and say, what type of interactions are perturbed when we look across the data set, OK? So I just described pertinent to you. I skipped all the information, all the different data sources we had to bring together to get these per position weights of importance. Instead what I described to you was saying let's assume we have all this and have done all this work, I gave a general approach to integrate multiple measures with these fast significance calculations. I told you that we're able to successfully uncover proteins with perturbed interactions in cancer. And that this was highly successful when we did validation tests. And I didn't show you too much but this is able to detect both frequently mutated genes, proteins, as well as those that are infrequently mutated but whose mutations fall in interesting places. Now I want to tell you about mutations in the context of networks. So if you look at biological networks, they have structure. And in particular, proteins with similar biological functions tend to be near each other in the network. And so here I'm showing functions by color. So this is red color and this blue color, OK? And so what that means is that function can be altered by mutations in one of many proteins. So if we want to interfere with cellular growth or change it in some way maybe like the mutation here or maybe there could be a mutation here. So the last bit is that proteins that are relevant for a disease often have related functions. And that's because a disease tends to hit the same or disrupt or change the same functionality. So unlike the first part of my talk when I looked at the proteins and said are the mutations falling in interesting places, now I'm going to say, when I look at these mutations and look within in the network, are there regions of the network that are interesting? Can I ask a question? Yeah. When you see [INAUDIBLE] functionality, the causal effect here is just the disease that's just really targeting a functionality? Or is it just because it's targeting the same functionality that it becomes classified as a disease? So let's talk about this maybe cancer or one kind of disease. So it's thought that certain types of functionalities need to be disrupted for cancer to happen, OK? So the cells acquire mutations and they tend to hit those functions, OK? And so I think getting into the molecular underpinnings of the diseases is a little hard but typically you do start with shared phenotypes and try to uncover what the causal effect might be for those shared phenotypes. So you could argue that cancer is a disease or that it's really a collection of many distinct diseases that have different causes. OK, so what we're going to do is we're going to look at a protein interaction network. And this is a more tip, I drew these little cartoons, but this is a more typical view of what these interaction networks might look like. And as I said, that proteins that take part in the same functional processes tend to be within regions in the network. And then I'm going to look at our tumor data across individuals and list out where they have mutations. I'm going to map this information onto the network. And here our idea, here our formulation was, well, if it's that you can affect the functionality by affecting any of the proteins that participate in it maybe what we want to do is use the network and identify neighboring proteins that maybe individually aren't that frequently mutated but together are mutated frequently across individuals. And so you have many choices here how you might try to implement this. And I'll tell you about I guess two approaches in a way. And I'll start with the first one, OK? So here's now back to the cartoon where things are a bit more manageable. You have a list of individuals and you have the proteins that are mutated within those individuals. For each node in the network, we label it with the patients. So this is a protein, this is a protein. We label it with the individuals where that protein is found mutated. So this protein is mutated in individual 1, individual 2, and individual 3. And we want to try to do is we want to try to look for a small subnetwork. And what we decided to be our criteria was to look for a small network that covers the most patients. And by covering I mean that the subnetwork contains a protein that is mutated in that patient. So there's this natural trade-off between the size of the subnetwork and the number of patients that are covered. So you could choose all the proteins in the network. You'd, of course, have very high coverage of patients but you're losing any sense of proximity, nothing, you're just outputting everything, not uncovering disease-relevant proteins. You can identify on the other hand very few proteins in the network that are proximal to each other. You have lower coverage of the patients. But the connective component is more likely to consist of functionally-related proteins, OK. So our formulation is that we're going to try to find a connected subnetwork g prime of g that minimizes two things that's balanced by this single term alpha, or on the one hand, we want to minimize the fraction of uncovered patients. And on the other hand, we want to minimize the size of the subnetwork that we're outputting, OK? So we initially solve this using integer linear programming where we have a binary variable if a node is selected to be part of the subgraph x sub j. And we have another binary variables that's one for each node. We've another binary variable for each patient that tells you if a patient is covered. So our objective function, again, is we to minimize the fraction of uncovered patients and the size of the subnetwork and balanced by this alpha. It's really easy to set up your constraints so that your patient is covered if one of its mutated proteins are selected to be in this subgraph just by this constraint here. Similarly, it's also very easy to set things up so that your patient is not covered if none of its proteins are selected. The hard part actually is trying to figure out how to build a subnetwork and maintain this connectivity. And so here we adopt a flow of commodity technique introduced by Bob Turgeon and others to ensure connectivity. So what you do here, I'll just give you the intuition is you make these edges bidirectional, you inject flow into this subnetwork. You let each node drain one unit of flow as things go. And then eventually, if all k units of flow are drained then you know these k nodes must be connected. And so you can write these equations to set this up so that you're keeping track of what goes in and what goes out, all right? So this performs well but it's time-consuming and we decided for-- and by pouring them out, well, I mean in terms of finding cancer-relevant genes. We run this many times, we decided we'd do something much simpler. And we ended up going just with the greedy heuristic. So we pick a starting protein greedily and add it to the subnetwork where we're building. And then for each protein that's in the subnetwork, we examine its neighbors and the neighbor's neighbors and then we add to the subnetwork the one or two proteins that improve the objective function the most. And then we just repeat this process until we can't add anything else. So this has reasonable performance as compared to the exact optimization but of course, it's much faster. So there's one part I didn't cover yet, which is we have this trade-off parameter, this alpha. And how do we set this parameter alpha? We could look in a crystal ball which we don't really have or we can let the data speak for itself. And so here we borrowed ideas from type of validation that you would do in machine learning type settings. So this is not machine learning but we're going to use some of the same type of ideas. So we're going to assign this parameter alpha. What we're going to do is withhold 20% of patients as a validation set. And then using the other 80%, we're going to run the greedy procedure on our network and we're going to output this subnetwork. And then we're going to say, are there proteins that we output, are they also covering the patients that are in this validation set. So if we output a good subgraph that really covers a lot of patients, it should also cover patients in this validation set. And so that's what we do. So we vary the value of alpha and we ask what fraction of patients are covered. And we do this multiple times. We do it for a training set shown here in blue. And then we check to see what you get, when you just look at the training set, how that performs on the validation set. And then we automatically pick a parameter when these two things start to diverge. Because after that you're just building up your sub network but you're not really learning anything useful. So you're putting it all together, what we're doing is we have nutritional data, we have a network, we annotate the network with each node with the patients where that protein is found mutated in. We do this procedure to automatically find the alpha. And then we actually don't run this procedure just once, we run it multiple times where we hold out 15% of the patients at a time and each time I find this connected subgraph. And then finally, in the end we rank order our proteins by the number of times they're found over all these runs of the algorithm, OK? So now again we want to do evaluation. So this is just an example on kidney cancer, where we looked at 400 patients. On average, 22 proteins mutated per sample. And we looked at the HPRD network. And again, we're going to say, what happened as you go down our list? And in this case, we're plotting the fraction of proteins that are in the known cancer cells, a set of known cancer proteins. And we're going to consider our version or our formulation, which we call nCOP for Network Coverage of Patients. We're going to compare it to a version of itself that ignores network information altogether, just deals with set cover. And then we're going to compare it to just the frequency-based type of approach. And so what I want you to look at is red, which is nCOP as compared to blue which is the set covered approach. Is just the greedy algorithm for set cover? Yes. So I take that back. This is the greedy one. And for this actually we use ILP going up with larger values of k until things are covered. So that was actually exact, OK? So the difference between these red and this blue is the difference in performance you get by considering network connectivity in addition mutational data, OK? And you can see that it performs better and of course, it performs from frequency based method as well. One way to quantify this more is you can look at the error under each of these curves because an ideal case, this would be sitting-- you'd have very, this should be sitting up high, you look at the area under the curves and compare the two. And so we do that. And so we compare it on this slide on 24 different cancer types shown in these rows, a total of 6,000 samples. And we saw how much better. So this is a log fold change in this area when we compare nCOP to set cover, which is the version of itself that ignores any network information or to a frequency-based approach. And you can see that it performs better than that set cover approach across all 24 cancer types and better than the frequency-based method in 22 and 24 cancers. And these two cases where we're not doing as well correspond to cases where I think very few genes sufficed, OK? So we also compare it to a previous network-based method just to show that it's not just the fact that we're using mutations in the context of networks but that we're doing it in a right formulations. So coming up and thinking about the right formulations is also key here, OK? So to summarize this part. I've one last part after this in case you're getting too hopeful. So networks provide a context within which to interpret these per individual mutations profiles. So we didn't aggregate the patient data, we really thought about doing coverage here. We try to find these small subnetworks where many patients have mutations in at least one component protein. So in this case, we don't require that all the proteins have to be mutated heavily but because we're looking at subregions within the network, we're able to take advantage of the fact that we have this structure and these networks where proteins that interact with each other tend to participate in the same process and we can identify infrequently mutated cancer-relevant proteins as well. And I showed you in the last few slides that this is highly effective in practice. So for the last part of my talk. Yeah? How much harder does the problem get if you say I want to have not just one notation communication but [INAUDIBLE] application? Yes. So to extend the coverage approach to do that, we haven't looked into that but I think that would certainly be a very interesting thing to do. People have looked at things that are similar formulations not with mutations but with expressions and in other disease context. And so you may be interested in looking at that. But the details are really quite differences, a bit distinct. For even like mutual information? Yeah. OK. So the last part I'm still going to be talking about discovering disease proteins. This is again, in the context of networks but the methods I've told you about thus far are really considering what are we doing with new experimental information, for example, proteins that are found mutated in cancer patients and trying to figure out which of those are the important ones that we should think about as potential drug targets. But there's also this other information which is this prior information. And these are proteins that are known to be disease-relevant or in this case, cancer-relevant. We've used this primarily as a way to do validation but maybe we should be using this type of information together as opposed to just as validation. And that's what I wanted to think about, OK? So it's been established for many years now that disease proteins can be discovered via propagation of signal within networks, OK? And so you can have prior knowledge. So maybe that these two genes are involved in some disease or you can even look at new information and say, that maybe these two are things where we think or evolve a disease and we try to spread a signal through the network, OK? So we're going to try to propagate information. And there are many established propagation techniques for this. Bonnie mentioned that I work on propagating information through networks in the early days, we used a flow-based techniques. [INAUDIBLE] Rohit? Yes. That was good. And Rohit and Bonnie as well. And then there are more modern methods that people use as well. So for example, one way which you can propagate information in these networks is through diffusion where each disease node pumps in some flow, the flow diffuses uniformly across the edges, and each no drains flow at some constantly. And you can figure out how this propagates or diffuse a the network. There are also strategies based on random walks with restarts, which Bonnie also looked at and many other people have with that as well. So the idea here is you have a walker who is moving in a network. And with probability 1 minus alpha, the walker looks to see what's next to it, what the interactions are, picks one, and moves there. But with probably alpha, the Walker ignores the neighbors and instead goes back to one of a set of initial nodes. And so with this formulation that's nice because you can compute for each node, this Q sub u which is the probability that you end up at this node in the limit. And this restart probability lets you, of course, keep track of some local information, you won't just lose it as you walk along and do these random walks, OK? So that's the same as personalized page rank? Exactly. So personalized page rank, the restarts could be anywhere, sorry, the regular page rank could restarts could be anywhere. In personalized page rank is when you have a subset of words. X intervals. Exactly. So there is exactly that. That's one kind of way in which you can propagate information in many settings and it's been used widely in biology, for example, by Bonnie. And so you can take your prior knowledge. So moving back to think about prior knowledge and say these are disease-relevant nodes. You can run this random walk with restarts or diffusion or some other method. And in the end, you're going to get the probability by which you appear that you would end up in any one of these nodes, OK? And so this Q that you can determine gives a ranking of the nodes and it's shown that it's predictive of disease proteins, it's turned out to be useful for functional annotation, it's had many applications in biology, OK? But we also have this new information. So this could come, for example, from known cancer genes. But maybe we also see that these genes are mutated at some frequency across tumors. So now we want to interpret this information. So previously what people did with said, well, why don't we just also do this kind of spreading of this information? But we'd like to think about doing this propagation using what we already know from this prior knowledge, OK? And so what we do is simple but effective. And so what we do is we use these Qs or the station of distribution of this walk to set the transition probabilities in this network. So when you do this random walk with restarts now, you're now more likely to walk towards these more orange nodes, OK? And so we're going to perform these guided random walks with restarts. And so now then we're going to look at the limit there and we're going to end up with a distribution that tells us how likely we are to end up in any of these nodes. So basically, our guided random walks starts from nodes revealed by the newly acquired experimental data. For example, this could be proportional mutation frequency or it could be from the first part of the talk if we determine in different ways. We're going to move to a neighboring node probabilistically based on this Q that we've determined. Essentially, we're going to move to it proportionally from its value as compared to its neighbors values. And then we also have a restart as usual with probability alpha. And then the stationary distribution of this gives the final ranking of the proteins, OK? So when we do the guided random lock with restarts, there's restart probability for the guided random lock plays a really critical role. When alpha is equal to zero, the new information is entirely ignored. When alpha is equal to one, the prior knowledge is ignored. When alpha is between zero and one, we're balancing new and prior knowledge. And I'm just going to talk about alpha is equals 0.5, we've tried other things. And then also importantly for a fixed alpha, we can also perform an unguided walk. So this is just using new information. It's performing these walks but we're ignoring prior knowledge, OK? So in this case where we're ignoring prior knowledge, we're just looking at mutations all frequencies. OK, so the testing here is a bit tricky. But we start off with a set of known cancer-relevant proteins. We've tried different parameters or different ways of testing and what effect it has on the number of prior knowledge, et cetera. So we fix a test set of randomly selected known cancer proteins. So we're going to measure performance with respect to this test set. Then from the set of cancer-relevant proteins that don't contain this test set, we're going to repeatedly sample our prior knowledge. And then we're going to repeat this multiple times, OK? And so now again we're going to get the same type of plots that we've looked at before where we look at an increasing number of proteins considered. We're going to ask what the fraction of proteins that are in the set of known cancer genes. And here I want you to compare this blue line, which corresponds to our approach, which combines prior and new knowledge. So this line here compared to the purple line which is when you are taking a random walk just from the new knowledge but you're ignoring the prior information already. So you can see that these approaches are all predictive but this is the benefit that you get over just taking unguided walks, just considering rotational frequency, or just considering prior information. So is there a new thing that you're doing [INAUDIBLE] lazy random walks where it's separately proportional to [INAUDIBLE] where it's just staying where you are? No. Lazy random walks are another type of random walk but this is distinct. So some of the diffusion-based approaches are more closely related to the lazy random walks. So the new thing that's happening here is we're doing two phases of walks. One where the walks come from propagating prion information and the other where the walks come now instead of just doing a random walk on the network, our walks are biased. And they're biased by what we already know. So it's possible I think. We could think about how we could do this maybe in a different way. But the idea is to use both types of information together. It's possible that other random walk-based techniques are also effective in this type of setting but the idea is to bias the walks using the prior knowledge to guide it, OK. So here again we're looking at 24 different cancer types and we compare how much better our combined approach does compared to the unguided approach. And so this the log full change in the area under the curve that we just showed. And we can see that it does better in all cases. We can also compare to just the case where you're propagating information from the prior information. So this is a class of techniques that people have used frequently in the past. And we can also see what happens compare just mutation frequency. So in this particular case-- so this setting is pretty general. For most the talk, I focused on cancer, but we've also looked at other diseases in this setting as well. So for example, macular degeneration which causes blindness, epilepsy which most of you know. ALS also known as Lou Gehrig's disease. And in this case, we're going to have our prior knowledge comes from known disease genes from this data set called OMAM. And the new knowledge comes from newly suggested or implicated genes through association studies. And again, we can say how much better does this combined approach, we're using both prior information and new information do as compared to looking at just prior information alone or versus new information alone. And you can see that for all three diseases, you're doing better in both cases. So combining both prior and new information is important for performance, OK? So just to conclude then. And here we developed a general framework for combining prior knowledge and new information on networks. I want to say that as we've already heard about through questions and stuff, these ideas of walks in networks are used all over the place not just in biology but elsewhere. But in the framework of biology, they've also been used extensively to do protein function annotation. So this idea is really a general framework that's not just applicable to disease. I applied it in this case to cancer but also other heterogeneous diseases and showed that you're able to recover known disease-relevant genes or proteins. Importantly, this type of approach outperforms using either just prior information or either just new information. And I didn't have time or didn't include slides to show that we actually can see what happens as you consider larger amounts of prior information and to see how performance changes or how you might change alpha in those settings as well, OK? So with that, I have told you about three different methods for looking at disease mutations in the context of networks. The first part of my talk was work that was largely done by Shilpa Kobren. And in the second part of my talk I told you about methods for looking at disease mutations in the context of their larger networks. And that was largely the work of Borislav Hristov. Thank you. 