 T hi s is a T e st . This is a Test. This is a Test. This is a Test. This is a Test. This is a Test. This is a Test. This is a Test. This is a Test. This is a Test. This is a Test. SPEAKER: Welcome to this Welcome to this community day . SPEAKER: Some parts of the wall it is definitely not Some parts of the wall it is definitely not winter. It is winter here. Definitely. SPEAKER: When we have to go outside again , it should be worn. Warm. SPEAKER: I work here and I'm a big fan and you might have seen Amsterdam , we were joined with the University and one of the community members . We are happy to be hosting here today. SPEAKER: I am a community advocacy program manager responsible for Western Europe at Microsoft and we are hosting this together. And the people behind the cameras the cameras that we are looking at and a lot of volunteers who spend their time making sure we have wonderful sessions today. Making sure we can do this today and we also have our tea and coffee and everything we will need. SPEAKER: So people are doing this and hopefully the book website , they contract with us . There is a live chat on either side . Speak Speak  You can reach us on Twitter if you want to chat further. So if you are joining us from your work from home office or anywhere that will be fine. SPEAKER: 's divide. We have a competition running, there is a link there. People can go to the link on the screen and register and start completing challenges over modules on my Christoph Learn and there are prizes to be one, I believe  Did you hear about that? You will need to talk about it? We definitely want me to talk about it. Complete modules on Microsoft learn and you will appear on the leaderboard and winners will be picked randomly. There is no sense driving us on Twitter or anywhere. It is randomly chosen and you will receive a Casual or -- as your care package. December 14 we will have listed the winners on the page and you will then share your details with us and will send your care package.  We also have a code of conduct on the website in the top right corner so familiarize yourself with that and follow all of the rules and be kind to one another.  That sounds good. It was for people interacting in the live capital software. I think we should get started.  I think so. We have covered all the admin.  The house rules.  The last me a CD someone challenged our first speaker to get into the tech and she is here to tell us about get help actions from cognition IG. She founded the group of community women in tech so Esther, welcome to the show.  Thank you for having me. And for allowing me the opportunity to be the first speaker of today.  We are happy to have you. The floor is yours. OK, I will start sharing my screen and then we can have some fun. As I was introduced perfectly by the mark, and floor, I will try to get help actions today. Just to make up for a little bit of time Just to make up for a little bit of time because I wanted to pack the presentation with a lot of demos I will skip through the let's get to know Esther part of this presentation. As he already mentioned, I've done community programmes focused on dev ops and automation. I've moved to cloud for the last year using rest APIs, Jason, and power show. Today I want to focus on cloud tops, get hub actions, and Windows virtual desktop because that is currently my end user because that is currently my end user computing environments that I want to automate and deploy. As you can see. As you can see. And I promise to include some demos. First of all cloud them up, is there one tool to rule them all? Basically, that is the idea that I started with when I switched or drove into dev ops. when I switched or drove into dev ops. If you look at the definition of dev ops it is mostly a set of practices. That basically means that even though I translated it to certain tools like power shell scripting or using Windows virtual desktop for deployment and operations I started with my Azure Jeff ops to make sure I fully could understand and embrace the dev ops culture. And as I mentioned, this presentation will mostly focus on power shell and Windows virtual desktop. I did quickly realise, if we are talking about dev ops we are talking about a lot of different tools for a lot of different stages in the entire dev ops process. There is basically not just one tool to rule them all, it is more, in my opinion, all about the journey or the adventure that you can have in the fund that the journey brings in automating and taking on some challenges. If we are talking about challenges, I definitely have to go back to the previous edition of the virtual Azure community Day in July for one simple question basically started a whole new adventure for me. The last time, I already talked about Azure automation and how I'm building customized templates using the Azure power shell module. Driving into Azure automation and even tiptoeing as your dad pops. There was this one question at the end of the presentation (Video plays)  I will let you know about my experiences.  Me before the next one we can have you come back and you can tell us about your experiences will stop  That would be amazing I would love to do that.  So welcome to part two of the cloud Davos adventure. Let's dive into get hub actions because I eagerly took on this action. If we look is a tool that will help you If we look is a tool that will help you automate tasks within your software development lifecycle. I do have to admit I am more and operations girl so for me I am still finding what tooling and processes to use for my infrastructure as codevelopment. The cool part about it have actions is that they are event driven so you can run a whole series of commands after you specified the event that will trigger. So zooming in it all starts with that event. A specific activity that triggers a workflow. That can be a web hook or scheduled or even a manual trigger. The workflow itself can contain one or more jobs. In each job is a set of steps that you execute on the same runner. That basically sets up your environment. If we are zooming in on the steps these can be individual actions and if we zooming in on the smallest part we are talking about actions. You can either create your own actions or you can use those provided by the get help community. The other thing we need to know is that the workflow syntax uses why AML. That is something you definitely need to dive into. To get you started you have to get your workflow files from the work. There which means that they will be ultimately picked up automatically and I will show you how that works in just a second. I did intend to include an entire speakers in the presentation but as I wanted to make sure that it was fully demo pack, I'm just going to skip through these slides real quick and ask you to freeze the recording for second to really read over them and they just assumed that you finished your speed course. Because I want to focus on what the real fun is. Those workflows. And how you can build your own actions workflow for instance in my case to deploy your Windows virtual desktop environment. First of all, I mentioned that actions can be either created by yourself or can be community actions. There is a whole marketplace that offers over 6000 already prebuilt actions. actions. So please check it out before you start diving into creating your own actions because, for what you have in mind, there probably is already an action available. The other thing that you need to be aware of when you start with actions is that you can use the secrets, and secrets are configured within your settings allow you to import sensitive information in an encrypted manner so make sure that you're not exposing any secrets, security information straight into the workflows. There is a special secret that I wanted to get your attention on and that is the Azure credentials one. I will show you in a minute where to use it, but the cool part about it is that you can copy your Jason information file when you create your service principal for Azure. And use that information, use citations stringing your secret and that will automatically be picked up by a login Azure action. Super Union on Windows virtual desktop, and I do realise that not everyone has infrastructure for Azure computing background. for Azure computing background. Make sure you all understand Windows Make sure you all understand Windows virtual desktop, I will include a little crash course on that one. Windows virtual desktop mostly consists of a part that is managed by Microsoft which we usually refer to as the control plate. And of course there is this part that you control within your own subscriptions. That is mostly focused on desktops and remote apps. Because you want to make sure that you have your session hosts within your environment to make sure that you can send that virtual desktop were that virtual application to your end-users. These demos today will also focus on those windows Virtual Desktop components the session of the host will that contain them. If were talking about Windows Virtual Desktop and creating a pool, especially when it comes to creating the session host, there are two parts that you have the most influence on in the configuration. The first one is your virtual machine size which you can select any virtual machine sizing that is available within your region. The other cool part is that you can specify the image that your session hosts will be based on. This could be a standard image provided by Microsoft from the gallery but it can also be your own custom image that you have uploaded to the shared image gallery. The template will be used to run your build or your VM builder your image filled in once it has built that image, depending on the output it will push it to the shared image gallery. Going back to my action, as you can see it does take quite some time for as your Image Builder to build that image. This is me just getting nervous and double checking if everything is going as planned. So for now, zooming in, our image definition in the shared image gallery, is showing that it is creating an image with a specific version. You can even see that in that resource group some of the resources changed in the virtual machine that is temporarily used is replaced by that image. So everything goes right, as your Image Builder should be finished up pretty soon or I should have forwarded the video to the end results. Fortunately for me, everything went successfully. I can even check out details of the different steps that were performed. One of the things that Azure image builder action tells you is the name of the template. That makes it easy to ensure that the template was created. Once again, going back to the resource group and checking hidden types, and then that image template is available. So you can even reuse that and -- in follow-up calls. Scrolling down to the output that it generates, first of all it will tell us whether or not the template was built successfully. And of course it gives us the URI to that image which is distributed. So just double checking because we already saw that it was creating that version and now we can actually see that it indeed is succeeded. It is basically my first LAO as your Image Builder world action and got the result I wanted to quite an easy way by using that prebuilt community action. But this is just the first step because as I mentioned at the beginning I want to automate my WPD deployment I specifically want to be able to automatically make a host pool and use that just created image for my session host within that pool. So I also created a new workflow for the creation of a WVD host group once again using variables instead of using a prebuilt action for an arm template deployment. I wanted to have more flexibility I wanted to have more flexibility in the actions that I am performing and basically by using the option or the prebuilt action for Azure PowerShell script, I can just reuse that script that I already created and copy that is an in-line script for this action. This script will read those environment variables to get the input automatically. It will create a template parameter object that I can use with my arm template deployment. Let's just look at the action. First of all, this is Windows Virtual Desktop and I am checking the host tools. There is already one that I want to create this new host pool. this new host pool. So that is what my action is going to do. Once again, it is going to be run manually, and as you can see this is the naming of the host pool and the number of session hosts to create within that host pool, etc. So first of all, as I mentioned I am directing it or going to use that shared image gallery image definition. Then, zooming in on the job, once again running it on Windows because that is where I feel most comfortable, using the Azure login action using the Azure PowerShell action with that in mind script. Just going over the steps are quick, once again it is going to read the environment variables, retrieve even my sensitive information from key full, I even specified a postfix in this case and I am specifying all the input parameters for my arm template deployment with a template perimeter object. Making sure I am pointing to the right image definition. And because I am not having a version to it, it will always pick up the latest version. My arm template is also stored in GitHub so I am making sure I'm downloading the latest version of the template from GitHub on my runner and then I run my ARM deployment. So that looks like a pretty good reuse of my already tested PowerShell script. So let's check out GitHub again. As you can see, the YAML file is there so I can easily select that workflow and use that manual trigger to once again start the workflow. Once again, it is cued and then by consuming on the progress of the job. job. Logging into the Azure environment and then running my Azure powers shell script . Because we cannot see the progress of that script while the action is run, I am just doublechecking within Azure to make sure that the AIM template deployment is actually started. This is just me making sure that everything else is going as planned within my WVD environment. and deployment. So this is a virtual machine on the session host being created and that means that my deployment should be finished just about now. So, yes! So, yes! That is good news. Once again, I have a GitHub action performed successfully. What does this mean? It should by now, I can see the deployment is completed, it should have created my WVD environment. environment. I should have a new house but I will check. It should contain one section -- session host as I specified in my template and it should also have an application group created for the desktop and to make sure that users can connect to the Remote Desktop. Just to make sure that I can immediately test access and the content of the desktop, I am just going to add one of my ninja helpers and then slide it to this desktop. -- Desktop publication. He or she should all be ready for some testing and I am making sure that I add (inaudible) because that will make it easier to find your right environment within the Remote Desktop client. Talking about the Remote Desktop client, that is the easiest way to connect to your Remote Desktop. As you can see, I already asked my little ninja to help me out and to set up the Remote Desktop client. It now has access to that Virtual Desktop and can start it after authentication. can start it after authentication. As you can see, or at least as I immediately see, my customisation scripts run successfully with the Azure image build process and that added acrobatic -- Acrobat Reader to this image which is not standardly provided by Microsoft. Suffer damage -- several demo purposes checking that my customisations are actually copied to the local disc of the runner or the Azure image builder created. Now, I ultimately -- automatically can create an image, upload and use my WVD deployments to automatically have a full environment . One of the other things. I am doublechecking whether I have enough time for this. One of the things that this allows me to do as I mentioned before with the creation of the host pool, is that I can specify an image definition. If I do not specify a version it will always use the latest version and this allows me to basically keep the action separate that builds my image because I can use it to rebuild a new version of that image. Then I can use an armed deployment to add additional session host to my host pool and these session host will be based on the latest version of the image. So they will have a new image available. Once again, I am using a power shell script for the armed deployment to make sure I can run that. This is to show you that I used the image builder already to create a new version of my image. It only has some slight adjustments. Going back to the house pulls -- host pools , making sure there is only one session host available and in this case I am going to for demo purposes change the drain modes. That does not allow any new connections to the session host. This will ensure that I can fade out or phase out my current session host with an older built -- build image and then introduce those new session host which I will create within my host pool that will be based upon the image. I am checking with the in-line power script which we will use basically the same mechanism. It is reading environment variables but in this case I want to use the session host which is available. So that I can build a more dynamic script that will have the latest details without me having to specify that. I am even retrieving the VN VNET and subnet information because those are the and subnet information because those are the variables for my (unknown term) template. Once again creating a template update but this time I am using all the dynamically retrieved information. And I discovered that there are differences -- difference -- different parameters without using a gallery image or custom image. So I am taking that into account as well. As I mentioned previously, the ARN templates are in my GitHub repository so I am downloading it to make sure that I have the latest version and then using that with my ARN template deployment. Once again using the latest ASAP model. This is the last workflow in my getup environment and as you can see it is once again directly available. So once again I can Mandy started for demo purposes and Zoom in on the action once it is in progress. Logging into Azure and running the template to plummet again. The only thing that ending this is a direct feedback that I get with Azure (unknown term) but I know my way around so I know what to look for with the different tasks that I started and I do know that I can keep track of my AOM template deployment to my resource group. This is creating that second session host, once again joining it to the domain because that is required for Windows Virtual Desktop and it is pushing additional components like the WVD agent. The job is finished and we can see that as well with the deployment information within Azure, and if we go back to the host pool it now has two session hosts. One of them has drain mode on and the new session host is immediately available with drain mode off to accept new sessions. So asking my ninja once again to test my Virtual Desktop , authenticating , and I do have to and I do have to do a quick disclaimer that I did not make a lot of customisations in this new image version. So it is a new image so I do get a new profile in this case and as you can see I basically mostly change only the background image in this deployment. Just to show you that customisation folder is once again copied , you can see that it is always using the latest version from my GitHub repository . In this case I added a new desktop in the background. And this is the script that was performed by creating the install. That is a partial script that I run. -- Power shall -- power shell -- power shell . This was a little show of my getup adventure and a nice addition to my entire cloud of ops adventures and I wanted to make sure that I not only present my results but for those of you who are interested in automating your WVD environment, that I like to create I get up repository with the -- get up -- GitHub repository. So feel free to check out the So feel free to check out the repository because it does not only contain the workplace and the demo scripts and the scripts are used in this demo and the scripts are used in this demo but it will also contain all the power shell script that I am using with the Azure module . I will upload those and keep those up-to-date but I also focused on direct but I also focused on direct (unknown term) calls, including configuration files to pre-configure your post (unknown term) environment and the power shell scripts I'm using for those direct API calls. I made sure that I finished right on time so they should be still time festival to say thank you and to wish you all a wonderful virtual Azure day . Thank you so much ! I was trying to type that URL, could you post it in the chat later on? So that I will show it. It went a bit too fast for me because I was still grabbing my laptop but thank you for creating that for people. That is a huge help. Also, I really appreciated the quick run through ofYAM Y AML, I think it could have gone slower from my co-host here! Many learning to many things the same time!  I cannot multitask!  I will make sure I upload the PDF of the slide deck as well so that people can just go through the recording to get the information.  If you could share that on the chat and Twitter, we will make sure that is retweeted so that nobody is left out and that everybody can get started learning this. It has been really cool. Obviously a lot of the questions were about the shakedown for you because you have done this for DevOps and using pipelines so if you needed to shakedown, which one would you rather use? Or maybe for which his case does it matter?  Wow! I am convinced that the whole DevOps experience for the whole journey is all about finding those tools that you feel most comfortable with. Because I do not believe as I mentioned at the beginning, there are -- there is one wrong -- that is one tool to rule them all. I will not repeat it again! For me, as you can see, power shell is still the most comfortable scripting language for me using a Windows environment using a Windows environment is the most comfortable environment for me . If I look at my Azure DevOps expense, I felt very comfortable using the GUI and done using the (unknown term) points. Now I am a YAML expert! Or maybe I'm just a Google expert.  Are we all?  Not sharing all secrets today. For me, I do have a slide included that compares DevOps and GitHub pipelines but I kept it limited you to the top. The only advice I have is to try out different tools and try out those that feel most comfortable to you and support the translation of manual and support the translation of manual tasks or manual workflow to automated tasks and or manual workflow to automated tasks and workflow.  Great. I have one more question. I have one more question. My love? Timewise? -- Am I allowed? You mentioned there are a lot of actions that already live on the marketplace in GitHub and you should check those first GitHub and you should check those first before writing your own. Solid advice. Is there an action you found that you thought was curious for the funniest or weirdest that you came across?  I have to admit, I was very focused because of time pressures to deliver this challenge so I did not check out all 6000 or more.  You did not?  You did not? That is not research.  That sounds like something for the next VA CD.  Let me record this led and make sure that I earned my next spot on the next virtual community day.  This is like the never ending story. To use Susanna's words, deal. We have at least one talk for the next VA CD, apparently we are doing a another one, you heard it here first.  How to find curious actions in GitHub actions marketplace.  I love it. Done. Thank you so much Esther. I will try and recover. While we invite the next speaker to the stage. From GitHub actuals will talk about GitHub actuals will talk about containers and communities next, which are this shiny thing developers are attracted to and all want to use Because of YAML of course. Your favourite thing in the world. This also leads the ops department to do with things they're not too familiar with and give ops the fundamentals and tools in hand to make sure they can operate these things. We have a Microsoft Azure MVP and lead Azure architect with a lot of acronyms that follow. Because he's also a Microsoft certified trainer and a certified information systems security professional. Just rolls off the tongue. You will give us the fundamentals right on containers and communities? So there you go. Whenever you are ready. I love your background. Thank you! Good morning first of all, thank you for having me. My name is Wesley, I have a bunch of certifications, also I'm definitely with you on the YAML thing it was never my thing and probably never will be but we had to do with it. So that is what we are doing.  I feel like this is going to be a red thread through the day. Fred? Get it? Please take it away before it gets any worse.  What I want to do today is lunch a deep tack demonstration, I want to take people with me on a story that I've experienced with me on a story that I've experienced with customers, with people in the community on what we see with communities. So is a developer only thing, is it an ops thing, who is being left out? I do have a bunch of unpopular opinions that I will run beforehand.  Content warning this.  No ops were harmed in the making of this presentation. I think it will make for a good story and I just want to share my view on the world of communities with everyone.  Cool. Go ahead.  Let me share my content.  Dropped the needle.  No.  There we go. So our title is held! Our developers are using (unknown term) and that is what I see a lot of people wondering what to do with. Were you go from there? Were the problems you are going to face? Was there may be a different way we could go about this? I have already talked about me, my name is Wesley if you have any questions you can put them in the chat if you have questions afterwards you can send me a message on Twitter, I will be happy to help you and happy to respond. Happy to share my opinion. The agenda for today, we're going to look at a common scenario and look at the problem, and of course are going to look at the solution. And give you some tips and tricks and advice and what we need to take into account in your organisation start using this programme. I do have some opinions, but just a warning that rocks were harmed in the making of this presentation. Before we start, we go back to a more basic question or culture question which is absent vs. ops. So if you save container technology and So if you save container technology and the need to run to you need organisation and you're going to need to deploy different services, who's going to manage that? services, who's going to manage that? What I see is that that problem becomes more evident when you start using container technology because it feels like a big gray area but eventually it is not. So that is what I want to talk you through today. Let us first take a look at the worst case scenario. scenario. I'm not saying that this is bad for everyone or this is the default scenario, but it is something you can come across a review will feel a bit familiar. How it all starts, developers start losing container technology, they have written out, maybe they have some stuff running on PRA and. Then I want to scale it and that goes faster, skills better, that goes faster, skills better, and has more abilities to do what we want. It is very accessible so they start using container technology, they dive into it, they start using it and then the ops team is wondering what is happening here, what are they doing? Because the investor has put a lot of time in using the container technology will just say "don't worry about this, we have got this, we will just wait and see what happens". Then we have this popular meme and you see a few weeks later that is where the problem starts. Developers say that the cluster just went into production and that is where devs vs. props become more problematic because we need to make sure that everyone knows what they're doing but it was a developer driven project so it went to production and that is where the problem begins. That is really what we want to prevent, we want to make sure that everyone knows what they are doing it is going to invest time in learning the right technologies. This is really not the fault of the developer or the operations team, this is management. We need to make sure we plan your stuff before hand. If you're going to invest in technology we definitely don't want this to happen. Of course in the end, everything is going to Of course in the end, everything is going to be resolved what is going to cost way too much pain . And then it begins and if we go back to And then it begins and if we go back to dev and ops, the developers will build up the cars and the officer was also for building a road that is smooth that the cars could drive on. So the ops infrastructure management scaling, deployment, and the developers just build that at the car. The problem here is that you can drive a Ferrari but it is going to perform poorly on a port road. That goes for the other way around as well. You can build the Indianapolis Motor Speedway, but you need proper cars to drive it on. So Cavett vs. ops if we go back to that that is what we see happening the organisation. A lot of people are struggling with this. That is also part of marketing. It is largely marketed as something accessible to developers, easy to get started, and it definitely is, but it did not really for production. You need a team for that, you need to cooperate and collaborate. That is where the problem starts. Like I said, it is accessible, after deployment you can maximize your communities deployments, the developer of the development team has written a review they wrote an application, but either way they put it in a container or multiple containers. It is easy to thus go to the action portal and develop a new cluster and then deploy it. But then some people are left out. Like I said, is largely developed or driven and initially from my ops background I felt that the app was more like development than ops. That is not true, that is something I learned the hard way but what you see is by extension HKS is usually deployed by people using containers for the by people using containers for the development team. That is where that the man that picture came from is that the ops team are usually confronted with something that happened to them. So if something does not work goes into production, good luck with that. This not only Steve is the learning curve, because to be honest giving a basic intro for getting started is not going to happen in 30 minutes. You need way more time for that, you need to invest time and read up on a lot of documentation, and there is a lot of documentation out there. Having a semi-or fully production environment and being confronted with that or maybe a production ready environment, that Stevens -- Steve Eddins -- increases the learning curve even more. You need to think about who it serves and think about who it serves and how it works. how it works. Which makes things even more complicated. So it is better to start from scratch, collaborate, have the ops team on board when it all starts. You want to do this because, as I just mentioned, if someone goes to the portal and only has the basic and required information when it is deployed, that sounds pretty good but then you want to add features. EBV monitoring, review need secret management, maybe you need to integrate something, maybe you need multiple pools. Some things you can do on the fly, but some require constant redeployments which is hard to do if your customers are already using solutions that run on the programme. So initially it doesn't feel more dev than ops, but if you look at the definition from the website, where you can find great documentation, communities also known as K8 S is also known as automating deployment, scaling, and management. And that sounds pretty ops to me. Then we go back to that whole YAML discussion, you have to be able to write that code and if you are a traditional ops team with a traditional infrastructure management, being infrastructure management, being confronted with YAML and managing an environment with that code can be quite challenging. So initially it feels like more of a dead thing, they have to write code, -- gave thing because have to write code that is deftly not. Default AKS deployments are not that good. I don't want to be too negative about that for development and testing it is great before production you're going to miss a bunch of issues. You want way more features than just what This solution goes back into your organisation before you use containers. You different -- definitely need to plan for this . It is a steep learning curve and you need to allow people to make mistakes which is one of the good DevOps practices, you need to have a shared responsibility and you want to have team learning so why not have the ops team on board from the start? on board from the start? Have them play around with it because they need to test and develop as well. They need to develop coding and infrastructure. Definitely want to give them time and invest in training. What I have seen happening is that people just get started and then it feels production kind of ready but then they ran into scaling challenges, they cannot scale, I need a replica set. That is the problem because they need to redeploy your infrastructure and deploy a new cluster so why not start with that? Stop with them for the gaming -- Start with them from the beginning. I do not want to be as direct to say that -- developers to just write a code or publish it to a registry and ops to the rest, it needs to be collaboration and we do need to know which team has which responsibility. If you give the right team the responsibility from the start then they will but that great motor Speedway and developers will build epic cars and it will work. But that is not something I can stress enough, this is what goes wrong so many times. What you can see on social media or in email threats is that it is too complicated, there are too many add-ons, too many features, I need all of this open source stuff. That is not the problem, the problem is people are confronted it with it on the go instead of planning for it. You deftly want to automate everything, you do not want to do (unknown term) configuration manually, you want to have that from the start. That is why you need to start. You need to stop thinking about deploying the cluster and just go back to yes! Go back to your architecture and what you are building and work out what you are deploying. Visit front-end/back then? What software architecture and we using? Is it Microsoft services? Do I want containers? Maybe it is more modernistic? It is all good as long as you know. That will really , really result in your update scenario. If you know what kind of surfaces you are running then you can determine your update scenario and that is where you can actually leverage the power of things like (unknown term) and containers. If you know what you are doing , you can just update the want specific service with a script. specific service with a script. That all comes together when you start building the inventory and identifying workloads. Once you have all that, and the ops team becomes more familiar with the ability , they create YAML definitions on Microsoft DR. SIM BARHOUM: Page, you can just get started from the go and all the technologies you want, it will deftly help you -- definitely help you. You can start to understand solutions. They learn how to scale all the configuration then it can go back to development teams and say it needs to be stateless, that is how containers work et cetera. Is that true or do we need to make adjustments? Then we can get that bidirectional traffic or bidirectional communication and they can learn from each other and not the right solution. They start looking at the capabilities that you need, persistent storage, a service mesh, if you're using Microsoft surface you might want it managed by STO, it is definitely something you should look into. As technologies come after you have identified workers. And then this is probably something you have been confronted with regardless of the technical solution implementing, how much capacity do I need? That is when you start looking at node pools and if I go back to the default (unknown term), you maybe get two or three and played with the scaling a little bit. But what operating system do you want? Do want Dennis containers? -- (unknown term). You have identified workloads and then the behaviour for one workload means that only a little bit of memory is needed, maybe you only need cheap videos -- GPs or CPU . . The note d e pulls a virtual size so they can be a standard (unknown term) or a standard (unknown term) machine for different workloads. If you identify those that you can deploy the right node calls and you can be -- node pulls and equally efficient. You do not want to scale for that peak . If you have a peak moment in your solution and it happens every week, and if it happens once a week then you do not want to have that huge virtual machine running for a whole week all only for those two hours. Or maybe if you have specific workloads -- , then you can have specific what that. The note. Determine how much capacity you have. No. Can be winners and losers. You always have one the next note Paul L i nux n o de po k l o l s. Whatever you want, you can have it if you plan for it. As I have just mentioned, this is where it gets a bit more complicated for traditional ops teams if you have never run with containers or pods, pods are basically the (inaudible). Apart can contain multiple or one container. This is where it also gets complete complicated. If you are using soft surfaces , if it is decoupled then one part will contain one container. Maybe you have got to containers that are coupled that are dependent on each other, and you want to have those two containers in one part. The reason behind this is that pods are also the things that scale. They can be moved. Let us say that we have one note Paul, part one of my solution can use -- can live in the M1 and the other on the M2. If both contain one container, I'm not sure how much I can say container any more but if they both run on one container and are tightly coupled and are dependent on each other then that is networking and a virtual machine and a whole bunch of networking in between those two containers which you probably do not want to have. But as recent -- that is the reason why put two containers into one part. You can also put them together with your development team, they coupled or decoupled? What is it? They scale so if you are stateless which I would highly recommend , if you have one part running and it needs to scale because you have lots of customers are considering -- customers accessing it, then you just have 100 pods or 100 instances of your app running . How can you manage that? We'll come to that in a bit. Be aware that pods are the lowest and smallest deployable unit on Kubernetes. Everything is about pods. What is all that? Like I said, I am not going to talk about everything that you need to know about Kubernetes. Kubernetes. I just want to help you get started and help you with the right mindset and help you deploy the first cluster. What you need to know when you start talking with your development team or your organisation? organisation? What ingress controlled unit? What ingress resources do you need, services and parts? -- Pods? This is what you need to think about before you start deploying. How does that work? When I started with communities, this was like box to me. -- Kubernetes . It was so annoying to understand and then I started mapping it to my traditional infrastructure knowledge and that is where it became a bit easier but you will really be -- read the -- read about ingress, services and pods. In the bottom here we have those parts, the most smallest deployable units. You can see a surface on top of that and let us say this is one version of your app, this pod scale is to a replica of three. Let us assume the status and then you have the service . That service we could call a little balance. It is more than that but let us call it a low balance or a dependable (inaudible) for your pods. Nobody is talking directly to the pods, they are talking to the service. Then we have an ingress. As server -- server can also have a (inaudible). I want to skip the ingress, I can have a service and that will have an IP address and the provision of a load balancer if you are using Azure. But the service does not necessarily have all the security features that you want. Maybe you want to do TLS termination, host recognition , a bunch of stuff before traffic even hits your pod. That is when you need an ingress. The ingress consist of two parts, we have an ingress can -- ingress control and resources. This is where it gets complicated to me because it will forward traffic to my service and it for work. That is what I thought and I was wrong. The Inglis -- the ingress controller is like your firewall or routing. You have a corporate network at home ! Corporate network and it makes is X s available to get network. That comes in a bunch of versions. Depending on what hardware you buy what ingress controller you select, and will also determine the capabilities that you have. Some think we come across a lot is (unknown term) as an ingress controller, we can use Azure Gateway as an ingress if you are doing service management, you can use (unknown term) as an ingress controller. There are a lot out there and I will post a link later in the chat as there is a document that will show you what ingress X, Y, Z will do et cetera. You can select the right thing for your solution. You have selected your ingress controller in this assay have done an application gateway and what it will do is deploy as your application gateway to your Azure environment. Then we have the ingress resources and those are the important ones when you start configuring traffic. The resources are basically like the routers. I have got the route to set up, I have got a piece of hardware, the ingress controller and then I want to configure routes and the route table, the configuration by that you can see the ingress resources. It will say that I have got this hostname so It will say that I have got this hostname so that you can listen to AKS cloud adventures or it can redirect and it were to -- to a termination and give you a certificate, it will forward traffic to the service in the background. That will be the disservice and the possible app. Those determine whether your solution will work. There are other ways to do it but it's Beswick -- it's very basic form , this is what you need to get your application up and running. If the team are already deployed , and AKS environment is when you publish it directly through service. They designed DNS to it and then acts as a pod. you get and development and testing. But there is a solution for that If you don't have the right routing are not able to be with the right security, I need to stress that these four components, understand how they work and then go look into different technologies when you go look at how do I want my stuff to scale? How do I want to use name spaces? How do I want to use event driven scaling? All that comes after you understand this. You can just do this by deploying communities to Azure, get the demo app from the Microsoft page, and it will deploy this for you. You play with it, you tested, you break it, you fix it, and that is the start of your journey when you start experimenting with Kubernetes so please, don't do that next, next, finish clusters of limit. I cannot stress enough, it is going to cause so many problems that you just want to plan before hand . On the other hand, what you want to do is automate everything. I mentioned before we want to have your own deployment script. What you want to do is be able to throw away your Kubernetes cluster, redeployed, and have the same functionality and have your app up and running. Because other than that being a good practice to help you standardize your tractors, it will also provide you a recovery scenario. Because what if something happens in South Europe right to play most often, what if something happens in North Europe or something happens in the US with Anita run for compliance reasons? Then they need it with a different environment, and all you need to worry about is your data. All of that should be scripted and with the push of a button you should be able to deploy it. That sounds like a lot of work, and it definitely is, but it will save you so much time when you actually need it. First things first, you definitely want to learn how to write deployments in YAML. It is hard, but then again it also isn't that hard. I have got a little example here, J -- YAML. But I just showed that ingress service parts. You can see that it is a lot of code, actually it is three different deployments so three different definitions here. Separated by the dashes. I have got an app , deployment, , deployment, is exactly the same as the example that you get from Microsoft. I label it and then I say this employment it contains one container named ASP and ET ATP . image. Then this is where we pull the image from that suppository and will make it available on container 80. So this is the part that watches the unit. Then we have a service and this is like the load balancer. This will deploy that service and what it will do, because he lives in a bit, but we can see here as I have labeled this deployment ASP net app. In here that service will have that. Why is that important? We go back to this picture, I can run one or multiple pods. So how does the service know which pods it can access? Which pods can access for the specific application? That is what I am actually telling the service here. If someone calls you there service, you can forward the traffic to that pod with that. The figure from bottom up, we got the pod, we got the service, and then we got the ingress. The ingress control is not something you defining your deployment, this is just the ingress rule so this can be your routing rule. But you can see here is the host link, if you open a new tab it will probably work. I think I have deployed it correctly. So you have the default path, the root, it will forward traffic to those servers. There are about 84 rules, 84 lines, and this will deploy for different domains and forward traffic to the ASP.net so what is happening is I'm going to demo this and control says they have some rules for that. That means you need to connect to the service and that service publishes pots. So that ingress is not even talking to those pots, it is just talking to the service and forwarding traffic. If I decide to change pots or scale back down, scala backup, introducing pots, maybe with different versions of different labels, that is alright as long as I make sure the that is alright as long as I make sure the service is talking to the right pods with the right labels. I think we are already coming up on time, but stuff you probably want to set up. These are just some best practices you're missing out on. If you do the deployment something to require redeployment. The player cluster with diminished identity enabled. If you do as your community services you probably want to use or leverage additional services. Also enable Azure active directory authentication by default. That is not enabled. That means someone has got to configure file then they can access your cluster. What you actually want to do is enable Azure authentication, there will be an Azure ID and if you're a member of that group and you can access the cluster. You also want to enable Azure monitor, is the click of a button that I've seen a lot of people forget to do that. We deftly want to have insights into your computer work with. So it is running, now what? Is it performing, what is the capacity, Azure has great book -- workbooks built in. I can probably show you some of that in my browser here. So there's the operating, pretty cool, if I go to my AKS and we scroll down a bit this is really scaled down. I can make it a bit bigger. You can see here that Azure A/D integration is enabled, these are the notes sizes amusing, we got the version, because of integration with container insights and that means all the logging will go to a login analytics workplace. So all these workbooks they provide I can also build some alerting based on what I see in my login analytics. Something you want to do which we will talk about with labels, but the importance of labeling is what we saw with the services. I want to have this service talk to this pod, but I also want to keep identifying your work cluster. If your solution starts scaling in your running hundreds of pods, but as every pod doing? Just like tagging and Azure which all those people forget to do, add labels to your app. Make sure you can identify it, and then you can also identify them through the login analytics workspace. So if you're going to have a trigger on "if the app with this label starts jumping up and down I want to have an alert in as short monitor". That is something really simple but really valuable. Also disable the Kubernetes dashboard. Because I was already given to you by Microsoft let me see if I can grab it up on the screen. I probably can. The Kubernetes dashboard usually gives The Kubernetes dashboard usually gives you access to a lot of information about the environment but that is already done by Microsoft. So that is the control plate you get Microsoft Azure. He could see the name spaces, you can see the ingress, you can see the pods running, you can even show all the labels. You can filter all that. So that is basically on top of your Kubernetes environment. Then what you also want to do is integrate with entertainer registry. So you can add additional security there so maybe you want to do scanning on your images. As your container registry and then integrate with Azure cable. All that stuff is what you need the identity for. Out-of-the-box, there are still things enabled in the portal but as of this session will be scripting fruit tablets, learn yourself some YAML. The last thing I really want to stress, I have seen really large environments with hundreds of pods running . please use labels. hundreds of pods running . please use labels. Please! It is crucial to the success of your environment. In the period of the year you start scaling and you employ multiple staff, the whole thing with containers is "we started small, it works great, let's add more stuff". If you don't label it and in a years time you will have an unmanageable environment. So please use labels. Thank you. Great, what was the last piece of advice again? I did not get it.  Labels, labels, labels. That is great advice. Thank you Leslie, this is a great overview. Perfect. We have quite a few questions, so let's get to some of them. This question, what is your take on updating community clusters in production environments? Do you have some tips on that?  Yes, if you use as your Kubernetes obviously you can play through the Azure portal which works great. Have to say, I'm a bit more traditional about it. I like the whole scripting thing, so I can script, deploy, reapply my old solution, everything. I tend to do that as well so deploy the cluster with the new version, see if my solution is running, if I don't run into any troubles and it works then I update that scenario.  OK. And then truly wise, you mentioned scripts, you're talking templating or Terraform, or PowerShell?  I think that is a matter of personal opinion. I'm not a big fan of Azure PowerShell because it is missing some functionality. Azure CLI works amazing. ARM templates, they were great as well but if you want to have some logic in your deployment they can be a bit challenging. But maybe a bicep in the future will be the solution to that because you don't I don't want to end up with ARM templates or hundreds of thousands of rules. It might be a bit easier. But we did not talk about today things like package managers. things like package managers. You start with YAML, you start learning that, but then you want to scale the planets coming what on me coming would have version on your deployments and that is when you start looking at stuff like ARM.  Cool, thanks. I have one other question from Mark, "given heavy supportive committees, is the adoption of a swarm eight fruitless exercise or is there still compelling reason to do so? "  I have to say I played around with it but I was never an expert at it. I prefer Kubernetes over all, one for all of the community projects out there so if you want to do event driven scaling, for projects we have services like SCO, the integrate so well into Kubernetes. Just repeat your perspective, I prefer Kubernetes. There's nothing wrong with (unknown term) I just never got into the nitty-gritty of it.  The deep. Gritty. Get it? I didn't catch that.  Yesterday there was commotion on Twitter with the docking Kubernetes, can you tell us what effect that would have the casual user of DAPR for being integrated? Yes there was talk of talkers being integrated, and that is not necessarily a bad thing happening in my opinion. It is more efficient but I do not think people should be scared. It is something you can test I think for a lot of scenarios it will work out of the box without any problems. I think what was less fortunate was the morning, which some people on Twitter played really well. A lot of it is standardized on doctor containers or doctorate technology so I think it was a bit of shock with people read that. Maybe that could be communicated more carefully. But I don't see a lot of problems coming from that. There will definitely be scenarios  Do we have a Do we have time for another question? Would you rather go for (unknown term) or (unknown term)?  If you are looking at Kubernetes , if you are looking at certification then it will be good to deploy from scratch. I'm not saying use the deployment scripts that are provided, just the technology behind it. The API, et cetera and the controller. To set those up. That will definitely help you understand how the mechanics work.  Thank you very much.  Thank for having me!  Have a great rest of your day! What is next? The days of dragging development levels or over. Affordable cloud tools are a reality. Here to explore and environment with Windows desktop , Azure et cetera is a developer in the Microsoft VP. Callum also organises community events to global conference and roadshows. He is a contributor to open source projects and speaks at events and is a co-host of a YouTube series. Welcome, Callum!  How are you doing?  It is not like an online conference were somebody is listening!  Do I get the point of being the first one I need today? -- On mute? Fantastic!  Loving the seasonal background, by the way!  I cannot take any credit for that, that was entirely my girlfriend! I am loving that we have got went away as well! -- Winter where e ar e ar .  We are ready for your talk! Please go ahead.  Fantastic. Thank you for having me back at the virtual Azure community day. As Mark said, I am going to talk to about breaking up with your death machine. -- Dev machine. This is a sensitive issue because the computer you work on day-to-day is a core part of your life. We are going to talk about maybe changing things up and what the future holds as developers for us. My name is Callum white, I am a Microsoft MVP and Umbraco M VP . And because it wasn't happening, I love to travel the world and speak at events, organise them and enjoy community life. It might be that you started coding on a machine like this. A spectrum or one of the machines from the 80s. This was a fantastic machine, an innovation that was fantastic. When we look ahead to the 90s you probably had a setup that looks like something like this with Windows 95 or Windows 98. This may be how you started to learn to code all your professional life. That was 10 years later and you can see how things evolved. things evolved. If we then jump forward to the 00 s, you probably had it chunky laptop where you had back pain carrying it around. It was thick and heavy. Especially with developers, you would get beefed up laptops with extra memory, extra powerful CPUs and batteries meaning it would be even bulkier. This was always a challenge that I found carrying around laptops. Because I travel I wanted something light and also something powerful. Then to present day when we have things like the surface book or the MacBook Pro which are powerful machines with superthin technology. Incredible power underneath them, 12 hour plus battery life, really capable developer machines. It is up to you what to use as your developer machine. For me, this is a perfect choice because it is something I can carry around and feel comfortable with. The interesting thing here is that it seems like every 10 years there is an evolutionary shift in the types of machines we use as developers. Back in the 80s we were talking about these all in one apple to machines, in one apple to machines, go to the 90s and you have huge bulky desktop machines that you would not dream of dragging anywhere, they are too heavy and big. Go to the noughties and you have chunky laptops and this was when we were starting to see the value of portability. In the present day where we have these ultra-thin, super portable machines. Every 10 years we are having this change, perhaps, we could see breaking up with our previous way of working. We are on the cusp now of the new change. Let me go through my current setup. I ran a 2018 MacBook Pro with a 4.1 GHz I seven, 32 GB of RAM, I really like MacBook S so I ran Boot Camp Windows 10 petition, into story -- installing VMware, I can run it side-by-side with my MacBook. That means I can do my email is in day-to-day work on Mac OS but my actual coding and all the stuff I need to do on Windows. The primary reason for all of this is because I still work with a lot of.network framework stuff and despite not being deprecating now, the world has not moved on enough yet. I'm hoping that as my work moves further towards.net call or.net five in the aunt and beyond, I can consider whether Wendy should be part of my daily working life. I just visual studio 2019 and I have a SQL Server 2019 instance which I run locally, that is really handy because I can work off-line. Whenever I have been travelling around to events I'd like to be able defy -- I like to be able to fight my laptop work , maybe I am on a train or plane does not have Wi-Fi or anything reliable in terms of internet. This has been an important part of my workflow. Of course, I prefer connecting to infrastructures and a VPN I connected. I use Azure and source control. This is pretty typical This is pretty typical developer setup. Whatever platform you work on, this hopefully resonates. Why do we go from here? I'm happy with my setup and it is portable. I can jump on a train and continue working. I know others will prefer a desktop with multiple monitors. I do not feel like I need that. I am happy working off a single screen. When I have an iPad, I use that separately. So I can use that as a second screen if I need to. I would give my whole life up towards portability. With a whole global situation going on and the whole remote feature that we are looking at, it really just a question of what happens to these computers? Are we going to move further towards ever having a more powerful desktop machine and homes or are we actually going to see people start to prefer the idea of not being tied to anything? They want to be able to move, spend a month in another country and work remotely. The laptop is preferable then. Equally, you only have to look around at the news of the past two or three months when you have got Apple bringing out state-of-the-art machines running their own silicon chips and these things are absolutely blowing certain Intel machines out of the water in terms of battery life and capability. Equally, even the iPad over the past three Equally, even the iPad over the past three years has become an incredible personal powerful computer. You have got accessories we can click on a You have got accessories we can click on a keyboard. This has been the case for a while with Surface . We are starting to get this far more portable devices and then becoming more and more powerful but maybe they do not have the same experiences, Mac OS of course is a desktop operating system, the other operating system on the iPad is not. It is not necessarily the thing you think of when you want to want to write code and need a productivity tool. So where do we go from here? This is a really interesting question. I think with the advent of certain cloud technologies in the cloud becoming ever more present, we are going to start to see the developer machine becoming less relevant. On other days when you have to carry that bulky powerful machine around and you actually just have access to those . All of that power at your fingertips. It will be something that you can just beat up and get on your way. I like the idea that maybe the gateway into all of that is just some kind of shell device in the future. It is an iPad with relative amount of power but the experience is all powered by the cloud. So today I would like to show you some already existing solutions the ways that you can get up and running in a virtualised world. This is using maybe a low powered machine as your primary driver but all having all this power behind you in the cloud. My requirements are that I'd need something powerful, lightweight so that I can travel easily and can fit in my backpack and so that I can work anywhere. I do like working on planes and trains so I do need a solution that can fit all of these needs. Perhaps I am being a bit demanding. But there is definitely some solution out there that will meet these needs. Let us have a look at them. That's when I would to talk about is something called Windows Remote Desktop. When does of course people are unfamiliar with. It runs on single, you can install it on your machine but also as a server and enterprises have been doing this for a long time where they have a central server with all the storage and everything for the whole enterprise, a domain you can just log into and get access to. You can logon to any machine in your organisation. Traditionally that was the single site -- traditionally that works for single site but what we are moving towards is a more remote future. Through Microsoft 365 Through Microsoft 365 is a surface -- the service called Windows Remote Desktop which allows organisations to spin up large competing -- large computing and use that as the on premise infrastructure all hosted in the cloud. They can have a domain, assigned users and tell them what they can log into, make sure that they can like changes and profiles to all of those machines that when a user logs in to get everything they expect. The benefit is that they can login from anywhere they like, even more so with Windows Virtual Desktop you can login through the browser and you can interact with your browser through the desktop. You can even use an iPad app to fire those things up so you can do pretty advanced computing just directly through an RDP type connection. They also have apps and these are a way of a way of running the office suite in the cloud. You can just connect to a single app rather than having to open up a whole virtual machine. This lives in Azure by default and you can configure it all there. I will show you what configuring that looks like. The Windows Virtual Desktop service in Azure is primarily targeted at enterprises but you can use this with a small team as well. We will look at the cost in a bit. You have to create a pool and this is just a collection of virtual machines that you say you are managing under this big Virtual Desktop environment. It means that when a user logs in, they will be directed to any one of those virtual machines and they will have their own bit of dedicated computing power running on one of those machines. When you go in and try to set up an environment you do the usual stuff when you are sitting on Azure resource. Give it a name and she's the region. Currently, Windows Virtual Desktop on as your Mac Azure is only available in the (unknown term) regions and then you can sit set configuration. If you set it to people that means everyone who logs into the organisation can use any service, if it is personal they are allocated to a specific machine. We will just use automatic a stunning Here we can define specifically the spec of the machine. These are just standard Azure VM's. You can choose whatever you want or whatever your budget allows. This is pretty cool because of course Azure has a huge array of different machines for different purposes. If your developers are typically doing, let's say, a lot of AI you could actually give them a machine with a really, really powerful GPU or something extra that they maybe would not be able to get normally in their desktop machine. Perhaps it would just be too inefficient. Choose your machine, you choose a region wanted and then you choose the number of virtual machines you want. In this case, I just got one. Then you give it a name, and you can choose an image, either from storage which you provided. Let's say you have come as an organisation, one image that you want to relaunch all your BMs. You can do that no problem. You can choose an image from Azure zone gallery. In this case, we just want Windows 10. Choose your discs, choose your configuration, the important bits here though is you tie it to a virtual network, of the net in Azure. Meaning you have control of what goes on in your virtual desktop environment. You can configure the subnet rules, you can configure the inbound and outbound rules, you have literally full flexibility over what you can do. That is pretty impressive. This is the kind of stuff you get with on premise infrastructure, but you actually get it all here. Like I say, configure your domain as well. It also choose help the users get assigned to specific VM. So if you want to make sure you are balancing the load across three or four, you can say only two users per VM if you wanted or you can wrap up the VM cited have more powerful VM's but with fewer people , with more people on the commissary. This is a typical one here, I have nothing configured here, it is a very basic setup. You have a concept in here called groups where you can have developers get assigned to a certain speck of machine and our admin team get aside to a lower back meeting you were not going to be paying for huge clusters of service -- servers you will need. Equally, applications are an incredibly powerful tool. These are not virtual desktops, these are virtual applications that are in the cloud. virtual applications that are in the cloud. Like I say, Microsoft office or Outlook, tools that someone must be able to click a button and able to pick up where they left off. I'm going to quickly show you the amount of resources that this creates. I have gotten here and created a virtual desktop environment which goes creates IP addresses, gives you a V-neck, gives you virtual machines in this case I have two. Each gets an IP, you have security groups for each of those. We would literally do everything you normally do managing virtual machines on Azure which is really cool. They are just standard Windows 10 servers running in the cloud. As an example, let's say I'm a developer and I've got Visual Studio solutions I needed to run and I've got to. A.net framework one and a .Mac standard one. -- .net standard one. She can see an open source CMS installed. These would probably be to typical applications I would work with. So I pulled them down here from GitHub, in this case onto my virtual machine on my Mac. On my desktop from . is this going to connect to the wrong thing? Demo gods fail. Give me a second. I got these virtual machines running in the cloud and I can log into those directly from any Windows machine is a remote desktop environment, but also I can log into that from the browser or from a few other things if you are, say, working in an organisation. I have got this configured to use a domain that I have got. Hopefully this will work so I can just login. Connect. Great, so I logged into one of my pools of servers, and I have got here my environment already set up. I've got Visual Studio installed, I have got all the power you would expect from Windows, but it is running in the cloud. it is running in the cloud. I am writing this of course on a Mac, I do not need any special hardware to run or build any D6, the only thing I really needed this case is an internet connection. As a quick example, just to show you that it is working, this is the same solution you saw in working, this is the same solution you saw in my virtual machine on a Mac but of course here I'm running it on a virtual machine and I'm going to put up the.net core solution to show you the kinds of things we're dealing with. So we've got standard .NET Core app which I've been able to build on a virtual machine, I can check back in, and then whenever I go and pick up my connection somewhere else I can pull it up and continue I was working. The one thing, if you are using Windows Virtual Desktop for actual desktop and not for the individual apps, it does not retain the state that you left the VM in. So if you were to fire up another connection to this exact same user on the same virtual desktop network, it would just give you a blank desktop, you would have to open everything again. If you are using apps it does retain the state. So you could fire up word and you would be exactly where you left off. So, like I say, this is pretty cool. You can access it from anywhere and I have full flexibility over that machine. It is just a Windows 10 machine running in the cloud. I guess all the top right knee, in this case it was Visual Studio with a jot or Ed.net frameworks. You define the VM size, you dictate how many users or what type of users you want and HBM. -- On each VM and you can roll out patches and security updates. The important thing here is they have full control over the network. Let's say you have corporate VPN that can already be tied straight into the Vnet that you have got configured any internal constructors or anything the VPN can access. There are a few downsides here, and for my tastes I don't think is going to work. It requires a stable internet connection. Let's imagine I have nothing but an iPad, and that is how I work. When I want to do development, I would have to have an internet connection to fire that up. I cannot start developing on a train or a plane, trains notoriously have very patchy Wi-Fi on planes is almost nonexistent. The organisation does have to patch these PMs and apply updates, just like you would on any other network. That is OK, it just means is a bit of maintenance overhead for the organisation. And for now it is US data centres only which, for me, would not be too much of a problem because performance wise it should be fine, but I know a few companies that maybe deal with sensitive data, this would not be a sensitive solution. -- Sensible solution. So how much does this cost? Well the actual Windows Virtual Desktop service and Azure does not cost a thing, it is just a management service. What you pay for the underlying AVMs, the storage, the addresses, all of that sort of stuff. The base cost would be $70 a month for a pretty small virtual machine, eight gig of RAM and two CPU. You can bring your cost down with the hybrid benefit, but it is starting to look expensive. When you look at that over a year or two years, $1500 over two years, you start to think maybe actually this is not any more cost-effective than running a powerful laptop. Of course you can scale this up infinitely, this is just the recommended configurations that you might want to use. Multiple people can use a single machine, which is very nice. My view is, if you are or have dedicated workstations, let's say some lower powered desktop machines, as an organisation this is a great move to bring some extra power to those users. But you probably would not want to use this as your daily machine if you were traveling around. The next solution I would like to talk to you about, hopefully solve some of those problems. It is something called code spaces. Code spaces started life as a DevOps services visual services code faces I think it was. This is IDE code, effectively, and let's use it on the fly. You can use on the browser, you can interact with your code, and work almost as you would locally. It is very impressive. Of course, VS code is actually built in JavaScript. JavaScript. The application runs on Electron and that does mean you can lift a lot of the shared components there and actually run them in the browser. So you do get some of the benefits of having, effectively, the same IDE that runs on the desktop or on the cloud. desktop or on the cloud. Also when it comes to extensions. We can ask all -- we can install that runs on code space directly and they can be run directly on there. It is a GitHub service currently in preview, a beta that you can apply to and I will have a link for that in a minute. Of course, it would be silly talking about this when I can just show you what. As I mentioned I have got to repose here, a .NET Core repo and a .net framework repo. I am in the code spaces beta so I can watch that with a click of a button. As it is centred around GitHub, you have to have a code in GitHub in order to get started with this approach. You open up your repo, you go to a code tab and here you have a little but in the justice open with code spaces which is really cool. I created a death machine in preparation for this and this is going to spin off in the cloud as you will see my repo inside of the DS code or something that looks very like the S code. What this is doing under the hood is using a container. It has bundled my repo into a container and my development environment into a container. You will see that it maintains state and you can understand exactly what I was doing from where I left off. It has pulled out my whole repo here and I have access to get because it is just a GitHub repo. I have kept the sounds of it as well. It is really cool. I can go in and make a change but I can also I can go in and make a change but I can also run this locally or almost locally which is really nice. Of course I can build code, I can make changes, but wouldn't it be cool if that pulled through to the front end. How about we go.net build. This is just a command line, hopefully something were all very familiar with working with.net. Just going to build my solution, and change directory to .NET Core sample ".net run. This is the impressive bit about GitHub code spaces. You cannot only build but run your solutions that live in this container and actually in the cloud on the internet. I'm not running anything locally here to get this up and running. You will see here my app is listening on port 5001 but of course I cannot go to four person users for now. You can follow any local host links and get this up and running. This is a GitHub URL unit two this which has my app running and listening at it. Just to show you in real time what that would look like, I dropped in! And you can look in real time. This is studio code so you can manage all of your ex extensions here. I have got a C# extension installed because that is I am working with C#. G It e i t L ens is a fantastic solution. Next time I cloud my Code Spaces , all of my settings and everything I had before will be there. will be there. You can also spin up your repo when use ! When you last used it . It is just age that and put a message adding . .. And we're done. Next time I pull down this repo in another environment such as Windows Virtual Desktop, that is going to be there. That is cool. So the benefits of this, it runs in the browser, you do get a choice of Linux or Windows. That looks That does come with a caveat, if you are part of the Visual Studio GitHub Code Spaces trial, you also get access to Windows containers. The benefit of Windows containers is that you The benefit of Windows containers is that you can run slightly more advanced things and I'm hoping maybe.net framework will work one day. It does seamlessly integrate with GitHub. I do love that. The fact that I can jump to arena how to a repo, is truly excellent. I am up and running very quickly and I have a localish environment in the cloud and missing tips. VS Code makes this less painful. You would think running in the cloud with no operating system, I cannot install bits of software I need but a lot of the runtimes and languages you need are in the container already in the Olympics Linux container already. But for the dev tools you need, you can try the container. I am trying out fake shopper apr per , you can install this into a code space VS Code and it runs the program underneath to give you the benefits of this in the town. c lo ud. . There are downsides. Unlike Virtual Desktop, the internet can drop in and out and itself recovers. Every single file that you load up, it was a request to get up to go and get that file, pull it down and equally your local environment is polling the container remotely and you were just in preview of that container running. But if your connection drops out or is flaky, it does not just kill your session, you can keep working and coding. So maybe this is a compromise from it when I am a train I can run this and it will not affect my work too much. The other downside is storage. You have to be careful about what you are You have to be careful about what you are persisting. If the files that you do not want to check into the repository, maybe temporary data, those live in the container and they are great. If you destroy the content you lose all of that. You do need to consider that maybe you need a storage service. This comes hand-in-hand with the next point which is if you have a database or any kind of any type! Any kind of external service you need to integrate with, you cannot really integrate here. You do need a cloud service. GitHub cloud service units you would uses you would use Azure. You are limited to the platform and supports, currently I think it is only.net core 2.1 and .net core 2.3. We do not have (unknown term) support yet but they do need to hurry up on that. You are limited to the platforms that are included with these. I am sure as the product evolves and goes into general availability, we will see more control over what you can actually configure with a code space container to be. Perhaps we can put some customer stuff in there. You can run a sequence within code space, it is entirely up to you. You are slightly limited to what the platter -- platform can do as a managed platform. The pricing is a -- pricing is about 10% cheaper than running a Virtual Desktop instance. Starting with 4 GB RAM and two cores, going all the way up to 16 GB of RAM and eight cores. Of course, you cannot share these. When you could have won the MP3 of hors When you could have won the MP3 of hors d'oeuvres, this is per user. I'm sure they will be enterprise level plans later but right now you cannot. Of course, developers get a bit touchy about the specs of the development machines and somebody probably taught -- turn their nose up at 4 GB of RAM. But the other way of looking at this is that you are not paying to run the underlying infrastructure, where Windows might give you 500 gig of RAM running on in any PC, this does not. This is clearly the amount of resource use This is clearly the amount of resource use to run your container, your environment. You didn'ton' t need to worry about the specs too much, I think these could be ample for most developments. Of course, you have options. The final thing I want to talk about something called VS Code remote. This is one of the most promising and exciting things you have seen in a little while. This code remote allows you to connect five (unknown term) code to any container, SSH, you can have tonnes and tonnes of different things you can connect to and makes the expense of developing a lot nicer. Rather than coding in the browser like you would with Code Spaces, you can still use VS Code directly. They are also extending it to Visual Studio in one of the 2019 previous if you have a play with that. You can actually log into Code Spaces directly through Visual Studio. You can create them and create Windows once and do a lot more. So what does this expense look like? Again, we have these in the cloud, I need to stop whatever I have running. I am going to connect to the same bit opcode space getup space but I'm going to connect by my local machine. Let us imagine I have a Mac, I do not want to install install any frameworks or anything machine, I simply want to just connect, get up and running and the on my way. In VS Code, we can do exactly that. If you install the extension we can see all of our Code Spaces. It boots up VS Code and it opens my Code Spaces. Like I say, this can be any container, it does not have to be a container running Code Spaces, it could the container running in Azure or anything you have on internal infrastructure. I can put this up, edit my files, you can see it has the! I added earlier, it has the GitLens in my Code Spaces and hopefully I can do my .net bill that I did before. This is going to do a build . . .. I can go.net run. It has failed because in here I need to kill this process. It is being a bit weird, terminal..net run that core sample .net run. Hopefully I can run this one still. No I cannot. So because it is showing the same container, you cannot run it twice! (Laughs). For some reason, I cannot stop this one probably because I killed the code space . OK. He will have to trust me then for the sake of time as well. What this does is that I have can connected to a container I can run the.net bill. It does not run on my local machine, that happens in the container which is fantastic. If I am on a Mac, I can do the building of.net stuff and it happens in the cloud away from the... I can work in any language in this environment which is cool. When you do.net front, just like in Code Spaces it will run at the local terminal environment, a proxy container for you, in this case rather than being on getup URL, it is a local one and gives you a local port. You have all the access to gate Git like you did before because it is in VS Code. Benefits here: connect to Code Spaces container, essays SSH, W S L . You can install anything you like in that container. If I did want to run .net framework on a Mac .net framework on a Mac or develop with this on a Mac, I could create a container, install the.-- the windows call server and run very happily. I am still on my local machine and I do like that I am not diving in and out of a browser. I am using native applications on my Mac which is great. There is a downside, you do require an internet connection to actually push changes and to build but it does handle built a little bit better with intermittent connections than say Windows Virtual Desktop. Windows Virtual Desktop. The same things as before, storage needs to think about what you are persisting because that is stored in the container. Equally, any infrastructure you need to connect to, you may to consider remote databases or put one inside your content yourself. It is entirely up to you. There is one other benefit that would I want to add to this, because you can manage when the container lives, you do get a little bit more control over the security of it. If you needed to run the M locally, you could run a VPN locally, that would fall to the responsibility of whatever machine you are using. If you need to protect that container where it lives, you could password protect it. If you are not comfortable with it still -- being stored in GitHub, or new customer say this data cannot be in this country, you can get that control in. So a quick comparison of all three. I will share my opinion, T. I will share my opinion, T. Virtual Desktop, you can run that on a browser, desktop or an iPad. That is really cool. I can pick up any device and be on the way. It is really nice. Code Spaces is all in browserr, you can do on a browser or an iPad. VS Code remote, I do like that it is on my desktop and is integrated to my environment but it does mean I do not need a powerful desktop, I'm just running VS Code remote and it needs an internet connection. The internet connection does require VS desktop does require a strong internet connection. The other two, the required connection but it can be intermittent which is very cool. With Virtual Desktop you do need to install everything you need but then you get infinite tax ability. With the other two, you can install things that you are slightly more restricted, you have to install it in a container or wait to see what the platform offers you. Virtual Desktop, you have full control over the security and the access but you do not the security and the access but you do not get that with Code Spaces because it is currently public. We await to see what happens when the product goes into general ability. availability. With VS Code remote, you have some control the local container would be however it is configured. My personal view is that Virtual Desktop works great for organisations, the organisations that need fixed stations, people who have a fixed station to work on that. It will be fine for occasional travel with strong connections. For the other two, it is far better to have full personal control. If you want to spin something up in the browser, Code Spaces is fantastic for that. You click and you are up and running. I am certain that as it matures, they were meant to -- install enterprise features. localhost 5001, this is running on the cloud. To some extra reading for you, I highly recommend applying for the Code Spaces preview, it is going to be really helpful. And with that, I think I got a bit over time, but thank you so much for listening. I am on twitter if you have any questions and I will happily take some from the floor and Mark as well. Welcome back Callum, thank you for your talk. We do have a couple of questions. Do you want to start with yours?  Yes, because I'm a big fan of VS code and Code Spaces, I just opened it up on my surface go and it works there brilliantly. I've got all the extensions locally, is there an easier way to get them on Code Spaces without reinstalling?  Just like locally when you reinstall extensions, you get that.BS code which allows you to set your preferences and configuration. All of that is stored when using Code Spaces. You can create, in your GitHub account, a You can create, in your GitHub account, a repository, I think just your username and create a VS code folder inside that drop all of your configuration in there. Meaning every time you create a Code Spaces that will be pulled into that space and you can keep running with your usual configuration.  Excellent.  I was just sharing with Mark, back in the  I was just sharing with Mark, back in the days like three months ago when it was still called Visual Studio online, people would share a little shield or badge on their get hug -- GitHub repo which was immensely helpful for people just getting started with the project. A question from Twitch, for those who don't know we are also live on Twitch, someone is asked if the tooling you discuss is limited to web development or other works? You can use virtually any stack and anything, if you're just doing.net services or developing open source projects even, it absolutely applies. I have not really experimented with, say, running test suites in Code Spaces, I assume you could do that. So I don't know how it would work, necessarily, and something where your end product deliverable is a library you would want tested, but it is something I might go away and look at because it is differently just a divine developmental in the cloud.  One last question, I love you did a little shakedown at the end. For the people that just want a straight answer, what is the most bang for your buck in terms of cloud death environment.  That is a great question. In reality, if you just wanted a virtual machine then a VM in Azure would be the way to go. If you are working as a team or want something more lightweight, then Code Spaces absolutely the best bang for your buck.  Thank you so much. Thank you for joining us today. Well onto the next topic because we do not break, apparently.  Go, go, go. In the current situation of working from home and being remote and having these virtual events, it is even more important to take great care of yourself. And of each other. And perhaps adopt other routines and patterns in that way. For me, I work in a remote team so I am used to the whole distributed working. But still, I think working remote in the pandemic time is very different from working remote in a normal situation. I have picked up knitting, I think people saw when they were following the stream for a little bit longer. Just stress knitting. You will not believe it when you see Mark right now, but I think until this year you were completely oblivious to the greatest invention of all time which is sweatpants.  Sweatpants. True story.  You by your first sweatpants this year. Exactly. I had heard of them, I knew they existed, but I did not think they were for me. But I was so wrong. I was on the wrong path for so many years. So I'm thankful for all the people who said that I should try it.  And now you are converted.  And now we even have company-sponsored sweatpants!  Your company gets it.  Totally,  Totally, it is really comfy, super soft, a bit oversized, is lovely. STEVE HUGHESS: Do you do business Friday now if you are not doing casual Friday?  Yes, I think that will become a thing now. Enough about the topic, but luckily we have an expert on who will share with us what kind of practices we can all apply. You are a developing team lead and community event organizer and you care very much about the people side of technology, making sure that our industry is as inclusive and empathetic as possible. I try to. Mark, that hoodie is impressive, I love it!  I borrowed it but I'm not giving it back. That's OK, that's just 2020. So whenever you are ready, we are ready. Sure, I will share my screen right now. Hopefully you all can see that screen.  Yes, perfect.  Great, so we will just get started. I've Artie been introduced, but I'm a development team lead and community Meetup organizer and always a lot of that has gone virtual. You can catch me on social media or on my blog, the handles are there. In case you can't tell by the accent I am based in Glasgow in Scotland. I will try my best to speak as slowly as possible. As has been mentioned, I'm really interested in the people and community side of tech as well as being a developer. So I help run the meet ups for ladies of code Glasgow and the Glasgow bronco meet up and I am also host of a podcast called candid contributions and we talk about open source development but also talk about the people side of open source and there are so many different ways to contribute. As part of that, what I have think -- been thinking about 20/20, is self-care which you hear quite a lot about in 2020. This is the definition we are talking about. The practice of taking action to preserve and improve your own health and the second definition there that I have included is particularly during periods of stress which is particularly important this year with the pandemic. Where everything is just a little bit turned up to 100 on the stress scale. So anything you can do to make our own health, our mental and physical health, better and to be mindful of it, I think is really important and we must try our best to do that. I will say, this is from me as a developer, not a doctor, so if you have serious concerns obviously reach out to a healthcare professional. But what I have learned about self-care while working from home as a developer, but it wasn't quite as catchy a title as self-care for quite as catchy a title as self-care for developers. This is what I have learned and picked up over the last nine or 10 months of working from home as a developer. So the basics, I am really rubbish at the basics of self-care, including drinking water. These are things that humans need to survive, that I am really bad at. I bet enough in the office where at least I had people around saying "you have that water sitting on your desk from this morning, you should probably drink it" or getting to me cups of tea. It may seem like an obvious one, but let's get the basics down of the things that we need to do to look after ourselves and it's amazing how much staying hydrated really helps. And move! I am really bad at just sitting here in this I am really bad at just sitting here in this corner all day and all night. So trying to get up and to move. Lunchtime yoga has been something I've been trying to do, is not real yoga is just stretching around at lunchtime. Or getting out for a run when I can because I know that is something that really helped both my physical and my mental health a lot. my physical and my mental health a lot. Yoga and running will not be it for everyone, but find what works for you and even just going for a walk or making sure you get up and get away from your desk. I know a lot of people have standing desks as well, and I really recommend that. It did not work for me, maybe you have to persevere, but what ever you have to do, just move around. We don't have a computer at the moment, we don't even have walking around the office to different meeting rooms and things, so you very much can just sit in one space all day if you let yourself. So we have to be conscious of that and actually make the effort to move. And sleep. And sleep. It is something that I know myself and a lot of people, with the change in routine, it is not your usual commute to and from work and the commute back home from work was always pretty good way for me to mentally close one door and then come home and have evening routine and sleep well. It was something that I had to adapt to quite a bit with working from home. I know a lot of people have been the same. So if you have any advice to share it with us because I know different people have different techniques. For me, it is podcasts. Whenever I'm really interested in a podcast I seem to fall asleep, which is annoying because then I have to go back and find the place where I was. So those of the basics. Let's sure that we look after ourselves at the most fundamental level that we hydrate, we move, and we sleep. And then from there, one of the things that is really important is the worklife balance. It is something we hear about a lot before the current situation were in, but I do think with working from home that work /life balance is difficult. is difficult. One thing I've heard from a lot of people is that you are not working from home, you are living at work. I think it is quite important to see it that way sometimes, make sure it does not cross over to the wrong work/life balance. And to balance the two, essentially. One thing I have found really important, and a lot of my friends have two, is making sure the environment you're working in his comfortable and friendly and that you are happy there, even if it is you've got like me, my little Lego pieces on my desk. I know so many people like me have invested in house plans, I don't think houseplants have ever been so happy as in 2020 because people were trying to make their workspaces come to blows possible. I put my tree up, because if it makes you comfortable and happy being at work or being at home, then go for it. And logging off. This is one that I really, really struggle with. As a developer, I know were not in person, but put your hands up if you've ever done this. So many times I know my husband has heard this. "I'm going to log off soon, I'm really close to fixing this one thing, I just need to compile and test it in that I will log off." And then two hours later I am testing another version. And we have probably open in this situation to where we've gotten up for the night and then gone back to the next day, will probably fix it in the first five minutes. You just need to switch off. Let your brain think about in the background, and quite often in the morning you will have fix it. So I do not always full my own advice on this one, but we must try to and don't just keep one, but we must try to and don't just keep working, working, working on the same issue into the evening. And so part of that, actually switching off. Logging off and switching off are two different things. I can close my laptop, but one thing I realised is my brain takes a long still thinking about the problems I've been trying to work on. I do think not having the commute is a big part of that, that used to be where I processed my day. On the walk home, I used to have a half-hour walk home which I don't have now. I have been crocheting to keep busy and not to look at screens. As has been the way for me to keep busy but to not be always looking at a screen and a device. Also, reading. I am quite guilty of reading a lot of technical books or books around the tech industry. So I have tried to read a lot of fiction recently and switch off from coding, tech and from work in general. We are all on calls all the time and what I sometimes think we miss out on is that walk to the chat of the start where your waiting for everyone to join. Things you would have in real life but less so working from home. I do think it is important, where appropriate, to have time for that catch-up in the meeting with colleagues. in the meeting with colleagues. The same is with that meet ups with meet ups, making time for social conversations and that our brains are getting that part of social interaction we used to get before everything was virtual. On a similar thought, Zoom fatigue is a similar thing and we did find myself and friends at the start of working from home and everything being virtual , we were saying that meetings were so much more exhausting than they were. I could sit in a workshop all day physically and not be this tide. Why is it? It has been proven why we are like that. Apparently even the delays, between one and two seconds delay between you speaking and someone responding and your brain picks up on that and it is different to what it is used to processing and it is working harder to do all the things we do without even thinking about it. Like reading social cues or people with hand gestures or anything like that. So our brains are having to work harder to do the same thing as we did previously which is go to meetings or have conversations with people. Zoom fatigue or Teams tiredness as I like to call it, it is real. It is OK to be more tired after meetings than we're used to being. It's important not to beat yourself up about this. Even, I know that Teams and zooms Zoom has come up with a feature of hiding your camera view. You are not used to looking at yourself all day in that corner where it shows your camera and it is like having a whole meeting in a meeting room but in front of the mirror where you have to look at yourself the whole time! It is something again that our brains are not used to doing and makes it extra tight. tired. With lots of meetings there are lots of virtual catch-up and social Zoom calls but sometimes it is OK to say no or not to put the camera on. You will realise that a lot of other people are feeling the same way too. But through all of this, what I have learnt through the last nine or 10 months , what is right for you? Different things work for different people. I think it is important to be mindful, be conscious of what works for you and what does not work for you. Or any days where you are more tired than you would have been or you are not as great mentally as you have been on other days or days we are feeling great, take note of that. What happened that day? Any changes to your routine that may have influenced that too? What might mate affect your mood that day physically or mentally. Take a note of that and maybe take a note of that build that into your routine . Maybe your day was good after you change your workspace a little bit. Maybe removing from your dining table to your actual desk . I have that expense, it made a difference having a dedicated workspace that I can use in the evenings. You seem to acknowledge that. It is really important to be kind to yourself, too. It is really easy when you are working from home to be negative or when you have had a bad day or to be in not a great mood, to blame yourself and think it is only. Other people are experiencing this Other people are experiencing this committee. We are all doing this for the first time, even people who have been working hopefully home for yours, acknowledge this is not home for yours, acknowledge this is not the same thing. So be kind. Give the advice you would give to a friend. It is all about learning and adapting and trying something if it does not work. That is fine. Learn and try something else. If it does work, great. It would be really good to share what does work for you, what you find works and if we could tweet on the hashtag on the screen, it would be really awesome to start a conversation of what has worked and what hasn't worked and we can all then from each other. That is me! I hope that is in time! We are running a little bit over but please do tweet so that we can chat about what works for each other. Thank you.  Thank you! Thank you, Carol. I have to admit that I am guilty of not taking too much care of myself , especially the logging off, I am very bad at that. I definitely need more self reflection when it comes to self-care and working from home so this is a good reminder. from home so this is a good reminder.  Which one of our users has shared music for productivity? Also people keep saying stay positive. Keep sharing what makes you happy and you can use the hashtag.  Thank you.  Thank you so much!  OK. We ready for unexpected?  Are last session!  Exactly because we are doing a handover as well!  You have a new set of hosts after this  You have a new set of hosts after this session!  It was so nice. But there will be another because we have been invited already.  Let is give the other hosts a chance!  Next up, Janet shares at her insights about inequality and what can we do to fix it? She is software developedr She is software developedr and she studied computer science . She enjoys solving complex challenges like gender developers and tech and developing stack in C stack in C . Welcome!  It is a great privilege for me. We are happy to have you!  Can you only OK? -- Can you hear me OK? I will just share my screen. Can you see my screen, now? Yes. Today I would like to share with you I talk about gender inequality , why I think it is a problem and what we can do to fix it. This is a topic that I am very passionate about and really want to advocate over our industry. Let me start by telling you a story about myself, my name is Jonah A nders son , . It is an icon T I T consulting company with hundreds of employees and sweet i n S w d e de n. Originally I am from the Philippines, I grew up in the Philippines and when I was young and wanted to be an engineer, it is because I enjoyed sketching buildings and helping people to build modern homes . Because of economic circumstances back then back then my parents, we were in the middle class class , my parents could not forward a a ff ord to send to school to study engineering. In the Philippines it is not free to get a degree in college or university so what I did is that I took the initiative to take a community test but I had an opportunity to take a scholarship in computer science. In 2001, that is when I started my journey getting into programming. So my first programming language was written in Windows 98 and Visual Basic was my first language. I also remember that 3 1/2 inch floppy disk with a small storage on it and also learned with a small storage on it and also learned about binary codes, and gates. After this I worked in different branches in different industries , when I moved to Sweden nine years ago I decided that I really wanted to pursue my passion in technology and I really like working with computers. working with computers. As well as modern take. I studied three to four years more about agile system development in C# .net and Java. .net and Java. Looking back from 2001 to today, I am working from two computers to multiple screen computers in modern technologies like Azure and modern managers like C#. Let me share with you a story that I cannot forget when I started my career intake. forget when I started my career intake. # te cfh . h . I was in a job interview which I was invited to do programming tests and the first interview went well. The programming test was the second one. When I was invited back in the interview I was in a room with two or three programmers , I was prepared and I received a programming test to do . They were sitting at the back of me while I was answering my lines of code for that programming test. I was intimidated and I lost focus . To make a long story short of that scene, I felt very nervous so I quit and did not continue that programming test. And I came home asking myself, "It's programming really the kind of job that I really want to do? Is tech really the industry that I want to spend my entire life in?" But I thought that I really want to work with computers and make a difference to I did not make that situation and hinder to fulfilling my dream to becoming a software engineer or developer. I would like to share that when I started working, I like reading fax or justc tyss and statistics on the internet. We have challenges with gender inequality and we know that women are underrepresented in tech on most major companies. When I see this picture of a woman dominated by men, I can really relate to it. Working as the only software developer in the team sometimes makes me feel left out or alone. So I really want to advocate that we need to make changes for these problems. In 2002, / 2013 , there was a survey that in GitHub women had to hide their gender to have the pull request approved. That is not good to hear from me as a female developer because we are in a modern world and I believe that we should have the freedom to disclose who we are regardless of our agenda. We used to switch off -- ways to switch off, find whatever your thing is. There was also a project called The Gender Shades project, I'm not sure if you've heard of it but if you want to check it out there is a but if you want to check it out there is a website for it. website for it. It was a project that evaluates the accuracy of AIA in the classification products. We still have a problem with AI misunderstanding the face of the female subject. So there is still a high rate of ever there. So I've had a lot of questions about these things that I read online and me working in tech. I can completely relate to it. I dug deeper and searched for what gender inequality really means. I do not want to involve it with feminism, because I believe being equal is a freedom. Gender inequality and its description is being treated as equal regardless of gender. And I would like to share with you my favourite quote from Albert Einstein: " favourite quote from Albert Einstein: " everybody is a genius, but if you judge a fish by its ability to climb a tree, it will live its whole life believing that it's stupid." So just like females, if they are told that they are less than men then they will spend their lives believing that they are. believing that they are. I personally identified some challenges that I see in our tech industry right now but I believe we need to face and try to solve. First is that we have a trend of gender stereotypes. And gender bias in AI learning and machine learning. Unfortunately we have high rates of women leaving tech jobs, not because they are not competent with the job they are doing or not skilled enough, it is because they are not comfortable with the company culture and most of the research done on the cause of this it is because the woman does not feel accepted for who they are at the workplace. We also lack female programmers because we have a shortage of female role models. We also have unfortunately a problem of discrimination in workplaces. I personally am living testimony of that, and that is a separate story. I see it as a challenge as well. We need to build the future that is sustainable, and that requires diversity and different ideas. Our digital skills along with our modernisation, different skills are also essential. We need to act now and do something about it, try to do something about it. It is not easy because there are challenges for that. But I believe that tech companies should create or start creating gender equal and inclusive, diverse workplaces. This means that a technical role, for example, like programmer tech lead does not have to be connected to a gender. So a male or female or anybody can be a great programmer without specifically identifying it with a specific gender. We need to raise better awareness of equality in our workplace so that the few women working in tech feel comfortable and don't want to leave the company or the organisation. We also need to encourage female role models. We have a shortage of role models and that is a great barrier in achieving equality in our branch. For example, there was a study done by PwC in the UK, and there were females who were interviewed and were asked about and were asked about their favourite female role model working in technology. And they do not know anybody. 83% of the respondents don't know anybody. This means that we need to as women in tech get out of our closets and get out there and inspire others. Because we need each other. Not just men and women we should all help each other. We need to inspire technology to the younger generation. This is me in this picture. When I was involved in teaching programming to kids. We used Scratch and Tinker and some Rowboat programming. I did this on the side of my job as a programmer, and I believe that we can make a difference by inspiring technology to our younger generation. And that starts at home, in school, and even as technologies can make a difference by sharing our knowledge and the good things about this tech stuff to our kids in the younger generation. Those we get to connect with. Also as developers, Also as developers, and people working in the IT field, we need to build applications and technology products and solve problems with equality in mind. This means that, would rebuild applications and services we think of others too. We think of other genders and different diversities in our end-users that will use our products. It all starts with leadership and good leadership. So if our leaders don't understand the importance of diversity in the benefits of it, then we will still have the same problem. So it is important that our organisation leaders understand the importance and diversity importance and diversity -- of diversity, not just for the sake of having an equal man to woman ratio, but it also improves our workplace with faster problem-solving, increased creativity, and there will be reduced employee turnover. Also create safety and collaborative learning environments for everyone. When I was starting in technology, I had to really work hard to try to level up with my fellow male programmers. So this is important for junior developers that when they start in a company they need to feel that they are welcome and they get the support that they need from other developers. Then, diversity and tech talks, like we are doing right now, Virtual Azure Community Day. Their welcoming me Their welcoming me and other women to talk about tech and I believe this is something that we need to do more. Songs and even in programming events. So what am I doing then to help fix the problem? I volunteer to inspire and mentor different young, and old, women who want to get into tech. So regardless of ages it is not too late to get into tech. I am involved in several organisations in Sweden and even internationally volunteered to do mentor ship when I have free time. When I have a chance, I inspire tech with my friends and I'm also part of the gender equality group at my workplace where we are trying to advocate and make changes in organisations so we will be more equal here in our country. Lastly, we should not forget the women that made a difference back then in order to create the modern computing we have today. We have Grace Hopper and lady Ada Lovelace. So let's inspire and motivate to have more of them in our industry today. Lastly, equality is freedom. It is the freedom to be ourselves and to be valued for that. If you have questions, you can contact me and this contact information. And I would like to thank you for the opportunity to talk . Thank you so much Jonah for highlighting all these challenges. I definitely do recognize them. Representation matters so we have to make women and people who identify as women more and more visible. You mentioned you were mentoring in some programmes to make a difference. You know of any other initiatives either locally or more globally that we can point people to to learn more about this to help more?  Sure. As I had in my slides, locally in Sweden we have a few groups and in my own city I have the guidance network where I get to meet the women studying programming and we meet online because of coronavirus. But I inspire them to ask questions about what does it mean to work as a programmer? What are the challenges? Were the differences between working in an IT consulting company with working directly in a product company such as Microsoft? I am really passionate about it and I'm glad I have the opportunity to talk about it because we need role models. Role models make a difference. There are women out there who are very good programmers but just hiding in their closet don't want to show off and we don't inspire others that way. And it's also teaching kids programming. I noticed that in my classes we had most of them were boys and there were two or three girls. I noticed they were not so interested in learning about Scratch programme and I asked them why and I try to inspire them. But they were too busy with Instagram. And things like that. So we need to change that.  Cool. Thank you. The one initiative I can think of right now is global diversity CFB day.  Which is coming up soon. January 23.  So that is good to mention.  Especially when you talk about representation. People get more comfortable with public speaking . We track lots of mentorship. Using their free time to help.  And just one day before, because it will be an online event as well, I think that is one of the key things this year, and I think that's been good in terms of accessibility right now. Is she #WizKidLiveYT # # for example. They Q so much, I am really glad that I got this opportunity to speak at this event. I hope there will be more so I can inspire others.  Very good.  Go for it! Thank you very much. Alright.  That's it for us as well. Do you want to plug the link one more time? If you go to a.k.a.MS /VA CD . challenge We have little break coming up so this is the perfect moment to check out the challenge and maybe get started with completing some of the modules. modules. And then our next post will take over. Thank you all, and enjoy the rest of your day.  Stay tuned, enjoy. (BREAK) Thank you for staying with us and virtual community day. In case you haven't noticed you can join us for a skills challenge and you can win a beautiful backpack. He will give away the prize if you join and complete it. You have until 12 December and after that we will randomly pick 10 winners who get the speed of the backpack and we will put some other stuff in there as well. Also useful, the little QR code for Azure Heroes, you can earn the learner badger . Azure heroes is a community programme where you can earn a badger by completing a challenge or doing a Hackworth on or just attending this event and that wonderful? You can also recognize others for being an inclusive mentor or that they create cool content you enjoy, so please check that out at Azure.heroes. As you can see your hose have changed, I am Susanna Daniels, developer engagement lead at Microsoft and this is my coat was. I am Wolf, I have been working in engagement for three years and I have been working at Gemini. I enjoy keeping up with all the latest and greatest online and Azure, I think maybe I'm up to the challenge to maybe share some of my knowledge. I will be hosting together with Susanna, but maybe we'll see me on the other end of the screen in the future. screen in the future. If you watched earlier today, you know that this is a thing right? At VA CD. When you say you want to do things that is a promise, so we expect your next presentation for the community day. I will start preparing.  What will be about?  Is a good one.  Is a good one. I've been messing around with Azure functions a lot and I think that vehicle want to show some integrations with all the other Azure components and can integrate with.  We love Azure functions, so it's brilliant. We also made a little switch with our plus she's, so we are joined by Brady, our open source mascot, and AutoCAD -- talkshow the cat has a friend. We are running a little bit late I guess so let's take a look at the next session.  Next up we have David who is telling us all about how to flex our Azure puzzles with A bicep. It is a new technology but I don't want to take his base, so let's introduce him.  Are you there?  Yes!  There he is.  This is the thing about live, will it work?  Oh gosh, I am live. Thank you, I hope you can hear me.  I can yes.  Perfect. Greetings from Norway. Snow we but raining a bit. Welcome to my session which surprisingly will not be about fitness. And I should probably share some content as well.  Yes please. It will be about a very interesting project that can help you practice code on Azure. Just let me double check that I can see the deck as well. Yes, perfect. My name is David and I work with Microsoft in Norway as a senior cloud solution architect. I've been writing many projects were big customers are trying hard to adopt infrastructures code practice predominantly using ARM templates but also alumni or Terraform. -- Pulumi and we all agree that Jason, as a foundational language, is very verbose. It requires you to be very mindful about columns, about the precise syntax in the structure. The team around Azure resource manager kicked off a new project which they are now calling Bicep. Why Bicep? Because we have ARM. There was a poll about what the future name could be but for now it is Bicep. We will explore what Bicep is, what it is not, and we will walk through from a simple example going to more advanced workflow, something I do when I am working on Bicep files development and in the end we will look into some automation through ketone actions in the roadmap that is also publicly available for anyone to see. The project Bicep, it's goal is to create a new DSL language that would be sitting on top of a resource manager template. And as such it is an abstraction. It is not trying to compete with ARM, it is something that sits on top as you can see in the diagram. the diagram. When you want to deploy your Bicep, you actually need to compile it or trans pilot, into a proper ARM template and then you can use your regular tools like PowerShell to run the deployment. The best way, it is really an abstraction and we can think about ARM Jason as an intermediate language from that perspective. So you still require your standard tools to do the actual deployment, but what is exciting is that we will look at how simplified the syntax and the use of the Bicep languages. As I mentioned, there are several goals of this project has. Reduce current pain, that is probably goal number one. number one. And help people with adopting code that is in the ARM or Jason world not very easy and you need to be quite skilled to actually do that. Since it is a transparent abstraction, it means that it can leverage all existing benefits of The Platform. It also means that anything that you can do an ARM language and on Jason language you can You can see what will be available hopefully at the beginning of the next quarter. But that is the whole idea, all of the functions that are available in ARM language are also available in Bicep. The goal is to provide modularity. When you begin with ARM templates you start by adding more resources , so one single file , and in the end you can end up with hundreds of lines of code. So there is currently a possibility to link templates. Having your code in separate files. But it has a few caveats. Firstly you might be selling some tablets to someplace where you might not be able to reach out and pick them up for the deployment. Bicep is reducing this pain using modules. It is also good to talk about what are not goals. It is not something that would override all other possibilities available. Bicep is not trying to compete with Terraforming , its primary focus is measure, and never say never, but I do not think it would be used to be deployed any time in the future. But there might be uses in (unknown term) for instance. It is really an abstraction , and since ARM cannot deploy to any other sort of providers the focus is clear. The general purpose language, it should not abstract the complexity and help you with writing your code. Even though the syntax is new , rather than using a lot of similar constructs that you will actually find very familiar from other languages. So, today we are on version 0.2 and it already has a lot of great capabilities. First of the support for tooling , so the primary tool for Bicep today is usually code, which is not surprising at his skill ranked as the most popular . This version allows for code formatting , modules, falsifies the ability to split your current space integration modules by briefly mentioned as well as something called Scopes. So the possibility to deploy active resource group level in one single template for instance. It is also good to remember that it is still an experimental language, it is not ready for production. In other words, you cannot, today, submit a support request to be -- the Azure support. But that will eventually come. Some limitations, you cannot copy loops or condition properties. Another thing is that single line objects and arrays, you need to actually stretch them into multiple lines, so one array member for instance on each line. But apart from that that is all for today. The whole project , the code base, and the roadmap, everything is published. You're more than welcome to contribute . If you are following the progress of the plan there was a feature scheduled for version 3 which was called decompiler allowing you to take your existing ARM template and transfer it to Bicep. But it was surprisingly released in version 2. This will kind of help you with taking your existing code and looking at existing code and looking at how it looks in Bicep. We switched to a really nice web tool, so in case you do not want to install any tool yet on on the team -- on your machine, the webpages a really good playground. You can start with, there is a number of things on the window stop you can immediately window stop you can immediately see from the left what the code looks like. And on the right outfit would translate or Compaq -- compile. You can start adding new things, so You can start adding new things, so I want to add a new parameter. And as you can see this tool is actually telling you the comp failed, because you need to complete writing that specific block of code. So parameter, string, and now I have completed that it compiles well and it has been added. In this way you can take those existing samples but what is also useful is that you can take your existing template and actually decompile! So if I check ... To see which one is ... Yeah. That was something I want to show as well. If you're existing file includes some of those properties that are not supported, in this case it is actually complaining about templates, it will not decompile for you. Let's take something more simpler. Yeah. So I took my existing template I was creating for one demo and now I can see how it looks in Bicep. I can just take that code and refine it and forget about it. After it is compiled to ARM you can use any additional tools that you used to, testing, validating and employment. It is all done in exactly the same way. So that is Bicep Playground. I highly recommend you just look at it and see for yourself how this looks. You can even start from blank and start typing but it has support for code covering, so I would not use it as a primary tool. We will definitely switch to code shortly and will see how different the experiences. -- Experience is. You've probably heard about Azure Cloud Shell. In this phase, it is not included in the image that is supporting your Cloud Shell. We need to install it yourself. And just a word of caution ... Since you are limited with permissions, you cannot add things to the path. The only way you can add is to reinstall it somewhere else. And I will show you how this was done. So this is a simple script and I would like to call out Chris and his great website. Where he is publishing a lot of great articles. This simple script allows you to install Bicep on the side in some directory and then Bicep on the side in some directory and then you will be able to call it and use it to compile or decompile. You can use this rudimentary code editor , or the built-in code editor but the primary user experience is tailored around visual studio code. So I would like to switch to this one. It is interesting how my otherwise powerful device is now extremely slow due to be broadcast. I hope it will not crush on me. OK now it is working. Perfect. There are two components that you need to install in order to start using -. -- Using this. You can get the artefacts from GitHub and this is also available in the same repository. The team has published a benighted guide on how to install it depending on what operating system you use. And how to consult the extension from the marketplace will stop if you use it with this version you need to uninstall it first. Great. I have installed all of the tools on my machine already. So I can just check that I have Bicep installed. Yes it is. There is a really nice tutorial with animated gifts -- GIFs around code completions , basic set of snippets, as well as code navigations. So things you used to, peak definition, references, everything is working in the same way for all of the languages. But I think that the most powerful thing is that if we really wanted to start from scratch here. If you want to start from scratch it will give you it will give you a lot of leverage just like sort of hitting control and space and getting what should be the next block of code in your sequence. Obviously Bicep support single and multi line , , so if you type a keyword you can now have several options. I will just use this one. And let's say location And let's say location , space, string. That should be enough. That should be enough. If you want to extend it to some default value, the SSA Norway East. If you want to extend it's a bit more than you need to provide more than that. The names of these properties will be different between Bicep and ARM. This will obviously be an array , so we can say Norway East , Norway West , the local regions. But I realise when trying to write the code from the beginning I was doing a lot of mistakes with commenters -- commerce. Because you used about when working with Blazor . If we switch for a second and look at the declaration. This is basically copied from the requisition. -- Repository. You can see what kind of datatypes you have You can see what kind of datatypes you have available. Parameters, valuables , variables, and the definition is really , variables, and the definition is really nice and clean in that sense. do in a Bicep. And of course, if you want to run the exam want to compile, you can hit Bicep, and if you are seeing this, which I hope you are, this is a new predictor which is a really nice, new feature for PowerShell or it can show you not only the history but based on what commands for Azure modules you are using, it can actually predict what would be the logical next step. It is powered by some artificial intelligence. And I say declarations. And of course I would need to be in the right folder. OK. Sorry. You are compiling the right keyword is actually built. Hopefully this time it will produce the right... no it didn't! Perfect. So that is something you will get when you try to run that command. But the tele-sense and that in section is showing you some of these warnings already. You can see that I have two objects , or there is one parameter and one variable and they both have the same name. In ARM Jason this would not be a problem because you always presented with the function, it is either parameters or variables. In bicep, you actually need to make sure that those logical names that you have for each code block are unique. So I need to change the name and make sure that it is unique. That is the first one. Then it tells me that I don't have any endpoint property in his own resource. So it is not just giving you hints about any syntax errors, it can also cleverly check if that kind of declaration makes sense for that kind of expression makes sense. I can't really use endpoint I would need to change it to something else. Like zone type, for instance. So that should work fine now. Then of course, you can see another example there is no example resource provider. Obviously that one would not compile. So get changes for something else. There typically a lot of questions around will Bicep support your changes coming in ARM and resource providers, and how frequently will it get updated? Well, the thing is it is functional so even if you are running an updated version of the Bicep CLI and you put on a brand-new version of the API as part of the resource declaration because that is how it is structured, I will warn you that it will actually compile it. So if you have access to some private preview and an API that maybe the language server is not aware of, it will still allow you to compile it it will still allow you to compile it and deploy, if that would be valid for your environment. That was the key argument are some of the parameters for variables, resources, so again much cleaner, much simpler, the power of using or referring to properties which is well known in other programming languages. It is extremely useful if you compare it to a standard expression. You would need to come up with two define such an outcome. As I said, once you have your code compiled to ARM Jason, you can use any tool that you are used to to work with it. I would like to move on a bit. We went through this one, we set up our environment. This is an example of a workflow I adopted while working with bicep. I first did it in VS code, then I compiled it and that I validated using ARM's toolkit. I can use an extension as well and continue through ARM viewer with through ARM viewer with a validation that runs deployment so I can see the difference between the state in Azure comparing to the code I actually work with, and eventually I use it to deploy to a test environment. What I did, let's say I use this one as an example. It is really slow. I have a file for network interface card, of course I could start typing those into the built-in terminal but what I thought would be really nice to have some built-in tasks. If you're not familiar with VS task, you can run a task and I defined this so first I will build and it should actually build my code to ARM. And it did. If I want to, I can also look at it through the viewer if this happens to launch. viewer if this happens to launch. OK, it is not really interesting, as you can see the label is somewhat corrupted. corrupted. But it is a single resource so it won't give you much of value. The next step would be to run the next task which is tested with ARM's test toolkit. If you are not familiar with that, it is a custom PowerShell module using (unknown term) a great test framework for PowerShell. It should It should be based on the predefined set of tests and give you some output about the health and if you are following recommended practices for authoring your ARM templates. As you can see it is flagging the location, so if I change that in Bicep and then recompile and run this again it would result in a much cleaner syntax. And then the last one I could actually use this deployment. While it is running, I will show you how I created those tasks in VS code. So it is a simple Jason file that you could have in every repo in VS code , sorry, not VS code, directory. It is the definition where you are declaring what should be running, what arguments, etc. We can see that my what if is getting the status because Jason does not use any estate files unlike Terraform, for instance. You can see that the result of this test is that I will deploy one new resource to that defined resource group defined resource group and one is ignored, meaning that there will be any change. Fantastic. So all that code, So all that code, of course, I will share on my GitHub depot -- repo but I thought this was a nice way to show . And I have the syntax link to this code so I can simply have that workflow in a semi automated fashion. When I am happy with what I produced, then I could probably step up and take it forward with a more advanced scenario and use some automation, for instance GitHub actions or get help workflows. Before I do that, I wanted to call out that greater capability -- great new capability using modules. Let me find the right example. Module is like any other Bicep file, does not have any special syntax. You can include the declarations of variables, resources, and outputs and if you want to use it ... is not this one. If you want to use it, what you would do is you would simply use a keyboard module, give it some name, and then provide a path, its relative path, to the file system you have that module located. Then it has some body where you have to define a name. That name will actually be used as a nested deployment name. Then you can or should provide any parameters that were declared in that module. And maybe pass through those from this main template declared parameters and variables. But that is how you connect those. You can have as many modules in this main file as you like. You can even refer one module from another module . But what happens in the end as a result, when you build using Bicep CLI, it will produce one deployment file with several nested deployments. If you are familiar with link and nested deployments, nested diplomas are those who have actually invented the entire template inside the main template. It is not referring to some external endpoint where you can find the code. you can find the code. One module equals one nested deployment, but one module contains many research declarations as you'd like. So that is what it would look like when you compile it. That is a capability that is native to Bicep. I would also like to pull out a second option which I also find quite interesting and it is a possibility to use template specs which is yet another new feature in the ARM language family, if you will. Allowing you to store your templates directly in Azure meeting that you will be able to refer to those templates like it was any other resource, So this is an example of using such deployment. Highlight that variable link ID I am using the function and using my template which is being stored in another research group I am pointing to. I need to refer to that one as well. It is then just adding that versions name, and number. In this way I could reuse the code that I have built using tablet specs and the deployment of those specifications . But that is a different workflow, not part of this one. So just to show you how it looks you have the template spec and then the main template. The template specs and a really interesting way and how you can share your code across Teams for instance. All you need to refer to this is to have a reader have access to the access point so that is not really directly related but I like the fact that you can combine them and leverage some of the things coming to the resource manager. They are in public preview. The last part I would like to show you is how you can automate this workflow using GitHub workflows. There are so many great articles and content provided by the communities, I would like to pull up Sebastien , I probably pronounced his name incorrectly. But he wrote an article about Bicep. So he created a custom action, actually two. So of course you can review the source code and inspect what it does, but more importantly you can easily start consuming. In my file for the workflow I am just checking out my code base and logging in. In two Azure , I'm not using Nike code base , but I'm using deploy via Bicep. This will compile and it will also deploy it to a defined resource group . Just show you what this looks like . I have been playing with it quite recently. I do not think I managed to employ end-to-end due to some small mistake. But it is an interesting way of how you can make that deployment. So there is some error that I need to work with but feel free to create your own customisations and maybe improve it or send a request, I think you would appreciate it. So when it comes to the roadmap the code reuse, we top stop time and specs , using GitHub workflows, I will provide full reference to the article that he wrote. And now we look ahead and see what is coming. You can check the road mac yourself using the projects in that repository on GitHub . But what is coming in January, they are planning to complete the ARM templates so whatever you can do using that will be possible using Bicep. There will be the possibility to deploy directly . And last but not least reach support status by customer support services from Azure which means it will be ready for production usage. And you will be door to submit support requests if you run into any issues so I think I would like to just provide so I think I would like to just provide the link and I would encourage you to follow (unknown term) on Twitter so you will get any updates, what is coming. They also publish events which are streamed live. So that is a great way to keep up with community members and here directly from the team about the allotment of this great and interesting project. OK, I think that is all from my side.  Alright, thank you a lot. We are kind of short on time so I just have one question for you. Show that the action can support it directly, will we see this for other pipelines as well? That is also something I probably forgot to mention. As of now, an official Microsoft provided action or if you sort of look into the the , what needs to be available within the pipeline, defending and what agent you are planning to use. And then making sure that we can execute the comp and then you would use a regular agency app which will be useful to those . And to deploy.  For some, OK. One nothing, knowing which outputs you have available is awesome. Instead of having to look them up in the documentation. So thank you a lot. It was my pleasure, thank you for allowing me to talk during this great event.  Next up we have Chris.  I do not think Chris needs an introduction, he is all over the Belgian community. And also arranges the club group conference was how is that coming along ?  As I wanted to state in my presentation I am more involved this year but unfortunately because we are all really looking forward to having a drink , hopefully 2021 night to another club room. Let's hope so, I think everybody watching as well could really use a great community and and enjoy some beverages as well.  So looking forward to that, seeing people face-to-face, hearing laughter.  So you are going to do a session on Blazor , really looking forward to it. Take it away. OK, let me share my screen. When you can see my screen, please shout out . OK, good afternoon to everyone around the globe following the session. Welcome to this presentation about service interactions with Blazor . I am a Microsoft professional since 2007 and around 2016 and became part of the team. I also work together with some other great people. As we just introduced, we cannot do the club group this year. But given the fact that so many conferences are virtual I want to show the very tasty Dutch beers. So I will be moving next year. Hopefully, fingers crossed. We will get the cloud group next year. So with the pandemic, most of the Belgian shops, for those familiar with Belgian , they have lots of small family shops in Belgium . But are not really on the internet yet. I decided to make this presentation about a local bread shop where I like to go. It is called Broodbroeders and those smiling faces of their other people who work there. They have grown into quite a few people and to see quite traditional shop making very tasty bread as we can see here. It is my wife's favourite, very tasty. I sat down with those people and permission to make use of all the image below it, so we are covered in that part. The requirements are that it should be easy to use, and the report should be in Excel. I came up with the idea, let's try to make use of the functions, logic apps , because of Excel, one drive , and Excel online. Traditionally some person , and I'm sorry about my drawing skills, this person what a router on the internet. Traditionally you do that, there will be a program running over there. As you can send out to either excel or you can input it into a database. In a local shop, somewhere in my city, what if they wanted to go to way more places . Everyone is ordering wine nowadays. So we wanted to scale out. Let us try to think, traditionally, there is a shift, a shift, all of a sudden you went to places like local databases and all of the servers will outsource this. using his resource ID. Perhaps you don't need it because maybe they are closed on Sundays or perhaps people don't want to order in the middle of the night. There is a gap where after 4 o'clock and unlocked order anymore for the day after. So they might not need all that power. So what about serverless. I was thinking this one. This is going to be a rather low cost one but you can still scale out if you want to. The person who's going to still use a browser is going to download Blazor which is something cool and new running in the browser and it is going to connect to Azure functions which, on its own, I did not want to add another component to write a Excel so so so the OneDrive that we can access and there's also something called service space which is great connectors to connect to whatever you want including XL online, it can add extra rows into it. With that, most of our slides are already done. Let's try to go to demo time. This is, of course, already going to be like the end results, basically our Excel file which is running on the one drive. As you can see I tried it here eight minutes ago to make sure everything is still working. This is basically what we want to have. This certain date, something has been ordered, how many, how much is the total amount, by whom, and of course these are fake mobile phone number so please don't dial them, I just made them up. So let's get started. First with the Blazor application. I first going to try to run everything on my local machine once that is done, I already have it in Azure because having an account in measurements it might take a while to get in and were limited on time. So I will show the end result will be get there. Let's start with Blazor. I'm going to make use of Visual Studio because people like to make use of other alternative operating systems besides Windows. So let's try that one. So let's try that one. Let's see if I can still type. And this will be the output. Because it is for them that we are creating something. This is already been completely created by using that template. Now I will go into code. I get to see this one. So what has been created over here is a programme that serviced everything in the meantime. OK, super. So this is how to start it up. And yes, I would like that to happen now. Basically for people who are not used to this, I created hereby accepting that one, some extra tests, and the launch. So now when I press fire or start it knows where to go to set up the URL etc. I think reader needs to be running in the browser, it always needs a place to go to lichen HTML page. This HTML is going to be the roofer to go to. -- Route for it to go to. So there is still a bridge between the Scripture trying to load in the Blazor components which is basically .net compiled and if you are worried that perhaps it is to know and will not run on your machine, most of the browsers nowadays also on mobile apps will run Blazor. Basically this is nothing more than what the assembly which is standards you can use it is C#, my preferred language is, but also another language is like say for example Java or whatever. But we are going to make use of Laser. -- Blazor. Should know the syntax, this extension sorry. So will have a counter which we are just going to run and a (unknown term). Let's try this one. Let's open this one. Great. I'm just going to do.net run. It is going to build because it needs to be built, because it is an assembly remember. Great. Sorry my voice, I noticed yesterday that my voice is been deteriorating, must be something in the air I guess. I don't think is going to be COVID, just a regular cold or something. I will try to hold on. So basically this is my index. Blazor page rendered. I can click here and I can fetch data. So this comes standard out-of-the-box, very easy, quickly up and running. You can see there's some styling over here. Now we're going to see some funky, weird things here also being added by Blazor. here also being added by Blazor. The CSS is going to be transpire by laser so you can see from just turn pages going to make a mix of selections over there. Is quite specific to blazer and I'm not going to go too deep into it because it is virtual community day but if you want to look deeper into that just be sure to study it a little bit. It is quite easy, actually. Now let's go back to her coat. Now let's go back to her coat. I can show this on. We want to make a small workshop. -- Web shop. We will make a new page and be mindful, behind the scenes is going to be compiled (inaudible) following the conventions, this should start with a capital letter otherwise it will not compile. compile. There's our shop, and I'm going to be very lazy now and I just want to copy and paste this one over and were going to change the small parts. So this I'm going to call shop and let's see here. Shop you and on and this one OK. So this should still work. Way back in 2002 and I would look at web forms I am not really a fan of putting codes together with my Blazor file. So when you close this one over here to learn to take one out and create a new file called shop.CS. I am going to write a public partial class called shop from component-based. from component-based. Let's add some from code, if I can reach it. When I have here? I need to add some class. First I'm going to use these ones here. Super. I'm having problems with switching my screens apparently. Now let's add in some code. My mouse is not working correctly. I'm having some problems with this computer today. So these are some side causes that I will be needing. Let's see it now things will work. Basically this is going to be generics inside the browser. Because we're going to have a web shop we are also going to need to have a couple of images. Let me copy and paste those in the top root. Most workspace ... OK, let's copy these over. I have some Jason class and here I have some and here I have some images, say for example some rye bread or whatever we're going to make use of. This is the simple part. Apparently this one here is not very much like, and the product comes from this variable here. There is another one here, this course because I copy and pasted it. Now part of my code here... and I replaced this one. Great. So a phone now goes OK, I should be able to run this. I can add something over here to my product, so whenever I click one of my buttons, I have the button in the row to update that. Let's try to run this again. I'm going to put it on this one because since .net five came out the nice thing about it is I can now watch something being done. OK. So let's see here ... OK, super. LSC hereby can just do this. As you can see, I am no designer. This side, let's add this one. To that side. So we can see if something is going to happen over here. I would like to show that I was calculating quantity. As you can see and now because I changed something over here you can see here directly in the browser . Let me see if I can update this again. Great. It is basically a mini calculator at the moment. Cool. I like the layout but still says date, date, date. Let us try to make it a bit better looking. OK, and let's see. This one over here, that's good with this one. Let us show this one. OK. Let's see if it still works. OK, super. So what we have done over here we have looked at the product , I have looked up the day of the week , I have looked up the day of the week , and got on the product. I've made use of this link, no more heavy JavaScript , just beautiful code in C#. If you like that, I like it team. -- I like it too . What I'm going to do here in this now is make a new function. this now is make a new function. I'm going to create here a new project. And I put the other one in here. And I am going to make use of C# . I'm going to make use of a trigger and I'm going to call this one company function. So this is some template code created by the generator. We are going to take basically most of this out. OK, that is nice. Before I continue I would like to try doing this locally. to try doing this locally. What I can do is, in my local settings over here, I can have here, cross origin , I go across any other site locally. My functions in this local port , let me see if I have got the script. This one can also go, I just want to leave the log in information . OK so basically this is going to get the date and time of the server. Let me see if I can start the if I can start the e-book stop . -- Debugging . Now I can go and call this one. And it should show the particular time. As you can see can easily localise your function. can easily localise your function. Let us try that again. I am also going to add this one , this is going to be like another function so I could take out this one for the moment. And it could be in some other classes. OK, super. Great, so everything should work here. Generic collection. OK, super. Should work. Now in my application I want to get the date and time of our service. So in our task we going to put in a call to our service. And I'm going to call that like this. And of course we need here ... Date and Date and time ... I'm into copper string just to make sure. OK. So this is going to respond to this So this is going to respond to this . OK, that is nice. Let us see here. Let us see here. This one. This should supposedly run , let's try that again oh 02 because ... So now if I run this one and over here, now let's see. This should have the current day and time as the function. So let's start this one. So here I have the break points coming from the application. And I have got the time. Just to make sure, you can see here that we make sure our function gets hit. OK, cool. Let me see how I am doing on time. For the people in the Netherlands do I still have enough time? To go on? No response so I guess we are good. I am going to go let this run over here. It is a bit annoying for me to keep on typing, so let us fix that one and go here too shared. I need to be in a different menu. I will have to type everything myself. This name corresponds to this name. So now whenever ... I did not build of course. I did not build of course. Let us see here. As you can see you have to recompile . Because behind-the-scenes this is all being translated, compiled into assembly. So here is my shop . OK, great. I do not need this. Now my file over here , whenever I do something , I also want to make a purchase. , I also want to make a purchase. Let us grab that over here. Let us quickly add some test code to see if it works. OK. My server over here is still running supposedly. Let's start this one. Now let's say we are going to buy nine of these and five of these. Here and debug. As you can see, I got two orders because I made up some sample codes. Now we're going to send it to our logic Series. Now it has everything over here. This morning, when I was rehearsing again with this application, because it did not work anymore. For some reason the latest new version of this I tried my logic at -- at --@ did not work for me anymore. So if I now can go into this one basically now this is just called my logic out on this one. So I will get rid of this one ... and that does not want to seem to go. What I can do here is making you file -- make a new file and are going to mimic just like I would keep post. So basically this is my workflow, I'm going to show that soon. Here I'm going to let say Hank is also on the call and Hank likes to eat a lot of croissants. As you can see, it is pretty fast and took a few milliseconds to get call and response back. But I did hear is mimic what I would normally do here in code. As I told you since this morning, when I looked into it anymore, this is exactly the same service but unfortunately somehow the Jason (inaudible) anymore. General fits that we have a new version since I tried it or something else, so I will have to figure it out later on. I might put a blog post about it later on as well. Luckily, I can still make it work like this. If you're curious what I'm using here, this basically just like an extension as you can see over here. So functions, storage, as her, -- Azure, C#, and the line looking for now is called RES T T client. So there's a nice exhalation everyone to get used to it. If I don't want to open post man, I make use of this one. You can try it out yourself. It works very nicely. Super. You can see, I am not kidding you, I ordered Hank and this gets added. Had I set this one up? It created a new one drive and added a new Excel workbook. And here I give the date the name so for example I can do here three 3/12/2020 and the name is Chris. So the trick here is to insert the table. This so you make up a table. This is also what you need from logic apps that you can connect these two things together. So I have shown you those, you saw here with these just got called in. Let's try that one again. Just to make sure that I am not kidding you. Hank, very famous and that those in the past, let's take this 48 croissants which does seem that it more expensive. That one again. And hopefully, in a couple of seconds., Look here. So you see, that is still working. How did I create that one. How did I create that one. It's just by going to a logic So whenever an HD PE request is conceived, were going to do this. I created here whenever new designer is online this is been generated by me if I put in sample codes. Basically what I did was say what I just showed you to put in the post command plan that over here, it made a small skiing over here, and just receive something of what should we do with it? So I could hear the body and as you can see here there are other things I can make use of here there are other things I can make use of for work it's in. Then the next step, because this is a collection, I want to have Roper row, item for item invite Jason collection, I want to write it down in that Excel online file. So here for each, I make use of the body from that former step which came in here. But I also find handy is that this colour corresponds to this colour, so it is for each control. Here's my excel file. So here I can connect that one for example, and here I can select my excel online file. I can select my excel online file. And here I can select the table, so because it is Excel online I did not open it in Excel on the desktop. Just give me table 1, otherwise I can change it over here. And this is basically all the things from Jason that has been showing, that's another set. It's another variable sorry that I could make use of. Here I'm going to respond -- correspond in my Jason file that requested that I sent. I can make use of datatype quality and then thanks to this one I can make this other grow into a table which something from logic apps itself. I can have it inserted into my Excel file like you just saw here. So far we have been using everything here off-line on my PC. So even though I want to create, for example, something to upload, I already did that before because it is good to take some time. Of the function at, I can just point to a function app series and insert my description, for example. And I can create a new one which needs to be globally unique. It is not yet.net 5.0 here in Western Europe. So it will send this one out. For the shop. It is something similar to go to put it online on the account. But we need to do here now the following step is we're going to publish a local assembly document published. He was going to create a DLL file is going to be here whenever it is done. Students in tree shaking her finger, compressing, so it's not too big. -- It is doing some tree compressing, so it is not too big. That is unique enough. OK, so this one is ready in the meantime. I think I made a small mistake over here. Either way, this is going to keep on running, so I already did that for you. Now what you can do is in this published file, right click here he will see the index file. That is the starter point. This is an important one, it has been enabled for static web hosting. I do not know why it is put here that is trying to make something but not finding anything. So that is good. What we now can do here, for example, is to pull your static website for your storage. And I'm going to make use of this one over here. I just made an azure test, as I recall correctly, Blazor . As you see I have been trying on this account a lot already. Now it should start. Of course, what I did not doing this one is basically not change all these URLs. So these URLs are still putting out to the local version and not to the versions are going to be deployed online. So make sure if you have So make sure if you have developed some scenarios and put these keys in, I just want to show how you deploy that over there. Just make sure you get the URL from the functions. Secretively logical over here, this is a sample. One that I created before. Here I can see, for example, functions. And here I can copy the function URL, for example. I just want to make sure that we get 'The Book of Not' over here. -- (inaudible) over here. Now it is a bit faster. Now it is a bit faster. The reason I do it twice is because I want to show you another pop-up saying that I want to replace something. Replacing means deleting stuff, and then later on uploading it again. So this is done. They can now be published. I like use of this one again. Soon it will say delete and deploy. Soon it will say delete and deploy. I feel I was going down, so I'm going to drink quickly. It is taking a little longer than I expected. As we are going to wait I am going to round up over here. So let us go back to this one. Hopefully you liked the demo, there is now time for the Q&A. If you want to reach out you can connect to me on Twitter or email and let us see over here. If it is slowly going to upload it is taking longer than usual but it should be OK. So back to you guys for some questions. Thanks . One of the questions I have is whether assembly , whether assembly in general is going to be taking over? From JavaScript ? There are some articles written about this, is it going to replace it, I think is going to replace both of them. both of them. There are some people out there who are still writing JavaScript so JavaScript is still going to be there for some foreseeable time. For me personally For me personally this is interesting because even though I wrote a lot of JavaScript I was trying to make things on the web. I also played with G query a lot of the time, those kinds of ready applications. For the last couple of years For the last couple of years I did mostly back-end development. So a lot of C#. I am not really following up another new things or whatever. I wanted to do it, I would have to learn it again. again. But with Blazor it is like I am writing C# code. So for me as a developer I can be more productive more easily. So from that point of view, for me personally, Blazor is the next best thing. I'd not mind it for being productive quickly , so like , so like half a year ago had to do an application for a customer and consider trying to figure out how I should do it, what framework , within a day I had a folder full of application. So JavaScript is still around but Blazor is that estate as well. -- Is there to stay as well. Blazor is the flavour of Microsoft , we do not need a plug-in like we had four flash, and I think that is a big advantage. It is fully supported and that is why I can has a future.  Good stuff. He showed us a lot of stuff in about an hour I think, and I want to thank you for that.  Thank you. Yeah, see you again soon. We'll have a conversation about security, about security, about hosting your WordPress site, alternating or social life, a lot of stuff going on today. Check the schedule below. Also we have a skills challenge and the URL is down below. You can win, win, and have some fan, so just join the challenge and join the fun. We are going out for a very , very short break. And after that we are going to listen to Eleftheria Batsou on how to design with your users needs and expectations in mind. So yes, I would like to face you after the break but that is not true because our hosting duties are now over. We are going to switch around in the break after after and you will see the new hosts. Goodbye. (Video plays) Hello everybody. My name is Eleftheria and it is an honour speaking here, we have hardly had some amazing talks and speakers was to feel free to catch me on social media , you can find me on my email or on my Instagram. Firstly contents, we're going to talk about user experience vs user interface, and what are the differences between those two things. We will be working through Heuristics and how you can benefit from them even if you are a developer. And we will be talking about the dos and don'ts of user experience, and the good and bad of user interface. If you search over the internet you will find that the tone of content out there , what is different here in this session is will be trying to cover this topic from two different angles. One is the perspective of the designer. Although, again, you may find a ton of content out there, most of them afraid to do a poor job . Let's get started. We will start from the users perspective as it is easier to be understood. From the users perspective , a user interface is what a user interacts with in a product. What they tap or click on, and the entire experience is localised within a device. Such as a mobile phone or a laptop. On the other half, the user experience is about the user interacts with the product, how a user feels as a result of the interaction. The whole experience is located in their physical body. You can understand if he or she is happy or not. Let us talk of it about the perspective of the designer. Here, things are a little bit different. When you see a big icicle like in the image you only see the tip of that huge icicle. But beneath that one is a whole mountain. If we are going to transfer this to the user experience world, we can say that the interface is the top of icicle, and the experience is what lies underneath. So from the designer's perspective the user interface is what you can see, and interact with. But the user experience is what you can see alongside with what you cannot see. And hold everything together. So let us go a little bit deeper this time . And try to give some more detail , and hopefully you will understand these terms a little bit better. Let us start with user experience again. It can be subjective, does a user connect emotionally with the app by cracking a smile? Are the icons and illustrations understandable , is a user pleasantly surprised by the interaction as well as feedback? And it can also be objective, does the user actually accomplished what they want to do. Is the speed of response from the application satisfactory? And is a swiping gesture more satisfactory than a tap, all of these things are questions that will be answered with user experience was it has a lot to do with the function of a product. It's about the function and usefulness. Let's start with an example. Would you prefer a ride sharing app that does not show your favourite destinations, does not let you have multiple stops for you and your friends, does not show where you are in real-time, where your driver is, where you are heading. Or would you prefer a different kind of app that is have your favourite destinations, we will let you have multiple stops, for you and your friends, it is going to be quick, is going to do things. Which one would you prefer? That one was a drastic examples I think you get my point. Sometimes when we are working on our own products because we know our products we feel like everything is fine, everything is cool, everyone can understand. But try to give it to a user ... Not explaining to them anything about your product... Will he or she be able to use your product? Will he or she be able to use your product? Will they be able to find everything in your product? Let us see. I hope that by now I have convinced you why UX is very important. It is important because at its core it is about putting people before technology. The focus is always on creating positive experiences for users, and by some resort , some methods, it creates a useful product which is a win-win situation both for the user and for the business was both for the user and for the business was . So what does a UX designer really, really do? Before I give you the answer to that question we will do a quick history lesson. We will get to the answer, do not worry about that. It started with this person in the picture on the screen right now. He is the founder of UX. He was an Apple engineer , but he continued with his studies as a ecologist. He focused on observing people, observing how they were using computers , how they were using technology. So he was in that field before people even thought to see how people interacted with that field. So he made UX as being at is -- as we know it is. If you want to learn more , he has written a ton of books, and one of the more famous one is the Design of Everyday Things. Let's go back to our question. The single answer is UX songs real problems for people , and UX practitioners practice UX. They are the facilitators who bring everybody to the table together. It is not only other designers but programmers, marketers and, of course, users . Users are the most important thing for a UX designer. The UX designer will ask some questions, like, "Who are the users and what do they want? What are their current behaviours? How will the product connect with them? What type of content do users want and need? " When they have the answers to these questions, they will go in multiple rounds, in muffled cycles -- in multiple cycles and ask other people like programmers, stakeholders, everything and everybody who has something to do with the product. So, continuing , we can see this pyramid. These are some of the things that the UX designer will have to do. Very quickly, let's see them. I will start from the bottom. I will start from the bottom. We have user research and analysis, content strategy, information architecture, interaction design and visual design. Again, starting from the bottom, a UX designer has to do a lot of things that have to do with user research. Some of those things are user interviews, behaviour studies, focus groups and surveys, and when he has all of the information he or she needs, he can continue with creating user personas and stories. When we have all of those pieces of information, we can continue to the next level, the next step, which was the content level, the next step, which was the content strategy. This is a really crucial step. Some people may think it is boring. But, after this step is complete, the designer can continue with information architecture. Again, this is a really important step that connects the previous one with the next one that will come. come. How the data is presented to the user plays a big role in how the user will perceive the product and how he is going to use it. At this stage, some of the things that have to be done are sorting, side maps, wireframes and user flows. If we continue more, we have interaction design and visual design. In interaction design, not only the UX will work but also the UI designer. We will speak about the UI design in a bit. Stay with me. In this step, the designer will have to see the user interface. He will have to work with interactions to enhance feedback and interactive prototypes for user testing. prototypes for user testing. In the last step, in visual design, we have to make sure that the principles are there and we are working according to those things, according to the standards. Some of the things we have to take care of our combining colours effectively, using typography and also having some icons and images that are satisfactory for the user. OK, so, by now I think that you have an understanding of understanding of a UX designer, what he does and what is the discipline. Now we can continue with the UI designer. We are going to follow the same structure and we will start with what is UI? we will start with what is UI? A hint . it's not only how it looks. A lot of people think that you are hers to do only with -- that you are -- that UI as only to do with colours and the interface, but that is not yet. If you are one of those people, you are wrong. A UI designer works on other things as well. I am going to give you an example of that. You are -- UI design is not only visual but it can be audible, too. Imagine Alexa . You can just shout out to a machine and it will give you an answer. It will tell you the temperature, the time, it will play songs, change the temperature of your room... It can do so many incredible things. This is UI, this is audible. Let me give you another example. UI design can also be textual. I didn't say I would give you a good example, just an example. Imagine a terminal like this. It is just usually a screen and you type something and you type something , usually in a white colour, and you have a minimal type family and font size. It is minimal. But again, this is UI designed. OK, so, let us see why it is so important. A well-designed UI as valid your product. It creates a friendly environment that users can understand. It establishes a strong and clear visual language. language. And it creates smooth interactions and an enriching experience. What does a UI designer do? This time, I'm going to show you this pyramid, but we will focus on the top two levels, interaction and visual design. In these two stages , buttons, colours, typography, but he will also give finishing touches to the whole product experience. They UI designer somebody who has to have a lot of knowledge in many different things, such as typography, colour theory, iconography, illustration, visual hierarchy, alignment, layout, spacing, readability and contrast. There are many, many more things. Now, in the last step, in visual design, here the focus is on the screen and how it will respond to the user's input. Everything has to be covered. At this stage, the designer has to design patterns effectively, create component libraries and find technology changes, which happen what the time. The designer has to be very well informed about the latest trends, which can be quite tricky! Alright, I hope that by now you have understood the differences between UX and UI , and I hope you have understood a bit better what UI designers and UX designers do. We can all have . mock -- we cannot have -- we cannot have UI without UX. They are connected. In the perspective of the user, these two disciplines are quite distinguishable. A user can understand what is UX and what is UI. From the perspective on the designer, they are more connected. Now we will change gears a little bit and we will start to talk about heuristics. So, what is a heuristic evaluation, why do you need it and how can you take benefit from it as a developer? In simple terms, in a nutshell, a heuristic is about making decisions with information that you already have. I will start here with a quote from Don Norman that I really like. Think about that. It is easy to create a good -looking product but how about a useful product with a function for the user? Now, let's see heuristics. What are they? It is a key part of designing a great product that users can easily engage with and find value in their interaction. The purpose behind that is to detect usability issues that may occur when users interact with the product and also find ways to solve those problems . It is conducted against a predetermined set of usability principles, known as heuristics. We are going to talk about those heuristics in a bit. Now, although there are numerous heuristics and numerous ways to inspect them, we will not focus on everything due to time, obviously, but also because the validity is different, although the end goal might be similar. I am going to show you the one you can see on the third column, that is the heuristic analysis. So, let's dive into that. How to conduct an effective heuristic evaluation? It is something that is being done in steps, but the steps are like well thought out. You have to follow a specific order. Let's check that. The first thing you have to do is to define the scope of your evaluation and ask yourself , "What do I want to test?" This is one of the most important steps. Secondly, know your end user. Who is the product for? Who are you developing for? Ask yourself which usability principles should be followed, should be followed, continuing with setting up an evaluation system and identifying issues. How well evaluators assess and report a user issue? Last, analyse and summarise the findings. What do they show? Again, this is one of the most important steps. Maybe you know your users. Maybe you have a good product. But how you can improve that, what your refinements are and how you can benefit from their . you have to analyse them. Let's also see another statistic that I find quite impressive. This is a big number, and you can understand how important is heuristic evaluation. Now let's dive into the third-party of this session, which is 10 usability heuristics . As we said earlier, Jakob Nielsen is the father of UX. He, with Don Norman , developed usability heuristics, the 10 rules that you should be following! Let's start. The first one is visibility of system status . Always show your users the things they have received. Imagine, for example, your mobile phone. Even if you hold it right now, you will see quite a lot of information, like the time, notifications, emails... You will see the temperature . Maybe you will see your steps, if you have a counter. You will see a lot of things. How would you feel if those things were not there? What if somebody removed those things? Wouldn't you feel like something is missing? Always show your users the things they have to see. see. The secondary will is too much between the system and the real world. Users use other apps. Don't try to change things that work. We have home office design. As a designer, you will As a designer, you will try sometimes try sometimes to design something as it is in the real world, for example think of the icon you have for the compass on your mobile phone or the folder on your mobile phone or , I don't know, the trashcan, maybe. It is the same as we have in real life. This is because we want to have some similarities between the real world and our phones, so everything is easier for the user. Don't try to change everything. Don't try to change everything. The third rule is user control and freedom. Give your user the freedom to do what he or she wants to do , and always have something like an emergency exit mechanism. . Sometimes the user will click something, they are in a hurry. They want to take care of something. Always have that mechanism to go back. The other thing, which personally I find the The other thing, which personally I find the most important, is consistency and standards. Just because you are different doesn't mean you are useful. Think about that. In the sense of your users , the user 80% of the time will use other apps or other products and not yours. Again, knowing how things work. Don't try to change them. Think about the functionality of copy and paste. It was everywhere the same. You don't have to change that. Let as continue with the fifth, error prevention. This is quite important. It is believed there are two types of errors, slips and mistakes. With slips, there will be some errors because you were not careful and you cannot do a lot of things about that but as I should said, you should have some feedback mechanisms or clear exit buttons so that you can undo the action you try to do. The second part of errors are mistakes. This one is a bit more serious because it means that the model is wrong. It should be , a mistake should be something that should be found in the testing phase. This means that the user wants to do something but they can't accomplishment because maybe there is an error but it doesn't have anything to do with the user. It has to do with the way the product has been designed. Let's continue now with the 60 - 41 which is recognition rather than recall . . Recall is when you try to remember about something, usually it has to do with numbers and sometimes with people, and names. We have made a quick change over the last session. I will be hosting the next few sessions alongside Esther. For those of you who missed the opening session, I am Esther . I am a technical consultant with a computing background. Currently switching to DevOps , , cloud DevOps. cloud DevOps. We are also organising this event from the studio for it is very nice to be here. It is the first time in an event like this was I am looking forward to the next session. I don't know if he is already in the session but let's bring him in.  I am. Hello Aaron. How are you? Great, thank you. I have read something about you. I see you call yourself a DevOps engineer but also the architect , which is interesting to me. How did you make that work? What I like to do in my job is that I like to be in multiple areas. I like to develop stuff, thinking about how we can architecture our systems. architecture our systems. But I also really like to think about making sure that if you make a change in code or configurations, how do we get that into production as fast and safely as we can? I kind of like to be in all of those fields at the same time. That is what I really love to do.  I can relate to that. I am feeling the same, working as a architect but also working with the DevOps team. Today wille will be doing a session about (unknown term) was without further ado , let's go to your session . We are looking forward to it. We are looking forward to it. Cool , let me share my screen. Right, so, for the next for the next 30, 45 minutes, we are going to be talking about cake EDA which stands will Kubernetes , event, driven autoscaling it helps us to do autoscaling on cabinets. Kubernetes . It mixes It mixes makes it easy to do (unknown term) we have already talked about me for top I am from the Netherlands which probably the only thing we didn't talk about. I want to go over what is a container I want to go over what is a container Kubernetes. If you have not done anything with it at all, you can still follow the session and see where it fits in and what it can do for you. I will be really quick for those who do have experience. What is a container? It is a simple, lightweight package It is a simple, lightweight package which contains whatever your application needs to run. Where in the past you had to install all of the dependencies for a application , to make sure a server was available. All of that stuff is now in a container. Audio server needs is a container to run that. You don't need all of the specific dependencies of your application anymore because they are in the container, making it very flexible. Having a container is not enough. , especially if you want to run a container in a production environment. You would want availability, (unknown term) and stuff like that. We need staff to manage all of those containers in the production for us. That is where Kubernetes comes in. It is a container orchestrated. There are more out there but Kubernetes is the most well-known, and most used . . It is available on all of the bigger cultures et cetera. The main components are the masters and workers was the master make sure what you want it run on your cluster and it actually does run on your cluster. The workers are responsible for doing the actual work, that is where your actual application will run. If you want to run anything on your Kubernetes cluster, you provide your master with a definition of what you want to run. For example, you could run a (unknown term) which specifies what you want to run. It is very simple. You will get the idea of how to work it. You have to specify a few things in this realm, like what is the container you want to run . On which portal do you want it to run , and how many instances, replicas of the application do you want to run? You create the file and give it to the API, master and then the master will try to figure out figure out that particular run and tried to divide it amongst the workers you have in your cluster. It would then continually try to monitor, if the stage you told it to be in the application, if it is always the state in your cluster. If in the unlikelihood event that the work is carried down, because of a power outage or another failure for example, the master will tell us that you no longer have three replicas running but just two. The master will try to get that fixed for you by moving the workload into another worker, making sure it still fulfils your needs that you specified in the demo file. These are the basics of Kubernetes and getting something run. Let's get into how scaling works. Scaling in general can be done horizontally or in a vertical manner. By horizontal we mean that we would add additional machines to our computer resources. So you could add a additional virtual machine or hardware. virtual machine or hardware. That means the resize of the machine , it needs to be already had. We could be adding additional use, memory or stuff like that. If we then look at what there is in by default in Kubernetes in terms of autoscaling, this is the table we get. There is autoscaling and in Kubernetes we can do that based on nodes or pods. Our kind of the smallest thing we can run on a Kubernetes cluster. We can scale the number of nodes in our cluster and also scale the number of clusters, pods that are on it. We can add or remove nodes . In terms of vertical scaling, we can resize the nodes, add more CPU, memory et cetera to the existing nodes we have in our cluster. As we can also limit the number of resources that a pod can use, we can also give it more or less resources as it needs. It depends a bit on where you run you the cluster and which of these features you get. Not all Kubernetes versions have all of these for options available , so you would have to look into the documentation to find out which you can specifically use on your cluster. specifically use on your cluster. But the thing is always that this whole scaling thing in Kubernetes can only react to the sensor. It can see that the CPU is going up or that memory is low. It can't react to the actual cause of why it is happening. That is where KEDA comes in because it can. We can do autoscaling and actually react to events, rather than CPU going up . We can also install it on any Kubernetes custom week like, any cloud or machine. It does not matter. It is very easy and simple to do with their help charts that are available for you. This is a architectural overview of K EDA, so let's go through it. It only exists with three components and tries to average what is already in Kubernetes by default. Let's talk about the scalars. They are the things, components that can read a particular event source. For example in a demo later on we are going to see a demo of (unknown term) which uses a AJUk ey . There is a scaler that can read how many are available in the A JU k ey to get the metric and the skill based on that. Today there is quite a big list available of scalars in in K ED A by default. This is the holiest. list. Although K ED A was initially created by Microsoft and Redis, you can see that there are a lot of scalars available for non-Microsoft staff. There is cloud stuff, Google stuff There is cloud stuff, Google stuff . There are lots of scalars that can help you. you. When a scaler grabs a particular metric, KE DA uses its metric adapter to expose that metric to the horizontal output in Kubernetes. Most of the scaling of your workload isn't actually done by KEDA, it is done by the altar scaler . I say almost all of the scaling, I mean that Kubernetes by default will scale your workload from one and up, and down to one . You can never scale down to zero instances of a workload. With K ED A you can, that is the main control you see in this image. If we want to scale up from zero, that can If we want to scale up from zero, that can happen as well. That's a nice additional benefit. Other than that, KEDA uses what is available in Kubernetes already . One important thing here is that a scaler with another grab -- will never grab the message . It is still your workload to do the work. It is still your event source. What do I mean? This is how KEDA works in the flow. The scaler will reach out to the queue to find out how many messages are there and then a potential scaling of your application happens. Your application needs to go out to fetch the data. KEDA is not going to do that for you. There are alternatives in the market to KEDA to do scaling on Kubernetes and function, for example. They do it a bit different. They do grab the data from the queue, and actually they also passed the data into your application. Your application doesn't need to know anything anymore about the actual underlying event source. Your app doesn't need to know anything about the queue, only HTTP. The downside is that because you don't need to know any more, you also can't work with the benefits . So, there are trade-offs between the two, but KEDA not giving you data, that's up to you. So, the first demo we are going to look at today is one of BLAISE ROWSWELL: 's user functions. Combining with KEDA is a powerful function. We are going to run that on a combination of AKS and Virtual Nodes. Here , in this event-driven example, , in this event-driven example, there's really a good combination , and here is why. Imagine that you have a queue, for example, and that is being used to import data into your system. Sometimes, a million messages are being added and need to be processed. Other days, there's nothing to do and nothing gets added. The workload is very unpredictable. So, if you would handle that on your already proficient cluster, you might be overloading that, because it would have to scale out . But with the combination of the servers , you can give the workload to the container instance . Should I run this workload on Azure Kubernetes Kubernetes Services? It will have it destroy all containers contained on the Services. It's a powerful combination. As a developer, you only need to speak to a single API , to a single endpoint, to trigger your workload, and you don't have to do that yourself. That is ready cool. OK, let's dive into a demo. So, I'm going to show you how to run a function on Kubernetes , and I'm scaling that using KEDA. This is how we can do that. Have it in a current folder and then we can specify that we want that in a container. You can choose a particular workload. The functions are not just about Microsoft or C#. Microsoft or C#. There are other functions. We are going to create our first new function. Then we get to pick how we want to trigger it. In this first demo, we are going to use the Azure queue. That has come up with a very original name. It is going to do a template here to get that function going. As you can see, there's quite a few others that it came up with. Let's open it up a bit. The first thing you will notice is that we actually have a Doctor file. -- Docker. It uses the Azure It uses the Azure function base image as your function right here. For those who have never seen an Azure Functions before, this is all there is to it. The thing has a name . Here is the trigger defined. This run function is your entry point into this particular function. All I need to do now is specify what is the name of the queue we want to use, and then specify the connection string to that particular queue. So, when we would have done that, you could be deploying your application . Let's quickly look at how we would do without. The only thing you would really have to do here is to specify your connection string. I am going to leave that out for now. If you want to deploy this application, we could again use the Azure line and run this could again use the Azure line and run this particular command. You have to give it a name. Let's use, again, very original hello-KEDA . We have to specify the registry that we want to put this image into. Here, I am using my own. If I now hit enter, in the background it would start to build the container image in which my function would live. That would push it out into my registry. It will then turn my Kubernetes cluster to start deploying and run that application. That will happen in the background for you. If you actually want to get a bit more control over what is happening, you can do something like this. We could output whatever is being done there, in the background, into a file to see what is the background, into a file to see what is going on and to be a bit more specific in what we want to do there. Let's move over to another window here. I have already done that and created the image, etc. Sometimes, that takes a bit of time, so I did that beforehand. that beforehand. You would have run that command we see in the previous window and then end up with this deployment. It specifies what should be run to get this function running. If you have ever worked with Kubernetes before, dish should be kind of familiar with you. This is just a secret. It is now contained with my connection string. Here is the deployment, which specifies my container image that contains my Azure function. This is default Kubernetes stuff. Don't here is where you -- down here is where you see stuff specific to KEDA. It is a KEDA-specific thing. This is where you specify the scaling of your workload. Down here , it specifies, again, the trigger I want to use. For example, here I am using the Azure-Q trigger. This is available since version 2 of KEDA, which was released just a few weeks ago. We cannot have multiple. For the trigger, you need to specify a few parameters to make it know how to do the work. You would need to specify where it gets its connection string, which Q should it be watching, stuff like Q should it be watching, stuff like that. There's also a few additional , interesting settings you can use here as well. For example, in how many seconds KEDA will reach out to get the metric. I have set that to a low value here. But as because it is a demo. You also goes -- get to specify a value. That is a cool thing, because now you also get to limit the scale -out. That comes in handy because if you would let this scale out to 1000 or 2000, which could easily happen, you would probably overload your Sequel database, if you need to call into that . It comes in handy to be able to limit your scale-out here. OK, that's how this is being built up and what needs to be put in place to do the actual deployment. Now we have this YAML file . I have already done this, because it could take some time. Let us take a look at what it looks like if we put it into action. You can see it has already been deployed here on my Kubernetes cluster. That's the one over here. Alright, so, let's have a look at some internal LAURA BURFOOT: -- Let's have a look at some internals of the cluster. You can see the AKS node. There's also the virtual node. There's also the combination . What we can also look at is the fact that there should not be any pods running. So, as I mentioned before, we can scale down to 0 with KEDA, which you usually cannot do by default. As you can see here, it says that there is zero running. If we look at the horizontal pod on the scaler definition, you see that it has been created for us. It references the deployment. We just saw a YAML fail. It says it has a minimum pod of one, because it can't scale down to 0 default. It took me a few hours to figure out . I hadn't noticed that particular thing. This is actually what you want to see. That is normal. Let's have a look at how that scaler would work, then. Let's have a look at that particular deployment. As you can see, the deployment is there, and there are zero instances. If I would then give a bit of workload to my particular queue, we should see something actually starts to Gurion. What I did here is create a simple application that can put a number of messages on , so let's have that running. It should say somewhere down the line that it is done. done. There we go. So, we already see that KEDA picked it up and started to scale our workload. It will continue to do so until the queue is empty. So, let's head over to the Azure portal . Then we can see something interesting here. As you can see, there's already four instances of my Azure Functions on the cluster. You can also see that one of them was deployed to the AKS Poole and the others were deployed to give virtual node. The one deployed to AKS happened pretty quickly. It's only needed a few seconds, whereas the ones on the fortune node are still spinning out. They do take more time to get on there. Let's see , it scales up to a right now. You can see it can start , it should be scaling back to zero because it processed all of the events and it should be scaling down right away. There we are , it started scaling down to zero because there wasn't any work left to be done. Right, that is the first demo on how to create a Azure function that will read a Q, the implement it into your cluster and then do the actual scaling. Let's have a quick look at another example. As I mentioned before, you can run it on top of any other Kubernetes cluster. A combination of AKS or nodes but the next demo that I will be doing on my machine, I will be using a combination of K-3 S and K-3 D. K-3 S is a lightweight version of Kubernetes . It replaces some of the main components with smaller ones and it makes it more lightweight, being able to run being able to run on a different machine. You can use K-3 D to actually run docker inside of docker , which lets you run a multi-node Kubernetes cluster on your machine. Then you can do testing on Kubernetes on your local machine and still have a kind of identical Kubernetes cluster with multiple nodes to test about. The next thing we're going to look at is how we can use scaling with KEDA , based on metrics that we can find in Prometheus. Prometheus is a timeseries database where we can store a lot of metrics . We can then query that database through KEDA database through KEDA and do scaling . I created eight .NET Core API and exposes Prometheus metrics based on the number of requests and the duration of the request et cetera. KEDA then reads it from Prometheus and when certain thresholds are met, KEDA will start to scale the API to start adding more instances. Let's have a look at how it will look like . So the first thing we should probably look at is the start-up of my controller , to let you see that adding Prometheus is pretty simple. It is installing a few parodies -- and then adding a default measurement. It will then add some default measurement s . The ones I mentioned before, they are exposed by default. You can also add your own metrics . I did that here in this particular example where you can observe , you can also do increments of particular counters if you want to. You could be counting the number of orders and things like that. The data will be exposed to Prometheus and Prometheus will start reading particular data and then you can use that to do scaling. So, let's have a quick look at the scaled object, to see what it looks like. It is kind of similar to the scaled object we saw in the previous demo. The only difference here is that we have a different scaler type, now that we are using Prometheus. It also means we have a few different settings to implement that work. Let's see that an action quickly. Let's switch clusters and make sure this stuff runs. This should now be an API, this should beat Prometheus and it is. This is the default dashboard in Prometheus that allows you to create the database, if you want it. We are going to use that today to track the user. So I have already specified a particular query here to access the data. It is the same query that KEDA uses. We are going to look at implication over the last minute. It should then start scaling the application. Let's see if it does by looking at the scaling again of the compartment. So you can see, this one , this is the application of the cor e. one for as you can see, it is running this time , it makes sense to have it live all of the time so you can get a response quickly. If I now try to make requests, that will be in the right and correct domain. Press it again. I am using a Kamala utility to trigger a node that I have written. If I go back, we should probably see the data, which we do not. Let me move this up. There we are! There is a little bit of delay between the metrics that are exposed and how quickly they end up in Prometheus. I guess by default it tries to fraction metrics every 15 seconds or so. So there is a bit of delay. We see the number of requests go up and that should mean we also see that KEDA started scaling our application for top ice at the threshold to three or five and we are definitely going over that. KEDA started scaling that particular API scaled based on the metrics. As we are scaling on Prometheus and Prometheus is a tool that is widely used . We can kind of write a lot of metrics into Prometheus. We can do scaling based on a lot of things will stop as long as we can write anything into Prometheus, we can use scaling basing on those metrics. It is a powerful combination to do your scaling workload. Alright, so, the last demo I have for you today is about how we will be able or how you can handle long-running tasks on Kubernetes. That requires an introduction. Imagine the following situation, you have your Kubernetes cluster and you have masters, and workers. For that particular point in time , you will probably make a decision to upload upgrade your working system , . What happens then is that we decide to spin up a new worker for you and once that has joined your cluster, it will destroy other workers and you simply remove them. But what happened if you have been running long-running tasks on any of those workers? Imagine you were doing decoding and coding on a particular worker and that process took hours to complete. If it then decided to kill your particular worker, that work would be gone. You would have to restart. Continuing that process is probably hard so you will properly have to start all the way from the beginning. How can we handle things like that in Kubernetes? By default , what happens when a node is going to be is going to be removed, a signal will be sent to your container to notify that it is going to be shutdown . By default you have 1/32 delay . In this time you can close databases and things like that. 30 seconds is probably enough to continue with ongoing work but if you are doing a long task, it is probably not enough. There is another problem, somewhere in those 30 seconds, you publish lose connection to your container. If you have no access, you can't do monitoring of the container. That is not ideal for long-running tasks. Luckily , there is something on Kubernetes that is called jobs. It is also called job control and that will ensure that one of our pods are successfully terminated before the node is shutdown or removed. A job does give you the guarantee that whenever you run a job, that will finish until the end even though a node will have to be removed restarted. It will always wait for a job to be completed for the pods that are running the job are only being cleared and cleaned up when the job has been completed. completed. The beauty is that KEDA can also do scaling based on jobs. Let's have a quick look at that as well. Let me get my editor . Right, so, again I created a small example for this one. It is even smaller than the other ones. I created a console application in (unknown term) C o o re , it will reach out to my fraction item, do something and die. This is where you will normally do long-running tasks. This is again a demonstration that KEDA can kind of scale any container. It doesn't really matter what is in their r e, as long as it can run in a container, KEDA can scale it. So, what is going to happen here is that I have already deployed this deployment and this time this will deployed a skilled job as opposed to a object. There are settings on how it will be done, how many parallels and completions can ache political job do. Things like that. The most important , what is the container you want to run inside of the job to get it actually to work? The last one of these, of course, is the trigger section which specifies that I want to trigger a silent on a particular Azure Q . That is a quick demo on how that works. Let's do a Q , CTL , job . As you can see nothing is currently running their, no processes or anything. If I add a message to this particular Q, it shouldn't take too long before KEDA sees the message and started scaling this particular job. It shouldn't take too long for it to complete and be done with this particular job. You can actually go into this particular thing , logs, jobs. It put a message in the log containing whatever I need in there. That is . Let me find the other one. Here we are. This specific line here. We got the message, processed, and then waited for something else on the particular queue. That is what I wanted to share with you today on the functions and autoscaling. This is a bit of contact information if you have any questions or you would like to reach out. There is a blog post and it goes through the first demo so you can go through the posture and getting it deployed . If you want to review the first demo, please put it there. I might get that . Use them for your own good. Thank you for joining me and let's see if there are any questions.  Thank you for this great session. There were a few questions and you have answered some about running tasks. Thank you for that. Another one is how fast will the container actually start when you go through the cold start time or any delays?  There isn't any other delay that you would normally have that would start to run a container . With the one expression, in the definition , the scaling definition indicator, it has this interval and it will be used to specify how many seconds to wait for each (inaudible) to fetch the number of metrics. If you send that to five minutes, it could take five minutes for KEDA to pick it up and scale the workload.  What is the minimum time you can set?  One second. I'm not sure if she should do that in production, but you can.  OK. Thank you for the question, but we are out of time and we have the next speaker waiting already. Thank you for being here and I hope to see you here next time.  Have fun today.  Thank you.  I guess this is my cue to remind you of our challenge. Check out the skill challenge in the link here. Stay tuned because we have a lot of cool sessions coming up, especially the next session. That is one I have been looking forward to. We have him with us already. Hello. Welcome.  Thank you for having me.  Thank you for joining us on this special day. I have to admit even though I am a bit starstruck by having what I call the Sentinel master ninja with us, I do have a non-technical question. That is just from my personal interest. I know you love flying and you actually are a pilot for a small aeroplane, not the big jumbos. How is that in the time of COVID? Does it mean the sky is the limit because there are less the sky is the limit because there are less flights happening? Did you have to switch to virtual flying to make your flight hours?  I have been very privileged to be able to fly this year. Especially in the beginning of the year when most flights were grounded or not there, there was a unique month where we could cross the airport at chef of Schiphol and we could do a flyby of the tower and try to land on the big airstrip as if we were a big plane. It has been a lot of fun to fly, but the clouds are the limit. I must admit with the flights simulator coming out in fall, I spent time flying virtually, but nothing beats going up in the air in a real aeroplane.  I'm going to take this opportunity to say I would love to join you on a flight and then make sure we are focusing on the technical things you are going to tell us.  We will go to Western Europe in made Amir.  You have built something in there.  You have built something in there.  You can also join us (inaudible).  You are going to do your session. If there is one guy in the Netherlands who knows about security, it is you. You are well-known for being the founder of expats live. I have seen Esther there. I cannot wait to see what your session is going to be about today. Let's take it away.  We have eight tips to share with you today. Let me get to my presentation. Eight easy steps to improve your security posture in as you are. We did the introduction already. People might know me, but I'm an MVP trying to specialised in security. The thing I wanted to convey is in this new cloud and hybrid world we need to defend, and we need to shift away on largely network-based protections and detections to something that fits a cloud world. Things are different. We have endpoints connected to the internet and we have different types of resources like platform services, but the connection back into the hybrid and on plan worlds. We need to figure out how we defend this new world. This would mean we need to adopt a cloud defender mindset. Instead of a server, we need to think what servers are running and how are they exposed and how do we protect them? The same goes for a directed force in our subscriptions and resource groups and so on. The highly privileged you users where we figure out credentials and the credentials themselves are some kind of credential pivot, but they are types of things we need to figure out. There are other network types like NSG or in the front door. We need to protect management aid. My first tip would be to figure out how to expose little to the networks. Restrict access to your resources. It is key that even though resources could be discovered through zero day, they cannot pivot away from other types of networks and resources. If you create it in any cloud, don't accept a default point. Make sure you understand what you are opening. It is a network we are connected to. Think if you can use a private link or other kinds of network connectivity and access. That is a real thing. I'm not sure if you have ever spun up a VM, and you have 3389, you will see hits coming from massive crawlers that are trying to bring forth passwords and figure out if you have tried any light passwords to get in. One thing you can consider is just in time access which is a feature from Axis Security Centre where you can see even if you need a default port, it won't be open all of the time. It will only be for a specific endpoint , like my company or home address, and for a specific duration and potentially with approval in between. That means it will be open for a short period of time to do administrative access . We will have a look at this in a second, but the other thing is the Asian network option has a lot of options for you to consider, not just opening ports or closing ports through network security groups, as those often come into things you need to figure out. We have talked about private links, but also it is the front door to explosion application. With the firewall, it can also give indicators of compromise to your Asia Sentinel environment through the security graph and something that adds indicators and things there. You can have a third-party virtual appliance that makes a lot of sense for networking. If you need access to any resource in Asia that are VM, -- A z ur e . You are figuring out access. You might want to do it on a different port, but there are massive crawlers on the internet that will try a lot of ports and try to grab high ports you are using. high ports you are using. Consider as your Bastian on . It shields your service, but provides you secure are deeply access. That is what you can do through your regular program, but there is also a web portal you can use to get the screen in your browser. The Bastian is providing you the space where your VM lives and it will be configured to access the outside internet. It supports connectivity from Bastian to your VM using SSL, encrypted traffic, so no snooping in between on the V-neck itself. It just needs one public IP address to the internet resources and the VM's can just have internal IDs and no need to have any public IDs on them that might make things even easier for you to disclose. If these are VM's and you need administrative access, look at it before you do other things. This is something that was provided out of the box. I would say the second tip is to figure out how to secure your identity. Identity is key. You have seen a lot of people talk about identity, and it is the new control plane. You get authorisation to the resource you need. In all of these steps, we can have built-in security networks to ensure that they have no access to identity and that only corporate and certain users are allowed in. It is key to any good security strategy. It is not about networking or getting Sims or other protective measures. Make sure you get it right and make sure you have a good security strategy. If you are using accounts from other tenants, look at things like az Azure Lighthouse and get access to all of the resources. It is also important from the share fact that often it will be connected through a disconnect or other mechanism. If those credentials are grabbed, there is potential for hackers and adverse areas to come into your on brand network through the credentials they have gained and access has been given. The ID is key and it is not something to think of lightly. It potentially gives access to other things and not just office 365 tenants. The other thing we are seeing is attacked identity is accelerating. We see automation, scale from the cloud. They are finding brute force mechanisms to compromise your identity and get in. It is accelerating, so make sure you get this right. A key metric is 60% of cyber attacks often start with a compromise of identity, such as spearfishing, spraying of brute force and dictionary attacks, but at least more than half start with I the attacks. It is under attack constantly. It is under attack constantly. They must have grown over the past year. Just want to make sure you do not become one of them. What can you do? Have a look at your Azure A/D identity protection which already gives you some automated detection and remediation. It has machine learning to find certain patterns in logins in your tenants and other tenants combined and it combines it with insights from Microsoft threat intelligence centre where they have a look at things like nationstate actors and so forth. All of these things combined in a a DIP will make sure the accounts get closed or flagged or blocked before bad things happen. It is part of Azure active directory. You do need a separate license but these signals can then be fed into other parts of AAD or your environment as well, making sure that if we find risky users or risky logins that extra steps can be taken before further damage happens. The same can also be fed into your SI EM as your Sentinel or any of the other third parties and make sure we use them as indicators of compromise in figure use situations. Microsoft analyses 6.5 trillion signals per day, so they are well equipped to find the things that matter and their machine learning is top-notch. They really find the things that help you prevent attacks. Another thing to consider is the Azure A/D privileged identity management PIM. Just in time access for your identity like we saw in the VM. We can make sure that we control access, that you can only get things like global rights or specific roles in AAD based on the requests you're doing. Based on things like validating your multi factor from your extra factor. But it prevents you from permanently assigning when you don't need them. PIM is built into Azure A.D., you do need a separate license which you can see here on the screen but other than that eligible users could require MFA which might require an approval step before they get that and or specify a reason why they are requesting that role so that management roles can then approve it. The great thing about PIM but also identity protection and Lighthouse, I would say any Azure identity related service is that it is auditable and we can alert on it. They will be logged into things like Azure Activity and some of the other logs, then we can also in our investigation or hunting look at signals or see certain behaviours from outside or rogue internal users and love them accordingly. So PIM highly recommended in any environment I would say. Of course the third tip is, understand your current status. Look at the cloud defender mindset, do we know how environment is currently running, which employees are exposed to and which accounts have a risky flag to them? That is hard, potentially, in cloud That is hard, potentially, in cloud because there is so much to look at. Things change over and over again. Without security controls in place, a lot of the breaches take months to discover or even longer. In 2019, there was a 95% increase in the cost of breaches. So the cost the company has when a breach happens when they have downtime or certain servers go down. So what could help that? I would say have a look at Azure security Centre, because for one thing it can protect your Azure workload, prevent things from happening like the just-in-time access or more. But more importantly, it gets you insights into the current status of your subscriptions. How are things configured, are they doing the right things do you have the right controls in your set up in your environment? There is a free and paid tier, so some of the more advanced options might be in the paid it here but certainly the free tier will give you all of the insights tier will give you all of the insights and I would say there is no reason not just to enable it and get this at a glance overview because it gives you recommendations. It gives you three mediations in the one click remediation button knowing that if you have not configured yet if you click that button it will configure and you'll be protected from that point on. Also it is the home of Azure Defender where we can then have things like workload protection on your VM like Lennix EDR. Also things like carbonated is -- carbon that is -- Cabernet this culture. Also machines get added, resources get changed, and deployments happen in your cloud environment will be going up and down. As your security Centre can also help you get insights into some of these dashboards you will see as your defender and you will see all of the things happening in all the insights to work from. Let me switch quickly to a demo. The first thing we'll do is have a look at Azure security Centre. Here I could launch into an overview page and I can see there is one Azure subscription and there are 30 active recommendations as well as 13 security alerts. There is also a button to go into your secure score. Of course this is arbitrary, but still it gives me at a glance information about my current status which has potential to improve because only one third of all of the recommendations are applied to this subscription, and when I click to the subscription and go deeper, I find that there are a lot of things I can still do. There are users that do not have NFA enabled, their vulnerabilities we need to fix, and just a one click button to quickfix things that you can use as you have the right to make sure some of these things go away as part of the recommendations that it gives you. So these will help you a lot that you can just have a look at. We can also go back to the security alerts which are the things we talked about earlier such as massive crawlers on the internet trying to do a brute force attack . There we go. That luckily failed, so my password is a bit more secure, but they are trying and trying and he gives you the information on certain things happening in your environment. These are the two VM's like VM Lennix EDR -- Lennix EDR. If you are an industry that you need to adhere to things like PCI for the financial industry, maybe ISO or you just want to make sure that I comply with the Azure CIS standards. You can go to Azure security Centre and based on the categories it tells you that you are not adhering to the identity and access management 1.2 and 1.1 but you are hearing to the one in chapter 2. And again from here, we can be very actionable, going to the alert, maybe quickly fix it and make sure that we are more protected. If we go into the Azure defender node, we see, and again this is part of the standard and paid option that we can do things like a vulnerability assessment. We can see 15 VM's unprotected. The just-in-time VM access we talked about in some of the implication controls, container image scanning, all sorts of these things to make ourselves more secure. Here with the just-in-time access, I can click the VM and start enabling it so that we only have ports available for certain periods of time. So these are the things that I would say help a lot and as your security Centre is a great option to look at. Going back to the slides, tip four would be let's dive a bit deeper into your environment. Because as her security centre you can look at regulatory compliance but if you remember this quote from John Lambert, the end of Microsoft threat intelligence centre, he said "defenders think in lists. Attackers think in graphs. As long as this is true, attackers win." Because it is almost undoable so trying to get to 100% will be a hard thing to do. do. Often attackers and adversaries are trying to find the shortest way to their goal. What are the minimum steps I need to do to get to a certain role or data and how can I compromise like -- these steps like a VM or other privileges to grab that data? So they are thinking in graphs, a short way, a route, if you will, to a certain resource. route, if you will, to a certain resource. One of the things people might know is a tool called bloodhound which is an open source tool freely available, great stuff from the people at Spector ops. They have released and Azure 80 version a couple of days ago. It uses the graph theory to real hidden and unintended relationships with your directory and also your Azure 80 environment. There are two parts which you can run independently. There is a collector which is a partial script that you run your privileges to all of the resources and potential connections between them and saves them in a big zip file. Then there is a visualizer front and that you import the zip file into which is backed by a NE O4 J database which allows you to understand and pivot through those graphs. The azure Hound is the name of the PowerShell module for the collection. It collects data from all of the tenants and It collects data from all of the tenants and subscriptions can read that you run it with and again outputs a zip file. If you combine it with Sharp Hound you get a full view on to azure attack paths. From what we have seen, about 100 From what we have seen, about 100 K user environments it takes about 60 minutes to collect which is a reasonable amount of time. If you run every now and then or at least wants to get the data in. Tenants, azure users, Azure security groups, apps, service Prince was, all of the things which might interest you. Here's a quick screenshot. You can look at an environment, take a specific resource and say give me the shortest path to hear. Cool. Let's jump back into the demo. I did the discovery and the collection ahead of time for this demo environment, so I have already collected the zip files the NE O4 J databases running and I can go into the act, bloodhound, there is a Mac version, and log into the database. I can start looking at a certain resource, let's take that VM again. And I can say well, give me the shortest path to hear. What you quickly see now is that this gives the information on how to compromise this VM. What roles would I need, what user has what access where, so the shortest path would be any of those four or five routes. This is what an attacker cares about. These are the things might be high priority for you, priority one, to start fixing. Make sure it is hardened or protected before we take on any of the other things in your environment. Pivoting through these graphs and make sure that you understand these resources in the paths to them helps. If you combine them with your A/D, you could figure out how to get from a regular user to domain admin, enterprise admin, global admin and what steps to undertake to protect them. So Bloodhound is a really great tool and something for you to consider. Tip five, implement endpoint detection and response, EDR, on your work. We talked about Azure defender four, we will talk about Microsoft offender now. EDR is not antivirus. Antivirus is often signature-based, your Riddick space, the things we know already and we need to be sure we have the low hanging fruit and cover them. EDR is also looking at patterns in your environment and starting to learn from them. So does focused on patterns and behaviour, I would say the known and unknown is that we can start to block. Microsoft offender now has EDR block mode so you can really prevent them from happening You could start using all of this technology. Of course, you can report back to your centralised console, security centre or whatever you are using for your clients on your network. You could do some advanced hunting You could do some advanced hunting and get a bigger picture with some investigation. This is not just DDR, but also possibility management and understanding the version and all of the other things in your environment if you need to patch these. We want to make sure we don't have any processes from the office application that involve macros or anything from there. Next-generation protection, EDR and mediation where you might want to connect the evidence or shut down some processes and this is something you can do from the console. console. Microsoft offender really helps and with 365, you can get information from cloud security defender for identity and endpoint to make sure you get the bigger picture. Let's go back into the demo. I will go back to my browser. This is Microsoft defender here. I can open up some of these alerts. In my environment, it says we have possible cobalt strike detected. That is a problem itself because it is used by adverse areas as a framework for automation and control. It comes on the conclusion based on four alerts that are happening. This is something to look at, looking at the machines in the environment, I can see this is coming from a certain computer and if we login to the computer it gives us information on the risk level, the exposure level from the threat and vulnerability and that is being used. We can go to the alert tab on the suspicious process injection and get more information on what is happening. When we have a look, that is interesting because it tells us that it will respond and then some execution got blocked by the macro inspection and it started launching scripts. That is certainly something worth looking at. If you have EDR mode , what you have prevented from happening altogether , and it gives information which is a senior vice president, so that is something to worry about. There are some nice things like mapping and tech framework, getting information on what is process injection and which active groups are using it and what can we do . It might be a big thing, and if you are worried about information altogether such as how they operate in these groups, the great thing is there is a threat analytics page in Microsoft offender. defender. I get information on a cobalt strike. Hiding in the red there is information and links that are potentially hit by a cobalt strike. The threat analytics I have seen are spot on and you want to know things about Mono crypt, zero logged on, the spikes, and everything happening in the power shell empire, this is the place to go. It is a great strength of the Microsoft defender suite. It will give you information on how things work. We can dive into this. The T number and the step it takes and the protection you apply. These are neat things to look at. That was tip number five. Tip number six, set up security monitoring. Why not? Make sure we have a tripwire where we understand what is happening. You are having Azure Sentinel and Microsoft 365 Defender. Some great concepts to work with. How about getting it into a bigger picture? Azure Sentinel has been the cloud native SIEM launched last year and received with great success. There has been limitless cloud speed and scale in Azure and that matters. Machine Learning built-in, easy integration with connectors and you only pay for what you use. These connectors are not only Microsoft-based, but for connecting AWS, your network environment, and there are three free connectors where you don't pay for your storage, such as office 365 or Azure Activities. Worthwhile understanding the identity and implications. O n e S e S IEM and it covers everything. You can dive deeper and not just stare at the text, but use your cognitive skills to dive deeper and get more information. deeper and get more information. Diving back into our Azure environment, we have Azure Sentinel workspace set up. What we will do is I will set it back 90 days so we have a bit more information and we can see things are happening and things get collected. I have got some data connectors configured. There are 61 available and more coming weekly. 12 connect and Azure Activity and Office 365 of in there. I have connected all of the things in my environment and they report back into Azure Sentinel. You get analytic rules out of the box. I only have a few enabled, but you will see the power of those. There is a ton of templates here. If I search for power cell empire, this is fully configured with all of the tactics and security events table will be used to ingest data. I can use it to create rules and it will be good to go. Going back to the incidents, taking a sample incident, going back to the suspicious process injection, there is a button to click on and investigate. We get and at a glance view of what is happening. We see suspicious process injection on the desktop. We see all of the things we need to worry about, but this gives us all of the things on the files, the people involved, and we can also click on the timeline. After that, we saw a process interjection. After that, we saw a process interjection. We might need to look closer. This is really powerful and something to consider. Tip seven, don't just think of alerts. This is important to me. If you are going after the last one by one, you are chasing your tail. You have people working from home or another IP address, or maybe they are out and about or working from the office has seen as an anomaly. There are so many alerts that come at you. One thing you need to figure out as a high One thing you need to figure out as a high priority, let's take an example. Let's say we saw an unusual log in from Indonesia. Maybe a user that is not normally in Asia. Maybe a VPN or something else is happening. Maybe a VPN or something else is happening. At 913, we saw an anonymous IP Tor T or exit node . . A VPN. Half an hour later, we saw suspicious power shell executed on the endpoint. That is something we need to look at. The last thing we discovered was an inbox forwarding rule for data exhortation. By then, it would probably have notified bad things happening. You can combine two or more alerts to get a true number of positives. We have the example of a compromised account leading to the office 365 mailbox expatriation. You can build it yourself, but it is also built into the product. There is a great tool from our colleagues from (inaudible) that can get things automated such as the power share model and you can get some use cases. It is really easy to do with the power cell model. Going back to the demo environment and sentinel, there is a new tab called entity behaviour. If I say VM Lennox ADR, the machine looking in this section gives us at a glance information of things happening . I need to set the last 90 days. This is flipping back in certain parts of the UI. You click on it and you can see the same timeline again. This is bad. Let's go a bit further. This can help. This can help. If you go into visual studio, you get some of the cases we recently built. All of these things you can put into a file with the AV sentinel model and make sure you get those in your environment in an automated way. Apply security with Asia ARK and that spans the control panel in places it has not been before. You can use that to attach to certain resources wherever they live. Manage them the same as any other Asia-based resource as you bring it into any other resource it has been in before, like AWS, or the hosting data centre. One thing to look at is the clouds for management, but also security. You have a Gerrard for communities and Asia data services you can use. Asia defender goes through as the art and uses that. You can enforce compliance and auditing even if they don't live in Asia and manage them from a unified portal and you remediate it and it gives you multiclass security protection. In conclusion, Azure has a lot of built-in controls you can use for identity network data security threat protection and security management. I would say that is the reason to make sure Microsoft is a security company now. Microsoft is a security company now. If you want to know more about Azure Sentinel, we released an e-book. Download it for free. I will link it on Twitter. You can stay up-to-date on my podcast you. This is what I had prepared. This is what I had prepared. Thank you for tuning in.  Thank you for giving us this session.  My pleasure.  I hope there is not a lot of background noise. It became a bit chilly in the studio, so we have an extra heater on right now. I hope it is not interfering with the stream. Hopefully you can hear us loud and clear.  Hopefully. We have some questions for you if that is OK.  Go ahead. What came through on twitch, will there be any identifying possibilities for filtering on MAC addresses on security rules MAC addresses on security rules in Azure? In which product?  I assume network, firewall in, that kind of area.  I need to understand more on the question. Feel free to do a follow-up question on Twitter and I will help answer it. I'm not sure where you would want to apply that at this point. We can take that to you personally. Thank you. The other one is if there are any certifications you would recommend if you are going from the beginning around Azure security?  There is a ton of stuff. AC 500 certification is good. It asks a ton of things and if you want to dive in to identity security, you have great courses from the Academy. If you want to get on the event aside, there is potentially in your environment before anything else happens. Some of the courses are really good as well. I will do some suggestions on twitter and if you have any suggestions feel free to add to them.  Next one is about your slide deck. If you are willing to share it after the session somewhere.  Absolutely. We will figure out how to get them to the audience.  Yes please, because you already made me aware of some of the rookie mistakes that I am making connecting to my Azure environment and forgetting to use Azure Bastian, and I hope no one heard that it is going to hack my demo environment now.  We can all help each other get more secure so I will be posting my deck.  Thank you, that will definitely help me.  Final question for you, this might be an interesting one. You mentioned Azure Arc for managing your own premise or hybrid servers, but there is also Azure Lighthouse where you can manage multiple Azure subscription from one of the same tenant. How did they play together? Can you even combine these like if you have Azure Arc enabled service running in a subscription somewhere, can you still manage them through Azure Lighthouse?  I have not tried it personally, but with Azure Arc, it extends all the capabilities you have in the public cloud into other places. I would foresee that if not now, then in the near future, that would be a possibility because all of these resources show up as resources through ARM. As your Lighthouse is utilizing all the things we have in ARM. So is certainly summing to look at but I would say they play really well together.  Alright, thank you for your info.  Thank you for having me.  I guess this is it for Martin, thank you very much. Hopefully we will see you next time.  So this is probably the moment for us to once again remind you of this awesome cloud skill challenge that we have. So check out the link down there and still sign up and try to become our new cloud skill master or ninja or hero. Badger? Of course. Our cloud skill badger.  And their prizes to win as well.  That is what I heard. So that should definitely be a good motivator. I am looking out of the corner of my eye to see if our next presenter is already eagerly waiting to be introduced. Hi Mark! Welcome to the show., So to speak. We had a very quick and brief introduction through LinkedIn to make sure that we were not complete strangers to each other We are never strangers in the community.  That is a very nice one. So you are not just an MVP and Azure enthusiast but you are also a great advocate for stem. advocate for stem. That is what I heard.  I do do a fair bit. I started off doing, if you have heard of (unknown term) dojo, I started off teaching kids how to code in Coder Dojo clubs and then I joined the S TEM Ambassador programme which means that, in the before times, I would go around to schools and talk to children about getting careers in S TEM and how wonderful it is to be a computer programmer or working in tech in general. These days I also go around and teach the teachers about how to do the computer science and the computational thinking, as I call it, area years of the curriculum, all the way up from five years old up to 18. So I have certainly got business going on there.  That is awesome, because we can always use more advocates and enthusiasm about any of the S TEM science related activities and learning skills. So kudos for that.  I'm an azure MVP but I do dislike to go out there and tell people about tech in general. I am in Northern Ireland and I am one of the founders of the Northern Ireland developer conference and that is a completely agnostic conference so we get people talking about all sort of things from project management to Java to typescript to DevOps and all sorts of stuff. It just brings everybody together. So I'm very much not a person who lives on one side of the fence.  Thank you for that, because I think we can use a lot of those sponsors in the community to not just look inwards and find our peers within the Azure community but also to look outwards to see how we can get new people interested and show the world show the world that we are not just a bunch of geeks, we also know how to be socially interactive, even in these times.  Geeks with social skills, what will the world think of next?  It is a special breed, I have to admit. Your studio is looking very splendid, by the way.  Thank you. There's a whole crew here behind the scenes working on the studio as well, so we would like to thank them, as well, for this great day. It looks awesome, I agree. So you're going to do a session about static website hosting, word press, was of the buzzwords I took from your session. Is the new static Azure web apps but we will talking about the whole static jump apps thing and WordPress is an example of how you might want to move from something old and clunky to a little more modern and slick.  I'm really looking forward to it. So if you are ready then let's go.  Let me find the right button. OK, so that is me, I'm a freelancer basically on Azure. And can you see my screen, I suppose I should say? Mark XA is me on all the things so if you want to come heckle me or ask me questions afterwards you can come find me on twitter. We will just start with a little bit of history here. I have done a lot of things. I have worked as a developer and lots of different ways but one of the things I have certainly done my time in his agency work on websites and stuff. I've worked with a fair quota of CMS based websites in particular and what always seems to happen with the CMS based website is everybody comes in and says "this will be great, you can look after your own content, you can manage it yourself, it will be brilliant." But would always used to happen was that you would also have a hosting package built into that which would involve a dedicated server costing you $100 or $200 per month just to run this one potentially quite small and lightweight website. When you are running a CMS site, not only does it generally cost a fair bit to run, but it is also quite slow. You've got your server turning away -- churning away, so you use a lot of Google Jews in terms of your page feed rankings and stuff. -- Google juice . CMS's have a habit of being hacked. I have more than once had to recover a website that someone came along and found a vulnerability with a slightly out of date and as of yet unpatched item. This is fresh off the press but just a few weeks ago someone that I used to work with still had their old CMS running on their dedicated server and their entire hosting provider got hacked. The whole hosting provider went down. So I'm not a huge fan of that. It is never that fast, never that cheap, never the secure. Another thing hot off the press from last week just as an example, I'm over in Ireland and over on Irish TV there is a fairly popular TV show that didn't appeal for children at Christmas. The appeal went up, everyone piled onto the website, and the website keeled up and could not keep up with everyone wanting to make the donations. And that was a professionally set a website, that was not a CMS, there was actually someone sitting there and had made a donation website and was expecting people but it still could not scale fast enough. So really what we want to look at here is something fast, cheap, secure, and scalable. One of the things I'll be talking about here today is what I refer to as "serverless websites". I will talk about what I mean by a serverless website, then talk about how you can but what together, but I will talk about the as your side of things, how Azure static web apps fit into the picture, then will do a fairly simple demo from moving from a WordPress site over to a serverless website. If you are coming here for the magic wand that turns your WordPress site into a serverless website and one button press, I'm afraid you're going to be disappointed. But hopefully will show the principles. Serverless website, like I say. That is my term for what you might also have heard of as JA M's stack. Sitting for JavaScript, APIs, and markup. JavaScript, APIs, and markup. It is a bit of a paradigm shift from the standard server-based models. Are not going to be talking about PHP site or an ASP.net site or a jingo site or anything like that. What you have when you're setting up a serverless website the first thing you have a static content. The core of your site, your homepage for instance, will be just sitting there purely as HTML and CSX. The rest of your site and all the rest of your posts and site areas and all that sort of stuff will be based on static HTML pages which get served directly from something like cloud storage or a CDN or something like that. Now obviously that can work for a lot of things, but every so often you were going to be able to wanted to do something interactive. So what you then do is add some JavaScript to it or I guess Blazor if you're feeling brave. But you add some client size -- side script to that. The service you have someone behind that is The service you have someone behind that is an API to provide the server-side things that you need to actually provide the functionality on your website. Like I say, it feels a bit weird if you are just used to writing an ASP.net or something like that but everything is static and everything goes through JavaScript. But it does have a lot of advantages. The first advantage is speed. If you are serving everything from just a content store and that is really, really fast. and that is really, really fast. There is no server in the way, you are literally just fetching resources. And if you got a CDN behind that you can have all the content that you need to serve up for pretty much the whole site right next to the pretty much the whole site right next to the person who is browsing it. The scalability, you are getting true server list. Basically, no one is using our site, you're not getting any bandwidth costs, you're not getting any functions as a service cost. From the serverless functions. Literally the only thing you're paying for is the storage of your content which, generally speaking, is going to be to the order of sense -- cents per month. On the other hand, with storage, there's no On the other hand, with storage, there's no limit. You can access cloud storage and serverless VPI can scale up VPI can scale up extremely quickly to the scale of whatever you're going to stuff there. You can scale up to practically infinity. The cost is low. If no one is coming in, you are not paying. When you are paying , it is for the bandwidth you are using and the standard per millisecond billing of the service. That is going to be a lot less than $200 or $20 for a hosted WordPress site or something. Security wise, when you talk about content, the static side of things, unless you go to the directory and go into the subscription and your storage, you are safe. There is no one going around trying to break in . Maybe the Chinese government is, but people generally don't break into Azure unless you are stupidly insecure. The function of service might be open, but you can wrap authentication around that. The code is generally your own with small bits of code that are nice and easy to make secure. It is relatively easy to send a secure API. Stability -wise, it is a platform, and there is no server to go up or down or be cached. cached. In theory, your site will never go down. It is portable. Because it is just files, you can lift them anywhere and you can run it locally or stick it in the storage bucket or in the local ISP. It is really easy to zip your files up and run them from anywhere else. You get a local lot of flexibility. You are not tied to a PHP or an ISP.net website. You have got your API behind it and as long they are an API you can access, you can have anything sitting behind it. There is no necessity to have your code in the same place or same language. If you have a third party plug-in, you can remove it. remove it. I should point out the other thing that is worth mentioning is you can do pretty much anything on these websites that you can on traditional websites. Some run their entire web stack and entire backend on this style of hosting a website. They have nothing except a servlet API. -- Server less . What will we have in there? We don't need any of these. They are all optional. It could be a couple of HTML , but it is a completely static site. Generally speaking, you want a static site generator. It takes contents like your blog post or image gallery entry all the comments , takes that raw content and turns it into a pure HTML site. If that is not enough to deal with, you take a JavaScript library framework and put that on top of it. You will then have the surplus API behind it. In terms of managing the content , most static sites edit the content directly and if you are used to a CMS, you will want something that goes with those. You will want content or something designed for this situation, or people who have migrated from WordPress to a server list model still use WordPress as the content management system but they never let the public see it. That is safely hidden behind the firewall. They use it to edit the content. You can pull the content out of WordPress through the API and put that into the content on the static site generator and it generates that. That is the moving parts. That is the moving parts. To turn that into a site you can browse, it needs to be hosted somewhere, so the bits you need , generally some CR CD work, so when you create new content something needs to rebuild your whole website and push that out to wherever it is hosted. You will need your storage, and Azure storage has had a static website post where you can dump storage and Azure runs a tiny bit of it in order to access it as if it were a normal website and then you need your functions of service platform and your Google cloud functions or whatever you have hosted. There is essentially three elements that you need to get everything posted and deployed. That is where the web apps come in. Static web apps are a new preview service coming out of the largely pushed Azure functions side of things, but that pulls it together into a coherent service. You get get hub actions, your CIC workflow, and your static website that provides certain content storage, and it is also integrated as a function in there. It is still in preview, so they are aware you might not want to use get help. -- Get ith ub . They pulled it together so you have one service doing it all. I don't like static web apps. It implies nothing happens and you cannot do anything and it sits there being boring. Marketing says static so static it is. This is the basic workflow. At the moment, it is only GitHub but they are working on other things. but they are working on other things. You have a repository. That contains your source content. Whatever you will feed to your static site generally , and every time you make a change through a push or PR, that will trigger a GitH ub action. It looks in your repository, and builds into static content and Azure functions, and pushes that content and those functions out to the static web app side or the actual hosting and as soon as it is deployed, your users s can look at the new version of the site. We will walk through what is going on there. In the CIC D , in the first bit, you create a static web app and what it will do is you always tell it where your repository is. It automatically provisions that repository the actions, the workflows, and it is necessary to work with whatever you have chosen to build your server list web app with. If you put the popular static side generators, it will use a project called Oryx which is in the Azure AB service to build those for you. It also understands the main JavaScript and front and framework and you can build an angular site so it will do all of the web packing for you. It will even build a blazer site. If there is nothing that you recognise immediately, it allows you to do custom-built steps. You could do a plain one where you built it yourself or people allow you to go for a build command and this is where it will end up and it then allows you to do that for Jekyll or whatever which is on the list . You can then put that in . The other really nice thing here it automatically builds the preview for you when somebody submits a PR. When somebody wants to make an update to the site, if they don't have the permissions to push directly to the domain, they come up with a poor request. As soon as that poor request comes in, GitHub will wake up, fire off some provisions, and set up a second site where you can see it privately. You as the person approving what is going into your website and actually you can see what the website will look like with the PR in, check everything and approve it, and when it is approved that is what you will get when it is deployed to the public. It means you don't have to do these horrible workflows where you have an update on public site and you have to log in to see the changes and you have all these complex workflows. You can see exactly what the public is going to see as soon as it comes in and is approved on that basis. Another nice thing that is built into stack it wet apps is authentication. -- Static web apps. They have added in some login endpoints you can use. login endpoints you can use. Essentially, you have a path for Facebook and if you direct somebody to that end point, it will go through the Facebook workflow and you can use it as your AD, you can use get hub or Google or Twitter. I imagine they might support Microsoft one day. There is always the hope. That is handled for you. That is handled for you. When you log in, those credentials give you a JW T token JW T token and you get some claims on that and you get to know what the identity of the person is, and which provider they use to login . It has a logout and point as well, but you can get that all built in and you can use that in your API. The nice thing about static web apps is you don't need to build a separate API project. You have an API folder in your web app and you can put in a standard function project in either C sharp, JavaScript, typescript or poison. I assume other languages will come along as well and they have only launched these ones since then. Those functions are part of your project and build at the same time. They run within your site so you don't have to worry about the cross origin thing as long as you have thought about the standard authentication. Either they are at the routing stage, but they also have access to the information about the user . That can be used for whatever you need behind the scenes to provide the information for the person who logged in. As I just mentioned with routing, you can configure a fair number of things in this special rootstock JS ON file. You can take parts that are coming in and send them to different parts. If you want a login to go to.org/Facebook so there is a Facebook login, and also at that level you can add rule base access control. In static web apps, you can define user roles and then you can invite your Facebook users or AD users to can invite your Facebook users or AD users to specific roles and then in that file ,/customers is only going to be visible with the customer role or admin role if they have/admin. throw at it. So if you want about to go off somewhere with a 301/3 or to redirect you can define that in. As when you find your custom ever pages. You can pop in an IME types if you really need them. Useful mostly from a security point of view, you can stick custom headers in there. So if you want to add the I mustn't be in and I frame header or things you think might be insecure, and you can do that through the J SON file as well. That is where we essentially do all that configuration to make your site look like you want it to. You might want to throw a load of roots in there for your old URLs once you've actually gotten your generated content. Some other useful things, it supports custom domains. If you've got your own vanity domains so if you want to make one specifically you can do that. That is not strictly true, in preview you can only put www. so they are not supporting Apex domains in preview. I assume, from the way they phrase that, that it will once it goes GA be able to do Apex domains but can't do that right now. So that is important to you and you can't do a redirect to www.you might have to step back for a minute. There is a Visual Studio code extension you can use to develop and deploy all of this sort of stuff. There's a list there of some of the frameworks that they say it will work with it. So as long as you put the right build command, as we were talking about, you can add on any of those frameworks and many more. Essentially out-of-the-box. So, this is normally where I would ask for questions, but not sure about that in the virtual one. So I will move to a brief demo here with a bit of luck. So what I'm going to do here, I'm going to be building a Hugo-based website. Hugo is a static site generator, the reason is called Hugh go it is based on the G0 template building language. So I'm going to take a dummy word press site, pull that over, turn it into a Hugo site with just a tiny bit of JavaScript functions going on as well so you can see how that hangs together. This is not the one I'm going to do, this is just an example so you can see how Hugo works and how that generator works. Here I have the Hugo site for the conference I was talking about before we started the session. You can see and hear, essentially, we have got a content folder, a static folder which contains icons and such and then there is a theme there which contains the layouts and things in the files for the particular skin or theme, whatever you want to call it, that our site is going to use. to call it, that our site is going to use. Then there is a basic configuration file there. In the content here, you can see files like this. It is basically marked down with some front matter. This is the content for a specific session that is going to be attended by a couple of different speakers. Day two, track two, 4:15. This is some mock text that gets generated. And this is how Hugo does it. Everything is basically templated off content. This is a Hugo template where I am pulling in all the sessions and then picking up day one track one, day to track one, sorting them by time and doing some simple templating down here, pulling in some partials. Very simply that is what one of the sites is going to look like before it is built. And then once it is built, it goes into this public folder which is not in source control and you can see that turns out with an index HTML file which pulls in all the CSS and stuff that we want. Second is generated into this purely static site here in the public folder. So that is what it's like the end of the day. You can see those testimonies going to the air, the testimonials content there, further down we've got sponsors, we've got a list of speakers. And that page I was showing you earlier with And that page I was showing you earlier with the agenda. That is how we have drawn that altogether through basic content and template. That is the sort of thing we're going to build. The next thing I have got here is just a little word press site so all I have done here is fired up a couple of containers with the standard.WordPress on my local machine and thrown in a bit of random content there. So there are a few pages, and if we go to the top level there we can see there are four dummy posts in there. What we then need to do is go and create a Hugo website and create a static web app for that. The first thing I will do I've got an MTS repository here in my Virtual Azure Community Day in my Virtual Azure Community Day repository there. I will go to portal and create the static web app. So I'm going to pull up one I have made earlier. Call it Virtual Azure Community Day demo and stick it in Europe because we are in Europe. I will sign into GitHub here. I'm just going to do this on the other screen. So I logged into GitHub. This is all live by the way, I'm not recorded. Mark XA is me. The ACD demo, going to be on the main branch. As you can see here it is got some presets. You see the standard, JavaScript, content frameworks there, and then static side generators you might want to use or, if it is none of those, then we can go for custom in here. We will go for Hugo there, you could configure those if we wanted to but we're going to dump our actual output the actual Hugo site is just different levels. I will then create as you can see here a GitHub action workflow that is what we need there to push it out to static web apps. So I will create that. I will deploy. Which doesn't usually take very long. And here we are . So if I hit that there we will see that we have an empty static web app waiting for us. Next thing I'm going to do and going to bring up this is Visual Studio. Don't worry too much about this code, this is stuff they have mostly nicked off of other places. Adjuster show you what is going on here, that is essentially some code that understands the WordPress API and then understand Hugo and can pool the contents and pages out of WordPress and turn them into Hugo content. This is where I cross my fingers very hard. And run that. Just go through all the stuff there. And it ran. So now, what we should have here is the Hugo website onto which we now have onto which we now have posts generated there. One is just been created there as you can see just now. You can see here is the content we have created. I go into console here, and looking for content, and I will put that up ... of course. But I was trying to do there was pull the changes and it went and pushed its GitHub actions into my repoll. So I refresh that. You can see here in GitHub workflows that is where YAML was created. Has ended up. If I go to GitHub actions here, you can see that it is now running that deploy for us. It will potentially take a minute or two, so I'm just going to go quickly back over here to show you what I have got here. As well as all these folders which are just standard Hugo stuff, the GitHub folder which is for actions, we'll have this API folder. Within that we have some Azure functions. Here we have a very simple Azure function in JavaScript which pulls in essentially the contact form we are submitting and returns it back as a response. In the real world we would have a contact form and we would do an output here to some sort of email service but in this case we are just returning a straight back. That is an example of an API. And if we go back should be nearly down there. And deploying. If it will work. These always run faster when you are not live. There we go. So if we go back here and refresh. There. We have now this is now a Hugo site with all the same content in it. Now here, we have an off-the-shelf Hugo theme rather than off-the-shelf WordPress theme. There are actually some WordPress themes that have a more impressive equivalent. Because here there's a different site but with all the same content. It is not that difficult to turn something into a Hugo theme because, at the end of the day, it is just the same HTML in the same content placeholders just in G0 format. You can see we have lived here to blog posts and we also got here this page we saw earlier. We also have ... we don't have... OK, just proving it's a live demo, the API contact form did not end up in there. For whatever reason. I will just generally talk here about, if you were moving from a WordPress site you could get all the content over like this. If you have got Gallery for instance and even an old jQuery one would do but they can take those images and turn them into a gallery. And any other WordPress plug-ins, if not the SCO ones that you don't really need a static website then you will need to replace those with something that you have either written yourself or some sort of plug in. So for instance, word press comments. If you want people to be able to actually comment on your blog post, Hugo by default has discussed plugged in, but if you want to plug in someone else's third-party JavaScript or write your own API and call out to that API, depending exactly OK, that's good. There is a simple way of doing it and then there is a right way. Let me take you through what I'm going to go through in the session. I would do a short introduction, with a little bit about application settings, app configuration, key vault, which is for managing the cloud and identities. Then I will talk about agile resources and lastly we were due at closing and look at how we can use it today. So let's start with a short introduction by actually saying that configuration , what is it actually? In the end, configuration , or configuration files are files used to configure the parameters and initial settings for some computer programs. This is what you get when you look up configuration. In the end, things like settings and how applications things like settings and how applications work, like addresses, or connection strains, or resources like databases. That is stuff that typically goes into a configuration, or configuration file. If we look at a long long time ago , maybe some of you still know this one. These were any files, or initialisation files , along long time ago this was how you configured an application. You had the settings in this type of file , that actually held the settings for you. After a while, Microsoft started with registry and having registry as a place for your application settings. That didn't go well , so for instance what can fig came along which was the way of configuring a speed.net applications. We using a lot of files to have settings for applications be available for your application. So this is kind of the road that configurations has taken throughout the years. Now that we are going into the cloud and into a more cloud native application structure , these might not be the best ways to go about it. Of course, registry isn't available on part like a back service -- as YEARSTU Azure . You might have the setting for one database and a lot of locations, tighter all the applications you are actually using that database in. There is a better way to do this. If you would like to share your configuration throughout different types of applications. First of all, let's see how we tie an application setting to an application if we are using Azure . That is something that is good application settings. If we take this example of the report , on the left-hand side we get all of the places we can choose, in the Saiss -- this place . It enables you to have application-specific settings and application specific connection streams. There is also some general settings but the upper service that you are actually running, like the version of .net you would like to target and all that kind of stuff. This is how application settings are available since early at services. -- app services. If you take HP.net or H we .net core and you host it on and at the service. The settings that you put in configurations settings are available as in -- environment variables. It is like the app settings over HB.net or the adjacent file and the values that you put in, application settings, configuration actually override the ones that are input config , or adjacent. Which means you can have a value specified in application settings in the cloud and when you push, then your web config or adjacent fault will travel with the application into the file. However the setting that you have specified will always overall. This enables you to have the example development settings in your local web config, or in your app settings , or specify production values. This application settings are hidden by default, which means they will not be available if you click on the configuration in let's say a demo during a session that is being broadcast on the internet. They enable bulk edit, which is quite cool. As I said, , especially with the fact that Microsoft is is our here to stay, probably. The fact that we have all types of servers and applications like functions, targeting our resources as well. Having these settings be available in a central location would be awesome, you have one location where you can handle the dials and for instance if your handle the dials and for instance if your database is migrated into a different type of database onto a different location and connection strings change. You can do so one location that will propagate through all of your applications. There will be awesome and that is where configurations comes in. So, outcome configuration is service in a Mac Azure and allows a configurations door. You have complete data input in transit. and at rest. It is targeting open source more more and it is an awesome thing that is happening to Microsoft and its ecosystem. It also means that outcome configuration will tie in natively with a lot of frameworks , not only.net, but also Java spring. So, as I said, with microservices or maybe even micro content being front and centre lately, and all types of server less applications in addition to our applications to actually form some sort of... ecosystem of applications that work with our data, it is really cool to have one central location to have that configurations so that it reduces complexity. It also improves security because it separates out configurations from code. For instance if you have the connection to a database, it is better to not have that configuration anywhere in your application, or in the configurations of your application that is committed to jet. I am going to tell you I am going to tell you extra information later on because storing your secrets straight in app configurations may also not be the best solution, but we will dive into that later. First of all, I would like to address something about pricing, because pricing has been made available quite some time ago. As you can see, you can have one free Azure Consideration resource within a subscript in. Each subscription can have one free tier of applicants. In . configurations available. With three you get 10 GB of storage, you can do a thousand requests per day and after that he will get . you do not get a licence, it is the free version, but also do not pay money. The priced version is better, actually it has been since January 19 of this year. It gives you 10,000 requests per hour within the quarter, and it gets you all of the free tier fans now plus encryption with your owner managed keys and private linker support. Private link is something I could go on and talk for another hour probably, but unfortunately we do not have time for that. The cost at this point is just over one euro a day, plus overage charge with 5.1 cents for each 10,000 requests that you do. And the first 200,000 requests are included in your daily coverage. So, going to a pricing example just to get it out of the way, if you have an average of 400,000 requests a day that any costs for having this configurations up and running would be one euro and 1.2 cents, because that is the dose based charge. They needed 2000 based charge. They needed 2002 -- with an average of 4000 requests a day the daily total would be two euros and .2.00 .32 cents a day. I think that is a relatively small price to pay given the benefits. But you should not take my word for it, you should have a look at the demo and see if we can get that stuff running, and see why it is so awesome to have it running. So, I am going to go into . I can see that my screen is still shared flat is a good thing. I'm going to go into visual studio right away. The reason I am doing this, I think will bump it up to make sure it is legible on smaller screens, the reason I am doing this is, here we have app settings.development .json. These are my local settings and I have the settings with the awesome name setting one, and the local value. That is my development version. I have a settings version, I'm really happy that the secret is now out of reach. You can see here that this value is from app settings .json. app settings .json. I actually added some HTML markup there that you would never do normally, but right now I did this to make it cleaner on the display later on. This is the value that we have locally, but it is the value for our production environment. If I jump into my Azure environment, you can see that I have a few resources available. There we go. At least then it fits the screen. One of the cool things that we have here is the application itself. This is actually a deployed version of the application we just saw. If we go in there, we can see that this is a website running .NET Core, it had some requests earlier today and if we go into configurations, we are not going to see anything that looks like setting one. Looking in this right here, I do not have a setting one anywhere. Which means that it should take the version that is in app settings .json. First of all, let's have a look@theappsettings.json and and if you go to the (unknown term) location of your app service and add/dev you got something that looks a lot like visual studio code. Right here this is the app settings .json, and to make sure we see this here is the actual result of the website. It says configurations on Azure done right, here is the value for setting one. This value is from app settings .json. If I refresh this I would probably get a small... I would get again this value is from app settings .json. I'm going to create this or change this and make it VACD for Virtual Azure Community Day, and you could see straight away that it has changed. We are actually taking the app settings version right here. What I actually wanted to show is, if we go into configuration again, and I add a new application setting and they call it setting one then I will say... this value is from config... settings. Typing in front of an audience, right? Actually I should say application settings, because then we can listing which it better from the other one. I'm going to save it, and of course this is going to have that small and slow start again here. If I get a refresh on the application, there it goes. on the application, there it goes. As you can see on the top it is loading. As soon as it runs we will see this value is from application settings. Right. So the code did not change, the only thing I changed was in application settings, and entered the setting and you can see it override my app settings .json. That is the first level of getting configuration in a manageable structure, because you can now change this in the portal. But we want more. We do not want this to be in each application, we want it to be in app configurations. So we only need to do one small thing for this. If I go back to the resource group and I go to app configurations that I have here, let's have a look at what we have. Because we do not only have access keys that enable you to connect to app configurations, we also have configurations Explorer where we can actually see some of the settings that we have in Azure Configurations are available, and now I think I have the same version available there as I have in my head. So if I put setting one over here... and I say this is from app config referral, just make sure we have something that is really explicit. And it is created, and refresh it to make sure. Then we go to the application, and while I click around I'm going to tell you that Azure Configurations does support auto refreshing when keys change. For now, I chose not to implement this. But you can have your application automatically re-get all of the information that it gets from Azure at configurations. If we refresh this one right now again I hope that we will get that this value is from app configured external, or that is what I expected to be. Just a bit of a cold start, that is not an issue. And here we go. This is from app configured external. Still, you can see that the application has not changed, but the values it picks up has. Let me show you how that is done, because it is actually very simple. Right here, there is this one thing that we added. When we get rid of this one so that we have the full view of what is happening... of what is happening... and I only need to show it up until here, and then just... just... last chance. Here we go. So what we actually did is in our start-up of our create host builder of our application, we get I configurations, and that is the standard for getting configuration in an ASP.NET Core application. We get the letter I configurations of the right here and we get it passed into configure app configurations and this one is one you get when you install the correct NuGet package. Now, what we do is we build the configuration builder because we are going to want to get a setting from our settings environment, so from the I configurations, and then we are going to configure Azure Configurations, and we are going to see that I would like to connect to this app configurations connection string. And this app configurations connection string is one you have for let's say storage or something else external. But that is also the issue that we have with this approach, we now have an app configurations connection stream that is actually a gateway to the storage of all of our secrets! Because the Configuration will store the connection but there will also be secrets in there because those are in connection strings. That is not what we want to do. We want to get those out completely. That is something that we will touch on in a little bit, the only thing I would like to say is this is the only thing we do to add configurations to our ASP.NET Core configurations. To show what this looks like in the actual index page page where we get the value, this is where things get interesting. This field only holds the normal I configurations and there is a setting one string which we use to output our information. And in the constructor, we get the normal I configuration in our constructor and we put that in the field we defined over here. And then for my code, it is completely transparent. I need to get a configuration guide, and in this case I'm going to get a value of type string with the name setting one and I will put that in the For me as I paid developer this looks like just getting a normal version of asserting from configuration. In the end, it takes life from In the end, it takes life from app configuration, if that is not available it takes it from the app settings , if those not available it would take the ones from app settings, or adjacent, or at config , you can stack the different configuration providers that you have. This is also something we see as far as filtering goes, and interested time we are not going to dive into that except for what we say here is that we stack the filters while connecting this app configuration where we say, take the one that is labelled fraud, if it is available -- prod. If that is not available, then default to not having a label at all. This will enable you to have some setting stacking, however this will get the exact same result as what we have deployed right now. Let's go back and see what key vault has to offer is. Key vault is what it says it is, it is a vault of keys. is what it says it is, it is a vault of keys. It stores secret certificates in a vault that you cannot just simply access, you will have to make sure that you are authorised to get access to the key vault. Only then you can look inside and you can see passwords and certificates, API keys, encryption keys and those are available in the key vault. It also enables you to store secrets that are backed by hardware security modules. So you can choose to do it either by software, or level II security modules. Why would you like to use key vault? As I said earlier, you do not want app configuration to hold all of your secrets and then them still have a secret that enables somebody to get into application. Of the app configuration. You would like to securely secure your keys some external and it will be awesome if those integrate with Azure Azure . Alters a logical group of secrets , authentication to get there is needed , you can either use a service principle and secrets, or a service principal certificate, or you can use managed identity. Let's just have a look at how we do that. So, this So, this extra liner added earlier and you might have seen in the previous demo actually already gives away a little bit about how we are going to access key vault. First, let me show you that we have two pages, one is key vault one, one is key vault opening two. Key vault 01 is the first version of how we are going to connect to key vault. Let me enter here so you have more clear view of what is happening. On the gap of this page we are going to create a new secret client that uses configuration in Azure . key vault two. It is the location of the key vault that we created and then we get the secret from there and this actually looks a little bit strange, because we have a double value at the end. The first value is the value of the get secret action which actually returns the key vault secret and a key vault secret action holds quite a few types of information about the secret itself. One of those things it holds is the value of the secret. That is the reason we are going to go with value.value. Now what this actually does and this line is particularly interesting as it says, it will use a default Azure credential. What this means and I am guessing why did I implement this here? Because it should be in key vault two. Let me check, real quick. No, this is correct. What I did here as I am going to create a secret client, which is actually a class that has been added to the package which allows us to talk to key vault. I am going to use a managed identity to connect to this key vault. I will manage the identity and I will dive into this in more detail later. I managed identity is actually a service principle that an application, or another resource inside of Azure can get that is actually stored in Azure that you can then use to give role-based access to different types of resources. What we did here as we created a secret client to talk to our Azure to talk to our Azure KV to key vault and get a secret warrant from that key vault. Using that managed identity. Now go back to the application that we have running, you can see here that we have this key vault number one page. That is going to create the secret client first and it says "the secret is from key vault!" This looks like in the back end , under this resource group that we have we , under this resource group that we have we have this key vault available right here. This key vault is actually the storage of all of the secrets that we would like to have in this key vault and as you can see . Let me check real quick. We have some keys here, encryption keys that you can have here. Some secrets, so here is the secret one that we are actually getting. We have certificates and then there is something really important that I need to tell you. Because, we used to have the Azure policies, the access policies. They used both access policies and then he actually had to give identities , specific rights like like listing secrets, or getting keys listing secrets, or getting keys and that was the authentication mechanism. With the rollout of role-based access With the rollout of role-based access control based on identities through the Azure program. We also have access control in preview as it says right here on the selected bullet. bullet. Access control in preview. What you do here is you can have role assignments that enable specific users to do specific things. As you can see the website and myself are key vault secret users , which actually means that these are service principles that can read secret contents. This is a central location where actually I manage you can do what inside of this key vault. Because I gave this website access to this key vault , I do not need an access token , a specific access token , or a connection string with a secret to connect. The Azure platform validates the application itself as having rights to actually connect to the key vault. Just a short Just a short trip to the website again, where you can trip to the website again, where you can actually with most resources say I have an identity. I have put that to be on. As soon as I add this system is on on this website, I can go to other resources and give this application access to that resource. I will show you why that is. First of all I'm going to go back once more to the app configuration and I am going to show you that right here under configuration Explorer, we have this one here which actually has a specific type of icon in front. It says that this is secret one. Thinking about this, this is actually the Thinking about this, this is actually the same name that I gave the thing in key vault. What I actually did was I added , so when you choose create you can choose from key value, or key vault reference. What you can do with app configuration is you can give something a name , like very secret much hidden. And you can select the key vault you would like to access and then select the secret inside of that key vault. So you can have app configuration be the nomad between your application and key vault called secrets. Your application does not need to know , because as with the index , the index page that we saw earlier. Here we just said I have a great hate configuration it gets me a value of that string which is name." Because we now have a secret one available in app configuration which uses key vault to store the secret. We can just as well go into our different key vault page and say " hey, consideration get me a value of type string, but this time due secret one." This time if you go back to the application and we open up the page of key vault two, he was the same sequence. -- Secret. Even if I were changes in the key vault, look at the same secret. I am going to go straight back into that, we are a little bit close. As I said manage identities are managed identities for Azure resources are a way of creating an identity for your application that provides Azure services with an identity in an active directory which you can use to identify and authenticate your servers. There are a lot of services supporting that right now and both on the compute and functions and services as well as the resource and like key vault and data lake and storage. Let me show you the real power of actually having a managed identity in place. I added this extra page right here today, that is called storage. We have is going to dive into codes and have a look at what that storage page actually does. So, we are going to the storage page and I'm going here. Then, this might seem a bit complex, but it is actually not as much, I'm going to show you what is actually happening here. We are going to create a container client. This is a blob container client, so we are going to talk to a container in a blob search. To know where we would like to have the blob search located we are going to get a value from the configuration which is called session container. Here again, we used new, default Azure credential that allows us to use managed identity to talk to storage. Actually all of the other things are getting a reference to a raw client. Or a specific file and in this case that is called next session.txt. Getting a memory screen downloaded into the internet stream and putting the position of the stream back at the position zero. Reading it and then we will have two read to end a sink of the data. It connects to 1/3 account and get to the next . section. text file from the container and the outputs of the content of the file into the next session property. Then if we look at the page itself, of course, the next session is actually shown here. What we have available as the net -- next session which we are going to connect to our storage account is actually this one. . That is not a big of a deal. Here we have the session container. What I am saying is I would like to connect to the container sessions, right here. Under this storage account .core Windows net. I am not specifying any secrets right here, the only thing I'm specifying is the location of a blob container where I would like to get a file from. Then, again, if we go to the portal and we look here into our session container value right here. Close this one down. If we look at the session container right here, it actually also says card of storage.blob.core.W- indows.net/session. Just to make sure that you guys know I'm not having any smoke and mirrors here is the storage container and under blob containers the recessions and under session there is your next session.txt. I don't have any secrets available anywhere in configuration , or in my source, code. If I now go to storage , it will use its managed identity to actually go out and go to the blob storage and the platform will say "this is the other website, it has Azure as a directory service principle as a directory and under the storage account this same service principle has gained access to read blobs in this storage account." I do read blobs in this storage account." I do not have connection strings, I do not have secrets, I only had the description for the next session. It will be about exploring the capabilities of different Azure services were simple daily tasks. Closing up, real quick, configuration , I think should go into app configuration as soon as more than one application is talking to app configuration. You would like to minimise the chances of actually human error, because you might update one setting, but not the other. Then, if you would like to make sure that you do not even have any secrets, anywhere as far as accessing Azure resources goes. Have a look at the resource that you are trying to talk to and it enables you to use and manage identities and please have a look at using those. If you go to the URL list.com/ CI ADR there are a lot of links right there about all of the content that goes into the session. Of course, you can email me and see if you can get me from there. setting one property. And then that is output on the page. Thank you very much for that, Rick. There was a can . a lot of stuff there I will need to take a look at as well for double I also wanted to say thank you for the last from the past from those.any files. That was really cool. And somehow there was never a backup of the right file, right? Know, and everyone always comes out with, "Enema ma changed anything which eventually time to I changed it but I changed it back. Were needed better. Yes, exactly. That Mac We have one question from YouTube, we have a question which is can variable accounts also get their information from app configuration? I want to say yes, because I have automate on top of mine as well. Let's see if Azure automates... I'm not sure if it is supporting managed identities. In the last one I checked a bit of... but it's something I do need to talk about. Microsoft has actually expressly said that they are going to support managed identities throughout the is your portal. So they are actively working on getting it everywhere. If I look into Azure blueprints... no, it is not in the list of services that support managed identities friendly . But it may in the future? They are committed to getting it across the entirety of Azure, as I would expect it to come in eventually. For instance, blob storage and Q storage, they both support managed identity at this point. However, table storage in storage accounts does not yet. So they are actively working on it. OK. Did you have any other questions? Actually, Rick, I have one question for myself. How do you go about maybe any advice about... how do you integrate configurations and your application together in your CIC pipeline? Because you do not want your configurations to be out of sync with the changes that you have made an application to so any advice? Yes, because Azure app configuration enables you to use different types of infrastructure as code solutions to update the settings in app configuration. So, what we do right now is we use the Azure CLI that can be used in the Azure pipelines as well. And we use that to keep the settings up-to-date. As far as it is not secrets of course, because secrets are going into KeyVault. But there are several infrastructures to provide and support setting values . It becomes an integrated part of your CIC the pipeline as any other thing? And I think that is the place you would like it to be, because then everything is repeatable and if somebody said I changed it but I changed it back, then we can just with a push of the button change it and know for sure that the values are there that we expect. Indeed. I say automate everything. Would meet all of the things! Thank you very much again for joining us today. And , Nick, who do we have at next? , A warm welcome to Tomas who is a data platform MVP with a MVP with a passion for SQL server. Also data science. And actually, data science is a bit of a passion for myself , and so, one thing I really liked in researching a bit the work from Thomas is that he is actually a maintainer of the useless R functions. So you may wonder why that would be interesting for me, one such function that is in there is basically a job title generator which is kind of nice. And kind of fun. And kind of fun. I am hoping Thomas can also share a bit on his ROWN TIPPINS-GRAHAM: What other functions are coming which may be more or less useless for people. may be more or less useless for people. On the upcoming session we talked about automation, Thomas will be talking about automating your social life and... we have a dinnertime coming up, and the smell of food is food is entering the room here. So it might be an ideal time to get everything automated so that we can spend some time and grab some food . Get some energy. Get some energy indeed. Without further ado I want to hand it over to Tomas, are you there? Hello, good evening, I hope you can hear me. We can hear you fine. Beautiful. That is a lovely ensure you made, especially with those useless functions. useless functions. It is going to be hard from there. There are a lot of useless functions coming up, but if you have any useless ideas it is more than welcome! Especially, it is a beautiful thing to see people collaborating in GitHub and sent me ideas immersing there is a bug is an increment of the function... I just love it. And the job title, I used to run a website also in (unknown term) that was actually SQL job titles, but I decided to put it down. Thanks a lot. So you have the floor. Take it away. Beautiful. I'm going to share my screen, and let me know if you can see my slides. Yes. Dutiful. OK. Is that 40 minutes or so? Yes, roughly. Thank you. Good evening everybody. It is evening in Sylvania . Slovenia and we probably had 15 to 20 cm of snow today, so it is a beautiful winter day. Today I'm going to talk about how to automate your life, especially online with Microsoft as your. I am a huge fan of automation, so anything that can be automated is my flavour. A couple of words about me if you want to get in contact go to GitHub or follow me on Instagram, LinkedIn or Twitter. It was a beautiful interest I'm not going to lose any time here. So the agenda for today, I'm going to go through what is social life and its features especially in this time, this unprecedented time of the beautiful lockdown system. I'm going to talk about some Azure features, especially functions, logic apps and vertical machines with IPhone. Ideally, just to give you a glimpse of how you can go along, how you can get started with as you as a child, I have seen some people have developed fears and things, so it is more about getting people on board and saying that it is not that scary, you can do a lot of interesting stuff. Recipes for today and the way I have envisioned it are how to get the best prices with your particular local retailer. How to prove Twitter accounts and tweet some useless stuff again that no one really wants or no one really wants to read. About how you can create an Instagram profile and share some photos of your cats, dogs or wine, which are three from what I have seen, the most favourable topics people like to share. Hiring people just to give you an idea of how you can go along with that. Getting some real estate, there are a lot of people wanting to be the first to get a better deal when renting an apartment, or buying a particular land et cetera. The last one, sending a text or SMS to all of your contacts when they have birthdays. So it is also something of course, you need to do. (Laughs) And as of those social things like tinder, I am not going to discuss that, but there is huge potential also. I have marked them in red and green, so the green means that it can be fully automated but red means that it sometimes needs your attention. What is social life? In this COVID time I think the best way to go is to get some Azure credentials and credit and get on board. Otherwise digital social life, what would that be? Social networks, chat portals, work-related networks, shops, sport engagements, online games and portals, video portals, news libraries and again I'm not going to discuss and talk about soul-searching and dating. But, you know, I have seen a lot of people from different community events where they are definitely interested in that part. OK. So as you can see today the topic is going to be more fine orientated and not really into details of codes and stuff like that. But first of all, I just want to give you a glimpse or an overview of what you really need, and that is in as a tax scripting. There is a free trial if you want to get on board. You need a computer, some scripting or programming like witches, some social identities like GitHub, witches, some social identities like GitHub, Instagram, Amazon or Twitter. Of course you need a COVID-19 lockdown, hopefully it will y l be over soon. And disclaimer, all of the recipes are small just to get learning people on board and things like that. Examples are created to bring closer cloud service to everyday life, no GDP are or data had been exposed, definitely honouring code of conduct in everything I am showing today and it is also available at GitHub. OK. So a couple of words about which services I have used. I have used function apps, Apple provides basically endless competing resources. It is server less and also stateless. It integrated many cloud services like code for blob storage, server list workflows, the bass response changes with the different types of databases like Kosmos and stuff like that. You can run some schedule tasks, queue systems, and of course those near real-time data processing. Also supports .net, Java, JavaScript , partial Python, in order to create also some custom handlers for Azure function apps. And also automated deployment is super great but this is more like next level in terms of the talk today. I have created also automated (unknown term) using all of this gives you can basically get down with when you are creating the services and as attack. If you have any questions left. The second one is logic-apps , is especially used for scheduling, automating and orchestration of and orchestration of more business orientated tasks and of course in my case some users tasks, business process integration, workflows, especially those enterprise integration , interpersonal integration and stuff like that. Again, southerners, it goes Again, southerners, it goes beautifully together with Azure event grid and Azure functions. Unison product ? it is in project apps to link in order to get to link in order to get started. Of course, virtual machines, sometimes a bit lazy, it is much easier to create a virtual machine instead of writing the function for it. It is pretty cheap in comparison to virtual machines, rob storage , all types of databases and not to the forget about GitHub . The languages, .net, .net core, hyperacusis. With those machines there are some additional functionalities that you additional functionalities that you need to ensure if you're running back. You were neither package management. Of course, visuals and some other stuff. You can do regardless of the OS , you can go with Windows, clinics, no problems at all. Since the corona lockdown I wanted to get a Apple tablet and I'm not endorsing this particular brand, but the idea was since the stock was running out extremely fast, I wanted to be the last one to grab this tablet from this particular provider , the vendor. Since it was only once per day updated, I did not know when it was updated , I was more annoyed of waiting and staff and refreshing every couple of short minutes. I wanted to get not the best price, but I wanted to get the last stock. This can be applied to getting the best price for anything. In this case, what I did was I created a virtual Azure function app, anything that could be server list. I created the Python environment and what I needed in this case was a web shop, or anything of your desire. Of course, when I started coding this I was coding on my machine and then once I turned off the machine, it was a problem because I cannot have it running, up and running. I decided to, that is why I decided to go with a virtual machine. The couple of searches for the automation, the script code to get the section of the website you're interested in. Store the report and the result. The third one was particularly interesting. OK, so once I went and examined the website , I found the particular CSS wanted to observe , it is a place for this particular tablet. I wanted to examine whether it is going to be on stock soon, or not. Basically I went into that, started writing a Python code. Beautiful soup was my favourite, just to get a quick examining and then once I had that , I could see OK, there is still not stock and I could be hitting refresh every minute on my computer as I was thinking about , so how do I get this reporting back to my mobile phone, or is my computer? Or to my email? So I was thinking OK, go with each database and then start refreshing websites, posing it through the API, or something. I can write a service function , but then I decided OK, let's try telegram, telegram has an open API that you can basically just code. It is available also on Python. The only thing that you need basically is your mobile number and you register it and that is it. It is very similar to what's up stuff like that. Again, the automation process, the first two parts were parts were VM checked and I was stuck with the last part. I decided to go with the telegram. The telegra In this case it was late in the evening and I was quite agitated, because I could not get my hands on that tablet, so I decided the fastest way was just to get a virtual machine up and running. In this case, you know, the cheapest version with Windows, because I am all into windows than Linux , so set up the virtual machine. There was also a database where I would just scrape the web and put the stock from this particular product and to some prices in the database. Then I said "no, I would rather go with telegram." So setting up with telegram was pretty straightforward, what I did was I basically just downloaded telegram for Windows and just hooked telegram with my phone and got the bot up and running. Then some additional Python coding. There is is telegram. EX T that you basically need to peep in-store using beautiful soup. I was up and running in no time. Once that was fixed, I just ran this scheduled job , basically it was running on my virtual machine and it still is running, literally. It was, you know, scheduled to go to that It was, you know, scheduled to go to that particular website, every minute, or so. Then it would start retaining me the results on my mobile phone. This is my mobile phone , a printscreen from my iPhone actually and everything is in Slovene , you can see that I was setting up timers for 60 seconds, hundred 20 seconds. At the beginning I was quite thrilled to get the product is still not available, it is not in stock, but better than getting a message every minute better than getting a message every minute , especially when it is not successful in terms of being in stock or not. Then, later I decided to set it every hour and then I decided , well, should I basically just reverse the logic and go pigmeat the first moment when you get it in stock. Literally, I recoded and recoded everything and started the other way around. So basically the system was sending that website every minute to see if it is in stock or not, once it was in stock, I got a message. Just a side note, this was running for like two or three weeks and I thought , literally, that this was not working and , literally, that this was not working and after two of so weeks I went to my virtual machine and the ping was still running. The problem was my local retailer still hadn't got the tablet on stock. Literally, in three weeks or so, the thing actually responded and I was back to the shop to order it. I got my tablet. I got my tablet. So this was more like experience of how to get the best prices, but how to get the communication from the website. It can be a web shop, it can be anything else, back to your phone and this was amazing, because sometimes you just want to close the computer, but your phone, you go everywhere with your phone. Recipe number two was let's create a Twitter profile and tweet again , the tweets that nobody wants , uses tweets . In this case the idea was, let's try something new , let's try word of the day, quote of the day, maybe the current weather for my hometown, or current time. I was still pondering this idea of what would be the worst way and in this case of course you need a Azure subscription , sub- engine like logic-apps, you could also go also with Azure functions. Also some database, some persistent storage where you can in case you want to store some results that would be also possible. results that would be also possible. So what I did , I created a Twitter account with this particular Twitter account I had to have also development account , authorised, which means that you can then access to some Apis to get hold of the tweets. To read the tweets, search for particular hashtags and things at that. Or even post the tweets. I created logic-apps , logic-apps are in this case relatively straightforward. Of course access Of course access to Azure database, or Azure storage for storing some extra. My automation was given a period , I want to tweet something. Following that condition, particular Following that condition, particular condition, do something, tweet something. Of course, repeat and after some time, basically forget about the account, which really happened to me , I forgot the password. I can sure get it back, but still (Laughs). I have created logic-apps , the logic-apps itself is said to basically do the currents and then get correct times for those who do not understand. The name of that logic up is called literally translated in English would be accurate time. The idea was to basically mimic the church bells the church bells in my hometown of Slovenia , how many times they do Dong , Dong, Don and the hour. This is where the idea came. I said OK, let's go with logic-apps, I created the currents and then I got the current time in the base of the current time, it is not the full-time, do not do anything , at the full-time, count what time it is, if it is 5 PM, then do five dingdong. If it is midnight, they do not do anything. Then the condition of cause, based on the condition, because the church bells do not dingdong during the night, they start around six 7 AM and they go all the way until 8 PM. That is it. Basically the idea was to create a condition that would basically do just that. In the meantime, I also wanted to impact my logic at in terms of , if it is a public holiday, also give some sort of greeting, or if it is a important day, do something. In this case you can see the query that goes to the school database and gets all of the public holidays and also the idea was to get some extra weather events, like today if it's going to be full snow, or full frame, do something like that. At the end, I decided also , I have basically created this, I then decided just to go with the dingdong's. Again, useless idea to learn and connect those services. This is basically how the logic app was working in the trigger and you can see that done. When I was preparing this section, it was 24 November and apparently I wanted to go into my logic apps, of course not, knowing my password and apparently it failed a couple of times. apparently it failed a couple of times. Again, this is my Twitter account , I wanted to be sneaky and just have it running , it was basically in the first Corona wave, I basically created this again properly out of boredom, or something. In the meantime, I have managed to get zero followers, say hurray! Anyway dab it up and running, you can see it was more than 2300 tweets a couple of days ago. The picture is a central touch in my hometown. There is no fate to that. The problem was when I was digging into YouTube and tweets, there is every time something is treated, you can see that there is Microsoft Azure logic-apps in the background. This was then something that I did not know how to remove and I have just left it there. I'm going to quickly switch to this account and you can see that again, this is up and running and if I click on a particular tweet, you can see it still says it is running Microsoft blue Microsoft Azure logic-apps. This was quite fun, it is still their up and running and it still happens. I actually had one follower , I'm quite proud of that. I have seen a lot of people doing some caps, dogs wine, things like that, and it is getting extremely popular. Since I am a coffee drinker and coffee lover, I said I would do a coffee lover SI, which stands for Slovenia, account, and let's try to automated. So basically I have created a coffee lover account and from there on I decided whether I wanted to go with virtual machine or if I wanted to go with the function app. I definitely needed blob storage to store my photos, then some functions either to connect to a virtual machine or to connect to the blob storage. At the time I did not know how to do it, and then of course, Python, because I was also exploring Python with Instagram years ago. Then I said let's do Python of course. Back then I did Selenium which is a Python package, but I have seen just recently there is this (unknown term) package which is particularly for Instagram and it basically does the same as and it basically does the same as Selenium did. Of course need an Instagram account and a phone. The idea behind this automation process was, let's make some photos. So these days are making quite a lot of photos. But in these days it is coffee -related photos, and have those photos sink with my Azure blob stories. And then, periodically, I said OK let's do it on a daily basis. Select a particular photo and then basically you store it on your phone that goes to the blob storage directly and then posts it to the Instagram account and posts it with some hashtags. I managed to find a hashtag generator that is called if you Google Instagram hashtag AI, it is an artificial intelligence, you can basically get there and ere and find the best hashtags for your particular idea. So... what I did was and this is the print screen from my Azure storage and you can see that the photo I have somehow managed to get those photos, I was thinking how to name the photos so that I can basically script and there is a timestamp that I basically converted to just date time . just date, because I'm posting photos on a daily basis. I am not doing it on an hourly basis or anything, so daily is fine. The idea and probably the hardest part was to get my phone connected to my blob storage. And again, some .NET scripting to get my SQL server, that is connected to my phone, and then it stores the photo itself on the blob storage. The function was then a sickly , basically, again it is just a test script, you can see that there is a lot of name spaces involved in this and everything is automated. Again, this was the hardest part in this case, everything else was more like a piece of cake, and in this case some.net coding was quite thrilling. And then, the last part I needed was just to have a lot of material, a lot of photos, I have you know before the lockdown, I had I collated the amount of coffee I needed in a worst case scenario if we are in a Slovenia lockdown until March or April because I'm drinking a particular brand of coffee that is not t available in Slovenia and I usually drive to Italy to get it. This is my calculation so you can imagine I have a lot of coffee. So, once I have this, the only thing I need to do, then, is to make a photo with my phone. Then I need to store it and then the Azure function basically connects to my phone, gets the data and put it there. And then on the other hand I have a Python that basically acts as the blob storage and puts it on my Instagram account. How then at the end does this look? I have basically the coffee here as you can see, this has been up and running for 10 days or so. I have managed to get number (Speaks foreign language) Followers stretch tag scratch tag and for each I am basically getting the hashtags from the site, I was also thinking maybe I can do in the next step may be a can do some image recognition in order to say OK, this is the filter, so get some filter hashtags website. Again, I am still thinking about how to do that, but this is the next step of how to improve that. next step of how to improve that. And this turned out quite interesting I have to say. The fourth recipe is, again , how to be the first at buying real estate. Or getting the best deal. This is again, something from a Slovenian website that basically states all the real estate from apartments to houses to a land where you can buy n buy something. In this case, I was just pondering on how to get things done. And immediately once you go to this website, you can see there is this URL that has the particular names where you can easily access and then, using some script, and then, using some script, decoded and that's it. In this case, Azure logic-apps, again stateless and syphilis which means that you need to get the idea of how to manage not to send if there is a new ad and you have already seen it, so that the logic-app says that you have already seen it so you need to know the last time you accessed it. And then of course, the return information, so in this case this was back what I did was just sending it as a message. If I read it today I would definitely use telegram. So the idea of the automation giving and interval check for the content interval check for the content and send an alert if something is available. So... again, logic apps, that is a beautiful HTTP trigger which is executing some gravel stripped . JavaScript code, and then JavaScript code, and then this was relatively cheap back then to get some quota of short messages. So basically, connected and every time something has happened, this is pretty much straightforward. Again, JavaScript is stateless and several less so the only thing I had to figure out was how to run this continuous innovation through time given a period of time, check how many times basically basically have passed since the last run and then do something with it. OK. The last recipe The last recipe is sending texts when your contacts have birthdays. Again, never forget, and always send. So it is just not to forget about your friends. So again, logic apps tends to be the best way. Again, with that previous program but that was a couple of years back. The idea basically has slightly changed. Now that mobile phones have basically become quite powerful. Since then I have used scriptable which has nothing to do with Azure, I'm not endorsing the product itself but the idea with the first process was that I was not really checking on the language of the person or if my content was from Austria or from Germany or from England that they would know how to switch the language, or if some body was from the other side of the world to get the right time zone. With this JavaScript script with which is an app available on iPhone you can basically automate that as well. So check the language, the time zone and send some personalised short messages. The idea is that you can also get a widget where you can see how many times or how many days until a person has a birthday. And again, sending SMS with happy birthday wishes, it always gives you a warming feeling when you get it. To conclude, on the costs per recipe and estimates, the first virtual machine, the first recipe costs around roughly .30 a month to run. Again, you can recode everything or reversed everything and get it done much cheaper. The second recipe basically , because it is running eight times per day, so this is the tweeter Donna for 31 days , personally 31 days a month it costs around half a dollar basically for half a dollar basically for everyone. The third recipe again if I'm taking the virtual machine it is again .30 per month, or I can recode everything using function apps and the storage, the storage is relatively cheap but there is a beautiful part about Azure that is that you get up to 1,000,000 executions for free. It really is no-brainer. And recipe number (Speaks foreign language), It's basically logic apps 100 times a day, again extremely cheap. Plus using all of those telegram messengers which is free for now, and also Twitter and Instagram which is free unless you are either using business profiles. All in all, for start-ups small to medium or big businesses or enterprises, something of this small can definitely give you better digital recognition for extremely low costs. How to save on costs, especially if you are running out of virtual machines, you can go server less, use function apps and apply the rules. Of course, virtual machines are more comfortable, especially if you are lazy or if you are programming at midnight and do not really feel like coding too much. Virtual machines are quite easy to use. But, it comes with additional costs. Working progress and further ideas, definitely ... entering competition , winning swagger or winning prices . prizes, this is definitely worth doing, usually in December it is a time where a lot of companies are doing competitions and you can easily do Azure function app or something and then do the reporting back. The one that I find most interesting and thrilling is, of course, if you are coming from a big enterprise, hiring people is always a problem. How to do it, I had a go on applications like Meet up and GitHub, GitHub archive is You are searching for people on LinkedIn endorsements, you can find particular programming language particular skills you are interested in. And get people and start engaging in conversation. On the other hand, GitHub archive is also part of a search engine that basically goes through the whole GitHub goes through the location, goes through all that text for the languages, Are you also use GitHub , you can also expose that and go with that in a semiautomated style. We have covered short messages , the first one, turning out , the alarm goes off and your mobile phone acquires Rasberry Pi. You can spend some time at different services to give you a much better overview of what can be done. And not into all of that gaming, twitch, but it is definitely doable there. All those daily tasks. Don't forget to go outside. I'm definitely fond of stripping everything, don't forget to store username, passwords, we have a special design for that. I am very famous for forgetting them. The usernames and passwords. That is about it. The last part was that I have managed over two years to get a Twitter account up and running and having 500 followers, it was quite interesting. With hundreds of tweets also. And I have decided to sell it. Just to see if there is an area in Slovenia to sell a Twitter account. After posting an ad on a Slovenian website, where you can sell literally anything, I managed to sell a Twitter account. I do not even know if this is an area or a market for that, but I am curious. Thanks. If you want to get in touch, please do so. Well, Tomaz, what are wild to of technologies, and how you have applied them to automate your social life! A question I have from my side. You have shown some recipes in doing this with your personal life, how you can automate things, what other kind of things that people can take away from this in maybe automating some professional services or tasks? I would definitely go with into programming or scripting , any type of, let's say websites or developing software, stuff like that, you can script a lot of stuff on that as well. So RPA robot processing is something that is, for instance, when you are, let's say, writing web code, you have a couple of those tasks you want to run through and see what happens with what I am coding, if I'm building a product in my basket, what happens. Those types of processes can be easily automated, instead of you writing every time or putting the product in the basket every time, you can basically script that. We can use Python, there are things extremely powerful for that. You can go online and use these functions as well. This will definitely, for enterprises, be a typical case. case. Another one would be finding the best prices. This is what I have covered in the first place. There are many many cases where you can, not to mention if I go to, let's say, Google Apis, there is a huge varieties of Apis available that you can use for automating a business process. The list goes on and on.  Indeed. I think this is a huge callout to automate whatever you can, as much as possible. Once again, thanks a lot for your great session Tomaz and I hope to see you again next time.  Thank you.  Stacey, what is coming up next?  Coming up next, we have got Glenn, if I have said his name might, I am so sorry if I did not. He was going to be talking about is your digital twins. We live in a world these days, I can't remember last time I found -- bought a gadget that was not connected. Within connected in such a way, it is interesting to see what you can get out of them. As your digital twin is the new kid on the block, for customers to create a New World block, for customers to create a New World inside the digital world. Sounds awesome. Glenn is going to talk about that with us today, he is a co-founder of and CTO of Azure in Belgium, since 2012, and and CTO of Azure in Belgium, since 2012, and currently focused on designing and building secure scalable cloud-based solutions. I also happen to know that Glenn really loves Lego and is possibly the reason I have a couple of models in my collection. Glenn, are you there? How are you?  Good evening. I plead guilty on the Lego party! -- Part. I have to admit, I took up playing with Lego again during the entire lockdown, so I plead guilty on that one.  It is a fantastic hobby to have.  It is the perfect excuse to buy Legos for the kids.  Exactly, it is also my son, not for me, all for him! OK, Glenn, take it away.  Alright, thank you for . So, as already said, I am going to guide you today through the magical world of Azure digital twins. Using the time, let's go over the intro slide a bit quicker, as I said, I am Glenn. I am a Microsoft certified Trainer, you can find me on Twitter where I tweet about IoT and Lego. In the code I will be demonstrating today, other code that used to be during the demos, are available on my GitHub accounts are definitely take a look at their to see what is happening. Azure has been something , it is focusing on platforms as a service. We have four We have four Azure MVP is in total and since this year we have an office in Belgium and also our headquarters are in Helsinki, Finland. Let's jump into the main topic of this evening. Let's talk a bit on IoT. So, IoT, if you follow all see the presentations, it is about getting those things, getting those things connected and creating insights and actions upon that. insights and actions upon that. But we have seen today that the focus on connecting things, connecting your assets, assets, the focus on that area is a bit of moving away. There is new trends There is new trends within IoT solutions and the IoT landscape emerging where customers really are moving beyond just connecting their assets, they really want to connect their entire environments , they want to connect their offices, in some cases people are doing quite some interesting things with getting their home office connected. We are really starting to move from a connected assets environment to a more connected environment area. area. And we can see that things will And we can see that things will definitely start to move towards a more connected ecosystem, where environments start to interconnect and so on. What we are seeing is that customers and IoT solutions are starting , customers want to model their environment first, they want to model out their home office, they want to model out there factory floor. They take that model, make a digital copy of it, and keep that model up-to-date with IoT data, use that model to do some simulations on it. And some other automated processes on those models. This is where Azure digital twins comes in. If I had not mistaken, the services is still in preview. It should be GAM anytime right now, I think was announced that ignite, that it would go GAM the end of November, pinning -- beginning of December so we are waiting about that. As your digital twins is about creating the next generation of IoT solution. It is focusing on modelling your environment, by providing you on opening modelling language, but I will come back definitely more on that later. It gives you a live execution environment to basically see the model alive, see the model alive with some live data. some live data. You can input data onto the digital twins from the different IoT systems, from business systems, and you can connect output systems , you can connect timeseries insights and output data connect timeseries insights and output data to storage or to an analytics layer, allowing you to further process or analyse that data. That has been living inside your digital twin. For people that already . The digital twin, currently in preview, . The digital twin, currently in preview, is actually the second generation for people that already played with the first version, you will see some significant changes first of all. In scaling capabilities, In scaling capabilities, also the more flexible way of ingesting data, or extracting data from it. or extracting data from it. As already said, it is based more upon open standards, and the first version. -- Than the first version 1st . This take a peak of each of those different topics, to set the scene for the demo later on. Creating a digital's -- digital twin starts with the open modelling language. The open modelling language, the digital twin definition language, that is a whole mouthful, is completely aligned with already existing services onto the as you are platform. It is alive with IoT plug and play and live with the timeseries insights data live with the timeseries insights data model. The digital twin definition language, it's based on it's based on json LD and it describes your twin or you can describe your twin via different properties. So, a property . A data field that is living on the twin . It has storage, so it is basically a state of your digital twin. Then you have the telemetry. That is the measurements, those can That is the measurements, those can be events, from sensor readings, or any other event happening inside your digital twin. The main difference with properties here is telemetry is not stored on the digital twin itself, it is just a series of data that is flowing The next part you can use to describe your twin is a is a component. A component allows you to build, let us say... you can see that as a sort of interface. For example, if you are describing a smartphone with your open modelling language, you could define a camera as a component. And then you can use that camera component to model out your front camera, or your back camera. And that is the way how you should see the component. And then maybe the most important thing is the relationships. Relationships allow you to define how your digital twin is interacting with another 20. I will definitely come back to that later, because that is really a key component of digital twin. So to come back to the mobile phone examples, you would have a mobile phone digital twin and you have a front and a back camera as a component, and then you can say in the relationship that your mobile phone has a relationship with those two cameras. There is a bit on how that works. Once you have described your digital twin you can create a graph via those relationships, and really show a visualisation of your digital twin and how that twin is working together. Once you model that out, everything basically goes through the live execution environment. So... inside your life execution environment, you can create twin instances based on the models you defined. Those twin instances are connected through relationships and for a graph of the environment. Currently, it is not possible through the Azure portal to visualise your digital twin, but Microsoft has an open source tool called the Azure digital twin Explorer and with that, you have the possibility to visualise basically everything that is happening within your digital twin. The life -- the life execution environment allows you to work with events or process additional data through external computers. So for people that were working with the original version of digital twins, you notice that the compute was inside the digital twin environment. So with V2, the compute has been extracted from the digital twin environment, and you can basically bring your own compute for example, you can use functions to do some data processing, or to process the event from the digital twin. So it is really more based on eventing and data processing via external computer versus doing all of the processing on the twin itself. To communicate or to interact with the digital twin environment, you have to communicate through an API. So everything you do with the digital twin environment is done through an API. So if you want to query data, you need to call the API. If you want to create twin instances or upload a model model towards the environment, you need to do that through the API. There is, of course an SDK available that basically is a wrapper around that API, but ... in a very basic statement, the digital twin service is just a set of APIs made available for you to model out your environment. Inputting data is done through IoT harbour, or it can come from any other data sources you would like to update your digital twin with. Again, updating data or updating or putting data inside the digital twins is done through the API, or with the SDK. It is just a matter of taking some compute and then hooking it up, and there is data flowing inside your system. Then there is the whole storage and analytics. It is basically now we have defined our twin, we have modelled it out, we have some data being in adjusted and some data flowing through the system, so that is an important one. And then you can output data to other sources. You cannot write that via Event Hub, Event Grid or Service Bus. And those rows and endpoints basically open up a range of new scenarios allowing you to store your digital twin data in a Data caplet. digital twin data in a Data caplet. You can connect it to Logic Apps, to automate some stuff in the previous session's we saw that, to automate your social media, if you want to have some data from your digital twin but put it to your social media, you can use the Logic Apps from the previous session and then put some data from your digital twins out there in your Logic Apps. You can connect digital twins with Time Serious Insights and to have that historical view of your data. So, that was a very quick was a very quick walk-through on all of the capabilities of digital twins. And I think it would be best to just show you an end-to-end demo on how digital twins work, how you can create your digital twins so you can really have a good understanding or you have a basic understanding on how you can create those digital twins, and what the use cases are. So this is the setup that I will be So this is the setup that I will be showing you tonight. So... it has always been tricky with live demos, but I'm pretty confident that this thing will work out. So what we first lets give a high level overview here, so first of all, what we will do is we model out our digital twins. our digital twins. We will upload those models in azure tag digital twin itself, and once we have our models there are, we will start creating our digital twins. And we will also create our relationships. So once we have that model running we have created our relationships, what we will do is I will use a set of simulated devices that are connected to our IoT harbour, and through the external computer, through functions I will update the digital twin and basically update all the running instances of my digital twin. As the last one, what I will show you is how you can extract data that is flowing through digital twin. I will show you how you can create a rout that is connected to timeseries insights where you will be able to see data flowing through the entire system. So what we will do is, let me open up visual studio, and I hope this is visible. So, as already said, what we will do is, we will start before you can create any digital twin or any relationship inside the live environment, you need a model. Sorry, for that, you need a model. In this case, for my demo, I have modelled out my house. So, in the end of the demo you should see a visual demo of my house. So, how does this modelling language work? This is an example of how this is constructed. Let's take a quick peek on how that looks like. The first important thing, that you need to specify, is the ID. ID. The ID The ID consists of DTM I, it has to contain a domain, a model identifier and then a version. So this is just to give your model a certain ID. Then you give it a friendly name and both this type and this context are fixed values for digital twins, and just have to be there for the digital twin API to understand what you are doing. All remaining interface data or data to describe your twin is found within the contents area. Here, to define my house, I have added two properties. One with the construction year and one with the owner name both are string types, so remember, properties are fixed values and these are stored on the twin itself, and remain there. And then, the important thing here is the relationship. And with defining that relationship, you basically define on your model, that this interface has or can have a relationship with a target. The target again, is just specifying another ID in a minute I will show you all this ID. It has to have a target, and have a target, and a name as well. We will come back to that name later. So my house has a target or a relationship with floor. So let's open up the floor here again. Same approach, you have to give it an ID, give it a friendly name, and the remaining interface data is within the content area through your systems. I did not specify any particular values, I just had my floor has a relationship with rooms. If you can visualise what we are doing. I have a house, my house has different floors, and on my floors, there is a relationship with rooms. So, let's take a look at how a room looks like. Again, here it has an ID, I specified two properties, as you can see here, you may be wondering, those look like measurements to me because they are temperature and humidity. Yes, absolutely correct. But what I wanted to do with this demo is inventing, what I will do as I will update my digital twin with data coming from IT hub and store that value to the digital twin in these properties. So, my room, again, has those two properties that I will update through external events. And has a relationship with my senses that I will deploy in my room. If I take a look here, if I go to the senses, you will see here the senses, my senses -- censorious -- sensors , they have the values coming in. What I will do in the demo here, I will approach this model, I think there is already a proto-. Let me. Let me first show you the Azure Digital Twin inside the Azure portal. When you provision, Azure Digital Twin, you basically get a service like that, the most important thing for you is the hostname. Where the IP will be hosted, allowing you to interact. So, we have created our models and to gain sometime during this demo I have already pre-uploaded those models to my digital twins. What URC here is the Azure digital twin Explorer, and I already mentioned this is available on GitHub, you can just download it build it on your local machine and run it. Connected with your instance and it allows you to really have some good visualisation. As you can see here, these models are available and I have uploaded them, if I click here those should come up. You see here these models are here, my house model is there. My room model is there, and so on and so on. You can see that these models are available here. So, I will just run this query to show you that currently we do not have anything within our digital twin. We only have the models defined. So how do you know create your digital twin, creating the digital twin is based on the models. models. As I already said, how you interact with the digital twin environment is through the API or through the SDK. What I will show you is how to interact with the SDK and how that works. Where are we in our demo? We have our digital twin definition language, we have uploaded them to digital twins. Now what we will do is we will create our graph within our digital twin through the SDK. Let's jump back. Close this one up. Let's open this one. So how do you interact with digital twins through the SDK, by using the Azure Digital Twin's.call new get package. NuGet package and what you do there is created digital twin client. And that is just an endpoint that you need to provide. The endpoint Azure Digital Twin instance, and you need to provide the token. With those, you have a digital twin client allowing you to interact with the Azure Digital Twin. And here comes the way on how you can build of those digital twins. First of all, you need to specify some meta data. So, what are you creating? What model are you using? In our case, I am using my house. And the client here is digital twin, that is again, the apex value that is needed for the underlying API to know what is going on. You take that meta data and add it to a new dictionary, or a new json object. But together with that meta data, you add the properties that you defined on your model. If we take a quick peek on our house, remember here we have construction yeah and owner. While creating an instance of our model, you can see here that we have construction here and owner. What you then do is use the digital twin clients and you say create digital twin a sink. a sink. -- Singh. And use the data you have created here. In that way, what we will have in our graph, we will have a digital twin called house. Based on the house model, with the two properties we have provided here. Basically, this is the thing you can do, for example, let me show you the floors full stop again, you need to specify in the meta data. Let's not do that. So, this is using the floor model. Again, digital twin. Remember, on our floor, what we has , we did not have any properties there, we only had a relationship , so we are not specifying any other value than our meta data. Based on that, we basically create three different digital twins, one floor, two floor and three floors, each of those twins will represent an instance based on the floor model, but with a different ID. So, now we have our house with the three floors. This is basically how you model out the entire structures here, I have created the kitchen, living room, some back rooms, and so on. I have also created the senses (Laughter)Sensors -- we have five senses and so on. The most interesting part of the start, we now have created digital twins based on our models. We have our house, our different floors, our rooms and those senses. We will create relationships between those digital twins, allowing you to create a graph of your house. Of a house or your environment. Again, how you do that is you create a dictionary with some meta data in there. You need to specify a target ID, remember this is the idea of the digital twin you created. And the relationship name is the relationship name of the relationship that you defined in your model. Remember on our house, we had a relationship floors allowing you to target a certain floor. When you see that coming back , so our relationship name is floor and we are targeting, in this case, floor one. What we are doing then is creating a relationship a sin, we give it the digital ID and give it a certain ID and then pass it along. This is how you create the relationships between those digital twins. Let me close that up. Let's not save that. And hit the wrong button. And hit the wrong button. -- Run the button and let's see what comes out of this creation. So, what we will do as I will go here the set of digits. What it will do is it will code and code, what I have just walk you through. It will create the digital twins and It will create the digital twins and creative relationships. -- Create the relationships. This is available on GitHub if you want to take a peek. If you want to improve or if you notice that some things have been horribly coded , I am very happy to accept simple requests. Let's hit set up digital twins, it is going, important I need to authorise first, click this, let's close this down. And if all goes well, you should see some things being created. I see that there is some error over there, let's take a look at what happened. Unexpected error where I am trying to re-upload my model again, but the models I already prepared those so that should be an expected error. As you can see, we have a log file here, saying our house was created on different floors, different rooms, different sensors have been added, those relationships have been mapped out. My digital twin has been signed off. It's close this window down. Let's go back to our digital twin Explorer, if all goes well. Take a look at hit run query. And as you can see here, this is now the digital model that is available in digital twin's, based on the things we have created. You will probably notice here that I have connected this sensor to both my kitchen and my living room. This is just a way of demonstrating to you there is no need to have one-on-one mappings, but you can definitely not that digital twin out as you wish. -- Map that . Now that we have everything available here, let's start shooting in some data inside our digital twins. As I have already said, digital twin is a set of API's and how you interact with it is through the API and therefore, you need to hook up external computer to ingest the data inside digital twins. In our case, we will be using these functions. Let's jump to IT hub, if all goes well. We have IoT hub. What I have created is a set of devices , you noticed here that I have deliberately named the devices after the device of the idea of my digital twins, just for the ease of having a more easier setup on this demo. How do you now extract data that is now entering IoT hub, how you ingest this into the digital twins? You can use this by leveraging the event grid capabilities of the IoT hub, in this case, I have created an event grid subscription that filters on device telemetry, so every time there is a device telemetry available and I/O -- on IoT hub, an event will be raised. Excuse me. An event will be raised and that event will flow onto the event grid and will be picked up by my Azure function point and will process the IoT have messages. So, you can ingest data from IoT hub. Through digital twins by using this event capabilities. Unfortunately, there is no direct routes from IoT hub, towards digital twins for the moment. But it would be nice to see that better, to get the story here showing up, but leveraging this through event great with Azure functions is also a perfect solution. But it would be nice to have that route to digital twin here as a first that route to digital twin here as a first class citizen, but with this it works perfectly fine. We have our event grid subscription here. And we have our Azure function, and that as your function is just a basic set up, what it will do, it will listen on event grid, and it will filter out the data and. Let me jump in this code, what it will do here is use the digital twin client , update the digital twin a sink, with the twin ID, in our case, it will update the senses. -- Sensors, therefore I fuse it with the same name. And it will pass the value of the measurements here. So, let's start the simulator. So when the simulator is now starting to transmit data IoT hub . Remember IoT hub when there is device JACK: Pocket map it will use the SDK to update the digital twin. If we go to the digital twin and take a look at our sensors, you see here is that the temperature and humidity has been updated. You can see here that we now see live data flowing through the system. So, now we are using the live data to keep that between up-to-date and to process data to process data within the digital twin. Another capability of digital twins is, as already mentioned, let's go back to the overview, so that we do not lose the overview here. So we have our simulated devices through IoT hub with functions updating the data on the digital twins. What we now want to do is that we have our life execution environment being updated by our Azure functions, so what we want to do is hook up some external processing tool or an external compute or something else and, in this demo, I have hooked up time series insights. How we will do that is by defining a route on the digital twin towards event hubs and then connect time series inside event hubs. Let's take a peek on how that works. I am just looking at the time, so let's go a bit quicker. So you can create an endpoint on your digital twin. So for people that are familiar with IoT harbour -- hub it is the same thing. You can connect the event point to a service hub and then give it a name. Then, you can use event routs that will use the endpoint, in our case, we will use this one. So every time a twin gets an update, so remember we are updating between through functions, we are updating the telemetry of it, so every time it gets dated a message will be available on event hubs. If all goes well, that event have should already have some data of my simulator, and if I go to time series inside, come on... if I look at time series insights if I look at time series insights you with C year that I will see some data flowing through the system right now. So this is life data coming from my digital twins being shown in time series insights. So, this is the end-to-end functionality on how you can use digital twins, and how you can leveraged the different capabilities of extending your digital twins with external compute and so on. Let's jump to the conclusion. So digital twins is there to build those next-generation IoT solutions, moving away from just having your assets connected, it is wheeled around the ability to model your environment and it is really an event driven approach. Remember, the most important way on how to interact with the platform is through APIs or through the SDK. At the moment and there is no real, tangible things inside the Azure portal, but if you want to visualise your groups you can use the digital twin Explorer which is available on GitHub as open source. With that said, I see I have ran a bit over time, excuse me for that. If you have questions do not hesitate to which out -- reach out through Twitter or any other social media channel. Thank you for attending, you can find the source code on my GitHub account. So thank you everyone for attending, enjoy the rest of the community day! Thank you. Thank you very much for that was in session! Again there was a whole lot of stuff that you managed to get through in that time there! Unfortunately we do not have time for the question that we do have, but as Glenn said if you need to get in contact with him go via Twitter. Thank you very much for joining us this evening. Thank you, see you! Thanks, Glenn. Our last session as hosts. Who'd we have coming up? Coming up next is Ebru Cucen and she is a principal consultant at Contino. Next to being also a trainer, a book reviewer and lots of other things, she is also the co-organiser of PowerShell London. It is great to see that. In a recession you will be talking about policies. BDD. So behaviour driven development of policies. For me, policies as a developer that is typically something that comes in when stuff goes wrong. When you notice I should have had a policy to When you notice I should have had a policy to control myself! And I think that is the case for many people and developers who only see policies as a new when things go wrong. Hopefully you can shed some light and make policies more palatable, more digestible, for other people. And so looking very much forward it to your session on applying BDD on policies. I think we cannot hear you. I think we cannot hear you. Ebru? Can you check if you are still on the. No ... you should be fine, can you speak up again? Hello, Ebru can you hear us? Always fun with technology when you are doing a live session. We will be sitting here until everything works, so that gives us a couple of more minutes to chat. Can you hear me now? Yes! Welcome. I'm not sure how the two will work but ... We can hear you find that you have the floor. Thank you very much. Let's see how this will go. OK, can you see my screen? Yes. And now? Yes, you are good. Thank you. And I will start with who likes rules and regulations? When we have no input we cannot understand why they are there in the first place. I started my career about two decades ago when we had to follow specific processes and control. I knew it was all good intent and to keep our efforts safe and today I am at the height of IT department working with finance seems to put these regulations into place. I am working at Contino as a consultant where we do the right . best practices for the right outcome for the cloud journey and implement the right operational model to allow engagement from older parts of the business. And today I will tell the story of how we change the mindset of the central IT team to enable collaboration with other teams. I am a developer at heart, I am the lead principal consultant at Antigua. I have been vegan for the last five years, M during the ethical and environment impact I've done so far, selfishly. And I have an eight year old son who enjoys roadblocks and PlayStation and with COVID he has more time to enjoy screens, as a parent I have no comment. So my agenda today is talking about problems with the security guard rails with an insight on what happens when a team on boards the cloud and these God reels, how they are set up. I will explain how we enable collaboration between the application teams and the central IT team, which sometime there's a blocker. I will mention the trade-offs we had to make and maybe it will help you if you are in the business of implementing existing ones. After the first 15 minutes I would like you to remember three things. First, application teams should also play a role in those controls as it is not just one teams those controls as it is not just one teams as possible and see. as possible and see. Secondly, BDD is great. If you have not checked out it allows the human readable test to increase collaboration across multiple teams. And lastly, try small iterations, do not be afraid and innovate. It is OK not to get it right on the first attempt. So, what happens when on the first day in the central IT teams, I will call it control teams such as networking, security and finance, they should provide guardrails for the new client platform. They needed to secure each service, such as stored accounts, key falls and event hubs ... sorry. A good flow product manager would be able the priority of the items and ask for what features they offer and how these will be enabled into life. And the control functions teams will question the baselining standards if they have any similar services in-house. An example is the crypto keys. Sometimes there are no policies, sometimes there are regulations which are not we are guided . are not clear guidance, they may have a technical inconsistency or procedural inconsistencies. Sometimes it is sparse and there is no guidance or definition on what achy is. The definitions can be misleading as well with the cloud's new terminology. It does not help the big picture. Another one is the regulations for nonhuman actors or service accounts. Where your tickets will be stored if they have to have certificates? And what their certificate expiry policy will look like. It can end up in long discussions. If you have a cloud already, this is the second cloud that you meant the same domain conventions to? Should it have three characters or two characters? If you don't get the rights or agreed decision, you may just be redeploying the whole infrastructure after one month in the line, and everything becomes immutable. It is OK to redeploy, it is just another iteration of the policies is not in place, not agreed or contributed multiple teams. multiple teams. The neck logical thing, is to lift shift. The teams modify the stickers, they should not modify, renew their certificates. Some regulations could continue on without us, with a number of features the club provides, the cloud journey becomes journey to the rest. And then the teams admit the traditional controls do not fit, the response we get from the management is . Sad trombone. For a couple of weeks that are usually spent reading online documentation, training for fundamentals, now we have more questions, bring out our people, retention policies. This is the service definition documents, this is hundreds of pages of documentation to be prepared for the use, to create the holistic view of the products of the cloud. A good approach here is to execute well organised workshops. I'm hoping there is people with good herding skills to keep focus on these meetings, otherwise you may find yourself in circular otherwise you may find yourself in circular conversations. With our technical advisory boards, the technical and strategic decisions, they need to be prepared and documented ready to be implemented, are they the right one? Then all the hard work goes in, the core platform engineering teams are there, publish it, the product teams can be surprised, if not frustrated, with what cloud . Enabling innovation , if they give a limited area to publish their applications to, if the decisions on behalf of the users, what will cause the platform? The shift left never happens. -- Shift left. And there was disappointment, or should I say. To get the right outcome, application teams should be part of these discussions and I believe this is our cloud journey. So, to get it right, what we should be doing , and I hope we get this right, have we enabled the collaboration between the central IT the collaboration between the central IT team and the application teams? Would you invite the application teams to this workshop. They are part of the journey from day one. Then build a policy pyramid, shed across the whole platform, with senior roles, the baseline policies are on for everyone on board for the platform. They are global and go on through to management level, so they've provide insurance to everyone. Secondly, -- second layer is about share policies that can be conceived by multiple teams. And at the top level, we have application-specific policies. Which enables rules and regulations. But organising policies like this gives the control teams to focus on what they care about, and not to be the bottleneck for others. be the bottleneck for others. A good example is regions, but I going to be used. What type of tagging policies are going to be integrated with other systems? And maybe which resources are going to be allowed across the platform? And application teams can have a voice and freedom on shared policies and application policies. The key mode implementation, for example, can be customised according to the use case, if a client wants to merge the keys, list the keys. When these policies are all called, application teams can pull this and the security team can review and if they are happy with the policy and the test, they can approve and they can just go live. To look at how we did it and what BDD looks like. This is an example of a feature file. And BDD is behaving during this development, it's a combination of BDD and BAE. We need to figure out if it works, with the test and businessperson, to look at the test and understand what it does. To get the right outcome. The tests are readable, it is alive application of the test. The feature at the toast. -- The feature at the top, it is to group the related tests that we can run together. Gherkin is the common cell which is to make it readable, rather than statements that allow us to write the tests and execute them step-by-step. To run a group of features together, we can use tags. They allow us to provide for specific sources or specific people when you want to execute them. As a framework we use the cucumber.JS. And here this is where the feature file is executed and we have a type of script. This is the test implemented, we have annotations after and before. To set up and tear down the test. We have gherkin annotations, giving statements, to match the step definitions defining the feature file. So, what type of challenges we have? The first one is about the measure policy. It is in real time , the engine runs up to 24-hour is to get the compliance reports. It may take, worst case, 24 hours, so we categorise our tests to find out which ones are going to be the first ones to get the results of the last ones. The second one is each effect has a different weight time, they will happen at the same time. Some other quicker response time, compared to the audit policy, which is a misconception. The last challenge is the custom policy, which would support as much as we would like to. So we want to build on policies to track versioning and run them reliably. This is when the effect has time, the axis has the time. This is when modify happens, then deny, then audit is last. The most restricted once had a different order disabled, we had to take all of them into consideration when we write the test and agree on the effects. In terms of BDD, we had to do some trade-offs. The decision on the language, which one are we going to pick? The infrastructure code that we implemented, Golan would be the main choice, but the learning curve is so low that we could on board teams and the tests easily. Second, is when to run the tests. We executed the policy assignments together with the employment as well. So a dedicated integration environment where the management can enable us to provide that real test. Here are some books , this was part of our team when we implemented him, we are lucky to have him. Speak -- specification by example as a good book to read and also the BDD implementing, that is the book to check out. I also include three references, Liz has lots of good insight on how it should be implemented and this blog post is also interesting, if you want to check it out. Thank you very much. Thank you, Ebru. For a great session. I think in the interest of time, if you have questions for you they can reach out through social media and also post their chair . mac questions here in the chat. Once again, thanks a lot.  That is if our time hosting the event, it is time to go for us, I am afraid. I hope that you have enjoyed the last few hours as much as we have, it has been fantastic. Be here and guiding you through the sessions. Don't forget the challenges we have, a.k.a. the MS/VACD challenge, try to remember to do that in English, whenever I'm speaking Dutch is so hard. Thanks for spending this time with us. Coming up, we have Barbara and they are going to guide you through the next sessions. You have a fantastic evening coming up. Goodbye.  Goodbye. Hi, welcome back. We have seen some excellent sessions today. Together with Barbara, I am going to host the last couple of sessions, we are here until I think 10:15? Maybe sometime afterwards because we are running a bit late, but it will be OK thing. I'm excited to be here, I have been a developer for about 20 years, I am very passionate about cloud and I'm very excited to host these sessions here with Barbara.  Yes, hi. My name is Barbara, I'm an IT consultant and Microsoft (unknown term), and very excited as well. Some great sessions coming up. , I'm looking forward to the next session because I am a developer and the next one is about application runtime, from some amazing speakers. One of many sessions with speakers today, so let's see if that works as well for you can post your questions in the chat. We still have the skills challenge, so the link is down below. Definitely check that one out. Our Edwin and Nicole ready? Yes we are, thank you for having us.  It is great you could be here. I always have a knack for ruining people's interest lies so I'm trying to do that this time. But you are both principal architect, you both work at Info Support, you are both Microsoft MVP, how can we tell you apart?  Wyoming Azure MVP -- while I am on Azure MVP, I'm just developing technologies, I do a lot of Azure, but I'm not the Azure in Fifi -- MVP, there is some distance between us.  So, you can tell which MVP you are. We will see tonight for your session, We will see tonight for your session, take it away. Yes. Cool. So we are going to talk about Dapr today and we are also going to take a look at eShopOnDapr which is a reference or you shop or container you might know, and we basically ported that to Dapr. We are going to use that, we talk about a couple of examples we are going to show today. So, without further ado let's first look at what Dapr is. Stands for Distributed Application Runtime and that is exact what it is. It is a runtime you can use to build distributed applications, and that is a broad concept of course. What we see when we see people building these kinds of microservices applications, we often see a lot of infrastructure being used. So you will have message brokers for pop-up messaging, status stories, maybe you have some discovery service or finding services, all of these kind of infrastructure and components. This is exacting what Dapr can help you with. Dapr is there to hide all the capacity of the components you can focusing on building your application and not focusing on all of the infrastructure so to speak. That's where Dapr comes in. So the Dapr runtime offers quite an extensive set of building blocks. You see a couple of them, but most of them we are going to look into more detail later on in the session, but I will walk through them briefly now. briefly now. First of all you have server to server invocation. This is basically for calling other services in your microservices application without knowing where they live. Then there is state management and this is for storing keyvalue pairs or some contextual information, in an outside story so you can basically build stateful services you can rehydrate the state when you need it. Then there is a publish and subscribe building block. There is also some research bindings and triggers, and these resource bindings and the triggers can be used to integrate was OF different external systems. We will also go into more detail on that later on. Next to that, there is also a virtual actor model that Dapr offers so that you can build also actor model based things, and you can see what goes on within your system and monitor your services. Finally there is also a secret building block and this is for handling stuff that you do not want to publish, things like passwords or connection strings or certificates, and there is a whole suite of tools around that and Dapr can help with that for you. And with the last block it says extensible, because the whole Dapr ecosystem is basically completely open source, and we think that the more building blocks will come as time progresses. The cool thing about Dapr is that all of these building blocks can be called using standard HTTP or GRC communication. This also means that you can leveraged up a basically any platform or language with which you can do HTTP or gRPC can medication, which is probably from everywhere. Of course there is also a more convenient way of working with Dapper because there are several STKs available for different baggages. There's one for Java, for.net, for NuGet, and it offers a more intuitive way for you to use the building blocks instead of through the raw HTTP API for example. There are more frameworks that are nicely integrated. ASP.NET Core, as your functions are Azure Logic Apps. We are going to focus on a couple building blocks and show how these work. And finally what is also very important is that you ... you can run it on your laptop or desktop, but you can also win it in the cloud in azure tag, AWS or anywhere. So that is critical. So that is critical. How does Dapper do that? Dapr uses sidecar architecture, because it is running alongside your application code. So, in this case, So, in this case, there are several ways of running Dapr , in this case we are running it in a stand alone mode. You just have your application code in a separate OS process, and then the Dapr sidecar is ran alongside that. Your application communicates with it to leveraged all of the building blocks. Another way of running Dapr is in the cupboard ET's ET's . It runs alongside your application. Because in a pot, these containers can talk to each other it is very easy for them to talk to each other and communicate. So this is how this works when you run in Kubernetes so Sander is going to tell you all about how we did support the eShopOnContainers. Sander, can you share your screen? Yes, Yes, I just got a note from the production that my I just got a note from the production that my video is not showing. I hope that at least my window is showing because that is actually the important part. I can see it, but I do not know if it is in the live stream. If you can see my window I will assume it is also in the live stream. So, we are going to go into some of the details of these building blocks, how they work and how they can improve the existing application. As Edwin already told you in the beginning of the session for the existing application we are using eShopOnContainers. So this is a core application by Microsoft and it is an implementation of a website that lets you buy some cool .NET merchandise and now; Dapr merchandise. It uses Dapr containers, so let's have a look at the architecture. So, we have a (unknown term) and we can request our handle via an API it with. It is using Envoy. In In Envoy we will forward the requests are a number of back and . backend services, and they are the services we provide coffer rationality. The tracking of the items in your basket, the ordering process et cetera. Now most of the costs can be directly routed to a single backend service. However, there are some scenarios that require multiple backend services to work together to complete a more complexity request from the front end. In these cases, each shop uses an aggregator service which is called a web shopping aggregator to mediate that work across those different services. Finally, back and services can also communicate with each other, and that is done using messaging to keep them nice and independent. This is the original eShopOnContainers solution. And then we started to add Dapr to it. For the Dapr version, each component now has a Dapr sidecar. It makes it possible to use those building blocks. What you can see here is that we not only added a sidecar to each .NET service, we also added a sidecar to the Envoy. So now, let's look at some of those building blocks in more detail and see how we actually apply them to each. We will start with service invocation. Service invocation enables our systems to communicate with each other. I will also talk a little bit about observability because that is one of the benefits you get from using the service invocation building block. I have shown you the overall architecture but let's zoom in a bit and look at an interaction between the aggregator service and the basket API. Your original solution when a user adds or changes in item in basket, the aggregator service calls the basket API to store the updated basket. This is a (unknown term). Let's see how we change this to work with Dapr service invocation. So first of all, if you use Dapr, Dapr takes care of service recovery for you. Instead of calling the endpoint of the basket API directly, you now make a call to your Dapr sidecar. Here is an example of making such a call and so that you are aware, and so that you are aware, it contains the application idea of the service to call which is the basket API as well as the method, which is API the one basket. Using the application IDD Dapr sidebar use all of the discovery for you and allows it to call the basket API. That sidecar intern will make the final request on the actual service. Another response will flow back the same way. Now, because these calls now flow through sidecars, Dapr can inject some very useful crosscutting behaviour. For example, Dapr can ultimately retry calls when they are failing. Dapr can also make calls between services marked secure using mTLS authentication, and it also includes automatic certification and you can add access control policies to control exactly what operations clients can control exactly what operations clients can do on services. And Dapr can ultimately track metrics of those calls and services to provide you with insights and diagnostics. So, I think it is time to look at some code. Let me switch over to visual studio. As you can see here, I have got a complete solution appear and the solution is completely open source, it is on GitHub and we will share the link with you after the session so that you can play with it yourself. You can see here, we have some files here to run a shop locally, that is called self hosted modes in Dapr terms. Kubernetes is actually the preferred way to run applications with Dapr. I go on to the container here you see a lot of containers are running so my application is already up and running, I should be able to go to my web browser and show you the shopping front. Welcome to our store, we have some great in here. As you can see, we have just received some new Dapr swag, so for example say I want to buy this night . nice And Leinster in a Dapr hoodie as well, let's good back to the top. You can see my basket here which now has two You can see my basket here which now has two items in it. If I click on that, you can see the items, maybe I want to buy some more caps and buy some for my friends, and then I can check out the basket all fairly basic web shop stuff. You enter your shipping address, you enter your payment method and then you can place your order. Then you will see this pop-up notification, because now there is an ordering process that is being started, and you will be kept up-to-date on the progress of that signal are notifications. That is basically the you shop functionality in a nutshell. Let's go back to the code. So in a previous life, I talked about an So in a previous life, I talked about an aggregator service making a call to the basket API to save the updated basket. If we look at the code of the aggregator, so the aggregator code is here ... and here we find this basket service class, which is responsible for medicating with the basket API. This uses service invocation, and because eShopOnContainers is completely written in C# tag, we really wanted to use tag, we really wanted to use the SDK to make it more natural. Using that SDK, we kept this nice Dapr client object which is injected into our constructor. We can use that Protagoras client to basically call all of the different Dapr Apis on the tag . sidecar. This PacifiCorp happens here, so this will get an updated basket and here we use the Dapr client to make the call to the basket API and keep that current basket. That method takes a couple of parameters, so first we need to give it the logical name first we need to give it the logical name of the servers I want to call which is stored in this here, so the actual value is basket API.  I shall share my screen. Lastly, because the basket API is connected, I need to use -- send this, notice we use HTTP extension. You can also use this to specify other details whenever you are calling a server that has HTTP rest endpoint. Things like query string or the HTTP firm which is supposed by default, by the way, so I don't need to suppose it was called. A quick look at the other side of notation, because this is of course the basket API, so let's see what that looks like. Let's go to services basket. There is not much to tell, because this is just a plain old controller. The receiving end of the equation does not need to know anything about Dapr, it just gets called. The call will go to this update basket and then it will take the updated basket, and you will see the await response Atari better, - repository. I'll show you that later on in the session. Now that we have used the Dapr service, one of the benefit is that Dapr will automatically collect those traces and we have configured the Mac to export all the telemetry data . You can see all this tracing information. I just did in that you shop content. -- In that you shop . We doing this for free, just because were using this, we can go to this very nice dependencies page we can go to this very nice dependencies page where you can get a very nice service map of all your services, because you can get that information from the traces, because yet again everything is running to sidecars. There are other things you can use, the deputy is currently adopting telemetry, so basically everything that works with telemetry, you consent to tracing information to. And with that, let's move to the next building block, and look at some async messaging. Take it away, Edwin.  Yes. Let's look at publish and subscribe. If you look at eShopOnContainers, then all the services that are in the eye communicating asynchronus only by communicating asynchronus only by using this, to our container, one for MM cucumber one service bus, this is pretty handy because you could use this for your local development and test, then use Azure service bus once you go to production, for instance, you can switch between them. Because this is an obstruction, there was also to implantation, there were several classes take double asthma created for creating a connection with a message program and also the behaviour of the message broker like sending and receiving messages. This is quite an elaborate set of code that was created by the creators of eShopOnContainers. Besides that being quite a lot of code, which is all infrastructural stuff , want to build business credit business value, also using these building blocks, if you look in the starting class of one of the services, first of all you have to figure out whether you are running your service +4 whether you are running your service plus 4MQ, -- service bus. It is quite a bit of code, there is quite a lot of knowledge that you must have around these two message brokers, in order to use them. This is quite cumbersome. This is exactly where the building but from Dapr can help you. Let's look at how that works. This case we use a component of the corporate, and that is what we want to use for sending this message. Next to these components we also have to specify the topic that we want to send a message to. the topic that we want to send a message to. The topic is basically like a mailbox you can send messages to, and other services can subscribe to these messages on a certain topic. This is how I can send these messages. And when I call this API, and like we said before you can use HTTP or G RSPCA. Once we have called this API, the Dapr sidecar will eventually call the actual service broker and deliver the message to the service broker. I could be using nets, but as the service bus, Dapr extract that from me. On the subscribe side, there are also Dapr sidecars for the receiving end and these Dapr sidecars will create subscriptions for the topics that you are interested in as an application. Here we see to sidecars, we have two services, in this case, that is the catalogue API and the payment API, a subscriber was created on this topic. Once the message is sent to the message broker the Dapr sidecar will pick up this message because it has a subscription on it and it will call a certain endpoint and deliver the payload. This is how it works if you look on an API level. How can you configure this? Well, if you are using .NET and he -- eShopOnContainers uses that, there is a great way of using the core integration for that. that. What you see here is that he went API methods, these are two different controllers, with a methadone, status change, status update, these are just -- method on, what I can do to make sure I have some script on a certain topic, and a certain M -- API is called, I can add the topic attribute to it. This is something that comes from the .NET SBK, the integration package, you could just for the topic on top of it and you specify, again, which component you want to use, in this case pops up, and the name of the top you want to subscribe to, then the sidecar will pick up all of these topics and create subscriptions accordingly. It's pretty easy to do. The start-up of the application of your service, there are some convenient methods of adding Dapr to the application. In this case you can call Dapr on the services under control as you already have therefore your ASP .NET core integration pipeline, and this will be services to dependency injection that is needed to use Dapr. Also in the configure method of the start of class, you see two calls, one issues cloud events, and this is because Dapr uses the cloud events format, this is a standardised format for sending messages, so when the messages received, it can be unpacked be read by the application and by the sidecar and also the map subscribe handlers that you do in the endpoints part, this will make sure that all the subscriptions for the topics that you have added, will be created when the application starts. This is all you need to do. Then you automatically start reacting on messages that come into the certain topic that you have configured. It's pretty easy. So, how do you configure the component? In this case, the pub subcomponent? You create and assemble these files, you can You create and assemble these files, you can create the Dapr configurations files, there are several parts. First of all, you name the component and as we saw in before, the name was Pub sub, but this is something you can provide. You can also have multiple configurations, each with their own name, this way you can switch between different configurations. There is also type in there. In this case, it is a purpose of building blocks, and the one it uses the message streaming, one of the brokers supported out of the box. And finally, each type of message broker some specific meta data that you can add to it, this will defer the message broker. And by adding this configuration file, and giving that to your Dapr sidecar, it knows how to impact with the pub sub building block, this is how it works. Let's quickly look at the example in each can - eShopOnContainers, this is C#, I only have some orders, I have a message on teams. Someone wants to get in, that is not for me. What I can do now is look at all of the topics that were created on the application that was started. You can see all kinds of topics here, these are in red this, I'm using it as my subcomponent, and when the starts of, they will create certain topics that are used for sending messages. As you can see now, there are no messages on these topics now, but when I, for instance, order a cool Dapr sticker and hat and I order a cool Dapr sticker and hat and I check this out, I buy this. What you just saw , in the background, some ordering processes being simulated, so there are all kinds of events flying around between the services. If I now go back to the commander and look inside this and refresh it, we can see that all the events come in. This is a user checkout accepted event, or an order started integration event, and as you can see here on the right, these are just plain json files, or json payloads, that contain all kinds of IDs and stuff like that. This is also according to the cloud events format. This is how it works. With that, this go on to the next topic and that is state management. Take it away, Sander.  I'm going to switch screens again. It will be online now. It will be online now. State management. Statements put in Dapr from a value API, you can use it to create estate services for stoppages meant to store some contextual data, not to store your entire relational database in it. relational database in it. If you look at the original eShopOnContainers solution that uses a variety of data stores, many of the services use sequel services, and the use this framework to access the database, and it is perfectly fine because the framework is very good for that. In an entity framework, there is a support for different databases, and that already gives us a level of portability for relational data. There are also some services, an example of that is the basket API. The basket API stores its data in Redis. To make it work there is a dependency on exchange in Redis, which is part of the SDK which you will need to learn in order to use it. We changed that that implementation to actually use the Dapr statement building block. As a result of that, we have got many different stores to choose from without having to learn any other SDKs. Let's see how this works. The Dapr state management, you can store by simply making CORSIA sidecar. If the basket API wants to update the basket, it can simply do a request like this with the data it wants to store. And then in the sidecar, it will persist that state, to make a packing status for work, whether that is a basket or customer store. To read that back from the store, you can get a request, if you want to query, then the Dapr side, look at it for you from the packing store and return it. Let's see what that looks like in code. C# already used to this C# already used to this to access the data store. For example, if I go to the basket API, For example, if I go to the basket API, then infrastructure depositories, we have the basket repositories. Of course, the original solution was not called Dapr repository, it was called Redis basket repository, we change the name, and the Emperor mentation, double mesh -- implementation. Let's start at the top as you can see, we use the client and integrated the structure and it allows access to one of those nice data Apis. Apis. As you can see, these APIs pretty simple. If you want to delete something, we can just retrieve states, here we use get state entry, which makes it possible to get something from the state store, change the value and save that back to the The state store that actually does the persistence is located in the component file. Let me go over to the component file and as Edwin already showed you you can see that we are currently storing our basket state in table storage. This file also makes use of another Dapr building block which is the secrets Dapr building block which is the secrets building block because I do not really want to store my storage account key in this file, because this file will get sent out to GitHub in source control. So instead I'm storing secrets in a KeyVault and I'm telling Dapr using the secret block. Let me quickly show you , let me go back to the shop. You can see right now we just placed an order to my basket is empty. Now if I go to storage Explorer and I go to the storage account, my table should be empty, because there is nothing in my basket. And I'm currently the only user of this instance of the application. As you can see there is no data in the As you can see there is no data in the back and the story yet. Now, let's add something. Let's say I want a .NET hoodie, now we have something in the basket, if I switch back to storage Explorer and refresh now you can see if I like this and open the details just to prove it to you, you can see that there is now a .NET black hoodie in my back end of story. There's a very easy example just a show you how it works, but realise that you can now very easily change or switch back and source bus by changing the configurations. You have to be aware of subtle differences between implementations. So for example, with table storage, a single value has a maximum of 30 to thousand characters. . 32,000 characters. If you are switching between a star without limit and a store that allows you to store unlimited blobs you may run into problems where one story cannot sort all of the data that you want. So you get a lot of what ability, you just need to pay attention to the actual indentations a bit. I do think we still have some time to talk about bindings, Edwin? Yes. Let me share my screen. Resource bindings are basically a way to integrate with different external systems, this is a very broad aspect, and you could argue that also pops up for instance as a way of integrating with different external systems. There is quite a small difference there because this does not only focus on message brokers and sending events, but it also has all kinds of bindings for twitter and Twilio, SendGrid and Azure storage. It is basically a way of getting events from certain systems, for instance when a file is blocked and dropped into a blob storage , and then you get an event litigation. Or if you want to send an email or whatever, there is also a way of integrating them for instance with Twilio SendGrid without needing to know how they work. There are two types of bindings, input bindings which are also called triggers and they react to events and you also have external or output bindings that you can use to do outbound communication like the Twilio SendGrid. You can use them through HTTP or gRPC to call them or the SDK. How does this work? I have an example here for input binding and going to use a Twitter example. I have my service running and my Dapr sidecar running, and it has a configuration file it interprets when it starts up. In this case it is an input binding for Twitter specifically. As you can see in the configurations file, I give this thing a name, tweet. This is very important. I will show you what this is. Also you see the type, bindings.Twitter, and uses specific meta data. In this case it is some credentials to connect to a Twitter feed. There is also a query in there. That is one of the functionalities of the Twitter binding, you can specify the search term and what a tweet is sent that has this term in the text it will be picked up by the binding. So, once Dapr has read this configuration file, it will connect to Twitter and start looking for tweets with Dapr in the text. Once the tweet is detected, it will pick that up and it will call your service on a certain endpoint. Here you see that the name is really important because this name is used as the endpoint that is being called on your application. In this case, the application is running on endpoint 5000 and have to have one that listens to end tweets and will get the information. You can handle that whichever way you want to do with the tweet, and then you can send to do with the tweet, and then you can send back 200 OK to tell Dapr the message was handled successfully. This is how you can use an input binding. The outbound binding is similar, so I have a service with the Dapr soundcard, and in this case it is a Tulio . Twilio sent great binding. This is what we use in a shop.sending emails when an order has been completed, we can call abounding endpoint and we use the same name as we have used in the configuration for this output binding. We give it a payload, and then the Dapr sidecar will invoke SendGrid and ascend the period over there for the email to be sent. In this way only needs another Dapr API and I consent email from my applications without knowing how SendGrid actually works. I only have to provide some API key or something in meta data so Dapr can connect to the service. That is pretty cool. As I stressed before, Dapr is completely open source. You can write all of these components yourself, and you can also of course) for all kinds of different systems that are not kinds of different systems that are not already out there. You have to do it in GO, but there is a possibly free to build your own input and output bindings for your system or some other service you want to use for your application. So, with that, of course Dapr will send you a to hundred OK when the message has been sent. hundred OK when the message has been sent. With that, we are through with the stuff we wanted to show you. Thank you for listening, thank you for having us, it was great being part of the ritual as your community day. There are some researchers here and something to the eShopOnDapr. If you want to see more of us you can watch our dotnetFlix video channel, and I don't know if there are any questions in the chat, I think we have a little bit of time for questions. I have a question myself. Can we expect to see this in the wild at the moment? Well, we are still in . there is now a release candidate version, and we are working closely with the team, and I think that somewhere around the first quarter next year there will be a 1.0 release that is generally available. So 2021 we can expect to see this out there? So cool, it is such a great and clear explanation of Dapr. Even I understand. So that is usually a good sign! Thank you so much for this amazing session. Thank you for having us. Two yes, it was great. Thanks. That might have a great rest of the conference everyone! Thank you. Awesome. Without further ado let's move onto the next speaker. It is Loxley are you there? Yes, I am there. Two welcome! You are a senior at the University of Washington, you major in Informatics which is pretty cool. You have a talk about Quantum Computing which sounds really awesome and a great subscription of that talk! How did you get that idea for this talk?! I learned about I don't about it two years ago, it was one of the first three things he said would change the world especially in as attack. After that I have been following learning modules, talk to a couple quantum expert and I have created a very good talk for beginners to understand the beginning parts of what Putin is, how does it work and how to connect it to azure tag. I'm very excited and it is a topic I'm very passionate about. That sounds awesome. I cannot wait for you to start, so I'm not going to ask any more questions really. I would love to hear the talk, so the floor is used you are. OK. Welcome so much to my talk , I am so excited to be here. Here I will just quickly dive into our agenda for this talk. We will first cover what Quantum Computing is and dive into how specifically it works, we will then discuss why it matters and also some really important problems that it has the potential to solve. And finally, we will tie this all back into how it specifically relates to Azure and how you can learn to lament it right now. First things first, what is quantum computing? And to understand the fundamentals of Quantum Computing we have to go back to 19 we have to go back to 1935 where this thought excrement was first shared with Albert Einstein. Schrdinger asked what if we locked the cat in a box, specifically with a poison substance that had a 50% chance of releasing or not releasing an hour. Before we opened the box we would have no idea whether the cat were alive or dead. It could be alive and dead at the same time. And so the concept of existing in two different states at the same time is at the core to understanding quantum mechanics. This thought experiment was first applied to the concept of quantum computing 50 years afterwards in the nineteen eighties. nineteen eighties. Quantum Computing is a new compositional paradigms, this means that it runs Tiffany then your phone, your laptop a server or any other piece of technology you currently own. And what sets, Quantum Computing from a classical computer is speed. It has the potential to solve specific problems exponentially faster than a classic computer full stop what takes a a classic computer full stop what takes a classical computer billions of years to solve could be completed in a matter of hours with a quantum computer. So how those quantum computer in work? Let's go back to our Schrdinger's cat example, when you think about a cat being alive and dead that is fundamentally different to how classical computers work. Right now, our computers run on bids which are in strings of ones and zeros, this is binary code. Just how a cat cannot be alive and dead at the same time, a bit can never be a one or a zero at the same time. In addition to being discreet, bits are deterministic. This may . this means that when we create the same compositional app and it will always give us the same outcome. If we took the computer to run 5+5 it would always send back 10. Whether you write it once or a million times it would still, a classical computer would still only return 10. But Quantum Computing in comparison has quantum bits or that are more commonly known as qubits. Unlike a classical computer there can only be a zero or a one, quantum computers can have an infinite or continuous number of possibilities. Unlike a bit. Qubits are what's called probabilistic so if you told it to add 5+5 together it will not always return 10. What makes a quantum computer useful in this way is that if we told it to run 5+5 once, it may return a different number like eight or 11. But if we told it to run 5+5 1,000,000 times it would increasingly get closer to the number 10. This means that the repeated capitation and our confidence and the answer increases. In addition to that, qubits have two superpowers that bits do not have. Here we have a visual example of a bit and a qubit. As we can see, a bit can only be a zero or a wine at any given time. But let's say you have a single qubit. The qubit can be a one, a zero or both at the same time. And what a qubit is both a one and a zero, this is known as superposition. Now, let's say you have two bits and two qubits. But can only be one of these orders at any given time, either 00, 01, 10 or 11. Let's compare that to having two qubits. When combined, they can simultaneously be all of store, in this way we can do it update. This grows exponentially, if you add an extra qubit for this, it doubles. What are the benefits of superposition and entanglement? Exponential number of potential output Exponential number of potential output decreases the time it takes for a program to run, compared to a classical computer that has to run linearly. The following is a great representation of how a classical computer would solve a maze, compared to a quantum computer. With such a speedier runtime, why haven't we started implementing quantum more into our day-to-day lives? There are two reasons for this that we are going to cover. Number one, it is debatable whether we have achieved quantum supremacy. Quantum supremacy can be defined as what a quantum computer can perform a task last -- better than a classical computer. Giving them the same question to solve. Currently, a classical computer can simulated quantum computer and up to certain number of qubits, a quantum computer would have to surpass a classical computer inability because before classical computer inability because before it could be considered as supremacy. As research improves, this will hopefully soon be achieved. Some types of quantum computers can become mainstream in the next two to five years. Next up is running a quantum computer is incredibly expensive. Or in other words, just a very high maintenance. Let me explain. Running a quantum computer can cost over 1000 times what the same they would run to run a -- it would cost to run a classical computer. Dear member in ant man when he had to shrink down into the quantum well which was so unstable that he almost did not make it back out of it alive? This is kind of like how quantum computing works in real life. in real life. When they are incredibly sensitive to that, so that if any interference affected at this scale, it will pretty much disrupt the computer's calculations. And so a couple of things I used to control the environment. Which is temperature and noise. Quantum computers currently need to run at a temperature of nearly -460F, which also equals -200 and Quantum computers currently need to run at a temperature of nearly -460F, which also equals -270C. But that in perspective, it is -455. Maybe the solution is moving all quantum computers to outer space, which probably won't bring costs down. To bring costs down, because you are probably going to keep all quantum computers enough for now, we need to run at room temperature. In addition to being insanely cold, it also used to be insanely silent. Researchers are currently creating environments with the lowest noise levels on record. Just for quantum computers. So the challenge in controlling these environments makes quantum computers more likely to produce in current outputs, so creating quantum computers with air correction awful tolerance is incredibly important. In other words, -- full tolerance. But we will be carrying around quantum laptops in the future. Some of these challenges will take years. It will take some of the industries lightest minds to solve, I think it is still possible. Here is why. Classical computers as we know them today have been around for about 70 years and have seen trillions of dollars of investment. In comparison, quantum computers have only been around for a little more than 20 years. I was able to billion dollars of investment. They have already come in incredibly long way, in the last 20 years full stop as investment continues, so will innovation and they will soon continue to improve. And so what do we know about the current shortcomings about quantum computing, it is interesting to know why it is still important and why it matters. Here are some cases that have the potential to change the world. First up is machine learning, machine learning is based on both sampling and optimisation methods. This means quantum computers can help a lot in this space. Quantum computers can provide more distributed and reliable data than classical computers, and can use optimisation to show how machine learning should be improving over time. Next up is climate change. Next up is climate change. Quantum computing can discover solutions that would officially combat warming. The Redis global warming. -- Global warming. It can make batteries more efficient, and they can be major breakthroughs in can be major breakthroughs in healthcare. Classical computers can take years to process the complexity of medical treatment, but a breakthrough in quantum computing would mean treatment can be optimised and developed exponentially faster. As we have covered, quantum supremacy could be possible, quite literally within the next 10 years. next 10 years. The great thing is, to get prepared for that, the best thing that we could be doing right now is learning with it and experimenting with it so that we are prepared for when it starts transforming these industries. And the great thing is that Microsoft is aware of how important quantum will be ended making it into the issuer ecosystem right now. -- Azure, today you can run a simulation a classical computer, and even the real deal with as you are quantum. Finally, even cooler, Microsoft has created the open source quantum development kit, which includes free libraries, as well as the programming language Q shop. I highly recommend to learn more about this module, to learn more about how you create your first Q shop program. -- You sharp . . If you create a quantum program, I would love to see it, take me on Twitter with what you create. Thanks for coming to my talk and I hope you learn something new today. Thank you.  That was a great explanation actually, that was really awesome. I look at the chat and people were asking questions and then you are answering them already! (Laughs)  That is awesome. I don't know if there is any questions that I did answer that we have time to cover.  Let me see. Something came up. Someone says as a developer, I am worried about quantum computing, since it can decrypt RSA keys very fast. With the formula. What do you think? Would it also be something that could be Would it also be something that could be used for evil, quantum computing?  Gosh, that is something I've done a bit of researching. There is this discussion that if we created quantum computers, we would be able to pretty much break any password that currently exists in a matter of seconds, which a classical computer would take millions if not billions of years to crack. But I have had the ultimate side to that, if we are creating these quantum computers that can crack this pass was, we will be creating quantum solutions that can also make our environments more secure. So on the one hand, it does have the potential for evil, but on the other, we are investing in the technology right now so that when people start using this text for bad, they will be like superheroes there to save the day. As of right now, I am confident. But it will be interesting to see what the next decade brings. I would be to be interested to hear about that.  Yeah.  Yeah. Really interesting stuff. I have one last question. A personal question I want to ask, I looked at your blog, I do so you are working on the seasons of surplus challenge? How did you find the second challenge? Every dinner already?  That is what I am doing well after this talk actually. I was actually writing an article about custom vision about two weeks ago, so I am super prepared to go in and start working on it. I am so excited about working on it. My goal is to have it done by this Friday and I will probably make an announcement this week and once I finish it.  Really cool. We are going to read it. Great talk, also. I really learned something. Thank you so much.  Thank you, have a great rest of your day.  Will come to our next session. It is about how we can use (inaudible) info have -- for that we have an amazing speaker, -- for that we have an amazing speaker, we had some technical setbacks I think a couple of hours ago, but he is here. I think easier. I think easier. Nigel, you hear?  I am here, can you hear me?  We can hear you. It is good for you to be here., I'm glad to be here. You are in Texas? Yes, I am in Austin Texas. I am here.  Excellent.  Excellent. Early no achy F's -- I only know a KFC were the solution for JACK BELL: And I thought could tell us how we were going to do that . for that.  Are you able to see my slides?  Could you share video? We conceal slides, could you share video as well?  The video is on my end. I will turn it off and on again.  That always works.  Thank you so much, the floor is yours.  Thank you so much. Hey, very excited to be here with you, managers -- my name is Nigel. I want to talk to you all about open shift. You might wonder about why a person from a competing cloud might be here to talk to you about a cloud solution and I am excited to talk to you about how we can all work together to get Kubernetes going. The call over -- the call open shift Kubernetes, one of the things we love that development has had to develop quickly because we all know that as we are developing solutions, for our clients, if we are able to do it for our clients, if we are able to do it quickly then the value diminishes. When we are working with tools like Kubernetes, like open shift, you are able to work with any kind of application that you can dream of, and part of the benefit of open shift as well as you can deploy it in different spaces. You can run it on the edge. You can run in your own datacentre in hybrid and multi-cloud environments. If you are worried about switching to a new platform, do not worry, you are not alone, there are a lot of people who are using open shift now to deliver value to their clients. Pretty successfully, as you can see from some of the companies that are listed here. And part of the things that folks run into when they are using Kubernetes is that Kubernetes is often very overwhelming, there is a steep learning curve. Out-of-the-box, it means configuration, different tools that go with it to make it usable for your enterprise environments. What you have read open shift, there is open source things built on things built on top of Kubernetes, as opposed to what Kubernetes is often referred to as an orchestrator or as containers as a server solution. When you are working with Kubernetes and enterprise environments, you are going to do things like logging and monitoring that are not really available out-of-the-box, things like service discovery. One of these problems are solved when you are working with a tool like open shift. Every time there is a Kubernetes release, there is an open shift release that goes with it. What goes into that release. The biggest contributor to the Kubernetes project are Google who created the project and then Red Hat. They are working on a lot of things inside the Kubernetes stack, to make the tool itself better. And then present to you a more opinionated view of open shift with defect and performance fixes, adding more integrations, and the support when your Kubernetes cluster falls over, who do you call? With open shift, if something goes wrong, you have with you the engineering support in the Solution Architect with Red Hat to help you solve whatever problems you ran into. It is a consistent container application platform. That integrates a lot of things that you might look for in Kubernetes that may be difficult to do, or in any of your deployment strategies with cloud native complete the BETH MAVIN: Computing. What you have at the bottom is e -- competing. -- competing. Would you have is Kubernetes on top of that, I like that Kubernetes is the engine that drives a budget. If you are running open shift, you are running Kubernetes. If you are familiar with Kubernetes, all of your command lines, all of those things are valid in open shift. But what you have on top of open shift or what you have on top of Kubernetes a suite of these values at the same time. Feel empowered to be able to control your application security, defender infrastructure and extend your security ecosystem with OpenShift. One of the amazing things that came out of the newer versions of OpenShift is a one click install, fully automated so that if you are running OpenShift yourself, you are not using it on a cloud provider like Azure or IBM cloud you can rest assured that the process for getting it going is straightforward. And updates are pushed to you over the error so you not have to bring your cluster down in order to be able to get the most updated security fixes. And why is Red Hat the best choice? Because they are computing to the code base. They have a lot of customers that are using it successfully, you can use it on whatever cloud you prefer. And it is a comprehensive suite of tools. With that I want to get into hands on lab to show you a bit of what I'm talking about so it is not just me saying that it is a great tool. I am going to be using a lab that got created for you all. I said that OpenShift can run on any cloud and what you are going to see is a lot of things that are referring to IBM cloud, but it is OK. Everything that I am doing for OpenShift is the same across the different cloud versions. So we have a tool on IBM cloud called open Labs which allows you to get a free OpenShift Costa to be able to build and demo applications. We have a few tutorials that are set up for you, this one here when you provision the cluster you have about four hours. you have about four hours. It is not going to take us the whole time, I realise we are running a bit behind, but I am going to show you how to deploy an application on OpenShift, and then show you some of the other features of how it works. So, what we have in this exercise is first talking about the things specific to IBM cloud when you provision a cluster, which isn't necessarily relevant for those of you running it on as a dad. I just want to show you what you get when you open the OpenShift web console. So when you login you get a lot of information you do not get in Kubernetes, it is a much more helpful dashboard that gives you access to a lot of information that you would have to search if you are using Kubernetes. One of the important things I like to point out is that you have two personas, you have an admin view and developer view. So depending on who has access and what people need to do, you are able to direct their access through the access control that you have got with your admin and developer views, as well as the multi-tenancy of your cluster. Let's jump ahead to what it is like to deploy an application. You mentioned before that when you get OpenShift you have a lot of things available to you like the monitoring and being able to do the multi-tenancy, the security, but for those of you who are not already using Kubernetes and do not use containers in production you may be looking for a way that OpenShift might make it easier for you to deploy your applications. This point me to a tool that is developed by red hat called source to image. What that is is basically a set of builder containers that are made in a lot of popular languages, and you can add your own languages if you would like. The documentation is pretty good about that. You can just point OpenShift as a repository of code, and then it will take your code container it and put it in OpenShift and give you a link all in one go. If you are the Miller with Kubernetes you realise it takes a lot more steps than that, and Kubernetes by itself does not have a way to build your container images. And that is something that OpenShift provides. Let's go ahead and get that going. The first thing we want to do is switch over to our developer perspective. When you are ill the developer perspective you not see much of the information we saw before. Our topology right here it shows it is empty because there are no workloads found but we will change that. We are going to want to select creating a project. If you are familiar with Kubernetes, there is a concept of name spaces and projects are named spaces with a few extra features that make them better for use with Kubernetes . or with OpenShift. I'm going to create a project... we will call it examples hell. I will just copy and paste this here. Let's create that project. We are going to give it that same name, example health. And now we see that we are in the context of examples health. OpenShift using OpenShift to deploy OpenShift, it is all Kubernetes all the way down so you will see other name spaces created that make OpenShift run, and then the ones that we define. You will see that the default one is there for you and whatever wonders you create on top of that. The next step that we want to create is that we are going to use the builder images to point our cluster from git repo. You can check out this GitHub repository of the application and see what it is we are deploying, it is a simple application that would be a sort of front end for a health app. It is all generated data, there is no one tablet . there is no one actual medical information in here. We are going to navigate to the repository path. We are going to look at the advanced options, and the actual application we are deploying here is this node sourced image OpenShift master. We will drop it in the context directory so it knows exactly where to look in our repository to build the application. And then, we are using a Node JS builder image and we want to change the application name to patient UI, it will make us that make it easier for us to know that it is the front end of the application later. And in later laps there is the back end as well that you can deploy and it shows you how to manipulative those separate. I would encourage you to check that out if you do want to look at how this application is built, and how to do these examples. And then we are going to hit create at the bottom of the application page. This box is checked, creator up to the application, which will automatically give you the URL to hit whenever you are deploying. So you hit create, and So you hit create, and we get taken to our apology window. This is what it looks like. We see that this build is pending, and we can get some more information about it by clicking on it. What we see is that at first it just tried to pull the image to check if it was in our container registry which it isn't, so it has to build it, so we see that build one is running. It need jump back and make sure that there's nothing else I'm supposed to point out here. So what you see at the top are the pods, it is trying to create the pods for each of your container. So containers at their core are processes that are running in isolation, and with Kubernetes and with OpenShift, we do not talk in terms of containers necessarily, we talk in terms of pods so you can think of pods as a group of one or more containers running in isolation. or more containers running in isolation. If they are running together in the same pod they are going to be some tightly coupled resources. The next thing you see is the bill. We have built one that is already complete, we have services if you are familiar with services it is essentially a way for us to about traffic to our pods. And then we have rocks which is the public URL that we want to hit. What I was going that explanation you could see that our circle here turned blue, and the application itself is running. We can either hit this button here or hit the rat itself to see the application running in production. That was it. We had our Node JS application built, I entered some information about it and pointed the URL to it and whilst I was explaining the outlay of it we see that the application has already built and deployed. This is just a sample login so whatever you enter here will work. So I just put test test & inner, don't say that. And you can see the information here. This is the application. Especially if you have never used something like Kubernetes before, or even having not used containers before, this is just a pretty quick way of being able to take an application that you have already built, get it deployed in a secure cloud native way. The next thing that I would like to point out for you is that there is a CLI that is away available as well so it is not just all using the beautiful UI that you have their that you can totally customise the dashboard and all of that was not there for you to be able to do that you can see what information you would like. But if you are working more from a shell, shelled into the resource the CLI is there for you. If you are familiar with Kubernetes, you might have seen the cube CTL, keep control, however you want to you want to say is, command use and OC is the OpenShift CLI and it extends the ability of cube CTL to be able to address OpenShift specific things. But you can say, if you know the due CTL command you can use OC and it will still work. OK. So... in the lab was typified the process, so you can log into your cluster by clicking here. These are all of the things that are pertinent to my cluster because it knows from what it set up in the background. You will see a success message. Get nodes will tell you about the actual hardware that you are customer is running on, so we have two notes customer is running on, so we have two notes here. And we can see the IP addresses of the nodes, see what starters they are in, how long they have been spinning, what version they are running and everything we need to know there. If you want to see all of the services, all of the deployments and pods currently running on your cluster you can hit this button here. OC get services deployments pods on them spaces. We talked about name spaces in projects, these are projects which are basically name these are projects which are basically name spaces with a few extra spaces and in full stop since our terminals go to cluttered we can clear it up there. I mentioned before that OpenShift using that uses OpenShift to the points of commerce all the resilience you get either for your applications as well in case something crashes or fails, so that the entire component does not cause the entire cluster to crash. You can do the same thing with OpenShift. You can see this example health we created just a moment ago for top if you want to target that project we just say OC project example health. And it tells us that we are now using that project, so we are in that context. We can also get the rout that are associated with the project, this is that URL I was single for that I hit to show the actual application I was running. In the next exercise we want to look at logs and events, and so we need to create some work for our cluster to be doing. We are going to add another terminal here, and we are going to send it and infinite loop that is just going to be querying information so that we can actually see work being done on the cluster. We will simulate a load this way. Whilst that output is sticking we know our cluster is getting hit. The next thing we want to do is have a look at the logs. Out-of-the-box this is what is available in OpenShift, and you can add in different operators, different software suites to extend that even more if you would like. So, in hours apology, we are going to look at our pods and then look at our logs, and it also was the low over time. If we jump back to our apology, look at our pods and we are going to see that... pods and we are going to see that... and we will see each time our infinite loop hits the endpoint, we get a little ticket here on our blog that tells us we get it for information. The other thing we want to look at the events tab which tells us chronologically what has happened over the course of our application deploying so you always have some source of truth of how this whole thing has been going on. We can see that... the pod was successfully assigned, and then it pulled the image, started the container and then started the container again, we must have changed something or something changed so its point of another container for, no sorry, we got the container for, no sorry, we got the application running and then got the service and modular components that make it a lot easier to using I'm going to be able to demonstrate some of that today.This is useful for identifying the timeline for events. When tracking the state of a new rollout, managing assets or something similar like exposing a rout, and identifying the timeline of activity. Let's look at what metrics and dashboards are available as well. For this we have to switch back to admin view. Our admin context. Then we are going to look at our dashboards. If you are familiar with this, . We are looking at the wrong. Let's look at the computing resources in the example health one. This is overtime what happened, we initially created the part here about five minutes ago. We started hitting the infinitely about four minutes ago and we see that it is answering all of these requests. Our memory usage jumped up and then pretty much everything is cached. We do not have to worry about that. But you get as much information as you like and all of this is being fed into this, if you're more comfortable looking at if you like that, you can look here and looking with your OpenShift credentials. And then you will see the more traditional output and you can make your own dashboards if that is something you would like to do, maybe you have your own that you would want to bring in for Kubernetes. We can look at the same thing here. The maker look at our example health namespace. And we get the same information. All of that is out-of-the-box, with OpenShift, that you would have to do some configuration and monitoring. To be able to deal with Kubernetes. Everything is just kind of there for you. Also, shipping with it is alert manager, she can use Prometheus to set up alerts, set up queries, customer alerts, for anything that you would like to do, but the anything that you would like to do, but the really interesting thing that I like with OpenShift is how intuitive it is to scale applications depending on mode. We can imagine that we are looking at elasticity for Kubernetes, if we have a lot of traffic coming in, we want to see in our application up and when the traffic dies down, we want to be able to scale it down, so we are looking at the ability to do that. Our horizontal scale that we have an open shift makes everyone straightforward. We are going to dig a little bit here but it We are going to dig a little bit here but it is OK, there is stuff you can copy and paste so there is less of a concern of messing up, from an admin perspective we have to look at our employment. Which managers our application, entertainment -- it manages what is out in the world. We are going to edit that deployment. It was in the workloads, deployments. We are going to look at our deployment here. And we are going to take the action to edit our deployment. And then we get to this tab, it is colour-coded, if you make a mistake, don't worry. We will look down as well in the editor, so you can see the resources. Want to set some researchers -- we want to set some resource limits. Indentation. Indentation. Line 46. We are going to replace that with that block of text and save it. Error, I made a mistake. Oh, this line came out. My bad. That should be at that level. Save it. OK, reload. So, if you make a mistake, it catches it and tells you something is wrong just take a look at it and make she did not make the same mistake I did and we should be good. We saved and reloaded that and we can look at our events to verify that our replication controller has been changed in our application control is what is responsible for us sending out new copies or just copies of our pods. And if we look at our events, we will be able to see that that has been updated. Let's go to our events. Creating the container. Deployment. So, it scaled down our replica set, it deleted what was running, stop the container and. Before I did that, it brought the new ones. Let's go further back in time. Yeah, so we have got our new replica set coming up. Deleting the pod, scaling up to one. Deleting the pod, scaling up to one. That is creating that new one a minute ago. Which of the new deployments and made a new pot that follows those rules. Once that pod was healthy , we started scaling down the other one. Once we successfully had the image in the container, we scale down the old one. I used that process of the application never going down as we updated. Then what we are going to see is that when we have the resource limit in place, we will set up an autoscaling. an autoscaling. We are going to look at our workload deployment, and look at what the deployment says, how many pods there are, we are going to set up a horizontal pod autoscaling, let's get to that point and will come back. We want to look at our deployment. And then we will look at our horizontal pod. In the deployment, if I click this button, it will give me a second pod. But we don't want to be sitting here watching that, if the application seems it is overloaded, let's add a bunch more pores. But we can have this to add for us. We have this pod scaler, don't worry, we have this created for you. We are going to replace everything. Take note of the values we are setting for the minimum replicas, and the target utilisation for CPU. It allows us to see some scaling going into effect. We are not seeing a lot of traffic, so want to set the bar low so we are actually seeing what we are intending to see. I am going to jump back over to here. Create that horizontal pod autoscaling. And we are going to watch it work. Yet, it will take a little while. But if we look at our deployments page, we will see that there is one running now and we will let this take over from a little bit -- take -- tick. And then we will make sure that is still running. Still running. So after a little while we will see that go up. While we are waiting for this to take place, do we have any questions? Folks have about this whole thing? About how it has gone? Are there any questions about OpenShift or Kubernetes in general that I might be able to answer?  We don't.  We don't? OK.  Was not perhaps -- (inaudible)  Sorry, I can't hear. , Is your microphone on right? We are having some sound issues here. But there are no specific questions at this time,. this time,.  OK.  Just checking the sound here.  Alright. No worries.  I just flipped back over to the page and you can see now that it is running, if you caught it, and it automatically scaled up. I know it is saying there is even more load that the four can't handle, now it is scaling up to eight, we set a max of 10, so it won't go past 10, but if we look back at our dashboard, we will see more pods coming up. will see more pods coming up. In our namespace, to handle the load and the limits that we set. More and more are spinning up that are replicating the same application, to spread out the load and Kubernetes is managing out the load and Kubernetes is managing between the pods, that is configurable. Making sure that each pod is handling a good amount of traffic for what it can do. And we are all the way up to the max, 10 pods. Essentially, what we wanted to cover was give Essentially, what we wanted to cover was give you an introduction of being able to run Kubernetes with OpenShift. Kubernetes being this big monster that can do so many things, but a lot of times it can be overwhelming and sometimes we are looking for a more platform solution to be able to handle application employments. -- Deployments and that is where OpenShift can coming to you if that is what you are looking for. 