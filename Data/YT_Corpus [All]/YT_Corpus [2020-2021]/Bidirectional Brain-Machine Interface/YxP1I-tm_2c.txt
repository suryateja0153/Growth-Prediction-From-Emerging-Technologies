 [Music] hello I am here to talk about the natural state of computers and even though it ends up being something like this most of the time I'm going to go into a you know a bit more about how things work before it gets to this point so I am amber Hockey Brown my Twitter account is at hockey owl I'm from Melbourne Australia so it's been quite a long trip to get here look when you live in Melbourne it's hard to get picture of Melbourne so I had to make do it's from when I went to play Cascades yeah the same thing good copy most people know me for my work on twisted so twisted is a asynchronous networking framework which will be relevant to my talk but it's been around for quite a long time and I am its release manager and one of its longest standing contributors so this is sort of a bookend to a talk given nearly five years ago at this point at a conference called Django under the hood of which the deep dives day is sort of inspired by God I look you and yeah still do I got carded the other day anyway so this was given in 2015 and around that time we had Jango channels was nearing 1.0 was 1.0 it was in development so the early version of Durango channels was more oriented around keeping Jango sort of synchronous as it was but having a sort of asynchronous shell around it so that you could do asynchronous things without having to care so much about your actual Jango being natively asynchronous so I've got a couple of slides from that talk and at that point I was I was fairly convinced that the original channels would kind of work out because it seemed pretty injuries laughing in the front row that would work out because it seemed relatively low effort low impact and if I know anything from software that doing too much at one time usually ends up hit failure but I sort of was thinking about a sort of an alternate situation from what I was assuming was the foregone conclusion and that conclusion was that we needed to replace whiskey because whiskey is at its core as a synchronous II style interface you don't have an asynchronous event loop or anything around it and you generally run in a thread and it just didn't work for that sort of asynchronous future you could do responses asynchronously but it didn't work for asynchronous protocols as whole like WebSockets now that has come to fruition which is SB 3.0 it's stable it's well-defined and it's got multiple servers so I think we're in a world where that whiskey two sort of does exist I also talked about how maybe in this world where we have an asynchronous whiskey that's Jango might be able to spawn both asynchronous and synchronous views and that could be pretty interesting and I was considering at that point that maybe a native asynchronous method would be the long term way forward but it would would be rather invasive and later on Andrew will be talking about how how that project has been less invasive and not required hopefully any of our connectors at all maybe some fried ones but no broken ones so think of this talk as a book into that 2015 talk because 2015 we didn't have what we have now now we have in development a synchronous capable Jango SB is developed and stable and we've multiple implementations of ascii clients and ascii servers but let's start from the start and talk about why asynchronous stuff is a big deal now when I say async through this talk I mean async ris input and output it refers to specific programming techniques that minimize the amount of time that you code blocks waiting on external data walking can be quite bad when responsiveness is key such as in graphical interfaces blocking on waiting on external data or blocking on heavy processing can turn your application into a stubby slow mess usually the solution is to move any sort of processing into an alternate thread leaving your main UI thread to be very snappy very responsive and just delegating anything that will take any significant amount of time to worker threads or processes unfortunately in Python it isn't that easy we simply can't make a thread on put on another course who doubled the amount of work we can do because a voice called the global interpreter lock the global interpreter lock is both in a central part of Python and makes it very easy to program in Python and have some threads occasionally without having to learn entirely about how to write for it safe code but is also its Achilles heel it allows Python code to execute on only one thread in that process at any one time we don't have to worry about one bit of Python changing a list or a dictionary from underneath us which can happen with asynchronous things in other languages usually you have to have a lock and make sure that things you know all know things underneath it are changing in python we don't have to do that the gill is a very rough flock as the global in the name like hint it means that we can simply say that this code over here doesn't need the guarantees it provides or then it safely locks because if we were to turn it off code that we depend on and call would likely break horribly because when you're writing Python you're not only writing your own code but often losing lots and lots of other libraries that maybe don't have those sort of I'm okay with being multi-threaded and a lot of Python code is not multi-thread safe so for now we're stuck with it so if we can effectively only run Python on one thread at any given time how can we best make the use of that well if we only have one thread and do all the work on that we don't have to pay you all the costs of threading like thread switching and locking and memory overhead but if we want to block when waiting for external data we'd only be able to request to process one request or one thing at once but instead we can use non blocking techniques for our and process as many requests as we have the CPU time for so the solution to blocking and only having one thread is rather than using rather than waiting for the data to arrive we use the operating systems non-blocking i/o functionality it's not instant since we have to shove the data off to the operating system but it blocks as little as possible when we want to receive data we just have to check for it instead of stopping the world and waiting for everything to arrive at once async isn't new and because asynchronous i/o underpins basically every high-performance networking system there is operating system support is nearly Universal our particular are the e pol and KQ AP is on the smbs DS and io completion ports on Windows and it's not you in Python Eva because twisted and tornado had been using these interfaces for many many years and G events and similar things like that being alternate so almost as long twisted and tornado are quite similar and built on the concept of an event loop or a reactor which we'll get into or to vent was a bit different and used a sort of green light rather than an explicit event loop green lights are similar to threads and have a lot of the same downsides but a managed in user space instead by the operating system which gives your program more control all these options are very mature and in wide use by sections of the community that need them unfortunately it is a little bit hard to do asynchronous code in Python due to its lack of first-class language support back in a you know the Python 2 days you could implement Co routines which make it you can make it a lot easier you could implement it using generators but they're slow and give messy trace back since generators work on exceptions to finish finish them and they're very easy to miss use one of my co-workers has a post-it note on his monitor that says have you remembered to yield since when using these old sort of hack ting co-routines forgetting to yield on something python had no way of telling you we didn't know that you were supposed to do that fortunately everything's a bit better now we have real co-routines real ish and you can write them using async death and then inside that instead of yielding use a weight to the user the flow looks like you're blocking or waiting for the thing you will wait it to finish the await keyword actually gives control back to back to python and the asynchronous system meaning that you're not actually blocking your code just seems like it is so it's much easier to follow if you're you know used to regular synchronous techniques it's much easier to understand the your callback method of doing it and it's a very familiar sort of API of course there's new kids on the block as well Aysen KO was designed as a common kernel for all the asynchronous systems in python but has kind of become a thing on it of its own frameworks like trio turn the conventional systems on his head using the new native cur teens to provide a bit of a different API for your non-blocking i/o but I'm here to talk about Django as well Django 3.0 will come with native asynchronous support for quite a few of its API is meaning that asynchronous IO is going to be something that innumerable Python developers will now get to take advantage of on a day to day basis this is really unspeakably huge as it brings async truly into the Python main screen the effect of async I own twisted is going to be relatively small compared to frameworks such as Django adopting these systems but if twisted is been around for nearly 20 years and async IO has been around for five well I said only now that something like Django is adopting its ideas the benefits of using asynchronous IO especially in web development have been clear for a long time but we're sort of approaching a tipping point it seems now that async is no longer something that you can just opt out of supporting after no js' came onto the scene the time sort of started turning no Jess Ennis early days was very similar to Python at the time where had little language support a native language support for doing anything asynchronous things it was bound by single thread limits but despite that people were able to lots of new and exciting things with it we all know now that async and JavaScript is more or less as good as you can get in an interpretive language interpreter problem funnily enough it was actually JavaScript that adopted twisted side ia's first what we know is promises a-plus actually started out as a port of twisted stafford to javascript which was then subsequently iterated on through different frameworks and standardized to the form we know today now upon seeing sort of how good async could be everyone sort of wanted in on them wanted this native support languages that didn't have a good story at the time like Python ended up losing users too once it did like go so first-class support for asynchronous code seems to be a must last last month the Russ long are coming async/await support landed and I'm very much looking forward to see where that goes but what is this require for a nice Inc story driven by is it just people you know wanting this fancy new feature is there some technical reason why it's so effective I think it's that computers are not getting faster we're having to be more efficient with what we have and for a lot of networking based workloads which which a lot of web development sort of goes focuses on especially when you're talking to web clients and databases and all that much do your time is not actually in Python but in networking and when you're doing that asynchronous i/o is far more efficient than that of a traditional blocking regime so you might think computers are getting faster very much so why would we buy these new ones if they weren't getting faster this is the history of you know mid-range Intel CPUs for the past ten years and as you can see that the passport numbers are going up faster it's close right but benchmarks don't tell the whole story those jumps in performance are because those CPUs added more cores if we divide that performance by the number of cores and the clock speed we can see that there hasn't been a significant increase on the whole since the core i5 660 launched in 2009 in fact performance has even dipped a bit for the past couple generations why is this Specter and meltdown has sort of ruined everything they round back nearly every relative performance improvement that Intel CPUs have had since they're called core series's inception and even the tenth generation Intel CPUs doodle launcher Christmas won't really improve at that much all these improvements that we've been getting are based on things in the insecure and we're now having to roll them back of course we have improved technology to the point where there is a point to new CPUs they use less power and that means that even if our CPUs are the same performance clock for clock we can still run them at a higher clock speed to a limit add more cores and stay within the same power envelope that's a massive boon for consumers as even a low-power CPU can turbo reached to four gigahertz and higher for short times but it doesn't really help those of us that deploy to servers or do enough work that the frequency boosts eventually have to stop because you're causing too much heat and I don't think that this is just the past ten years I don't think that the next 10 years is going to get as fast to computers either arm is looming on the fries and ready to take over the server market but that's because of price and power efficiency not speed you can fit more theoretical performance per square foot with an arm cluster but each one of those individual CPUs is slower than an equivalent x86 core and might well always be other contenders like risk 5 are promising but yet to be proven the first few letters in risk stand for restricted instruction set in contrast to the complex instruction set of sisk processes like x86 Intel's early solutions to performance problems was to introduce instructions that did more per single instruction therefore getting more performance out of less instructions per clock cycle that you get in an sis regime this let them dominate the computing space for the next few decades but the complexity of sisk processors now makes the relative simplicity of platforms like most quite attractive not for performance or really for security but for the ability for a human to maybe understand what the hell is going on inside of it it's unfortunate though that we are ultimately hitting the limitations of our control of physics the smaller our transistors the harder they are to make the lower the yields and the more difficult the next iteration we have taken every easy option available in the hardware so it's time to start taking the harder ones so what are the kind of processors and platforms we as Python developers or developers at large will have to operate on in the future we can look at the present for what the future holds this is the rice and 5 3600 the CPU that's in my desktop at home it's actually picture it's a consumer CPU but it's very similar to the high-end and server processors that AMD are releasing just use they're using the same core architecture and even the same silicon just arranged differently on on larger chips this cpu with six cores here costs 200 American dollars well the epic 77042 you might find in a new render farm server cost seven and a half thousand US dollars but that has 64 cores and 128 threads they have a lot of products in between but it's worth looking at what we'll do with those ones in between and it's likely that you'll find things similar to this or working on the same architecture in render farms and general-purpose servers over the coming months but again it's not about any one CPU or any any one series of CPUs it's a small scale replica of where the industry's going multiple and more CPUs and cause on a single processor and all of the interestingness step that implies intel has announced a similar topology in their future service EP use having multiple chips inside the same processor and it is logically similar to the existing multi CPU boards you can find its servers now if we look under the hood this CPU has an i/o die which is on the left and a chip late-- which has six cores on it you can see underneath the text there's a bit of a room for a second one and that sort of is the core of this architecture is that you have the ayodhya and then you can add couplets to get the number of course you want on each one of those quarter Chipola dies there are two core complexes you can see them vertically each core complex or CCX has 16 megabytes of l3 cache each and 512 kilobytes of l2 cache for each of its four cores since the phase 600 only has six cores two of the calls on this die are fused off and inoperative MD has built their entire CPU line server and consumer around these eight core giblets and use the ones to have faulty cores but otherwise work in lower spec products similar to the Athlon X 3s that were about 20 years ago now they were four core chips that had one faulty core so they sold it off and made a three quarter now they're using these eight core triplets in everything from their consumer CPUs to their server CPUs in something like the epic 77042 I mentioned before it would have a single of those iodized and eight of those fully operational triplets to provide the 64 cores as we can see with this little diagram the core complex has a chunk of l3 cache and - cause they're associated l2 cache mirrored on each side so you can see the two cores Neal - in the l3 and it's the same on the other side the Yoda is sort of marketed by Intel as controlling what they call the Infinity fabric which is basically just a marketing term to refer to the high-speed connection between the i/o die and the chip lots which scales up depending on how many triplets you have on this single triplet CPU doesn't really have a lot of benefits but it's important to think about architectural now the ayodhya is responsible for connecting to your RAM as well as being what physically hosts the PCI Express Lanes it also has some high-speed USB and that sort of thing that's the responsibility of this die and not that chipler cause the chip tlit just has CPU well this IOT has the other peripheral things that you would expect a CPU to host some of those PCI lanes then go to the motherboard chipset which then hosts things like SATA and more USB ports and all of these parts have to communicate and synchronously and it's a hard requirement that they have to asynchronous communication between your motherboard in your CPU is core to allowing different parts of a computer to get faster without requiring the others to act in lockstep nowadays everything on your computer will have a different clock speed PCI Express has a different clock speed than your CPU and your AM this wasn't always the case as the old frontside bus design of of 90s and early 2000s computers and a frontside bus clock speed and then a CPU clock speed multiplier your frontside bus clock speed often had to be the same as your RAM clock speed which would introduce a bottleneck especially since your frontside bus clock speed on some systems would also dictate the clock speed that your PCI devices had to operate so this one clock speed not only made how fast your PCI was in your RAM was but also how fast your CPU was one computer support all of these having different clocks and to have different clocks you need to run them and communicate synchronously in fact the frontside bus on computers no longer exists and the CPU will often have its own memory controller and will interface with things like PCI Express and etc directly on the newer CPU architectures like the Zen 2 of the Rison 3600 even the clock speed of the Infinity fabric interconnect between the i/o die and the chip le'ts can be independent of the ayodhya and the giblets meaning that you can overclock those giblets without overclocking the i/o die or vice versa who remembers these I don't a little bit now I was an Amiga kid anyway these two keyboards are - the the real real-world ones operated on a interrupts system when they had something to report like a mouse movement or a key press they would send a CPU interrupt to tell the CPU that something had happened it kind of is what it sounds like it interrupts the entire CPU she gets message through this as you can guess kind of sucks if you're trying to do other things with your CPU so we eventually replaced it with asynchronous systems such as USB USB is instead a polling based interface where it's up to the host computer to query think from a device to make things a lot simpler as well for certain devices like mice and keyboards the instant response of interrupts was advantageous but with modern gaming mice pulling it like a thousand Hertz this is no longer really a problem and it means that when we have multiple cores we're not shutting down the world to tell you that your mouse moved polling doesn't quite work with everything though especially higher performance platforms things like PCI Express use a bi-directional asynchronous communication system which is always comparable to something like F&N you send packets of data down to the BC ie device and it sends packets of data back like it like it were speaking ethany getting these messages doesn't halt the CPU and we've liked these newer things for the I die it's actually quite far away from CPU cause the PCI data is then funneled over the interconnect to the CPUs which again is running can run at a different clock speed than both the CPU and the PCI Express bus this gets relatively important when you look at for example the new PCI Express v4 which is even higher speed because if you were limited by how fast your CPU was we wouldn't be able to get these improvements in hardware without having to have entirely new generations of CPUs that are much faster because we can't get much faster with the CPUs we have to you know untie them of course the ayodhya being central and separate to the CPU cores sidesteps a significant problem and there's Numa Numa stands for non-uniform memory access and describes an architecture where you have multiple CPUs that don't all share the same memory you can have a large common memory bank but that can increase licensee as the memory gets physically further away from the CPUs on the motherboard and in the chip and more CPUs are using a limited common bus instead when you have lots and lots and lots of cores a Numa system will have some CPUs having local memory which is very close in high performance and an ability to ask other CPUs to pass through memory that it controls now the previous generation of AMD's high-performance systems implemented Numa and I was a real brain scrambler for traditional software you had close memory in file memory and a lot of software that we arrived doesn't interact well with that in Python there's no way of saying no no no keep things over here and avoid calling out over there and this was the case for games and that sort of thing this got better because Windows and Linux and that decided decided to keep things in the memory that they were dealing with but early on it would just put them anywhere and you'd end up paying these Numa costs without any real benefit now that's not the case with these newer CPUs but if you have multiple CPU sockets per machine you still have those Numa characteristics because each CPU will have its own memory banks even if all the cores share the same memory bank on its CPU so what does this leave us with we have small cores which is great everyone loves more cores we can access more memory especially on your soccer machines because of improvements over the past 10 years and high-powered interconnects means that we have more bandwidth to everything such as PC or RAM or whatever or between different chips but these performance improvements have an architectural cost we've got lower single core performance especially in alternative platforms like arm we've increased latency because things like the i/o die are simply more things between us and things for accessing and even if it allows us to access more at a higher bandwidth we're still gonna have to wait longer and it adds a bunch more complexity and every level especially when you factor in things cache coherency so let's look at one of those things let's see because that factors internetworking it's important when you think about asynchronous i/o but it's also important to recognize what affects latency has on the different things we might to do might want to do on the on local to the computer so 3.6 gigahertz CPU like my one does 3.6 billion cycles per second each shaking 0.7 nanoseconds each now the l3 cache on that chip takes about 40 clock cycles to communicate with which is about 11 nanoseconds the ddr4 to 666 which is fairly decent Ram mm-hm take 55 clocks a clock cycles to access and that's pretty good as well now the fastest SSD you can buy and we're talking tens of thousands of dollars here we'll take 220,000 clock cycles or 60,000 and a seconds so it's quite a big jump from cache and RAM to SSD now my local router to my local ISP takes about 12 milliseconds or 12 million nanoseconds it's about 45 million clock cycles after we do Australia to Los Angeles which Melbourne to Los Angeles which was the flight I took for a person that took 14 hours for a packet it takes three hundred and fifteen milliseconds and I can tell you which one I would prefer but that's like 1.2 billion clock cycles since you know getting quite large think of us like the Katia's like your desk you grab the paper the DDF is the filing cabinet maybe in the next room depending on the speed of your RAM the SSD is you know an in-state parcel coming in and sending something to Los Angeles's quite literally just flying to Pluto it's it's like most of a human lifetime in comparison so how do we make the best of this kind of latency well when I said that the rise in 3600 had six cause dos 12 threads and that means that supports simultaneous multi-threading now simultaneous multi-threading is the practice of having two virtual or two or more virtual calls / real call with the assumption that one of those virtual cores will be waiting on RAM or disk to do anything meaning that the other can actually use the processing parts of the CPU all the other one is waiting it's not something you can really control but it's worth knowing that some workloads operate better with it and some don't in the context of Python because we're accessing things like the RAM a lot and they did I disc a lot it is likely that they will operate on the whole with a performance improvement you can get about 150 percent of the performance of a single core with this because you're simply slotting things in when they're ready and utilizing that actual processing part of the CPU more effectively it's also important to maintain cache coherency with maaan CPUs having upwards of 16 megabytes of l3 cache per set of cores keeping that case valid and closed is extremely important by staying on the same core or the same core complex it will stay coherent and fresh and close and that means that with such a large amount of case you can store significant parts of executables in your cache without having to load it from disk again things like sending CPU affinity keeping processes local to their cache therefore can become extremely important for getting the most out of those CPUs so the best thing to do in this sort of scheme is having every available working core have only one thread running on it staying as active as possible but sometimes waiting is unavoidable those fetches from Ram or disks will block your application no matter what you do and it's not like we can avoid fetching from Ram or avoid fetching from a disk sometimes there's things that we have to do but the point of asynchronous i/o is avoiding it where you can and networking is a great place where you can avoid it pre-emptive multitasking can be a bit of a mine killer here as well as the operating system will see that potentially you're waiting on some RAM or CPU or hell even networking and will attempt to go oh well you don't need a CPU so I'll put something else there settings CP affinity means that you won't get moved to other cores but the offering system my put something there that overwrites all your precious cases while you're waiting for disc read so there's things you can do in your operating system to avoid that but basically just reducing the number of things that can be scheduled there's having lighter weight servers that run just what they need to is also important but again waiting is unavoidable factors from around will block you no matter what you can do but the point is sitting not sitting around for longer than we need to and how do we do that by using asynchronous networking techniques like I've been going on about we can avoid the blocking to be just those things that we can't avoid but keeping the networking which again takes many to many many many times longer making that not take not having that not block a loss level we want non blocking sockets these will not block when we try and do things with them like reading and running will instead rely on us checking on their readiness state before we try and do it if the socket has no data then it'll Razer would block error telling us to come back later as well as if we turn right too much data to it and the operating systems write buffers become full it'll tell no I can't accept any more data without blocking so we have the problem where we can't always talk to these sockets so how do we figure out when we can now select is the standard UNIX API for working with these non-blocking sockets there's improvements to the same basic formula in KQ any Paul which are the things that you you know actually use but the main uses the main core of it is the same you give it the sockets you want to know if you can use and it'll tell you which ones can be read from written written to more or have errors such as being unexpectedly closed we've this basic API we can then implement what's called an event loop the parts of our application that then use these non-blocking sockets registro on this event loop and the event loop will then notify them when the sockets already turning this more into sort of our hey tell me when this is ready okay here's the event which is data the olaf analog is leaving in microwave to do it state until it beeps rather than standing there and waiting for yourself I mean standing there is simpler you you can you don't have to go anywhere you can just look at it you don't do anything you don't worry about the microwave beeping more in your middle of some other task but you are being inefficient with your time being natively async lets us work within the greater asynchronous system there's a computer and avoid wasting time you're taking advantage of its natural state a synchronicity totally what let's say see how I'm doing Python got a drink first well this async is making me thirsty so let's look at conventional grab and I'm sorry but this is floss because I couldn't fit a Django wipe on one screen I love you but you have a lot of code but this hopefully you should get through my point so we have the flask app and we have a routes and we have main and we do request don't get to fetch something from say localhost which is why I've been using from my benchmarks I've got a local web server running that's nginx and then we return the content from now how does this look in in asynchronous web app so client is sort of a implementation of flask like API but using twisted and asynchronous things so as you can see the code is very similar we have track which is like requests but twisted what credit naming and instead of just calling it we have to await it because it does some sort of networking now I'm using deff async and await there which is syntactic sugar for callbacks if we were to look at the raw third callback in way of doing it this is what would look like it's you know not much more complex here but we've larger code bases it does make a lot of sense and using callback system with like loops can be quite difficult so async to F in a way makes things quite easy now the cool part of a code in Python is that you don't end up returning something like a promise or a deferred it's usually some sort of shell that says hey in Python I kind of have to return something but the values not here yet so here's a box that will have the value at some other date now in a co-routine you when you will wait it you actually await that promise or 2/3 which will then suspend the co-routine add a callback or such onto the refer return 2/3 or future and then resume it when that treats so all of that call backing sort of happens invisibly to you now can't see them here some kind of okay so this is a chart of our concurrency so on the left is requests handle per second and on the bottom is the number of concurrent requests I'm throwing it so at once so this is on my right since a 600 so it's quite beefy so I don't expect to run into any problems where I'm hitting CPU lib that's quite yet so as you can see being asynchronous actually is not doing well it's actually slower and why is that the case now when we have low latency such as communicating with local host for this benchmark we block for less time and working on a local machine like this effectively has zero latency meaning that the threaded solution will not actually block much more than the asynchronous one and because there's not much walking difference between them the extra computation handling the event loop and registering you know the sockets and all that sort of thing means that we actually spend more CPU time serving each request in the federal model which bottlenecks us but what happens if we changed at zero milliseconds Lansing introduced 350 milliseconds latency well things end up being quite different I actually had to add a second grunick on there with 12 threads and 12 workers instead of just a normal four so in this case the single threaded twister application handles many many more requests per second now the reason for this is fretful exhaustion because when you're running a flasks a pink unicorn you have a limited number of threads that you can run it in and when you hit that thread limit where all of them are waiting for that lesson see you can't serve any more requests you have to wait for a thread to finish and then you can put new requests in while twisted and upper asynchronous systems don't really care and can just keep adding adding them to the event loop so you end up with doesn't show up on here so I need to remember what charts I'm looking at so the 95th percentile licensee you can see here that the licensee goes far UPS and the 95th percentile is like the the worst five percent of requests is that guna corn will get really bad when you give it more requests and it starts exhausting it twisted will happen not far behind but we've twisted on pi pi which is a just-in-time into a compiler interpreter for Python which lowers the CPU load that it sort of levels out of it because at that point we're running out of CPU and that's what's causing twisted Leslie to spike so we can see the concurrency limits here with unicorn and other similar thread of systems the hard concurrency women is a number of possible threads you can run with async it's a bit different because it's your single core performance once you start having more than one second of work per second your event loop is no longer reactive and it falls behind and licensees go up so we can add more threads that always works but we can't always add more sync or performance so that sort of puts a hard limit on us and the Python gel means no apply for multi-threading so there's not really that many ways to make that better because we can't just start up a second event loop in the same thread and handle but there are methods such as shining now you are limited by your single core performance but there are tricks to make it so that you can have more processors which have their own single core performance limiting them so multiple processes accepting incoming connections on the same socket works very well so that just round robins between the multiple services arguing the requests which means that then effectively you double your performance you can do this with some linux api ins that will let you bind twice on the same socket or you can have something like a char a proxy on the front end that then directs the between multiple servers you can also have a single process doing these accepts and then delegating it to sub processes and because those sub processes are in different processes the duel doesn't affect them this works much better with not with this lengthy thing which way you do no work but say you need to do a little bit of work and you hang your cpu performance limit based on that by delegating you can level it out but at the end you're going to have a limit where you can't serve more ross TCP connections you're going to hit that limit eventually which is what we are seeing there where it was like several hundred and this is a desktop computer and we've faster servers you might be able to handle thousands of requests a second before having to run into that now these sort of things are workarounds and they don't solve every problem but they do sort of turn some synchronous problems such as processing into asynchronous problems if you have a subprocess worker pool or a distributed worker cluster those things that you might otherwise do in your application you can just put somewhere else and this works very well if you have different kinds of workloads so say your application is something it does some data forecasting and they send some JSON and you get that JSON and then you put it in the you decode it and go yep this is a valid request encoded again we don't know what on a worker send it off the characteristics of your application that you're writing and off the worker don't have to be the same your worker could be running on something like pi pi that's extremely fast and consider like a deserialize into your lies json very quickly while your workers might be running on very beefy servers with lots and lots and lots of running cpython we have numpy and C extensions and maybe GPUs so by adopting this sort of thing for any significant amount of work you need to do that's heavy processing you sort of allow yourself some level of scaling but now I have a distributed system you say I've got a worker pool and all of and that's things that you kind of have to care about and it's too bad you already have one threads are a distributed system and when we start up more threads we kind of don't realize some of the implications that that means if we're running something on the thread it's like okay we don't have to worry about the worker going away but it is there are potentials where a thread can jam where a thread can trigger an out of memory exception that sort of thing so when you run into these limits where you have you know distributed system problems with threads everything just falls down in a heap well if you do it properly you have things like retries and locks but I've been talking a lot about performance and for me that's one of the main benefits I in my day job work on making networking systems that can scale to thousands and thousands of concurrent users not everyone has this problem but it does being asynchronous does give you some benefits apart from just being able to serve more users at once now I'm gonna say something controversial and say that server-side rendering is effectively dead and if it's not it's at least half way to planning for Fodor's we instead shipping data to the client not rendered HTML if we are shipping rendered HTML that's not that much often this is over things like by like WebSockets which is bi-directional or with server sent events leveraging longer-lived connections that the synchronous request response cycle does not feel well with being asynchronous means that we can not only communicate asynchronously with the things we're getting data from the things with any data - and when you look at things like Mobile's that can be extremely important because when you have a mobile setting up one TCP connection over a 4G network takes several hundred milliseconds if your web application requires setting up a TCP connection every time you open the page to load the HTML that's going to be slower than if you had client-side rendering and a concurrent connection that just sent the data over WebSockets things like KGB to sort of hack around this by letting you send you know blobs of HTML over one connection like sending a lots of pages over one connection but there are certain situations where sending the small potentially smaller data over the wire to those clients is potentially better and we're not dealing with big data big data isn't really a thing we're dealing with lots and lots of small data very often sure when you know it's added up it becomes a big data but those problems are usually not our problems and they're usually in the domain of the data warehouses the people that care about like handling those big data that because we're handling with the small data we have to deal with being able to get it to where it becomes big data and doing things results that come out and doing that in an effective way so being asynchronous we can send lots of little bits of data to various different systems and do that without you know blocking the web request message passing systems are great and simple to scale relatively because talking to other systems such as sending signals or sending messages to other parts of your distributed system or writing to the database for example for statistics because it no longer strictly blocks the response we don't have to worry about making it technically slower this gives a lot of potential for moving things weird usually do in a request to a message queue for example to be picked up by an independently scaling system that's maybe more formal it also means that when we're doing requests we can be like oh we need to update this counter that's no longer something that strictly means that you're going to make your requests slower because you can update that counter and serve the users request at the same time so Django it's coming soon thanks to the work of a very determined individuals in this room looking at one but there are many others that are working on making this reality yay it's something that really excites me is someone that is not being a Django developer for quite a while because it means that now I can use Django for these applications that I would otherwise have to go and use twisted or a sink eye or something for and you know all of these things aren't quite in there yet but handling things like WebSockets and simultaneous database queries and highlight and see things like triggering Web books is now something that I can just be like okay I'm gonna write this in Django I'm gonna have the nice things that Django gives me I'm gonna have Django migrations I'm gonna have tango middleware I'm gonna have all this stuff that I like and not have to sacrifice for it of course this new sort of asynchronous Django is not built on whiskey although it is backwards compatible with whiskey but for the native async stuff you need to use ASCII Yuva corn or Daphne are two such servers that will serve ASCII hopefully I'm getting support in twisted soon's that you you know you can just set something up very simply and this is kind of exciting because you've a corn and Daphne are Python so we're no longer worrying about something like nginx controlling our web servers it's Python serving - serving HTTP is just Python all the way down which means that us as Python developers we now control the whole stack we get to do really neat things at the HTTP layer without going well that happens in nginx and we can't improve that we just have to wait for in the next fix well we know we want to support htv-3 cool let's get some typing develops around and make that happen it sort of gives our sort of brings the destiny for that sort of thing into our own hands instead of worrying about what everyone else is doing and when you have got a server you don't really have to just limit yourself to ASCII and nothing else because you're natively asynchronous you can run other asynchronous things inside that process and have them all kind of work well together most useful things for debugging I've ever used is twisted twisted college manhole allows you to SSH or telnet into your running Python process and get an asynchronous rifle like you're a Teenage Mutant Ninja Turtle debugging Mac hassle it's great and you shouldn't do it in production but I do sometimes you can also implement protocols like DNS and run that as a novel service inside the same process having your async Jango app serve the web interface and hosts the database now that seems so like oh cool you can do DNS but when you look at the success of something like PI hole which is ad blocker that you can run on your Raspberry Pi or other things that blocks DNS dynamically it actually kind of seems a bit more interesting because you could now do that sort of thing in Python when IOT means that you have all these little things around with all these protocols it means that being a native village a synchronous allows you to do a bit more and the sky's the limit here you're no longer constrained by water wizz gap can or can't do cuz now you can do anything that you're synchronous Python can you can do asynchronous serial to an Arduino or web to the world DNS or serving SMTP or even XMPP you don't only have to be able to talk them you can now serve them from what is your Django app and this is kind of important for Python in the sort of IOT space because when you have your Django app you might want to talk to say ZigBee and you can now sort of do that a little bit better because you're not worrying about the thread that goes away you can have your persistent communication and your app and then just have it all in one process maybe not the best solution but hey getting something working is better than something it doesn't work so what kind of progress are we at so DEP triple zero 9 has been accepted which talks about the implementation of Django async the S key support has this pheromone oil landed yep Django 3.0 will ship with async fuse and icing no No when did this change Oh God okay just ignore that Gengo three-point-something django 3.0 on my heart just use Django develop it's fine okay so ignore the numbers here I wrote this a month ago before this change so a Django will ship with a sing for you to wear hey Gengo after that point will ship with an async capable RM not an async native ORM and potentially a sink Templar you and the future is sort of upgrading the rest of the parts of can go to sporting like asynchronously sending emails so it doesn't block your requests and having a natively asynchronous ORM that interacts in that event loop natively you rather than putting it in a furball I want it now if you want now with Django you maybe can just go Sandra I'm going hey okay honey async for me I want spacing and then you know which I'm sure will be happening at the sprints yes so at the sprints if you would like to get a taste of it in Django going away Andrew and help him love God love of God burn so but if you want it now and you want to play with asynchronous systems there's a couple of options so twisted is the one that I'm biased towards obviously so there's client which is works too based API and twist it as track which is a request space API and takes photographs which gives you natively a single post gross for a sink a oh there's library slang a HTTP in a sink PG they let you do Postgres trio which is very interesting has H Levin which is a HP 1.1 implementation and Sri o PG which is async Postgres and is something to perhaps look into and play around with just to sort of see what's possible even if you don't you know username anger and at this website this website my web ring I've got some links to all of this various thing various parts of things for you to look at such as depth zero zero zero nine and various announcements as well as background posts from me and glyphs and others about asynchronous systems in general now it's been lovely being here and I'm loving you know being in States again sort of as much as I can and this is my first django con and it's been wonderful and thank you all for having me and I hope you all have a wonderful day and that you all go see Andrews talk where you see about how this is happening in Django in reality and that's all thank you very much [Applause] [Music] 