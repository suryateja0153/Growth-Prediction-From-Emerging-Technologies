   [MUSIC]   [NARRATOR] Welcome to Unite Now, where we bring Unity to you wherever you are. [JEFF] Hi, everyone.  My name's Jeff. I am one of the Project Managers here at AI Unity. Today, I'd like to introduce two of my colleagues, Chris Goy and Willis Kennedy, who will be talking about using ML-Agents and Unity Game Simulation for Playtesting. Today, we'll be providing a hands-on session using deep reinforcement learning and large-scale simulations to test game mechanics through three Unity products: Unity ML-Agents, Unity Inference Engine, and then Unity Game Simulation. We'll demonstrate all of this using the Silver Kart Microgame. and we'll provide a QR Code so you can follow along. After this session, you'll understand how these three Unity products can be used together for testing game mechanics. You'll also get hands-on experience implementing ML-Agents with a sample project, as well as training a neural network model that can be used as a player bot. You'll also see how to embed this neural network model back into the Unity project using the Unity Inference Engine. And lastly, implement the Unity Game Simulation SDK and set up experiments using the dashboard. So what is the ML-Agents Toolkit? It's an open-source toolkit for training and deploying intelligent agents using the latest in deep learning. It's one of the top open-source projects for AI and Deep Reinforcement Learning. It contains everything you need to get started to create intelligent agents in Unity games. What is the Inference Engine? This allows you to run any ML-Agents neural network model on the CPU or GPU of any Unity-supported platform. It's high performance, it's platform agnostic, and it's fully integrated with ML-Agents. What is Unity Game Simulation? Unity Game Simulation is a fully managed, on-demand Cloud simulation solution that allows game developers and designers to run large-scale simulations of games. It includes an SDK, which allow you to configure any game variables and instrument metrics, and a dashboard to configure and run experiments and, of course, download results. As I mentioned, we have everything here that you need to get started and additional information, and with that, I'll turn it over to our presenters. Thank you. [CHRIS] Hello, everyone. My name is Chris Goy. I'm a Senior Software Engineer on the ML-Agents team. Today, I'm going to be walking you through how ML-Agents was integrated into Kart Microgame. The Kart Microgame is a simple game that's in the Asset Store where you can easily modify this to your own liking. So let's take a quick look at how it works. I'll click Play here. As you see, it's a simple racing game. You can move forward or you can decelerate, move backwards and turn left or right. Okay. So now that you've seen just a preview of what this Kart Microgame is, let's talk a little bit about ML-Agents. ML-Agents is a software package that allows you to create emergent behaviors in your game. and what that means is we can do something like teach the driver of this kart to drive around basically any track that we build. So let's go through how that works. First, you're going to have to install the ML-Agents package through the Unity Package Manager. Here, you can see the ML-Agents version 1.0.2. I have installed that already on my machine here, so we'll just get going with a scene where we're going to train our agents. So I'm going to open the KartClassic_TrainingDemo. In this Scene, we see four different tracks. This allows us to train at least four different karts. So we have the very simple oval-shaped track, and then they get more interesting as they go to the right. Let's take a look at one of these Prefab Agents in this Scene. So if we look at the Inspector,  we can see a few components that are directly from the ML-Agents package. Here we have Behavior Parameters, the Decision Requester, and the Agent Script. So let's go through and talk about each property of these scripts. We'll start with the Behavior Parameters. The Behavior Parameters are the properties that define what behaviors our Agents will do. We have our Name, we have our Vector Observation Space Size, and what Vector Observations are things that we're going to send to our neural network or training process in order to give it information about the world from the perspective of the Agent. In this case, we're going to have a Space Size of 11, and that means we're going to send 11 float values to our neural network or training process. We're going to Stack our observations, and that means that we're going to send the current set of observations plus the last two to the neural network. And this gives the Agent, a temporal view of the world, so it can make an action based on the last three observations, including the current one. The Action Space that we're going to have is discreet. That means that we're going to be taking very distinct actions as of moving left, right, up, or down as opposed to continuous actions where you might rotate at an angle and go at a certain velocity. In this case, we have a Branch Size of 2, which one is for steering and the other one is for acceleration. The Branch Size for steering is 3 because you can go left, right, or straight. The Branch Size for acceleration is 2 because you can accelerate forward or backwards. Here, we have a pre-training neural network model, but we're going to train our own, so we'll get to that later. For the Inference Device, this is the device that the neural network will compute... we'll execute the neural network on, so we want to do it on CPU since our Vector Space Size is 11, that's relatively small, and sending out that little bit of data to the GPU to compute on the GPU actually will take longer than just computing it on the CPU. The Behavior Type tells ML-Agents how you want this behavior to work. You can pick Heuristic Only, Inference Only, or Default. Default means that if there is a training process running that we are connected to when you start your game, then we will be in Training Mode. If the training process is not there, we will go and execute in Inference mode, which means we'll try to use a neural network if one exists. If a neural network does not exist in your game, we go into Heuristic mode, and what Heuristic mode means is you can write code in order to control your Agent, to send actions to your Agent. We're not going to worry about the Team ID. You can find information about that on our GitHub repo. And Use Child Sensors means that when we start the game, the Agent will look for Child Sensor components underneath the root GameObject and use those as well as the ones attached to the GameObject. Since we don't have any Child Sensors in any of our Transform Hierarchy, we won't need this to be checked. So that's the gist of what the Behavior Parameter component is. Next, we'll move on to the Decision Requester. The Decision Requester is a component that allows you to set the frequency at which an Agent can make or request decisions. For this demo, we have the Decision Period set to 1. That means that the Agent will request a decision for every frame. Since this is a racing game, it makes sense that you would want to have the Agent request a decision every frame, since they are moving at a relatively high speed. Some games, you can make this frequency lower. For instance, you can set it to make a decision every 5 frames, and that would work for some games. For games that need to make quick decisions, though, you might want to make the Decision Period as low as possible, which is 1. That's the only purpose of the Decision Requester. Otherwise, you can script your own. You can write code to basically have your Agent make decisions when it's appropriate for your Agent. This is just an ease of use for our users. Finally, we have the Kart Agent component. You can see right off the bat there is a Mode property, and we have it set to Training right now since what we're going to do is actually go and try to train these objects or train these Agents. We have the Observation Parameter, so we have a Raycast Distance set on this component. These Agents use Raycasts to tell how far away or how close they are to the walls of the track. And if we zoom in a little bit, you can see that this track is not flat, it actually has walls. The Mask here allows the Raycasts to hit whatever is checked. So you have Default layer, Ground, Environment, and Track. Those are the things we're looking to hit. If the Raycasts hit any of those, then there's code in the Agent that we'll look at in a little bit ... that handles the situation when these Raycasts hit those things. Next, we'll see the Sensors array. The Sensors array is an array of Transforms from which the Raycasts will be fired from. So if you can imagine... If we zoom in on this Prefab here, we can see that these Transforms are set all around the Agent. So what you're going to see when we start training is that there's a fan of Raycasts being set when the Agent wants to make a decision. And so this, like I said, allows the... It sends observations to a neural network or the training process that tell the Agent how close it is to a wall. The next part we see is this Checkpoints which has a Collider array. So these Colliders are all along the track, and what you'll see is that these Colliders allow the Agent to know which direction it needs to go, as it's going around the track. These are very helpful in training because if these are set up on your track, you can add the Agent train on any track. So with that, let's go on to the next property. This is the Agent Sensor Transform, which is just the parent Transform for all the Sensors. The Checkpoint Mask is, again, another Physics mask, a layer mask that tells the Raycasts what they're allowed to hit. So, in this case, they only want to hit the TrainingCheckpoints. So now we'll move on to the Rewards. We have a Hit Penalty of -1. This means that when the Raycast reaches its hit threshold, if we are 0.2 units away from the wall, that means we're going to get a Hit Penalty of -1. And giving a negative Reward to a training process means that that's a bad behavior, and you want to avoid doing that in the future. You'll see that the next Reward is the Past Checkpoint Reward. This is a positive Reward for the Agent passing through a checkpoint. The Agent also gets a tiny Reward for moving towards the next checkpoint in the right direction. And the Agent also gets a Speed Reward per frame. So that's for the Rewards. We also have these Inference Reset Parameters. These are used for when the Agent moves off of the track, and we need to reset the kart. Then, finally, we have some Debug Options, which allow us to draw Raycasts when the Agents are requesting decisions. Now that we've gone over the components in the Editor, let's move on to what the code looks like. Okay, the first method we're going to look at is the CollectObservations method. This method is overwritten from the Agent class in the ML-Agents package. The first line in this method we see is an addObservation call with a kart.LocalSpeed. If you remember, in the component of the Kart Agent, we saw there's a Reward associated with the speed, which we'll see later on, but it's important to note that we want the neural network to be able to associate the Reward with this speed, and so that's why we're adding it as an observation. Next, we want to see what the next checkpoint is that we're heading towards. So, you can see we have an array of Colliders that we're looking at. We keep track of the checkpoint index, and we use that in order to find out what direction the kart is heading, if it's heading towards the next checkpoint. We also add this as an observation, as a dot product between the velocity vector normalized with the direction that the kart is heading relative to the next checkpoint. This also is associated with a word that we saw in the Kart Agent component. Here we have a ShowRaycasts logic where we draw Debug lines, if we have that property checkbox set to True. Next, we have a loop where we go over the Sensors array. What we do here is we actually send a Physics Raycast from the Agent's position in the direction of what the forward vector of the Transform was in the array. What we get back is a hit info. Based on what is in that hit info, we check the distance, based on the Raycast's distance. What we're doing here is we add a default RaycastDistance that we're sending the Ray, so the Ray is on a set length. We're dividing that to get a ratio, and we also add that as an observation to the neural network. If the hitDistance to anything is below the current Sensor through HitThreshold, that means we're too close to an obstacle, and we want to penalize the behavior. So we give it a penalty, and we want to end the episode to tell the training process that we've done something bad, and we want to start over. Once we get out of this loop, we add the accumulatedReward and the EndEpisode, which will make our Agents' OnEpisodeBegin() function be called where we reset our Agent at a random checkpoint. So let's take a look at that real quick. So here is our OnEpisodeBegin() method. This method is called whenever an episode is ended. As you could see from our previous CollectObservations method, when we collide or get too close to a wall, we end our episode. You can see here that if we're in Training Mode, what we do is get a random checkpoint, and we set up our Agent to start from that checkpoint. That's basically the simple OnEpisodeBegin() for this Agent class. The next method we're going to look at is the OnActionReceived method. This method passes in a float array that we're going to fill up with actions. Note, this method sends in a float array of actions that we use to control our Agent. Here, you can see that we have this InterpretDiscreetActions. Let's take a look at what this method does. Here we can see we set our steering based on the 0 index of the action, which means it's going to be either -1, 0, or 1, since we had resized three in this branch. And the acceleration, which you can see is either 1 or 0, or forward or backward. These are used in the Kart demo code in order to control the kart. So there's this Generate Input method that's called. It's abstracted away nicely here, so it allows it to work well with ML-Agents in this case. Okay, so the last function we want to look at is the Heuristic function. The Heuristic function also passes in a float array of actions, but in this case, we're going to write to this array instead of read from it. So, if you look closely, you can see that the Interpret actions actually read from the Action array, and in the Heuristic function, we are writing to this array. Because what's going to happen is we're going to take the input from the user and use that as our actions for our Agent. This is one of the strengths of the Heuristic method in that you can use this to control your Agents however you want in your game. If you don't want to use a neural network or training, at the beginning, when you're making your game, you can use the Heuristic function in order to control your Agent and make sure everything works before you start the training process. Okay, so now that we've looked at the code of the Kart Agent, we're going to install ML-Agents through the command line, so we can train our Agent. Now we're going to install ML-Agents through the command line, and we need to have Python installed for this. So here you can see I have Python 3 installed on my Mac. I made a typo there so I'm going to check my versions. So I have Python 3.7.3 installed. As you can see here in the ML-Agent's Using Virtual Environment Guide, we have tested ML-Agents with Python 3.6 and 3.7. We do not support 3.8 at this time. Since I already have Pip installed, I'm going to skip this part of the instructions, and I'm going to move on to the macOS setup for Virtual Environments. So I have a ~/python-envs folder already, and I'm going to create a new virtual environment. So I'm going to do "python3 -m venv~/python-envs/" and I'm going to call this "mlkart-demo" Doing that will create a virtual environment with that name "mlkart-demo." And then, once it's done, I will source that, so I will then be able to install Pip packages in that virtual Environment without stomping on any other Pip packages I may have installed on my machine. Now I'm going to do "source ~/python-envs/mlkart-demo/bin/activate" So here we are in our Python Virtual Environment, and now I'm going to do pip3 install --upgrade pip Now just to make sure I'm using the right Pip, I'm going to type "which pip3" And this is Pip 3 from my virtual environment, so that's good. Now I'm going to do "pip3 install --upgrade setuptools" Okay, great. So now we're done setting up our virtual environment, and now it's on to install ML-Agents. As you can see here, I just need "pip3 install mlagents" Actually, I've made a mistake. So, if you remember, we are using Release 2 which has Version 1.0.2 of the ML-Agents package, So we have to install the 0.16.1 version of ML-Agents. So the way you can do that Pip is like this. It's very important that you install the matching C# and Python package versions, so just to double-check, let's go back into Unity, and we'll go back to the Package Manager window. Yes, we're using ML-Agents 1.0.2. So, yes, we installed Python 0.16.1 package. Now that we have the ML-Agents Pip package installed, we can check that it works by doing "mlagents-learn -h" for help. Okay, so it looks like we're good and set up to go. Next, what we're going to do  is actually start training. So, I'm going to do "mlagents-learn" and now we have to specify where the config file is, and if we look in our Project... under Karting Prefabs Add, you can see there is this kart_mg_trainer.config, so that's what we're going to look for. So ./Assets/Karting/Prefabs/ AI/kart_mg_trainer_config.yaml So this is the yaml file that is used to set up the neural network. There is a lot of documentation on all of these properties for this config file that I'm not going into right now. But, basically, we're just going to type this in, hit Enter. And now we're going to hit Play. Oops, okay. It looks like I've done this before, so I'm just going to "run-id" and I'm going to do "kart-demo-01" So now it's telling me to press the Play button in the Unity Editor. So I press Play. So let's check our command prompt. It looks like we're hooked up in training. So we're going to go back to the Scene view, so we can get a higher level view of what's going on here. So I'll have to zoom out of it. And here we can see that this kart is trying to make its way around the track, and when it runs into the wall, it gets reset to one of the checkpoints, which is the code I showed you in OnEpisodeBegin() So we're going to let this train for a while, and then we'll come back and check in on it to see how it's doing. We've been training for about 15 minutes now, and as you see, the Agent has learned pretty well how to go around the track. So, at this point, I'm going to stop our training session by hitting the Play button in the Editor again. I'm going to look  at our command prompt. So here, you can see we have an ArcadeDriver.nn file. Let's see if we can find that models/kart-demo-01/ArcadeDriver.nn Okay, there it is. So let's copy this file into our Unity Project. Here is our KartDriver.nn file. And what I'm going to do... is I am going to put this into the Prefabs folder for now. Now you can see we have this ArcadeDriver.nn file. What we'll do is in this Scene, we're going to search for the "KartAgent" type and see that the four Agents that are on the tracks have shown up. We're going to open up the Behavior Parameters component, and we're going to select the new ArcadeDriver for this neural network. We're going to save the Scene, clear our search, then I'm going to hit Play. Oh! First, actually, I need to turn the KartAgents onto Inference mode here. So Save, and now we're going to hit Play, and we're going to see what it looks like. Okay, we have our Agent driving on the track. This is only 15 minutes of training. We can train it for longer to get a better result. But this looks pretty good for 15 minutes. So now, in the next portion of this video, my colleague Willis Kennedy is going to talk to you about how you can measure metrics in order to better balance your game with Game Simulation, using this pre-trained Agent from ML-Agents. Thank you. [WILLIS] Hello.  My name is Willis Kennedy, and I'm a software engineer on the Game Simulations Team. Today, I'm here to talk about how you can use existing or newly created player AI in your game to achieve better balance and to improve your player progression. So, in particular, we're going to be looking at the Kart Template project as a Unity sample project. We're also going to be building on the work from Creating a Game with Learning AI in Unity. This was a recently released video that focused on using the ML-Agents package to train an automatic kart racer who could race around different types of tracks and perform pretty similarly to a player. This automatic kart racer  is called KartClassic_MLAgent, and we have imported it into our main Hierarchy. You can see we have a red and a yellow cart. The yellow cart is the automatic one, and I'm going to go ahead and play the game so you get a little bit of an idea of how this is working. The yellow kart is racing around, able to avoid the walls and go through the track. In this track, you can see we have three different purple checkpoints. These three different purple checkpoints are required to go through before you finish the race. When thinking about the balance of a game like this, in essence, going around the track once is the key success criteria. We want to know how long does it take a player or a bot to complete this track. In evaluating this key metric of lap time, we can then look at how different parameters affect lap time, and possibly even create different play styles for different karts that we might want to add. You can think, in particular, of Top Speed and Acceleration as parameters that obviously affect how fast the lap is completed. If you increase the Top Speed of a kart, whether it's a bot or a player, they're going to be able to complete the track faster because they go faster. If you increase the Acceleration, then they can make the turns more quickly, they don't have to slow down as much, and they can achieve that Top Speed more quickly. So both should have a positive effect on completing the track faster. But as game developers, we probably have a certain idea for how long we want a given track to take. Perhaps this very simple one, we want to take around 30 seconds. So we can create an experiment where we try a lot of different values for Top Speed and Acceleration and target a 30-second completion time. Maybe, even, we'll be able to find different combinations, like a fast Acceleration and a low Top Speed that completes the track in 30 seconds, along with a high Top Speed and a low Acceleration that does a similar performance. This trade-off creates different levels of playstyles that we can then incorporate into different pre-built karts for players to enjoy. Now, you can imagine, finding those specific values that would work, like it could take quite a bit of work. We may try Top Speed from like 1 to 50, and we may try Acceleration from 1 to 20, and then we'll have to try combinations of each option. So there's a lot of possibilities very quickly, not to mention that our player AI or players are going to perform differently on average each time they play the game. We want to see, regardless of random effects in the game, what's the average lap time. So we may have to run certain instances, like a Top Speed of 10 and Acceleration of 5, multiple times to see what the true average is. Now you're talking about a lot of different tests and trials. Where game stimulation comes in is we can help you deploy your build to the Cloud and then run it with different remotely configurable options for parameters in your game, parameters like Top Speed and Acceleration, and we can do that as many times as you need. So, if you need to run a thousand different combinations, then it, all of a sudden, becomes quite simple. Then we serve you back your metrics as downloadable CSV files so you can parse your data and get an idea of what happened. So how do you actually  use Game Simulations? First thing to do is locate our documentation. We have our docs on docs.unity3d.com, and I wanted to take note that the package isn't currently publicly available because it's still in an earlier preview. But you can just take this line and add it into your manifest.json. I've already pulled up the manifest.json for this project, and I've already added the line. But if you just add it here, then all of a sudden, you'll have the Games Simulations package pretty much integrated. Now, when you come back to the Unity Editor, if you click on Window Game Simulation, then this is a new window that helps you manage your game simulations and your remotely configured parameters and do your build uploads to our Cloud. You can see that I already have two parameters defined here: topSpeed and acceleration. I can add more parameters. I can take these away. These have default values. Now, how do I translate these remotely configured parameters into actually making changes within my game code? Well, I like to make them as close to what I'm trying to change as possible. In the KartClassic_MLAgent, you can see that we have this Base Stats struct in the actual ArcadeKart object, and we can change these, like we really want to overwrite Top Speed and Acceleration. When we go into the code, if we open up ArcadeKart, then you can see we have the struct of Stats, like I was saying, we have Top Speed and Acceleration, and we need to overwrite these values with our remotely configured ones. In order to do that, I've already imported the GameSimManager, which is the core class you'll be needing for these operations. And then this FetchConfig call will obtain a configuration payload and then feed it into a callback method that you define, and in this case, I've named it OnConfigFetched. This callback method just returns you the GameSimConfigResponse, which then you can look into, peek into, get ints out of, get different types out of. In our case, we really care about the ints, we want to take that Acceleration and Top Speed that I showed you from the window earlier and overwrite our base values. So we've overwritten our base values, we can save it, and now we can move on. Now that we've overwritten our base values, we need to actually define what metrics we want to be saving from each run. So we'd already talked about using the lap completion time as one of our key metrics. In order to get that lap completion time, we need to think about when do we know that information. When the game finishes, right? So what object is managing our GameState? In this case, we have a GameManager object with a GameFlowManager script that's managing how our game updates, how it transitions to that Win screen, and so on. You can see a bunch of those parameters to find in the code, and then some of our classic Unity methods, Start. We're really looking for Update because they have the End condition defined in the Update method: AreAllObjectivesCompleted or is the time expired, the maximum time? And then they have an EndGame method, and this EndGame method,  it makes a sound, a victory sound, it changes to another Scene that says that you won. We don't really need that specific information. We just want to be able to save how long the race took. So I wrote another method: EndSimulatedGame. If you just replace EndGame with EndSimulatedGame, then this will help us have defined just the metrics we need and make it pretty simple  to implement. So we still are taking in whether  the player or bot that was racing won, like completed the race. This is because there could be a bunch of bug situations where maybe the player has hopped through a wall or done something unexpected while you've been developing the game. Just having this Win in there is a good marker for whether something might have gone wrong. Or if it was just too slow to even possibly win. Now, the other counter that we've been talking about is the raceTime. We have, of course, saved that as our realtimeSinceStartup, and now we'll be able to know how different permutations of Top Speed and Acceleration performed. Then, the final thing we have is pretty much just a Quit method. This will totally exit our game when the race has been completed. We want something simple like this with a strong Application.Quit() because the game's going to be automatically running. If we don't have an Application.Quit(), then it will just keep on running forever, and we really don't want that. It doesn't give us any useful metrics, and it just costs money for nothing. So now we've implemented our key metrics, and we can go back  to our Unity window. It gets the updates, and now we can race again and see that there have been  some changes, in particular, the remotely configured values have now been loaded into the yellow kart. You can see pretty quickly it's going a lot faster. The Top Speed is twice as high as it was, and, predictably, the bot is crushing. [LAUGHS] Clearly, it already has had that impact. But this hasn't really helped us at all be able to run the full range of parameters that we wanted. So when we go back to the Game Simulation window, all we've done is define defaults this far. In order to define the ranges of different options we want to try, we need to go to our Web UI to set up a true experiment. Before we go to our Web UI, we need to upload our Build to the Game Simulation Cloud. You can name it whatever, "KartingMicrogame" Then you'll just click this Build and Upload button, and it will go ahead and upload the game to us. But I'm not going to click it right now because it can take a little while to do it, and I've already done an upload prior to this. Instead, I'm just going to hit that Create Simulation button, which takes me directly to the UI. Now, you can create your simulation. Pretty normal. We can make it Test-Kart2. This is the Build that I was talking about. We already have a Build that's ready to go. Maybe I want to try different topSpeed values. We can just enter in several different ones, and we want to try different accelerations. This is the randomness factor that I was talking about before, the Runs per Parameter Combination. Given a certain set of parameters, maybe your bot's random, maybe you have enemy AI that are random, maybe you have scenario effects that can randomly occur, this can balance out those random elements and give you an average over how many runs you want to do. Say you want to do ten runs or something else, you can set it to whatever feels right. Then the Max Runtime per Minutes helps solve that problem, where if you didn't have an application quit, maybe there was some bug, maybe some path happens so that the game couldn't quit, then this will exit the game, regardless of what you'd set up, and thus limit your cost. I'd suggest setting this at a little higher than your game locally takes to run, because sometimes in the Cloud, the runtime may be different. Then, all you'll need to do is click Run and the simulation will start on our Cloud, and you'll be able to receive updates better. But if I go back to the All Simulations page, up in the upper right, then you can see I've already run several different combinations of these simulations, and I can click on one of them, and you see it has all the parameters that I defined, the total number of runs and so on. Then I can click Reports, and I have a couple of different options here. Player Logs is just for your own debugging, if something is weird in the data you weren't expecting. Raw Data ignores this number of runs. So you'll see one instance for every run. So you'd see ten instances of Top Speed 10, Acceleration 5. Then Aggregate Data does that randomness aggregation that I was talking about. So you see the average value over ten runs for each combination. We can download the Aggregate Data. It comes in this tsv. This is a Notepad, so it's not as easy to see it in. I recommend importing it into a tool like Google Sheets or Excel. I have this other tool that I've imported some past data into. In this other tool, you can see I've already filtered down to raceTime, and I've sorted by the average lap completion time. We were looking for around 30 seconds. You can see there are a couple of different options that are pretty close to 30 seconds, Top Speed 40, Top Speed 20. But you do see that this Top Speed 40 has a pretty high max and a high standard deviation. It makes me feel like probably something a little funky happened. Maybe it glitched through a wall, maybe something else happened. 40 is a pretty high Top Speed. We already saw that 20 completed very quickly. So there's a lot of room for error in that high of a Top Speed. But, regardless, we can say Top Speed 20, Acceleration 4 was a good target. That can be our basic kart. You can imagine how this experimentation for a more complex game can get way more challenging. In particular, think of a game like Mario Kart . You have pretty similar parameters, your speed, your acceleration, a couple of other things like your weight and your traction, and how well drifting works, certain elements like that, and then each kart has these combined or has these parameters. So you have a lot of options. And you even have special options, often special abilities play our differently on different karts. With this many options, it's pretty quick to imagine that you might have hundreds of thousands of different combinations. Or if you're accounting for randomness too or different tracks, you might be looking even at millions of combinations that you want to test out and measure the performance of. Once you look at these numbers, manual testing no longer really is a suitable fit. When you have a game this big, it really helps if you can get a baseline idea of your performance, using a tool like Game Simulations, where you have player AI either play against each other, or just play on their own and race on the track. These wide range of options, you can get these basic values and find a bunch of error cases and fine-tune so that a lot of different kart types are all well balanced. We think that this actually helps manual testing because then your manual testers can focus much more on how fun the game is or how player progression works, or a lot more of these human elements that you're just not going to be able to do by running the game a ton of times. This can give a lot of time back to your game testers and your game developers to make your game much better in the end and much more stable. Currently, Game Simulation is a beta product, we're always looking for additional people to sign up and try it out, and these resources will be available in the YouTube link, so please check them out. Once again, I'm Willis Kennedy, and thanks so much for watching my session.   [MUSIC]   