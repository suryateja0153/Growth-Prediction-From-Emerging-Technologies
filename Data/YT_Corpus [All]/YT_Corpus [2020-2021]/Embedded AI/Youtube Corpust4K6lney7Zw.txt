 [Applause] all right welcome it's great to see everyone here really excited about this opportunity as you know our AI accelerator has officially kicked off all of your teams are ready to go and we wanted this to be an opportunity to sort of as a team come together and develop some common foundations some common technological foundation some common language for talking about these very challenging AI problems and so with that I'll hand it over to Vijay all right we'll kick off with the first lecture which basically provides some overview AI context for this you know again welcome to the class we're really looking forward to this what we're gonna present this morning is really a lot of overview material right many of you here are you know know a lot in AI and machine learning this is really meant to just sort a level set before we start the program before we start these classes so you can see this generic title artificial intelligence and machine learning and we're gonna try and cover all of that in about an hour so some details might be skipped but what we'll try and hit some of the the salient features all of these slides are available for you to use so if you're presenting back to your own teams please feel free to pull from these slides you know we've actually gone through over over some time putting you know a good set of survey and overview slides together so if any of these are useful to you you know just email us or we'll make them available to you you're more than welcome to use any and all of these slides if you're trying to kind of present this back to other people so with that let's begin so we're gonna do a quick overview of artificial intelligence again a lot of level setting going on here we're gonna do a quick deep dive these aren't the deepest of dives again given the amount of time that we have but just talk very quickly about supervised unsupervised and reinforcement learning and then summarize and we can certainly stop for questions philosophical debates cetera towards the end we'll try not to get a lot of the philosophical debates on camera if we can all right so first question what is artificial intelligence and this is a question that probably a lot of you get and I certainly have received this from a number of people and that actually takes a lot of it took us a lot of time to come up with this and so we are very fortunate to have professor Winston spend some time with us out at Lincoln Laboratory and we actually brainstorm for you know could however to really trying to come up with what is a a good definition for what we call artificial intelligence and what we sort of came up with is that there are two aspects to artificial intelligence first that we should not that we should not confuse with each other one is the concept of narrow AI and another is a concept of general AI and sometimes in conversation we tend to conflate or mix the two so narrow AI according to our definition is the theory and development of computer systems that perform tasks that augment for human intelligence such as perceiving classifying learning abstracting reasoning and or acting certainly in a lot of the programs that we work in were very focused on narrow AI and not necessarily the more general AI which we define as full autonomy so that's a very high-level definition of what we mean by AI now many of you in the crowd of probably saying well he has been around for a while people have been talking about this for 50 60 plus years why now what is so special about it now why why is this a conversation piece now well from what we've seen it really is the convergence of three different communities that have come together the first is the community on big data the second is a community on computing and in a lot of computing technologies and finally a lot of research and results in machine learning algorithms the other one I forgot to put is dollar signs down here people have basically figured out how to make money off of selling advertisements labeling cat pictures etc so that's maybe the hidden why why now in particular but these are the three large technical areas that have evolved over the past decade or so to really make AI something we discuss a lot today so when we talk about AI there are a number of different pieces which make up an AI system and you know we love the algorithms people but there's a lot more going on outside of that so we've spent a significant amount of effort just trying to figure out what goes into an AI system and this is what we call a canonical architecture very much in line with Lincoln Laboratory thinking we like to think of an end-to-end pipeline what are the various components and what are the interconnections between these various components so within our AI canonical architecture shown here we go all the way from sensors to the end user or missions and a lot of the projects that you all are working on are going to go all the way from here to there a lot of our class however for the next few weeks is going to focus on sort of step one where a lot of people get stuck so we take data that comes in through either structured or unstructured sources these are typically passed into some sort of a data conditioning or data curation step this data is through that process typically converted into some form of information that that information is then passed into a series of algorithms maybe one or many algorithms there are lots of them there is life beyond neural networks once we pass them through the algorithms these typically form this information is converted into knowledge is typically then passed into some sort of a module that interacts with the end user or a human or the mission and that's what we call a human machine teaming step and that finally that knowledge with the human complement is becomes insight that can be that can then be used to execute the mission that the AI system was created for all of these components sit on the bedrock of modern computing many different technologies that make up modern computing and the system that we're using today has combination of some of these computing Hardware elements and certainly within the context of a lot of the projects that we are interested in all of this also needs to be wrapped in a layer that we call robust AI which consists of explainable artificial intelligence metrics and bias assessment verification validation security and policy ethics safety and training we'll talk very briefly about each of these pieces in detail in a little bit as I mentioned AI has an extremely rich history this is just a very Lincoln and MIT specific view of the history of artificial intelligence but certainly there's been great work since the the folks of Minsky Clark Dineen Oliver Selfridge etc since the 50s we've seen a lot of work in the 80's and 90's and certainly recently there's been again a resurgence of AI in our parlance and our thinking of the way AI works so without going into too much detail about each of these eras and why the winters came about etc what yeah I think John launch period DARPA actually put it very well when he talked about different waves of AI technologies that have come about and when he talks about he talks about the three waves of AI or the four waves of AI and the first wave which you can kind of think of as the first decade right of AI technology resulted in a lot of reasoning based systems which were which were based on handcrafted knowledge so an example of an output of this would be an expert system right so a lot of work in that so these if we kind of take the the four dimensions that John launched burries suggests of the ability of the system to perceive learn abstract and reason these are typically pretty good at reasoning because they encoded human knowledge right so a human expert sat down and said sat down and said what's going on in the system and try to write a series of rules so tax software for example does a pretty reasonable job of that we're a chartered accountant or a tax expert sits down encodes a series of rules we have a question in the back example of an expert system for my yep so so the question is you know are there examples of expert systems a certainly one would be tax software my graduate research was actually an autonomous vehicle some of the early autonomous vehicles used a form of expert systems where the states on a finite state machine were maybe handcrafted but the end the transitions between them were sort of designed that way there was some machine learning wrapper around but expert systems you know certainly played a large part in some of the early of an autonomous vehicle research that went on alright so over time we we were able to use these expert systems and you know don't get me wrong these systems are still extremely valid in cases where you have limited data availability limited compute power a lot of expert systems still being used or in cases where explain ability is a very important factor you still see expert systems because they do have the ability to explain why they came up they can typically point to a set of rules that somebody wrote which is usually quite interpretable by a human however as we were able to collect more data we were maybe able to understand a little bit more about what was the underlying process we were able to apply a statistical learning and this sort of led to the next era or next wave of AI technologies which is often called the learning wave and this was really enabled by lots of data enabled non expert system so what we mean by that is we were able to kind of dial back the amount of expert knowledge that we encoded into the algorithm maybe put a higher level of expert knowledge into that but usually and then use data to learn what some of these rules could be an example of that in case someone wants to ask would be in speech processing for example so we were able to say well I kind of realized that speech follows this sort of you know Gaussian mixture model so I can encode that level of statistical knowledge but I'm going to let the system sort of figure out the details of how all that actually works works out and there are many other cases I again coming back to some of the research I did on autonomous vehicles we were able to maybe use some high-level expert rules that there here are a set of states that a car may be a car may be in but I'm gonna let the algorithm actually figure out when these transitions occur and what constitutes a transition between different states so looking at the four feature the four vectors that you could think about it these systems had a little bit more on perception obviously we're doing a lot more learning but their ability to abstract and reason was still was still pretty low and by reasoning we mean you know can you explain can you tell us what's going on when you give me an output to the result the next wave which were maybe at the beginning stages of is what we call contextual learning or contextual adaptation this is where an AI system can actually add context into what it's doing I'm not sure I have too many examples of people doing this very well I think what most of most of the work today probably falls into sort of the the end stage of the learning wave of AI but we're able to sort of combine a bunch of these learning things to make it look like it's contextual in nature but the but the key concept over here is being able to have the system automatically abstract and reason so the way that we we think about things right so if I see a chair over here and I put a chair somewhere else I still know it's a chair because I'm using other contexts maybe it's next to a table or stuff like that some early research going on in that area and certainly the next wave of this is what we call abstraction and there is very little work on this but if we sort of have to think out in the future but this is really the system of an ability the ability of an AI system to actually abstract information that it's learning so instead of learning that a chair is a thing or a table is something with a leg at the bottom it learns that a table is something you put things on and is able to abstract that information or that knowledge to any other any other domain or any other field do we have any questions before I continue from here okay great so that's a little bit on the evolution of AI the reason we like to go through this is because you know there is great work going on in each of these waves and nothing nothing that people are doing in any of these waves is any lesser or more it's typically a dependent on what you have at your disposal what I like to tell people sometimes is the way to think about all of this is you have a couple of dials at your disposal right turning dials the first dial is how much compute right yes how much ability do you have to crunch data the second piece is how much data do you actually have available and this can be labeled data in many cases and the third dial is how much knowledge are you able to embed into an algorithm in certain cases where maybe you have very little computing very little data label data availability but a lot of expert knowledge or a lot of ability to encode information into an algorithm you might be able to use an expert system right and that's a that's a very good use case for that an example may be on another dimension where you have you want to encode very little human knowledge but you have a lot of computing and data available would be where neural networks fall in right where they're essentially learning what the human knowledge you know what that encoded information should be a lot of statistical techniques also fall into that camp where maybe you encode a little bit of information as to like what the background distribution of the process is but it learns the details of exactly how that distribution is modeled based on the data that it sees so you have a lot of different settings that you can use and there are a number of different techniques within the within the broader AI context that you can use to achieve your your mission I'm sure many of you are going to be doing different types of algorithms and a lot of that decision will be dependent on well how much data was I given right how how good is this data that I'm using is our inability to to learn anything from this and if not you might have to encode some knowledge of your own into it saying well I kind of know that this process looks like that so let me tell the algorithm to not waste too much time crunching the data to learn the underlying distribution which I can tell you why don't you learn the parameters of the distribution instead that makes sense right and as you know there's just a lot going on in AI and machine learning you can't walk two steps without running into somebody who's you know either starting something up working for one of these organizations so it's it's really it really is an exciting time to be in the field alright so that's a little bit on the overview but let's talk now in a little bit of detail on what some of the critical components are within this AI ki architecture so one thing that we like to note and there's a reason that as we've been reaching out to a number of you we've been talking about getting data right work with stakeholders to get your data in place and the reason we talk about that is data is critical to breakthroughs in AI a lot of the press maybe on the algorithms and the algorithms that have been designed but really when we've looked back in history we've seen that while the availability of a good canonical data set actually is is is equally if not more critical to breakthrough in AI so what we've done here is we've just picked a select number of breakthroughs in AI our definition of a breakthrough in this particular example is something that made a lot of press or something that we thought was really cool so here are some examples of that in different years and we've talked about the datasets sort of the canonical data set that maybe led to that breakthrough or that was cited in that breakthrough as well as the algorithms that were used in that breakthrough and when they were first proposed this is notional in nature you clearly this you could adjust these dates a few years here or there but what we really want to get across is that the average numbers a number of years to a breakthrough from the availability of a cool data set right are very important well-structured well labeled data set is much smaller than from the algorithms first proposal when the algorithm first comes out so as you're developing your challenge problems as you're developing your interactions with stakeholders certainly something to keep in mind that there's clearly a lot of algorithm algorithmic research that's going to go on but having a good strong well labeled and documented data set can be equally important and making that available to the wider AI community the wider AI ecosystem can be very very valuable to your work and the work of many other people all right so back to the AI architecture we're going to just go through very briefly different pieces of this architecture so the first piece we're going to talk about is the data conditioning which is converting unstructured and structured data within the structured and unstructured data you might have structured sources coming from sensors you know network logs for some of you metadata associated with sensors may be speech or other such signals there's also a lot of unstructured data you think of things that you might collect from the internet that you might download from say a social media site maybe it reports other types of sensors that maybe don't have well that don't have strong structure within the data set itself and typically this first step consists of an um you know what we call the data conditioning step consists of a number of different elements you might want to first figure out where to put this data that can often take a lot of time and there have been religious wars fought on this on this topic we're here to tell you that you're probably okay picking most technologies but if you have any questions you know feel free to reach out to me or to others on the team we have a lot of opinions on you know what's the right infrastructure to solve the problem typically these infrastructure databases might provide capabilities such as indexing organization and structure very important in unstructured data right to convert it into some format that you can do things with they may allow you to connect to them using domain-specific languages so it's sort of converting into a language that maybe you're used to talking they can perform provide high performance data access and in many cases a declarative interface because maybe you don't really care about how the data is being accessed you want to just say select the data give it to me and then move forward from there another important part of the data conditioning step is data curation this unfortunately will probably take you a very long time and this is it requires a lot of knowledge of the data itself what you want to do with the data and how you receive the data but what you might do in the data curation step is you know perform some sort of unsupervised learning maybe reduce the dimensionality of your problem you might do some clustering or pattern recognition to maybe remove certain pieces of your data or to highlight certain pieces of the data that look important you might do some outlier detection you might highlight missing values you might there's this dot dot dot etc etc etc a lot goes on in the data curation step we could certainly spend hours just talking about that and the final thing especially within the context of subha supervised machine learning but even in the world of unsupervised learning would be spending some time on data labeling right so this is taking data that you've received typically doing an initial data exploration it could be as simple as opening it up in Excel to see what the different columns and rows look like if it if that's a suitable way place to open it up you might look for highlight missing or incomplete data just from your initial data exploration you might be able to go back to the data provider or to the sensor and say you know can you reorient the sensors or recapture the data you know I noticed that every time you've measured this particular quantity it always shows up as three I can't imagine that that's correct can you go back and tell me if that sensor is actually working or is it actually three in which case you might want to know that and you might look for errors biases and collection you know of course on top of the actual labeling process that you're doing to highlight phenomenology within the data that you'd like to then look for through your machine learning algorithms oh I'll pause for a second yes so the question is what's the ratio we see between structured and unstructured data that's a great question so the ratio in terms of the volume of the ratio in terms of what you can do with it because those are actually almost the the opposite so again I'm talking about a few data sets that I'm very familiar with the unstructured data can often be 90% of the volume and 10 person you know maybe the 10% is the metadata associated with the unstructured data most of the value however comes from the structured data where people really analyze the crap out of the structured data because they know how to there is certainly a lot of you know a lot of potential within the unstructured data so that when we when we talk to people we that's why we talk a lot about infrastructure and data bases as being an important first step because if you can just take the unstructured data and put it into a structured or semi structured form that itself can provide a lot of value because there's very often in problems that we see you know sort of the 90% volume of data is largely untapped because people don't know how to get into it or don't know what to do with it or it's not in a form that you can really deal with so when I think next class we're going to be talking to you about how to organize your data right strategies for organizing data that can get you a lot more value out of the unstructured data does that answer your question yes is it typically like you you so the question is when you apply AI or machine learning techniques to a problem domain is it typically a single modality or multiple modalities I'd say the answer is both certainly there's a lot of research and you know back there we have Matthew who's actually doing research on that right now on how to fuse multiple modalities of data I know a lot of projects that are that are being discussed here are certainly looking at multiple modalities if I had to say as of today a lot of the work that's out there right the sort of published work may be focused on a single modality but that's not to say I mean I think there is a lot of value on multiple modalities but the challenge still comes up on how do you integrate this data especially if they're collected from different systems because the technologies that can purse the waveform and all of this data conditioning or mature enough so the question is why would speech or something else like that fall into structured versus unstructured and you're absolutely right I think when we pick speech and I'm sure there are others in the room that might disagree with that and might stick it over here when we look at the type of acquisition processes that are used as a software that's used they typically come out with some known metadata they follow a certain pattern that we can then use right there is a clear range to where the there's a the frequency to which the data is collected and that's why we stuck it in the structured data type of course if you're collecting data out in the field without without that you could probably stick it into the unstructured world as well but that's probably a good example of something that can fall in between the two places okay right now for the part everyone's really interested in machine learning right so alright you got through the boring data conditioning stuff which will take you yeah a couple of years or something like that you know nothing nothing serious and now you're ready to do the machine learning and now you're you're you're given a choice well which algorithm do you use neural networks you might say right I I there's a lot more though beyond the neural network world so there's numerous taxonomy --zz I'm going to give you two of them today for how you describe machine learning algorithms one that's really kind of an interesting way is from Pedro Dominguez at the University of Washington in which he sort of says that there are five tribes of machine learning so there are the symbolist switch an example of that would be expert systems there are the Bayesian tribes which an example of an algorithm within that may be naive Bayes there are the analogized analogize errs which an example of that would be a support vector machine and the connectionists an example of which would be deep neural networks and evolutionary's an example of that which might be genetic programming what what really you know I'm trying to get across I'm sure the author is trying to get across here is that lots and lots of different algorithms each have their relative merits and relative strengths apply the right one for your application apply the right one for again given these dials that I talked about earlier the amount of computing that you have available the amount of data that you have available and the amount of expert knowledge that you're able to encode into your algorithm that you think is generalizable enough if we actually talk about this is a very useful chart I've found in describing to folks that are not familiar with AI that might say well isn't a I just neural networks and neural networks are a part of AI but not necessarily all of it so if we kind of think of the the big circle is the broad field of artificial intelligence within that is the world of machine learning within machine learning our you know connectionists or neural networks to sort of fall into a small camp within that deep neural networks is sort of a part of neural network so can anyone maybe give me an example although I've said it numerous times or something that might fall out of machine learning but into artificial intelligence from an algorithmic point of view yes graph search could be an example I would maybe stick that into some of the connectionists however yes exactly so expert systems is sort of the one that comes to my mind or knowledge based systems are an example of maybe something that falls outside of the realm of machine learning again in the very strict sense but maybe within the realm of artificial intelligence from an algorithmic point of view okay so that's a little bit on the algorithms next let's talk about some of the modern computing engines that are out there I mentioned that data compute as well as algorithms have been key drivers to the resurgence of AI over the past few years what are some of these computing technologies for example so clearly CPUs and GPUs they're they're very popular computing platforms lots of software written to work with these computing platforms but what we're seeing now is that with the end of Moore's Law and a lot more performance engineering going on we're seeing a lot more work research and hardware architectures that are custom in nature and custom architectures are almost like the new commercial off to sell off-the-shelf solutions that are out there so an example of a custom architecture could be Google's tensor processing unit or TPU there's some very exciting research going on in the world of neuromorphic computing I'm happy to chat with you all later if you're interested to know what's going on in that area and maybe our role in some of that work and there's just some stuff that you know we would still call custom these are still people deciding you know designing basically looking at an algorithm saying ok here's the data layout here is the movement of day or information within this within this algorithm let's create a custom processor does that does that an example of that could be the graph processor which is being developed at Lincoln Laboratory and you know obviously know no slide on computing architectures or computing technologies would be complete without mentioning the word quantum in it there's some early results on you know solving linear system of equations but I think applied to AI it's still unsure or unknown or unproven where quantum eight play a part but certainly a technology that all of us I'm sure I've heard of are continue to track and are just interested in seeing where that where that goes to so within the within the first few however these are all products that you can buy today you can go out to your favorite your your computing store and just purchase these off-the-shelf solutions a lot of software has been written to work with these different technologies and it's it's a really nice time to be involved okay so the exam the question is what you know can I think about neuromorphic what should I think about when I'm thinking about neuromorphic so there's a few features which I say fall into the camp of what people are calling neuromorphic computing one is what they're calling a brain inspired architecture which often means it's clock las' so you typically have some sort of a so a lot of these technologies have clocked right movement of information these might be clock less in nature they typically sit on top of different types of memory architectures and the you know trying to think of what would be another parameter that would be that would be very useful let me send you I can probably send you a couple things that that help I'll I highlight that have certainly wouldn't call myself an expert in this area but yeah I think the the term that's used is it's supposed to mimic the brain in the way that the computing architecture actually performs our functions so lots of research as well and this is work that we've done here at the lab on actually trying to map the performance of these different processors and how they perform for different types of functions so what what we're doing here is basically looking at the power on the x axis and the y axis is the peak performance and Giga operations per second different types of precision are noted over there by the different shapes of the boxes and then different form factors and the idea here is basically to say there's so much going on in the world of computing how can we compare them they all have their own individual areas where they're strong so one can't come up and say well the GPU is better than the CPU well it depends on what you're trying to do and what your goals of the operation are so some of the the key lines to kind of note here is that there seems to be a lot of existing systems on this hundred Giga operations per watt on this line over here this dashed line some of the newer offerings may be fit into the one tera op per watt and some of the research chips like IBM's TrueNorth or area or intel's area fall into the sort of just a bit under the tent our operations per what sort of line that we see there but you know depending on the type of application you may be okay with a certain amount of peak power so if you're looking at embedded applications you're probably somewhere over here right if you're trying to get something that's you know on on a little drone or something like that you might want to go here and if you have a data center you're probably okay with that type of power utilization or peak power utilization but you do need the performance that goes along with that so I'd say the most important parts to look at are essentially these different lines those are the different those are the sort of the trajectories for maybe some of the existing systems all the way up to some of the more research-oriented processors out there okay alright so we we talked about modern computing let's talk a little bit about the robust AI side of things and the basic idea between robust AI is that it's extremely important and the reason that it's important is that the consequence of actions on certain applications of AI can be quite high so what we've done here is kind of think about you know where are the places that maybe humans and machines have their relative strengths so on the x-axis we're talking about the consequence of action so this could be to somebody get hurt if the system doesn't do the right thing all the way down to no worries if the system doesn't do the right thing right which could be you know maybe some of the labeling of images that we see online might fall into this category I'm sure people disagree with me on that but maybe a lot of national security applications a Health application certainly fall into the area of you know high consequence of action if you give someone the wrong treatment that's kind of a deal and then on the y-axis we're talking about the confidence level in the Machine making the decision so how much confidence do we have in the system that's actually making the decision in certain cases we might have very high confidence in the system that's making a decision and obviously in certain cases we do not have much confidence in the system making the so in areas where you have a low consequence of action maybe high confidence level in the machine making the decision we might say those are best match to machines those are good candidates for automation on the on the contrary there might be areas where the consequence of action is very high and we have very little confidence in the system that's making the decision probably an area we want humans to be intricately if not solely or involved or responsible and the area in between is where machines might be augmenting humans is anybody want to venture a couple of you know maybe a couple of examples help come up with a couple of examples here that you might put into each of these categories so maybe what's what's a good problem that you can think of that might be best matched to machines beyond labeling images for advertisements assembly lines yep that's a good example thinking within like spam filtering could be another example where there is I mean it there is some machine augmenting human it does send you an email saying this is spam are you sure but for the most part it's largely automated I'd say a lot of the work that many of us are probably doing falls into this category maybe on different sides of the spectrum but of where machines are augmenting humans so the system can be providing you know data back to a human that can then select it might filter information out for humans that then the humans can then go ahead and say okay well instead of looking at a thousand documents I could only look at ten which is much better and then there's obviously certain probably we want humans to be heavily involved with any kinetic you know anything that involves life or death we probably want and they're probably legal reasons also that we want humans involved with things like that one of the examples that you know we often get which is you know autonomous vehicles and it's always a little confusing where autonomous vehicles fall into this certainly the consequence of action of a mistake an autonomous vehicle can be pretty high and as of today the confidence in the decision-making is medium at best but people still seem to somehow be okay with fully automating that just shows how terrible Boston roads or driving in general is that we're like I'm not really sure if this thing will kill me or not but totally worth the worth worth trying it out sort of slowly expand the yellow yes I'd say that the question is you know is the is the yellow expanding I think so I you know what one could make the argument that is it shifting that direction are we finding areas where and I think that's maybe the direction I'm where we are probably looking at automating certain things a little bit more as confidence and decision-making goes up so you might think about this frontier sort of moving down so that maybe the green expanding slightly and the yellow kind of taking over a little bit of the red there might be some places where over time we're more open to the Machine making a decision and the human having largely supervisory supervise having a largely supervisory role which I would put right at this frontier between the yellow and the red I can think of some examples but maybe I'll share it with them share it with you later so certainly robust artificial intelligence plays a very important part in the development and deployment of AI systems I won't go through the details of each of these I'm sure many of you are very familiar with it and I know a few of you are far more knowledgeable about this maybe than I am but some of the key features would be explainable AI which is a system being able to describe what it's doing in an interpretable fashion metrics so being able to provide the right metric right if you want to go beyond accuracy or performance validation and verification there might be cases where you're not really concerned about the explained ability but you just want to know that when I pass an input I get an own output out of it and is there a way to confirm that I'm able to do that another could be on security so an example of this are not having security to be counter AI right so when we talk about security within the context of robust AI it's almost like the cryptographic way of thinking about it which is can I protect the confidentiality integrity and availability of my algorithm the data sets the outputs the the weights the biases etc and finally of a lot of significant importance is policy ethics safety and training this is actually very important in some of those applications where you know in the previous slide we had the yellow and the red where humans and machines augmenting humans where that falls a lot of that might be governed by policy ethic safety and training which is some of the examples that I can think of where there are policy reasons that make it that only a human can be involved with this may be with minimal input from a system okay and the final sort of component of our AI architecture we've gone through conditioning algorithms computing robust AI is human machine teaming and think what we want to get across with human machine teaming that is it really depends on the application and what you're trying to do but it is important to think about the human in the machine working together and there's this there's a spectrum of where the machine will largely will play a large part in a human largely supervisory or to where the human plays a large part and the machine is you know very targeted in what you do with the machine or the AI of the system but a couple of ways to think about it would be of course you know we talked about the confidence level versus consequence of actions but also the scale versus application complexity so on the top chart over there we have on the the x-axis is the application complexity how complex is this application and on the y-axis is sort of the scale you know how many times do you need to keep doing this thing places that machines might be more effective than humans or where we have low application complexity but very very high scale so again spam filtering falls into this the complexity of spam filtering has gone up over time but is something that you know is is is reasonable within systems but the scale is very high that we just don't want a human being involved with that process and on the other end of the spectrum is where you have very high application complexity that'll only happen a couple of times so this could be say reviewing a situation you know maybe a company's trying to make an acquisition it's not going to happen over and over so you might have a human involved with with that that kind of goes through a lot of that may be they target the system to go look for specific pieces of information but really it's the human that might be more effective in that especially given that the situation would change over and over all right so with that we're going to take a quick tour of the world of machine learning I'll stop there for a second any questions okay all right so what is machine learning always a good place to start it's the study of algorithms to improve their performance and some tasks at some task with experience and in this context experiences data and they typically do this by optimizing based on some performance criteria that uses example data or past experience so in the world of supervised learning that could be the example data or past experience could be the correct label given an input data set input data point machine learning is a combination of tikrit techniques from statistics computer in computer science communities and it's the idea of getting computers to sort of program themselves common tasks within the world of machine learning could be things like classification regression prediction clustering etc for those who are maybe kind of making the shift to machine learning from traditional programming I found this again for Pedro Domingos to be a very useful way of sort of describing it to people so in traditional programming you have a data set you write a program which would be if you see this do that when you see this do that you know for this many instances do the following thing on it and then write an output out right so you input a data into the program into a computer and the computer produces an output where it says okay I've applied this program on that data and this gives me the output machine learning is sort of a very different way of thinking about it in which you're almost inputting the data as well as the output so in this case the data could be unlabeled images the output could be the labels associated with those images and you tell the computer figure out what the program would look like and that's just a slightly different way of thinking about machine learning versus traditional programming what are some of these programs or algorithms that the computer might use to figure it out so within the within the the large realm of machine learning we have supervised unsupervised reinforcement learning what we have in the brackets is essentially what you're providing in the world in the case of supervised learning you're providing labels which is the correct label associated with an input feature or with an input data set a data point in unsupervised learning you typically have no labels but also are limited by what the algorithm itself can do and in the world of reinforcement learning you're providing instead of a label per data point you're providing the reward information to the system that says if you're doing more if you're doing the right thing I'm going to give you some points if you're doing the wrong thing I'm gonna take away some points very useful in like very complex applications where you you can't really figure out the the labels associated with each data point within the world of supervised learning the typical tasks that people have and I should note before I go through this there's a lot of overlap between all of these different pieces so this is a high-level view and but we can certainly argue about the specific positioning of everything I'm sure we can so within supervised learning you can fall into classification regression unsupervised learning is typically clustering dimensionality reduction and within these there are different algorithms that that fall into place so examples could be things like neural nets which sort of cover all of these spaces get logistic regression PCA which might fall into dimensionality reduction lots and lots of different techniques and also some in the reinforcement learning world and there's just more and more and more if you open up a survey of machine learning it's it'll give you even more than all of these techniques over here and the thing to remember when you're using machine learning is that there are some common pitfalls that you can fall into an example of that would be overfitting versus under fitting where you you come up with this awesome model that does really really well on your training data you apply it to your test data and you get terrible results you might not have your you might have done a really good job learning the training data but not necessarily learning being able to generalize beyond that sometimes it could be just the algorithm itself is unable to correctly model the behavior that's exhibited by the training and test data I won't go through each of these again but there might just be bad noisy missing data that certainly happens you end up with an algorithm with terrible results and you look at it and you're like why is that and you actually look at the data that you did and it was incorrect that there was just missing features or it was noisy in nature such that the actual phenomenology that you are trying to look for was hidden within the noise you might have picked the wrong model you might have used a linear model in a nonlinear case where the phenomenology are trying to describe is nonlinear in nature but maybe you use a linear model you've not done a good job of separating training versus testing data etc etc so we'll just take a quick view into each of these different learning paradigms so the first is on supervised learning and you basically start with labelled data or what we call grows often refer to as ground truth and you build a model that predicts labels given new pieces of data and you have two general goals one is to one is in regression which is to predict some sort of a continuous variable or a classification which is to predict a class or label so if we kind of look at this the the diagram on the right we have training data that we provide which is data and labels that goes into a train model the that's typically an iterative process where we find out well did we do a good job that that is now called the supervised learning model that we then apply new data or test data or unseen data to and look at the predicted labels typically when you are designing an algorithm like this you'd separate out you take your training data you'd remove a small portion of it that you do know the labels for that's your test data over here and and then you kind of run that and you can see well is it working well or not and most of these algorithms have a training step that forms a model so when we talk about machine learning and in the super both the supervised and unsupervised sense we'll often talk about training the model which is this process and then inference which is the second step which is where you apply unseen data so this is the the trained model in deployment or in the field it's performing inference at that point of course no class these days on machine learning and AI could go without talking about neural networks and as I mentioned neural networks do form a very important a part of machine learning and they certainly are an algorithm that many of you I'm sure are familiar with and they they fall well within the supervised and unsupervised and they've been used for so many different applications at this point so what's a neural network a computing system inspired by biological networks and the system essentially learns by repetitive training to do tasks based on examples much of the work that we've seen is typically it being applied to supervised learning though I'll mention some that we are doing some research and actually applying it for unsupervised learning as well and they're in there quite powerful the components of a neural network include inputs layers outputs and weights so these are often the terms that someone will use and a deep neural network has lots of hidden layers does anyone here have a better definition for what deep in neural network means beyond lots for definitions anywhere three and above yes more than one layer but not necessarily one okay so so one definition here for deep is and this is anyone have a better no that's the the the one to beat right now is is sort of a feature of a deep neural network could be recurrence within the network architecture which implies that there you know that there is some sort of depth to the overall network so above three with recurrence deep alright lots of variants within the supervised world of of of neural networks such as convolutional neural network for cursive neural networks deep belief networks one of I think in my opinion again since you've all asked me to opine here no you've not but I think a reason that these are so popular these days is there's so many tools out there that are very easy to use you can just go online and within about five minutes write your first neural network try writing a hidden Markov model that quickly maybe there are people who can but in general in general so what are the features of a deep neural network so you have some sort of input features you have weights which are essentially associated with each line over here as well as biases for each of the layers that sort of govern the interaction between the layers and then an output layer so these input features can often be combined to each other so these feature vectors that are coming in can often be combined I think Jeremy will talk a little bit about how they the matrix view of all of this but you can kind of think of it as if you're an example could be if you have an image it could be the pixel the RGB pixel values of each of each pixel in that image could be the input feature so you could have large numbers of input features if you have a time series signal it could be the amplitude or the magnitude at a particular frequency or at a particular step there's often a combination of features that you might do so in addition to the pixel intensities for an image you might also then combine the spatial distance between two pixels or its position within the image may also be another input feature and you can really go hog-wild over here just trying to come up with new features and there's a lot of research just in that area which is I take a dataset that everyone knows and I'm just gonna spend a lot of time doing feature engineering which is coming up with what is the right way to to do the features so coming back to an earlier question this is an area where people are often looking at supplementing maybe a given data set with additional data and then fusing those two pieces together for example could be audio and text together as input features to a network that you can then learn that might do a better job but all of this you know sort of governed by this really really simple but powerful equation which is that the output at the I plus one layer is given by some nonlinear function of the weights multiplied by the inputs from the previous step plus some sort of a bias term then and when you're learning when you're training a machine learning model you're essentially trying to figure out what the W's are and what the B's are that's really what a model is defined as so if we kind of zoom into one of these pieces it's it's actually pretty straightforward what's going on over here so you have your inputs that are coming from the previous layer so this could be your Y sub I here are the different weight so w1 w2 w3 these are the connections are the weights going into a neuron or a node and you know performing some function on these inputs and that function is referred to as an activation function so let's just take an example where we have some actual numbers maybe I've gone through I've trained my models I figured out that in this just for this one dot in that big network that we saw earlier that my weights are two point seven eight point six and zero point zero zero two my inputs from the previous layer would be is maybe negative zero point zero six two point five one point four and all I'm doing is coming up with this X which is negative zero point zero six x two point seven plus two point five times eight point six plus one point four times that that gives me some number twenty one point thirty-four I apply my nonlinear function which in this case is a sigmoid governed by that equation at the top right and I say F of twenty one point three four so somewhere way over there is approximately one right so this probably a little less than one but approximately one for the for the purpose of this and you just do that over and over so really a neural network that I think the power of a neural network is it allows you to encode a lot less information than many of the other machine learning algorithms out there at the cost typically have a lot more data being used and a lot more computing being used but for many people that's perfectly fine right and but it does take it's just over and over back and forth back and forth back and forth to come up with what's the right double use in order for this to give me a result that looks reasonable lots of you know lots of work going on in just deciding the right activation function I showed you a sigmoid over there we do a lot of work with riilu units you know the choice is you know there are certain applications certain I should say domains or applications or people have found that a particular activation function tends to work well but that's that choice is something I kind of leave to domain experts to maybe look at their problem and figure out what are the relative advantages each of these kind of have their own advantages I know like for example one of the big advantages of rectified linear unit is that since you're not limiting yourself between a negative one zero and one range you don't have to do that you don't run into a problem of vanishing gradients that doesn't mean much for people that's okay we're not I spend too much time talking about that anyhow yep hmm that's a so the question is picking the activation functions picking the number of layers we'll talk about that in a couple of slides but there is a lot of art trial-and-error yes but also a you know call it art as well that's involved with with coming up with that a lot of what happens in practice however is you find an application area which looks very similar to the problem that you're trying to solve and you might borrow the architecture from there and kind of use that as a starting point in coming up with coming up with where you start I'm not I'm sure people are doing it I'm personally not familiar with that research I don't know if anyone else in the room has yep okay you're trying to learn close the architecture Network the activation function or all the other attributes because you try to just go from DSM to machine learning system with no intervention so the question was is there any research into parametrizing the activation functions I guess the model as a whole so yeah there there's there is and and you know one of the responses was that there is a program run by DARPA which is the d3m program which is really looking at you know can you go from data to result with no or almost no human intervention so that would be there certainly I'm not familiar with activation function parameterization but certainly network model parametrizations absolutely there so people who are running optimization models to basically look for I have this particular set of resources what is the best model architecture that fits into that you know maybe I want to deploy this on a really tiny processor that only gives me you know 16 megabytes of memory I want to make sure that my model and data can fit on that can you find what would be the ideal ideal model for that so that's absolutely something that people are doing right now but I'm not sure if people are like trying to come up with I guess brand new activation functions all right so lots lots of stuff in the neural network landscape and as I mentioned earlier neural network training is essentially adjusting waits until the function represented by the neural network essentially does what you would like it to do and the key idea here is to iteratively adjust weights to reduce the error so what you do is you take some random instantiation of your neural network or maybe based on another domain or another problem you might borrow that and you kind of start there and then you pass the data set in you look at the output and you say hmm that's not right what went wrong over here and you kind of adjust kind of go back and adjust things and do that again and again and again and again and again over and over until you get something that looks reasonable that's really what's going on over there and so real neural networks can have thousands of input data points hundreds of layers and millions to billions of weight changes per iteration yes [Music] yes there's a lot of work being done to paralyze this and and by default so the question is is you know when as I just described it right now it's a serial process where I pass one data point in it detector goes all the way to the end it you know it says this is the output goes back and adjusts are there techniques that people are doing to do this in a distributed fashion and the answer to that is is a strong yes it's a very active area in especially high performance computing and machine learning we might talk about this in are we talking about this on day three we might we might talk a little bit about it but there is you know model parallelism which is I have the model itself distributed across multiple pieces and I want to it adjust different pieces of the model at the same time there's research and lots of results I think we might even have some examples that people are doing with that a little bit earlier yeah so there are many different ways to paralyze it one would be data parallelism which is I take my big data set or big data point and I distribute that across my different nodes and each one kind of independently learns a model that works well and then I do some sort of synchronization across these different pieces there are also techniques where you have the model itself may be too big to sit on a single node or a single processing element you might have to distribute that so yes a lot of a lot of very interesting research going on in that area and by default you're when you do run things they are running in parallel just even on your GPU they're using multiple cores at once so there is some level of sort of within the node itself parallelism that runs by default on most most machine learning software so inference as I mentioned is just using the Train model again and the power of neural networks really falls within their non-linearity so you have that nonlinear F function that you're applying over and over and over across your layers and this crudely drawn diagram on my iPad is not clear at all it's the you know if you had X's and O's it reminds me of a song and you have features over here and you're trying to basically classify you know which is an X and which is an O a linear classifier I could go do a pretty good job in this type of situation and you know you could apply a neural network to this but maybe it's overkill in that type of situation but in in some sort of a feature space if this is how your X's and O's are divided divided amongst each other and you're trying to come up with the right label you know one thing I might suggest is maybe find another feature space that you could maybe get a better separation between the two or a technique like a neural network might do a very good job or any of these nonlinear machine learning techniques might do a very good job for looking for these really complex decision boundaries that are out there all right so you mentioned earlier you know when you're designing a neural network what are what do you have to do like what are the different choices etc there's a lot going on here so you get you have to pick the depth the number of layers the inputs and what the inputs are the type of network that you're using the types of layers the training algorithm and metrics that you're using to assess the performance of this neural network the good thing however is it is so expensive to train a neural network that you largely are not making these decisions in many cases you just pick up what somebody else has done and you start from there and then you start that's your that might be I don't know if that's a good or a bad thing but that that's often a way in practice that people end up doing this but there is some there is some theory on like the general approach I think in the in this short amount of time which I'm already over we we won't be able to get into it but I'm happy to actually these slides have backups on them so when I share them with you that do have a lot more detail on each of these different pieces all right very quickly we'll talk about unsupervised learning and the basic idea is the task of describing a hidden structure from unlabeled data so in contrast to supervised learning we are not providing labels we're just giving the algorithm a data set and saying tell me something cool that's going on over here now clearly you can only do you you can't you can't label the data if you do that but what you can do is maybe look for clusters or look for dimensions or pieces of the data that are that are unimportant or extraneous so if we observe certain features we would like to observe the patterns amongst these features and the typical tasks that one would do in unsupervised learning is clustering and data projection or data pre-processing or dimensionality reduction and the goal is to discover interesting things about the data set such as subgroups patterns clusters etc in unsupervised learning one of the one of the difficulties in supervised learning we know right we have an input we have a label and we're like okay if that input if my algorithm doesn't give me the label bad go you know retrain or I know what you know I can kind of go back use that as my performance metric on unsupervised learning there is no simple goal such as maximizing a certain probability for the algorithm some of that is going to be you know something that you have to work on is it the inter class or intra class distance that I'm most you know kind of having that separation is that going to be my performance metric is it the number of clusters that I'm creating is that the is that the metric that I'm using but it is very popular because it works on unlabeled data and I'm sure many of us work on datasets which are just too large or too difficult to sit and label an example that that comes to my mind certainties in the world of cybersecurity where you're collecting billions and billions of network packets and you're trying to look for anomalous behavior you're not going to go through and look at each pack and be like bad good you know what it is but you might use an unsupervised technique to maybe extract out some of the relevant pieces then use the supervised you know then go through the trouble of labeling that data and then pass that on to a supervised learning technique and I'm happy to share some research that we've been doing on that front some common techniques are within clustering and data projection clustering is the basic idea that we want to group objects or sets of features such that objects in the same cluster are more similar to those of another cluster and what you typically do for that is you put your data in some feature space and you try to maximize some sort of an intra cluster measure which is basically saying I want the points within my cluster to be closer than anything outside of my cluster right so that's that's a metric and you sort of iteratively move the membership from each but you you you set a number of clusters saying I need five clusters it'll randomly assign things and it'll keep adjusting the membership of a particular data point within a cluster based on a metrics such as the squared error so in this example we might say that okay these are three clusters that I get out of it dimensionality reduction is the idea of reducing the number of random variables under consideration very often you'll collect a data set that has hundreds to thousands of different features maybe some of these features are not that important maybe they're unchanging or even if they are changing it's not by much and so maybe you want to remove them from consideration that's when you use a technique like dimensionality reduction and this is really really important when you're doing feature selection and feature extraction in your real datasets and you might also use it for other techniques such as compression or visualization so if you want to show things on Excel right showing a thousand dimensional object may be difficult you might try to get down to the the two or three dimensions that are easiest to visualize and of course you can use neural networks for unsupervised learning as well surprise surprise so as much as a lot of the press you've seen has been on things like image classification using nice label datasets there's a lot of work where you can apply it in an unsupervised case and these are largely used to find better representations for data such as clustering and dimensionality reduction and they're really powerful because of their nonlinear capability so one example I won't spend way too much time on this is an autoencoder and the basic idea behind an auto-encoders you're trying to find some sort of a compressed representation for data and the way we do this is by changing the metric that we use to say that the system has done a good job and the metric is basically if I have a set of input features that I'm passing in I would like to do the best job in reconstructing that input at my output and what I do is I squeeze it through a smaller number of layers which forms this compressed representation for my data set and so the idea here is how can I pass my inputs through this sort of narrow waist to come up with a reconstructed input that's very similar to my original input and so my metric in this particular case is essentially the difference between the input the reconstructed input or the output and the and the input and the compressed representation you can kind of think of as the reduced dimensionality version of my problem we've also done some work on replicator networks which are also really really cool happy to chat about that as well and finally we have talked very briefly on reinforcement learning and the the basic you know again at a very high level the the reason reinforcement learning is sort of fundamentally different than supervised or unsupervised learning is that you're not passing in sort of a label associated with an input feature so there is no supervisor right or a person that can label it but just a reward signal and the feedback is often delayed and time is important so it kind of steps through a process and the agents actions often change the input data that it receives so just to maybe in the interest of time just to give you examples of where reinforcement learning could work and why you would use a technique like reinforcement learning so flying stunts maneuvers in a helicopter so you if your helicopter is straight you say keep doing more of whatever you're doing to keep it there if the helicopter tips over you say stop doing whatever you just did to do that could you create a supervised learning algorithm for doing this sure right you would basically look for all the configurations of your entire system every time the helicopter was upright and you would look for all the examples where your helicopter was tipping over or falling and you would basically say okay you know my engine speed was this much my rotor speed was this much and there are probably people here who fly helicopters so pardon me if I am completely you know oversimplifying this problem here however you could certainly label it that way and say all these all these configurations of the helicopter meant the helicopter was upright all these configurations of the helicopter meant the helicopter was not upright would be a pretty expensive and difficult data collect to do not sure how many people want to volunteer for let's let's do all the ones where it falls and and lots of other applications beyond that so these are really useful especially in cases where they're the what you're trying to model is just extremely complex and the other really powerful thing is this tends to mimic sort of human behavior and so they're they're very useful in in those type of applications so a reward would just be it would be very similar to you get points so you have your algorithm that's basically trying to maximize the number of points that it receives for example and as you do more it's very similar to what you or I would consider a reward playing a video game right every time I get points I do more of the activities that make me get points and in it's essentially the same same concept over here all right so with that I will conclude only 20 minutes behind schedule so you know I guess the the long story short is there's lots of exciting research into AI and machine learning techniques out here we did a one-hour view of this broad field that research has dedicated about six to seven decades of work towards so my apologies to anyone who's watching this or in the room who's worked I just like jumped over the key ingredients however and I think this is most important to to this group is you know and I kind of look at what are the problems where AI has done really well these are some of the key ingredients sort of data availability computing infrastructure and the domain expertise and algorithms and I think it's very exciting to see this group over here because we do have all of these pieces sort of coming together so great things are bound to happen there are you know I think large challenges and data availability and readiness for AI which is what we're going to sort of scrape the scrape the edge off during this class and some of the computing infrastructure is something that we'll be talking to you about in a couple of minutes and if you're interested in some of the more detailed look at any of these things a number of us actually wrote you know maybe a biased I think it's a great great great write-up but no I think it's useful it has its place there's obviously a lot of lot of material in here but we try to do our best job to at least cite some of this really really interesting work that's going on in the field so with that I'll pause for any additional questions but thank you very much for your attention you you 