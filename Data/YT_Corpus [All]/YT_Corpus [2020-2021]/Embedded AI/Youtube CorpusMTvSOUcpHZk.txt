 hello the AI that I'm about to describe runs on every microcontroller that Cypress has ever made 8-bit 16-bit 32-bit FPU/no-FPU each and every one of them zero exceptions we'll get to what is behind that claim in a minute but first let me share with you a cry for help that I read about in a blog post on medium this cry for help is from an embedded developer named Andrew Zhuravchak his cry for help is directed at machine learning developers he posted it on medium in January of this year and gained 20,000 views within a couple of days Andrew made two key observations number one machine learning experts do not really appreciate memory latency power and cost constraints of embedded systems for example embedded developers want to design solutions around commodity micro hardware devices like Jetson Nano and Raspberry Pi are not practical for their applications the second issue is the number of complicated steps that they need to perform to chisel down a conventional train network to a size that fits the constraints of low-end micro-hardware they are looking for a network that just works as is without going through all those additional tedious steps at the end of his article Andrew mentions CMSIS-NN as a promising avenue let's explore the steps involved in that framework there we go the first five steps check if the layers are supported by CMSIS-NN check the data layout of the parameters based on the activation statistics select a quantization scheme compute Q-formats three more steps generate the code test optimize sounds promising I don't know but this leads us to a fundamental issue CMSIS-NN is part of the conventional approach that is train a complex model and then optimize it for inexpensive micro-hardware at INFXL we are working on a simpler single-step approach that is train for a dual objective reduce approximation error and reduce complexity and do both of them simultaneously introducing the lightweight network a deep net optimized for embedded systems it is very concise fits within 110 bytes on a Cortex M4 we have tested it successfully on problems of different complexities and sizes more than a dozen hidden layers millions of weights tens of thousands of features it is optimized for data originating from sensors like accelerometers pressure sensors biochemical sensors let's now highlight some of the unique features of this lightweight network the lightweight network runs on a huge variety of microcontrollers DSPs graphics chips as well as FPGAs our friend Andrew who works at Cypress will be happy to find out that the lightweight net supports all micro-hardware made by Infineon as well it can do inference on microwatts making it ideal for battery-operated devices like wearables it is super efficient making it capable of real-time inference it requires only a few kilobytes of memory and it can run on chips that cost less than 25 cents how easy is the training process you don't really need a background in machine learning to use it how easy is it to integrate in an embedded project let's look at the code for the trained lightweight network this is the complete code for a trained lightweight net the instructions of interest here are the three while loops and the if-block note the lack of any 32-bit data no floating-point no multiply-accumulate just accumulate only 8-or 16-bit data on 32-bit micro-hardware you can speed things up using SIMD instructions please note that the code is universal the only thing that changes from project to project is the first line after deployment if you train a better lightweight net than before the only thing that you need to update is that first line let's look at the training workflow now first we select the training data do the necessary pre-processing and place it in two distinct files one for features the other for labels these files then are uploaded to a cloud-based cluster which trains several models the model picker then selects the model that best meets the customers requirements the customer may specify that the model should be as accurate as possible or that it fits in say 4 kilobytes or have a latency of X milliseconds or power consumption of less than Y microwatts the selected model is then transformed into highly efficient hardware-agnostic code that I just showed you let's move on to use cases now they can be divided into three groups real-time applications battery-operated devices and applications where cost is an issue lightweight net is ideal for smart battery-operated wearables due to its ultra-low power consumption predictive maintenance is an important need of the manufacturing industry but it is expensive and is reserved for expensive machinery only lightweight net can run on hardware costing less than 5 dollars and therefore can be used to ensure the health of inexpensive machinery predictive maintenance ensures long and economic operation condition monitoring on the other hand predicts imminent catastrophic failures here the ultra-low latency characteristic of the lightweight network is of huge importance ultra-low latency characteristic is also very desirable when you are trying to predict the data traffic on a telecom base station low latency is even more essential when you are trying to detect malware in real time on a computer network INFXL's lightweight network has applications in a great variety of industries from manufacturing to healthcare to automotive a limited version of our technology is accessible through the web just upload the data and receive your optimized deep network within 24 hours you can try it right now free-of-cost this is the front page of our portal you log in and you'll be taken to the file upload page upload the features and the labels data files and you'll receive your highly-efficient lightweight network the very next day after training is finished you'll receive an email similar to this one you click on the provided link and are taken to the page for the trained network here I want to highlight the info at the bottom of the page it gives you an idea about the range of possibilities the cost here by the way is a rough estimate of the complexity or the size of the network please note that 5th network is one-third the size but only one percent less accurate than the best network such trade-offs are generally available for most training data sets the key characteristic of the lightweight network is that it does simple operations on simple data that means that we need a mapping between real-world data and what is required by the lightweight network conventional deep networks accept floating point inputs in the range 0 to 1 or -1 to 1 the lightweight network requires integers in the range -127 to 127 therefore we prepare our data as we will for a conventional network and then apply a very simple mapping after the application of the mapping the features data looks like this and the labels data like so please note that the lightweight network is for classification tasks only for now you may be familiar with the Arm micro-tensor library for developing streamlined neural networks for micro-hardware they posted some results on the MNIST data and here you see the comparison with the lightweight network smaller code lesser number of parameters better accuracy here is a comparison between ST's Cube.AI library and the lightweight network their stuff works but only on high-end processors those with floating-point units the lightweight network works without the floating-point unit even on their 8-bit devices they claim that they can run several networks on a single device we can run hundreds if not thousands of lightweight networks on one of their high-end devices they follow the conventional approach train and then optimize we use the single-step approach their stuff is quite universal and covers image and voice as well our lightweight network is focused on industrial sensor data only at the end I want to quickly highlight the IoT data overload issue IoT sensors are capturing just too much data it is impossible to manage all that data in the cloud the solution of course is to put some intelligence right next to the sensors so that they upload only the actionable insights conventional sensors continuously transmit data whereas smart sensors use AI to find out what is important and transmit only that and thus reducing the amount of data transmitted by a factor of a thousand in some cases this reduction is transmitted data lowers power consumption and increases security a wireless smart sensor may look like this our friend Andrew will tell you that you can find the gray boxes in one of many Cypresss and Infineon SoCs they also make devices for managing energy harvesting and some sensors as well the AI for smart sensors has to be ultra-low power and able to run on inexpensive micro-hardware with a tiny memory footprint in INFXL's lightweight net fits the bill perfectly for example for an activity recognition wearable it does inference on a single microwatt on a Cortex M4 while using around 5 kB of memory it can run with acceptable speed even on a 16 MHz 8-bit microcontroller I am Altaf Khan representing INFXL we developed ultra-efficient embedded AI that is tailor-made for IoT edge devices I'm happy to answer your questions now 