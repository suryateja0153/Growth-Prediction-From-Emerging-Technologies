 [Music] so thanks so much for a CL and I'm really excited to be here to talk about a new project that I think is pretty cool so Tessa blow like four microcontrollers what's that all about so this all comes back to when I actually first joined Google back in 2014 and as you can imagine there were a whole bunch of internal projects that I didn't actually know about as a member of the public that sort of blew my mind but one in particular came about when I actually spoke to Vaz Yale for the first time and he explained and he was on the speech team at the time working with Alex who you just saw and he explained that they used neural network models of only 13 kilobytes in size at that time I only really had experience with image networks and the very smallest of them was still like multiple megabytes so this idea of having a 13 kilobyte model was just amazing for me and what amazed me even more was when he told me why these models had to be so small they were running them they needed to run them on these DSPs and other embedded chips in smartphones so Android could listen out for wakê words like hey Google while the main CPU was powered off to save the battery these microcontrollers often only had tens of kilobytes of RAM and flash storage so they simply couldn't fit anything larger they also couldn't rely on cloud connectivity because the amount of power that would have been drained just keeping a radio connection alive to send data over would have just been prohibitive so that really struck me that conversation and the continued work that we did with the speech team because they had so much experience doing all sorts of different approaches with speech they'd spent a lot of time and a lot of energy experimenting and even within the tough constraints of these embedded devices neural networks were better than any of the traditional methods they used so I was left wondering if they'd be really useful for other embedded sensor applications as well and it left me really wanted to see if we could actually build support for these kind of devices into tensorflow itself so that more people could actually get access to that at the time only people in the speech community really knew about the groundbreaking work that was being done so I really wanted to share it a lot more widely so today I'm pleased to announce that we are releasing the first experimental support for embedded platforms intensive low light and to show you what I mean here is a demonstration board that I actually have in my pocket and this is a prototype of a development board built by sparkfun and it has a cortex m4 processor with 384 kilobytes of RAM and a whole megabyte of flash storage and it was built by ambach to be extremely low-power drawing less than 1 milliwatt in a lot of cases so it's able to run on a single coin battery like this for many days potentially and I'm actually going to take my life in my hands now by trying a live demo so let us let us see if this is actually it's going to be extremely hard to see unless we dim the lights there we go so what I'm going to be doing here is by saying a particular word and see if it actually lights up the little yellow light you can see the blue LED is flashing that's just telling me that it's running influence so if I saying yes yes I knew I was taking my life into my hands here yes there we go so I'm going to quickly move that out of the spotlight so as you can see it's still far from perfect but it's managing to do a job of recognizing when I say the word and not lighting up when there's unrelated conversations so why is this useful well first this is running entirely locally on the embedded chip so we don't need to have any internet connection so it's a good useful first component of a voice interface system and the model itself isn't quite 13 kilobytes but it is down to 20 kilobytes so it only takes up 20 kilobytes of flash storage on this device and the footprint of the tensorflow light code for microcontrollers is only another 25 kilobytes and it only needs about 30 kilobytes of RAM available to operate so it's within the capabilities of a lot of different embedded devices secondly this is all open source so you can actually grab the code yourself and build it yourself and you can modify it you can actually I'm showing you here on this particular platform but it actually works on a whole bunch of different embedded chips and we really want to see lots more supported so we're keen to work with the community on collaborating to get more devices supported you can also train your own model just something that recognizes yes isn't all that useful but the key thing is that this comes with a tutorial that you can use to actually train your own models and it also comes with a data set of a hundred thousand uh Turin C's of about twenty common words that you use as your training set and that first link there the aiy projects one if you could actually go to that link and contribute your voice to the open data set it should actually increase the size and the quality of the data set that we can actually make available so that would be awesome and you can actually use the same approach to do a lot of different audio recognition to recognize different kinds of sounds and even start to use it for similar signal processing problems like you know things like predictive maintenance so how can you try this out for yourself if you're in the audience here at the end of today you will find that you get a gift box and you actually have one of these in there and all you should need to do is remove the little tab between the battery and it should automatically boot up reflashed with this yes example so you can try it out for yourself and let me know how it goes just say it say yes the tensorflow I liked is the and we also include all the cables so you should be able to just program it yourself through the serial port now these are the first 700 boards ever built so there is a wiring issue so it will drain the battery it won't laugh it would last more like hours than days but that will actually knock on wood be fixed in the final product that's shipping and you should be able to develop with these in the exact same way that you will with the final shipping product and if you're watching at home you can order preorder one of these from Sparkfun right now for I think it's $15 and you'll also find lots of other instructions for other platforms in the documentation so we are trying to support all of the or as many of the modern microcontrollers that are out there that people are using as possible and we welcome collaboration with everybody across the community to help unlock all the creativity that I know is out there and I'm really hoping that I'm going to be spending a lot of my time over the next few months reviewing pull requests and finally this was my first hardware project so I needed a lot of help from a lot of people to actually help bring this prototype together including the TF Lite team especially Raziel rocky dam Tim and Andy mr. Nathan Owen and Jim at sparkfun were lifesavers we literally got these in our hands middle of the day yesterday so the fact that they managed to pull it together is a massive tribute and also Scott Steve Arpit and Andre at ambach who actually designed this process so and helped us get the software going and actually a lot of people are armed as well including a big shout out to Neil and Zach so this is still a very early experiment but I really can't wait to see what people build with this and one final note I will be around to talk about emcee use with anybody who's interested at the breakout session on day two so I'm really looking forward to chatting to everyone thanks Pete we really hope that you try this it's the early stages but you see a huge effort just to make this happen I think we we think that it will be really impactful for everybody now before we go again and I promise is the last thing you hear from me I want to welcome June who's gonna talk about how by using tensorflow light with HTTP use delegate are able to train these teachable machines [Music] thanks Riley oh hi my name is June Takens I'm actually one of the lead software engineers inside of Google's new choral group and I've been asked to give a talk about the edge TPU based teachable machine demo so first I should tell you what coral is coral is a platform for products with on device machine learning using tensorflow and TF light our first two products are a single board computer and a USB stick so what is the edge TPU it's a Google designed ASIC that accelerates inference directly on the devices embedded in it's very fast localizes data to the edge rather than cloud doesn't require a network connection to run and this allows for a whole new range of applications of machine learning so the first product we built is B coral dev board now this runs this is a single board computer with a removable song it runs Linux and Android and the psalm itself has a gigabyte of RAM a quad core a 53 SOC Wi-Fi and Bluetooth and of course the edge TPU and the second is our choral accelerator board now this board is just the edge TPU connected via USB C to whatever development system you need be it a Raspberry Pi or a Linux workstation now this teachable machine shows off a form of edge training traditionally there's three ways to do edge training there's K neurs K nearest neighbors weight imprinting and last layer retraining but for this demo we're actually using became nearest neighbors approach so in this animated gif you can see that the TPU enables very high classification rates the frame rate you see here is actually the rate at which the TPU is classifying the images that I'm showing it in this case you can see that we're getting about 30 frames per second it's essentially real-time classification and with that I actually have one of the are teachable machine demos here so if we can turn this on there we go okay so on this board we have our edge TPU development board assembled with a camera and a series of buttons now each button corresponds with a class and lights up when the model identifies an object from the camera but first we have to plug this in now every time I take a picture by pressing one of these buttons it associates that picture with that particular class and because it's running inference on the edge TPU it lights up immediately so once it's finished booting the first thing I have to do is train it on the background so I'll press this blue button and you can see it immediately turns on this is because again it's doing inference in real time now if I train one of the other buttons using something like a tangerine press it a few times can see it can classify between this tangerine and the background and further I can even grab other objects such as this TF light sticker it looks very similar right it's the same color so let's see what was it okay sorry so now even though it's a similar color it can still discern the tensor was full of flow light logo from the tangerine oh sorry tangerine there we go so you can imagine in a manufacturing context your operators with no knowledge of machine learning or training in machine learning can adapt your system easily and quickly using this exact technique so that's about it for the demo but before I go I should grab the clicker and also I should say we're also giving away some edge TPU accelerators for those of you here today will have one available for you as well and for those of you on the live stream you can purchase one at goop that coral dot with Google com [Applause] [Music] 