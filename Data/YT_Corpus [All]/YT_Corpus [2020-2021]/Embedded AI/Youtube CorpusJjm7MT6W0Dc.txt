 [Music] I'm Sarah I'm the engineering lead for tensorflow light and I'm really happy to be back at i/o again talking about tensor flow line again woohoo thank you to all of you for joining us here today I'm Tim I'm the product manager for tensor flow light and today we're here to tell you about doing machine learning on mobile and IOT devices so I expected most of you here are already familiar with what machine learning is so I won't be going into that well let's talk instead about what is tensor flow tensor flow is Google's open source cross-platform machine learning framework it allows you to build your models and deploy them to servers browsers and all the way to two edge devices it's a full end-to-end ecosystem which goes all the way from research to production from training in the data center to deployment in etch like I said tensorflow has support for multiple languages Swift JavaScript and Python so what is tends to play light and how does it fit in tensorflow light is tensor flows cross-platform framework for deploying ml on mobile devices and embedded systems you can take your existing tensor flow models and convert them over to tensor flow light easily but I wanted to walk you through why this is so important to tensor flow there has been a global explosion in edge ml driven by the need for user experiences that require low latency and closer knit interactions further drivers like poor network connectivity in many geographical regions around the world and user privacy requirements have all fueled the need for ML on device this has led to a whole revolution of machine learning and product innovation in nearly every single industry vertical we see it all over the place driven by on device machine learning running in environments that utilize small compute that have a constrained memory capacity and consume low power so that's why the tensor flow decided to invest heavily in making it easy to develop build and deploy ml that is cross-platform capable tensorflow light can be deployed on Android iOS Linux and other platforms it's easier than ever to use tensorflow and convert your model to tensorflow Lite and deploy it anywhere so at this point you might be wondering what can you do with tensorflow Lite we really want you to be able to solve any kind of problem that you can imagine directly on the device and later in this talk we will be talking about the many ways developers are using tons of flowline but first I want to show you a video of a fun demo that we built which was featured in yesterday's dev note developer keynote this highlights the cutting edge of what is possible with tensorflow lite the demo is called dance like let's roll the video dance like enables you to learn how to dance on a mobile phone since our flow can take our smart phone camera and turn it into a powerful tool for analyzing body posts we have team at Google that had developed an advanced model for doing post segmentation so we were able to take their implementation convert it into temporal light once we had it there we could use it directly to run all the AI and machine learning models to detect body parts it's a very computationally expensive process where we need to use the on device GPU pencil library made it possible so that we can leverage all these resources to compute on the device and give a great user experience teaching people the dance is just the tip of the iceberg anything that involves movement would be a great candidate and so that means people who have skills can teach other people those skills and you know AI is just this layer that really just interfaces between the two things when you empower people to teach people I think that's really when you have something that you know is damn changing [Music] all right cool so to build dance like as I talked about yesterday we built ourselves this audacious goal of running five on-device tasks in parallel in real time without sacrificing performance and I want to walk you through what they were so we're running two body parts segmentation models we're matching the segmentation models in real time we're running dynamic time warping we're playing a video and encoding a video and let me emphasize this again this is all running on device and to show you I'm actually going to do a live demo I spent a lot of Io dancing and if we just cut to the app what you'll see here is there's a few dancers you can choose from so real time in slow-mo I'm gonna select slo-mo because I'm a beginning dancer and so you know I can fire up some dance moves you can see me the pose model running and basically what's happening now is it's segmenting me out from the background and identifying different parts of my body and then as I follow along with the dancer a second segmentation model starts running but this time on the dancer so now there's two segmentation ones running by the GPU and that produces the matching score that you see up in the top right hand corner and what that's doing is giving me some feedback on how well I'm matching the dance the dancer it's pretty cool right but we went further dancing is cool and dancing in slow-mo isn't that cool so what we thought we would do is we would use dynamic time warping to sync my slow mo moves with the real-time dancer and so what you get is an effect where the user all running on device can output this this type of content so you can come and try this in the AI sandbox you can see for yourself you can get a video you can share it and it's really really cool and it's all because of tensorflow lite how awesome was that and props to Tim for agreeing to dancing on stage at i/o and not once but twice besides dancing what are some other use cases that developers are using tons of flow light for the major on device use cases that we see are typically related to image and speech so things like segmentation object detection image classification or speech recognition but we are also seeing a lot of new and emerging use cases come up in the areas around content generation and text prediction tensorflow light is now on more than 2 million devices around the world running on many different apps many of Google's own largest apps are using it as our apps from many other external companies so this is a sampling of some of the apps which are using tons of low light Google photos G board YouTube assistant along with several global companies like uber and airbnb so 10 to flylight also powers ml kit which is our out-of-the-box solution for deploying Google's best proprietary models on device you would have heard about this yesterday too we're also powering that in the backend so now let's move on to how you can get started with using tons of low light yourself it's fairly simple to get started I'm gonna walk you through how you can use an off-the-shelf model or retrain your model or use a custom model that you may have built for your own specific use case with tensorflow light and once you've done that it's really about validation and optimizing your performance for latency size and accuracy so first let's dive into how you can get started as a new user the simplest way to get started is to download a pre trained model from our model repository on tensorflow org we have models for popular use cases like image classification object detection estimating poses and smart reply and this is an area where we plan to keep adding more and more models so please check back often these models that are hosted there are already in the tensor flow light model format so you can use these there in the app now if you did not find a model which is a good use for your which is a good fit for your use case you can try retraining and this technique is also frequently called transfer learning and the idea here is that you can reuse a model that was trained for another task as a starting point for a model for a different task and the reason why this is useful is that training a model from scratch can sometimes takes take days by transfer learning can be done in short order note that if you do retrain a model you will still need to convert that retrain model into tens of lowlights format before you use it in an app and later in this talk I will show you how you do that conversion okay so once we have a model intensive lowlight format how do you use it in an app first you load the model then you pre process your data into a format that your model will accept then you change your application code to invoke the tensorflow Lite inference library and finally you use the result of the inference in your code so let's walk through some code which shows this this is code which was taken from the image classifier example this is hosted on our website it's in Java written for the Android platform you can see that the first thing that we do here is that we load the model and then we construct the tensorflow Lite interpreter we then load the image data and pre process it and you'll notice that we are using a byte buffer and the reason we are doing that is to optimize for performance next step is to run inference and classify the images and that's it that's all you need to do to get an image classifier on Android I won't I don't want to highlight that the example that I have run through is in Java but tensorflow light also has bindings for Objective C C++ Swift as well as Python so the next thing I want to move on to is how you can use your own cost a model with tensorflow light so the high-level steps here are that you train your model with tensorflow you write it out into the same model format and then you would need to convert that into tensorflow light format using tensorflow lights converter and then you make your changes in your app to use the model like I walked you through just now so this is a code snippet showing how you can convert a saved model into tensorflow light model format as you can see it's fairly simple to do it's a matter of constructing the converter with your saved model and then invoking the convert function on it please check out our website for more documentation on this and it also has documentation on how you can do this with tensorflow to dot o which is the newest release coming out from tensorflow speaking of conversion we've heard feedback from our users that tensorflow light conversion is sometimes hard developers sometimes run into issues that the ups that their models are using are not supported with tensorflow light or they might be using semantics which we don't support yet for example control flows rest assured that we've heard this feedback and we are actively working to improve it we are building a brand new converter this converter is based on M Li R which is Google's latest compiler infrastructure and our new converter will be a lot more extensible and easy to use and debug so that was all about how you can convert and deploy your model by tensorflow line I want to walk through some advanced techniques which you may use if they are useful for you so many developers who use tensorflow Lite care deeply about keeping the binary footprints more selective registration is one feature that can really help you and the idea is that you can only link in the ops that your model is using and this thereby makes the size of the binary small so let's see how this works in code you create a custom op resolver and you replace tensorflow lights built-in op resolver and then in your build file you specify your model and the custom opera solver that you just created and the intensive low light will scan over your model create a repository of opps that were created that are used in your model and then when you build the interpreter only those ops are linked and this in turn will reduce the size of the binary another advanced feature that I want to mention is tensorflow select it allows developers to access many of tons of flows ops via tensorflow lite the caveat though is that it does increase the size of the binary but if your use case is not very sensitive to the size of the binary and you are running into issues that you need ops which tensorflow light doesn't support yet I highly recommend that you check this out so this is how it works in code it's a small modification to how you would convert your model to tensorflow light this is pretty much the same code that you would use for normal conversion the only difference here is that in target ops you specify the set of tensorflow select ops you can find a lot more documentation on how to do this on our website it also has information on how this is working under the hood as well as usage examples we get a lot of requests to support control flows in terms of low-light these are constructs like loops and conditionals we're actively working on this and we hope to share more about this in the coming few months with you and the last thing that I want to cover in this section is on device training this is an exciting new area and we believe that this will open up many new opportunities for research and product innovation we are working to add training support to tensorflow Lite and at this point I would guess that this would be available towards the end of the year for developers to try out great thanks Sara so now that you have your model up and running now you need to validate it and get it fast to get started we recommend benchmarking your model with our benchmark tooling this will enable you to validate your models accuracy size and performance and make adjustments depending on the results before I do get into that I wanted to share the key performance goal of tensorflow Lite and that is to make your models run as fast as possible on CPUs GPUs DSPs and NP use fast is what we care about so if you don't know what all those terms mean I'll explain them now most phones have a CPU and many have a GPU and a DSP CPU is typically the best option for simple ml models GPUs are usually really great for high energy processing at fast speeds and DSPs tend to be best for low powered complex models that require very fast execution depends on the use case and experimentation with you but the great thing for tensive low light is that allows you to execute your ml on all of them and our team has worked incredibly hard to have optimal performance across all these different architectures for example mobile net be one achieves eighty-three millisecond inference feed on a pixel three with a single CPU thread drop that to just fifteen milliseconds when you delegate that across to the GPU so we have a lot more CPU optimizations coming in our pipeline to get even better performance across 2019 in addition to more up support on arm and Intel architectures so what about the delegation API is such an important mechanism inside tensorflow but you're probably thinking what is this magical API and how does it work so that the delegation API delegates part of your graph to another executor at runtime it accelerates any or all parts of the graph if it can get better performance and falls back to CPU when it can't so here's a great way to visualize it a graph is made of a series of operations and for operations supported on a particular architecture tensorflow light will accelerate those if you ask it to if certain operations aren't supported on that delegate it will fall back to the cpu automatically so now that we've spoken about the delegation API the Android neural network API uses it to standardize hardware acceleration across the Android ecosystem in P it supports around 30 graph operations and in Q it will have more than 100 and support use cases like image audio speech and others for example you could achieve 9x latency improvement on the ml kit face detection model using the N an API and it's really easy to use you just flip the set use N and a flag to true and you're done you will get acceleration your graph will accelerate where possible using the N and API so the other thing we've done is we release the GPU acceleration ops using OpenGL ES 3.0 or Android and metal shaders for iOS so this will give you a 2 to 7x speed-up in comparison to floating point on the CPU but it does add a tiny bit more to your binary size Oh No and so it's really sorry I went backwards anyway we're working on making the GPU faster is basically what I'm saying here we love feedback so please reach out to us and let us know what's important so the edge TPU is another example of the delegate delegation API working with a custom ml accelerator that has high performance low powered acceleration at the edge it accelerates tensorflow light models and you can find out more in the edge TPU talk tomorrow so now we've got a really exciting announcement around DSP performance we've partnered with Qualcomm to enable DSP delegation through tensorflow light directly for their 600 to 800 series devices which is hundreds of millions of devices while we recommend that you use the nn API in android q and beyond this announcement just gives us another option for accelerating your models on the DSP you'll be able to include a Qualcomm signed binary and rely on the delegation API for acceleration and we hope to have this released later this summer so you're probably wondering well what type of performance can I get on the DSP you can get up to an 8 point 3 X speed-up delegating over to the DSP on Qualcomm devices which is an incredible performance boost that we are excited to bring to tensorflow Lite this is just another example of how we are enabling more accelerators to work with your ml models lastly as I talked about earlier you want to ensure that you are benchmarking and validating your models and so we offer some very simple tooling to enable this for threading and powerup profiling and here is a way to execute the perab profiling by the command line with basil and adb so this is basically what you get as an output when you are doing powerup profiling it really does enable you to narrow down your graph execution and then go back and tune performance bottlenecks now let's talk about optimizing your graph using the tensor flow model optimization toolkit we offer a simple to quick to optimize your graphs and enable them to run faster at a smaller size for those that already understand we're adding more techniques for during and post-training quantization and if none of these concepts are familiar to you don't worry I'll explain so what is quantization you might be wondering quantization is really just the reduction in precision and weights of activations in your graph essentially reducing from floating point to integer base numbers the reason this works so well is that we try to optimize the heaviest computations in lower precision but preserve the most sensitive ones with higher precision so there's no accuracy loss there are variants post training occurs once you have an outputted graph and during training which preserves the forward pass and matches precision for both training and inference so now you know what quantization is the goal of the toolkit is really to make your graphs run faster and be smaller by abstracting all the complexity involved with all these different techniques so we strongly recommend that you start with post training quantization as it's a simple flag flip to utilize it and you can see and you can see what type of performance improvements you can achieve we've seen up to a 4x reduction in model size 10 to 50% faster execution for convolutional models and 3x improvements on fully connected and RNN based models on the cpu and this is how simple it is to do it it really is just a simple flag flip so in the coming months we'll be improving our support for quantization with Karis enabling post-training quantization with fixed point math and adding more advanced techniques like connection pruning and sparsity support but one thing we really want to emphasize is that post-training quantization is really almost as good as during training quantization if you're an advanced practitioner have and have access to the model and the training data then during training quantization might be for you but for most post training quantization can all almost have the exact same effect on size and accuracy so here you can actually see the difference and there's really only marginal differences between during training quantization versus post training quantization so again please try and start with post quite close to training quantization first thanks Tim so is tensorflow light only for mobile phones it is not tensorflow light is already being used in many many products which are not phones it is being used in smart speakers smart mirrors vacuum cleaners in even small space satellites from NASA it's because of this demand that we see from our developers and also the fact that there are a huge number of microcontrollers that are out there we've decided to invest in making tons of low light even lighter and suitable for use on these platforms ok so let's first talk about what is a microcontroller they're essentially small computers on a single circuit they typically don't run an operating system they have very limited RAM and flash usually just tens of kilobytes of it and they only have memory CPUs and perhaps some peripherals and the way microcontrollers are used many times is that they are used in a cascading fashion so they perform lightweight processing and based on the result of that it triggers heavier processing on some more powerful hardware so as an example my microcontroller can be checking to see if there is any sound and this in turn can trigger processing on a second microcontroller with which which would check if the detected sound was human speech and this in turn would trigger processing on a heavier application processor so at a high level the architecture for tensorflow light for microcontrollers is the same as what we have for tens of low light we use the same model format and we used the same conversion process the only difference here is that the interpreter is a stripped down lightweight version of tensorflow lights interpreter we've been working on getting example models ready for you to use on microcontrollers we have one already on the website for speech and there's another one for image classification that is coming out soon and these models have to be pretty small too as you can imagine if you go to our site you'll find instructions on how you can use these models and if you are here at sorry and it also has suggestions for how you can procure hardware to get started this brings me to another exciting announcement we are happy to announce a closer collaboration with arm on the development of tensor flow light for microcontrollers arm is a well-established and respected leader in this space and we are very excited to work closely with them we will be working closely with arm on the development of models framework design as well as performance optimizations the embed community is the largest community of developers in the space and we will be integrating deeply with the tooling there to make tensorflow light easy and performant for them to use this is a relatively new effort and we would love to work closely with the tensorflow community to make this successful so please send us your feedback ideas and also code contributions and if you are here at i/o you can go to the code labs area where you can try running tensorflow light on a microcontroller yourself so where can you go and learn more about tensorflow light and everything we've shared today so the first thing is we really listened to you and heard that our documentation just wasn't good enough so we've worked really hard on making it a lot better so you have the resources you need to develop what you want to do we've tensorflow light we have new tutorials better demos and a new model repository available and live right now just go to tensorflow org slash light to get started and you'll see our revamped website that makes it easy to navigate and find what you're looking for now as an open source product we're working hard to engage with the community and be even more transparent about where we're headed that's why we've published our 2019 roadmap on tensorflow org slash light so you have visibility into our priorities so please fill to check it out and give us feedback we also have new code samples and models on the site for common use cases like image classification object detection and the others that you see here so now we're excited to show one last demo which is very unique fun and shows off the performance of tensorflow light in a non mobile environment so sara is going to be the star of the show for this demo which we'll look at the virtual Tryon of glasses and hair color and then we'll take a look at what's going on behind the scenes so Sara come on up and let's do this so as you can see this is a smart mirror this was built by one of our developers as you can see this is a touchless mirror which is operated only by hand gestures it's running the Kerr OS operating system running on Qualcomm hardware all the machine learning on this mirror is powered by tensorflow light and we are accelerating it on the GPU to get optimal performance okay let's give this a whirl all right so the first trial experience we're going to show you is realistic virtual Tryon of eyeglasses so as you can see Sara doesn't need to touch the mirror since it works through a touchless interaction the embedded Google i/o technology runs in real-time and has driven end to end with GPU acceleration for model inference and rendering as glasses are looking awesome so now let's try maybe Harry coloring the blue oh that looks awesome this is enabled by state-of-the-art segmentation model that predicts for every pixel the confidence of being part of the users hair or not Sarah look great so now why don't we take a look inside what's going on behind the scenes to achieve this experience so for every frame a tensorflow high fidelity geometry model is being run to predict over 400 points on the face and it even works for multiple people I'll show you so all up this is an awesome example of on device ml using tensor flow light on a non mobile device you can check out these segmentation models that we have available on tensor flow dot org slash light and if you'd like to find out more about the mirror please come up to us after the show and we can direct you where to get more information that's all we have folks please try out TF light if you haven't done so already we would not have gotten to this point if it wasn't for our developer community which has helped us a lot with feedback as well as code contributions so thank you a lot we're really grateful so a few of us will be at the AI sandbox today and tomorrow please come by to have a chat and try out one of our demos will also be at office hours today at 10:30 a.m. right after this talk thank you very much thank you [Music] you [Music] 