 good afternoon everyone is everybody doing having a great built conference so far awesome awesome my name is Krishna Mahmud apakah I'm a principal program manager on the azure stream analytics team and today we are here to talk about session BRK 3:05 weight from cloud to intelligent edge driving real-time ditions with Azure stream analytics and I do realize that it is a great big conference there are lots of other events that are going on simultaneously but I do thank you for making time to attend this session and today I'm going to be joined here by my colleague John Sebastian Bruner from the stream analytics team and then we have a very special customer speaker from down a group in Australia Stuart Trotter who is a senior IT director there and together we hope to bring you a very interesting information and most importantly session which is probably very actionable for you something that you can take back into your regular lives and implement some of the learnings from here so before we go into the agenda let's just do a quick show of hands in terms of how many here have actually used stream analytics in the past I would say about 35 40 percent of the room so I'm hoping that in the agenda we have something for everyone right so we are going to spend some time talking about the whole concept of real-time analytics what is really real-time analytics then I'm going to go into some of the key concepts of a gist stream analytics and we will do that through some interesting demos let's just hope the demos work the next part is I will actually hand it over to Stuart who is going to talk to you about how is downer industries down a group implementing as a stream analytics in some of their mission-critical scenarios then Jess will come over and talk to you about how can stream analytics be a core part of your Big Data ecosystems and he will also talk to you about some of the key announcements that we are making today at this particular event and we hope to have about five minutes at the end for a Q&A my earnest request for you is to actually hold off on your questions because we have quite a bit of surface area to cover until the last five minutes and we promise we will answer each and every question if not right here on the podium we will be outside to take all your questions is that Alright excellent thank you so let me just get started talking to you about what is real-time analytics so I believe the word real-time analytics is fairly used and abused by various different technologies various different companies but being a purist the way I define real-time analytics is really comparing and contrasting it with traditional batch analytics so in traditional analytics what happens you have data you bring the data you put it in some kind of table a database a worksheet you write your query you bring the query to the data and then you glean insights from the data but in stream analytics you actually reverse it rather than taking your query to the data you are bringing your data to the query let that just sink in a little bit rather than taking your query to the data you are bringing data to the query the query is always on and live on the wire so to say and as the data is flowing through the wire we are trying to identify certain trends and anomalies that the businesses can leverage let me give you a very simple example we've all been to supermarkets we've all paid at the point-of-sale terminals so when you go to a supermarket and then you swipe your credit card within one or two seconds you get a response back if that transaction has gone through or not do you think the credit card gateway company has all the time in the world to take the transaction put it in a database somewhere check it if there is enough balance left on your account and maybe also run some machine learning algorithms on that transaction to see if it could be a fraudulent transaction chances are no they are probably intercepting that event and doing some analytics on it on the fly so that you get a response back within one or two seconds and those are really the scenarios where we are seeing more and more adoption of real-time analytics there are more examples that I'm going to give as we kind of move along in the last decade or so we have seen an exponential growth in real-time analytics across various different industries whether it is remote monitoring predictive maintenance in the IOT scenarios remote patient monitoring remote device calibration in healthcare remote monitoring of the power grid in the case of power in utilities lots of companies want to ensure that their workers or safety so the lone worker safety and continuously tracking your assets as they move through the supply chain those kind of scenarios are some of the key examples of real-time analytics also if you look at financial services fraud detection as well as high-frequency trading these are all you know different scenarios where you can actually apply real-time analytics for your business advantage having defined and gone through some of these key scenarios where you can apply real-time analytics let me start talking about Azure stream analytics so as a stream analytics is a fully managed past service on Azure for you to do your real-time analytics and real-time data processing so what really differentiates Azure stream analytics why do customers really Lau stream analytics first and foremost it really comes to ease of getting started so - and a half years ago when we were still on the drawing board trying to figure out what kind of language head to provide on top of this really powerful real-time analytics engine we looked around whether it was our competitors on the open source side or on the proprietary side everybody required the customers to use some kind of imperative coding language but Microsoft always believes in making things as easy as possible to our customers so we said let's make a bold bet let's take a bold bet let's put a sequel language head on top of this really powerful real-time analytics engine and fast forward two and a half years when we look around our competitors whether they are coming the open-source side or on the proprietary side or trying to emulate the success that we've had with this approach whether it is CAFTA with case equal whether it is part with sparks equal or even other competitors they are trying to provide a sequel like language so we feel pretty good about the bet that we had taken three years ago and kind of lead that we have got in the market developer productivity we integrate with more than 15 services on Azure and us customers can integrate with services downstream and upstream from stream analytics without writing even a single line of code it really goes to talk about the time to value and the ROI that you can achieve through using stream analytics intelligent cloud and edge stream analytics you can deploy in the cloud or on azure IOT runtime running on IOT edge and our differentiator there is that it is exactly the same language whether you write it for the cloud or on the edge and it is very different again from the competition which requires you to write a very different language on the edge versus in the cloud probably because their technologies evolved through certain acquisitions which are not properly integrated in things like that we are a fully managed platform as a service there are no clusters for you to manage there are no beams for you to provision you only pay for the time that your service your job is actually running and VRD cheapest service in the market we start at 11 cents an hour for the smallest computer and memory footprint and we our enterprise ready we have a whole lot of certifications whether it is ISO PCI sock HIPAA so that we cater to all the industries all the customer segments and we have a financially bagged 99.9% SLA guarantees and for the time that we are not providing that SLS we automatically refund the money back for the time the service was not providing the necessary and guaranteed SLA so this is how end-to-end canonical architecture for stream analytics looks like we will go into deep deeper level when Jess comes over comes and talks to you about some of the key architectures that are possible with stream analytics but at a very high level you have devices and applications on the left that are continuously generating the data we pulled the data from event hubs or IOT hubs depending on the scenario we provide integration with Azure machine learning service so that you can do real-time scoring using pre trained machine learning models and once the data is processed we can store the data in services like Azure blob storage Azure data Lake cosmos DB sequel DB sequel data warehouse you can power real-time dashboard with power bi we work very closely with the power bi team in order to enable dynamic dashboarding experiences there and I will show you what I mean through some demos very soon and then a very important part of what happens downstream is once you detect that there was some kind of anomaly in the system that you're monitoring you want to be able to take some action maybe you want to send a SMS message maybe you want to send an email maybe we want you want to trigger a workflow maybe create a ServiceNow helpdesk ticket so all that is possible through our composable service with services downstream like as a function service best topic and so on so having talked about the service at a very high level what I will do now is kind of short very quick demo we will start it in a perfect cooking show fashion we will show the end product working and then we will try to disassemble it and then figure out how individual components in that in this demo have been developed ok and the scenario here is we will use the public data set that is provided by the New York City taxi cabs and we will replay it so that it is kind of a mocking a real-time scenario and then I will show some of the key concepts as we kind of walk through this particular demo so give me a second for the dashboard to load okay so what you see in the screen here is the real-time view a mocked up real-time view of New York City taxicab data and this should be a dynamic dashboard in a little bit you will see actually the data updating so let me get you situated on what is it that you are actually seeing here on the dashboard so the first tile here is showing the number of pickups that are currently happening in different regions of Manhattan okay so I can as a taxicab company owner I can see how many pickups are happening in individual regions in Manhattan the purpose of this is if I see that there is the there is a little bit of fewer pickups that are happening in a particular region then I can send more cabs into that particular area so that they can balance the demand and supply properly here I'm showing the number of average number of passengers per trip here I'm showing the number of drop-offs that are happening around the Microsoft Store in Time Square in Manhattan right in the last 1 minute how many drops are happening have happened within 50 meter radius of the Microsoft Store in Manhattan here I'm showing the in seconds what is average trip duration and here in this tile a real-time view of my current pickups versus my historical pickups overall just to compare in contrast how is my business doing as compared to historical average another thing that we are showing here is the number of unsafe incidents by driver and I will come to it how we are doing all of this in just a moment so by medallion number of the driver how many unsafe incidents has that driver been a part of unsafe incidents could be collisions or it could be certain braking rapid acceleration although things and we will do that through some interesting IOT based scenarios all right sorry just give me a second all right so before I show you the code that actually went into developing these dashboards let us get grounded into the architecture that is actually powering this demo so we are getting data from the New York City tax taxicab leak data set which we are replaying in a way that it appears as if it is real time data if we had access to a real-time API it could very well be real-time data we also have certain devices that are on each of these taxi gaps that are showing unsafe driving incidents and we are emulating that through a real Raspberry Pi which I will show you in just a little bit so all the data is coming into the cloud through event hubs and IOT hubs we also have data augmentation right stream augmentation that is happening through sequel so we have different geofences different parts of Manhattan defined in wkt or well-known text format in sequel and we are joining that with the real-time data that is flowing through and then we have historical data that has been processed through SPARC systems in the back and in order to compare the current pick up trends with the historical trends the data is processed by stream analytics and then we are showing the data in power bi you can write certain alerts to Azure functions in order to trigger alerts and of course the processed data is continuously being pushed out in Parque native Parque format which is a new announcement that we are doing this week so that it gets processed and gets fed back into stream analytics in order to compare current historical trends with the past let me talk about built in geospatial functions energy stream analytics we've been seeing a huge demand huge growth in the connected car scenario in ride-sharing scenarios from various different customers both automotive OEMs ride-sharing companies etcetera and in order to make it a lot easier for them to actualize geospatial scenarios we introduced built in geospatial functions that you might be familiar exist in the sequel world in Azure stream analytics so using very simple functions like create point which helps you define different points or define the location of the asset in a supply chain you can define the geospatial coordinates of these assets create polygons helps you define geofences for various different areas that you want to continuously monitor st underscore distance helps you measure monitor distance between multiple assets at any given point of time everything happening in real time HD within it helps you determine if a particular asset that you're tracking is it within a geofence or is it outside of the geofence HT overlaps helps you determine if there is an overlap between multiple geo fences or not so let me show you using those functions a very simple query that we had to write in order to build this very informative real-time dashboard showing the fleet management status of my taxicabs so I will show you the code using vs code Visual Studio code so this is then another announcement that we are making today stream analytics add-in for visual studio code is available as we speak so if you are interested we will send you some links and we will point you to the resources where you can go and download and play around with this feature so in the first part of the query here all I am doing is getting the pickup and drop of latitude and longitude information so that I can continuously track where the pickups and drop-offs are happening in the second part of the query as I said HT underscore distance function can be used to calculate distance between any two coordinates at any given point of time we are continuously monitoring the distance between the drop of point and a fixed coordinate of Microsoft store right I know the Microsoft store in Times Square has coordinates of forty point seven and 73-70 three point nine eight so I'm continuously monitoring the distance of the drop of from the Microsoft Store in the third part of my query here I'm only measuring those drop-offs that have happened within 500 feet of the Microsoft Store so I'm in effect filtering down the data so that I can only project those drop-offs that are happening within a particular distance of my Microsoft store in this particular part of the query what I am doing is I do have data in sequel right which I am using as reference data for stream augmentation so this has geofence information for different regions in Manhattan so using wkt format which is a very frequently used format in the geospatial circles we were able to define all these different geo fences that are representative of different areas in Manhattan and we are joining this information with the drop of information so that we know exactly how many drops are happening in which parts of Manhattan in this part of the query here we are sending in parque format this aggregated data into Azure blob where it is being processed by spark in batch mode so that we can compare the historical numbers continuously with the real-time numbers so we are also announcing today that we have full support native support for spark egress from edge stream analytics so if here I am configuring an output to Azure blob storage and you as you can see here sorry it is a little light because the job is running but you can now serialize your data natively in parque format so that if you are writing data to your SPARC subsystems things can be much more efficient there so with this investment we are getting a lot more closer to your big data ecosystems okay so for the next part of the demo I will move slightly into the IOT side of the world like I said we have certain devices let's say we have it in all the fleet of vehicles that the company runs and the reason for doing that is we want to continuously track the safe driving practices of the drivers and also give them a way to understand and you know self-correct some of these unsafe incidents that they are probably involving in so the architecture for this part of the demo is something like this I have a Raspberry Pi and on the Raspberry Pi I have deployed as a stream analytics on IOT edge runtime so that the stream analytics job can work in a completely disconnected or offline mode because I cannot guarantee that these taxes are always connected to the Internet but I want this logic to be always executing so we can for that reasons exactly deploy the stream analytics job on your devices that are running ith runtime we have a sensor that is constantly looking at the accelerometer readings XYZ coordinates and I have a c-sharp UDF user-defined function you can extend the sequal language that is supported in streamline lytx with c-sharp UDF so for example here we are using a very complex trigonometric calculation to calculate the tilt from pitch and roll as given by the raspberry PI's sensor so in order to do that we are making a simple call to a c-sharp user-defined function which shows the extensibility the C sharp UDF's are available both in the cloud and on the edge currently available on the agent we are going to make an announcement about the cloud zone stream analytics available in cloud edge and we are making the announcement this week that we are also a part of the azure stack that is running a IOT edge runtime another concept that I want to talk to you about is the built in anomaly detection models that is available in Azure stream analytics so as I said we have integration with the azure machine learning but customers told us that they probably need some help in order to work with anomalies machine learning models that are much more common among the community so we work very closely with the Microsoft Research and we got models that are now baked into edge stream analytics so that you don't have to invest in our data scientist to develop these machine learning models for real-time purposes and the whole complexity of defining training and developing your machine learning models is now reduced to a simple function call so we can identify anomalies like spikes and dips and also slow increase in slow decreases and non momentary anomalies let's say you are monitoring the VM continuously for memory leak this is not an anomaly that happens suddenly it slowly creeps upon you so we can also help you identify anomalies of that nature and all you need to do is write a simple function signature in order to invoke these machine learning algorithms the scalar expression is the attribute on which you want to run your anomaly it is it might be a field that is coming directly from your data or it could be a field that you might have computed as a part of the query confidence level it actually sets the sensitivity of the model higher the confidence level fewer the anomalies lower the confidence level more the anomalies history sighs like I said these are these are not pre trained models these learn and score in line with history size you are indicating to the model how many events to learn from before the scoring starts and partition by it is a very powerful feature let's say using a single query you are monitoring thousand devices right so when you add that partition by clause to your query we will spin up thousand different models in memory and each model monitoring each tell mitrice trim separately so that you can find that needle in the haystack in a very easy fashion so with that let me quickly go to the second part of the demo here okay so what you see here is a real-time view of the telemetry that is coming from the Raspberry Pi that is on my desk here so if I move the Raspberry Pi you should very soon see anomaly trigger both at the device level as well as you should see the number 13 increment to 14 because it might take a few seconds given the network connectivity as you can see the driver gets automatic alert saying that hey there is an anomaly you've been in some kind of awkward driving situation and the power bi dashboard which can also be provided and packaged as a mobile app for the driver they can continuously see some relevant information like how many number of trips that they have done in this particular shift how many unsafe driving incidents have they been involved in and so on all right so you've seen some of the key capabilities that are available energy stream analytics some of the built-in and analytic functions and so on we today have thousands of customers who are actually taking a big bet and our stream analytics for various mission-critical scenarios and we have this customers come from various parts of the world from various industry verticals from various segments as you can see and one such customer who is doing some very innovative things with Azure stream analytics happens to be downer industries in Australia and before I hand off to Stuart to talk about how they are using gaseous stream analytics let me quickly play a video that talks through what is download industries Stuart sometimes the world just happens while we're happily unaware of the momentum that's going on behind the scenes at Donna we're not often seen in the limelight which is fine by us we're focused on helping our customers make people's lives better every single day we're a leading provider of integrated services in Australia and New Zealand exploring new technologies and innovative approaches while providing our customers and the community with services that they rely on we don't simply deliver services we manage our customers assets and build relationships that create success harnessing digital technologies to enhance our end-to-end service capabilities while supporting our communities at downer we're looking ahead doing what we do best so you can get on with life good afternoon everyone my name is Roger I'm this software development manager for the ANA digital data services and part of the down'n group in Australia and hopefully what you can take away from that video is a sense of the diversity of services that we offer everything from building trains and roads to renewable energy and defense and what we've learned from that is there's quite a few challenges that are common across those divisions within the business the first one is that there's a real driver to move away from kind of schedule based maintenance to condition based maintenance and the other one is that our customers these days are a lot more discerning and are really asking for smarter solutions from our business so with that in mind about three years ago my team and I started I started looking at building and platform and is a self-service IOT big data analytics platform called near averse and it's built in net on top of a whole heap of repairs services and we've started rolling that out we started rolling there down through the business and tackling a few a few of these challenges and what I want to talk to you a bit about today is what is what we're doing with has our stream analytics within the platform to address some of the issues in the business - of the projects I'm going to talk about is train DNA and rams I'm probably going to focus more on train DNA and train DNA is really about a solution that we've delivered to our rolling stock services division and what they do is they build trains and then they maintain them and they've got really long contracts like 30-year contracts and they have a real need to move to condition based maintenance but also to continually improve on the delivery of safety and reliability for their customers what we've done initially now is we've brought on the Waratah fleet in sydney and that fleet consists of about 80 passenger trains and those trains give us about 31,000 signals per train every 10 minutes that we ingest into the into the platform and we've also got about six years worth of history that we've brought in as well so we've got a wealth of data to draw from and to give you an example of what we're doing with stream analytics there we have a safety use case around batteries and that I'll describe now so trains are have got heaps of batteries onboard and in Sydney and the hotter months they they're susceptible to a condition called thermal runaway where basically there's a chemical reaction that occurs within the battery and that reaction is self-perpetuating and the battery gets hotter and hotter until it becomes a real fire hazard on the on the train what makes it even more difficult is when you look at the data there's a lot of false positives coming through and so it's quite hard to actually pinpoint when this is an actual event so what we've done with stream analytics and what we're able to do there is look at a six hour time window and within that window we're able to eliminate the the false positives that are coming in and detect the very initial signs of a true thermal runaway event at that point we use those functions to send a notification out at fleet engineers they are able to dispatch a field technician out to the Train at the guy hops on board he isolates the battery making the train safe and then that batteries swapped out the next time the train goes into the maintenance yard then we have rounds and what happened with rams is our contract mining business came to us and said look we really need a lightweight fleet management system for a mine out in Queensland so they've got an open cut mine there and they want to monitor their their fleet of haul trucks and shovels so what we're doing there is we're taking about 200 signals per vehicle every second into the platform and a handful of signals per tire as well every 10 seconds what that gives us is the ability to monitor not only the health of each individual vehicle but actually the performance of the fleet as a whole so we can track you know payloads that are being loaded into the haul trucks where they're going in the pit and so on which has been a game changer because we can actually track all of that now in real time for our customer okay so I want to talk a little bit about the architecture so this is this is train DNA so in the left hand side we've got the train itself producing about 31,000 signals and that's being collected by another down a system called MFA I guess and then we take a feed out of there into a device simulation service it has some very lightweight transformations on the data before sending that up to azure IOT hub and then we have a suite of stream analytics jobs that are consuming those messages and they're performing a bunch of analytics such as the you know the thermal runaway use case with monitoring airbags and the suspension we're monitoring exhilarate our supplies all sorts of things we're also using the geospatial queries to enrich the data as well and so for example we detect when a train has been certain maintenance yard for a while and we flag it as being in maintenance as opposed to an operation in addition to that we're using a lot of time windows one of the time windows that we're using is actually looking for the absence of data for a train so we can basically detect when the train has lost communication with the network and we can get someone out to investigate as I mentioned before we're using Azure functions as an output when we need to send a notification to the engineers and very importantly for us stream analytics enables a lambda architecture as well so you can see on the right there we're outputting to both as your sequel database and as your data lag store and the way that works is within the job we select a subset of the signals that we require for real-time reporting we dump those into sequel and then we take all of the signals and we stick them into into the data lake where they're partitioned up above with got service fabrics our whole platform is underpinned really by by service fabric and in this case here it's actually managing the data retention policies that we enforce on on our sequel databases so the users can basically define windows of data that they want to retain within sequel and we truncate the rest and then on the far right hand side we've got PI bi embedded taking data straight out of sequel for the near real-time reporting and we're also pushing data into Azure analysis services and providing models to our users for self-service reporting as well and then lastly down the bottom we've got Azure data bricks which is running our sequel jobs both scheduled and and ad hoc against the data lik just a very quick one on ram's surrounds the architecture is very similar slightly slightly simpler but on here you can see some of the signals from the truck actually go directly to the IOT hub by our agent mesh network and a secure VPN and then we've got our stream jobs again and again we're making heavy use of geospatial queries good example in this case is we're actually suppressing alerts when they're not useful so if a truck has got a flat tire and it's set in a workshop we don't want to be sending alerts out so jump into a quick quick demo it's quite a feedback science with this straight after here all right and I said well we might get to dinner a bit later and let's just talk about why we chose a si in the first place so very important for us is it's a platform service we've got a very small team and historically we've worked a lot with with VMs but at this sort of scale for us to actually maintain as a whole bunch of VMs would be nigh on impossible so we really really focus on platform services to help us out and ease of adoptability we've got a large user base that has a has a lot of sequel skills and background so having a service that supports a sequel like syntax it's really important native integration obviously with key Azure services like IOT hub data like store functions ml is important ease of scalability I'll talk about deploying to the edge in a minute on the next slide and then the last one was really important for us is the data mean ability because we offer this feature as a self-service option within the platform but for us time series data is immutable so we needed a service that was going to allow us to support that and a sa does that out the box by ensuring that when we outputting to sequel and velak store we can only do inserts so next steps for us edge analytics we basically want to start deploying stream jobs down onto the train and monitoring we want to be monitoring sub sub second data on the train itself and raising those low latency notifications to have the driver in the guard that are on the train and what we're also going to do with edge analytics is we have another system another project that's on-the-go called track DNA and we're where we basically we build railway lines as well and do the signaling and so we want to get involved in the signaling there but we need it needs to be robust and highly available so we're going to deploy to the edge and if the site goes offline they're still getting their alerts and their analytics trained DNA we're expanding we're adding extra fleets this year voted on a hololens POC there where we're actually streaming live telemetry down to the hololens through stream analytics smart facilities we're connecting up a whole bunch of mining camps through the to the platform renewables DNA we're building one of Australia's largest solar farms and connecting that to near averse as well and then we're rolling out that ramp solution to our roads business who build Australia's roads and have a whole fleet of vehicles that they need to manage okay right so in lieu of a demo this is kind of a view of the alerts for the warrior triathletes so we can see the the fleet and where they are we can get a view of the statuses of each of the trains if you click the eye there you get an idea of all the business rules that are active that are all backed by stream analytics and then we can drill down into a specific train have a look at the alerts that have been raised and basically scan through the use case recommended actions that sort of thing from there Engineers can actually drill down to specific signals and if they really want to they can hop into a notebook and do some analysis I just want to show you this as well so in visual studio we can monitor the metrics for for the jobs we can do this in the portal as well but it looks cooler in yes so here we're looking at inputs outputted ents we're looking at the utilization as well so this one's running between 30 and 40 percent which is pretty pretty good and then in Azure in the portal we've that we're looking at the same stream job here but here we're looking at the alerts so these alerts are monitoring those same metrics and basically telling us if if we lose our input events for a period of time we need to send out emails sms's and so on to the team if our utilization exceeds 75% for more than five minutes then again we need to get some so that's it for me thank you very much so jess is going to talk to you a bit about stream analytics and big data thank you everyone I'm Justin veneer I'm a program manager for registra magnetics and I want to take a step back after this is a great scenarios like just show how you can use traumatic for your scenarios so you should we show the diagram before I just have another version here or where I separate the hot pass above hot pass analytics where you want to have second or subsequent analytics and the cool pass or warm pass and of course you can combine all of this in your capture so let's see step by step how to build this kind of architecture for your product and make something very similar to what we saw before for Trend DNA or in the demo before so I just want to start by the stream ingestion so stream antics take data from event apps including event of Kafka so you can take any device that's in Kafka data and take it to a bathtub and then to schematics IOT hub if you need a bad directional connection to talk back to a device or blob storage data leak and we support different formats so we support out of the box Avro CSV and JSON and today we are very glad to announce we have custom visualizations eShop that is available in preview that means you can take any form at any weight you can never see up the LL or sis your project that will open it so think about protobuf xml any kind of binary format you have can be open with genetics which one screenshot in just one second and I want to talk about scale as well so because traumatic sea is all about scale I will give some numbers later but you can paralyze the job to make very very powerful and parallel pipeline because you can scale up and I'm up here also two unknowns that we have automatic partitioning now out of the box with compatib leader than one-point-two so contrary to before where you needed to do like partition by manually we will find automatically when there's a natural partition and partition it for you so it's much easier to scale out that being said just want to show you the project to create custom visualization you can see we have a specific project with a template in Visual Studio and the template is already documented here and so you just have to fill your code and it's plugged directly instrument X it will run in memory in the cloud super efficient so it used to be only available on the edge no it runs the cloud as well super efficient and can open any format so it was step one in just data so let's move to step two and rich data so we saw that in the demo before when Krishna show how we can read data with a neighborhood from New York the Manhattan neighborhood in develop it Katie but what is unique with dramatics is you can take this slow moving data its static or its removing their rights refresh up to one one time for a minute and we actually provide full repeatability so I have an example here if you think about Finance like you may want to compare your transaction with currency rate but you want to replace the data you want to make sure you always have the same results because at the time of one of the first transaction in purple here you had the first currency rates but later in the day the currency rate change so when you replace it this data you want to make sure this reference data is completely version version and we do that automatically automatically for you every result is always repeatable we all pass if you run now if or if you run in the past we will issue the same result full repeatability I want to highlight that because in term of mission-critical this is very important and of course it is loading in memory so it super fast doesn't slow your stream so it was step two once the data is done rich with Sakura blob storage then you can run the full stream processing so we show that we show that before we show the video special analytics anomaly detection but we have a full secure language and it's super efficient because of the parallel processing I mentioned before but today and just today we announce we can a 40% bigger scale and we are talking about 13 million even per minutes about certain gig per minute which is about 220,000 evan per second so that's a quite big data I think about all the scenario and probably this data of course the exact number will depend of the complexity of the quays exact event you have but think about all you can do is 12 million or 13 million event per minute is quite interesting so 40% more scale compared to before and will continue at scale we really target your big data pipelines and again support sequel language on the left hand side you can see like if you want to do windows we just extended the group by 60 sequel statement and from the previous tumbling windows where you can have a count over 10 second very readable three lines of code if you compare to other programming language do a stateless wario quits the last stage full story state full function will be much more complicated so stream processing is very very scary scalable stream processing and after you have to choice so let's start by the hot path so the hot past usually can can pour some real-time dashboard so we implement what you call the streaming data set from power bi so that the dashboard is true before in the demo with the taxi so it's a push to power bi and basically it's refresh at the subsequent level you don't need to have a kind of scheduled refresh Romania to refresh it just continued to update we are one of the only product to actually have this integration in this direction it's push integration but also you can have alerts on actions so the best way to do that is to connect to message verse for example area function or event hubs when you connect to them you can create an alert says an SMS text or a mail or trigger an action back to the device when something is over eating for example you can stop the device or do something like this so subsequent answer and you can also add send that back to the edge if you need so that's a hot bus and you can connect to other other services in Asia and close the loop what you call a digital feedback loop and next step is in the coal pass so that can be done in parallel with some jobs and you connect to multiple outputs here I just chose few of them like blob storage cosmos DB sequel sequel DW so very efficient and Lola MC connection to this output and you can store data for a long time then for for reporting of or if you go to storage blob storage you can actually train machine running model using Apache spark initial data breaks or energy inside so thing about all this and I will show before for example the scenario from from the taxi of New York or trend DNA where all the data is creating some alerts in the hot path but we are also storing years of data in the cold path so we can train the model refine the quarry and when we are ready we can deploy all this data to the streaming analytics so it's really two different paths but they are all connected together because this is what Porter will tap roads this is what pourer actually the insights you will have but you need data scientist or you need another team to create this insight from much more than just a real Tonetta like historical data but one or two years of data and I want to highlight to again krishna mention it before the new output to Parker that makes this very efficient because 4k is like the de-facto stoner for big data analytics so by outputting porque no can have Apache spark running some super efficient query so this is a non-student preview and you can access from today I will give you the information in the in one of the next slide so that basically like the few steps to make your project using dramatics and the rest of usual projects but you can even make that easier so you have all these path services but the next step is how can I actually make that in in seconds so in term of ease of use I just want to show the integration with event tab so let me just switch to this PC which went to sleep so I just want once again sorry this screen switching is little slower than it should be so here I'm in Avon so that's the event tab that is powering the first demo we saw is a taxi so you probably recognize the event of a porter here we are announcing a new integration with Romantics here it's called processed data that is in event a portal so when you have your data flowing to the cloud you click here and the computer just coming from sleeps little slow and you have a few way to interact with your data and schematic is the first one to integrate here and in one click here if I click on explore I will be able to see the data flowing in my event herbs that are coming from the taxi taxi demo so we are connecting to another brightener and this is done it should be happening the more sorry for that we are fetching the data and oh here we go and you can see the data is flowing we sample some of the one so you can see the shape of the data you can see the schema here you can see I didn't do anything I just click here one click and after you can actually create some queries so for example I have this query I can say ok select everything from this and I just want to filter where for example the distance is greater than 2 sorry my keyboard is jumping everywhere and you can interact I create your query here I didn't even create this Romantics job test the query and we do that on the fly on the stream of data so this is the non student preview we're still refining the speed as you can see that it takes few seconds but when it's productized it's real time it's just the testing it's a little slow you see that we have the data here we've filter everything greater than 2 and then we can refine your query and when you are ready to deploy one more click here you can deploy the query to stream analytics crater job it will run 24/7 and listen to your data and react whenever the condition are met so brand-new integration today and just to summarize all the things I mentioned today so we have this slide and I need to go back to this story we have this like showing all the announcement in the last few months but I want to focus on the one we announced today so just to summarize 40% more scale or cured put c-sharp custom rasterization one-click integration is advantageous Jo vs code available vs visual studio to run or GA we have also a essay on IOT edge running on stack so the demo we saw on the stack boost and it's got good real demo yesterday and a new window set a grenade that's a new function that is available available today so a lot of new thing like as you can see and if you want to access just a code for action so we we have the dog here but you have a special or URL we don't have the right URL but so you can follow us on Twitter we will we will give also information about these new features and so you can subscribe to all the preview we have the demo about geospatial and taxi is on github and of course if you want to contact the team don't hesitate to send a mail to us so this is an image that goes to the PMT mainstream analytics I will be happy to answer any question and we a few more minutes for question here so let me know let me keep the slide here while we have the zucchini but don't hesitate to either come to the microphone or ask your question good all of them so full scale is available like food cloud scale it evaporate on the cloud and after if you want to do local processing with low latency filter very private information before you can run on a ith on a stack yes so in your lambda architecture with the hot path in the cold path you're using IOT hub in that particular example I think there's some confusion around when IOT hub routes should be used versus a streaming analytic table to do such routing can you help us understand the best practices there that's a good question usually the wrote are useful when you need to do stateless Korie so basically a quarry where each event are looked at individually but if you need to do anything that has to do with the time window this event has something related to another one if you want to see what happened it that's before our that when you you need to use kinetics so for the lambda architecture for the purposes of offline data storage and batch analytics or archival it didn't seem like streaming analytics would be necessary for that use case if you have a very simple architecture where you just want to put all the event or filter them you probably done is traumatic but in many case you want to see if there's an absence of Heather and pre flag something dynamic evaluation understood oh yeah okay when you send an event to a function based on a condition can you see the data that causes the event are you sending the whole banner you just sending the notification itself so it's up to you what kind of schema and what kind of attributes to send into Azure functions you can just send what is relevant for you to just the basic information for you to trigger an alert or some kind of workflow downstream or you can send the entire tuple so to say so that you can store it through as your functions in a database of your choice or and also trigger actions downstream so you have all the control in your hands dude so you specify that in the in the stream analytics portal you specify whether to send the whole alright thank you question so I think you may already answer this but I didn't cut it so basically for the real-time analysis and also abnormal detection can it only happen on the edge devices I mean sometimes our devices or edge box can be disconnected from public cloud so can the system I mean the whole process do happen on the edge devices or they have to be connected to the public cloud I know it can run completely offline so you can disconnect it it's it's it's fully standalone you build some architecture that the hybrids so that some analytics on the edge and on the cloud but you can't restore an either or as well the only time that it needs to be connected to the cloud is as you are deploying your query on the device but after that you're 100% disconnected mode processing is supported great ok another quick question so what's the relation between this analysis platform and the data I mean the edge she called it her base you just announced so that's a good question so basically under the curve registra metrics is the engine powering all the temporal analytics for c college so I just wanna take this engine powering Azure database age for temporal Cory's so basically they are connected together in a very seamless way and support preview so we do exactly when processing and exactly once output deponent which is cosmos DB for example is possible with sequel we need to have an index sorry primary key and it's possible as well so I think there's only few outputs that will not support that so for example with dev and hope we may have duplicates but all the storage outputs we have a way to to have exactly one I'm trying to connect two different streams coming from event hub you do not have their support but if it the stream is coming from cosmos DB I can do that well I was talking about the output so you can combine different stream and input your output is Kosmos DB or sequel or edger table we can guarantee exactly once but not exactly once delivery is guaranteed based on the target not based on the source yeah so if we are writing to a cosmos DB or a sequel or Azure table it there is a way for us to ensure exactly ones delivery but if we are writing to for example event hubs today exactly once delivery is not guaranteed exactly once yeah we all did the doctor recently we realized that was a very very common question and it's very recently maybe three weeks ago we had it in the dark I can point you to the doc thank you just send us an email to ask us a Microsoft any last question before we close this session we'll be outside for more time thanks everyone thank you [Applause] 