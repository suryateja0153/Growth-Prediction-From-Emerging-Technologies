 (light music) - [Rob] Hi, I'm Rob Futrick, Principal Program Manager Lead for Azure's HPC Software and Services team. I'm gonna talk to you about the HPC and AI software platform. So the overall HPC platform is kind of a stack of resources. At the bottom we have our compute-focused offerings. Really these are HPC workload-optimized offerings, aimed at giving you the best priced performance that you can get anywhere. Our H Series, which is CPU optimized, or our N Series, which is GPU and other accelerators, or even Cray. Paired along with that is our high-performing storage, which is offerings like Azure's HPC cache, NetApp files, or ClusterStor, but it also includes things like attached disks and files offerings and just regular NFS filers. Connecting it all together is our fast and secure networking. This could be ExpressRoute, which is what connects your users or your data centers to Azure, or InfiniBand, which is what connects all of our HPC components inside of Azure. And finally tying that all together is the HPC software platform, primarily Azure Batch and Azure CycleCloud, which function as workload orchestrators. I want to note that the software platform is also an integration point or a bridge to Azure's transformative services, Azure Machine Learning, Azure Data Lake, Quantum, and others that allow you to not just move your existing HPC workloads to cloud, but to really drive new capabilities not previously possible. So Azure Batch is a cloud-native job scheduler. It's HPC as a service model. The resources for now are cloud-based. And really, give us your jobs. We'll spin up the environment, run them for you at any scale, give you the results, and shut it down. Azure CycleCloud is a complimentary offering. It's really more of a traditional cluster orchestration model. It allows HPC admins to run their HPC clusters either in a hybrid or bursting model or in an Azure cloud-native model. And while it itself is not a job scheduler, unlike Batch, it is a way of essentially Azure-enabling third-party schedulers. So Azure Batch contains two primary capabilities. Batch pools, this is the way that you create and configure virtual machines at any scale: single VMs, tens of VMs, thousands of VMs. And it will automatically scale it up and down to maximize your utilization, minimize your bill. And it supports all of Azure's offerings, whether it's our spot offerings, our regular on demand, Windows VMs and Linux. And then paired with that is the Azure Batch scheduler, jobs and tasks. Give us your jobs. We'll run them. We'll monitor them. And we'll make sure your work gets done. Azure Batch is also a developer-focused tool. It is a SaaS platform for enabling these workflows. So in addition to being a cloud-native scheduler and handling things like auto-scaling and job scheduling, offerings like Ansys, Cloud, and others are actually built on top of Batch. Now when running in Batch, you have rich plan management. You can get your plans from blobs, from Batch app packages, custom virtual machines. You can also run Docker or singularity containers. Containerizing HPC workloads is definitely a growing use case and something you can run at scale. And while running at scale, Batch includes tremendous amounts of monitoring, error detection, auto-recovery, as it provides metrics and logs so that you have the understanding of what's going on in your environment. So Azure CycleCloud is not necessarily a developer-focused tool. It's really aimed at HPC administrators. It's kind of an HPC control plane for Azure. If you're the kind of person that would call a vendor, have your equipment delivered, and rack it, stack it, cable it, burn it in, configure it for your scientists and engineers and quants, Azure CycleCloud is your way of doing that conceptually in Azure, but in minutes instead of weeks or months. You can deploy a complete HPC cluster in as little as 10 minutes. And as I mentioned earlier, you don't have to rewrite your applications. You can conceptually lift and shift. We understand HPC environments are some of the most optimized that our customers have. So cloud complexity can definitely push past kind of manageable difficulty. CycleCloud is aimed at reducing that cloud complexity by providing all kinds of governance and technical features. That includes cost reporting and cost controls, a full RBAC system, audit, event logging, and other capabilities. And since it is not a scheduler, it does support traditional job schedulers as you'd expect. These include Slurm, Altair's offerings, whether it's PBS Professional, Open PBS, or Grid Engine, IBM Spectrum Symphony and Spectrum LSF, offerings like TIBCO GridServer and others, as well as produces modules for you to cloud-enable your own customer schedulers. And so a typical scenario here is to take your environment and essentially capture it in a template that then CycleCloud can then move to Azure. And again, this is a conceptual lift and shift that allows you to give your users what they're used to using in kind of the lowest friction way possible. And it does not require you modernizing necessarily or rewriting your applications to start. Once your in the cloud, either in bursting or hybrid, then CycleCloud solves those governance issues by providing policy-based systems for access, authorizations, cost, reporting and controls and compliance. And so we're excited to talk about CycleCloud 8, which will be released later this year. It's really about a renewed focus on core HPC operations and Azure platform integration. We have cloud-init for configuring your virtual machines. We have significant improvements to performance to get you faster time to results, updates to support the latest versions of OpenPBS, to support Univa Grid Engine, CentOS 8, Ubuntu 20, and SLES. We've also added in default templates, options for NAS storage to allow you to mount NFS file systems. We've added support for Slurm's job accounting, which is a near future development as well, and additional Slurm features such as stop the allocating nodes instead of termination and auto-detection of GP resources. Of particular note, I wanna call out the Azure Event Grid integration. You can sync cluster and node events to Azure Event Grid, which means you can subscribe to those events and then take downstream actions. And that's much more powerful than it initially seems. And then all kinds of work to give you access to your logs and insights into the details of your workloads so that you can understand who is doing what, at what scale and what data and at what cost. That is the Azure HPC platform. (light music) - [Evan] Hello, my name is Evan Burness, and I am a Principal Program Manager for the Azure High Performance Computing team. Today I'm going to be talking to you about our H Series virtual machines for high performance computing. I'll be covering how we in Azure think about and design for our HPC customers. I'll be giving you a close-up look at our H Series offerings. And I'll be revealing some of our leadership class performance scale results on real-world applications. Azure is focused on providing leadership in HPC and AI workload performance, scalability, and cost efficiency, and not just for the public cloud, but anywhere. We believe we can achieve this by designing purpose-built HPC infrastructure that is deeply optimized for the real-world applications our customers run. Finally, we believe that through our products we can drive customer innovation and creativity by allowing them to unlock the power of supercomputing scale. The Azure H Series is comprised of two families of virtual machines, the HB Series and the HC Series. These are our flagship offerings for CPU-based high performance computing and provide the highest levels of HPC application performance and scalability on the Azure cloud. They are complemented by a broad array of other types of virtual machines on Azure, including our F and D Series for compute-optimized general purpose needs, our E and M Series for memory-optimized needs, and our L Series optimized for storage scenarios. The latest offering from the HB family of virtual machines is the HBv2, or as we like to think of it, supercomputing on the cloud. It is powered by the most powerful processors available today, the second generation AMD Epic CPUs, code name Rome. It features four teraflops of double precision compute performance and eight teraflops of single precision compute performance, and 350 gigabytes per second of memory bandwidth, which is nearly twice what you will find elsewhere on the public cloud. It features Azure's first deployment of 200-gigabit HDR InfiniBand, which enables HBv2 to scale to an astonishing 80,000 parallel processes for a single MPI workload. And every HBv2 virtual machine comes equipped with a 900-gigabyte NVME SSD and support for Azure premium storage. Azure also offers the HC Series virtual machine, the public cloud's highest performing and highest scaling intel-based HPC solution. HC Series virtual machines feature Intel Xeon Platinum first generation processors, code name Skylake, with 2.7 teraflops of double precision compute performance and 5.4 teraflops of single precision compute performance. Each HC Series virtual machine features 190 gigabytes per second of memory bandwidth, 100 gigabit EDR InfiniBand, and support for MPI workloads up to 20,000 cores in size. Each HC Series virtual machine also features a 700-gigabyte local SSD and support for Azure premium storage. Azure is the only public cloud to offer InfiniBand for high-performance computing. InfiniBand offers wonderful benefits for our HPC customers, including support for all MPI types and versions, up to 200 gigabits of bandwidth, latencies as low as 1.5 microseconds, and bare metal performance through SRIOV technology. But Azure goes a step further to provide additional performance for our customers, with features like a non-blocking Fat Tree topology, hardware offload of MPI collectives, support for the dynamic connected transport, and intelligent adaptive routing to take our customers' largest-scaling applications even higher. Let's talk performance. SpecCPU2017 is an industry standard compute benchmarking suite, providing tests for both integer and floating point-based workloads. The floating point-based workloads are especially useful for HPC customers to look at because they include a broad array of applications and scientific domains. SpecCPU2017 is a useful tool for gauging the relative performance differences for small and medium HPC workloads across different CPUs and generations. Using the Xeon Gold 6148, a commonly deployed processor for HPC customers, as our baseline, we can see that competing public cloud HPC offerings fall just a bit short in terms of single VM performance. On the other hand, Azure's HC Series actually surpasses the bare metal on premise baseline. And Azure HB Series, with HBv2, offers a whopping 2.5x performance advantage. While one-node performance advantages are great, where Azure really differentiates itself for our HPC customers is our ability to scale MPI applications in ways that are simply not matched elsewhere on the public cloud and are more in line with the capabilities of leadership class supercomputers. Here you can see that on Azure with both the HC Series and the HBv2 Series virtual machines. Azure is able to take real-world application performance like CP2K, the Graph500 test suite for graph analytics, STAR-CCM+ for computation fluid dynamics, weather simulation, and biophysics applications up to an astonishing 86,400 parallel processes per job. This is 12x higher found elsewhere on the public cloud. Thank you for joining me for this overview of the Azure HC Series virtual machines on the Microsoft Azure cloud, the world's computer. (light music) - [Ian] Good morning. My name's Ian Finder, and I'm here to talk to you about how we deal with AI and GPU infrastructure at every scale in Microsoft Azure. When we started shipping GPU compute SKUs back in 2016, the AI and ML world looked a lot different. It's hard to believe that CUDA has only been around for about 10 years. In 2016, most AI and ML workloads could run on a K80. The low-end CNM training, the Batch inferencing workloads, they were mostly the realm of research curiosities. There weren't the same cost sensitivities and time to solution sensitivities as there are today, where AI is a critical part of everyday business. Wow, in 2020, things look so different. We have customers rolling out multi-region inference-based services that need to hit new price points to be able to appeal to a wide user base. We at the same time have customers pushing the envelope of what natural language AI can do. And for them, training a billion-parameter-plus model is routine. The gamut between a real-time inferencing requirement where you need a lot of small GPUs and a really wide world-wide footprint to the effective use of a GPU supercomputer to train the models that you might even eventually deploy to those VMs, it's an immense latitude. And we're really proud to have a portfolio that has kept pace with these requirements. So what you can see is, overall, customers have a huge selection across all these segments. We have these grayed-out GPUs. What these are is older GPUs that we're not deploying more of, but that we've discounted. They offer cost-effective solutions for customers who can use them in the regions where they already exist. In the meantime, across this entire spectrum of requirements, we're deploying new capacity on newer GPUs from one T4 all the way to thousands of A100s. If your workload doesn't fit the common use cases on the left, the spectrum of offerings is still the same. If you have a workload that runs on a GPU, Azure has a cost-effective and optimal size for you to run it on. So we're showing that, every passing year, there are more AI and GPU-centric options in Azure available to customers. And we're not slowing down because we realize that our customers rely on us. They rely on the availability of these options to deliver innovation across all these different market segments. One of the stories I wanted to tell is about our partnership with a company called Riskfuel. Riskfuel is in the financial analytics space. They have built a workflow that can speed up the valuation of complex equities, specifically complex derivatives, by 20 million x. Normally valuation of a complex derivative is done once a day. It's a CPU-bound task, a Batch task, where at the end of the day you sweep through a bunch of possibilities. It might take a few hours. You look through everything in your portfolio, and you compute a bunch of extremes. And the next day, that's your value. But what Riskfuel's done is built a workflow that leverages CPU-based VMs, high-end GPU VMs for training, and mid-range VMs for inference. They'll use the CPU VMs to generate an immense corpus of training data, which they then train on and can deploy as a standard machine learning inference model so that these look-ups can be done in real time. No longer do you have to wait until the end of the day to get a new value for the instrument that you're training. This kind of technique, of taking traditional simulation and applying the power of deep learning to it is immensely powerful. Now we're seeing AI and ML help us cheat Moore's law even on traditional HPC and simulation problems. In our partnership with Riskfuel, we found that the Nvidia Tesla V100 options offered in the NCv3 and the NDv2 Series were the ideal platform for doing inference and training respectively for their workflow. But not all customers have a workflow that can saturate a V100 GPU. So we've introduced a more cost-effective option for customers who wanna leave a system spun up at all times that may or may not be actually using that GPU in a full throughput capacity. Furthermore, we have customers who wanna experiment with the mixed precision modes offered in the Turing architecture. And we have customers that just have small models. So we have a new T4-based offering, the NCv3 T4 family. It's available in one, two, or four T4 GPUs with the latest AMD Epic processors. So if you wanna co-locate a web service worker or something compute-bound on that same node, maybe you wanna do pre-processing on your input data, you have a powerful deployment path. Furthermore, we expect to bring the T4 SKU to more regions than many of our larger GPU SKUs can accommodate. So this is a very, very powerful way to deploy AI infrastructure across the globe for new workloads. Many of you are familiar with the T4, and many of you are familiar with the Nvidia flagship A100 GPU. So we won't spend a lot of time on this slide. Suffice to say that most customers are gonna be able to see 2x speedups moving from V100 to A100 right off the bat. This is a really, really powerful new chip. And Nvidia's done a great job of pushing any potential memory bottlenecks or IO bottlenecks forward. This has more memory bandwidth than before. It has more NVLINK bandwidth than before. So customers are gonna enjoy a great speedup right off the bat. And with features like multi-instance GPU, customers are going to be able to run more CUDA kernels in parallel and better utilize resources than ever before. So this is a great SKU. But a great SKU also requires a great building block to wrap it in, which is why I'm really proud to talk today about our NDr v4 A100 product family. This is a system that incorporates eight A100 GPUs on NVLink-3 in a single node powered by a dual-socket AMD Rome processor. This is 96 cores, available to you, the customer, directly. We even hold back a set of cores to manage the Azure hypervisor and the storage services. So this is a fully virtualized, true virtualized offering with bare metal performance for the specs listed below. But we didn't stop there. The reason we picked the AMD Rome CPU was to get PCIe gen 4 technology into our box, which allows you to interface from your CPU cores to your GPUs with 2x the level of performance as ever before. But we're not stopping there because every single one of these A100 GPUs is paired with an Nvidia Mellanox InfiniBand HDR 200 gigabit NIC. This VM product comes in one size. You can deploy eight A100s in a box. But eight of those A100s come with a direct network attached for a staggering 1.6 terabits of bandwidth for VM into an InfiniBand backend network that allows you to use GPU-direct RDMA and NCCL2 to scale your data parallel or model parallel jobs or your traditional parallel HPC jobs out to levels never seen before in a public cloud. What kind of level? Well one GPU, eight GPUs, thousands of GPUs. Topology agnostic, automatically provisioned. VM scale set sets up your IB connection, and you're good to go. This is a massively scalable AI supercomputer that we'll soon announce will rival some of the top 500 machines at the top of the list that you can get one slice of or you can get thousands of GPUs of on demand. This is such a cool product, and I'm so amazed at the great work our team has done in making GPU direct work under a normal VM with all the manageability benefits of a virtual machine. This is an amazing product, and it's gonna help us and our partners and you crack that next frontier of AI workload. The ND A100 v4 is moving from private preview to public preview later on this year in November, at which time we'll be releasing benchmarking numbers, recipes, and a lot more details, as well as a sign-up sheet. So look out for that as we take Azure from the world's computer to the world's supercomputer. (light music) - [Scott] Hello, my name is Scott Jeschonek, Principal Program Manager with Azure storage, and I'm going to share some of the HPC storage choices available to you in Azure. To start, I'd like to review storage considerations when deploying HPC solutions. Although the main goal is really to saturate the CPUs and GPUs of each compute node in your HPC cluster, there really are several other dimensions of storage access that may be required or considered. For example, you'll need to consider storage requirements for your scheduler, information such as your logs and your queues. You might use a VM with attached disks for this purpose, or you might consider a database service or roll your own MySQL database. In smaller (indistinct), this really may not be a huge requirement. But if you're running a larger cluster and you're running a lot of concurrent jobs, this may become a bottleneck for you. You also may need to ensure your libraries and tool chains are accessed quickly by the compute nodes. One way to do that might be to copy the tool chain information to the local disks on the machines. This is especially a concern if you start to run a large number of HPC cluster nodes, and the start times are dependent on the access of those libraries and tool chains. Another way you might have to optimize is use shared storage for those tools or tool chains to some sort of scaled-out NAS solution, for example. This method may be more useful at larger scales. You may have scratch space or checkpointing requirements. That means not only does your read performance have to be optimized, but you also have to ensure that there's sufficient write performance to not create any bottlenecks in the workflow's throughput. And finally, being able to write out the results. You can still write data back to your on-premise storage environment if necessary, or you might use something like a blob or an NFS solution in Azure. Ultimately, you might want to consider putting all of that data into a tiered data lake that can then be consumed downstream by other services beyond your HPC cluster, such as Azure ML or other analytic tools. And so putting all of your data into blob might be of interest, and we'll talk about that a little bit in the next couple slides. Here we have the overall Azure cloud storage vision. The vision diagram doesn't explicitly show disks, but disks are definitely there. Azure offers multiple managed disk solutions, ranging from HDDs to something we call ultra SSD managed disks that allow you to co-locate your data beside your VMs and containers. And those discs obviously have different price and performance metrics, the HDDs being the cheapest and the ultra SSDs being the most expensive, but the highest performing. But what you see in this diagram is a multi-tiered storage stack that has all the resiliency, redundancy, compliance, and security that you would expect from a world-class globally distributed storage solution. It has automatic life cycle management and ability to provide movement of the data between different performance and pricing tiers. On the right side of the diagram, you can see products and services which you can use to ingest the data from your on-premise environment into the storage stack. Depending on what you're doing, maybe you are an autonomous vehicle vendor, and you want to upload the car information into Azure. You can use some of these tools to do that. Of particular interest in this diagram is the red area above the tiers. And more on that in the next slide. One of the key aspects of this vision which is available today is something that we call Azure Data Lake storage. Azure's implemented a hierarchal name space on top of the object storage solution. It's just a feature of object storage environment. You just enable it. By enabling the data lake, you open the door to having multiple protocol access of the underlying blob storage whether you're using the blob API to make calls or if you wanted to run a Hadoop or a Spark environment against some of the data in object storage, but you wanted to use an HDFS file system. You're able to enable that on top of your blob object storage. Also part of Azure Data Lake and something that's in preview now that's soon to be GA is our NFS v3 endpoint. And these protocol endpoints are really a unique value that Azure brings to the table with their entire object storage solution. And that object storage solution is ultimately a singular deep geographic dispersed life cycle managed solution, and it's fronted by multiple protocols. We also have a series of file system offerings that complements the overall storage vision. One of the main partners in this is Cray Compute, which offer their best in class Lustre file system called ClusterStor to customers in need of very high performance, large scale parallel file systems. ClusterStor is a bare metal offering that is placed directly into Azure data centers. This table gives you a breakdown of the three main offerings. ClusterStor can provide anywhere from five petabytes to 45 petabytes of storage. And it's important to note that these Lustre systems are built on demand. These are bare metal systems that are deployed into Azure data centers based on your needs. Consistent with our single stack storage vision, we have an HSM solution that allows you to store the large portion of your data in the lower class blob and only transfer the working data in the Lustre systems as you need it. We also have Azure NetApp Files, which is NetApp's actual hardware and software solution deployed in Azure. They put NetApp services in Azure on actual hardware, and they provide multiple tiers of performance and multiple file protocol access. Azure NetApp Files supports NFSv3 and 4.1, and also SMB3. And you can use ANF not only for HPC workloads, but also for general purpose NAS use cases, including hosting databases or file sharing. This slide gives you an idea of the pricing and performance tiers. ANF offers pool volumes at standard, premium, and ultra. And you have the ability to move between these tiers of performance as well as being able to move the size of the volumes up and down dynamically. ANF supports provision throughput up to four and a half gigabytes per second, and the max volume size is 100 tebibytes today. And obviously you can have multiple volumes, and there's ways that you can bring your volumes together. And let's talk about one of those ways next. HPC cache is a file system cache that lives adjacent to your compute or container workloads and allows you to create a pseudo file system that points to the storage, whether it's sitting in your existing data center NAS solution, such as (muffled) system. Maybe it's sitting in an ANF volume and you need it to geographically access volumes from other regions without syncing the data. Or maybe you wanna build a cloud NAS out of multiple ANS volumes or potentially a blob container or all of the above. This is the service that you can use to do that. A great example of how you might use this is if you want to try an HPC workload in Azure, and you're not yet at the point where you're moving all your data into the cloud or you don't wanna move all your data into the cloud. Running HPC Cache will give you the ability to stand up a transient cache, access the data, when it's done being processed, delete that cache, and the data's gone from Azure. And this is something you can use for that. This slide gives you an idea of the architecture. HPC Cache sits in front of... Sits in your Azure subscription and can access your storage, whether it's across an express route or in a different region. You can also bind together multiple on prem systems or cloud systems or both into a single directory structure and serve that up to your HPC compute cluster. And these are the different HPC Cache SKUs, ranging from two gigabytes per second in throughput up to eight gigabytes per second with various sizes of cache disks depending on what your working set is. And with that, I wanted to thank you very much for your time. Please check out all of the Azure storage solutions at Azure.com. (light music) 