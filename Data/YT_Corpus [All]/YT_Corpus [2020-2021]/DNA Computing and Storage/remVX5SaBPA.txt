 Good afternoon everybody. My name is Jeremiah Dooley. I'm one of the Cloud advocates at Microsoft and what we're going to talk about today is Selecting the Right Data Storage Strategy for Your Cloud Application. Right up front, we want to draw distinction between select the right storage for your Cloud application, which we could do an entire hour just on let's talk about file storage in blob storage and block storage and all the different permutations of them, and let's talk about how we think strategically around data storage strategy. How do we build something that is sustainable within our organization? How do we build something that will allow us to grow the applications and to grow build new features for the applications that we have as easily and as efficiently as possible? So we're going to spend about half of the session, may be a little bit less, if I can get away with it on some slides around, how do we look at data and how do we build a strategy and what are some examples of strategies and then relate those back to Tailwind Traders and the data that we're seeing in the type of organization that we've built for the Ignite Tour, and then the other half of it we're just going to spend doing demos. We're going to look at lots of different ways to handle data in different ways to build applications or to build features into applications that maybe traditionally we've done one way that Azure gives us some different ways to look at. So my two goals here as we go through this, one is not to fall off the stage. I love that they gave no handrails, no nothing, I tend to walk around a lot. So we're going to try to remember where the edge of the stage is, and the second is that we're going to have all three of the demos go sparkling well, because I think that those are really the focal point of everything that we're going to do here today. So how's that sound? Everybody okay with that? All right. Here we go. Agenda-wise, we're going to start with what's important and we've tried to do this in each of these sessions, but this is a business and the business does have requirements and some of those requirements are things that the business is aspirational towards and some of those are things that the business absolutely has to have. So we always want to start before we start looking at what are we going to do technically or how are we going to solve this problem. We want to look at what is the business challenge and what is it that the company needs. We'll pivot into what's the reason for having a storage strategy, why does it make sense, what are the things that it allows us to do easier. We'll talk about choosing a data storage approach. We'll look at some different types of data and some different ways that we can organizationally look at how is it that we're going to consume data in order to be able to build better applications or to support applications better. Then we're going to jump right straight into the demos. We'll close out with some next steps and some resources, and for those of you who like taking pictures of slides, there are a few in here that will specifically call out around resources or around documentation that will be useful, that'll make sure that we pause on and give everybody a chance to take a look at those. So let's start with what's important to Tailwind Traders. Tailwind Traders is our retail app, and I think everybody's gone through three sessions at this point, I don't have to rehash it too much. Tailwind Traders is our retail company, global worldwide company, lots of data, lots of customers, lots of consumers of data. Internally, we have some things that we need to maintain as a business. The first one I feel like is the aspirational goal of every development group is that we want to be able to spin up new services or features for existing services quickly. But we also want to limit the impact of those new features and of those deployments onto our existing environments, because there's nothing worse than saying, I have a brand new feature that our customers want that is going to be great for the business and we deploy it and it breaks something that's been working for the last three years without any problems whatsoever and now we've ended up making everybody mad. So we want to be able to spend those features up quickly, but we have very strict limits on how we're going to be able to impact the existing production environments when we do that. Philosophically, we want to put as much of the control as we can around deploying the specific infrastructure that's needed to release those features into the hands of the developers. The more that we can do as a service, the more we can do where all of the pieces of infrastructure that are required to be able to deploy something out are included with the tooling or the features of Azure that are being used to deploy those features the better and we want to be able to scale things up and down, But not just from a cost perspective. It's not just a matter of we want to be able to pay for things when we use them and not pay for them and we don't use them, although that's always important, we also want to put as little operational overhead as possible. Because the cost of the resources is one thing, the cost of the people who have to manage those resources is a completely separate thing. We want to be cognizant and aware of both of them. Then finally, we want to be able to leverage the right tool for the job. There are always going to be times that we have to say whether it's to the operations teams or to the developer teams, this is what we're going to use. Where we dictate to them, these are the tools, these are the platforms, these are the limits that we're going to put on this. But if we can and where we can, we want to allow the developers to be able to use the right tool to be able to deliver the features that they want and the features that our customers need as quickly and as easily as possible. So with that in mind, here's who we are, here's what we need. What's the reason for having a storage strategy? Everything that we build as developers and everything that we inherit from an operational standpoint, has an impact not just on how things are today, but on how things will always be going forward. Rarely do we introduce something new into the environment and then never go back to that or go back to the old way of deploying things or go back to the old technologies that we used and never reuse that thing that we finally got introduced into the environment. So every time we make a change, we know that both from a development and an operational standpoint, we're drawing a line in the ground and saying, we're never going to go back behind this line. We're always going to continue moving that forward. We want to be able to help the operations teams modernize the legacy processes and the legacy procedures that they have but at the same time, we have to respect that those processes exist for a reason. So part of having a storage strategy is so that we can help facilitate the upgrade of those processes, but we don't run so far past the operations teams that we end up causing problems while we do it. We also want to allow for more velocity to create those products and features. If everyone is on the same page and we don't have to constantly go back and re-examine how we're going to do something or look at the strategy that we're going to use forward or what product are we going to use or how are we going to implement it, if we have guardrails in place that say all of the people who were interested in this, whether that's the developers, the operations teams, customer service teams, executive teams, all of the stakeholders in the decision, if we're all on the same page, we don't have to constantly go back through this change control process every time we want to introduce something new, particularly within a specific Cloud framework. If we do this well and we all have the ability to use the right tool for the job, and we're going to drive more of that controlled into the developers, were going to by definition, or by example, break things down into a much more manageable place from an iteration standpoint. Rather than having to rebuild an entire app, rather than having to rebuild an entire monolithic stack of things that work together, we can trade one piece out for a new feature. We've already had to start doing this because the Cloud companies like Microsoft are not going to stop iterating just because you've started to use this service. We're going to upgrade services, we're going to deploy new services, we're going to deploy services that fit in the same space, but maybe serve a different type of use case. The more we look at the strategy as a way to be able to swap things in and out, and the more that we use that as a way to provide the air cover that we need to say we're going to change how we do authentication, we're going to change how we do load balancing, we're going to change how we do searching or caching, but we're not going to have to rebuild the whole stack, then the more iterative we can be about breaking that down. Then finally, no matter what we do, we always have to keep in mind the security posture that the company has. Just because we want to use a new feature doesn't mean that we can do so knowing that it's going to break all of the things that are important from a compliance standpoint, from a data regulation standpoint, from an operational management standpoint. So if we have that strategy in place, and everybody's bought in, and everybody had a seat at the table to discuss how we're going to do this, we can maintain that security posture a lot easier as we go through and do the things that we need to do from an operational and development standpoint. Azure has a huge number of storage services. Like I said at the beginning, we're not going to deep dive into any of these. However, there is a fundamentals course, I believe later on this afternoon that Lauren is doing that is specifically how to store data in Azure. So if you want a fundamental look at the individual pieces, we certainly will provide that for you here over the course of the two days. Just not necessarily what we're going to do in here, but as we look at all of these, we've got all these storage back-ends that are deployed in different ways, that are delivered in different ways, whether it's infrastructure as a service or whether it's as part of a platform. But if we go one step bigger, this doesn't get any easier. We still have down at the bottom, our fundamental infrastructure services. We still have compute, we still have storage, we still have network. But our investment, and really the pattern that we see customers using from a consumption model, is all of the things that we can use to turn those basic infrastructure services into things that can be usable, into services that can be used, into features that can be used, into places that we can build more functionality into the apps that we're all building together without necessarily having to introduce a new type of storage, or introduce a new type of networking. So the one thing that I want you to take away from this is that the storage networking and compute aren't any less important than they've ever been, they're just more abstracted. But at the end of the day, everything on there still counts on storage. It might be disk storage, it might be cached storage, it might be memory, but everything on there is going to count on the storage that sits underneath it, and the operations people in the room don't need me to tell them this, but there is no bad day like a storage bad day. Everything else in the data center, we have the ability to have a failure, no matter how catastrophic, and always be able to recover immediately back to that point in time. If the network goes down, we can bring the network backup. If the compute goes down, we can bring the compute backup. Storage is the only place, both from an operational standpoint and from a development standpoint, where we have a worst-case scenario that actually involves things being gone forever. Even if we have backups, even if we have redundancy, we still have to be able to plan for something that doesn't exist anymore from a data standpoint, and how are we going to recover, and what sort of tolerance does the business have. So while we're abstracting this as much as we can, and by doing those abstractions, trying to make this easier to understand which of the pieces on the bottom make the most sense, it doesn't mean that the pieces are any less critical than they've always been, we hope it just means that they're easier to consume and easier to manage. So we're going to talk about different types of storage. We're going to talk about different types of data that we will be storing. We've talked about all the services that sit on top of it, we've talked about how are we going to provide data strategy for different applications. For most of the operations people in the room, and probably a fair amount of the database administrators in the room, you're probably asking yourself one question, and that's why can't I just use SQL for everything? That's worked so far. This is one of those things where sometimes this is the imposition that we make on the development process to say this is how we store data, and this is where we store data, and these are the rules around storing data. It's not that we can't do it, and everything as we go through and do the demos, in every one of these we'll stop and ask ourselves, "Is this something that we could have done with just a standard relational database?" But as we saw with the slide previous to this, there are lots of different ways that we, as Azure, are trying to abstract those up that may end up fitting that business criteria list that we had. It may put the developers in more control than a standard database would. It may give us the ability to be more granular about the functions that we're deploying than it would. So when we talk about choosing that data storage strategy, the hard question that we get into is what tool do we pick? If it's not a database, then what is it? We have the best answer that any operations person has ever given, which is it depends. It depends is a great answer because it's always right. Most people hear it so much that as soon as they hear it depends they just roll their eyes and walk away, and you've solved both of your problems at once. We're going to try to dig into it a little bit more here around what is it that we're seeing from a data standpoint that would dictate different ways that we're going to store it so that we can work with it from a development standpoint? The first part that we're going to look at is data classification. Data classification is going to look at different kinds of data. When we talk about data classification here, these are the three basics that we're looking at. Structured data is the data that we've always used. It fits easily into a table, it's easy to identify the indexes that we're going to use to span tables, it's easy to get data into, it's easy to query the data out of the data format itself, the rows and columns is pretty rigid. Everything that goes into that particular table is going to include all of the columns that are inherent to that table. But this is where we came from, this is the bread and butter of how it is that we look at structured data. Semi-structured data is going to be data that may fit into a table, or may fit into a combination of different types of tables, and may use other types of classification to be able to tell us what it's for or where it's going. We may use things like metadata here. We may see data formats like JSON here, things that yes it has structure to it, but it's not going to fit cleanly into a table. That Tailwind Traders Business has a really good example of this that we'll look at in a minute, where you, at first blush, you think this can fit into a table pretty easily, but then when you start thinking through all of the iterations, maybe you need something that's a little bit more flexible than that. Then the third is going to be the unstructured data. This is your image files. This is your video files. This is your Word documents. These are things that by themselves are going to have no structure to them whatsoever, and we have to use external things to be able to identify them. This is where we use folder structure names, or we use image names to be able to tell us when it was generated. There may be metadata that's associated with it, but the object itself is not going to have any structure at all. After we've classified what kind of data it is, we look at the properties of that data. Volume, velocity, and variety are the three things that we're going to look at. The volume of the data is simply how much is there. For any of you who've worked with data sets of any size, you know that there's a big difference between a 10,000 row database or 10,000 row table and a 10 million row table. We just do things differently. We have to build those queries differently, we have to build that infrastructure differently. So when we classify the data, it's important to know just how big is it going to be? Velocity is how much does it change, because it doesn't matter how big or small the database is, if we have a gigantic change rate in that data that's in there, again, we have to know this, we have to be able to account for that when we build both the infrastructure and the applications that are talking to it. Then the last is variety. Do we have all one type of data coming in, or is our application looking at multiple types of data? Do we have image files, and IoT streaming data, and inventory data that's being pulled out of a relational database that's all being accessed and used at the same time, and what does that mean for how we build the app or how we set it up? If we look at some examples from Tailwind Traders, we can look at the product catalog. You've probably seen it a couple times as we've gone through the sessions today, but we've got all of the different products that Tailwind Traders sells, and then the information about them, the price, the size, the weight, the color, the manufacturer, what stores it's available in, those sorts of things. You would think that this would be a nice easy structured dataset, that we could put it in a table and we could forget about it. The challenge comes when we move from one type of product to another type of product. If at first Tailwind Traders cells hammers, it's pretty easy to know what sort of data do I need to have, what columns do I need to build to describe a hammer. If at some point I move from hammers into smartwatches, there's a completely separate set of data that I would want to be able to manage or track for every object in the database that is a smartwatch. I can either create all of the columns that I would need for both hammers and smartwatches and then I've got to go through and figure out what do I do with null values and do I put something in that field, does the null, the placeholder, how do I handle that when I just want to do a query and pullback smartwatches and I don't need to know what the hammer manufacturer is for smartwatch? We need to be able to be more flexible with that because the type of data that we're collecting is different for the type of products that we're using. So in this case, we're going to take that product catalog and we're going to say that it's semi-structured. Does that mean that it can't live in a SQL database? No. In our case, it actually does live in a SQL database. We just need to be aware that the type of data that's going in there may not be perfectly well fit for a single static type of database schema. The volume of this data for us is going to be high. There's going to be lots of products and every product is going to have an entry in the database, the velocity is going to be low because this dataset is only going to change when we add or remove items from the inventory that we're going to be selling. The variety of this is also going to be low. All we're taking in is the semi-structure data. Inside the datasets itself, we don't have any images, we don't have anything else. We may correlate those with images when we build them into an application, but by itself, the product catalog service that we're offering is going to be straight semi-structured data. The product photos on the other hand are going to be unstructured and that's the easy part. Their images, they're going to be unstructured. The volume here is also going to be high. We're going to have multiple product images for every one of the items that are in the database. The velocity is going to be low and the variety is going to be low. So it's going to mimic what we see on the product catalog side. It's just going to be the images instead of the semi-structured data. The sales data is a whole different beast. The sales data is also going to be semi-structured because we're going to be pulling things in from IoT Edge devices, we're going to be looking at things in JSON format that we have to then massage to get into whatever data format it is that makes sense for us from an internal business value standpoint. The volume of this is going to be medium, especially the IoT Edge devices do a pretty good job of being efficient with the volume of data that they're sending, just by the nature of what they are. The velocity of this is going to be extraordinarily high. We'll see you in one of the demos just how much data it is that we're gathering over a period of time that we have to be able to manage here. So the velocity is going to be high, the variety is also going to be high because it's not just cash registers that were getting data from when we're looking at this dataset. We've got things coming in from suppliers, we've got things coming in from inventory, we've got things coming in from lots of different places that we need to be able to manage as we go through here. So the variety of the data that we're pulling in is also something that we have to manage from there. So we've talked about some of the different storage options, we've talked about the different types of data, how do we match them up? We really see customers using one of three different methodologies or one of three different strategies to match these up. The first is to drive that strategy from the storage standpoint. This is simply going to be where the Operations team or the business is saying, this is the storage that we have that you're going to build on top of. Now, on the plus side, it's a sunk investment for the company. We don't have any additional expense outside of the regular maintenance and power and cooling that the storage needs. On the downside, the developers have to know way more about storage than we ever want developers to have to know. They have to know how far they can push it, they have to know what they can break, they have to know different ways to be able to get around limitations of it. Whether it's IO limitations all at once or whether it's retention issues. There have to be lots of things that developers have to know that we would probably not really want them to know from a development standpoint. The second one splits the difference. The second one is going to be Cloud driven. When we talked about the services and we had all the services at the top and then we had the Cloud storage pieces at the bottom, this is going be the consume Cloud storage, but to operate it much in the same way that we've operated our On-prem storage. So we can deploy to the storage that makes sense. So we have lots of different things that we can deploy too, but from an operational standpoint, we still have the overhead of being forced to manage and to consume each of those pools separately. From a development standpoint, we still have to know what it is that we need, and where we're deploying it to, and what the characteristics of that are. So we've made some headway into the operational and cost side of things. We're still not getting the developers to the point where they don't have to know the bits and pieces of the storage underneath it in order to be able to build an efficient app. The third one and the one that we see customers as they start to get on-boarded onto Azure really get excited about, is to have this be function driven. Let's talk about what it is we're trying to accomplish from a business standpoint, how we can accomplish that from a technology standpoint, and then what features, which of those functions, which of those storage, network, and compute abstractions that Azure offers can we use to be able to simplify all of our choices from a cost standpoint, from an operational standpoint and from an infrastructure standpoint while giving us the ability to move a lot quicker on what it is that we're deploying. So my guess is there's a good mix in the room of people who worked for companies who are in different places and have the ability to embrace different strategies that are up here. We see over time and really it shows in the way that we roll out new products and the different types of products we're rolling out, that if we can facilitate the operations teams getting what they want, the business getting what they need, and the developers having the tools to be able to build those applications quickly and securely, that more companies will move towards the function driven storage strategy. But we also know that that's a process, right? That might be months or years or in some cases in some industries decades away. But one of these three is most likely going to be the way that your company and your developers look at the storage strategy that they have. So we talked about slides that are great to take pictures of, this will be the first of them. If you want to know anything about data storage strategy or data storage approaches in Azure, there is a learn module. If you've been to the docs.Microsoft.com site, the same team that builds all of the docs for Azure also builds all of the Learn Modules and learn content. So this is free training. It gives you lots of examples. It gives you lots of scenarios. It'll go through and talk in more depth about the different data classification options that we looked at and those sorts of things. So let's talk about demos. In this demo we're going to talk specifically about Azure Search and Azure Blob Storage. How many of you are using Azure Search? Good, a few. This is one of the ones that I learned about as I was building these demos and it just fascinates me. There's so much fun stuff we can do with it. Business-wise, what are we trying to accomplish? We want to be able to search the inventory database. We would like for that search function to include fun stuff, we'd like to be able to filter, we'd like to be able to autocomplete, we'd like to be able to provide suggestions for customers, we'd like to be able to do things like build custom scoring so that we can return the search results we want to based on items that are popular or the part of the world that the customer is searching from. We want that search to be full-featured. We need it to not impact that production database. So the database that's sitting in the back that has all of that inventory in it, we can't beat that up, we can't ask them for more infrastructure, we can't put any undue stress or pressure on that database. If we can get away with it, we'd like to not pay for storage that we don't need. So could I do this in SQL? Absolutely. SQL has had Full Text Search in it for a good long while depending on which version it is that you're running. This is not a lightweight process. There's a lot of stuff that goes on under the covers in SQL to be able to make this happen, and so could I do this without providing any or without putting any impact on the production database? Categorically not. Could I stand up another database and replicate them just so I can turn on full text searching and search it? I could. Now we get back to the why am I paying for storage that had don't need? Simply to get a function that I want to be able to build into the front end for my customers. Azure Search is a perfect answer here because it is a full search as a service function. It is completely encapsulated inside Azure. It includes all the pieces that we need to and from a feature standpoint. It's management free, we set it up and we forget it. It's not something that we have to consistently go out and touch from a management standpoint. It gives us all of the features that we're looking for and it's going to integrate in with all of the rest of the things that we're doing in Azure whether it's from a storage standpoint, whether it's from a security standpoint or whether it's from the front end of the app that we're building. The overview of this one we've got the Web UI in the middle, and we're going to talk about that here in a second. We've got the SQL database that holds all of the inventory. This is the one that has the nice pretty golden wall around it that we're not allowed to beat up too much. We're going to put Azure Search in line to be able to return those queries directly back to the web UI and then we've got our users who are hitting the web UI from the front end. So the first thing that we're going to look at here is the Search Service. The way this search service works, think of it as a caching engine, where it's going to go out and it's going to index a table and it's going to index it in the specific way that you want to and then it's going to cache all of that returns as a web service, so that you can call that from any web front end that you want to and it acts as if it's an integral part of the application rather than add-on that we've plugged in through the Azure Cloud. So the first thing that we're going to look at here is the actual data sources. We can see that we've pointed it at the product catalog and there's a couple of things that we can do here in the name of let's not put too much impact on our production database. The first is that I can pick a specific table or view. I'm not going to indiscriminately go and scrape or query the entire database to find everything that's in there, I can create a specific table or a specific view for me to be able to go use as part of my indexing for Azure Search. The second thing that I can do is I can track changes. I don't have to set this up and say even if you haven't made any changes I'm going to go out and check for changes every five minutes or every 30 minutes. I don't ever have to worry about the database is changed and my search results are now invalid for some period of time before the script kicks off and updates the index on the backend. What we can say is, I want to track changes by looking at this high watermark column. When this column changes, I want you to go re-index the database. This gives us a good balance of chances are, the production team is not updating that database in the middle of the day, and whenever it is that they update it that will also just naturally include updating the index on the backend. So we're behaving as well and as nicely as we can with the production teams that are managing that database. The next thing that we want to look at are the indexes themselves. The index is pretty straightforward. It's just going to be a collection. In this case, it tells us there's a 136 documents, and if we search, it'll come back and here is the JSON output of the Azure search web service. I can now take this return and I can pull whichever pieces out I want and I can format it however I want to into my front end, but it's just the search returns. This is everything that he pulled out of the database that it was carrying inside that index. When we talk about what is it pulling out of the database, we can also be very clear about which columns we want to pull back into the index and what features do we want to have exposed to those columns. For some things, like the name or like the price, we would like that to be sortable, but we don't necessarily need price to be searchable as much as we need it to be filterable. I don't need to say show me everything that costs $8, I need to say show me everything that exists within this range from a price standpoint. So I can be very granular around, what do I want to do with the content that I've pulled back out of that database? Where do I want it to go? Do I want it? Do I want to provide suggestions for it? Are there different analyzers that I want to be able to use because I've got data of different types? There's so much stuff that we can do in here but the important part is we're playing nice with that original production database. So we've got this index, it's running anytime that inventory database gets updated. It's fully accessible through the web front end. How do we integrate this into a front end? How do we integrate this into what the customer sees? One of the things that went out of preview and went into production with the Azure storage accounts over the last couple months is the ability to use a storage account as a static website. So what we did here was we threw together a quick Node.js app, that is actually running as a single file that's sitting on top of a generic Azure storage account. We went in here and we clicked the box that said I want to enable a static website for this and it gives us a primary endpoint, it's real quick and easy way to get a static site set up. When we look at that, what we're going to see is, oh stop it. What we're going to see is, the most basic, the most generic not really ugly, but kind of ugly web front end that we could possibly use the front web search. We've got the basics. On the left-hand side of the screen, we've got all the things that we said we wanted to be filterable or fasterable. If I want to be able to search between a certain price range I can, if I want to be able to select by product type or filter by product type or supplier name I can, and I'm going to return the image out of the image database and I'm going to return all of the content that I have inside the inventory database for each of those items. Now, I can search and we actually left this in here so you can see where that JSON is coming through. If I just want to search for safe, it's going to go through and it's going to pull all of the items that have safe in any of the fields that we selected to be searchable. So name, description, those sorts of things. When I return that back, it's going to give me everything that has safe. Now if I wanted that to be everything that includes the characters S-A-F-E in, any of those descriptions I can do that as well. Remember, all of these results zero percent of them are hitting that production database. It doesn't matter whether I have five or five million customers who are searching against that front end, everything that they're hitting is on the Microsoft side, everything they're hitting is on the Azure Search side. The Azure Search like most of the services at Microsoft has lots of different tiers. So what we deployed this on was the free tier. Yes, there's a free tier. Which means you can leave here and immediately go deploy it and start playing with it and it will talk to any type of storage that you have inside Azure today including SQL databases and Cosmos DB databases and anything else that you'd have in there. But as you go up, it includes more storage because again from a strategy standpoint, we want the function to own its infrastructure and in this case Azure Search includes its own storage and it includes different levels of redundancy. Do I want to have a geographically distributed? Do I want to have your multiple partitions in there? Do I want to be able to have that be scale out? We would make those decisions based on how many customers we had hitting it, not based on how much the database team was yelling at us for hitting their database. So we've got a front end that's quick and easy, we've got a web service that allows us to pull in all of the features that we want, while making everyone happy and everything that we just deployed was completely in the control of the developer. The developer who was tasked with building this front end has now added a super powerful search feature to it without having to buy any storage that they didn't need and without putting any undue pressure on that database that's sitting in the backend that everybody is so concerned about. So pretty fun. We have another docs alert. So if you'd like Azure Search, we touched on about 10 percent of it right there. There's so much interesting stuff that you can do with complex searches, search scoring, all sorts of fun things. If you want any more information about the short URL down at the bottom there, is aka.ms/search-dev40, and that'll take you to the doc section that includes everything you want to know about Azure Search. You can see on the left there we've got fun things like tutorials and samples and configurations and everything that you could need for different scenarios that you would want to use Azure Search they're going to be there. The second demo that we're going to do builds on the first one. So we've got a front end for customers to be able to search the database, we've got lots of functionality on the search side. Now, we're going to add Cosmos DB but particularly the Gremlin API portion of Cosmos DB in there. What are we trying to accomplish here? Now that we can search the inventory, customers want to know how much it cost to ship stuff to them. So they want us to be able to take that same model of please don't put any of this burden on the backend database to be able to add shipping costing into that front end. We want them to be able to see that through the UI, we'd also love to be able to have that programmatically. We'd also love to be able to not have that be all front end code. The function needs to be geographically aware if we're talking about how do we get goods from point A to point B, there are usually lots of routes. We'd like to pick one that makes sense for the customer and sense for the business. Are the results need to be stored, we want to be able to do analytics on who's searching for what and when and why and be able to use all of that data. Again, I'm sure you've heard this one before, don't mess with the primary database. So Cosmos DB can do lots of things and we could spend an entire hour talking about Cosmos DB. The part that we are most interested in for the sake of this demo is the fact that Cosmos DB is natively a multi-model database. We can front that unstructured database with lots of different APIs on the front end to allow people to transition from whatever product they're using into Cosmos DB without having to change skill sets, without having to change tool sets, without having to change any of the work that they've already done. So we have these APIs for SQL, for Mongo, for Cassandra, for folks who are looking to get more resiliency out of their Azure tables. That's all they really need as a table, but they need it with better reliability, they need it spread across multiple geographies rather than migrating everything we simply use the table API front end to be able to allow them to treat Cosmos DB the same way. The one that we're going to talk about is Gremlin. Gremlin is an open source project that is specifically used for graphing. When you have a big dataset where there are lots of relationships between the datas in there, think org charts. Every person on an org chart connects to another person on the org chart. The question is how and what's the path between them? Those are the types of things that Gremlin API is really good for. So we are going to from a demo standpoint, look at this much the same way that we did before. We've got the Web UI, we've got the untouchable inventory database and we're going to take the Cosmos DB with the Gremlin API, we're going to build out the things that we need to deliver that functionality, and then we're going to plug that right directly into that Node.js front end that we built to be able to display the inventory. All right. The first thing that we're going to look at here is the actual Cosmos DB database. This is literally a just keep clicking next install. We've done nothing fancy with it, we've done no customization to it whatsoever other than saying we're going to use the Gremlin API for it. In this case, what we've done is we've taken a pre-populated set of airlines or airports rather. What we have from a relationship standpoint is how all of those airports connect to all of the other airports. If we can do that and we know where it is that we're shipping from, the customers telling us where they're shipping to, we have distance and we have hop count from Gremlin, we've got size and we've got weight from the inventory database, we can start to do calculations to figure out how much would it cost for us to get from point A to point B. So that's the goal of what we're going to try to deliver here. If we look at the routes themselves, let's get rid of all our little tabs here, we can see the entire list of all of the airports that we have in here. For each of those airports, we can see that it has information that goes with it. There's an ID with it which is the three-letter code for the airport, the latitude and longitude, the city and country, and then all of the other airports that it connects to. If we wanted to see what that looks like from a graph standpoint, let's take an airport we probably all know and love, and we can see there is the Amsterdam Airport and there's 10 different pages of it, but they're all of the airports that it connects to. We can drill through. So if we wanted to go from Amsterdam to Copenhagen, we can now see all of the airports that Copenhagen connects to, and we can see any of those connections and whether or not they also have Amsterdam in common. So in this case, it would be London Heathrow. So, in the in the terminology of Gremlin, we've got all the edges and vertices that we need to be able to find our way from any airport in the world to any other airport in the world, and that's all we're using Cosmos DB for, is to be able to build out and be able to query that huge database of airports so that we can figure out how we're going to get from point A to point B. Now, once we have that, and that's pretty straightforward, we can go get the airport information from lots of different open source data markets that that's not the hard part, now we have to put in some business logic. Because we've got the node front ends, we can just add in some code to that to be able to do what we need to do, and we'll zoom back in here for everybody. So the first part of this is going to go out and build all of the Gremlin connections. So we can actually see the Gremlin client execute, and we can see the query that it's building using the input that we have from where are we shipping from and where are we shipping to. The next thing that we have to do is we have to be able to filter the routes themselves. If I want to go from one city to another city, there may be dozens or even hundreds of different permutations of how do I get from point A to point B, I want the shortest. So take all of those routes, stack them all up and sort them by which one is the shortest, because that's the one that we're going to use for the customer. The next thing that we have to do is we have to determine the shipping rates themselves. Now, in your real-world environment, these are most likely going to come from an external source. They're going to come from a shipping partner, they're going to come from logistical partner, they may come from the government, they may come from lots of different places. Because all I do is build demos, it was nice and easy for me to cheat. So what I did here, was I said, we're going to have a base rate of this much for every shipment no matter how long or short it goes. Then on top of that, we're going to have a multiplier for distance, so the longer the distance, the more it's going to cost, and we're going to have a multiplier for them wait. So things that weigh more are going to end up costing more to ship. These are in no way real-world values, but they give us a good way to look at, here's how we would bring those variables in and here's what impact they have on the front end. So if we go back to our search site, and we look at our safety cone here, we added in a link that gives us shipping cost. When we go into shipping cost, all the cost, all that's being asked of the customers to put the destination city in. We'll say since the warehouse that we're shipping these from is in Seattle, Washington in the US, we can ask the customer, where do you want to deliver that to? It will go out and it will run the Gremlin query, it will look at the constants that we put in, and it will return a shipping value. We didn't touch the production database at all. We built everything specificly on the minimum amount that we would need based on the datasets that we're working with, we've done all of that from inside Azure and we've surfaced everything as a web service so that we can use it and consume it. However, it is that we want to. I don't know how well this is going to zoom. So I apologize in advance if we have to. Oh, it's not too bad. So there is actually a direct flight from Seattle to Amsterdam, and so this was the minimum amount that we could charge from a number of hops standpoint for an object that weighs as much as a safety cone that's being shipped from the warehouse to a customer in Amsterdam. Pretty fun. There's a couple of you that are excited about this, I know it. If you want to know anything about the Gremlin API, really if you want to know anything about Cosmos DB, there's a ton of content out there. But specifically, if this idea of the graphing database or you've been using Gremlin On-Prem and you want to be able to use it as an Azure service to build it into the different applications that you're building, the docs alert that's up there, that's aka.ms/gremlin-dev40 is going to be the best place to be able to go get that data from. All right. Last data or last demo. We're going to go off the page here. We're not going to talk about the Node.js front end anymore, we're not going to talk about that. We're going to talk about a different challenge, and we're going to solve that with an IoT Hub and an IoT Edge device, we're going to talk about Azure Event Hubs, we're going to talk about Blob storage, and we're going to talk about Data Lake analytics. Not data lake, but the really cool out onto Data Lake that ends up sharing the same name that doesn't need data lake at all. If you're confused, I'm confused. But we're going to talk about Data Lake analytics. So what are we trying to accomplish here? We sell things to customers. Sometimes, the manufacturers come back to us and say, "That thing was not good. That thing was painted with lead paint, that thing has a piece that breaks off every time, we have to recall the product." Or at least we need to be able to tell all of your customers that they can get a refund, that they can trade it in whatever their options are. So we've got retail transactions that are happening all the time, and we need to be able to go and notify the specific customers that bought those products if and when those products end up being recalled. Because the number of customers can be really large, and we'll see an example of that here in just a second even just in our demo environment, we would love to be able to distribute the processing of that lookups. I don't want to put more CPU into the server that's doing the query. I would love to be able to distribute that out and just increase the number of nodes, the number of workers that I have running that query as those get big. I don't want to pay to store data, I don't want to pay to have a report database, I don't want to pay for any of that stuff that I don't need, I simply want to pay for the query that I'm running. So if I could have a billing model that is very tied to how big is my query, how long does my query taken, how many times do I run my query, that would be perfect. For those of you who don't know, Data Lake Analytics is an on-demand analytics job engine that was introduced as part of the larger Azure Data Lake, but stands alone in that you can use any sort of storage on the backend to be able to query. It does not have to use, although it can use, Azure Data Lake. The things that makes this attractive to us are going to be that it's very dynamic on the scaling side, that we get to use familiar tools, we can use Visual Studio, we can use Visual Studio Code, we can use the things that we've been using all along to be able to develop and optimize these queries, it uses a format called U-SQL. How many of you know what U-SQL is? I didn't either it's like four of you, that's awesome. So U-SQL is basically a SQL query language that has additional programmatic features, that has the ability to write more code into the actual query itself to be able to do stuff with the data before you generate the results. So it's not the last step, it's the last step and maybe the two steps before that to be able to get to where you want to be. It integrates in with IT investments in that it is role-based access aware, it integrates in with Active Directory, it will use all of the different types of storage that Azure currently offers, so you don't have to stage anything to be able to get it to drop into the query. It's going to be cost-effective, and we'll talk about that when we look at the demo. Again, it's going to work with any of the data that you have inside Azure. The overview of this demo, for those of you who went to debt 10 first thing this morning, we talked a little bit more about how we're generating all of the data for Tailwind Traders. Basically, we built an IoT bought army that are representing cash registers. Those cash registers are constantly generating random transactions. They've been generating random transactions for the better part of three months at this point. Those transactions are the lifeblood of everything that's sitting in the backend that we're going to ingest into the IoT Edge, and then use the Event Hub in the middle to be able to collect all of that. We can do real-time parsing of that data, we can archive it out for things that we need to be able to do later like figuring out who needs to be contacted in case of a recall. In this case what we've done is, we've actually output all of the data that's being streamed through Event Hub into a Cosmos DB database that's just sitting there. It's just a repository for all of the archive transactions that are coming through that are going to end up being used for lots of different things in addition to that's what we're going to use as our query here. The IoT Edge, the bots that we built, the Docker containers that are running, it is easily one of the coolest parts of the entire tour. If you go out to that GitHub repo, and I'll have the link at the end, there is an actual link to being able to pull all of that down and run your own random data generator from an IoT standpoint if you want to play around with any of the IoT Edge or Event Hub pieces. But with that, let's jump into the last of these demos. All right. So Data Lake Analytics. Each of these IoT Edge devices is generating a stream of data that gets saved out as a binary. So it's not even a text file that we can go query. So we have to do a bunch of stuff to the IoT data before we can run the query against it and that's where the power of U-SQL comes in. In this case, I actually built and ran the whole thing or built-in and configured the whole thing in VS Code and then since there's a plug-in right directly for Azure Data Lake Analytics, had the ability to do all my editing there, and then run it, and then do the editing and run it. It took a couple of tries, but it's pretty easy to be able to iterate through. So if we look at all of these jobs, we've got a few here that we've run and we've succeeded. I did a new one this morning just for you because I wanted to see what would happen and it ended up being really cool. So that's the one that we're going to talk about here up top that says, "Urgent Amsterdam Customer Recalls." if we open this up, we can see all of the different pieces of it even though it is a job that's already run. So the first thing that we can look at is we can look at the "Script" itself. You'll see lots of things in here that look like pretty straightforward SQL and then lots of stuff where you're like, wow, that's not a query language and U part of the U-SQL being in here. So, we've got some assemblies that we build at the beginning, we declare all of our file paths. So here's where we're getting all of that cash register storage and all of the pieces that are in there. Here's the file that we're using that says, "All of my Recalled Products" are in here and then we're going to process these. We're going to extract the data that we need from them. We're going to apply a schema against them, so that when we run the query we've got a known structure. In this case, we're going to extract these three things out of the CSV that gives us the recalled products. The AVRO files which are those IoT Edge device generated files, we're going to extract some basic stuff out of it. But again, we actually have to cast this as a string before we can even start to pull anything out of it. So we have to do a little bit more work to be able to set these things up as strings so that we can actually query them. Then we tell it how much data do we want to query. This was the fun part that I did. We'd always done like just show me the last month of data that we had in there. I said, show me the last year and it turned out to be a lot more data than I thought it would. But it still ended up being pretty fun and then we just do a select. So we built that receipts function we're going to select out of it, we build the queries that we're going to look at and here's all I want to return. Just show me a customer name and an e-mail. I could put lots of stuff in here. I could say and also append the time and date of the purchase, append what store the purchase was made from, I could do lots of things in here, anything that I wanted to do with the data. We made this one pretty simple just for the sake of the demo and then I want to output that to a customers with recalled products file. If we go all the way back up to the top here, we can look at the data, right? So here's our inputs and there's our recalled products CSV which by the way, it looks like a CSV. We can look at the AVRO files which are binary files. There's nothing you're going to be able to get out of here, that's why we had to be able to transform those before we could actually use them. Then we have the "Outputs". We've got that file that lists all of the customers and their e-mail addresses. So what did we do? If we look at how we structured the query, we can then see that visually after the query is run or even while the query is being run and have it show all of the different pieces and all of the different parts that went in there. So when this decides to come up, what we'll see is each of those datasets that we went out and queried, the one is just the CSV and the one is all the AVRO files. Then it'll show us everything that had to do and how much time it took and all of the pieces that it needed to be able to get all the way through there. So what we've got here are our datasets that we had at the beginning, and then we've got that extract. We had to pull the data out of them. On one side we've got four rows cake, no problem. On the other side, we have 25.5 million rows. That's how many rows of data we pulled out of that AVRO for transactions that we've generated since we started the Ignite Tour. So it's just sitting in the background constantly generating these transactions to the tune of 25.5 million of them. Now, if we wanted to query a 25.5 million row database in SQL, could we do it? Categorically yes. Could we do it without impacting the database that we're querying? If you bought enough hardware to be able to do it, sure, but that's certainly not something we're going to be able to do on demand or with any sort of guarantee that we're not going to impact it. So we've got our 25 million rows and then we cross-reference it, and it brings it down to 1.7 million, and then we run the aggregate on it and it brings it down to 540,000. Then we finally output that 540,000 to a 25.8 meg CSV that only includes the name of a customer and the e-mail of that customer all of whom over the course of the last four months, bought a product that was on that recalled list. If you think about it, we did that without ever touching the database. We did that without ever having to worry about how real is the data that we had, because we pulled it right directly out of the transaction stream. We didn't have to worry that data had been dropped or the data had been lost and we could make this as granular as we wanted to. If we wanted to search for a week's worth or a month's worth or in this case all of it, we can do so. Now, what did that cost me? One of the nice things that Azure Data Lake Analytics gives us is a full analysis of the costing of that afterwards. What we can see here is that I allocated 30 AUs for a total of 10.49 AU hours which is the increment that I'm going to be built in to be able to run this query, and for the first 25-27 minutes, I used every bit of them. That run time for me to be able to query 25 million rows of binary data that had to be cast as a string was 20 minutes and 59 seconds and the whole thing cost me $15.73 US. On-demand, no additional requirements for any sort of infrastructure to be able to do it and now I'm done. Once that query is done running, I don't pay for the query anymore. So I can go back to my safety department and tell them, you owe me $16 to cover the query that you ran because you had to recall this product, right? So it gives me everything that I asked for. Its programmatic, it allows me to use the tools that I want to be able to use, it's a fully managed service. I don't have to buy additional storage, I don't have to manage additional storage, there's no additional operational overhead and I got the best bang that I possibly could for my buck and I can even model that. So one of the things that it gives me the ability to do down here is to say, what would happen if I only used one AU to be able to run this query? If I want to trade time for money, in this case, I could take six hours and 33 minutes to run that exact same query, and I could cut the cost roughly in half. If this were a gigantic dataset or a query that was pulling in lots of different data, maybe cutting the cost of the query in half by simply allowing it to run longer is a smart thing. In our case, we didn't have to decide because it was only taking a few minutes to run. But I can model that and see exactly what that would be, so that the next time I run it I have more intelligence to be able to decide how I want to configure that query. So docs alert, anything you want to know about Data Lake Analytics and building those queries and U-SQL and all the rest of the stuff that goes in there, aka.ms/datalake-dev40. Yes the storage is more complex than it's ever been. I think we all acknowledge that. The hope is that as we abstract this and build more of it into functions, that we make that easier for you to understand and easier for you to consume without increasing the amount that you need to know about storage. We want to be able to give you more direct access to the developers, we want to be able to maintain the posture and the procedures that the operations teams have in place for good reason, and we want to give you more control both over the experience, but also the cost of that. If you want to know anything, if you want the presentation, if you want the GitHub repos that we use to build the Node.js, front end to build all of the U-SQL queries, everything is going to be at that link in the middle. So the GitHub repo, you can actually just clone it and run it directly. For some of these like Data Lake Analytics, you only pay when you use it. Azure Search has a free service. Please go out and use these things, find out where it is that they will make sense. This is the first time Microsoft has done these learning paths where we try to curate it down and say, if you're trying to solve this problem, here's a full day's worth of content to be able to build it out. We've gotten good feedback about it, we've gotten lots of constructive feedback around how we can make it better. If you can take 10 seconds to open up the app that's on your phone, go in and leave feedback for the session. Does the room layout stink? Was I terrible? The demo's not do anything for you. Anything that you need to put in there to help us make sure that the next time we present this content, we can do it better and we can do it easier. I would really appreciate it. With that, we're done. Please enjoy the rest of the day. [MUSIC]. 