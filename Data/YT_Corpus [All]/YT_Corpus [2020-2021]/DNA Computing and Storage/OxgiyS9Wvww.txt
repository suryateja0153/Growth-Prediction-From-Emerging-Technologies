 Rebecca E Batorsky: Alright so Hello, everybody. Thanks for joining the RNA seq bioinformatics workshop. Just wanted to let you know that this meeting is being recorded. It will be posted online and will also be sent around to participants. I'm going to be muting everyone who is not already muted, just to avoid the background noise. You can feel free to unmute yourself if you want to ask a question. Another option for asking a question is going to be to type it in the chat. I sent a test chat, and so did Wenwen, just with some links that you might need for finding other workshops and the DISC workshop series and workshop material for this course. So I'll, we'll start with some quick introductions, and then we'll go into our intro slides about the course format RNA seq basic and intro material. My name is Rebecca Batorsky. I'm a bioinformatician with research technology, which is a part of Tufts Technology Services. I work on bioinformatics analysis and software development, a big part of my job is to make sure that researchers at Tufts have access to bioinformatics tool, software and also the analysis they need The best way to reach me is to email TTSresearch@Tufts.edu. You can also reach out to my personal Tufts address with any questions about the workshop or about bioinformatics in general. I'd also like to introduce Dr. Wenwen Huo, who's a postdoc in the Isberg lab at the Tufts Medical School. Wenwen, would you like to introduce yourself and a little bit about what you do here at Tufts? Wenwen Huo: Yeah. Thank you. Yeah. Like she said, I'm Wenwen and my research, basically involves both bench work and bioinformatics And more specifically, I had worked with different kinds of high throughput sequencing and analysis. With that being said, I'm open for collaborations. So feel free to email me at my Tufts email. And I think this area is a really fast evolving and really interesting area to me. So I feel it's really great to see many of you are interested today. So hopefully you can all learn something from today's workshop Rebecca E Batorsky: Great. Great. Thanks. And with that, we will start the introductory slides. So first we'll go through the course format here. So we'll start today with a one hour zoom introduction and there were a whole lot of people registered for the workshops, of course filled up pretty quickly. I let everyone else attend the introduction, who was interested in People can feel free to ask questions. But if we do get too many questions, such that we won't be able to complete the material we might take some offline or afterwards. After that, we'll all go to three hours approximately of self guided material which is on GitHub. I SUGGEST THAT WOULD BE COMPLETED WITHIN THE NEXT WEEK, such that we're all working on it at the same time, and we can work together on getting our questions answered. When we're able to meet in person. I strongly encourage, well when I am able to teach this workshop in person. I always do encourage people to work with a partner That's a little trickier over zoom, but not impossible. So if people do want to find a partner to work with, just to sort of sanity check commands and find, you know, missing white space and things like that. You can always post on the course website on Piazza, which I'll talk about in a minute, or even just reach out and I'll help you get hooked up with a partner. So from our self guided material, we will go to the Piazza. which is our course website, please feel free to ask and answer questions liberally there, everyone that enrolled should already be enrolled in the Piazza page. But if you're not, these are some steps to get yourself enrolled, and again email us if you can't get on there for some reason. It's going to be the best way to get a lot of eyes on your question, for the instructors to see what similar issues people can have and get answers to questions efficiently. We've also decided to have a second session, which will be our office hours on Thursday, June 4 at 11am via Zoom, of course, and the link for that is on the Piazza website. Great. The requirements for this course are that you have an HPC cluster account which is available to all Tufts affiliates. And if you're working from off campus, you'll need VPN. The workshop materials going to assume a fair bit of knowledge about Linux and HPC usage in general, as well as the R programming language. These are all workshops that we do offer as part of this series, but since we're not able to really offer them as frequently as users would like all the online material has been posted in a more or less self guided fashion as well and my recommendation would be before you start the course material for RNA seq, if you haven't already, to go through the intro to Linux and the intro to HPC at the very least. because that'll help you move through the commands efficiently and understand what you're doing. An intro to R is also useful, but you could probably pick up what you need for R as we go along. We'll test that access together during this session. And like I said, if we need to follow up. We can always follow up afterwards. So the next few slides will be a big picture of RNA seq about what we are aiming to do with this type of analysis and also some basics about experiment design and library prep and sequencing. That will be my part. And then I'm going to pass it off to Wenwen, who's going to give an introduction to our bioinformatics workflow and we'll test out some access to the HPC platform. So I always like to step back and look at the different functions of DNA and RNA in a cell and the implications that those have for our bioinformatics workflow, the types of questions we can ask Here we have a schematic of a cell that's the big yellow blog with a little gray blue blob in the middle, which is the cell nucleus contains both DNA and RNA. RNA is transcribed from DNA, transported out of the nucleus. At which point, it's mature mRNA which can be transcribed into proteins which are the unit in the cell that performs most functions. So most of the phenotypes and the interesting behavior we might see is the result of different levels of different proteins. There're two types of sequencing that we teach about in these bioinformatics workshops. One is DNA sequencing and one is RNA sequencing bioinformatics. And they're different in many ways. And one important way that I'd like to point out, for DNA sequencing, we typically have a fixed copy number of a gene per cell that's barring any strange copy number variations. We know for a given. For example, it's eukaryotic organism or deployed chromosomes, which should have two copies of each gene per cell. On the other hand, Our analysis goal for DNA sequencing is typically variant calling and interpretation. We'd like to know how the sequence of a given gene that we obtained from DNA sequencing is different from our reference genome of a well studied organism, or if it's not a well studied organism, and we might be interested to know what that gene sequence is On the other hand, we have RNA sequencing, where there is not a fixed copy number of a gene or mRNA transcript in this case per cell. The copy of the transcript, the number of copies of the transcript that we have, varies depending on gene expression. So in some conditions a given gene might be not expressed at all. And in another condition, it might be highly expressed and our analysis goal for RNA sequencing is thus different. We typically seek to look at differential expression and interpretation. So between conditions, how does the expression or the number of copies of the transcript that we observed change. And what does that tell us about the functional interpretation of that gene. Today we'll be looking at RNA sequencing and not DNA sequencing, I've put the link here for the workshop we did a couple of weeks ago, which focused on variant calling and DNA sequencing. So why is differential expression useful? So typically what we're looking for is an explanation of observed phenotypes. Here's a very simple schematic of an experiment with my choose to do, where we have some yeast cells which are "wild type" or unaltered, and some yeast cells which have a mutation, for example, a knockout mutation in a given gene. We might look at different replicates that these cells and try to understand which genes change from condition "wild type" to condition "mutant" We want to know what causes it, for instance, phenotype. And of course we know that it's actually a difference in protein activity that might cause the cells to, for example, looks different on the outside or behave different in our observation. Let me check the chat. Okay. So ideally, we'd like to look at the protein activity in each of the cells and understand why the mutant cells look slightly different or behave slightly different However, mRNA is easier to measure than protein. So we typically use it as a proxy. So here's a picture of where we'll wind up somewhere in our bioinformatics workflow, where we have short reads and we've assigned them to genes: A, B and C. We have these genes and their reads and we assume that the number of reads we have is proportional to the number of mRNA copies that were inside the cell when we started to study it. We also assume it is that those mRNA copies are proportional to the concentration of the proteins that were in the cell. And on another level. We assume that those proteins that the concentration of protein is proportional to the protein activity itself, which we then know is responsible for the phenotype of "wild type" or "mutant". All of these assumptions or assumptions of proportionality are often violated and they're violated basically at each step. So, failure to map our short reads to the proper location. Or biases in our library preparation step could mean that we observe maybe less of a gene than what was really there in our sample. On another step, that can be violated that mRNA templates have different speeds of protein production and alternative slicing of the Of the RNA transcript can actually give us different isoforms of the same protein, which we would have to then separately quantify. The concentration of protein is also independently regulated, which makes things quite complicated. And so at each step we will have sort of something that's unaccounted for, which could skew the amount of a gene we actually observed with respect to the actual function of that gene in the cell. And because these assumptions are often violated, we look at comparison. So if we have a wild type cell with a certain number of reads aligning to each gene, and in a mutant cell or cell culture where a certain number of reads are aligned to each gene, we will look at the ratio of reads in each of the cases. So in this case, Gene A has more reads aligning than the mutant case and Gene B has more reads aligning in the wild type. So we'll always seek to look at comparisons between two or three or however many conditions are of interest for your experiment. And of course, another source of variation in the coverage or the reads mapping to a given gene is just random variation or some technical artifact or even just biological variation between cell cultures. And because of this, we need to have replicates to to nail down that the change that we do see from wild type and mutant cell culture is not just happening by chance. And so here I've shown for the same wild type and mutant cell examples of how the variation in reads might change from replicate to replicate for three replicates shown for each condition. And a nice quote that I always remember is, how can we detect genes for which accounts for read change between conditions more systematically than expected by chance. So the answer is, we must design experiments where this hypothesis can be tested. And so begins a long discussion about how many replicates and how to design our RNA seq experiment. So the summary of what we typically hear here when we're asked questions about how to design an experiment: how deep the sequence, how many biological replicates to choose. And of course, these questions are difficult to answer in general. A rule of thumb that we often tell people is that we need at least three replicates or 20 million reads per replicate to detect differences in strongly expressed genes. The best possible scenario is that you design a pilot experiments To test whether or not you're able to detect a gene that you're interested in. And with proper study design these pilot experiments can often be combined with your final With your final sample, so that you wouldn't have necessarily wasted money on sequencing. And the best possible case is that before you even sequence a single thing you talk to your sequencing core, your analysts to make sure that your experiment design is Going to be power to give you the type of results you're looking for. So I'll go through one example about one aspect of experiment design that I find very powerful. And it also will drill a hole one point about experiment design and the need to account for batch effects. So we'll look at some lessons from the mouse Encode study which was published first in 2014. And the study had a simple and important goal and it was to test "the common notion that major developmental pathways are highly conserved across a wide range of species, in particular across mammals." So another way to say this for this particular study, how close are the mouse and human in terms of gene expression across multiple tissues. Mice are very often used as a model organism. And we'd like to make sure that the gene expression, the conclusions you would draw from looking at gene expression in mouse in a particular tissue are going to be valid for human And so the initial publication showed something surprising, which is that the mouse and human samples cluster separately. So the figure you're looking at here on the left is a heat map that's made with using the hierarchical clustering methods which you'll see in this workshop and often and RNA seq Where on the x axis you have all sample, mouse and human, which are from different tissues and labeled either M from mouse of H for human. And again on the Y axis, you see all of the same sample. So a given cell is colored By a value which indicates how similar the sample on the x axis is to the sample on the Y axis is, for example, this dark red color means that they are entirely similar And you would expect by comparing a mouse over a sample to that same sample. So you see this on the diagonal. You see 100% similarity. So what they saw is Here's the quote is overall results indicate that there is considerable RNA expression diversity between humans and mice, well beyond what was described previously. Likely reflecting the fundamental physiological differences between these two organisms. So that was not expected and we expected, for example, to see that Human brains and mouse brains were more similar than mouse brains and mouse kidney, for example, that would be helpful for you continue to use the mouse as a model organism to study. So they were surprised by this result. However, further analysis indicated that batch effects were present. And once those batch effects were accounted for, they saw the expected clustering by tissue between human and mouse. Here we have the same figure generated in a similar manner. But for example, you see. mouse and human heart clustering more closely than mouse and mouse heart and mouse over your mouse heart and mouse pancreas. So the batch effects were there and they were able to be accounted for during the analysis, to be able to analyze the similarities across the tissues. The reason for this was at the study design was not optimal, and this came down to how samples were sequenced together on a flow cell and on an instrument. This table shows different samples that were sequenced together on a flow sell. Each column is a flow cell in a lane, the samples that are listed here are different tissues from mouse or human. Mouse is in blue and human is in red And you see that human samples were almost universally sequenced together, mouse cells were almost universally sequenced together. And because of the technical artifacts of just being processed together and the flow cell and the lane variation Those effects dominated and caused the samples that were sequenced to have similar characteristics in terms of gene expression as a cluster more closely. There are also another confounder, which was that many tissues were not sex matched, so you might have brain tissue in humans coming from a female organism and in mouse from a male organism. And there are sex specific gene expression differences so those can also account for some reasons that human and mouse might have looked further apart than they really were in terms of the underlying questions of interest. And so the takeaway from this and there's a great article here I've linked it at the bottom. That goes through a couple of studies, not just Encode where these batch effects were uncovered. So the takeaway is to avoid batch effects when possible, of course it's not always possible to avoid such effects for example on this Encode study The mouse where I think six week old litter mates and the humans were of a variety of ages, but mostly older where the types of tissues of interest were able to be obtained. And so, in cases where batch effects are not avoidable they'll need to be included somehow in our differential expression analysis as covariance So that ends the discussion of experiment design . Next, I'll go through just a brief overview of library preparation and sequencing. Would anyone like to ask a question? I do see a question here. Does this make it a bad idea to combine separate batches of data? For example, a pilot study with a follow up study. Rebecca E Batorsky: It is not a bad idea. Ideally you would have both your pilot study and your follow up study include all categories of things you're looking for. So if I have mouse and human and all different tissues. Well, it would be great if I had a good representation of those different those different variables, both in my pilot study and in my follow up studies so that the batch effect is not going to make me have some false differences or confounding differences between human or mouse. So you could imagine if the the pilot study had all kidney samples and the follow up study had all heart samples, you're going to see some differences between heart and kidney which originates solely from the batch. Now, if that's if it's unavoidable to really have a full sampling in the pilot and a full sampling in the follow up study, you can incorporate covariance, such as batch in your final analysis, and It will make your study less powerful to detect true differences from the conditions you're looking to test, for example, if it's tissue. You know, to some extent you can account for batch effects by including covariance in your data. If there is only a single effects, or only a single condition and those overlapped entirely with batches and there's no way to avoid it. You know, so all of my heart samples were sequenced all at once and all my kidney samples and another, including batch and tissue as covariance in the model, it won't be able to disentangle the two. So you need to mix it up somewhat, but you can also include this later on. And the statistical model will look to see whether or not the effect you observe is solely due to the the batch variable or whether or not it also can be due to the tissue differences. So that was a great question. Are there any other questions on study design? For the material we've covered so far. Okay. So we'd like to give a quick introduction. This is a very, very again big Picture introduction to the RNA seq library preparation and sequencing method This will hopefully motivate you to understand where all of these short reads come from that you'll be analyzing me in this workshop So we will start with, the process starts with RNA extraction, where the RNA in the cell is separated from all the other gunk in the cell. And because mRNA, which we're interested in is actually a tiny fraction, something like 5% of the total RNA, most of it being ribosomal RNA. mRNA enrichment is the second step in our protocol. And there are a number of ways to do this. For example, and in Eukaryotes to enrich for RNA with polyA tails. Once we have the mRNA enrichment step done, the mRNA that we've that we settled on are fragmented into 50 to 200 base pairs typically. And this is going to depend on which sequencing protocol you'll follow The next couple of steps. are specific to RNA seq and that is because our sequencer machines are designed to sequence DNA. The RNA fragments are randomly primed and reverse transcribed into cDNA, which is this little black string and then the second strand of the cDNA is synthesized so that we have the cDNA fragments to the cDNA fragments, sequencing adapters are ligated which will allow these fragments to bind to the sequencing flow cell, and then usually on the sequencing flow cell the fragments are amplified to give the instruments a signal to be able to observe And optical imaging on the high throughput sequencers enables the instrument to read out the different bases from one end of the fragment to the other end of the fragment. Another important detail for RNA sequencing is that a double stranded DNA contains RNA transcripts, both on the positive on the forward and the positive and the negative sense strands of the DNA. And that information about whether or not a gene with on the forward or the reverse strand is totally lost in this classic Illumina RNA sequencing library prep. It can be important, especially if you're looking at genes and organisms that may overlap where there'll be a given gene, Gene 1 on the forward strand and Gene 2 overlapping on the reverse strand. And so there are protocols that preserve this strand information you'll, you'll see this referred to as stranded sequencing. And although we won't go through that today, I highly recommend for people interested in stranded sequencing to look at this reference on this slide that give the pros and cons of a couple of different stranded sequencing protocol. I'm going to skip over for the sake of time, this next generation sequencing overview. So once we have our RNA RNA fragments in double stranded cDNA form with adapters. So go through the same process that DNA sequencing will use an Illumina machine to generate short reads But I'll leave it as an exercise to go through and look at this Illumina video which is very helpful for the visualization of this process. The last thing I'll go through is a quick introduction to our data set. It's from a 2015 paper called statistical models for RNA seq data derived from a two conditions 48 replicate experiments and This is also a great reference to go through if you're interested in learning about proper experiment design. It contains mRNA data from 48 biological replicates of 2 saccromyces cerevisiae populations. So two yeast populations. There are the wild type and what's called an SNF2 knockouts with delta SNF2 and it's really an unusually comprehensive analysis of the variability among sequencing replicates And you know this whole purpose of this experiment is to understand how many replicas are needed to observe a certain level of gene expression differences between these two populations. And again, we see here a heat map where samples are clustered on the left, they are The wild type samples. And on the right, they're the mutant knockout sample and the correlation here and the similarity is shown From where dark is less similar and light is more similar and the diagonal in this case goes from bottom to top right, just to keep us on our toes. I mean, see that there are some samples that are bad samples and some samples that are good samples and those samples are you know, analyzed together to try to understand about what the number we need with a given level of a sample dropout. in the course will take. Sorry. Wenwen Huo: I just muted two participants. I wonder if they have any questions. Rebecca E Batorsky: Okay, thanks. Rebecca E Batorsky: I'll just say one more thing and then we'll stop for questions for this whole section. So in the course we will consider subsets of this experimental data set, which is really a monster data set. We'll consider a seven sub samples from one wild type replicate and one SNF2 mutant, which we will use to demonstrate differences between populations and also look at the details of processing batches from different conditions. One of the big tech takeaways from this paper is that it's important to invest in replicates. So the most The most important way to improve the detection of the differential expression for especially for lowly expressed genes is to add more replicates rather than adding more reads So if you're doing your calculation about whether we should add another biological replicant or sequence to another 10 million reads Most often, another replicate is the way to go because you sampled the variation in the population and the following figure just shows that. It shows the average of, in black, it shows So here on the x axis position along leads and on the y axis is the read count in the position along this genome. And this is just an given position. The black line is showing an average of good replicates with the standard deviation shown in gray and the red line shows the average of bad replicate. So if you're looking to compare genes. You know, to have coverage variation like this between two different conditions. The paper concludes that you'll need six biological replicates and the yeast is a relatively simple transcriptome, so it would indicate that The majority of studies which we see have three or four replicates are losing something so they're not detecting changes and some genes, which might be might be underline And that's all for my introduction, I'll stop just for a minute to see if there any questions about the introduction material and then I'll pass it off to Wenwen. Rebecca E Batorsky: Are we going to have a workshop for single cell RNA seq? I would love to. you're going to help me organize it. Rebecca E Batorsky: We can follow up. I think that would be a great idea. Rebecca E Batorsky: Yeah, we will work on that. I'll follow up with you. Rebecca E Batorsky: Okay, thank you very much. I'm going to hand it over now. Wenwen Huo: Okay. Wenwen Huo: Going to share my screen. Wenwen Huo: Can you see my screen? Rebecca E Batorsky: Yes, I see the slides and you're not in presentation view, as far as I can see it. Wenwen Huo: Oh, no. Wenwen Huo: How about now. Yeah. Wenwen Huo: Cool. Okay, everybody. Wenwen Huo: So now let's get to some coding. Before I go into the analysis pipeline, I just want to give a brief introduction about Tufts HPC cluster, some basic command line and some basic R So Tufts HPC cluster is a compute cluster. You can think of it as a cluster of many mini computers. By clustering them together, it can perform high can perform some complex computations for many users with high performance. The computers are also called nodes. So here is a schematic view of what the nodes look like. The black nodes are computing notes and the blue nodes are storage nodes. When you try When you try to connect to the cluster from your laptop, you will enter the login note first. So think of this as a tiny computer that's assigned only to you, you can perform some simple tasks such as opening files, editing files. There are many, many ways to connect to the login node, but here we're recommending using Ondemand, and you have to use Firefox or Chrome to connect to the Ondemand After you enter Ondemand, you can click on clusters and then enter Tufts HPC shell access After you do that, it'll show you a black screen at the bottom of the black screen. You'll see your username shown up and as well as the login node here. Besides the login node, there are compute nodes. So here is where the high performance computing resources are located, and there are many different groups of the compute nodes called partitions. Since your login node has limited computing resource we highly recommend you to to always connect to the compute nodes, when you need to run your program. To get on the compute nodes you simply copy paste this line of code. So here shows that you want to require three hours of time, you want to request a 16 gigs of memory, and you want it to be on one node with four tasks, and for the purpose of our workshop practice, please use the preempt partition. There is a reservation under bioworkshop. So this will give you a compute node very quickly. And lastly, you want to do a pseudo terminal that runs the bash shell In your future study, you don't have to do partition with preempt, you can just leave that out. So that will give you a computer node that fits your need. So after you run that code, you will see here the computer node name will change from login001 to something else. Yours may not show up exactly as pcomp41 but it'll be different from login 001 so please always make sure the name has changed before you run your program, otherwise you might risk getting your jobs getting cut off. To transfer data between the cluster and your own laptop, you will use the transfer node. So to access the transfer node you again go to Ondemand. And then there's Files. Under Files, there's home directory and projects directory. Those two correlate with your own home directory and the project storage, the home directory is a very small space that's assigned only to you. This storage is five gigs and it's only visible to you. The project storage is for some collaborations. So if you're in collaboration, or if you're in a lab, you will have access to the project directory which will give you a bigger storage room so you can put larger files over there and it's also collaboration friendly. You can see each other's files. So for the purpose of our workshop today you will have access to our project storage for this workshop. So to access that you just click on projects here. After you click on projects, it'll take you to a screen that shows up like this. On the very top, it shows that you're located in the cluster folder within the Tufts folder and You can click on go to and copy paste the project storage that's listed on here. You can also find that on the online tutorial. You copy paste that line over here. Click OK. That will take you to our project storage and from there you will find your own folder and you own files to download, upload or View. Okay, so Now I just want to give a quick refresher on the command line basics. Some of you might already be familiar with this, but just as a refresher. I wanted to share some commands that I really, that I use all the time. First is the directory related commands. So when you work on your own laptop, you tend to save files in different folders to be organized And you will navigate through your folders when you need to locate a file. It is the same concept here, it's important to know which folder you are in and what files you have in that folder and how to navigate among folders. So PWD here will Print Working Directory. So this will show you which folder you're in right now and LS will list all the content in that folder. And CD is very important for you to change directory to another folder. To go back one folder you do CD space .. This will go back up one directory. If you want to go back up two, you do CD ../... so you get the concept, you can just do cd ../../ to go back many different directories. So here is an example to get started on the workshop material. So after I login. I see my username. I see the login node. And here I want to print working directory. After I hit enter, it'll say, Okay, I'm at my cluster home directory for my username. And then I want to change directory using CD, plus our project folder and that will take me to the project folder and you can see the folder name here changes to users. Once I'm in the project folder, I want to create a new folder using make directory or MKDIR with the name of my username. So this way, I will have my own folder to work with. After I create a new folder I change the directory to that new folder that I just created. To copy the course material you simply do copy or cp plus the the path to the course material with the name of the course material, and you want to copy that to the current directory, you use . to represent the current directory Because this file is a compressed GZ file, you want to do tar to un-compressed it After that's all done, you can change the director again into the course material. So, finally, here is where all your course material is stored. And now if you do list content. You can see there is a info text file, there are two folders, one folder called raw data, one folder called scripts. So another way to see the content of a folder is to use tree. What that looks like is that if you do tree, it'll still show you the same information. So here shows within the course material you have a info text TXT file. You have raw data folder, you have Scripts folder. Within the raw data folder you have simple info TXT file, you have two sub folders called SNF2 and wild type. Within each folder, there are seven fastq file that contains the raw sequencing reads. So sometimes when you run multiple commands, you can actually instead of running one line at a time, you can compile them into one script file. With an extension of SH and run that scripts using SH command. So when you run the script using SH command. Make sure that you contain the path, sometimes you're if you just run SH with the name of the scripts. It'll search for the current working directory And if it doesn't show up, it won't run that script. For example, here, within the course material somewhere there's a script file called fastqc.sh If I do sh fastqc.sh, it'll tell me no such file or directory When I look back I realized that that's within the Scripts folder. So I have to do is I do SH, I put a folder name here with the slash showing you that I want the sh to look through Scripts folder and find the fastqc.sh and it'll show you that the scripts has started running Sometimes, at some point, you will need to edit a script. So here we're recommending using nano editor. It's very, it's pretty straightforward. You just open another editor using nano with the file name, and if that file name is within a sub folder. Make sure you give the path to the sub folder. So somewhere in the tutorial we would like you to practice We would like you to copy a pre written script to a new script and renamed that new script. After that, we want you to use nano to edit the new script that you just created. So once you do this, you will see Nano editor brings up the screen like this. So on the top, it'll show the file that you're currently looking at, it's within Scripts folder and the name is called star_align_ERR458500.sh And because this file was copied from a previously existed file. That's why you will see the content. That's why you'll see there's already content there. And the red cursor here shows where your current a cursor is and you can use the navigation, the arrow keys on your keyboard to navigate up and down, left and right. And make edits wherever you like. And after you're done making edits you click ctrl-x to exit this nano editor. If you made modifications, nano editor will ask you if you want to save that or not, you can just say yes or no. Sometimes I would like to quick view of file without opening nano editor To do that I can do head plus the file name, so this will show the top 10 lines of a file. It is very useful if your file is really big. And you only want to see the first top 10 lines. And you can also use cat plus the file name. This will catch all of the content in this file and print it to the screen. So if your file is really big, I wouldn't recommend you doing cat. Sometimes you'll see a lot of coders, they use the bar sign a lot. Don't get freaked out. If you see this, it simply means chaining the commands together. So here it shows that I want to catch all the content from this file name, use that as an input for the next command which is the head, so I'm catching all the content, but I'm only printing the top five (*ten) lines. There are many other ways to chain the commends, but it just means the previous output is the next input. So after I made changes in the new scripts file. I want to see if the file looks right. So that's where I do cat with the scripts name and then the the cat will print the scripts to the screen. Looking just like the nano editor, but now it's on the screen. And then the comments here, the lines that starts with the pound sign simply means it is a comment. And sh won't read that line. This is a comment to yourself only to recommend to remind yourself something. And then I see there are two changes made compared to the original. Originally I had wild type here, I had a wild type here. So I see the changes were made. And after I made sure the changes were made correctly. I run it using SH command. So this would take actually less than 5 minutes to finish. So a little bit about R. So to run R scripts, there are two ways to run R scripts. One is to use Rscript with the scripts name. So this is pretty similar with the SH script you see before. This way, you don't have to bring up an R interface. All you have to do is write all the commands you'd like in one file and this line will run one line at a time and print out whatever you want to print out Alternatively, you can work with Rstudio. So most of our course material is based on Rstudio. So you will find online tutorial to see how to setup and enter Rstudio. Once you're in Rstudio here are the top three things that I find useful useful working with R. So first is similar with with working with command lines, always know you're working directory Because if you're not saving your data in your working directory you won't be able to access it. So once I open Rstudio, I usually do Get Working Director with parenthesis, then hit enter, and this will print out the current working directory And when you first started, your current working directory is probably your home directory. In order to navigate to our project folder, you want to do set working directory to our project folder using this line here. Because our analysis will use a lot of libraries, or packages, instead of installing those packages yourself you can use this line here to include a previously built library that's located in this location. And after you do this, you will be accessed, you'll be able to access the libraries that were pre built. So this way you can load all the libraries, such as the DESeq2 without installing yourself. And thirdly, this is how you assign a variable. So here for example this line, I am reading a table. That table is from a file within the raw data folder and the name of the file is called sampleinfo.txt. And after I read that table, I want to assign this variable into a variable name called meta So this will be what it looks like when you open R studio. So here shows the scripts editor and you can edit all the scripts here. You can also run the scripts. Run a specific line of scripts by putting your cursor on that line and hit run after you do that, the line that you want to run will show up in the console. or you can directly typing in your command in the console and that will run that command as well. And here is the saved objects or the saved variables. For example, the meta I just created from the previous slide, once I hit Enter, it'll show up here. So this way I can keep track of all the variables that I created. Here you can navigate through the files, you can, if you are generating plot the plot will show up in this tab. And you can also see what packages you have loaded in this current working session. One thing is that my little trick is that I would like to set working directory here to the project folder and then I navigate over here. Go to the Files and hit More and I think there's an option for you to go to the working directory so that way you can set the working directory here and then you will see all the files over here after you do go to the working directory. So this way you don't have to navigate a lot in the files. panel here. Okay, so now let's get to the analysis pipeline. So here shows the general steps for the analysis. And this is the schematic view from Rebecca's introduction, where we have a wild type phenotype from yeast and we have a mutant phenotype from yeast So the process reads is basically a quality control that will look at the raw reads to see how good their quality was and the read alignment basically will take care of the Aligning Align all the reads to the same reference. So after read alignment, you will see all the reads aligned and you'll be able to visualize that. Then it's Gene Quantification for each gene or transcript, so it'll calculate the number of mapped reads in Gene A Gene B GeneC in both wild type and mutant phenotype and the differential expression will be tested using R. so this step will normalize those two samples to make them comparable to each other and calculate a Log2 fold change for each individual gene. and for functional enrichment, basically we're selecting some significant genes that fit our criteria, for example, Gene C here might be interesting to look at and functional enrichment analysis will tell me How many genes are like Gene C and what categories those genes belong to, what pathways they affect So, as previously shown, our raw data contains two conditions: wild type and SNF2. Within each condition, there are seven fastq files. So for the purpose of our practice. Let's think of each fastq file as an individual replicate. So in this case, we have two conditions and each condition had seven replicates The quality control and the read alignment will be performed on each individual one separately, and the feature count or Gene quantification will summarize those read count will summarize those read alignment and make them into one read count table and that one read count table will be used as an input for R analysis for differential expression and functional enrichment. So to run the quality control. We will use a software called fastQC and in order to use that, first you have to load that module. Think of this as in, you have to open a software before you can use that software. So on the cluster, you load that module and then Here we're also making a new directory called fastqc to save all of the output files, just so you're now saving everything in your home folder. Once you do that, you simply put fastqc to start that software. And here you want to designate where your fastq files locate. So in this case, they're located in raw data folder within wild type folder and They will have extension of fastq.gz. And here I'm saying that, please save the output in the fastqc sub folder and because the fastq are compressed in GZ format. I want FASTQC to extract that GZ format. And here you will see the star sign here, you may not be familiar with this. but it simply means it's a wild card. So it means that I want this fast QC to scan through everything with the wild type folder as long as it has an extension that matches FASTQ.GZ Please use that as input for fastq QC. So this is a very important, this is very frequently used wild card, you can use that in many scenarios. Now if you navigate to your project folder, you'll be able to view the results, looking like this from the storage, from the storage transfer node. After QC, in the online tutorial, we showed you one example of how to align one fastq file using STAR alignment. Before you move on to the feature count step, we would like you to practice aligning all other fastq files yourself. Eventually, we are expecting one alignment per fastq file and these alignment files will be used as input for the feature count. We also wrote a script to process all reads automatically. So feel free to use that script by running this line of code. It's saved in the Scripts folder. After you ran this successfully, you will be ready for the next step. And the next step is feature count. And follow the online tutorial to make your own script file and run it. Or you can run our prepared to feature count script within Scripts folder. After it ran, the output will be a feature count table looking like this. So here I'm using HEAD feature count slash feature count result txt to look at the top 10 lines. It looks a little messy on the top two, on the top two lines that simply because the name here is really long. And we provided a one line code to clean that name. And after you do that and you import it into R it'll be more visual friendly. It's actually a table that each row is a transcript name and each column is a sample name. So since we have two conditions and seven replicates pre condition, this table here will continue 14 columns. Two optional steps before we move on to the next step is that first, you can check how many reads can be aligned. You will see the purple line shows uniquely mapped reads number, it's good that most of the reads were uniquely mapped for all the samples. And you can obtain this This plot by running a script here. Another optional step is to look at the assigned features to the transcripts. So the orange color here shows assigned features. So it's good that most of the reads were assigned to a known transcript so that that's an indication that all the mapping was good and it's ready for the next step. So okay, so for the last two steps, differential expression and function enrichment, we will use R analysis. The input file for differential expression is the feature count table from the previous slide. Again, each row is a transcript name and each column is a sample name. And you will also need a meta table here. So we created this meta table for you. In case you will do your analysis in the future, just letting you know that you will have to create this meta table yourself. And some simple rules of creating this meta table is that the row name here has to match the column names here, he order doesn't matter (correction: the order does matter), but the name has to match. This is how R will match this (meta) to table (data). And also the "condition" here. So this shows what condition each sample is. For example, SNF2 here is under conditions of deletion of SNF2 And those are wild type. And the name of the condition here, the column name, has to match The variable after design here. So those are the two input tables that you will read into R and then you will run a default pipeline by running this block of code. One thing I need to mention here is that the contrast line. This is how you tell the R software how you want the Log 2 fold change to be calculated. So here I'm saying SNF2 before wild type. That means that I want the results to be represented in SNF2 against wild type. So if I see a positive Log2 fold change, that means, that means there is an up regulation in SNF2; if you reverse this order the effect will be reversed. So after you run this block of code, you will have two output tables. One is results table, one is rld count table. So the result table is a log2Fold change table looking like this where each row is a transcript. And the different columns represent different things, the things that you will care the most will be the Log2 fold change, shown here, the P value. The P value, shown here, and also the adjusted p value, shown here. And later on, you can use your thresholds of different P-value or p-adjusted value Or log2 fold change to filter this table to get your genes or get your transcripts of interests to see which ones you're more interested in to explore more Another table is RLD counts table. So this is a regularized log transformed counts by using this equation here. It's basically calculating, transforming your original feature account into a new normalized table. and this one will be having 14 columns, one per each replicate. For these two tables, you can save them into two different files and open them up using Excel or some other analysis tools, and and do your own analysis. But here, we're continuing using R to do the exploratory analysis. So first, the DESeq2 package comes with a plot PCA function. So, this will this will take the RLD table and plot it out. To be to be a plot looking like this. So this is similar to the plot that Rebecca introduced, the heat map plot. Basically this is looking at how far Each sample is from each other. And you can see the fact that all the wild type replicates clustered on one end, and all SNF2 clustered on the other end means that replicates were done pretty well. All the wild type are similar to each other and all the SNF2 were similar to each other. So this will be similar to the heat map that she showed you, they can contain similar information. But the heat map will be more (contain slightly more information) It'll calculate the distance among wild type (or SNF2) as well. You can do both (plots), but in our tutorial, we're doing the PCA analysis. If you need, we can help you with the heat map generation as well. So people also like volcano plot. It's a negative log10 adjusted P value against the log2 fold change plot, and this will show, each individual dot here represents one single transcript. So this will give you a global view of how the differential expression is among all transcripts. So we also provided some code that you can follow. Here we're labeling all of the significant up or down regulated genes in red, and that definition is defined using P adjusted value less than 0.05 and the absolute value of log2 fold change larger than one I'm sorry, do you have, anybody has questions? Rebecca E Batorsky: There's one question in the chat, do you correct for gene length bias or other biases in this analysis? Wenwen Huo: Know. No, I don't think we did that in our pipeline, do we? Rebecca E Batorsky: We do, in the normalization stage, you will correct For not gene length bias, because in this case we are not comparing We're not looking for differences in from gene to gene. We're looking for differences between Wild type samples and mutant samples in a given gene. So the gene length would be the same from in one sample to another, for example, differences in expression for SNF2 Gene are going to be, you know, the gene length itself is the same from in the two different samples. There are different effects that can be important and are accounted for in the normalization stage. Let's see. Wenwen Huo: Yeah, I don't think the normalization for DESeq2 will account for gene length. They're using a different method. So it's different than RPKM or TPM. Rebecca E Batorsky: That's right. The methods that they use is the blanking on the name now, but I think it's the trimmed mean of median. So it is a normalization that accounts for Primarily the composition. The composition of genes in your sample. And this is a little bit of a deep topic. Basically, if you have in some samples, you have a lot, you know, all of your genes are expressed in the same level and then another sample you have some genes that are expressed in a very low level and other genes that are expressed in a very high level, you won't be able to accurately measure differential expression because reads are finite resource on your sequencer. If you ask them, the core for 10 million reads and in some samples, they're equally distributed among genes, and some samples, there are a few very highly, in some samples, they're equally distributed among genes I meant to say and other samples, there are some extremely highly expressed genes that take up all of the reads it's going to make the vast majority of not highly expressed genes look like they have artificially depressed read counts. And that's because there just wasn't enough reads to go around sequencing very highly expressed genes, along with a lot of average expressed genes, and so that that's a little bit hard to explain, without some figures which we didn't include in this presentation, but I can also post those to Piazza But the takeaway is the DESeq2 workflow that we're talking about here does account for a normalization step which accounts for overall sequencing depth. So if you sequence, that's the number one thing, the place I should have started, if you have your wild type sequenced sample For example, with only average of 10 million reads per sample and you have your SNF2 Sequence samples with an average of 100 million reads, of course, you're going to see all gene higher expressed in your SNF2 sample. However, you need to do some normalization so that that's not the effect you observe. Another one is the composition effects that I noted and there's one or two more. We don't do lane correction because of what we mentioned, because we're just comparing one gene between samples. But yes, there is a pretty sophisticated normalization scheme built into DESeq2, and out of this DDS data objects that that Wenwen has shown here. You can actually extract the normalized account matrix which will give you a sense of how your original raw counts have been transformed by the normalization scheme. Wenwen Huo: Yep. Rebecca E Batorsky: Any other questions about normalization. That was a long winded response. Rebecca E Batorsky: Cool. Wenwen Huo: So because we only have one hour workshop today. That's why we simplified a lot of the actual steps behind this, all the logistics behind all these steps. If you're interested, please go Wenwen Huo: Read our workshop (online). Also later on I will provide you another link that has more detailed explanations and some tweaks that you can do. To fit your own needs. So like Rebecca said, this is a really complex analysis, it can have many different things, you can do a lot of things. And here we're only providing one example. Okay, so after the volcano plot you see there are a bunch of up or down regulated genes and from here you might want to select some genes. For example, you may want to select those genes. to be your gene of interest to look at. So once you have the list of the genes, you want to explore more, you can plot out a heat map table looking like this. So, this one will show you how the genes are clustered together and in this heat map, they also did a clustering using dendrogram shown here. It's pretty tiny for you to look at you here. But, it will actually, you can actually look at it when you generate the file yourself. But here there's dendrogram showing you which genes are closely behaved with another gene You can identify a cluster of genes that behave similarly under certain condition. And this can be very useful because if you want to construct a for example co expression network. This will be super helpful tool. And finally we come to the functional enrichment analysis. And if you're working with well known organisms such as human, mouse or yeast here or E. Coli in bacteria or malaria, you're very lucky because you can use cluster profiler. The cluster profiler created many databases for the well known organisms, and by loading those By loading the cluster profiler and the database, you can simply put in a few codes to generate a plot looking like this. So here shows each row is a pathway that are enriched in your Interested gene list. And the x axis here shows how enriched that pathway is. So the top right here, those three pathways, they're very much enriched for the gene list that you provided. So this is very straightforward. However, cluster profiler only supports about 20 organisms. In your future study, if your organism is not within this list, I recommend you check out String analysis. So it's a web server. And it also takes a list of gene names that you're interested in. After you put in that gene list, it will plot out an interaction network looking like this. As well as the GO terms that are enriched within this network. And this web server will also provide you a list of literatures in case you want to explore more for a particular sub network. I personally think it's very powerful and useful website to check out. And another, other tools such as BLAST2GO and BioCyc can also be useful. If you're interested in those we can talk more about it later. So moving on from here, we recommend everybody to follow the online tutorial. The goals are to get yourself familiar with the command line and R scripts, practice writing your own commands, as well as adjusting the commands to fit your own needs. Wenwen Huo: And feel free to apply the pipeline to your own data. And my simple trick to do this is that I will probably move my own fastq reads into the raw data folder, so that this way, it'll keep all the structure and It'll allow me to adjust the existing scripts with the least amount of modification and then I can apply the same pipeline. So if you have questions, post it on Piazza and we will actively answer those questions on Piazza A quick reminder is that, our projects, our provided project folder will be available for the next 30 days. So all the data will be deleted after 30 days. So if you want to save any of that, make sure you do that within the next 30 days. And here are some advanced reading material. First is the workshop that was created by Harvard Chen school (bioinformatics core), I think. So this, our workshop adapted some material from their workshop and they have really extensive explanations on everything throughout the procedure. So I recommend you to check out that And here is a package description for DESeq2, and you will be able to find a lot more tools that can be used, and different plots can be plotted, it's cool to read that as well. Another software that I want to introduce is Geneious Prime, not primer, it is a typo here. It is Geneious Prime Tufts recently has bought the license for Geneious Prime. So it's a windows based software. It's really user friendly and straightforward. I use that for my daily sequencing analysis for years and they have a Gene expression analysis tutorial as well. And their analysis is also based on DESeq2 too, I believe. If you don't want to get started on the command line, you can try to start using Geneious to get a general idea of how RNA seq works and what it looks like. One drawback is that Because you have to install this on your own computer. So if you are dealing with big RNA seq analysis, your computer may not have enough memory. So with that, I would like to thank for your attention and before you leave, if you want to test it out, please feel free to do so. So here are the initiating codes for the Bash shell access or the Rstudios. So we would like you to practice to see if you can make this work before you leave. But if you have other things to do, please feel free to go and With that, we would like to take any questions. Wenwen Huo: Should I just stop sharing or keep this slide up? Rebecca E Batorsky: I think it'd be helpful to keep that, keep that slide for a little bit, people can feel free to ask questions. Rebecca E Batorsky: I'm going to post on Piazza with a number of the questions that we got, just some follow up information. There was a one more question from the chat. If we are using R studio, we do not need to do the shell? Correct. For R, that's correct. But as you'll see on the course site. The first part of the pipeline is done just in the shell and not in R. So you'll have to do both at some point if you want to complete the whole workflow. Wenwen Huo: Yeah. Rebecca E Batorsky: I'll just plug again in case people missed it in the beginning, that we will be having office hours, you can find them on Piazza, on Thursday at 11 so same time on Thursday. There's a different zoom link that's posted there. So if people get stuck anywhere you can post your questions to Piazza or if you just want to show up and ask question. That would be a good time to do so Wenwen Huo: And if you have any question on initiating either shell where R studio, feel free to ask questions now. Or share your screen, so we can help you figure out what's going on. Sounds good. Daniel. Thank you for being here. Rebecca E Batorsky: Yeah. Thanks everybody. Rebecca E Batorsky: Alright, so are there similar pipeline toolboxes that you can recommend for Python? You mean, I assume the thing means to be loaded in Python as module so you can write Python code? What I do if I want to write a Python workflow, which is, I assume what what this question is asking is, I will use the execute shell command from within a python script, because of the way these these tools are packaged, if they're meant to run just on the command line. It's best to run it that way. I don't know of ways to execute this entire workflow within Python itself, I would say. Probably not necessary to do that. Wenwen Huo: Yeah, I think The DESeq2 was Specially designed for R. If you search online, there might be adaptation, so that you can load that as a package in Python, but I don't believe that's the official designs. So yeah, I'm not sure about Python usage. Rebecca E Batorsky: Yeah, and unless you're actually editing the code itself like making modifications, I would say there's really no difference between shelling out and using R or using STAR aligner and then doing the rest of your manipulations of the files in Python. I would say it's, you know, just exactly the same. Yeah. Wenwen Huo: And then the OnDemand On OnDemand, you can set up Rstudio we can set up Python is actually a very powerful website. I really recommend using OnDemand. Rebecca E Batorsky: I'll also say for everyone that's still here we have a galaxy platform. And I'll post this in Piazza just for people who don't really want to spend the time to get familiar with Linux and HPC. You can do a lot of these workflows in our user interface on galaxy. And we host one at Tufts. Rebecca E Batorsky: Okay, everybody. Thank you so much Wenwen, this is great. It's a great. Thank you for your help. Okay, I'm gonna hang up and we'll talk to you on Thursday. Wenwen Huo: Cool. Thanks everybody. Rebecca E Batorsky: Bye bye. 