 good evening to those who view her joining this session my name is David Sexton I'm the head of genome technology and informatics at Biogen I am joined today by John L Thorpe who is the chief product officer at DNAnexus and Frank note who is the technical director of healthcare and life sciences at data Brooks so the agenda for today is I will be speaking about the what the UK biobank actually is and why Biogen was not prepared for data at this scale John will be speaking about large-scale analysis of the UK biobank data using d and nexus and how DNAnexus helped us in scaling up the analysis of this data and Frank no text will be speaking about combining the best in breed cloud architectures to axillary UK biobank analysis and he will also speak to how data BRICS has helped Biogen to scale out a solution so what is the UK biobank and and how has Biogen used it to discover new therapeutics so the UK biobank is the premier data set for associating disease to genetics in the world right now so this is a long-term study of genetic predisposition and environmental exposure to the development of disease this data set has been collected in participants who are aged 40 to 69 this is over 30 years of phenotypic data and these patients are being followed over the long term measuring their their health over that 30-year time span there are 22 centers across the United Kingdom with over 500,000 volunteers and this is one of the largest and most detailed population studies ever undertaken and you can see that there are a large number of papers that have come out of the UK biobank so what genetic data is collected by the UK biobank in 2017 eight partner companies came together to form the life sciences to it's consortium as part of this consortium we will be sequencing the exomes of 500,000 of the UK biobank participants an exome is the protein coding region of a gene and as part of the consortium Biogen will be probing all of that genomic region for the protein coding genes in the 500,000 participants the participants of the UK biobank consortium Regeneron and GSK sequence the first 50 thousand samples and all 500,000 participants are to be sequenced in 2020 industry partners will have exclusivity until 2021 and we currently have 300,000 exomes in-house so how is Biogen leveraging the UK biobank data we are using human genetic evidence to rank our drug portfolio we are using the data to find new gene targets and we're using it to understand neurological disease biology so by our Biogen had some informatics challenges in using this data so our data infrastructure had challenges in not having enough storage capacity and data center so we were the UK biobank data will be approximately 1 petabyte of data and we did not have that storage currently in our data center we had issues with our our network and not having enough bandwidth to transfer all of this data across to our data center and when I came into Biogen in 2018 we had just had a one-week outage of our high performance compute cluster so we really needed a new data paradigm for bhaiyyaji for Biogen and this is where DNAnexus and data bricks has helped us so we needed to scale our infrastructure to deal with petabyte data set sizes we need to store and visualize our genomic data we needed to analyze this data at scale and we were required by a Biogen IT to be a to be cloud first and story and using our data and so now I will turn it over to John to talk about scaling the UK biobank data using DNA Nexus is Titan and Apollo products thank you David so when we look at the UK biobank and what Biogen needed to do easily it's useful to look at the steps that are required in processing a large genomic data set and so we've really break this up into two different pieces so there's one piece which is an upfront piece where you look at preparing the high-quality data set so the data that's coming off of a sequencing machine are these raw reads and you don't get the understanding of exactly what the the variants are and so there's a processing step that needs to happen across the entire five hundred thousand exhumed data set to be able to build that high-quality genomic data set and there's a second piece of combining that with the health and assessment data that's like demographic data and other types of data that get brought in as well that then needs to get combined into a large corpus of data that then scientists can actually go and use by querying the data asking questions that they didn't do statistical analyses to get to the final result of the data when we look at the UK biobank data that dataset is challenging because of the sheer size and complexity of the data so if we look on the right you see on the genomic side we have 500,000 participants in that data set each participant can have up to millions of variants that they are tracking across these that now give you essentially trillions of data point that that are needed to be able to understand what that looks like purely on the genomic side on the so-called phenotypic side which would might be demographic information health information clinical information that might come from medical records that is a wide data set there are a lot of different fields there over 3500 different phenotypic fields that are there and they're also quite complex in that you might have significant coatings of the values you might have hierarchies in terms of what the values could be you also have some level longitudinal data because people came in multiple times into the Assessment Center to measure things like blood pressure and things like that so there's a longitudinal aspect to the data and so once you've combined all these two together you have essentially a very very large data set that you have to manage to be able to do the things that David mentioned needing to do so what the when we look let's focus now at the different stages of this process that we could talk a little bit about how DNAnexus helped in the in this first stage with 500,000 samples you end up with about two million files as you have the alignment files as well as the output so called VCF files as well as index files that go along with it we computed that it's around one and a half petabytes of data that you have to process and what's really important about this this data is you really need to do it in a in a high-quality consistent manner and so if we look at the stages within that you might have you might take the raw reads and then you do a so-called alignment step which is aligning it to a reference genome to really identify where the different fragments stacked up and then identify for every point along the eggs oh like what exactly is the is a call at that point in that snip along with a variety of quality control effects and so this this data set was processed at that Regeneron genomics Center they got the samples in what process they have a cost optimized pipeline that takes about four hours to run first and so across the five we found 500 thousand samples we're looking at millions of hours on a machine and so that's a large no problem and so if we look at the the technology that that they used they use the titan product to be able to to process those they think that when we look at why this is hard if you're processing a few samples it's actually not that hard to do yourself in the cloud but once you get into the the levels of thousands tens of thousands hundreds of thousands of samples you really want to do it consistently and efficiently then the fault tolerance into the clouds are very important the ability to focus only on exceptions and in the science are not having to deal with cloud optimizations and such as is also quite important itself is moving data around and making sure that you have insistency in the data integrity is also incredibly important and for a research environment it's also quite important that you actually use the tools that that are particularly important and relevant to you and so these are all the pieces of a of what Titan can do and for example we had a another pharmaceutical company as part of the consortium and they reran a hundred thousand samples processing about a thousand exomes an hour and so in three and a half days were it be able to process 100,000 exomes and so like that capability is really required at scale as you go and process education we process the data in a part if I move to the next stage of the process now we have to combine that large corpus of genomic data with this clinical data and while they were 3,500 or more field it ended up being something like 11,000 plus columns that needed to get stored in in the system and they combined to be able to do effectively query the data as part of the consortium we provided a cohort browser that allowed the researchers to visually go through and interact with the data inquiry and ask questions of the data Explorer the data I'll show that in a second and what was quite important about that is that the amount of data so the data is both wide and deep so we ended up with something like 50 billion rows to go across the entire genomic data set down to the variant level as well as needing hundreds of tables to to manage the columns of data and so that those made real-time interaction with the data are quite difficult and so what we did is that this is built using the Apollo technology which takes the high quality data sets that are that are being processed out of Titan and combines it with sources of data that is the health assessment data and provides you an ability to do interact with it in a in a variety of ways it's important that with the data set this large you can't move the data around to the researchers and to the tools you really need to bring the tools and the researchers to the data and so this is the core aspect and the core engine that the powering Apollo is a spark based engine and so that's SPARC based engine is ones that is using spark sequel to be able to query down into that data but in order to do this we needed to do a few things so one is you we needed to be able to partition to the data heavily based on genomic location to really be able to query this quickly as well as do vertical table splitting essentially vertical partitioning to be able to query fast enough because the there are so much metadata across 11,000 columns you'd have to be you have to be very intelligent how they're actually free to be able to get response time in the seconds across this this data set and be able to really explore visually rapidly to be able to get that and then as well as being able to provide data Sciences that capability through Jupiter notebooks and other mechanism be able to enter the data and do statistical analyses and foot with what you the tools and the Python libraries or our script and things like that that they'd want to do another important aspect is to keep this all security in our environment so we've integrated the archive meta store into our platform access control model to really control the security of the massive data set and so that was incredibly important as we build this entire system together and provided it out to the authorized researchers that were part of the UK biobank and four-ship access well you know thank you very much John and David for you know leading into this section I'm pre Noatak I'm the technical director for healthcare and life sciences at data bricks so I manage our worldwide technical technical efforts beyond the product development side with our genomics runtime via a solution architecture team that works closely with with customers as well as in some of our functions working with partners like DNAnexus and what I'll talk about is you know how the data bricks platform comes into this and then how we work collaboratively between the DNAnexus Biogen and data bricks teams to achieve success in analyzing this large-scale UK biobank data so this slide summarizes what our data bricks platform looks like in the genomic space the way that we you know if you're familiar with the data bricks platform you know we have a kind of a cloud infrastructure layer that Optima the machines you work with who have a top layer that provides notebook functionality that makes it easy to use notebooks and a reproducible and shareable way but in the middle layer we provide a number of different runtimes that provide optimized software up to my software stacks for various different tasks that a customer's working on be it something like processing large scale streaming datasets doing machine learning or as we introduced in in 2018 and made generally available last year we've introduced a runtime specifically targeted at genomic data workflows our workflow covers the whole whole gamut of tasks from initial data processing through the large-scale statistical analysis variation data we've been able to use all of these workflows that Biogen but to focus on a couple of different ones on the upfront execution side what we've gone ahead and we've done is we've taken the GE T case best practice pipeline so for those of you who are familiar with genomics gatk is a standard pipeline standard set of pipelines for taking single you know single individuals you know raw DNA sequencing reads and turning it into either germline variants or mutation calls if they have if you're looking at a cancer data we've taken these pipelines we've made them easy to use through a one-click interface that will set them up and run them takes about five minutes to get the pipeline set up making it a lot easier to access these we've also extensively performance optimized them and made them work well with SPARC so ultimately we've been able to do things like reduce the latency by you know of running the gatk germline pipeline on a high coverage whole genome from 30 hours to under 40 minutes by having about a 2x performance improvement from a cpu efficiency perspective and then using the power of SPARC to paralyze this work across many corals we've then gone ahead and we've taken a very big focus at working on population scale data you know we actually have extended support for the GE T case during genotyping pipeline so this is the pipeline's that takes data from many single samples and blends it together into a single population we've accelerated that and paralyzes that out using spark and then we've worked to package up a cop open-source libraries Heil which comes from the Broad Institute & Glow which is a project we've actually developed here a data bricks in conjunction with the Regeneron genetic Center that allow people to go ahead and merge these citizens together while we control them and ultimately run large-scale statistical analyses on top of that data our ultimate ambition here is to move people to an architecture where they're able to use open source technologies like Lowe that make it easy to use you know many different languages be they Python are sequel on top of genomic data couple it along with efficient optimized open source file formats like our Delta Lake file format which will open source so that they can go ahead and accelerate the process of taking large number datasets wrangling and cleaning them up joining them with a variety of different data types be they clinical data be they images be the other lab measures in ultimately produce a set of Jewess results or other other statistical results that they can do machine learning on top of to generate scores in that they can go ahead and serve directly to research and clinical audiences when we look at some of the work that we've done done on top the UK bio bank data add Biogen you know go ahead and highlight some of the work that we've collaborated on with David's team around some of their genome-wide Association pipelines so with a you know with the G Watts pipelines what this is is essentially a statistical kernel that takes every single genomic variance in the data set and the phenotypes that were interested in and goes ahead and perform some sort of a statistical test to see if these two are associate you know this could be if I'm looking at let's say a common continuously distributed variable like height you know this could be something like a linear association between every single genomic variant and every single phenotype or if I'm looking at something more complex I might be using you know more complex tests like a Cox proportional hazards model or something like that with the UK bio Bank data set this is particularly challenging because the the amount of data that we're dealing with is very large you know UK biobank has over 2,000 phenotypes as well as over you know with the exome sequencing data there are tens of millions of variants that are associated so when you go ahead and do that full cross now you can be looking at running billions of billions of regression tests to associate this data together with the pipeline that that we were able to build out first we were able to go ahead and use the open source hale tool to ingest this data very rapidly and start generating our first results when these results were generated the Biogen team was able to take some of their some of their traditional annotation pipelines these these are tools that take those variants that we found that seem to have some sort of an association with the disease that were interested and add additional functional consequences to them you know is this a is this a variant that truncates of protein is this a variant we've seen in other diseases is this a variant that we know changes how a gene is expressed they're able to take a pipeline that previously took two weeks to process 700,000 variants and accelerated tremendously you know they were able to annotate two million variants than in 15 minutes so they had orders of magnitude acceleration there and ultimately this gave them a rapidly queryable database of genotype phenotype associations with joined and consequences that allowed them to really understand how these variants how these variants function and what these variants did ultimately and you know this is really exciting you know just earlier this month the Biogen team released the pre prints on some this work that summarizes the effects that they found in variants that are protein-protein truncating variants so these are a genetic change that causes a gene to you know be truncated so you don't get the full the full copy of the protein you instead get a a scrambled scrambled copy that doesn't produce a correct thing they've been able to find a number of variants in about six different genes that have a significant impact on human law on human lifespan and they've been able to understand the biology of complex diseases a bit better through that ultimately when we look at how all of this pairs together the the great thing that the Biogen team has been able to achieve is they've been able to achieve an architecture where they are able to use their their own environment data coming from the Regeneron genetic Center from this UK biobank cohort and they've been able to blend the DNA and axis and data works platforms together ultimately this gives them the best of breed best of breed solution where they have access to DNA Nexus platform with many best practice pipelines and best practice visualization tools across both the Apollo and the Titan project so they're able to quickly spin that up their able to quickly run the pipelines that they need ultimately generate the visualizations that they need for their bench science and clinical teams and they've been able to use the data bricks platform to do a really deep dive into into these workflows so ultimately by going ahead and combining the capabilities of these these three teams together bio Jen's you know expertise in the data their expertise in the science the you know the great tooling that's available on both the DNA Nexus and data Berks platforms we've been able to take a large set of challenges you know taking this mass amount of raw data you know over a petabyte of data from 500,000 individuals complexities around you know a traditional ecosystem that that I again had a need to to the cloud and ultimately the Biogen team has been able to deliver a lot of success with the findings that they have from this very large-scale database of comprehensive variation linked to comprehensive phenotypes they've been able to go ahead and identify new drug targets they've also been able to build out models that allow them to understand you know how how genomic variants impact you know impact the functionality and the possible success of other drugs that they've been developing so they've been able to go ahead and reposition and reprioritize their drug portfolio and ultimately as you look at the complex neurodegenerative diseases that Biogen is working on data sets like this give them a lot more insight a lot more precision a lot more ability to interrogate the complex biology of neurodegenerative disease so as you know as every month passes as we mind this dataset more we're all you know we are all developing as a community a better understanding of complex human disease with the power of this combined genotype and phenotype data in ultimately I've been very excited you know to see the collaboration between the DNA knacks the data bricks data works teams you know we see a tremendous amount of overlap where customers can benefit from both of these technologies be it how they're using the Titan project for many of their data processing needs data bricks for much of their ml and they're leveraging a lot of the technologies for visualizing understanding and interrogating genomic and phenotype data that are available in the Apollo product we see a tight integration coming up as they're you know Sparkle truly under power as many of the different technologies in this in the space and we're looking forward to collaborating a lot more we're very interested in talking to you know to anyone out in the field who's contrasting using these products together and who would like to influence our future roadmap on how these products in integrate together thank you again very much David and John for you know for joining us on this adventure you know I think everyone in the genomics community has been really thrilled to see how the UK biobank data is giving us a ton of you know tremendous insight into into the complex biology of human disease and I really greatly appreciate your partnership in these efforts you 