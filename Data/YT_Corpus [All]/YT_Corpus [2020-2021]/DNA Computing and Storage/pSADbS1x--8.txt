 [Music] so what we're here today to talk to you about is how we're working to transform Dow Jones into a data business so the data engineering team which Dylan was manager of we had the opportunity to move a 30 an over a 30 year archive the premium news data to the cloud and by experimentation with cloud-based services we're able to identify usage patterns and optimize our architecture in order to meet those usage patterns so what we're going to talk to you today is how we were able to learn through experimentation how we're able to identify opportunities to either optimized for cost or optimize for customer performance and how those lessons learned have influenced how we're transforming our business and future proofing in future data driven developments ok Dow Jones had an over a 30 year archive of premium news data that existed only an on-premise of infrastructure what we discovered is that it met the need of our existing use cases which were predominantly around advanced research and advanced analytics predominately focused at research librarians while we were getting increasingly frequent requests for extractions of data and that was actually really challenging in our on-premise infrastructure so what the data engineering team was charged with doing was actually doing the migration to the cloud to support existing use cases for the Advanced Research new use cases for machine learning and big data workflows as well as being innovative enough to be future proofing for whatever is next so Dylan what were some of the original challenges in making this data available to customers so for this new class of customers these data scientists they required more data so we were trying to like feel fulfill their requests but they needed more data than we needed for the researchers so with this on-prem database we'd have to give them the data on weekends or over the the low traffic times that we had for our research product but that just wasn't working we weren't able to fulfill the requests summers that were needing this data as a result we were actually post a question are we going to stay on Prem and try to scale up or we're going to move to the cloud where we can massively scale up he firmly like with the formal clouds so due to this lack of bandwidth in this lack of scale and the smaller the size of our team we decided to move to the cloud specifically the Google cloud because they gave us zero ops options I have some listed here like data flow data proc and also GCS I'll get over I'll go over those when you go over the architecture later so yeah I'm just gonna hit home again what the pros and cons are for moving to the cloud you can scale massively especially for these data science workloads where you're going to need large clusters to process all this data for us it's over 30 years of news content from over 33,000 new sources it also allows for smaller teams because you have these managed services that you can take advantage of it also allows to give you disaster recovery capabilities that previously were not available because on Prem we only had a limited number of locations with GCP or with cloud offerings that we have available to us now we can be located in different countries throughout the world and given this upfront costs we still wanted to move forward to the cloud I saw bring it over to Patricia to continue us on our journey well after I step us through the architecture here so here's our simplified architecture what we had was a news content feed on the the right side there it's kind of shown as a Mario warp pipe and that fed into GCP our GCS for storage and every day we'd have a dataflow job that would process all that those news events there were updates deletes or additions of articles and that be pushed into to datastore from there our customers are interactive Ian's API but this API we had we had to apply custom dsls so that was problematic Allah with the fact that it was pretty expensive off the cuff here in all that Patricia let's continue on so once the 30-year archive was moved to the cloud what we wanted to enable was really this the opportunity to integrate our data with machine learning workflows as well as big data workflows so we discovered that we identified two delivery mechanisms that we wanted to enable via cloud services first delivery mechanism was what's called a snapshot and that's a downloadable extraction of data custom data set for whatever is the specific use case that can be either transferred to a to a cloud provider to work in existing workflows or can be downloaded to their to any enterprise customers on-premise behind a firewall so if it's F privacy is an issue we're able to meet that need the second delivery mechanism we enabled was what's called a stream and that's a custom filter of incoming news of near real-time ingestion that can either be used to keep a data store up-to-date or it can be used for things like risk target identification brand management at scale due diligence at scale these are the delivery mechanisms we wanted to investigate in and make available by the DNA platform what we discovered early on in our initial experimentation with cloud architecture is that the usage pattern moving to the cloud gives us a better opportunity to track and identify and understand our usage patterns and our architecture needs to be adjusted to meet the needs of our usage patterns and we identified kind of trade-off decisions that were either to optimize cost or to optimize the performance for our customer one opportunity so what we're going to do today is we're going to use this balance as an analogy for what is either the the optimization of costs that's enabled by a cloud services or the opportunity to improve your performance to your customers by cloud by leveraging cloud services cost optimizations if we were going to improve our cost for example in the use case of a snapshot which is a downloadable extraction what we identified as the usage pattern is very consistent so these are workloads that can be done in batch processing these are workloads that can whisk interruption so what we have is we could make an investment in cost optimization by doing by using preemptable instances we could invest in cost optimization by investing in tiered storage by leveraging the advantages of tier storage you could do this the same with sustained use discounts and you'll notice as we leverage the cost optimization the performance may go up and it excusing the performance may degra gate so you're as you're as you're optimizing either for cost or performance the key is not necessary finding equilibrium it's finding striking the right balance of cost to performance for that specific use case now streams on the other hand which is a near real-time filter of data well that we would really heavily weight being highly available and being reliable in that event we'd really want to invest in managed services and we'd want to invest in readwrite access I and we'd also want to invest in analytics capabilities so in investing in our performance the performance time goes down but the costs may go up so it's really as I mentioned never a man a matter of trying to strike equilibrium it's a matter of trying to find the balance that meets your specific use case so Dylan once the data was made available on the cloud can you walk us through some of the costs to performance trade-offs that you discovered with datastore yeah so with datastore we were giving high bioavailability of reads and writes and also it integrates really well with our pipelining tool data flow it also allows us to scale a lot better than we did on Prem some of the cons where it's very expensive for the read and write operations six for the especially for the usage patterns we were seeing on top of that we had to create our own DSL and educate our customers on that DSL so there was a better way for us to move forward in patrícia yeah so for the product of snapshots we do require analytics capabilities but we also require the readwrite capabilities datastore proved to be a little bit to really have meet the need for analytics for the performance expectation of our customers but did not meet the cost performant needs so so Dylan what were some of the advantages in moving towards bigquery as our search index yeah so I'll explain the architecture and then kind of bring to light the the optimizations we made that made this a better architecture than the one that we had for for datastore and I'm gonna explain it from from right to left starting with the Mario pipeline of content and so yeah that's still feeding in into GCS with those events that were using to update our archive and then we still have that dataflow pipeline that updates daily except the difference here is we separated our core you layer from our storage layer and we for a query layer we use bigquery and big Cray allows us to have that sequel the standard siegel syntax that we can use and allow our customers to leverage as well and also we have GCS to store our full set of content another reason we had to split our query layer and our storage layer was because there were some limitations with bigquery specifically the extraction limits that we ran into we do not run into this with the current architecture and I'll kind of bring that to light through the customers journey when they submit a request for a snapshot and that can be seen here on the right side our customers interact with the api's and what they're requesting is a snapshot of data and once they submit that request an example request to be for all English English content that we have in our archive and that'll kick off a data flow snapshot or a data flow job and what that data flow table will do is it'll go to bigquery with that where clause and grab all the the IDS of the articles the most the identifiers for the most current version of each article and from that point it'll do a CO group co group by key or I just sort of join with the the data we have stored on GCS from that point once it has all the the content identified it'll then output the snapshot to a location that the customer can use to to download it but that's still done via our API is and our customer still has no idea that we're no longer using data store so that's the architecture and I'll kind of reiterate the the pros and cons of this so with these resources that we're spending up with data flow the affirm their referrals so we're not incurring that cost as time goes on we're only incurring the cost while we're doing the operation we're doing the snapshot for example we're not paying a high price for updates to the archive as we were before with data flow sorry datastore often we get that free query syntax that bigquery gives us with with sequel and GCS is very cheap so it's the perfect place to store our data when we're not using it so with this architecture we still have that very expensive joining process with the Lecrae layer and the storage layer and we also need a separate cluster for each in every operation so there may be a better way and you may see that further on in our talk here in moving to using bigquery for analytics we were able to do big workflows and relatively inexpensive big queries relatively inexpensive for analytics capabilities but we hit some quotas and we had some limits when it came to extraction which is why we then paired with Google cloud stores we really use the the cost optimizations of the tiered storage with the analytics capabilities of bigquery so Dylan in our snapshots architecture why was the mood made from data flow to data proc okay yeah I'll explain this architecture and you may see the cons kind of bubble up or what what no longer are cons with this are this architecture so here we still have that that Mario pipeline of content going into GCS updating daily and going into the content archive we have here that's a little different as you may see the customers still submitting API requests but when the customer submits an API request for all English content this time they're just going to be having a job spin up and we can use data proc and their sequel syntax to create the snapshot of content that'll be delivered to our customer there was there were some cons with this method but yeah I'll go over those after I go over the pros because this is a good thing so with this architecture we were able to reach share resources so when we had to customers make a snapshot request we can still use that same archive of content with data flow you'd have to load it into our cluster each time each and every time here we can reuse the same we can mount the same drive in the same content so this also more closely aligns with our usage patterns we have more and more customers join and use our platform and because of that we can use these higher cost resources when it comes to doing the operations there's also no need for that joining of the query layer and the storage layer because we're using spark on data proc which has a sequel syntax that we can leverage but with this there is still more operational overhead with with data flow it can recover from errors or any issue with the cluster with data proc we still have to kind of build in some logic to kind of restart the cluster and recover from errors on top of that when we moved from bigquery and went to the spark syntax we had to create some UDF's to kind of get bridge that gap between functionality and yeah for those of you interested it was it was related to regular expressions and I'll throw it over to Patricia to continuous on this journey so we have a relatively small team we started with two engineers and we now have five engineers through the experimentation in moving to the cloud we were able to identify that the usage pattern for snapshots is very consistent and it's very predictable so it was a worthwhile trade-off to go to the increased overhead of an unmanaged service as it gave us a increasing control over the expensive operations like joining and windowing so there was a cost benefit to us it was worth it and worth taking on the the overhead so Dylan what were some of it for streams which is a near real-time filtered stream of data where highly available services is weighted heavier what was the what was the choice made to move from data flow to data proc or did a prokta data flow okay yeah with with the streams architecture our customers are going to notice if it goes down for a few minutes or seconds unlike with snapshots they're not going to notice because it's those operations are going to take between an hour or two hours this it's very its low latency so they're gonna notice when messages are not streaming to them with this we're still hooking into that stream of content that's coming from the Marne YouTube but it's streaming on to pub/sub right now and we're still leveraging the data flow services to do our ETL processes and the great thing about dataflow is you can write job for a batch or streaming job and it's going to be either the same or similar so we're taking an advantage of some of the code we wrote for snapshots and usually in streams to de filter and update our content from that point we stream it out to different we filter it into different product types because we have content that's that's licensed to only certain products and we use data flow to filter that content from there this is where our customers kind of interact with our system they use our user api's as if they would be using a snapshot they submit the same query like if they want all English content they'll submit that little where clause and from that point we'll spin up a data flow filter before though that was a data proc filter and I'll go into the pros and cons as to why we changed that part of the architecture so now once the customer has created the stream and they have their custom set of content streaming to them on this pub/sub topic here they can then use our client code and get streamed all the content that gets filtered down to them but we had to make that move that I was talking earlier from data proc to data flow and that was because every time we would use preemptable x' in our cluster it would take down our our stream and your customers would notice with what data flow we actually have had streams that have running for multiple years along with that allows for this fully managed zero ops data processing and for our small team this is great because we have to wake up in the middle of the night and handle an issue it also allows for more seamless auto scaling which helps with smaller teams and with streams there was not a heavy load as we only receive like 800,000 messages a day and in big data land that's not a lot so yeah there were the cons of not being able to use preemptable which were very cheap if you do use data proc I I suggest looking into it and also we had to kind of leverage in in memory database for the queries right we didn't have the big query syntax or the the spark syntax that we could plug into and use sequel but if I were to do this again data flow now has a sequel to yourself which we have not used Thor whatever to you Patricia Thanks so sort of to summarize some of the lessons learned the usage patterns seem obvious but actually once we release it to the world we did have some api's that were used from ways we hadn't intended and all of a sudden your your costs go to skyrocket and you start becoming very painfully aware of where you have opportunity to optimize where you have opportunity to change and so what I would recommend and advise is initially we were sort of always optimizing for performant responses and if you invest too heavily in one side of the trade-offs you fall over on top of your suit yourself if you're only investing in one side you get skewed off balance really quickly and that becomes very apparent when you get the bill for the service we had a use case that have been like a $70,000 use case that we were not expecting so what we really learned is in anytime we do a feature development to first identify the usage pattern identify the behavior really understand from your users how they're intending to use it which might be different than your original assumption and then once you do understand the usage pattern to build in such a flexible way that you can understand the trade-off decisions you're made versus the performant experience for your customer and your opportunities that the cloud enables you to for cost optimizations so Dylan what were some of the lessons you learned in making this new product available on Google Cloud ok yeah one thing that we found useful as we're developing this new product that we've not built before was we built time in our road to iterate and it spiked on new infrastructure when we could Google is always developing new like zero ops tools that you can use and other cloud providers are as well we actually started on a different cloud provider before we moved to Google also we learned that being an early adopter is a double-edged sword you can guide the roadmap for their products but at the same time you can use one of their products and it can tip over in a way he would not previously have expected also Google very tightly they keep their quotas down to a limit that prevent you from incurring a lot of cost but at the same time it could cause an issue in the future so it's good to know your quotas especially when you're an early adopter so yeah this can actually be boiled down to three things when you're deciding in architecture the first one is cost the larger you can make that margin for cost of operations the quicker you can grow your team iterate on feed features another one is team size there are there are some architectures that may not work for your small teams especially if you want I have a team that can develop features and you don't want to be spending half at a time managing infrastructure this is why Google dataflow and in data proc have actually come in handy for us and kind of accelerated or roadmap another one is we kind of learned trial by fire is usage patterns we had a new product with a new use case and our customers are reusing in a way we did not expect them to use it as a result we took advantage of the fact that we had time and a roadmap from using these zero operational tools and also the fact that we built into a roadmap this time for iterate iterating our architecture so yeah luckily were able to adapt and and change according to this users patterns but it's very important to keep a very tight eye on that and maybe even log that data somewhere so you can be alerted of when like your costs are kind of spiking and it's time to address that issue so those are the lessons I learned so in making this new product available on the cloud we meet our existing use cases of advanced research we've all so now made a new line of business for a machine learning and Big Data workflows and dildos can walk us through some of what we've also enabled in terms of applying these lessons to even future proofing and some kind of new and upcoming things we've got going on yeah so this was just about news content but with news content you have different entities you have organizations you have people you have events you have like different industries affecting the news so there's a lot of context that can be gleaned from different sets of data and DNA has allowed us to kind of you like build on the top of the shoulders of this platform and creating products that can generate knowledge and actually you can get signal from these news events and we're doing that by structuring a sort of knowledge graph in the way that Google is doing for their search engine we're gonna be doing for the news and events so we've been able to work on that develop new context traversal relations through like our graph architecture and on top of that if you're interested in the kind of content we've been dealing with here we have a commercial data set in bigquery just Google bigquery commercial data sets DNA and you can take a look at the kind of news content we've been dealing with so yeah that's the knowledge graph is kind of the next big thing we've got coming up and we're so excited to have this premium news content but also available with you know the entities that exist the relationships between those entities see if the data in the context of the data which is something we're really excited about [Music] 