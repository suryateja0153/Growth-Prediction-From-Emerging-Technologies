 Thank you Peter. Hello and welcome to Accelerate genomics discovery with SRA in the cloud. My name is Yuriy and I and the SRA product owner. Before we get started with the webinar I want you to take this poll. This can tell me what your interests are in this webinar. I can tailor the message in the webinar on the results here. To make sure you get the most benefit out of the webinar. This is interactive poll so click on the screen. In a few more seconds when everyone finishes I will share results with everyone. These are anonymized. We will see the percentage for the options. Great. The major interest is why should I use SRA data in the cloud and what technologies you should be knowing about if you work in the cloud. Also the third topic of interest is you are here to learn about cloud data and how to use cloud. So this webinar we will cover basics like introduction to SRA and why it was moved to the cloud. The benefits you get from working in the cloud. And of course, the tools available so far. I will cover two case studies highlighting cloud usage and provide you some information and links of where to go to get started with cloud resources. Then we will have a Q&A session. For those of you who don't know about SRA, we are the Sequence Read Archive from NCBI. We started in 2008 and pur primary mission is to store and distribute next generation sequencing data. We accept all kinds of samples and all kinds of data from different platforms. The data is used in a variety of different ways, from bioinformatics development to detection of novel genes or microbial or viral species even. Why did we move to the cloud and why did we do it 12 years after it started? SRA has been growing exponentially year after year. We are over 14 petabytes of data at this point. We have over 25 PB of user submitted files that we can share with the community. This is a lot of data. Moving it from Bethesda, Maryland to anywhere in the world is technically complex task and also time-consuming. Some of our users need to wait weeks or months to just download data before they can do any computational analysis. Sometimes they discover some data they downloaded is not useful and they need to download more. This is a big barrier to doing quick analysis on the data available. Because there are great benefits, we want to speed this up for the scientist community. We went ahead and moved all 14 PB of SRA archival data into the cloud. From there you can get faster access to this data. You have powerful computer resources available to you on demand. You can easily expand the storage you are using in the cloud so you are never waiting to get more storage. The cloud can be a great collaboration platform. You can have your collaborators who may be in different parts of the world log into the same system and run the same pipelines and look at the same results whenever they want. You don't need to share those results individually since everybody can login into the same server. What we have done so far is we moved all of the SRA archival format to the cloud. That is available now. We also started moving the user submitted files which is roughly 25 PB more of data. This is a new feature from SRA. Instead of using archival format, you can get to the files that we received from the submitters. We are always trying to be more efficient and we want to be more efficient in the cloud. We started producing contigs ourselves. For now we only produce contigs for metegenomic datasets, but we will be branching out to RNA-seq genomic data sets and other types as time goes on. In order to access this data we updated our API which is the SRA toolkit to work in the cloud. Instead of downloading data from Bethesda, you will be accessing the data in our cloud bucket. This will speed up file access. Another feature is we made available our metadata in native cloud architecture. For now this is just Google's BigQuery product but this was a popular feature and we will be moving all of this also to Amazons Athena, now that we have received positive feedback from the scientific community for this. The case studies I wanted to highlight was the NCBI virus discovery codathon. Is a good example of cloud computing and the benefits of cloud computing. And our interaction with the cloud customer who wanted to do things in the cloud natively. The virus discovery codathon was started with assumptions that there is a lot of viral data available in meta-genomic data sets. We brought several teams together and the goal was to build some analysis and processing pipelines for the data sets. I won't be going into the details of the actual code or what the results are but if you are interested to read about it there is a publication link you can follow. https://www.ncbi.nlm.nih.gov/pubmed/31527408 What I want to talk about is the pre-computation that had to happen. The assumption was that everybody would be working on contigs and not raw data. What SRA did is we took 140,000 runs of metagenomic data sets that were 7 GB or smaller and we built contigs from that. This was either de novo assembly or guided assembly. We ended up building 3.9 billion contigs out of the data. In order to accomplish this we had to take a two-step approach in computation. The first one was using 500 nodes or servers that were only 2 cores and 8GB in memory. These are very small machines and some of our cell phones are more powerful then this. Of course we had to use larger machines for the bigger files. We used 100 machines and they had 96 cores and 300 GB of RAM so they are big and powerful machines. Why did we do in the cloud? It only took a couple weeks to do this in the cloud. We knew that if we try to do this internally at NCBI on our high computing cluster it would take us several months to do this. You can see the bar change from a couple weeks to several months of computing these contigs. That is because SRA is not the only resource at NCBI, we share the cluster with other resources. Of course we would be queued up for computational needs and this is something that some of you might have faced before as well at your universities. In the cloud all of the compute is what we want and at our convenience. What we learned from this experience is if you want to get started faster in the cloud, collaborations help. So if you are collaborating with bioinformaticians or programmers who are familiar with cloud architecture that can help you get started faster. You can do this on your own. But if you will do this on your own make sure you start working in the small-scale first. This will help you learn the different technologies. If you are using known data sets that are small and quick to compute on, you know what the results should be. As you get more comfortable with technology, expand slowly. Premature scaling to 100 or several hundred machines, can lead to a large problem and you won't know where to debug the problem. It's important to scale slowly so you can understand what's happening and if you run into issues you can understand where to look for the problems. If you want to know what technologies we used in our computation approach, that was Kubernetes, Docker images, and there was scripting using Python/BASH and Snakemake. These are not the only technologies available. Cloud technologies have competitors. When you look into cloud technologies, some other technologies may be a better fit for you than the ones we used. We tried to make our compute cheaper so we try to use preemptable or spot instances depending on which cloud vendor we used. Those are cheaper options that you should look into. On the note section of this presentation I added links to different resources that you should check out. After the webinar feel free to look at those to get more information about any of these different topics. The community use case I wanted to highlight was from researcher or who was interested in the evolutionary relationship of microbial communities and marine environments. They were looking at marine metagenomic data sets. What they really wanted to do was build up an automatic pipeline to look through SRA data and the pipeline would run in the cloud. They needed to be able to search for these datasets quickly and efficiently. They wanted to be able to fine-tune their search parameters for these data sets. The other need was to be able to know what the microbial communities are within the data sets. For this we thought that BigQuery or Amazon Athena products would be a good fit so we started doing testing in the BigQuery. We started out putting SRA metadata there. We put sample information where the sample was collected from and what depth and all of this metadata that users provide. We also had sequencing information of how the sample was sequenced in terms of what platforms were used. Also statistic information such as the number of bytes or the number of reads or bases available in that data set. This allowed the user to select high-quality data sets from a particular platform or if they wanted the run to be a certain size. For the second need which was to know the taxonomic content of the run, for that we had to make the results from the SRA taxonomic analysis tool available in BigQuery. We haven't released this publicly yet as we were still going through testing with a small user group, but we will be releasing this soon and this functionality will be copied to Amazons Athena as well given more time. With this data set they can identify what microbial communities are available in a particular run. This is so they can fine tune their search parameters even more. Since this information is available quickly from BigQuery, they can take the sequencing information, they can take the search results from taxonomic content and combine and take the intersection of them, thereby getting a small data set of runs that they can run more expensive computational analysis on. Getting started is fairly easy. What you will get out of this will be customized filtering for datasets that you want to emulate. Obviously you will have powerful computing available to you. All you need is a laptop to login into the cloud architecture. If you are worried about costs, Amazon and Google have free tiers available for you can start playing around with this technology and moving your analysis pipeline or processes into the cloud. If you are working with small data sets on small machines, all of this can be free for you. definitely don't be worried about this. If you need to get data out of the cloud that is called egress and you should only be egress and things that are the results of your analysis. That is much smaller in size than other raw data or meta files you might generate between different tools. If you are ready to get started, we made videos available on how to start an instance on Amazon and Google so check out our YouTube channel for that. https://tinyurl.com/SRAonthecloud We have also provided help documentation for all of this information in terms of getting started with computing and some example queries for big query. You can find all of this in our help documentation. https://tinyurl.com/SRAcloudDoc Now that I introduce the tools, we are always looking for more feedback from the scientific community. Reach out to us and tell us if something works or doesn't work for you. that's more important. If we know what is not working we can try to fix it and make the tools better for you. the more feedback we get on anything either for SRA or NCBI, we will take the feedback and improve resources to meet your needs. If you want to continue following NCBI and what we are releasing into the cloud you can follow the NCBI insights blog where we will post more information about upcoming new functionality. We also send out messages on Twitter and you can follow NCBI for that. SRA is also registered with Amazon open data registry so you can find us there as well. That is about it. I can now open the floor to any questions that you might have. [Peter] This is Peter. We have some questions in the chat pod. Then I think you were going to do a demonstration of BigQuery if we have time. Someone did ask about that one. First other simpler ones. Somebody asked the question about your sequences that are submitted to ENA, the European Nucleotide Archive, so I think we basically get the sequences into SRA and they would wind up on the cloud, is that not true? [Yuriy] That is true. Since SRA is part of the INDSC, everything that is part of SRA is also in the cloud as well. [Peter] Another one that is similar in spirit, they were asking whether SRA is mirrored or split. I believe it is mirrored in both places. You can get it on either platform. Is that correct? [Yuriy] That's correct. Depending on your needs, you can use either platform. [Peter] One other simpler one although it is a sensitive topic because we don't like to talk directly about the cost that we've experience from the government point of view but can you comment on the compute cost associated with contig creation? [Yuriy] It's a difficult topic to answer. I don't know the certain number but probably several thousands of dollars were tens of thousands of dollars. In terms of computation. I don't know how that would relate to anybody else's expenses. Concerning the amount of data we processed and how fast we processed it, I think it was a good expense for us. [Peter] Thank you. I guess the other question was, somebody was asking about using BigQuery and the metadata. You have a demo you could do with that. That might be useful to do if you have time. [Yuriy] I can do that. The easiest way to find BigQuery is to do a Google search for it. Once you are here if you don't have a Google account you will probably need to create it. From here it is simple. You can go to the console page. This is the web interface that Google provided for their service for BigQuery. I think it's intuitive . You can see your query history and the saved queries you've run and some other options. In order to connect to the SRA data I find it easier to pin our project. I will open my project here. Go to the add data section and click on it. There is a pin a project selection. Our project is called NIH-SRA-data store. So I type that in and click on the pin button. Let's try that again. I think I had a typo. I may be typing something wrong. But once you add the data store you can look at what is available. But I will use one of our Saved queries to find some runs for you. I can click on open query in editor. And click run. In order to use open query you will need to learn SQL. We have some example queries available on documentation pages so you can copy those to get started. There is a lot of useful tutorials online for using sequel and learning sequel. To get started and it's not very hard. I have a more complex query here but as you saw, I ran the query and the results came back in seconds. You can see that I selected different type of runs. Either from DDBJ for this one and this was submitted to SRA. I displayed some columns that may be useful to me such as the number of spots used in them and percent unidentified reads. Between the performance of big query and the ability to write your own queries and customize the results, this is a very useful tool for pipeline developers or even if you just want to get access to the data quickly, you can use the web interface to search for your data sets. I think it is a powerful tool that can be very beneficial. [Peter] Thank you Yuriy, we have a few more questions . There are three of them or four of them actually that we should address. Someone asked, is there an SRA docker container store for toolkit and another apps. [Yuriy] SRA will be releasing a Docker image with the SRA toolkit. It is in our roadmap to put that on Docker hub so that will be coming soon. NCBI has a Docker hub account where you can look for other products that NCBI supports. Please check out NCBI at the Docker hub. [Peter] I have a question that is general and should probably be addressed. It has to do with what data are publicly accessible and I think it's worth talking about the difference between the controlled access data and the data that is not controlled access. And what do you have to do to get to those things. [Yuriy] Right. We have public data that's available right now. For controlled access, human data, you would need to get access to that data set through dbGaP first. That data is available in the cloud as well. If you have access to datasets through dbGaP you can access them in the cloud. [Peter] Thank you. Another question. Has to do with the contigs and you mentioned that they should be available before too long. How soon do you think they will be available? [Yuriy] The contigs for metagenomic data sets are available now. I think I added the link to the documentation for them into the presentation, but if it is not there, I will make sure to send out the link to the documentation for them after the presentation. And the contigs are available in both Amazon and GCP. [Peter] The other question is about the compute cost per assembly but I think you addressed that when you talked about cost when we were doing this for the contigs. [Yuriy] Unfortunately I can't answer the question of cost per assembly. I can certainly get in touch with whoever asked that question to talk to you more off-line about that topic. [Peter] That's a good idea. You can write into the webinars address if you want to. Webinars at ncbi.nlm.nih.gov. And someone else asked can you show an example of doing an assembly in the cloud. For a particular SRR. [Yuriy] Doing assemblbly in the cloud is not any different than running assembly at your local server. You use the same tools as you would there as well. The only difference is that instead of running it at local server that you have available to you or your desktop, you are doing it on a server located in the cloud. The process itself is not any different. In this case I would be calling up an assembler and providing data to it. I don't have a demo available at this moment but I'm sure that SRA can make a demo of that and share it. [Peter] Okay. Another question which I think you addressed this. In some ways this webinar was about this topic, but it is probably worth reiterating. What is the advantage of using an ec2 instance instead of using my own computer, basically using the could instead of this person's own computer. Maybe there are a couple of points to you share. [Yuriy] Yes. If you are downloading data using the toolkit, it usually pulls data from our servers but that is also where everybody around the world is accessing the data you so are competing for resources. In the cloud it scales basically automatically. You get faster access to the data there as well. Also if you need more powerful computers not available locally, you have those machines available in the cloud as well. As I said, we had those bigger machines that had 96 cores and lots of RAM. Those might be harder to get locally for you but they are available to you easily on the cloud architecture. Those are the benefits of doing that where you get faster access to data and more powerful computes. If you move your pipelines and processes there you can see it speed up in the overall analysis. [Peter] Thank you. Good question and a good way to seal things to give the main advantages of doing this. We mention access to the conttigs now available. Is it easy for you to show us how to get access? [Yuriy] Sure. Here I'm going to the virus discovery github page. There is a readme on how to get access. We provide instructions of how you would want to download the contigs. In this case if you are somewhere in the cloud, either Google or Amazon, you can run either AWS s3 copy command and download the conttig file to your processing area in Amazon, there are similar commands with gsutils for Google. You can even run a wget command from a cloud instance running at Amazon to download the data. That is all it takes, is running the one command to download one of the contig fasta files. [Peter] Okay. That's great. Somebody said we have one more. Just a second. This is an interesting one. We may not be able to address it directly. This is about the data from GISAID and does that data get reflected in SRA. It is relevant for the coronavirus data. It seems to be submitted to GISAID. [Yuriy] Right. Unfortunately I don't believe we have that data available at SRA. We are certainly exploring options on how to get access to that data. It would be a publicly available item. [Peter] Okay. I think that is all the questions we have. If there is any we didn't get to we will address them in the documents we will produce. I think we can wrap this up. Thank you everyone for coming. And thank you, Yuriy. [Yuriy] Thank you. [ Event Concluded ] 