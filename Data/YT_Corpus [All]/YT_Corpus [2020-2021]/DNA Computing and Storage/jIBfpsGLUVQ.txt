 Hello everyone. Welcome to today's online seminar on analysis ready data sets as a note this session is being recorded and you will receive a notification when the webinar recording and the slides are available to you. So I'm Julie Goldman Research Data Services Librarian at County Library at Harvard Medical School and I offer a lot of these data management trainings. But I'm also available to the Harvard community for consultations on identifying best practices for data management, creating a data management plan or if you need to find data. The Longwood medical area research data management. Working Group has a website, full of resources and information and this working group includes many members of the LMA community. From the library, information technology, our research course in affiliate hospitals. So we suggest that you bookmark this website and reference it as you need. So let's review. Today's learning objectives we have quite a bit of content to Cover in today's short session. So we'll understand how data management planning can lead to reduced data cleaning will differentiate between messy data and tidy data and will identify principles for or of analysis ready data sets. So in today's session, we will primarily focus on preparing data for Data Entry and capturing data in spreadsheets since spreadsheets are traditionally what we use to capture and create Data sets. However, I understand. A lot of you may use statistical software or programming languages to work with your data and the principles for working with a clean data set. Prior to analysis should be followed, regardless of where and how you are working with your data. So to get started. It is often said that you know 80% of data analysis is spent on the process of cleaning and preparing the data. So we need to really think about our data sets ahead of time. And actually, while we are collecting our data. So when working on a research project, you should take some steps to ensure that your data is safe authentic and available. And in addition, take steps to ensure that your data is usable. So to be useful to yourself. Other researchers bio statisticians data set should facilitate clear and M ubiquitous communication. Be useful by current software applications provide a traceability between the analysis and source data be linked to machine readable metadata and then be analysis ready So analysis ready data sets have been reasonably collected and reviewed so that the analysis of that data yields a clear, consistent and ever free results. To the greatest extent possible. So in the session, we will cover the following concepts for preparing our analysis ready data sets and those are creation validation standardization cleaning and documentation. So first, creating data in spreadsheets spreadsheets, are these two dimensional way to store view, analyze and alter two dimensional data. And spreadsheets are widely used software tools for data entry storage, but then also we can do analysis and visualization within our spreadsheets. So here are some here are the best practices for creating data sets that we will review each of these So all data should be labeled, you should have unique field names and when you use say Excel spreadsheet program, you have to do this yourself. So be sure to put field names at the top of each column. And each experimental subject should have a unique study ID. And be sure to add a study ID to each subject about which you are collecting data. So consider these names of rows study ideas must be unique. So that rows are not confused with one another, and you should periodically check your subject IDs and ensure that they are in fact unique And the example data on the slide lacks these unique IDs for each subject. The field names at the top of the file, make it clear that the data contains data for four subjects from the study baseline. Those are a 30 and three subjects from the follow up, he, he, but what is not clear is which subject did not follow up, or did not show up for that follow up. And without knowing that we don't know whether her and to pee score increased or decreased. So we would add a column to the left, which includes a unique number for each participant in the study so that we are not replicating column or study ID names. Alright so data should be in this rectangular format so rectangular format means that there are no duplicate values in any single row. When recorded rectangular Lee each cell is therefore meaningful non rectangular formats may look like a reasonable way to store data, but the file cannot be cannot be read in that form by a statistical package. It cannot be store easily. It cannot be used in say, creating a pivot table, it could not be examined using filters and therefore not easily understood. So you could choose either these examples on the slide to structure your data. The long Long layout or longitudinally. Uses a phase variable to delineate the different visits and then the wide layout uses different field names for baseline and follow up information so that phase variable is is not needed. So also rows should represent the appropriate unit of analysis, your columns should represent the unique attributes of the rose. This is also the concept of tidy data, which we will cover under formatting or data. So the first example does not meet this organization. Oops, sorry. So again, this first example does not meet this organization. But the second example has one Has one row representing the units being analyzed and each column represents one unique attribute being measured. So next data files should contain the same number of columns in each row problems will arise when data are missing in the middle of a row So in the example we are missing data in the middle of a row data row four is is missing that follow up underscore and to P value. So the subsequent values are shifted over And to fix this. We would insert a cell with the missing value. When the missing. And then the the ship and Then  all of the other values will shift over to to their proper place. And we might also want to add a Na or not applicable here for easily easy analysis. So data should be atomic within each column. This means discrete data should not be combined into a single column. So first, we see that the columns name. Hospital and site doctor have multiple data points in them. And ideally, we should we should break these up. So we should break up name into a first and last name. Columns and the hospital name should be separated from the city and the address location. And same thing with the site. Dr. Lowe should be two separate Different data points. So we can use text to column features to split up text combined contained in a single cell to to multiple columns. So we see next that we have done this and the hospital city it's now its own single columns from the hospital name and we have created a column for the first and last name. Alright so next concept is is formatting and we have already seen a little of this, but let's highlight the tidy data principles so tidy data specifically has become the standard format. For the sciences, because it is it easily allows people to turn a data table into graphs analysis and insight and in practice our raw data is rarely tidy and is much harder to work with. So the benefits of tidy data include that the tiny format makes it easier to understand the data and clearly convey the concept of a variable and what type that variable has second benefit is consistency. One of the most common challenges that data scientists face is the problem of formatting are messy data sets. So tidy data provides a standard For data entry and format that can be therefore become this consistent across projects. And then third, compatibility. So tidy data has full compatibility with more advanced analysis tools. So remember that spreadsheet programs like Excel. May not be compatible with other programs. So by organizing our data in a logical way that can be read easily by computer therefore that data is is more interoperable. So a tidy data set has this default structure each variable forms a column each observation forms a row And each data set contains information on only one observational unit of analysis. So each variable is placed in its own column each observation in its own row and each value in its own cell. Alright so next principle is is validation and validation helps ensure that data is collected correctly so data should be actively validated as much as possible. Throughout the process of your data collection and and and and entry and this will, you know, reduce the time spent later. Cleaning your data explaining your outliers and accounting for any missing data. And there are a few best practices for validating or reviewing your data sets that we will go through So programmed valid ranges for inputting data into fields. When applicable. So for example, when entering data do not allow an age greater than 150 years or allow only numbers to be entered into Numeric fields. And you can do this by using formulas and Excel to set data validation to these specific parameters. I you should apply data types to fields to prevent automatic formatting so set of date fields to only accept dates, rather than setting them up as free text fields. Again, use data validation and Excel to limit the inputs in specific fields. And the same thing can be applied to fields that may lead to errors in Data entry. So for example, if you use a list of options to restrict data entry, the spreadsheet will provide you maybe with a drop down list. Of the available items. So instead of, you know, multiple team members, trying to remember how to spell species names or auto spelling, you know, correct, you know, taking over. You can then select the right option from the list and everything will be standardized moving forward. So next, prevent the entry of leading and or trailing spaces or maybe other characters that may interfere With your data analysis. So you could go back and you know after you know all your data is entered and use Excel formulas or maybe open refined to clean up your data afterwards. But you can also set up your spreadsheets, set up your data collection ahead of time, set up your data entry tables so that these spaces are not allowed. And this is the Excel formula that you could use for this data validation Here we should plan for those other data responses. So this is mostly focused on survey data collection, but you should always prepare for all possible answers. And to encompass all possibilities. Ideally, just include and other category as an answer and create a text box to maybe capture some free text information from your survey participants. And you know, when reviewing your answers if more than 10% of your respondents select this other category, you're probably missing an answer and therefore can go back and modify your survey. And you can review these other texts. That your Respondents have provided and add those frequently mentioned to your list. So the example question on the slide doesn't include other options such as healthiness food. The price or value or some other option. Also plan for the preferred not to answer category. So sometimes respondents may not want you to collect certain types of information or may not want to provide you with those types of information. So such questions as you know income occupation finances personal hygiene political or religious beliefs are are too intrusive and may be rejected by the respondents So incentives and insurances a confidentiality, can you know make it easier to obtain private information. While current research does not support that. prefer not to answer options increase data quality or our response rates. You know, furthermore, different cultural groups may respond differently. So therefore you should always include an answer or place to record did not choose to answer or does not know this will also help avoid your no data and you don't have to address it later. Or that it will interfere with your data analysis. So next principle is standardization and this is used to establish consistency. It ensures the data is internally consistent that the data is the same kind and format for each data element that you are collecting It also helps minimize data collection and analysis errors and prevents any inconsistencies and these are the best practices for standardizing your data that will review. So use coding schemes harmoniously if data is coded as know equals zero. And yes, equals one, then all of your yes no questions should use this same coding scam. And each categorical valid variable should have a set of exhausted mutually exclusive codes and these codes should be thoroughly documented in your code book or data dictionary. And wherever possible standard coats should be used. And this facilitates maybe the comparison of results across variables or even across studies. And ideally data should be reduced to to Merrick codes whenever possible. This avoids the occurrence of those typing errors in entering literal answers. Leading to misinterpretation of to equivalent answers as being different, and, above all, greatly facilitates the use of variables in statistical models as most statistical packages could not use character values they really prefer these numerical entries So standardize your free text into these categorical data and wherever possible free tax should be standardized into categories. Rather than asking an open ended question asked questions with controlled choices, also known as closed questions. So yes, both Open ended and closed questions have these advantages and drawbacks, as shown on the figure on the slide. But notice that, you know, choosing one question type over the other actually involves a trade off. So although open ended question afford this breadth and depth of your reply responses to these open questions are sometimes difficult to analyze. You have to go into text analysis and use maybe other types of software to analyze those answers and, you know, while closed questions are On the other hand, easier to analyze. They don't provide a lot of that context or or depth on a topic, so you must think critically about, you know, which types of questions, you will use and how you plan to to analyze them. So treat date and time consistently so date variables are often the cause of many data analytical nightmares and date variables are often problematic as there is no real UNIFORM STANDARD FOR date formats. And therefore, it is very important to choose a standard and keep the same date format for each record for all date variables in the project. And when working with data files, you'll often require the date to be stored in a specific format. And Excel is a nightmare. When handling dates will just put that out front, but you can use it to reformat and change date format to a specific formula and then paste those values where where you need So again choose one date format and employ that standard throughout and ideally I suggest using the ISO at 601 standard that's The year, month, day, this is not how Excel will automatically format your dates. So just be aware when working with dates in in Excel. Alright, cleaning, so before performing analysis of your data, you should review the data set for any inaccuracies and consistencies or sensitive data. And cleaning your data allows you to identify those outliers, or errors before you, you know, compile all of your results. So here are the three best practices that we will go through So checking for outliers show and ensure all data elements are in the correct formats and correct ranges. Outliers are infrequent values, far from the normal often conversion errors or data entry mistakes must be, you know, those are the things that you must be explained or repaired. And remember back to our validation best practices. So while you can avoid outliers from the start. This may not always be the case. So going back and reviewing the data for outliers may be necessary. And then check for missing data showing sure there are no data items or records that are missing creating no elements and code you're missing data appropriately. Again, our best practices from validation can prevent this missing data, but sometimes some retroactive cleaning is is needed. And then ensure that your data does not contain any pH I or protected health information. So the Health Insurance Portability and Accountability Act or HIPAA privacy rules protects individual Medical records and other personal health information so hippo requires that researchers protect the privacy and confidentiality of their patients. So no individually identifiable health information should be included in your data sets personal identifiers include private information that subjects expect not to be made public. That are linked to information associated with a unique individual. Direct identifier, such as name you know mailing address, phone number, social security number photographs or date of birth are tied directly to one individual. And then indirect identifiers may include gender, race, disease, place of birth document take off you occupation income, education, age, those identifiers If you are working with, you know, have a data or bhi it's recommended that you use red cap which is a data or survey collection platform that can automatically flag personally identifiable data. So there are tools out you out there to help you work with this sensitive data. Right and documentation. Lastly, before analyzing or sharing your data, ensure that you have appropriate documentation. Of what we have documentation facilitates you know the understanding the analysis that sharing and reuse of your data. And these are the best practices that we will cover for documenting So data should be stored with appropriate metadata and again metadata is that contextual information required to interpret data. And should be clearly defined and tightly integrated with your data. So recall our tiny data. When data is organized in this way the duplication of information is reduced and it is easier to subset or summarize the data set to include the variables or observations of interest. So data set a at the top is untidy because it mixes these observational units. Species location. Location of the observations measurements of the individuals and the units are mixed and listed with the observation, more than one variable is listed And several formats are used in the same column for dates and geographic coordinates. Whereas data said be on the bottom is an example of tidy. Tidy version of the date is today. And this reduces the amount of information that is duplicated in each row. And by having species in a separate table. They can be identified uniquely using the species code. And it makes it easy to add information about the classification of the species. And also allows researchers to edit the taxonomic information independently from the table that holds the measurements about those individuals. And with this example if the focus of the study for which these data were collected is based upon the size measurements of the individuals information about the where, when and what animals are measured is considered that metadata of this of this data set. You should create a new user data dictionary and readme files. So metadata should always accompany a data set, wherever stored, but the the best way to do this. May depend on the format of the data. So as we saw previously some metadata will be included, you know, within the data table within the data frame. But metadata and information about the data set should also be captured In a data dictionary or readme files. So a data dictionary is something that describes the data in a data set and includes the, you know, overall description of the data along with more detailed descriptions of each variable. But here is an example of a data set with. It's a convening Readme file. Remember to create a new file with your cleaned or analyze data. don't modify the original data set or you will never know where you started, and keep track of the steps you took to clean your up to clean up your data or your analysis track these within your Readme file. And your Readme file should be in a plain text file stored in the same folder as that data file. So save your data as machine readable. Ideally, as in CSV or comma separated values file and coding your data keeps your data secure and also saves you storage space. Also storing the data you're going to work with for analyses in Excel default file format is not a good idea. This proprietary format may not be available in the future you will be able to open it. Also other spreadsheet software and may not be able to open file saved in this proprietary format. So storing data universal open and static format will help deal with, you know, this problem. So try tab delineated or comments eliminated files CSV files are plain text files, where the columns are separated the columns. And the advantage of a CSV. File, is that we can open and read the CSV using just about any software, it can be easily imported into other formats and environments such as maybe SQL light and are so on the slide is an example of a spreadsheet and the same data as a plain text file in a CSV. Alright. And lastly, we want to adopt appropriate file naming practices, this wouldn't be a Data Management Webinar. We did not talk about file naming practices. So ideally you want your file names to accommodate multiple versions of data files and the basic ways to incorporate burgeoning information into your file names. So when creating new versions of your files, you should record what changes are being made to the files and give the files unique name so you include maybe a version name or version number, excuse me, such as the one or view to or the 2.1 You could also include information about the status of a file, for example, including draft or final as long as you Don't don't end up with confusing names like Final two or final revise and ensure that you are using final correctly. You could also use dates and your file names as well. So remember to use that ISO at 601 standard for dates. And this works best if the date is listed first within the file name so saving multiple versions makes it possible to decide at a later time that you may need to revert back to, you know, a previous working version. So there you have it, your data set should now be nicely formatted validated standardized cleaned and documented all ready to to be analyzed. So let's quickly wrap up with some takeaways on today's webinar. So take some steps to ensure that your data is safe authentic available and usable. Data should be in this analysis ready phase analysis ready data sets have been, you know, responsibly collected and reviewed. So that the analysis of your data and cleaning yields this clear, consistent and ever free result. So you should take some time up front. When creating data to focus on two dimensional data. These tidy data principles establish some consistency and standards and record your metadata and if needed review and validate and clean your data prior to conducting any data analysis. We have some upcoming webinars so be sure to register at the link provided on the slide. On Thursday we will be talking about publishing your hopefully nicely cleaned analyze data with our research. So thank you for joining today's webinar, the recording and the slides will be posted on our website. I would appreciate your feedback on today's session. So please fill out the survey at the link provided on the slide. 