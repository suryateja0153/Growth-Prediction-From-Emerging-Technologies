 live from Miami Beach Florida 2019 brought to you by beam welcome back to Miami everybody this is the cube the leader in live tech coverage my name is Dave Volante I'm healed my co-host Peter Burroughs two days of wall-to-wall coverage of v-mon 2019 they selected the Fontainebleau Hotel in hip swanky Miami tad Brockway is here he's the corporate VP of Azure storage good to see you yeah great to see you thanks work for pretty hip company Microsoft Azure is uh where all the growth is 70 plus percent growth and you doing some cool stuff with storage so let's you know let's get into it sure let's start with your role and kind of your swimlane if you will sure so our team is responsible for our storage platform that includes the our disk service 4i as virtual machines our scale out storage we call Azure blob storage we have support for files as well with a product called Azure files we support SMB base files NFS based files we have a partnership with NetApp we're bringing Azure NetApp files is what we call it we're bringing that app on tap into our data centers delivering that as a first party service we're pretty excited about that and and then a number of other services around those core capabilities and it's really grown over the last several years optionality is really the watchword there right given customers many options file block object etc how would you summarize the azure storage strategy yeah so in and I like that point optionality and and then in and really flexibility for customers to approach storage in whatever way makes sense so they're there there may be customers there are customers who are developing brand-new cloud-based apps maybe they'll go straight to object storage or blobs there are many customers who have datasets and workloads on Prem that are NFS based and SMB based they can bring those assets to our cloud as well we also have we're the only vendor in the industry that has a server-side implementation of HDFS so for analytics workloads we bring the file system semantics for those large-scale HDFS workloads we bring them into our storage environment so that the customer can do all of the the things that are possible with a file system create hierarchies hierarchies hierarchies for organizing their data use Ackles to protect their data assets and that's a pretty revolutionary thing that we've done but your to your point though optionality is the key and in being able to do all of those things for all of those different access types and then being able to do that for multiple economic tiers as well from hot storage all the way down to our our archive storage tier and instead I shortchanged you on your title because you're also responsible for media and an edge right so that includes Azure stack is that right right so we have Azure stack as well within our within our area and data box in data box edge those are data box edge and Azure stack are our our edge portfolio platforms so the customers can bring cloud-based applications right into their on-prem environments Peter you were making a point this morning about the cloud and and it's distributed nature can you make that point I'd love to hear Tad's reaction and response so tad we've been arguing in our research here wiki bonds tilaka negra for quite some time that hmm the common parlance the common concept of cloud move everything to the center was wrong and we've been saying this for probably four or five years and we believe very strongly that the cloud really is a technology for further distributing data further distributing computing so that you can locate data proximate to the activity that it's going to support but I do so in a way that's coherent comprehensive and quite frankly competent and that's what's been missing in the industry for a long time so if you look at it that way tell us a little bit about how will that approach or that thinking informs what you're doing with Azure and specifically one of the other challenges is how does and data services impact that so we'll come down a great insight by the way I agree that the the assumption had been that everything is going to move to these large data centers in the cloud and I think that is happening for sure but what we're seeing now is that there's a greater understanding of the longer term requirements for compute and that there are there are a bunch of workloads that need to be in proximity to where the data is being generated and to where it's being acted upon and there are tons of scenarios here manufacturing is an example where we have one of our customers who's using our data box edge product to monitor a an assembly line as parts come out of the assembly line our data box edge device is used with a camera system attached to it ai inferencing to detect defects in the assembly line and then stop the assembly line with very low latency where a round-trip back to you know a round trip to the cloud and back to do all the AI inferencing and then do the command-and-control to stop the assembly line that would just be too much round-trip time so in many different verticals we're seeing this awareness that there are very good reasons to have compute and storage on Prem and so that's why we're investing in Azure stack and data box edge in particular now you asked well how does data factor into that because it turns out in a world of IOT and and basically an infinite number of devices over time more and more data is going to be generated that data needs to be archived somewhere so that's where public cloud comes in and all the elasticity and the scale economies of cloud but in terms of processing that data you need to be able to have a a nice strong connection between what's going on in the public cloud and then what's going on on prem so the killer scenario here is AI right being able to grab data as it's being generated on prem ratan into a product like data box edge data box edge is a storage gateway device so you can you can app you or your your cameras in the in the use case I mentioned or for other scenarios you can route the data directly into a file share an NFS blob or SMB file share dropping into data box edge the data box edge will automatically copy it over to the cloud and butBut allow for local processing to to local applications as if it were in fact it is local running in a hot SSD and via meteor and the beautiful thing about data box edge it it includes an FPGA device to do AI inference offloading so it's a very modern device that intersects a whole bunch of things all in one very simple self-contained unit then the data flows into the cloud where it can be archived for you know permanently in the cloud and and then ai models can be updated using the elastic scale of cloud compute then those models can be back up brought back on prem for enhanced processing over time so you can sort of see this virtuous cycle happening over time where the edge is getting smarter and smarter and smarter is pretty good so that's what you mean kind of when you talked about the intelligent cloud and the intelligent edge i was gonna ask you you just kind of explained it that's right you can automate this use machine intelligence to actually determine where the data should land that's right and and minimize human involvement you talked about yes driving marginal cost of storing your data to zero which i've always we've always talked about you know doing that from the standpoint of reducing or even eliminating labor cost through automation but you've also get some cool projects to reduce the cost of the cost from storing a bit yeah maybe you could talk about some of those projects a little bit that's right so and that was mentioned in the keynote morning and and so we're our vision is that we want for our customers to be able to keep their artifacts that they store in our cloud platform for thousands of years and if you think about you know sort of the history of humanity that's not outside the question at all in fact wouldn't it be great to have everything that was ever generated by kind for the thousands of years of modern or human history we'll be able to do that with technology that we're developing so we're investing in technology to store data virtually indefinitely on glass as well as even in DNA and by by investing in those advanced types of storage that is going to allow us to drive that that marginal cost down to zero over time epigenetic storage systems I want to come back to this notion of services zone where a day is located so ok again from our from our research what we see is we see as you said data being proximate or being housed proximate was created and acted upon that's right but that increasingly businesses want the options to be able to replicate that or replicates wrong words it's a loaded word and to be able to do something similar in some other location if the action is taking place in that location - yeah that's what kubernetes is kind of about and serverless computing and some of these other things are about but it's more than just the data it's also that it's the data it's the data services it's the metadata associated with that how do you foresee at Microsoft and what role might be in play in this notion of a greater Federation of native services that are made that made possible like a policy driven backup restore data protection architecture hello that's really driven by you know what the business needs and where the actions taking place is that something they're seeing in a direction that you see it going yeah absolutely and so so so I'll talk conceptually about our strategy in that regard and where we see that going for customers and then maybe we can come back to the Veen partnership as well because I think this is all connected up the our approach to storage our our view is that storage should be you should be able to drop all of your data assets into a single storage system like we talked about that supports all the different protocols that are required can automatically tear from very hot storage all the way down to over time glass and DNA and we do all of that within one storage system and and then the movement across those different vertical and horizontal slices that can all be done programmatically or via policy and so it's so customers can make a choice in the near term about how they drop their data into the cloud but then they have a lot of flexibility to do all kinds of things with it over time and then with that we layer on the Microsoft whole set of analytics services so all of our analytics all of our data and analytics products they layer on top of this disaggregated storage system so that there can be late binding of the type of processing that's used including a I to reason over that data relative to where and how and when the data entered into the platform so that's sort of modularity it really future proofs the use of data over the long haul we're really excited about that and and then and then that those data assets can by can then be replicated to use your term to other regions around the globe as well using our our backbone right so the customers customers can use our network our network as a customers network and then the way that Doc's into the partnership with veem is that just as I mentioned in the keynote this morning data protection is a use case that is it's just fundamental to enterprise IT we can make together with customers and with with veem we can make data protection better today using the cloud and with the work that VM has done in integrating with 365 the integration from there into Azure storage and then over time customers can start down this path of something that feels sort of mundane and and and it's just you know been a part of daily life and enterprise IT and then that becomes an entry point into our broader long-term data strategy in the cloud pretty cool but following up on this so if we if we agree that data is not going to be entirely centralized but it's going to be more broadly distributed and that there is a need for a common set of capabilities around data protection which is a very narrowly defined term today and is probably going to evolve over the next few years I agree with that we think that this is where I want to test we think you're gonna have a federated model for data protection that provides for local autonomous data protection activities that it's consistent with the needs of those those local data assets but under a common policy based framework that a company like beam is going to be able to provide what do you think so we're first of all a core principle of ours is that while we're creating these platforms for large data sets to move into Azure the the most important thing is that customers own their own data so it's it's kind of this there's this balance that has to be reached in terms of cloud scale and cloud the federated nature of cloud and these common platforms and ways of approaching data while simultaneously making sure that customers and users are in charge of their own data assets right so those are the principles that we'll use to guide our innovation moving forward and then I agree I think we're gonna see a lot of innovation when it comes to taking advantage of cloud scale cloud flexibility and economics but then also empowering customers to take advantage of these things but do it on their terms I think there's like the future is pretty bright in that regard and the operative term there is their terms I mean you obviously Microsoft has always had a large on-prem install base and the software estate and so you've embraced you know the hybrid to use that term with your strategy so you've never sort of run away from it you never said everything's gonna go into the clouds right and and that's now evolving to the edge and so my question is what are the big gaps not necessarily organizationally or process wise but from a technology standpoint that the industry generally and Microsoft specifically have to fill to make that sort of federated vision a reality well there's I mean we're just at the early stages of all this for sure in fact as we talked about this morning the the notion of hybrid which started out with use cases like backup is is rapidly evolving more toward a more sort of modern enduring view I think in a lot of ways hybrid was viewed as this kind of temporary stop along a path to cloud and back totally or discussion by debate you all are having there but what we're seeing is the emergence of edge as being an enduring location for compute and for data and that's where the concept of intelligent edge comes in so the the bottle that I talked about earlier today is hybrid about is about extending on Prem datacenter assets into the cloud where as intelligent edge is taking cloud concepts and bringing them back to the edge in an enduring way so it's pretty it's pretty neat stuff and a big part of that is that much of the data if not most of the data the vast majority even it might stay at the edge permanently and of course you want to run your models up in the cloud that's right and you look for a real-time processing yes right right you just don't have the time to do the roundtrip so that's right cool all right Ted I'll give you a last word as your direction your relationship with veem the conference take your pick yeah well thank you okay thanks thanks great to be here as I mentioned earlier today the the partnership with Veeam and in this conference in particular is great because I really love the idea of solving a very real and urgent problem for customers today and then helping them along that journey to the cloud so that's one of the things that makes my job a great one where we talk about digital transformation all the time in the cube it's it's real it's not just a buzzword it can't happen without the cloud but it's not all in the central location it's extending now to other your data asset and where your data wants to live so Ted thanks very much for coming on the cube it's great to have you ok thanks guys all right keep it right there everybody we'll be back with our next guest this is Vemma on 2019 and you're watching the cube 