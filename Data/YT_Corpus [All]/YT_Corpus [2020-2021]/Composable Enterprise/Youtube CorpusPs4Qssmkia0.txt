 BOB AUGUSTINE: Good morning, good evening good afternoon. Thank you for attending this virtual event. Today I'm going to talk to you about SAS event stream processing at the edge and its ability to reduce or eliminate the need to transmit data to the data center for analysis. I'm Bob Augustine. I've been working at HP since 1988, and I spent the last 12 years working with SAS as my primary or only partner. I just wanted to tell you a little bit about our alliance with SAS. HP and SAS have been alliance partners since 1986. We're both in each other's partner programs. And in addition, my partner and I also have 30 years of experience with SAS. I've been working with SAS since 2008. In fact, my first SAS Global Forum was in 2008 in Washington, DC. And my partner, Mark Barnum, who's not on the phone with me today, has been working on SAS for 18 years. So we have a combined 30 years of experience. HP and SAS have a good relationship with SAS's R&D people. We have biweekly meetings where we discuss issues, customer problems, things like that. Why this is important to you is we understand familiarity breeds ease of-- or we understand our customer pain points. We're able to take our products and SAS products, and design things that will work for our customers right out of the starting gate. And some of these things are a little bit difficult to configure and make work in a robust manner. Our customers can spend a great deal of time trying to determine how to put these things together, or we can test with that software on HP equipment, and we can share with our customers how to put those things together, greatly reducing the amount of time between the time the equipment arrives on site and value. Let me give you a short example. A customer-- when we do a test with SAS, we spend about four to six weeks determining how best to configure the equipment. If a customer were to do that, they would have to do exactly the same thing. We're able to take the information we garner from doing this testing with SAS software and provide that customer so they're able to go live probably six or more weeks earlier than they would if they'd had to do their own type of testing. So I wanted to show this slide to you as HPE and SAS's edge-to-core scenario. Now, we're going to concentrate on the left-hand side, the IoT Edge side. But the reason to provide this slide to you is to show you that SAS has products and solutions for every stage of the data center or the edge. And also HPE has equipment that goes along with SAS so we can tweak things and lower your spend, because we don't have to kind of push things into places where they don't really belong. So we have peak systems right at the edge, we have peak systems in the data center, and we can bring that to bear for a customer's site. Now let's talk about some of the reasons not to transmit data to the data center. Latency-- let me give you an example. Autonomous driving-- if I'm driving down the road and my car's driving, if I have a person in front of me start to break, if I've got to to transmit that information up the data center, have it analyzed, and then sit back down to release pressure on the accelerator and placed pressure on the brake, if I'm doing that, if I'm transmitting it, all of a sudden, I'm in the back end of that the person in front of me's car. So latency can make the data less impactful, or at worst case, especially with autonomous driving, can make it worthless. There'd be no way to do this if we were not able to analyze the data right at the edge. Bandwidth-- some of the data that we fall from the edge would take quite a bit of bandwidth. Our customers today, there's quite a number of them that are doing research on autonomous driving. And they go out for seven-hour day, or an eight-hour day, excuse me. There'll be a person in the passenger seat recording things on a clipboard. There'll be a driver that had his hands near the steering wheel, but not actually driving. And there's a computer in the trunk it's collecting data, all the parameters, things like cameras on the bumpers or on the roofs of the cars, things like that. That collects upwards of seven terabytes of data per day. In order to transmit that data, even in a 10-gigabit network, which is today an average for a typical network, would take upwards of two hours per car, and we consume the entire bandwidth of that 10-gigabit network. HPE makes equipment that can have the disk packs removed from the server and placed into the data centers so we can transmit that data or transfer that data to the data center much quicker and much more cost effectively. Cost-- the larger the pipe, the more cost. And it's not a linear progression. Going from a 10-gigabit network to a 100-gigabit network is not twice or even 10 times as expensive. It's quite a bit more expense. Compliant-- there's some industries, take health care for instance, where it's against the law to transmit data outside or away from the edge. HIPAA, for instance, disallows things like that. Security-- whenever data is transmitted, it can be hacked. Duplication-- and this one's quite obvious, but if we have seven terabytes of data at the edge and we have to transmit or move that the data center, we also have seven terabytes of data the data center. That requires storage in the data center as well as at the edge, and it also requires CPUs to process that data at the data center. And reliability-- no matter how good data transmission protocols get, sometimes they introduce errors into the data. And so we would like not to transmit data to data center unless we absolutely have to. Now, let's take a look at some of the edge use cases. Theft and crime prevention, things like shoplifting, or-- and we've all seen this on the news-- thieves placing facades in front of ATMs to catch people's PIN numbers and their card swipes. We can see that with a camera. And if we see that happening, we can do something about that. Customer insight-- this is a particularly interesting scenario, at least for me. So if we have a series of cameras, and we do already in retailers for theft and crime prevention. When a customer goes down a shelf, we can see through those cameras what the customer happens to be wearing. So if they're in an apparel store, for instance, we can see, oh, well, they like, I don't know, this type of shoe. And we can say, oh, well, we can read their faces and determine if they're happy with what they're seeing. We can make suggestions with what they might want to purchase, and so on and so forth, and hopefully enhance their customer shopping experience. Manufacturing, this is a this is a particularly interesting one also. We have a customer now that manufactures disk drives. They have cameras on the manufacturing line that look at the disk drives, and they determine if there are pits or problems with the disk right before they go further in the manufacturing process. So if I have a particular platter that's having a problem, I can eject it from the manufacturing process before it gets blended into an entire disk drive and we find out that it's faulty at the end of manufacturing, after spending everything. Smart City-- today we can do gunshot location. We can tell you the caliber of gun that's being used in the gunshot. We can tell you the direction. So we can get police officers out to sites of gunshot activity very quickly, rather than having to wait for people to phone in 9-1-1. And traffic-- think about this. Traffic movements and traffic lights is an activity. I live in a rather large urban area, and sometimes the interstates get rather clogged going in and out of the city. If I'm able to read those with sensors in the road, and maybe there's an accident on the interstate, I'm able to deviate traffic off the interstate to the secondary surface streets, and then I'm able to activate traffic lights so that the traffic on those surface streets moves much more quickly. And that helps public transportation and tourists. So let's look at the logical data flow. This use case came from SAS Global Forum last year. SAS provided customers with the ability to remote control a ball rolling around on a floor. There was a camera focused on the floor, and it tracked the ball as it moved around on the floor. The feed came from the video camera to the computer at 30 frames per second. We were able to, during those 30 frames, or once every 30th of a second, each time we had to identify the ball, identify where it was, compare where it was with where it was during the last frame, and then generate vector, speed, acceleration, and all that kind of information on the fly. Here's the software and versioning that we were using during our testing. Oh, yes, we use SAS Event Stream Processing 6.1, Viya 3.3, and then we used NVIDIA 41.104 and CUDA version 10.0. I want to take a moment to talk about the equipment that we use. We have a server chassis called an EL4000. This takes one to four computer cartridges. Each computer cartridge is functioning entirely independently. They cannot be blended, but they can talk to each other. The type of computer cartridges that we are able to insert into the EL4000 are m510s. Those come in two flavors-- 8-core 2 gigahertz or 16-core 1.7 gigahertz. We also have an m7 10x comes in a 4-core flavor with 3.2 gigahertz clock speed. And finally, we're able to put the NVIDIA C4 GPU into the server for use. Here are some of the characteristics of the C4 GPU. Notice the massive number of cores that are available to processing. What SAS does is they just aggregate the data feed. In this case, they would just aggregate it into 2,560 separate screens, and each one of those screens get fed in parallel to the C4 GPU. Now, think of each one of those cores like you would a CPU on a system. The most cause we have today, or which I'm aware is about 32 cores per processor, but we have this GPU, which actually has 2,560 processor. Let's take a quick look at the SAS data flow. We read the data in from the video file that SAS provided us for this test. We push that data to W score. W score IDs the ball on the floor. The we push that to tracking. This IDs where the ball is currently. We filter the data, and we push it to with outputTracks where the present location's compared to the location where it was during the previous screen capture. Then we push that to speed direction, which calculates the direction the ball is traveling and its speed over the past 1/30 of a second. The thing that we're interested in here, and the thing that I was interested in when we did this testing, is we want to understand what difference fewer cores with a higher clock speed might make the throughput versus more cores at a lower clock speed. And I'll provide you with a little spoiler alert ahead of time. There wasn't a whole lot, if any, different. We want to know the total frames we can process on a system-wide basis because that's going to tell us kind of how many we can process, how many streams you can process. And then we want to know how many frames we can process in a per process basis. And we want to know that because SAS speed fed the data to us at 30 frames per second, so we kind of use that as a floor. Now, you might need in your particular instance, you might need 90 frames per second. Or you might only need 20 frames per second, and that's a metric which we want to track. And then what impact is having a GPU in the system have to the overall performance of that system? What difference does the GPU make? Let's look at some of the performance results. As you can see here, we start out with one stream, and we go to nine streams. This is on the 8-core, 2-gigahertz system. When we start out with one stream, we're at 90 events or 90 frames per second across the entire system, and that goes up to 200 frames per second with nine streams. On the next slide, you can see what this does on a per process basis. We're starting again at 90, as you would expect. We get the two streams are down at about 75 frames per second. Three streams were at about 55 frames per second, and we don't dip below 30 frames per second till we get to nine streams. So even with nine streams, if you're able to handle 20 frames per second, we're able to do that with nine streams. You could have nine separate video feeds coming to each cartridge in this system. Here's the same set of slides or the same result in the 16-core system. And again, we start at about 90 frames per second system wide with one stream. We go up to about 200 frames per second system wide with two streams. I want you also to notice-- I didn't pull this out in the previous couple of slides, but I also want you to notice what the system is doing in the orange bars, in the number of frames per second that did a process without the GPU. Here is a slide that has the 16-core system, the number of frames per second per process. And again, you're going to see just about the same results with the 16-core system as with the 8-core system. The one thing to note is that the orange frames or the orange bars are a little bit lower with the 8-core system than they were with the 16-core system. Now let's do a performance comparison. This is the same data that I showed you before. We put it all in one slide, the 8-core system with 2 gigahertz. So the gray line at the top is the time to complete the entire test without a GPU, so just the system's CPU processing the data. And the kind of teal line at the bottom is it time to finish or complete. As you can see, it starts out with one stream at about 60 seconds, believe it or not, to process all the frames with the GPU in the SAS provided. With the CPU, we're over 400 seconds. And that went up, with nine streams, to 200 seconds with a GPU and over 1,000 seconds without the GPU. Here is the same graph, the same graphic, when you get to the 16-core system. And while we didn't get over 1,000, we came up with 950, 970 seconds. So that that's a little bit higher impact. But we did, on the bottom with the CPU, and we stayed at roughly the same throughput or the same time to complete those tests. Now, we're able, with NVIDIA GPUs, to keep track of what the GPUs are doing. NVIDIA, these GPUs are not actively cooled, so NVIDIA needs to make sure that we don't overheat, we don't melt the silicon. And as you can see, as we start to ramp these things up, the peak temperature went from about 70 degrees Celsius up to about 85 degrees Celsius. What happens with NVIDIA GPUs is, as the temperature goes up, they start to lower the clock speed to keep the temperature from getting too high having us melt the GPU. You can see that also the peak watts that we're used, and as we get to the nine streams, you can see the dip in the peak watts used. And that's a feature of having to self-regulate the GPU to keep it from going well over temp. That's with the 8-core 2-gigahertz results, and here is the same information with the 16-core, 1.7-gigahertz. You can see the same type of thing happening, except the knee of the curve, the amount of watts consumed, going down much further when we had to self regulate it up into the test. Here is where to get more information about HPE and SAS and what we've done with reference architectures, how we've configured things, what the throughput analyses looks like. At this time, I'd like to thank you for your time. And I appreciate you coming and viewing this video. I would like to call out on this particular slide the email addresses for myself and one of my partners, Sri Raghavan. You are welcome to contact us if you have questions, comments, or concerns regarding this video. Thank you very much, and have a great day. 