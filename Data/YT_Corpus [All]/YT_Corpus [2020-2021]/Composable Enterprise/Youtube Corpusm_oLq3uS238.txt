 [Music] hello everyone and welcome to today's session where we're going to talk about enterprise-grade etl and data preparation using power query and data flows my name is miguel jopis and i'm a program manager on the power query and data flows team at microsoft and i have a pleasure to be joined today by ben zak hey ben how's it going hey miguel all good awesome so let's talk a little bit about enterprise grid adl and data prep we're going to split this into a few different blogs the first one we're going to focus on discussing why do we need data preparation and then we'll talk a bit about power query and data flows and how they help in this space we'll do a bunch of demos as well we'll have a very demo centric session and we'll end with an update on capabilities that are coming to the product over the next few months and also with a few a snake preview of those demos for those new features why do we need data preparation so data preparation in today's world is actually very challenging in one hand it's really hard to find and connect the data uh for for end users who need to use it either for analysis or maybe app development and other purposes understanding what are the right locations for that data whether it's files or databases and even once you understand what it is that you need to access getting the right permissions getting the right credentials to access that data is really difficult to make matters worse the experience is to actually connect to those data sources and prepare data are fragmented they're different uh varying by different products and also the underlying technologies uh particularly when it comes to accessing the data source backends are also you know very heterogeneous which makes the task really hard um also you know after being able to connect that data in most cases very often if not always data will need to be reshaped before you can actually derive value from it this doesn't necessarily mean that the the data is in bad shape or you know uh of poor quality it makes it that's actually the case but it also means that even when data is clean and it's ready to be consumed it's actually not in the exact shape that users need for what they're trying to do the answer they're trying to get from their data so that you know data very often or always needs to be reshaped in many cases this shaping is actually one-off and it's hard to make repeatable and you know as users would like to continue refreshing that data over time having to redo the same tasks to actually get it into the right shape that they would like to use is very time consuming and you know expensive to make things even even worse if you know connecting to a single data source was not hard enough already we know that data never comes from a single place users very often need to actually combine data from multiple data sources and that makes the complexity of their data preparation just exponentially grow when we talk about big data we always think about the volume of data but there's actually three v's that are associated to big data the volume is clearly one but also the velocity and the variety the variety refers to the differences in data source structures schemas data shapes that need to be actually you know consolidated and normalized in order to make sense of that data and the velocity speaks to the differences in the rate of change of that data if you think about it you know there are data sources that maybe change once a month or once a quarter you know financial results monthly sales numbers but there are on the other extreme data sources that could change every hour or even you know multiple times per minute think about iot devices and many other sources that are near real time for a user a non-technical user to be able to actually combine all of that data together make sense of it so that they can derive value it's a really challenging task a few quotes that actually validate that that message this one is from garner one of the leading analyst companies in the world um that says that the data analysts spend up to eighty percent of their time on data preparation which delays the time to analysis and decision making so if you think about it somebody gives you a week to actually complete a decision process you're going to spend the first four days the first four days of the week actually working on just getting ready so that on friday on that last day you can actually make the decision isn't that painful and expensive um and that's even if you're lucky because look at this other quote from harvard business review which says that 75 percent three-quarters of the companies are not able to actually take action and derive value from the data that they collect mostly due to the you know the the disjoint systems and the data integration issues that they hid along the way and one last quote this is actually a prediction from garner who predicted that sometime this year by 2020 and this was a prediction from two years ago that the amount of data produced by citizen data scientists will actually surpass the data produced by the code professional data scientists so what this really means is that not only we need to make the process easy and feasible for the user who's actually trying to integrate data and prepare data across disjoint systems we also need to democratize it and make it super simple because the demand for lower end you know skills to be able to do this job is actually growing more and more to the to the point that it's surpassing the professional uh you know um users um doing this so with that context how does a tool like power query from microsoft can help with that um we talked a little bit about the variety on the data and uh you know power query providing connectivity to hundreds of data sources and that includes data of all sizes and shapes power query gives you a highly interactive and intuitive experience that allows you to preview data based on you know top 200 to a thousand rows of data so that users get enough of a sense for what the data shape looks like and to understand the operations they need to apply whether it's removing columns or filtering out some values based on profiling those capabilities are provided based on a sample of data which also takes care of the data sizes and the data volumes challenge the experience is very interactive agnostic of the full data set size it allows you to do things by example on the on the top uh data coming from that data source and ultimately it will run over the entire data set once you complete the design time uh transformations and tasks the experience we actually provide is consistent across all of these hundreds of data sources we provide 300 or more data transformations across all of these and they're consistent we'll take care of compensating whenever a data source is less capable than others in terms of query execution and the last point here is about combining data which as we said it's a big part of the challenge barcode provides seamless ways to combine data whether it's by doing joins or unions merge and append in power query terms with fussy matching and many other technologies along those lines that make the task much easier power query is the unified data connectivity and preparation experience and technology from microsoft that ships across eight or more different microsoft products and we'll dig a bit more onto that over the next couple of slides power query is available both as a desktop component that ships into products like power bi excel and analysis services as well as online the capabilities on both are consistent the mashup engine which is the underlying query runtime behind power query provides support for over 140 out-of-the-box connectors or data sources these are actually extensible as we will see over the next couple of slides we provide 300 or more data transformations since things like filters pivots and pivots spreading columns replacing values removing duplicates you name it we have most of those uh transformations and this is also an extensible set we provide the ability to combine data across sources and uh we provide a very code free and visual experience as you can see in the screenshot here with a very centric view of the table that the user is connected to and lots of information about value distributions histograms profiles error values that make their task easy just point a click at the data that you would like to transform or reshape and use the ribbon or the context menus to access those operations as we were saying power query provides over 140 data connectors today out of the box what you can see here is an animation of the data dialog inside of power bi desktop as you can see we have lots of connectors across different types of files databases the power platform azure data services other online sources and services and many other sources this is a combination of connectors built by us by microsoft and also connectors built by the community and the isbs third-party isbs who for example maybe own a data source or two examples here would be someone like dinodo who owns a data virtualization platform building their own connector uh for power query and certifying it with us with microsoft so we actually ship it as part of the product and it's one of those 58 certified connectors that as you scroll through this uh dialogue they seem very seamlessly integrated with the product and you may think they all come from microsoft but this is actually a strong community and isb ecosystem around it as well and on top of that anyone literally anyone out there could actually create their own custom connectors and easily plug them into power query we see thousands and thousands of those connectors active every every single day connecting to data as we were saying power query is also available as an online component that is integrated with many other products um namely things like data flows which we're actually gonna dig deeper uh in a few minutes uh within power bi power apps dynamics 365 customer insights we also have the power query capabilities embedded within other areas of the power apps experience power automate particularly to reshape and transform data on top of the sql server connector in power automate and also within azure data factory which is known as adf wrangling data flows which is soon actually going to be renamed to power query data flows for clarity the capabilities here very similar to what we were just describing in the desktop just from a product maturity standpoint we started the power query online project much later than the desktop experiences so we're still closing the gap on some of those connectors uh we are at about 50 connectors in power query online versus the 140 that we were just talking about in desktop we're very quickly cutting up towards that and you will see that through the roadmap as well with that i'm going to hand over to ben so ben can talk to us a bit more about what our data flows so ben what are data flows thanks miguel so let's review what data flows are data flows are a self-service cloud-based data preparation technology data flows allow customers to ingest transform and load data into either the common data service uh storage power bi workspaces or even your organization's azure data lake storage gen 2 account data flows run in the cloud they can be configured to run on demand or on a schedule dataflow customers authored dataflow using power query online uh which miguel just reviewed for us so let's take a look at the steps that are required to author a data flow so the process starts by the application that hosts dataflow and today data flows are available in power bi powerapps or customer insights applications the first step is to indicate which data source you want to get data from and as miguel mentioned there's hundreds of clouds and on-premise data sources and the list is constantly growing after you select the connector and authenticate to it you can transform and reshape the data using the power query online experience um there's as miguel mentioned 300 plus transformations customers can write their own custom mashup code to perform their own transformations the options are limitless once you are done shaping the data there are a few other configuration steps like how frequently it would refresh and sometimes if you load the common data service you you have an experience to map the entities in the data flow to common data service entities you can also save data flows to power bi workspaces or as i mentioned to uh your own organization's azure lake uh azure data lake storage gen 2 account and that other services can consume the data so one one thing that's cool about analytical data flows is that they're composable so data flows is a pipeline that loads data into a destination data source but then you can actually create other data flows that can load data from these data flows so if i'm in any one of the experiences that data flow's available in i can leverage instead of the hundreds of connectors and on-premise data sources that are available i can choose the power platform dataflow connector and then what i'll see are entities that other data flows have created i can continue on modeling them these are called computed entities and once i've finished transforming the entities i can load them into the same destinations that i mentioned before one thing that's interesting with data flows that are composable is that when one data flow completes its refresh process if you've composed another data flow on top of it within the same environmental workspace that will trigger the downstream data flow to refresh as well and in that sense you can build complex data pipelines that actually stay in sync and refresh together where can dataflow outputs be consumed from so when data is loaded to common data service power bi storage or azure data lake gen 2 you have a few options in the case of common data service data flows you can consume the data from the data flows in all four of these experiences using the common data service connector you can also consume the data from the power platform data flow connector if it was stored in a data lake whether your organizations or a data lake within any one of these three products that is any one of these two products that are is provided to you by default in addition if you load data to your own organization's data lake we store data flow data in what is called cdm folders which is a standard that microsoft has developed in order to allow services to interop with each other on top of the lake without actually having to integrate service to service what that means is once the data flow deposits the data in the lake you can obtain the data schema and the data from within the cdn folder using libraries or azure services to consume the data and then you can build data flows that basically enable collaboration between all of the services that you see above and below here via the lake and the cdm folder standard what are common use cases for data flows data migration or ingestion into common data service entities so a lot of times customers would like to sync data into the common data service and build power apps and flows on top of it so that's uh the one primary use case another one is uh lift and shift to the cloud whether you have on-premise data sources that you want to make available in the cloud in powerapps or in power bi you can leverage data flow with an on-premise data gateway to load data into the cloud and then microsoft cloud services can consume the data very efficiently from that point on using the dataflow connector or again the azure data lake storage gen 2. another application is business intelligence and power bi you can build reports dashboards and data sets on top of data flows and share those dashboards and reports with people in your organization you can also adjust data with data flows for use in insight application for example customer insights leverages data flow to ingest data from multiple data sources in order to provide customers a unified profile of their customer across all these data sources finally ai builder scenarios can be powered by data flows ai builder is a functionality within the power ups maker portal that leverages data in common data service and so the first step is leveraging a data flow to bring data into the common data service and then using ar builder to consume it with that i'd like to switch gears and move to a demo we'll kind of take you through creating standard analytical data flows and then how the outputs can be consumed from power bi okay so we'll start by creating an analytical data flow what i want to do is get data from a few data sources i work at a international company and i would like to organize my order data with customer data and the employees that submitted the order now i have orders from europe and rest of world and they're in two databases so you saw me getting data from sql dw and then my second order table is from a csv file so now i have all my entities here so i can start transforming and as you can see the orders table has the customer id and employee id but not the metadata for those customers or employees so the first thing i'm going to do is i want to append the rest of world orders and the european orders so i'm going to append queries and i'm going to choose the first table to append and then the second table and that's going to create a new entity it's first called append i'm going to rename it this is going to be the entity that has all orders so now i have the orders but i don't have the customer information or the employee information in this table but i do have them another table so what i'm going to do now is join these tables so let's join one table at a time we'll start with a customer id i'm gonna choose the customer table and then the column that i should join on and there's a few join options uh left outer i'm trying to join everything from the first table uh that matches everything from the second table this is gonna add a new column for me and that column is gonna be the second table but i would like to see the data in this table so i'm expanding the column and in this case uh company name contact name they're pretty self-explanatory so i am going to uncheck use original column names as prefix so i'm just going to see those names and now they're going to be expanded into the table so you can see they were added and now i'd like to see the employees that submitted the order so i'm going to join that with the employee table i did notice that the employee format was text and numbers but it should be a number so i transformed it to a number really quick and now i'm going to join again from the order table with the employee table and again left outer and again i'm going to have to expand the columns but over here last name first name they don't make enough sense so i kept the prefix and now you'll see employee name and then the first name last name next i'm done transforming my data now i need to decide how frequently it's going to refresh i like to refresh every day 2 a.m to let the data settle and then i selected the option to receive an email notification if the data flow fails now i want to load the entities to common data service and also demonstrate that i can get data from the data flow i just created so you can chain data flows and the first data flow is an analytical data flow they're perfect for transformations and the second data flow is just loading into common data service so i can use it in a power application or power automate this screen is the mapping screen so this is a specific to common data service entities it's normally a good idea to define a key field i'm creating a new entity so i can define the key field here it's currently saying that a key field cannot be multi-line text so i just need to change it to text multi-line text was the default and using a key allows me to make sure that every time i refresh the data flow it's not going to create new entities if those entities already exist so it's just going to update them so now i have two data flows one uses data from the other and next i'm going to show you how i can using the same experience in power bi desktop get data from this data flow so i'm going to quickly switch to power bi i'm going to hit get data and the power platform data flow connector and the power platform data flow connector can show me data flows created in powerapps environments or in power bi workspaces in this particular case we're going to go get data from the same data flow i created so i'm showing you how it's possible to ingest and transform once and then reuse the data very easily with data flows and then power bi desktop also uses power query same experience same look and feel but for a different use case so that's the first demo and i'm going to switch back now to the presentation thank you ben let's actually take a look after looking at these awesome demos to what's coming next here's a quick view on to the connectors roadmap and here's a recap on things that we recently shipped as you can see and as i was alluding to earlier um we are actually very closely uh you know very quickly closing the gap uh on power query online connectors and you see that we've released many many connectors lately including things like google bigquery amazon redshift spark impala odbc azure data lake azure sql database and data warehouse or synapse analytics connectors and many more we continue to work towards making the experiences easy to use so an improvement that you will see coming up in a few months to power query online is the ability to do a one-time upload of a local file within the file connectors so today in order to bring data from a local file you would actually have to use in on-premises data gateway which we understand you know introduces friction into the process we're trying to make that much simpler similarly you will see a new connector which is actually recently made available within data flows in powerapps to parquet files allowing you to connect and you know reshape and transform data from pocket files sitting within azure data lake storage accounts we will be lighting up that connector also within data flows in power bi and eventually within power bi desktop as well we also made a lot of enhancements and uh you know additions to both folder connectivity with connectors like the local and sharepoint folders as well as cube connectors we recently released the sap hana and sap business warehouse connectors within data flows in power bi and power apps and will very soon also release connectivity to analysis services both on premises and azure analysis services adobe analytics and google analytics so stay tuned to our blog announcements for that uh on the desktop front we a few months ago released the high lap connector we're also coming out soon um i think by the time we actually uh published this video in the late september it will already be in market we are releasing a new connector for azure databricks and we're also releasing an updated version of the common data service connector uh giving you much better performance and direct query capabilities as well the last button here i have is for by example capabilities which you know continuing with our effort around simplifying the experiences and making them more seamless it expands on our web by example and column from examples capabilities to new data sources particularly text csv files and also automatically being able to detect tables from excel and json files so we'll show you in a few minutes a couple of demos around those capabilities on the transforms and query editor capabilities again lots of great enhancements over the last few months the one that i wanted to call out here is coming soon is the ability to use passive matching capabilities as part of grouping values allowing you to do both fussy grouping and clustering of values in a text column we'll show that to you in the demo as well finally last but not least we're also making enhancements to the power query online experience so that you can easily copy paste your queries between power query desktop and power query online and also we are bringing the query diagnostics capabilities that you may have used within power query in the desktop we're bringing them to power query online but we're also expanding beyond that so an area that it's been recurring uh area feedback from many of you has been being able to clearly tell whether a step is folding is being pushed down to the underlying data source or not or being able to see the query plan and the underlying data source queries for for each of those steps we're actually starting to make some enhancements around that in the context of power query online which should be coming over the next few months so again stay tuned for those capabilities in the product and in the blog announcements really soon with that let's actually let me show you a couple of demos around the text csv by example and faster grouping capabilities the first feature that i want to show you is text by example in power query desktop this feature allows users to extract data from semi-structured text or csv files like in this sample file that i'm showing you which contains repeating information from customers throughout the file by going into the by example experience customers can easily provide those sample values that they would like to extract as you can see the by example experience helps users by providing suggestions of the values based on what they type once enough sample values have been specified power query will figure out the magic steps to make happen users can then continue providing other sample values for fields that they would like to extract based on the same repetition pattern like in this case specifying a contact title or a phone number after by example users can actually land the output into the power query editor and here you can actually see that the sequence of the steps that power query identified were needed in order to perform this data extraction can be reviewed or adjusted from the steps pane the second feature that i'm going to show you is faster grouping within power query online which allows the duplication and normalization of values like the ones captured in this table with employee names you may be familiar with the merge queries capabilities available in power query that included fussy matching logic in the past this would allow users to combine data across different data sources using fussy logic but it was not clearly obvious to users how they would be able to leverage these capabilities from the group by column experiences classically you would apply a group by operation to for example add up all of the total hours spent by week by each of the employees in this table and you will end up with repeated values with bill billy wheel as well as multiple variations of miguel with typos and uppercase lowercase differences with the new fassi grouping capabilities in group i users are able to leverage the same fussy logic that they have available to them within the merge queries experience in order to get normalized values such as in this case bill and miguel however notice how there's still a difference between bill and william which conceptually are the same values but they're not that close for the fastest logic algorithm to figure out that they need to be combined for that purpose a synonyms table can be used as the input to the group by configuration as a transformation table that will make the two values be considered the same and ultimately end up having bill and miguel as the two reconciled values fussy grouping will ship in power query online in the next few weeks and it will accrue value to all of the product integrations where power query online and data flows are available thanks miguel awesome demos so switching gears to data flow roadmap updates so the features in front of you are either in progress or coming soon so let's quickly go over them enhanced compute will allow dataflow customers to leverage azure synapse compute to increase the performance of analytical data flows these are data flows that are stored in the lake in scheduling and automation area we're making a few improvements we're adding the ability to refresh on specific dates and times in addition to refreshing on a frequency or manually we're also making improvements to our back end to have a consistent experience for data flow refresh that's similar across power bi power apps customer insight data flows in addition we're releasing a data flow power automate connector which will allow you to create flows that trigger when a data flow refresh completes you'll be able to see the data flow refresh status and then also take action uh like refreshing other data flows so this will allow you to kind of combine data flows across power bi power apps and with other services that once data completes refresh you can actually go and take action in a different service in the area of deeper integration with cds there's a lot of great things coming so cds performance improvements we're improving load speeds uh increasing the load limits and also reacting to cds throttling events all these changes will amount to higher speed and higher reliability for data flows that load data into cds in addition uh now you can create analytical data flows without having to connect your azure data lake storage gen 2 to the environment so by default you'll have a data lake provided by common data service that you can create analytical data flows and and then use the data there in all the services that support data flows or the data flow connection solution awareness is an application lifecycle concept and powerapps it will allow you to package data flows with other artifacts in the environment and deploy them between dev test and production environment or package them to share with other people cds entity mapping we're making a few changes to the uh map entities experience uh smart defaults to allow you to just click next if you don't have any special configuration and then auto-generated primary keys to simplify mapping uh keys in the data source to entities that have keys and then single and multi-line fields based on data profiling will auto detect those finally incremental refresh for data flows that load data into cds this is a feature that is already available for analytical data flows but now you'll be able to configure a data flow to just grab the latest increment of data as opposed to reloading all the data at once this will improve the overall performance of data flows and reduce the load on cds and with that i'd like to demo uh the power auto automate connector in action so for this next demo i'm just going to walk you through really really quickly uh how to use power automate with the the data flows we just created so a power automate connector is coming soon for data flows i can initiate a flow when a data flow refresh completes i just need to select the workspace or environment it was created in and then i can also trigger a refresh of a downstream data flow so the second data flow i created but what i'm going to do over here is i'm going to make it a little fancier and add a condition i should probably refresh only if the original data flow the first data flow completed successfully and if it fails why don't i send an email to me and miguel and then i can use metadata from the trigger of the data flow to construct the email so uh in the subject i'll use the dataflow name to let us know which data flow failed and then i can author the email with the parameters that i have from the data flow refresh like the again the name the start time of the refresh the end time and yeah status of the dataflow refresh now once the data flow refresher completes this flow is going to trigger and initiate the other data flow now i can also use the power bi connector to trigger a data set refresh and then basically orchestrate the transformation of data loading it into cds and then also refreshing a report and dashboard that's built on top of the data so this connector is going to be coming soon and that's my second demo back to you miguel thank you ben that was such an amazing demo and i'm looking forward to seeing that capability in the product i'm sure um our customers will love to see that as well we're getting towards the end of the session and i just wanted to share a few pointers for where all of you can actually go to find out more and learn more about power query and data flows your main place to go should be powerquary.com within powerquery.com you're going to find the resources page which is the screenshot you see here on the screen that actually has pointers to all the other product specific uh documentations forums uh you know ideas uh suggestions uh pages as well where you can actually suggest features um for power query across all of these products i also wanted to call out the power query documentation site the the back screenshot that you see here which is docs.microsoft.com power query we've added over 50 different power query articles new articles over the last month or two and we continue to add more and more this includes a full set of references for all of our connectors uh including troubleshooting guides prerequisites and you know any other capabilities that need to be explained for each of those connectors but also you know step by step and very uh deep dive uh you know documentation resources for each of the transformations provided by power query things like pivoting and pivoting when when is the right situation to use one or the other how do you go about using them and really helping users get started with all of those advanced capabilities and advanced transformations that power query provide the last link is to the m language reference which is the underlying formula language behind power query so we have a full reference for all the functions available as well with that this is the time for thank you and to actually you know in a regular year this would have been the round for open q a in this case uh all i can do is thank you for your presence and actually point you out our communities uh we listen uh to feedback and respond to questions there every single day so please keep those coming and we really appreciate your attention and your feedback thank you and have a good one 