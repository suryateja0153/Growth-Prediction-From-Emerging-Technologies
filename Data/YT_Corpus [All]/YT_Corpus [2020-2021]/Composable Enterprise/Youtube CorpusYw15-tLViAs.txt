 from around the globe it's the cube covering HPE discover virtual experience brought to you by HP e hi buddy welcome back this is state Volante for the cube and this is our coverage of discover 2020 the the virtual experience of HPE discover we've done many many discovers as you know usually we're on the show floor the cube has been been virtualized and and we talk a lot at HPE discovers you know a lot of storage and server and infrastructure and networking which is great but the conversation we're gonna have now is really we're gonna be talking about helping the world solve some big problems and I'm very excited to welcome back to the cube doctor angling go he's the senior vice president of CTO for AI at HPE hello dr. go great to see you again hello thank you for having us Dave you're welcome and and our next guest is Professor yo Keem Schultz ahooo is the professor for genomics and immuno regulation at the University of Bonn amongst other things professor welcome thank you welcome and and then Prasad Shastri is the chief technologist for the India advanced development center at HPE welcome Prasad great to see you thank you thanks for having me so guys we have a cube first I don't believe we've ever had of three guests in three separate time zones I'm in the fourth time zone so I'm in Boston dr. go you're in Singapore professor Schultz a year in Germany and per year in India so we've got four different time zones plus our studio in Palo Alto who's running this program so we've got actually got five time zones a cube first amazing very good such as the world we live in so we're gonna talk about some of the big problems I mean here's the thing we're obviously in the middle of this pandemic we're thinking about the post isolation economy etc people compared obviously no surprise to the Spanish flu early part of last century they talked about the Great Depression but the big difference this time is technology technology has completely changed the way in which we've approached this pandemic and we're gonna talk about that and doctor go I want to start with you you've done a lot of work on this topic of swarm learning if we could my limited knowledge of this is we're kind of borrowing from nature you think about you know bees looking for a hive as sort of independent agents but but somehow they come together and communicate but tell us what what do we need to know about swarm learning and how it relates to artificial intelligence and we'll get into it Oh Dave that's a great analogy using swarm of bees that's exactly what we do at HPE well so let's use the example yeah when deploying artificial intelligence a hospital does machine learning of the application data that could be biased you know due to demographics and the types of cases they see more also they sharing patient data across different hospitals to remove this bias is limited you know given privacy or even sovereignty restrictions right like for example across countries in the EU now HPE swamp learning fixes this by allowing each hospital who still continue learning locally but each cycle we collect the learnt weights of the neural networks have reached them and send it back down to all the hospitals and after a few cycles of doing this all the hospitals would have learned from each other removing biases without having to share any private patient data that's the key so the ability to allow you to learn from everybody without having to share your private patient that's swollen and part of the key to that privacy is blockchain corrective and you you've been involved in blockchain and invented some things in blockchain and that that's part of the privacy angle is it not yes absolutely there are different ways of doing this kind of distributed learning which swamp learning is however many of the other distributed learning methods require you to have some central control so Prasad and the team and us came up together you have a method where you were instead of central control use blockchain to do this coordination so there is no more a central control or coordinator especially important if you want to have a truly distributed swarm type learning system no no need for a so-called trusted third party or adjudicator okay professor Scheele so let's go to you you're essentially the the use case of this swarm learning application tell us a little bit more about what you do and how you're applying this concept as I I'm actually by training a physician although I'm seeing patients for a very long time I'm interested in bringing new technologies to what we call precision medicine so new technologies both from the laboratories but also from computational Sciences I'm marrying them and then basically basically allow precision medicine which is a medicine that is built on new measurements many measurements of molecular phenotypes how we call them so basically we assess on different levels for example the genome or genes that are transcribed from the genome we have thousands of such data and we have to make sense out of this this can only be done by computation and as as we discussed already one of the hope for the future is that with the new wave of developments in artificial intelligence and machine learning we can make more sense out of this huge data that we generate right now in medicine and that's what we're interesting in to find out how can we leverage these new technologies to build new Diagnostics new therapy outcome pretty so to know whether a patient benefits from a disease as from a Diagnostics or a therapy or not and that's what we are what we're doing for the last ten years and the most exciting thing I have been through in the last three four five hours is really when HPE introduced us to small running okay and Prasad you have been helping professor Schultz to actually implement swarm learning for specific use cases we're going to talk about Co vid but maybe describe a little bit about what you've been here are your participation in this whole equation yep thank as dr. Engel him go mentioned so we have used a blockchain as a backbone to implement the decentralized network and to that we are enabling a privacy preserved decentralized network without having any control points as professor explained in terms of the pressure medicines so one of the use case we are looking at is looking at the blood transcriptomes think of it different hospitals having a different set of transcriptome data which they cannot share due to the privacy regulations and now each of those hospitals will train the model depending upon their local data which is available in that hospital and share the learnings coming out of that training with the other hospitals and we try to over several cycles to merge all these learnings and then finally get into a global model so to that we are able to kind of get into your model which provides the performance is equal of collecting all the data into an central repository and trying to do it and we could really think of when we are doing it them could be multiple kind of a challenges so it's good to do a decentralized learning but what about if you have an on iid type of a data what about if there is a dropout in the network connections what about if there are some of the compute nodes which are stragglers or probably they're not seeing sufficient amount of data so that's something we try to build into the swarm learning framework you handle the scenarios of having non iid data or in a simple word we could call it as saying having the biases an example one of the hospital might see if we are trying to look at in terms of let's say the tumors how many number of cases and whereas the other hospital might have very less number of cases so we have kind of implemented some techniques in terms of doing the merging or providing the various different kind of weights or the tunable parameters to overcome these set of challenges in the swarm learning and professor Schultz au you've applied this to really try to better understand and in an attack the co vid pandemic can you describe in more detail your goals there and what you've actually done and accomplished yeah so um we have we have actually really done it for corporate um the reason why we really were trying to do this already now is that we had generated these transcriptomes from code 19 patients ourselves and we realized that the the signal of the disease is so strong and so unique compared to other infectious diseases which we looked at in some detail that we felt that the blood transcriptome would be a good starting point actually to identify patients but maybe even more important to identify those with severe diseases so if we can identify them early enough that we basically could care for those more and find particular for those treatments and therapies and the reason why we could do that is because we also had some other test cases done before also we use the time wisely with large data says that we had collected beforehand so I use cases learned how to apply swarm learning and we're now basically ready to test it directly with Kovac 19 so this is really a stepwise process although it was extremely fast it was still stepwise we were guided by data where we had much more knowledge of which was with blood leukemia so we had worked on that for years our we had collected many data so we could really simulate the swarm learning very nicely and based on all the experience we gained together with Prasad and his team we could quickly then also by that nod to the data that are coming now from Kobe 19 actions so it really comes back to how we apply machine intelligence to the data and this is such an interesting use case I mean the United States we have 50 50 different states with 50 different policies different counties we certainly have differences around the world in terms of how people are approaching this pandemic and so the data is very rich and and varied to talk about that dynamic hmm yeah if you vote for the listeners who are reviewers who are new to this write the workflow could be you know a patient comes in you take the blood and you send it through an analysis and our DNA is made up of genes and our genes expressed right they express in two steps the first day transcribe then they translate what what we are analyzing is the middle step the transcription stage and there are tens of thousands of these you know transcripts that are produced after the analysis of the blood the the thing is can we find in the tens of thousands of items right of biomarkers a signature that that tells us this is Kovach 19 and how serious it is for discus for this patient now the data is enormous right for every patient and then you have a collection of patients in each hospitals that have a certain demographic or a certain you know and then you have also a number of hospitals around the question the point is how do you get to share all that data in order to have good training of your machine the issue is of course you know privacy of data right and as such how do you how do you then share that information if privacy restricts you from sharing are the data so in this case Swan learning only shares the learnings not a private patient data so we hope this approach would allow all the different hospitals to come together and unite sharing the learnings removing biases so that we have high accuracy in our prediction as well at the same time maintaining privacy so there was it's really well explained and I would like to add at least for the European Union that this is extremely important because you know the lawmakers have clearly stated in the government's that even under these crisis conditions they will not minimize the rules of privacy laws yeah but you know the compliance to privacy laws has to stay as high as outside of the pandemic and I think there's good reasons for that because if you if you lower the bar now why shouldn't you lower the bar in other in other times as well and I think that was a wise decision yes if you will see in the medical field how difficult it is to discuss you know how do we share the data fast enough I think swollen learning is really an amazing solution to that because this discussion is gone basically now we can discuss about how we do learning together but rather than discussing what would be a lengthy procedure to go towards sharing which is very difficult under the current privacy also I think that's why I was so excited when I learned about it the first place this we're faster we can do things that otherwise are even not possible or would take forever and for crisis this that's key that's absolutely key and there's my product it's it's also the fact that all the data stay where they are at the different hospitals with no movement yeah now learn locally or only share the learnings right very important in in the EU and of course even in the United States people are debating with about contact tracing and using technology and cell phones and smart phones to do that press I don't know what the situation is like in India but but nonetheless the doctor goes point about just sharing the learnings bubbling it up trickling just these kind of metadata if you will back down protects us but at the same time it allows us to iterate and improve the models and so that that's a key part of this the starting point and the conclusions that we draw from the models are gonna and we've seen this with the pandemic it changes daily certainly weekly but but even daily we continuously improve the conclusions and the models don't we absolutely as dr. GU explained well so we could look at like the decay we have the clinics or the testing centers which are done in the remote places available so we could collect those data at that time and then if we could run it through the transcriptome kind of a sequencing and as and when we learn through these new samples and the new pieces all of them would kind of have that in the local data participate in the kind of big news from learning not just within the state or in a country could participate into and so I'm learning globally to share all this data which is coming up in a new way and then also implement some kind of a continuous learning to pick up the new signals or the new insight which comes up with new set of data and also help to immediately deploy it back into the inferencing or into the practice of identification to do this I think one of the key things which we have realized these two making it very simple it's making it simple to convert the machine learning models into the swarm learning because we know there are subject matters experts who are going to develop these models on their choice of platforms and also making it simple to integrate into that complete machine learning workflow from the time of collecting a data pre-processing and then doing the model training and then putting it on to inferencing and looking performance so we have kept that in mind from the beginning while developing it so we kind of developed it as a pluggable micro-services kind of architecture with containers so the whole library could be given it as a container with a kind of a decentralized management command controls which would help you manage that the whole swarm Network and to start and initiate or ensure the enrollment of new hospitals or the new nodes into the swarm network at the same time we also looked into the the tasks of the data scientist and then tried to make it very very easy for them to take their existing models and convert that into the swarm learning framework so that they can convert or enable their models to participate in a decentralized learning so we have made it through a set of callable wrists api's and i could see that the example which we are working with the professor either in the case of leukemia how in the Kovach kind of the things the neural network model so we are kind of using the ten layer neural network things we could convert that into the swarm model with less than ten lines of code changes so that's kind of a simply three we are looking at so that it helps to make it quicker faster and lower rate the benefits so the exciting thing here dr. go is this is not an RD project this is something that you're actually implementing in a real world even though it's a narrow example but there are so many other examples that I'd love to talk about but but please you had a comment yes a key thing here is is that in addition to a allowing privacy to be kept at each hospital you also have the issue of different hospitals having data they're skewed differently right for example demographics could be that this hospital is seeing a lot more younger patients and other another Hospital seeing a lot more older patients right and and if you are doing machine learning in isolation then your machine might be better at recognizing the condition in the younger population but not order and vice versa by using this approach of suam learning we then have the biases removed so that both hospitals can detect for younger and older population right so this is an important point right the ability to remove biases here and you you can see biases in the different hospitals because of the the tower cases they see and the demographics now the other point is very important to re-emphasize is what prasad but has a choice mentioned right is how we made it very easy to implement this right they started out being a so for example each hospital has their own neural network in their training their own all you do is we come in as percent mention change a few lines of code in the original a machine learning model and now you're part of the collective swamp this is how we want to be easy to implement so that we can get again as I like to call you know hospitals of the world to you 90 yeah without airing private patient data so let's double click on that professor so to tell us about sort of your team how you're taking advantage of this doctor go just described sort of the simplicity but you know what are the skills that you need to take advantage of this what's your team look like yeah so we actually have a team that comes from from physicians to biologists many medical experts up to computational scientists so we have early on invested in having these interdisciplinary research teams so that we can actually span the whole spectrum so people know about the medicine they know about them you know they're the biological basics but they also know how to implement such new technologies or they are probably a little bit less we're heading that but this is this is the the way to go in the future and I see that with many institutions going this way many other groups are going into this direction because finally medicine understands that without computation of Sciences without artificial intelligence and machine learning we will not answer those questions with this large data that were using so I'm here find and and but I also realized them when we when we entered this project you know we had basically our model we had our machine learning model from the leukemias and it really took almost no efforts to get this into the swarm so we were really ready to go and then very short time what I also would like to say and then it goes towards the the bias that that is existing in medicine between different places doctor goes like this very nicely is is one one aspect is the patient and so on but also the the techniques how we do a clinical assays if we're using different robots a bit of using different automates to to do the analysis and if we actually tries to find out what small learning is doing if we actually provide such a bias by personnel so we we just I did the following thing we know that there's different ways of measuring these transcriptomes and we actually simulated that two hospitals had an older technology and a third was provide a much not newer technology which is is good for understanding the biology and and the diseases but it is the new technology is prone for not being able anymore to generate a that can be used to learn and then predict in the old technology so there was basically it's deteriorating if you do think the new one and you make a make a classifier model and you try old data it doesn't work anymore so that's a very hard challenge we knew it didn't work anymore in the old way so we pushed it into swarm learning and to swarm recognize that and it didn't take care of it cater take care of it didn't care anymore the results were better by bringing everything to better I was I was astonished yeah I mean it's absolutely amazing that although we knew about this limitations on that one hospital data this one basically could deal with it and I think there's more to learn about these advantages yeah and and I'm very excited it's not only transcriptome for people to I hope we can very soon do it with imaging but the DC Andy has ten sites in Germany connected to ten University of hospitals there's a lot of imaging data you know CT scans and MRIs radio grants and this is the next next domain in medicine that are that we would like to apply well it's very exciting being able to bring this to the clinical world and make it in sort of an ongoing learnings I mean you think about again coming back to the pandemic initially we thought putting people on ventilators was the right thing to do we learned okay maybe maybe not so much the efficacy of vaccines and other therapeutics it's going to be really interesting to to see how those play out you know my understanding is that the vaccines coming out of China more built to for speed get to market fast be interested in us maybe try to to build vaccines that are maybe more long-term effective let's see if that actually occurs some of those other biases and and and tests that we can do that is a very exciting continuous use case isn't it yeah I think so happy go ahead yes in fact we have another project on going to use transcriptome data and other data like metabolites and cytokines packet all these are biomarkers from the blood right off the volunteers during a clinical trial with the whole idea of looking at all those biomarkers we're talking tens of thousands of them same thing again and then see if we can streamline that clinical trials by looking at a data and training with that data so again here you go right we have very good that we have many vaccines on in candidates out there right now the next long pole in the tent is the clinical chart and we are working on that also by applying the same concept yeah but for clinical charts right and then Prasad you know it seems to me that this is a an example of a sort of an edge use case okay you've got a lot of distributed data and I know you've spoken in the past about the edge generally where data lives bringing moving data back back to sort of the centralized model but of course you don't want to move data if you don't have to real-time AI inferencing at the edge so what are you thinking in terms of other other edge use cases that where this swarm learning can be applied yeah that's a great point we could kind of look at this both in the medical and also in the other fields as we talked about professor just mentioned about these radiographs and then probably using this with the medical image data think of it a scenario in the future so if you could have an edge node sitting next to the these medical imaging systems very close to that and then as and when this the system's produces the medical images be it could be an x-ray or a CT scan or MRI scan type of things the system next to that sitting on the attach to that from the model it is already built to this form learning it can do the inferencing and also with the new set of data if it looks some kind of an outliers or sees the newer images or probably a new signals it could use that new data to initiate another round of his forum learning with all the involved or the other medical images across the globe so all this can happen without really sharing any of the raw data outside of that systems but just getting the imprinting and then trying to make all of these systems to come together and try to build a better model so we could the last question if I may we got a we got a wrap but I mean I first I think heard about swarm learning maybe read about it probably 30 years ago and then just ignore it and forgot about it and now here we are today blockchain of course first heard about with with Bitcoin and you're seeing all kinds of really interesting examples but but dr. go I'll start with you you know this is really an exciting area and we're just getting started where do you see you know swarm learning you know by let's say the end of the decade what are the possibilities yeah you could see this being applied in many other industries right I so we've spoken about life sciences to healthcare industry now you can imagine the scenario of manufacturing where a decade from now you have intelligent robots that can learn from looking at a craftsman or building a product and then replicate it right by just looking listening learning and imagine now you have multiple of these robots all sharing their learnings across boundaries right across state boundaries across country boundaries provided you allow that without having to share what we are seeing right they just need to they can they can share of what they have learnt see that's the difference without having to need to share what they see and hear they can share what they have learned across all the different robots around the world right or or in the community that you allow imagine that time right that well where even in manufacturing you get intelligent robots learning from each other and and professor I wonder if is a practitioner if you could sort of lay out your vision or where you see something like this going in the future I'll stay with the medical field at the moment being I although I agree it will be in many other areas you know medicine has two traditions for sure one is learning from each other so that's an old tradition in in medicine for thousands of years but what's interesting and that's even more in the modern times we have no tradition of sharing data it's actually no it's it's just not really inherent to medicine so and that's a you know that's the mindset so yes learning from each other is fine but sharing data is not so fine but a swamp learning deals with that you know we can still learn from each other we can you know help each other by learning and this time by machine learning we don't have to actually deal with the data are sharing anymore because that's kept this us so for me it's a really perfect situation that um medicine could benefit dramatically from that because it goes along the traditions and that's very often very important to get adapted and on top of that you know what also is not seen very well in medicine is that there's a hierarchy in the sense of certain institutions rule others and slow learning is exactly helping us there because it democratizes onboarding everybody and even if you're not a so you know it's a small entity or a small institution or small hospital you could become men when the swarm and you become as a man way important and there is no no central the institution that actually rules everything and that this democratization I really love I have to say we'll give you the final word I mean your job is really helping to apply these technologies to solve problems what's your what's your vision for this I think professor mentioned about one of the very key point to use saying that a democratization of VI I'd like to just expand a little bit so it has a very profound application so dr. bill mentioned about the the manufacturing so if you look at any field it would be Health Science manufacturing autonomous vehicles and those two the democratization and also using that blockchain we are kind of building a framework also to incentivize the people who own certain set of data and then bring the inside from the data into the table for doing and so I am learning so we could build some kind of also a monetization framework or a incentivization framework on top of the existing form learning stuff which we are working on to enable the participants to bring their data or insight and then get rewarded accordingly kind of a thing so if you look at eventually we could completely make it as a democratized AI with having the complete monetization incentivization system which is built into that to make all the parties to seamlessly work together so I think this is just a fabulous example we hear a lot in the media about you know the tech backlash breaking up big tech of bateau tech has disrupted our our lives but this is a great example of tech for good and responsible tech for good and if you think about this pandemic if there's one thing that it's taught us is that disruptions outside of technology pandemics or natural disasters or climate change etcetera are probably going to be the bigger disruptions than technology yet technology is going to help us solve those those problems and address those disruptions so gentlemen I really appreciate you coming on the cube and sharing the this great example and wish you best of luck in your endeavors thank you thank you and thank you everybody for watching this is the cubes coverage of HPE discover 20/20 the virtual experience we'll be right back right after this short break [Music] 