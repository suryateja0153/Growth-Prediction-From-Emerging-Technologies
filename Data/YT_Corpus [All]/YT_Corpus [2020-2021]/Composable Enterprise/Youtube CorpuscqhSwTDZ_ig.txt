 hello my name is Sean Sadler I'm here with Dilek as well to talk about composable data processing first an intro I'm a principal scientist at Adobe Systems and I've been working on the Adobe experience platform for about two years Dylan started more recently in 2019 as a architect and he's also a contributor to Apache spark so first we're here today to give a story about scalability not in terms of throughput but rather the output of our engineering team first we'll talk about the why and we'll provide some context around the Adobe experience platform or on our team and some of the problems we have faced talk about the what by walking through the CDP design then we'll get into the half dilip or talk about the nuts and bolts around the different data processing techniques that were using in CDP and finally we'll talk about the impact the CDP has had on our team on the platform and our customers so AP is a platform for driving real-time personalized experiences and we do that by making data actionable and you can see that in a diagram above first we have data which is about people places events and things within your domain then we have semantics of control that's David governance about controlling and managing your how your data is used third is intelligence and that's about custom or out-of-the-box machine learning models to enrich your data and finally we have action whether it's providing Kempe targeted AB campaigns or a better web experience it don't be a peep of IDEs applications to enable those experiences now how do we do that we have a suite of applications from Adobe Analytics audience manager campaign target experience manager Magento Marketo and more that's built instead of intelligent services on top of that well below that we have Adobe sensei and then we have the data that's collected through Adobe experience platform it's that last piece that both dilib and I operate at so we're both part of the data ingest team for ingesting data into Data Platform called Seiken and the Zen of Seiken is about enabling producers to send chunks of data called batches that we make available bit to downstream consumers now it's not all about moving bits around we also need to apply transformations to that data taking it from raw form into an optimized format like 4k it's also about validation we need to apply validations to a target schema which we call the experience data model it's also about partitioning ensuring that we can have an optimal access for different data access patterns compaction the support sources then have high frequency small files it's also providing an exactly once guarantee to make that data durable without duplication and finally lineage tracking so we can track the who what where and when of the data that's been produced now even with all that said at this point in time we're early on we're still processing about 1 million batches per day as 13 terabytes make weights to 32 billion events so our focus has been on cross-cutting features and what I mean by that are those features you were to find across any big data application as you would expect things like queuing and throttling to handles buycks and workload also scaling based on backlog and or utilization we also need to break across workloads across different bulkheads to support the cases of either having back pressure and/or poison pills that could bring the system down we support logging custom metrics like - for monitoring alerting and to notify - ping consumers that data has arrived and then there's ability to execute these distributed big data applications across at one time like beta bricks and finally resilience we need ensure that there's 1 million batches are taken to completion or that were alerted within a given SLA our focus has in 2019 has been on harding these features or minimum feature set but 2020 is changing or being asked for a lot more features things like supporting a number of additional wall in just four months outside of CSP j7k providing additional ETL transformation capabilities to go from raw data to XDM provide additional validation capabilities such as format checks range checks even contact specific checks based on where the data's were in the land we're being asked to provide additional diagnostic and reporting tools we're being asked to provide alternatives and how you can how and where you can write the data to and funding as many opportunities we have in terms of tracking managing and mining and providing autocorrection features around failed of bad data in the system now considering all these different concerns that we have we could have different sets of people different teams working on each of these concerns unfortunately it's only a single team working on these data processing features and that's syphon and we're stuck with this zero-sum game but we have more resources working on cross-cutting features if we move them to data processing we have to take away from the hardening work we're doing there unfortunately we have requirements coming on both sides of the scale the challenge we have is how do we increase the amount of date process and features that we have one means not compromising these stability of the system so we've looked at several options option a is a path of least resistance this is really just coding to it but it means that we need to deprioritize the hardening work that we're doing it also means that developers will be working both on those cross-cutting features and the data processing features results and context switching which is overhead we prefer not to incur overlapping those concerns can also lead to spaghetti code making it increasingly difficult to test over time and English link difficult to maintain over time as well therefore we moved on to also look at option B which is more of passing the buck right so rather than having a single pipeline we bring up to have multiple pipelines leaving this initial cyclin I went untouched however this leads to a lack of four years all those cost couldn't concerns that we had with cyclin now need to be the duplicated in each one of these services it's also leads to lack of consistency because e 20 services may have different SL A's different delivery guarantees or different validations this makes the end end testing more complex results in a more complex monitoring as well and complexin that will build up over time however since we have multiple steps in bulk here - and multiple output paths so we result in increased latency and increased cost so this led us to look at option C which is composable data processing we kind of want to have our cake and eat it too here so we ask yourself the question what if we allow these other teams to plug in to syphon this will allow us to scale our engineering effort module as our design code we can also provide a clear separation of responsibilities it'll be easier to test the system easier to maintain and it'll maximize reuse and by having a single step a single pass through the data in the single output we're going to end up benefiting with a minimal complexity minimizing latency and minimizing the cogs that's what we've done is we've taken that zero sum game and made it a win-win game this leads us to the what we're going to talk about more about the option C we need to implement a framework that enables different teams to extend siphons data ingest pipeline so at a high level what we need is raw data as input and we're showing that the left-hand side and a target schema which we're showing on the right-hand side the the intent of the framework is to transform that raw data into something that fits into that target schema so in what does make it over easy go to daily and what doesn't make it over needs to be quarantined so we need to do things like map game over the first name last name you can map the b-day over to the birth date of type date we need to map that level over to Rewards meddling which has an enum constraint to find on it this is gonna require multiple steps but the first stage is person we're going to take their law data as input and translate it into a raw data frame however not all the data makes it over and we can see that we have one record that fails it's because it's corrupt data it's missing an ending bracket we need to record that fact into a separate error data frame now it gets us the second stage conversion the input to this is going to be the raw data frame we want to translate that into the target data frame based on the target schema so you can see that the name has now split out to first name last name the birthday has has been mapped over to birth date over once again not all the data has made it over the second row with ID 3 didn't make because an invalid date/time string we also need to record that in the error data frame as well the third stage is validation so now the input here is going to be the target data frame but now we have some invalid data we need to also filter out we can see that it's the second row here with IV for its environment the reason is because of the rewards level field that has an enum constraint so that value needs to be one of bronze silver or gold so the fourth stage we now have the target data frame with the data we can simply save it to data like and in the fifth stage things get a little more interesting is we now have a set of arrow data frames that we need to Union together and we want to join that to their initial draw data frame write that data out supporting so that clients can diagnose issues and/or trip current data finally we've looked at all the components and the various responsibilities these are implemented within a spark driver running on top of data bricks I'm gonna pass over now to dilla to get more details around the plugin one time Thank You Sean for giving a very good high-level overview of the problem we're trying to solve so we'll now look at some of the implementation challenges in detail I will start by discussing the DSL that drives the execution of various plugins which are responsible for either converting or validating the data will go through the api's or the contracts between s IP and the plugins next we'll look at the processing of the three different kinds of errors the parsing the conversion and the validation errors will then go through a small illustration that shows how the three different kinds of errors are consolidated into a common error schema finally we look into the details of how these errors are generated and capture at a row level all right so siphon ingest plug-in framework for acip performs the task governed by a DSL we can also think of it as a set of dynamic configurations based on which s IP does all its actions so in this case we can see that the processing of the input CSV file goes through various stages first it is fed through a parcel then a set of converters and validators while capturing the errors in each of the stages at the end the surviving rows and the rows that are free of errors are returned to the beta sync and the cumulative errors are written out to the error sync the data sink and the error sinks are basically persistent stores like a DNS of Cosmos dB so in the interest of time I am NOT going to get into the details of these api's of interfaces but Tom will touch up on some of the key points so firstly both the converter and validated taken data frame as an input and return the results so converter returns both the success and the error data frames whereas the validator returns just the error data frame the reason for this difference is the output of the converter may be of different schemas and the input whereas the validator works on the data that is already converted that is the data is already aligned with the target schema I'll discuss this more later in the talk so with this background let us look at each of the error in detail first the parsing errors so one thing to note here is that the parsing error cell applicable only text based formats of the CSV and JSON and and only processed by a selfie at the beginning of the validation process because we want to get rid of the parsing errors as only means to initial process as possible here we completely rely on spark to detect the parsing errors as far as the compute engine that reads the beta so that let's take a look at some of the examples so so actually the gist we have two examples here one is for Jason once for CSV but the CSB case is actually a little more interesting because we need to pass some extra information to spark whereas the Jason case is pretty straightforward so let's focus on the CSV case in the interest of time so we have two rows the second row is the problematic row because it has an additional column that does not conform to the schema and the schema has two columns named a nails so in order to but so get the parsing errors we need to provide spark with the read schema here we see that we have added one extra column call underscore corrupt record which will be used by spa to store the past in error and in the read options we see that we have instructed spa to use the permissive mode of pursue and also we have supplied a the column name of the corrupt record so with this 2x time from a piece of information when we read this file a spot will produce a data frame that will look like the one in the right here we see that we have got an additional column underscore corrupt occurred which actually captures the parsing error with this data frame if we can apply a simple predicate on the corrupt record we can actually find out the bootleggers and the bad records so now to the convertor and very distant runners so once we have processed the parse errors the surviving rows are subjected to conversions and validations so here again by using the DSL as the input s IP determines the list of converter converters and validators it means to invoke and calls upon them in a sequence so I convert a plug-in collects and computes both the output rose and the error loss and only the good rose that pass to the next plug-in in sequence this is because if we do not convert a row to confirm to the target schema then there is not much point in examining that row any further but this is different in the case of validated plugin in that even though a row is rejected by one validator plugin it is still examined by the next validator plugin in the chain for us to be able to collect and report all the possible errors in the data this is to minimize the number of round trips the user has to make in order to get his data in the state into our system so with that let's look at a small illustration that shows how we consolidate these different kinds of errors so we start with the input data and we apply expression called monotonically increasing ideal which which is basically to assign a logical row identifier true for each of the row now let's imagine we have a conversion that looks like the one that is shown in the mappings table here we have a mapping rule which says that we have to concatenate the first name and the last name column and the target column is going to be full name so here I would like to say that this is very simple a simple example of how a converter may look like but in real life actually at different plugins or different components that Sean mentioned who actually implement their converters using their custom rules and mappings so once we apply this mapping on this input data we see that we produce two data frames one is a success data frame and one is the error data frame we see that the row Laker with row ID 2 is rejected because the last name field is null and that is not accepted in our system in this example and the success data frame has two rows the row ID 1 and no ID 3 but we do see a problem with the second row because ace column is negative so but that is not a concern of the converter at this point we will see how validator process is this low so now we take the output of the converter the good rows and we apply a target stream on top of it in the target schema we have defined a constraint which says that the ace has to be positive so when we apply this target schema on this data we see okay we see that we have a error data frame and the row the record with row ID 3 is rejected because ace cannot be negative now at this point we have both the conversion and validation errors so we could at this point apply a simple relational Union operation to combine these two errors so one thing here to note is that just like we standardized on the API and the interfaces that we talked about earlier we also have standardize on the schema of the error that means all the plugins and the components actually return the errors using the same schema therefore we could combine this two data frames using a union operation so we have now the cumulative errors and we see that if we just apply two basic joint operations we can compute the final error data frame and the final success rate of frame when we take the original set of rows where we applied the where we generated the row identifier we take those rows and we join that week the cumulative error data frame on the join column being the low identifier it actually produces the final error data frame we can see that the final error data frame contains all the input columns as they were sent from the user because the data that the user sends to us may not have an actual row identifier by sending all the input columns of the input column values it hopefully provides enough context to our users to identify the rows that are problematic and fix them and be same so to generate the success data frame we take the output data frame the after the success data frame that we got from a converter and we do anti join why do we do anti joy because we see the second row which was which was having AIDS column and they make negative AIDS column actually was rejected by the subsequent validator and we must exclude that rope from our output that's why we need to do anti join between between the success data from from converter with the cumulative error data frame on the same join ID column that is row identifier to get final success data frame so once we have these two data frames like Shawn mentioned we could write them into a configurable sinks any error sinks so at this point we have looked at the three different kinds of errors and how they are consolidated and how they they are basically how do we compute the success and the error data frame but how how does this the error get generated in the first place so most of our most of our existing conversin and validations actually make use of UDF's so prior to introducing this plugin architecture we use to do data validations and data commercials but we used to work in a fail first mode that me when the first error occur we used to fail ingestion and report the very first error to the users and that is what we are trying to solve as part of this work so so we need we may need a way to be able to trap these errors at runtime and proceed to examine the next row in the sequence so and the second thing is that our data was heavily it was based on heavily nested type I mean mistake types therefore we have to write the nested UDF's and currently it is not possible to capture the errors from the nested Williams the way we want that's why we introduce this custom expressions that help us to trap the errors now let's look at how these custom expressions look like so it's a simple expression as we can see that extends extends from unary expression which means this expression has got one child which is a expression itself and then second thing is that the output of this expression is actually a struct type and it has input column output column and the inner information input column is nothing but the in the input that actually is passed to the child expression input to the child expression and then the output column is actually output of the child expression and the inner information obviously is whatever the errors that happen during conversion and validation with that okay so now let us look at the evaluate method which is again very straightforward we call upon the child evaluate method but we loved that in a try-catch block for us to be able to track the errors so we when we see error we actually save it in these variables error message and error name and finally we wrap them the result in internal row and send it as a return value of this expression okay now let's take an example and look take an example and see we have an input data frame which has two rows and two columns name and AIDS so we print it out we have a problem with the second row we are going to see how we are going to trap the error so we take a small UDF which simulates how a validation UDF may look like and that takes an integer as input and it checks if the value is negative then it through the exception and for us to be able to show the output value or output column of this error typing institution I multiply the input is by two okay here we we create this custom expression and we can see that it's child is actually this validating UDF which is as UDF and now when we map the a scholar and we attach this X new agree created expression with the ACE color by calling the width column API and then execute this data frame we see that that the errors are actually trapped in the second row the first row we see that we see both the input and the output column input being 30 and output is 6 typically multiplied by 2 but the error information she's actually null because we could successfully convert this law whereas the second row we had a problem converting therefore we don't have a output column value which is null and but we do have the inner information and then I cream the schema to just illustrate that the aged columns type actually has changed from in to struct and under the stock we we can we store all this input output and error error information so with a data frame schema the way we saw we could easily compute the error and the success data frame to find out the good rows all we have to do is to look for rows where edge dot error code is none similarly to find the bad rows all we have to do is to look for rows where s dot error code is not null so that's how we we trap the errors and we filter out the good rose and from bad rows with that I am going to hand it over to Shawn to share his final thoughts Thank You Shawn so remember that zero-sum game we had a trade-off between data processing features and stability that's no longer a problem with CDP we now have other teams three other teams in fact that are contributing not just code but components that can enhance the plug-in framework per cycle and this leads to many benefits for us on the syphon team we have a scalable engineering we have a clear separation of responsibilities between those working on cross-cutting concerns us and other teams that are now working on data prep and data validation we have more readable code at the end more testable code and it's easier to maintain if we look at the benefits for platform we actually need to take a step back because we initially had taken that option B meaning passing the buck or we had multiple connected pipelines but the thing we did notice is because we had a pass over the data in multiple times we had an increase in latency we also had a significant amount storage cost and compute cost it just wasn't tenable with CDP we've solved that problem and for our customers what it means is that we can deliver more features faster with ETL ETL for validation for every every reporting more importantly the overall experience for the average file size that we have going through these more complicated slows we're down from minutes to seconds in execution so this does end our session but we hope you get out of it is two things first think about extensibility in your software project and how it can help you scale your project and your development second also leverage some of the the data processing concepts and techniques and still talk about in terms of how you deal with failed data thank you very much you 