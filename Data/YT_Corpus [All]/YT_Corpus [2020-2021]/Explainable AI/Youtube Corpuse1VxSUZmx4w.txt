 hi my name is craig and i'm the director of product management for cloud ai platform here at google cloud platform today i'm going to be talking to you about how to create value with the breadth and depth of our ai platform creating value with the ai platform simply comes down to putting the best of google's ai technologies to work for you and for your company what does this mean fundamentally this means how can we map your business strategy to the right ai absorption strategy we'll talk about the different ways that google helps you deploy ai without compromising on flexibility speed quality or scale and finally we'll talk about some of the customers that have transformed their businesses by tapping into the innovation from google as i think about ai fundamentally ai only does two things one it helps you grow your market increase subscribership increase users increase their spend or increase their conversion or it helps you in the back end drive efficiencies reduce costs drive out waste from the system together these two capabilities allow you to achieve previously unimaginable results for you and for your company using this critical technology ai is not limited to this business or that industry opportunities to use ai can be found in every vertical whether it's retail our capabilities in health care financial services things like lending ai or anti-money laundering media and entertainment or industrial manufacturing with things like visual inspection capabilities or public sector all of these are rich with opportunities all of these are areas that we're helping work on with ai as you think about aia investments and ai absorption when you think about what google offers you can really break it down into three levels of investment the first is ai out of the box this is very low investment very quick to market but doesn't come with a lot of customization the second is deploying custom ai this on the other hand does allow for customization takes a bit more effort but is still very quick to market and lastly is building end-to-end ai this is when you and the data scientists you work with invest to build your own bespoke models for areas where this will be critical to your company's strategic innovation or your differentiation in the market we can think about these three levels with this example the first example of kind of the ai right out of the box would be something like optical character recognition you could imagine simply calling our vision api to quickly and easily read the documents that you may have second would be something like potentially content classification this is an area where the new york times invested with us on using automl to quickly and easily classify their entire content library and then finally we come to actual pure insight generation deep investments that provide unique benefit this can be seen wonderfully with kaggle an organization that we run that has competitions for data scientists and the work they've been doing seeking to extract critical findings from the academic literature around covalent sars viruses these three levels of of understanding can be seen in our products as well at the first layer we see pre-trained apis these require no real work from you to all you need to do is integrate with the api no data science is needed from you instead you can simply get your image labeled get your document understood uh you know do entity extraction or sentiment analysis on your document or in this case you can see using pre-trained video apis to track movement or track an object across frames second would be custom ai with automl with this capability say our models don't have the objects you need labeled you want specific fish labeled not just that's a fish but you want to know what species it is you can take your own photos quickly and easily label them with our data labeling service and then move them in and train your own ai model with no code having had been written and finally for those more powerful models or more interesting and bespoke use cases we offer the full end-to-end capability with our core tools you can think of it this way across the top of the stack we have our applications vision and video conversation language and structured data so whether it's labeling an image or working with video whether it's speech to text text to speech or setting up a conversational bot whether it's translation or entity extraction or sentiment analysis you have all of your needs with our language capabilities and finally with structured data you can build models on your structured data without python or any sort of coding required then beneath those applications we have the core tools designed to help the data scientists build the powerful models they want to both explore and build whether it's notebooks as a service data labeling training as a service prediction as a service all of the critical components you'll need to ensure that you're building the best ai we're excited to announce some new capabilities coming for example later this year we'll have new improved workflows for automl within the console a fully integrated experience no longer requiring you to make specific decisions and choices but instead making it all easy right out of the box you can see that with our stack you can start by creating a data set and then quickly and easily with no code required move all the way through to a model deployment putting your model into production ready to be called in a highly auto scaling environment now for those that wish to do something more sophisticated or who wish to build their own model they'll have the full capability to do so as well as we move beyond automl one primary area investment for us has been machine learning ops or ml ops in this area nothing has been as exciting to us as what we've seen customers do with our managed pipelines using those managed pipelines as well as a feature store continuous monitoring of models that are in production and metadata to help make decisions customers will now be able to support both a fully ci cd capability as well as to start creating pipelines that will support much more sophisticated operations such as quickly and easily retraining your model once skew or drift is detected these pipelines what they do is they allow you to take your monolith of of code that you've used to build this model and break it down into each of the individual components now it's much more easily debugged and understood as well as these components can be saved for future model usage and in the future when you are building a model instead of starting with a blinking cursor you can simply go in pipelines and start pulling the most commonly used pipelines down into your model to use for things like extracting data validating data or evaluating the model these capabilities significantly reduce the time of experimentation we had one customer telling us that they have increased their rate of experimentation by 7x using this technology we couldn't be more excited to see how your company will use this finally as is critical to any service of this type metadata management is is a key piece of what we we believe is needed to effectively run a mlaps environment whether that's automatic metadata logging from our hosted services or custom metrics that you may want to devise both are quickly and easily captured and then can be used for things like compliance ml ops continuous training and continuous delivery or experimentation development model and artifact comparisons as well as improved ml reproducibility and explainability the compliance story here is an interesting one it allows you to do previously very difficult tasks things like imagine that if a customer said hey please remove me from your records all you would have to do is remove the customer from the data set in question or from all of the data sets that they're in and then with pipelines and ml metadata you could very quickly and easily auto retrain just just by determining the fact that the data set hadn't changed automatically retrained the model and redeployed the model or without a human ever having to write code to do so we're proud of our ai platform because our ai platform does three things really well one it meets our users where they are two it quickly deploys the highest quality ai and three it allows you to scale and streamline that ai in unbelievable ways meeting customers where they are what does that mean well as you can see here i think we all fall somewhere on the spectrum of skills and capabilities within machine learning and whether you're someone who's just starting out or whether you're someone who's already comfortable with the most sophisticated tooling we have solutions for you when we talk about meeting customers where they are there's two companies that are interesting to call out the first is cruz cruz is an autonomous vehicle company making tremendous strides in moving to a world where my son will never have to get his driver's license with this cruise builds models and we have worked closely with them on optimizing their environments and optimizing their tensorflow capabilities to decrease their training times so that they can more quickly and more easily cycle through experiments and move towards a world in which cars are fully autonomous now on the other side is the city of memphis the city of memphis not traditionally known as a tremendous machine learning powerhouse but in this case they were actually building a model very similar to ones that cruz has no doubt had to build the city of memphis wanted to understand where they had the most problematic potholes so they used automl to build a pothole detector and sure enough we're able to save twenty thousand dollars a year in the reduction in claims costs due to these unaddressed potholes as you can imagine these two customers come with very different skills and expertise and both were able to find tremendous value with google and its platform we do seek as i said to meet customers where they are so for example our ai notebooks not only do they come pre-prepared for the use of classic or traditional ml frameworks such as tensorflow or pi torch but they also have data analytics capabilities so that you can get a big data image with things like apache beam pre-installed so that you can quickly and easily manage your own data proc cluster explainability is at the heart of understanding our machine learning we believe heavily and have invested heavily in this space as you can see at the top of the slide are images of explainability showing what pixels in particular the model may have used to render its judgment down below on the slide are is a picture of feature importance from a tabular use case both of these provide tremendous value to data scientists and both understanding them why the model is doing what it's doing as well as ensuring that the model is doing what it's supposed to be doing we don't think of explainability and explainable ai as some separate field that you know can only be used by the experts we believe it's central and as such we include it with every prediction in our prediction service we simply publish all of the explainability results in the same blogs we publish your inferences so that you can go back and look to see if you're experiencing unexpected drift or skew so that you'll know when it's time to retrain the model not only do we like to meet customers where they are but we ensure that we can quickly deploy the highest quality ai for easy business outcomes this might be in the area of site language conversation or with structured data in the area of site we're excited to work with box box is using our vision api to help their customers manage and gain insights from their image files and speed up image centric processes and workflows whether it's the work that box is doing or whether it's image and video search or industrial inspection our vision capabilities are unparalleled in the market next would be translation natural language capabilities whether it's entity extraction document classification sentiment analysis or in this case translation bloomberg is a company that sells information and for bloomberg to be able to quickly and easily sell that information in whatever language their customer speaks is a critical capability you can imagine how breaking down the divide of languages might be beneficial to your business next with speech voice and conversational bots whether it's speech to text text-to-speech video transcription voice commands all of this is made possible with these capabilities and we're excited about the work we've done with domino's using dialog flow so that they could meet all of their goals and milestones as they quickly and easily deployed a model to allow customers to quickly and easily order through the use of a conversational bot finally it comes down to structured data structured data is what many of our businesses run on whether it's in our data warehouse in the business systems we may have running now usually when we talk about machine learning we talk about very complex and kind of in very important use cases sometimes though it's okay for ai to be fun and this is one of those models we partnered with fox sports and they used our automl tables capability our structured data capability to build a model that would predict when the next wicket would fall in cricket now for those who aren't cricket fans or for those who are this you know may not seem all that exciting but i will say that they increased their app usage they also increased viewership as well as reduced a number of critical cost metrics and customer attainment because customers were so interested and curious now all of a sudden when i'm watching i can simply pay attention to the part of the match that i know is going to be relevant or i may simply want to tune in to see how well the ai actually predicts the outcomes finally the ability to scale and streamline with ease this fundamentally comes down to google passing over many of its well-built systems so that you can use them in your environment so that you have the same power of scale that google brings to the table nothing shows this better probably than etsy who when joining gcp realized that they wanted to sort through and index over 40 billion images working with us we were able to help them increase efficiency by 15 in doing this as i mentioned before ai notebooks are a critical way that many of our users interact with our services i can think back to the days when managing a machine learning team and how many of our data scientists at the end of the day would pack their laptops into their backpack and head home for the evening with their notebooks saved locally and probably a significant amount of data saved locally with our cloud notebooks available to you that's never again a concern our cloud notebooks come with tremendous levels of security including data exfiltration capabilities so that you know that both your code and your data is safe when your employees are using cloud ai and our ai platform notebooks finally uh our ai platform prediction service our ai platform prediction service allows you access to some of the most cutting edge hardware things like nvidia's t4 gpus it automatically logs all of your inferences the incoming payload of information as well as those explanatory variables and we log all of that in bigquery so that you have quick and easy access to doing some of the analysis on it finally it scales automatically with traffic all you have to do is tell us what kind of processor you want running it and how high up you would want us to scale if the traffic comes and as the traffic comes we will automatically add those resources so that you're only paying when you actually are seeing that traffic as i mentioned before ai platform pipelines is a transformational technology for many teams seeing a 7x increase in the rate of experimentation all of a sudden makes problems that were previously unsolvable solvable now all of a sudden work that would have taken me a week may take me a day or work that may have taken me months may take me only a week with the power of that kind of velocity increase you can only imagine the capabilities that your machine learning or data science team may be able to affect tensorflow enterprise tensorflow enterprise is uniquely available on google cloud tensorflow enterprise does three things first of all it gives you cloud scale performance with highly optimized binaries to allow things like increased i o speeds so that you can see improved training or prediction also it's used in all of our managed services so you know that if you're using training as a service or inference as a service you're protected by tensorflow enterprise i'd say protected because the third aspect is support many of us have many have concerns about using open source in production can you trust that it will be well supported with security patches and bug fixes we're excited to say that with tensorflow enterprise we extend the one year of support that tensorflow gives to three full years of support so that you your i.t department and your business can be comfortable ensuring that the that this software is fully protected from security and bug fixes for the full duration of the life of your model all of this comes back into the ai platform whether it's the applications for vision conversation language and structured data or whether it's the core services of notebook as a service or training as a service or pipelines that you may want to use all of it we are excited to partner with you to build models that go beyond your expectations finally to wrap it all up first map your business objectives to a relevant ai strategy don't ask how this technology can help you figure out what you need help with and then let's figure out the right technology to use second partner with a company that has tremendous experience deploying ai at scale we have learned from our mistakes we have learned them already you don't need to relearn them finally plan for the end-to-end ai life cycle beyond just the model training by using pipelines you have the capability to create a full ci cd system a system that will relearn as soon as the model starts to become stale or starts to give irrelevant results google has been called out by a number of organizations for our leadership and machine learning you can see here in forrester's wave they're for their computer vision platforms that they've named google as a leader similarly in gartner's magic quadrant cloud ai's developer services magic quadrant google was named a leader in 2020. with awards across the industry we are very proud of what we have built and very excited to work with customers like you on building and deploying new and exciting models with that i would like to allow one of our customers to speak for a moment on how they've unlocked the value of ai with google cloud cardinal health has about 50 000 employees across 46 countries our mission is to be essential to health care we provide products and services so that the healthcare providers can focus on their patients the industry has evolved over the last five to ten years the world is basically moving to personalization machine learning and artificial intelligence and we really just had a great connection culturally work ethic and also some co-innovation with google cloud cardinal health's first machine learning project went live in google cloud every day we have 2500 emails coming in from the customer through our sap crm system with different messages different intentions so we have to send it to the human routers a group of 40 people who are sorting and categorize the email before we send it to the agents to perform the tasks that is a very labor-intensive process we leveraged google cloud native services and created ml model that automatically sorted our emails we used the human categorized emails so we learned how the categorization can be done in machine learning and then we deployed our model into google cloud the people that we freed up through the machine learning they move on to do the actual work rather than sorting the emails we are very happy about this project because it's a milestone for us we proved that it's achievable and it's really improved customer service our ai platform of choice is google cloud we're very lucky to get to work with cardinal health and we're very lucky to see some of the solutions that they have been able to deploy with that here are some resources if you're interested and would like to learn more about our ai products or about our build and use ai capabilities thank you for your time today and i look forward to meeting you and hopefully working with you on deploying exciting ai for your business 