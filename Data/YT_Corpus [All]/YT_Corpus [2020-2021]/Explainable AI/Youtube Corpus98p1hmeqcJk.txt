 hi everybody welcome to responsible ai from theory to practice my name is tracy frey and i'm the director of product strategy and operations for cloud ai and industry solutions to start us off i want to root us all in this statement in less than 10 years ai will be the number one driver of global gdp growth this graph shows that growth through 2030 13 trillion dollars and not only that organizations that achieve ai absorption will be the leaders of the global economy in that time that growth has already started people around the world have become everyday ai users and soon technology that isn't ai enabled will feel like something is missing think of technologies that we didn't have 10 years ago direct payment applications like venmo virtual home assistance our reliance on ai has already begun and it will only become more pronounced to 2012 ai results closely tracked moore's law with compute doubling every two years post 2012 compute has been doubling every 3.4 months for example vision ai has gotten more accurate and more powerful take a look at the error rate progression here from 2011 to 2020. while this is undoubtedly good news for innovation it also requires more care and attention from a responsibility standpoint a more accurate and powerful vision ai technology when used in a harmful way can lead to harmful and intentional misuse unintentional failure modes loss of personal privacy contributing to severe real-life consequences for individuals all at incredible speed and scale ccs insights reports that trusting ai systems just like the vision ai is the biggest barrier to adoption moving from the lab to production for enterprises there's a perception that there is broadly a deficit in the areas of ethical diligence over these powerful technologies we hear a lot of questions on responsible ai once organizations start to think about moving to production and responsible ai is impacting ai adoption and as a team we are seeing more and more companies come to us asking for help because they are facing challenges there's a lack of trust in the ai systems themselves and when ethical issues rise which 90 of organizations have encountered 40 of companies abandon instead of solving for those issues these numbers show a huge risk in not taking responsibility into consideration it's not possible for any technology to be morally or values neutral values are interpolated at every stage of ai development and each technology is developed in order to do something which expresses a value decision that reveals a huge barrier to ai adoption and it's a key driver why one study found these reasons for ethical issues within organizations governance is reported to be the biggest enterprise requirement in ai in 2020 not only will enterprises embrace governance we can expect increasing legislation potential regulatory issues and consumers will begin to judge and select organizations based on these policies gartner reports that as consumers become more aware and savvy about how organizations are using their data and organizations are using increasing amounts of ai and ml to drive business decisions enterprises must embrace ideas like explainable ai and transparent data policies for both ethical and business reasons customers will begin to judge and select organizations based on these policies by 2020 gartner expects companies that are digitally trustworthy will generate 20 more online profit than those who are not this all fits into our belief at google that responsible ai equals successful ai we recognize that the decisions we make and the way we develop technology will have a lasting impact the efficiency gains and competitive advantage are only going to be fully realized in a world where ai is responsibly developed and trusted by end users if anywhere trust gets broken we will see ai getting pulled back so to us responsible ai really leads to successful ai to build responsible ai into our dna we begin asking ourselves a few questions this is rooted in the idea that people and businesses with the best of intentions can potentially cause unintended harm and we need to understand the risks and build out processes to avoid those outcomes for us at google as you might relate we feel an immense responsibility to develop systems to answer these critical questions and build in the proper principles practices and programs in support of responsible ai development to ensure that the ai we build and enable for the world is successful while we've come a long way this work is never finished it is far from perfected and we approach this work from a humble place we are constantly iterating and assessing our work and progress but don't pretend to have all of the answers our goal today is to share with you how we've approached building responsibility in by design into our products but more importantly our organization to support the necessary inquiry deliberation and culture to build products aligned with our values we'll focus mainly on google cloud because that's where we sit and because it's arguably one of the more complex areas to align we have a range of general purpose purpose tools like auto mls and apis all the way to solutions that go much farther up the stack like contact center ai the way these technologies can be used can be quite varied and hard to predict which makes it a good place to discuss putting principles into practice what grounds our approach to responsible ai at google are our ai principles ethical ai responsible ai and all of the associated terms can feel amorphous or nebulous how do you begin to wrangle all of the often contradictory ethical theories and critical components of ai responsibility into a systematic way to apply them to your development for us as for many organizations now we found that the most important first step was developing our company-wide ai principles in the summer of 2017. so in 2018 we published our ai principles these are not just a marketing campaign for us they are the constitution we use to build our ai products conduct research and draft our policies our set of ai principles has been one of the most important things we have implemented they keep us motivated by a common purpose succinctly communicate our values in developing advanced technology for the world and ensure that the way we use artificial intelligence is in the best interest of humanity we believe that organizations and communities flourish best when in addition to our own personal ethical values there are also shared ethical commitments within the community that each person can play a part in fulfilling so that our work is more about doing something good together not doing whatever we privately think is best and just assuming or hoping that others values and choices will magically mirror our own but as we know a plan on paper like these principles and practices are only effective if they're operationalized looking at a list of principles like ours can be hard to incorporate into our day-to-day work as we deal with detailed engagements of our products what remains true is that our ai principles aren't magic they don't immediately answer all of our questions on how to move forward they don't relinquish us from having hard conversations they are a starting point in establishing what we stand for and what we can agree to build toward the work that has ensured ensued beyond the publication of the principles has been to interpret and apply them in practice at the end of the day if something goes wrong with the technology and you have principles like ours and you can't prove what you did to prevent that then that is a terrible outcome we realized very quickly that in order to be true to the principles we needed to develop internal governance processes that put our principles into practice in a systematic and repeatable way this includes building responsible ai governance capacity in the form of product and customer deal review committees educational programs a partner program and trainings and how we consult and work with our customers we'll touch on each of these beginning with google's ai review processes the original hope was that the principles would be a decision guide but in reality they can't because the company has such a wide variety of the ways it uses ai the reality is that you can take any baseline technology and it can be beneficial when used in one way and harmful in another so governing ai is not only about the technology but it's the combination of the technology the use case the training data the societal context in which it operates and how it's deployed in production there's no world where blanket statements can hold water if you're going to align the principles and it requires that people in the organization are committed to the complexity so we've built a google-wide ecosystem around how we operationalize we have a central google function that consists of a senior executive council which is supported by a responsible innovation team that team is chartered with helping teams around google in every product area learn to operationalize the principles for themselves when we in cloud initially thought about how to begin putting our ai principles into practice and this happened before the principles were published our first instinct like many uh is that we all all we had to do was create a big decision tree and a checklist for what's okay and what's not based on the principles we established our ai principles are intentionally broad and google is a let a thousand flowers bloom kind of place and we have so much breadth in the various product offerings that it would be impossible to have a checklist or decision tree at the level of detail that everyone would agree with and i guarantee you that's the case for you as well checklists feel comforting but in practice have been ineffective at addressing governance processes for such nascent technologies in order to provide rigorous evaluations but flexibly allow for the new technologies to take shape in google cloud we've created two separate review bodies that are connected but purposefully distinct the first review body covers early stage customer engagements where we are engaging in custom ai or advanced technology with the customer or partner this group is a decision making group and it is very high throughput and its intention is to very quickly give an answer to the field team as to whether a project can move forward is not aligned with the principles and won't move forward or if it will move forward with some conditions we've seen a range of those decisions over the two plus years that we've been doing this the next half of the process is how we build and develop our products when it comes to the ai products we develop at google we undertake rigorous and deep ethical analyses risk and opportunity assessments across each principle and we create alignment proposals that we bring to those reviews where we often have very uncomfortable but extraordinarily inspiring conversations with a healthy product pipeline we aim for in-depth reviews every two weeks relatively early on in the product development life cycle which is essential in advance of the review meeting a member of the team takes on the task of doing an initial evaluation of the product working hand in hand with the technical teams to understand the product in depth in order to assess the principal's implications and recommend mitigations to build into the plans for the product this document serves as the basis for the discussion come review time this meeting of the minds is one of the most important steps in any principles evaluation to allow us to do two things issue spot as a team and come to outcomes and paths forward for products that are not just good in theory but are able to be actualized in practice over time these conversations have been effective at normalizing conversations about risky technology and potentially adverse outcomes that we can then prevent the next question i'm sure you're wondering is what do those paths forward look like the path forward can look like a lot of different things it can be that we more narrowly scope the purpose of the technology in some cases this can mean we make a decision not to launch a product as a general purpose api or without an allow list as you may have seen we decided in 2018 that we wouldn't make facial recognition available as a general purpose api even though the technology exists we won't make it available for anyone to use and we found a path forward for the technology by scoping it to a narrow celebrity recognition offering which we'll talk about in a bit other mitigations can look like identifying educational materials or best practices to package up with a product and launch it this has taken the shape of launching the product with the associated model card a project we took on to organize the essential facts of a machine learning model in a structured way or launching more specific implementation guides like our inclusive ml guide embedded in our product uis from there we create an alignment plan with the mitigations discussed in the meeting which then goes through sign off by our product and engineering leadership it's key to note that not all paths forward involve tech solutions or fixes because ethics problems are not always the result of technology lapses but are actually more a product of the societal context in which it's to being deployed and how it's used as intended over time through this review process we identify patterns similar to case law where we are creating a growing set of precedents and best practices on sensitive topics to inform decisions and drive consistency on whether a new product can go ahead and with what mitigations there's no one size fits all approach flexibility is important and it's not just a point in time as that societal context changes then we have to respond differently as we make our decisions going forward here are some of the imperatives and challenges we've learned over those past two years one thing that i really want to stress is the importance of a diverse review committee members are intentionally multi-level we have folks who are just starting their careers out of college all the way up through vps we're drawing on a range of experts from a diversity of backgrounds both technical and non-technical multiple team members including myself have deep social science and philosophy and ethics backgrounds and we are really intending to be cross-functional across every role that exists at google for the most part and that we are not just drawing from people within cloud but that we are drawing from expertise across google for each review we also seek out additional expertise whether that be on issues of machine learning fairness and robustness or ethical ai or a particular use case or technology or domain or industry we're really lucky that at google we have a lot of that expertise in-house and when we don't have it in our walls we seek it externally one of the other most important things we did in the beginning is we had the great opportunity to hire a visiting researcher shannon ballard who was a tech ethicist and world-renowned and she worked with us to help us build this process and engage in our reviews in a way that was incredibly useful for us so what we've learned we've learned that constitutions like our ai principles require interpretation predicting misuse helps develop guardrails this requires us to imagine what if and explore all the possible scenarios and areas of misuse in order to develop a comprehensive set of guardrails our principles aren't magic they don't answer the tough questions they're a framework for those conversations deliberation and healthy disagreement is key we do not hold our team as an ivory tower full of moral heroes but working collaboratively within our technical teams to service potential issues and remediate them together ethical ai is not only about the extremes and it's not always about ai for social good and it shouldn't be every ai product regardless of how innocuous it may seem should be put through principles evaluations here are some of the trainings we offer googlers to build responsibly ai into our culture and equip teams with the tools and knowledge to insert responsible ai into their day-to-day work some of these are available externally as well our aim is to collect connect and collaborate with partners across the industry we want to share our learnings learn hear fresh and varied perspectives and advance our collective thinking when it comes to responsible ai the final way in which we put our principles into practice is through a growing set of tools responsible ai tools and frameworks are becoming an increasingly important and effective way to explain and understand our ai models tools get at the questions how do i understand the predictions made by ai how do i know if my data or model or outcomes have unfair bias and how do i know what is influencing my model's predictions building tools and infrastructure to evaluate and improve concerns at scale today we'll touch on four tools we offer that get at some of the main concerns facing ai today fairness indicators what-if tool model cards and explainable ai we often get asked about de-biasing models and how we at google think about ml fairness broadly speaking there is no standard definition of fairness whether decisions are being made by humans or machines identifying appropriate fairness criteria for a system requires accounting for user experience cultural social historical political legal and ethical considerations several of which may have trade-offs i also want to stress that there is no future in which we will be able to say in my opinion that we have achieved fairness that we can say we're done with this work and we can move on this is something that we engage in for the long term and we are always learning more and evolving and we'll show you some examples of that in a bit so why does google care about these problems well our users are diverse and it's important that we provide an experience that works equally well across all of our users the good news is that we have the power to approach these problems differently and to create technology that is fairer and more inclusive for more people we'll start with an overview of what that means and i should also say that this is an area we are still learning about and always will be addressing fairness and ai is an active area of research at google and beyond from fostering a diverse workforce that embodies critical knowledge from training models to remove and correct problematic biases knowing that there is no standard definition of fairness presents both an opportunity and a challenge one thing that i also think is really important that we've built into our review process is a clear recognition that we may not in our group have the appropriate lived experience that can help inform how we build a product we'll show you some examples of this in a bit but it's important that we recognize and that i recognize and stress for our team that we are coming from a place of great privilege and to honor that as much as we can this requires creating a space that is rich with psychological safety and i cannot stress enough the importance of doing that work an important step on this path is acknowledging that humans are at the center of technology design in addition to being impacted by it and humans have not always made product design decisions that are in line with the needs of everyone until 2011 automobile manufacturers were not required to use crash test dummies that represented female bodies as a result women were 47 more likely to be seriously injured in a car accident than men band-aids have long been manufactured in a single color a soft pink my skin tone in this tweet you see the personal experience of an individual using a band-aid that matches his skin tone for the first time i will never forget i had a third grade assembly where somebody came a father of one of my classmates and told us pointed out that band-aids were made for white skin tones sober crayons so were hosiery that was what the nude color was and once you know this you can't unsee it it changes you forever and this is why surfacing these issues is so important a product that's designed and intended for widespread use shouldn't fail for an individual because of something they can't change about themselves products and technology should work for everyone the choices made in those two examples reinforce the importance of being thoughtful about technology design and the impact it might have on humans as in the case with those two examples humans are at the center of machine learning development and fairness concerns can easily become embedded into ai product design decisions so how can we approach technologies to make them more inclusive and fair unfairness can enter into the system at any point in the ml pipeline here from data collection and handling to model training to end use in this typical view of an ml pipeline you collect data you label the data you train the model using certain objectives the data is packaged and put into a product and users see the effect and engage the way users engage informs more data collection within that pipeline the way we sample data the way we label it how the model was trained and whether or not the objective leaves out a particular set of users how it was packaged and what kinds of unconscious biases our users bring at engagement and how that affects how they behave and thus how we collect more data can all work together to create unfair biased systems rarely can you identify a single cause of or a single solution to these problems far more often various causes interact in ml systems to produce problematic outcomes and a range of solutions is needed the work of ml fairness is to disentangle these root causes and interactions and find ways forward what's important is getting clear on the questions we need to answer there are a smattering of questions that can be used to guide your fairness work across the pipeline everything from upfront product decision which is often underspecified in a lot of cases really defining the problem is all the way to identifying if and when a model isn't behaving correctly we want to know why did the model fail is the model trustworthy what are the limitations we found these questions to be helpful in guiding our investigations underlying all of this is making sure the workforce is inclusive the best way to make sure as we that we as teams are equipped is to have a diverse workforce having these conversations back to our pipeline at each stage of this life cycle we have identified tools best practices and frameworks to help ml practitioners in their efforts to build responsible ai at the start we are looking at problem definitions and the resources we have when we initially qualitatively speak to what we want to accomplish but when we talk about fairness some of the tools and resources we have found valuable here for fairness are the stages at the end of the ml life cycle where we've really focused in terms of tool development and transparency frameworks is the evaluation and in the evaluation and monitoring stages i want to move now into some case studies to show you some examples of how this work has gone for us and what we've done both in the process of launching products and evaluating problems when they arose the first case study outlines our approach in cloud to facial recognition and how we got to this approach through our ai principles and the review processes we've put into place to operationalize them i'm going to start with the outcome of our in-depth review of enterprise spatial recognition technology you can see here the headline is the launch of a tightly scoped api called celebrity recognition available to professional media and entertainment customers we launched this last fall you may realize that video is all but unsearchable without expensive and labor-intensive tagging processes this makes it really difficult for creators and platforms to organize their content and cater to the increasing demand for personalized experiences or even really understand what content they have so celebrity recognition is a pre-trained ai model which means it's not customizable that is able to spot thousands of popular actors and athletes from around the world based on licensed images this is cloud's first enterprise product with facial recognition so i want to talk a bit about the history in early 2016 cloud leadership decided not to make facial recognition available as part of the cloud vision api offering despite it being one of the top requests from customers coming out of beta which was started the year before in 2017 facial recognition was identified internally as a key concern for potential unfair bias in our definition of algorithmic unfairness which was written by our privacy and security and now responsible innovation and privacy colleagues the core definitional document for google's machine learning fearless fairness efforts we decided to take face recognition through the process we described earlier in mid 2020 just recently we welcomed the news that other technology companies were limiting or exiting their facial recognition business and this was thanks significantly to the gender shades work which really informed how this technology can have impact across a range of unfair biases from skin tone to gender as you know we decided not to make a generally available face recognition api available because of our process back in 2018. so what we did initially within cloud is an in-depth review of face recognition technology by both our deal review and our product review bodies as well as the central google executive council our own internal review processes necessitated a broad look at their research societal context and complexities around facial recognition going through these processes and this was the second review we ever did gave us the open forum and the time to really think critically about the challenges and we realized that alone we couldn't do this so we sought help from third parties to work with us on both external benchmarking and performance and we began a human rights due diligence process where we called on domain experts and civil rights leaders and human rights leaders that would allow us to incorporate the perspectives of impacted people just a reminder of going back to that conversation about how our lived experience is not going to help us inform the lives and perspectives of impacted people and so it was really important that we did the work to understand that because the reality is everything we do every product we launch every market we enter does not exist in a vacuum and in order for us to make decisions that are aligned with our ai principles we have to take this into account at every step technical analysis alone is not enough because the reality is that there is systematic societal underrepresentation in media of black and minoritized actors and this needed to be the basis for our evaluation as a side note just with the analysis that the gina davis institute did on gender which showed that women-led films made 16 more in the box office despite men being both seen and heard twice as much as women on film you can see that films with racially diverse co-leads had created 60 percent more in gross income for those films but were grossly underrepresented in film more broadly human rights due diligence became a core part of what we needed to do human rights are a basic code of conduct for all human beings and we honor the universal declaration of human rights through the un and so again we really wanted to seek external voices to help us make this assessment and so we engaged in a process called a human rights impact assessment we did this through an organization called bsr and bsr stands for business for social responsibility and they helped us to really significantly improve our product offering engaging with bsr validated many of our already planned for alignment opportunities and we incorporated almost all of their additional suggestions across a range of aspects of the product their report is publicly available and i encourage all of you to read it all of this guidance and the reviews informed where we needed to do our testing and fairness analyses and it revealed to us both where the solution needed additional oversight and validated the decision not to offer a general purpose facial recognition api so in the coming slides i'm going to show you a deep dive into the investigations that then further informed the product decisions we made before launching so over three separate fairness tests we found a discrepancy in our training data sets falling on skin tone lines in one of those tests it's the one on the right the eastwood v2 this was obviously very upsetting and just like we saw earlier in the presentation for many organizations this is where they would have stopped and said we are just not going to do this but because we knew that we wanted to evaluate this as thoroughly as we could we decided to take a deeper look the first thing we had to determine was if the skin tone labels were accurate the answer was sort of they were not terribly accurate in categorizing medium and darker skinned people so based on the seminal work and the findings of the gender shade study we categorized skin tone into three buckets using the dermatological fitzpatrick skin type scale which is an estimate of skin sensitivity to uv radiation going through labeling the skin tones resulted in a smaller gap in the errors especially for the medium skin tones but there was still a problem in errors for the darker skin tones so here are the results of that relabeling 25 of the skin tone labels changed categories and it completely closed the performance gap with the medium female skin tone so that then led us to the additional discrepancies what we found was that a very small subset of actors and actresses represented a significant proportion of the total missed identifications in that evaluation data set and this was especially for the dark skin tone groups and especially for men so with that knowledge that a select few actors were disproportionately affecting the error rates in this data set we looked at those actors with the largest number of errors and found that they had nearly a 100 false rejection rate because we had made the decision earlier not to do a general face recognition api but to take this carefully scoped approach we were actually able to go one by one through the gallery and the test set to determine what the problem was so here's what we found it was really three actors where our celebrity gallery had images of those actors as adults while the eastwood training set had videos of them much younger all from the tv show family matters our model could not recognize the adult actors as the younger characters they had played many years prior in this instance for our celebrity recognition model we were able to correct that problem by hand labeling which closed the gap completely and that is what allowed us to get comfort with launching this product but the answer here is not to say that this was an instance of age of in the images because we took the time to look at the larger landscape and we know the issues of representation in media the fact that there's a discrepancy gave us pause to think about what might actually be happening here this is an example of why this process is so important and these were some of the headlines and in fact this is the bsr report as well but really it's not about slow walking it's about responsible development of ai which is what leads to successful deployment of ai none of this exists outside of the larger landscape we looked at initially and it's because we started with that understanding we were prepared to address it and we have to consider that at every step ethical ai is not only about the extremes and it's not always about ai for social good and it shouldn't be it's about being part of every use case where our job is to find a path through extremely complex and intersecting realities this case study will show you how we responded to two issues with a generally available product which was our google cloud vision api labeling is a common practice used to classify images and train machine learning models across many applications labeling is core to vision ai products and in 2019 we identified an instance that violated our ai principle to avoid creating unfair bias employees are our best source of feedback and that is how we were able to identify this issue a googler ran an image of themself in the api returning a result that misgendered them the cougar flagged this to the cloud vision product team which set forth an investigation into this issue through the review and assessment of this escalation it was important to put this issue in the context of the user by identifying the cause and effect this could have gender misidentification like what happened to this googler occurs because a person's gender cannot be inferred based on their physical appearance by a human or model their outward appearance that the model uses is not a determining factor when assessing gender and the impact of doing so exacerbates or creates unfair assumptions that restrict or harm those who do not look stereotypically male or female or who are a gender non-conforming person the steps to evaluate an escalation are not always the same some require in-depth product testing industry expertise third-party assessments and so on having identified the issue the potentially harmful impact to users we formulated a go-forward plan reinforcing google's commitment to build products for everyone we made the decision that a person's gender cannot be inferred by appearance and attempting to do so could perpetuate unfair bias removing man and woman labels from the cloud vision api and replacing them with a non-gender label such as person is the the act we took through this process we learned a couple key things that altered our approach to gendered labeled data there are often more targeted signals to attribute to a person that provide more valuable information to a customer taking retail stores as an example who might historically analyze foot traffic with demographic information a more targeted labeling could be a person's fashion sense to analyze foot traffic and cater their inventory to those fashion types we also came to the conclusion that when it comes to general tools where we do not have control over customer use cases to take a cautious approach with capabilities and functionality and build targeted solutions where we have a hand in the development and use case the second example is the result of an escalation of our vision api earlier this year 2020 and it shows how important an understanding of the societal context is in which our products exist here is the escalation you can see here the image with a black person and a handheld thermometer returned a label for firearm and an asian person holding a handheld thermometer returned a label for electronic devices this was brought to our attention through the tweet you see here on the left what happened next was someone taking the photo of the black man holding a thermometer gun and manipulating the image to make the skin appear to be lighter as you can see this changed the label from gun to tulle which is a deeply distressing result it is critical to recognize why a result like this can create increased harm the statistics here highlight some of the societal context in which this lives any robust evaluation of this escalation cannot separate this reality of racial justice racial violence and systemic racism from the technical analysis so what did we do our investigations led us across multiple teams and you can see here the results of some of those changing the skin tone in the first example through some of our counter factual testing does change the api's prediction but what we found was that gun was still a top five label response for all three images in skin tones although the api demo didn't surface this result we used our own explanations ai to look at saliency mapping saliency maps for the third hand images show that the attributes cluster predominantly on the thermometer for all three images with some bleed into the upper fingers of the hand for the darker and medium skin tone hands when we went into our counterfactual testing further you could see that a change of color of the thermometer gun in the original image also caused the gun label to disappear we then did stability testing and in this case the image appears very sensitive to image cropping even minor variations make that gun label disappear we also looked at our training data we don't have many images of handheld infrared thermometers and so this was a gap as it's a newer product in our own image database and it's not necessarily incorrect that this be labeled gum as they are called thermometer guns but if that's the case then they should be labeled that way unilaterally we found that many images were mislabeled gone including things like cameras for example this example represents an all hands on deck process that required a humble approach and thoughtful action to remedy by conducting a detailed assessment of the root cause of this issue we were able to understand that model accuracy and stability were key drivers and by adjusting the confidence threshold to more precisely return labels when a firearm is present we could greatly decrease the chance of this happening in the future this example showed us again that we have to do the work to put the harm in context in order to truly assess the extent of it it isn't enough to say that this was only about model accuracy and stability because this result was harmful and it exists in a world of much larger harms that we have to think about when we address the problem what this taught us through experience was that none of the evaluations we do exist outside of the large landscape and that context in which they lived and that's the case for every analysis we undertake for every product we launch for every decision we make about how to best respect our users the opportunities and each other thank you so much for joining us today i hope you've enjoyed this conversation and i look forward to many more in the future 