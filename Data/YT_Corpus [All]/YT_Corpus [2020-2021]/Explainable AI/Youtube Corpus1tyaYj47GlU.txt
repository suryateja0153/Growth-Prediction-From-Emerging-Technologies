 (digital music) - So I really don't have to introduce this topic as much. Everybody knows about artificial intelligence right now 'cause if it isn't saying that it's got a small solve your smart car, if it's not our thrusts on campus from precision health and medicine to what Marge just spoke about for even eldercare technologies, some of the things I'm associated with, material design agriculture and natural resources, security and defense, everybody knows about it. It's been promised to revolutionize our world right? Well, this is happening because we made a breakthrough. We're at a theoretical golden age. You know, it's different. We're ready to usher an AI. Well, we've been doing this for awhile, a 77 plus years. Debatably a third generation artificial intelligence. And it's really the big data. It's really been our computing power and it's really been some things that I'll talk about in a second that's kinda changed the way that we've approached this game. And so if we are in a golden age or if we're in more of an era of data science and some engineering, regardless of that, we are at a point where we can do some really powerful things. So when I got my PhD from Mizzou, I had been promised I could be professor smarty pants, I could learn big equations, I could optimize, I could do great things with AI, but yet the things that I could achieve now can honestly be achieved in some respects by a 10 year old. If you have access to the internet, you can learn and get the same material that was at my fingertips. You can get more data than I got five years ago and 10 years ago when I was able to do my studies. You can now have codes that do all the things in the books and all the state of the art cutting research that we do and please don't, but you can be bad at math. You can actually not know your calculus. And if you can write code we can automatically differentiate it. We can optimize it and we can solve solutions for you. So an eight year old could do what I could do when I became a professor smarty pants. So I think this is very empowering at the same time. Now the thing that I've had to address in the last two years is, okay, so maybe we're not really at AI, we don't have general purpose reasoning, we don't have common sense. Well, hopefully we have common sense, but the AI may not. But we may be at a pivotal moment, a point where we have technology that's good enough and maybe having domain task specific learners, it can help us to do things such as, Oh, you can make a lot of money. We've got a lot of billions being thrown around in the industry. We can also make a change in the world. But I think in getting to that point, we do have a responsibility. And part of that responsibility, is that we can't end up with things that we don't know what's happening. We can't have a black box. And so part of having a black box, if you like your smart car, you can't put a sticker on a stop sign and have it not recognize the stop sign and run right through the intersection. And I love this little video I was gonna play it, but actually it's the same algorithms that are being proposed to do your autonomy. They were learning to do soccer and the goalkeeper found out that if it did a dance it could fall over and get the penalty kicker to also miss and a learnable policy, but is ridiculous. Something you don't want to have happen. So we've got a lot of back doors that we have to fix and what we're doing nowadays, and thanks to many agencies from your white house to companies to national science foundation to agencies like DARPA, explainability is something that's getting a lot of attention. So what I'm doing in the lab right now is we've got some funding to attack this from different angles. One of these is that we're actually developing the brains. We're doing the drones and the vision, the computer vision and all this stuff that we're doing. We're working with a program called ivest to actually do augmented reality and we're the ones that are the eyes up in the drones that are being passed down into Microsoft and the HoloLens and making an augmented virtual reality. So all of the machine decisions, the humans are living in the same augmented space with the machines and now you've got a super space, but both the AI and the human and the human could interrogate what the AI is actually finding and see what that chain of reasoning is. That same stuff leads over to the fact that we're working for the national Institute of justice and we're actually using this for forensic anthropology. So we finally get this kicked off. The grant is here and the same drones and the same AI we're using to document and detect crime scenes, but we're also using geospatial now to go across the earth, not just from drones but at higher altitudes and to be able to do reasoning, a very complex scenes. Now what I'm trying to work on campus with many smarter individuals and myself is to try to take some of these things and to usher them in for things like agriculture and natural resources and see if we can make change in other areas outside of geospatial forensics and things like security and defense. And it is weird, but this is a wonderful day where these same theories also apply to things like material design. So we are funded here to actually work to do material design and innovation. And I'm gonna talk about two of these today being the materials and the geospatial, which includes remote sensing and also that from drones. So what we're doing is we're working with explainable artificial intelligence. We get a lot of data, build the machine in the middle and we wanna know why, what's happening, not just can you get a good result. And so to this extent, we're doing a lot right now during visual explanatory artificial intelligence where we're endowing the machine to take that, which it really doesn't have a great capability at and a lot of texture, a lot of contrast. But we're now seeing that shape and a lot of other features have been absent a lot of our modern architectures. So we're designing a lot of shape based machines that can be predisposed to learn shape. But also we can pull it back up and look at the shapes that they're actually learning and try to get a better understanding of what structure these machines are looking at. We're also taking the fact that one machine, remember earlier I said domain specific task owner, one of them doesn't cut it. Across the board we have to have multiple machines. And so we're doing the data fusion to be able to go through and to put these together, look at different combinations. So what if I make a super AI on top of all these individual artificial intelligences? And when you go, you say, what's explainable about this? He's doing visual processing and he's doing data fusion. Well, what the explainable part is is that we're using it for the drones and for things like remote sensing. And yet when we process all this information, we're trying to understand what's happening, what features are they looking at, what combinations of machines, and what functionality associated with those machines are we exploiting. So we're producing all of these types of graphical results that say, hey was shape and texture and contrast in different machines, looking at different architectures and all this, which ones are the best? How are they working together? And so we produced all these graphical explanations. Guess what I found out? They're an explanation, but they're not a good explanation. That's not what people want to ingest. So we explained, but the act of explaining isn't enough. You have to have an explanation that's natural to people. And just having a natural, explainable architecture or having graphical visualizations doesn't cut it. So as Marge talked about with eldercare and the last talk, we're actually producing linguistic descriptions. So we're translating all this now, which was graphical and all that information. And we're producing things like, hey, it's only one network that you needed to get this object class. Or we've never seen an instance where this one network performed well on any way. So you may not have the data that's actually supporting training the machines, so you've gotta go back and improve your system somehow. And these are the gaps. Those are the holes. And we're also producing statements such as we look at all these features and so forth that reminding, hey, what are the things that are not adding any benefit and what can they tell us about our problem and what can they tell us about the systems that we're engineering. So we're trying to make the machine talk to you and make it a little bit more natural. But the fun thing is we go both directions now the machine can talk to you, can you take the knowledge, you know, and in that language, can you talk back to the machine? Or we could talk back to the machine, right? But could we tell what we know? So we're taking that knowledge from the person and encoding and building architectures that takes that knowledge and now lets it works at an encodes the humans knowledge and it operates at that point. But that isn't enough. 'Cause guess what? I'm sorry. You're probably not optimal at what you do. So instead we go through and we optimize it and as we optimize it, we try to make it better. But I try to keep it really close to where the starting explanations were so that we don't get weird stuff that comes up. But we get something that had an essence of where you started and had some more learning. And we try to still keep the explanations grounded, you know, 'cause the machine's can learn a lot of crazy things. Now while this has been helping us for the drones for augmented reality, it's been helping us for doing geospatial analysis. The same explainability relates to material design. So we've been funded by the U.S army ARDEC to help to look at material design and innovation. So what we're doing is that we're going through and we're designing AI that looks specifically at what are the electromagnetic properties, what are their permittivity conductivity, what is the structural properties of the system that we need to design? And we're getting things like imaging optics that can get better ranging. We're getting things like cleaner image but also improved or degraded degregation of things out in the environments by doing these materials that we're trying to optimize. But low and behold, you had dust row engineers, you physicists, you will not take AI at face value. Whatever comes out, you don't care if I got a solution you wanna know what got it there. So what's the innovative part about this is that we've been working on trying to make the hard connections between the AI that does this material design and how it functionally works so that when you get a solution, we can relate it back to what electromagnetic propagation, what all occurred, so that you could open it up and have that explanation. So that's been really fun. But now we take it on a scene and we say, well, hey, I got a design now we put it out and it's optics. We are optimizing it. And so you wanna know where it's looking at an image. So we've had a lot of techniques that had been emerging over the last decade. You get a picture of a dog, we wanna draw it back and say, what are the network look at and you get this and it's like, hey, somewhere in the dog's face is that the nose, the mouth, the eyes? We really don't know, so we've been developing some techniques right now that allow us to put much finer level granularity to look at what the decisions that our machines are making into pop them down to the individual shape and structure level so that we can look at exactly what parts we're looking at. So we designed the materials and put them out in the environments. These are the things that we're highlighting and segmenting and showing and saying these may be part of our design process or things that are designed flaws and I'm kicking this back to the material folks. They're looking at it and trying to figure out how to make it better. I don't think that's really innovative. Okay, that's been fun stuff that we're working on. The cool part is that we've been trying to close the loop. What I realized is that when it comes to the material design and when it comes to the AI, the AI gets better when the materials get better and the materials get better when the AI gets better and they're not really two independent processes. So what we've been working to do is we make a system that makes data, you know, and what we do is we say for the materials that we designed and the optic system solutions that we generate, we get a bunch of new data and we say, can it fool the AI. If we do fool the AI, we wanna know about it. So those are the solutions that we present to people and they're looking at it and saying, hey, this is really interesting. This is a design that I wanna consider. But I also say this is a backdoor for my AI. I need to now include this back in and close that loop so that that soccer player didn't do a you know, goalie random dance and fall down. And that was something that can be learned. But that by itself is only part of it. We're now taken back when we didn't learn some way to improve the process. We're looping it back and I'm saying, hey guys, this is what's happened you now need to come through and to build this new structure or something with new, you know, dielectric constants and properties. And so now we're getting new materials and we're trying to get new things that can actually fool the AI, but we're putting them all into one big loop for trying to solve it. Now, as part of that, we're taking the visual and linguistic explanations and producing it out there in the top. So we know what the AI is doing and how it can explain itself. And that's one report that people will get. But we're also down here taking the solutions that we get from the materials, and we're looking at the physics side of it, and we're trying to say, well, what are the properties of the physics solution? Is this actually something that was realistic? Is this something that you know was just garbage? So what we're really trying to do is as I also sit here and dream about, will the real AI stand up, I think part of the standing up is the fact that standing up is that we're gonna do something with this even if we aren't there. We have very powerful tools and I think that even our 2020 AI is good enough to make change and we need to be putting it out there and figuring out what we can actually solve. I think that what is your threshold for acceptance? I won't take a black box. Maybe you don't want the explainable AI, but maybe it's these processes or other tests that you are gonna hold these systems to so that you will adopt some technology and I think in order to get there, we had 77 years of creativity and I think it's important that we maintain that diversity in combined mathematics, statistics, physics, all these things to try to explore a lot of things in these areas to come up with good solutions versus kind of a trend we have right now, putting all of our eggs in one basket. You know, we've got a lot of powerful tools, but don't be fooled by them. So kinda roll relate to that end. Up here and top is the people right now currently on these research grants that I was talking about, the students, the postdocs, the collaborators I'm only one spoke in the will and I'd like to thank every last one of them. And it's been fun being here for two years and meeting everybody. And if you're not a peer, I'm very sorry. There's so much going across campus for natural resources and agriculture. These are just the ones that are highlighted today, but it's wonderful being back at Mizzou. (audience clapping) (upbeat music) 