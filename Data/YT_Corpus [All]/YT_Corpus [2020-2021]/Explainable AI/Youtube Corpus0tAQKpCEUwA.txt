 hi everyone today we'll talk about what's new with bigquery ml and we also have demand based with us to talk about their usage of bigquery ml i'm abhishek kashyap the product manager for bigquery ml and data q a and i'm eric i'm a data scientist and i'll be discussing two ways that demandbase leverages bigquery ml my colleague leo bunkel was originally slated to present with me he did a lot of the work behind uh this talk so i'd just like to note that i will be presenting his slides on his behalf thank you eric for being here we'll start with talking about what's new and then eric would talk about demand-based use cases with bigquery ml to summarize we have added new types of models to bigquery ml to help you create many more machine learning models to address most of the use cases that you would have for machine learning in bigquery specifically for classification and regression models we have added the support for creation of boosted trees using xgboost deep neural networks using tensorflow and automl tables in terms of other use cases we have added support for creation of hundreds of thousands of time series models with a single sql command and you can also now create product recommendation systems using matrix factorization for use cases that require online prediction you can now export any model created in bigquery ml and use it for online prediction in google cloud ai platform or your own serving stack to set the stage i'll spend a minute talking about bigquery bigquery is google cloud's enterprise data warehouse we have exabytes of data stored in bigquery and customers are able to run sql queries on petabytes of data all this is enabled by the serverless architecture of bigquery and underlying that is the separation of storage and compute which has allowed our largest customers to store more than 250 petabytes of data in the query and the largest queries being run are on five petabytes of data now this is awesome for data analytics however when you want to do machine learning on this data we see a lot of tasks which are not really machine learning specific let's say you create a model in a notebook or a sample of data that works well but to create a production model we see the need to write etl pipelines for model creation and then you also have to do etl pipelines for predictions as well now bringing data to machine learning carries that list of tasks the bigger headache however is that all the governance policies that have been defined on the data just go out of the window the second you export it and train a model on a vm cluster outside bigquery which i'm sure no enterprise likes we are solving that problem through a sql interface to machine learning called bigquery ml with bigquery ml we are bringing machine learning to the data itself through two simple sql commands one you can say create model and that would build a model for you within bigquery and second you can just say ml.predict to use that model and get predictions in bigquery all your features go into the select statement so you can very very fast recreate on models by changing your features and the model type within that sql we do automate a lot of standard ml tasks like split of training and test data sets standardization of numeric features and coding of categorical features so where the minimum number of lines of code you can get a pretty good model and finally since this happens in bigquery there is no infrastructure management and all the governance and access policies you defined are automatically applied to these machine learning workloads as well to recap we are extending bigquery ml by support of all these new model types which should enable you to address most of the machine learning use cases you have on bigquery data to give you an example with a time series model one of our developer advocates felipe hoffa wanted to predict how many stack overflow tags would be there for each of those and for doing that felipe created 15 000 different time series models in 20 minutes and each of those is hyper tuned so we actually ended up creating 600 000 models within 20 minutes without any infrastructure management without any data etf another example of a capability we have brought on is addition of automl tables automl tables was created to build a best-in-class machine learning model on structured data without any feature engineering and without having to think about which network architecture to use automl tables searches through a set of architectures and builds the best possible model for you automl tables has been available through its own ui and now we have added it as a model type to bigqueryll so just like you can create an xgboost model or a logistic regression model you can now create an automl table classifier through the same sql interface and very easily build it into your data and machine learning pipelines now all these are being used for a set of use cases by many of our customers to give you a few examples of the most popular use cases let's start with the retail industry the use cases we are seeing are creating product recommendation systems to send products through emails which are very personalized for the end users calculating the propensity to purchase a propensity to click on an asset for each customer and again using it in marketing campaigns if you want to forecast demand of a particular product or category by store by region with time series you can now do that in a single sql command and use it for better merchandising and supply chain optimization and the fourth one is doing customer analytics by easily creating customer segments aka personas when we look at logistics and cpg the use cases we are seeing are estimation of the time a package will be delivered or when a vehicle will get somewhere predictive maintenance prediction of how much inventory of a product would be needed at a certain warehouse for financial services and insurance on the retail side we are seeing usage in calculation of customer and tv which can then underlie all business decisions for each customer we can improve the underwriting process by easily predicting loan default probability at the time of receiving an application you can use bigquery ml for the batch credit card for fraud detection pipelines that run overnight and finally there are use cases in anti-money laundering as well where you can forecast how many transactions may be stopped for aml checks the fourth vertical which is media gaming high tech has use cases similar to retail where you want to recommend content through an app or on the website or you want to predict player ltv you want to predict game difficulty similarly you may want to predict if someone is going to subscribe or if someone is going to churn and take a marketing action accordingly now all these use cases can be solved through bigquery ml with the new features we have added to walk you through an example a lot of our customers join data from multiple online and offline sources and create a customer 360 profile in bigquery now this customer 360 profile primarily contains behavior data transaction data and demographic data you can now use all of these together to create a set of customer segments right within bigquery by using k-means clustering and we have customers who are doing it on hundreds of millions of customers and once you have those you can build recommendation systems in place and run personalized campaigns to each of these customers again staying within bigquery and using bigquery ml for these machine learning tasks in terms of pricing if you are a flat rate customer or you use flex slots you can use your slots as is there is no additional charge each time you build a model uh we just use the slots you have already purchased if we are using automl tables or a model type which uses cloud ai platform in the back end we convert those resources into slots at price parity if you use bigquery on demand the cloud ai platform cost is billed as a passthrough through bigquery so there is price priority there as well for models which are trained natively like time series matrix factorization and regression the pricing is according to the amount of training data that you use with that i'll pass it on to eric thanks abhishek and thanks to google for giving us the chance to speak again i'm eric and i'm a data scientist at demandbase i'll be discussing two use cases where we leverage bigquery ml the first involves our digital advertising product which consists of training a model on massive amounts of data at scale and the second use case has to do with how we took a fairly complicated spark model that runs on demand and converted it to run in bigquery ml and ended up with quite a bit of savings both in time and cost so i'll provide a little more context for each use case before diving in and first a little bit about demandbase we're a late stage startup we have offices in san francisco seattle new york and london we've raised over 100 million in venture capital we consider ourselves an ai first company we process the world's business data to help marketing and sales organizations and our customers include over 100 fortune 500 companies we offer a sas platform for orchestrating account based marketing strategies which includes a targeting solution for advertising an engagement solution for site optimization and customization and a conversion tool to align abm initiatives with sales teams we have a strong data science and engineering team all of whom are quite active in the gcp ecosystem so the first use case i'd like to discuss involves our targeting solution which is effectively a b2b digital advertising platform we use ai to optimize delivery of digital media to all accounts in our customers ad campaigns we provide full transparency when it comes to advertising metrics such as media spend click-through rates account reach etc which means we're held accountable to delivering the highest quality inventory that leads to clicks engagement conversions and so forth we process roughly 430 billion options per month and we bid on those options around the clock an average of 50 000 bids per minute which means we're bidding on nearly a thousand auctions every second so just to give you a sense of the size of this data again we're bidding on nearly a thousand requests per second and each bid request consists of thousands of features for example this person here visits her favorite news site during the day we know the ip address of the request we know the company she works for right down to the location of the office in geo whatever site she's visiting we know the publisher the amount of inventory on that page the position of the ad etc and finally there's all kind of additional data such as the exchange the type of auction deal id and so forth on top of all that we could have dozens of our own customer campaigns that qualify to serve that single ad during that visit so we also need to work out which one to serve to do so we use a click-through rate model that essentially takes all these features and predicts just how valuable that option is what the likelihood is that it'll lead to a click etc and this scoring and decision making all has to happen within several milliseconds for each bid request so initially we did all our data science and local jupiter notebooks meaning we'd export the data out of bigquery in a python notebook for analysis training validation etc and scikit-learn besides being inconvenient limitations on the size of the data cpu memory meant that we had to down sample the data not just the number of rows but even the number of features that were considered for the model so it's no surprise with such a high cardinality of features that this would affect both the calibration of the model and its accuracy since our data already resided in bigquery we decided to use bigquery ml namely because it solves these issues and as i'll show next training a logistic regression model is just trivially easy you just point your model at your training set no matter how many rows or features and bigquery ml takes care of one hot encoding k-fold cross-validation and everything else under the hood so it's much more convenient much faster and because we no longer had to down sample rows or features we actually saw significant improvement in the calibration of the model and its performance so just to show you a real example this is how you train a logistic regression model it's really as simple as writing a short sql query you can see the table name as bid request dot training set you name the model whatever you want in this case model underscore name specify the model type logistic regression in this case add a parameter for l1 regularization and that's it when it's done you save the output just as you would any table and that table which is really a model can then be queried for validation or to make actual predictions there are a variety of things you can control if you want to do multi-class classification or if you want to standardize or scale your features it's all available so this is what we did this is how we moved off of scikit-learn and onto bigquery ml so what if you want to try a different model let's say a deep neural network for example almost nothing changes except for the model type and in this case how many layers you want other than a physical limitation of 512 megabytes i believe you can have as many layers and nodes as you wish so in this case i specified three layers with 256 nodes two more layers with 128 nodes each and that's it just press enter and wait for the training to finish so with deep neural networks you have similar options as logistic regression but also one specific to neural networks such as batch size dropout rate etc we also tried xg boost which is sort of the hottest algorithm around since random for us again you can see that very little is changing here same training data set just a different model type in addition to parameters specific to xg boost such as the booster type it also supports l1 l2 regularization and other options there's even a boosted tree regressor if you want to do regression instead of classification just change the model type so we've seen how to train three different models how do we validate them once they've been solved save sorry and the answer is once again as simple as the sql query you've saved your model with some name just select the precision recall f1 score from that model just as you would query a table this makes it very easy to inspect validation metrics understand future importance and what was important to us was being able to access the coefficients of our model what's really cool is that these validation metrics can also be viewed as interactive dashboards that are built right into bigquery ui so yeah it's very cool stuff and i should mention that the xg boost model did have the best f1 score ever so slightly better than logistic regression and dnn overall comparable f1 scores across all three models so the second use case is for a solution we call account selection basically a customer provides us with data such as keywords describing their business buyer titles or list of existing customers and we use ai to identify companies with strong buying signals for that customer our account selection model trains on terabytes of b2b signals and intent data and effectively generates a ranked list of accounts that your company should target next because our customers will prioritize their entire marketing and sales cycles with this list of companies it's incredibly important that we get this right customers should be able to update their profiles and relaunch account selection which basically trains a new model and generates a new list of companies for that customer the way we initially saw this was to build the model in spark and use on-demand clusters which were not only costly but would cause issues when the job failed because a new cluster had to be provisioned from the marketplace which increased the time and the cost etc so a new model has to be trained whenever a user pushes the button this means we have to support hundreds if not thousands of jobs per week and the cost was something like dozens of dollars per job in terms of size the features include intent keywords company from a graphics job titles and other b2b signals basically thousands of dimensions that we reduce using techniques like matrix factorization we previously stored our data in all kinds of disparate data sources from postgres to static files in the cloud in terms of time starting a cluster and then training the spark model typically took up to 40 minutes for each job to complete and when a job would fail a new cluster would have to be started it would occasionally take several hours for a single job to complete so along comes bigquery ml we decided to replace our spock model our spark random force model with a logistic regression model in bigquery ml um this was not without its challenges for example we had to either move our data sets into bigquery or at least copy them as temporary tables in bigquery to build the final training table this means all the feature engineering has to be expressible as sql statements or udfs so for example if you use the cosine similarity between vectors as a feature you'd have to express that as a query our approach was to build this pipeline as a dependency graph so what do i mean by that basically we created a data pipeline entirely out of sql queries in bigquery organized it into a graph so any extraction transformations feature engineering etc we express as a directed graph that creates temporary tables at each step this way if anything were to happen during the process we wouldn't have to run the entire thing again but just pick up from the temporary tables and then continue to run the pipeline until we end up with the final training data set so i've already covered how to train a logistic regression model so i'll just jump to the final output basically we end up with a rank list of companies that have greater likelihoods of becoming a future customer i do have to note this screenshot was lifted from our website this is not real data being shown here and again because our customer's entire marketing and sales cycle could be based around this list it's essential that decisions made by the model are explainable the first thing a customer will want to know is why did you include this account because bqml makes it easy to extract feature weights for each model we are in fact able to make explainable statements like well this account is similar to an existing customer or has a strong intent for keywords in your industry or whatever so for this use case we've been running the model in bqml for months and we've had zero failures which is amazing previous average times of up to 40 minutes to run the job now take closer to five minutes so the time to create a new list for customers was reduced by a factor of 10. if you break down the costs and again we've actually never had any failures bqml today the cost is now basically only a few dollars per job which means a cost reduction of over 10x so overall bigquery ml has been a huge game changer for demandbase not only improving the accuracy of our model in the first use case but saving us a factor of 10x in both time and money for the second use case so we absolutely plan on continuing to leverage bqml for our next generation of machine learning models at demandos thank you very much thank you eric for your time and these insights were extremely useful if you want to learn more we have a coursera course that would teach you how to use bigquery ml with some code labs using quick labs the use cases that are shared we are building template jupyter notebooks which we are hosting at the link i'm sharing here and finally if you decide to use bigquery ml and have any questions or feedback for our team please do write to us thank you for your time and hope you have a great day 