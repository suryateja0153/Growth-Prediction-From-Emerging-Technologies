 I'm Jonathan Dinu, currently an independent  researcher and this video is about recent work   looking at common interpretability assumptions  in machine learning model explanations. It was   jointly conducted with Jeffrey Bigham and  Zico Kolter at Carnegie Mellon University.   This video itself is short, meant to be a  visual abstract, but if you are interested   in learning more there is a paper, data, code, and  experiment at the url at the bottom of the slides.   For this work we're interested in looking at three  interpretability assumptions. The first of which   is that simpler models are more interpretable,  second model agnostic methods like LIME and SHAP   are data and user agnostic, and lastly that  any explanation is better than no explanation.   And to investigate these assumptions we conducted  a large scale human subjects experiment on   Mechanical Turk with 796 participants. Each  participant was presented with explanations of two   black box models and they were asked to choose the  more generalizable model based on the explanation   shown. Here's an example of the interface for one  of these pairwise comparisons. The participant is   shown two explanations of two models, algorithm  five and algorithm six, and asked to choose   which one they think is more generalizable  and how confident they are in that choice. For this experiment we were interested in four  different factors: two between-subjects factors   of the sparsity of the data and the explainer or  explanation method itself and two within-subjects   factors of the top-n features shown in the  explanation and the item or data example itself.   The experiment is live at this url, so if you  want to experiment with the experiment you can   go and see what the participants went through.  And everything is anonymous and there is no   data collected. To understand how these factors  influence interpretability, we fit a Bayesian item   response model where we modeled a person parameter  and an item parameter. In this item response   model, the person parameter represents the ability  of a person to pick the more generalizable model   and the item parameter (or the data example  parameter) corresponds to item easiness. Or   said another way, how interpretable is the  data example itself? And we additionally   modeled a variety of covariates that relate to the  factors and the different levels of the factors. Here shown is the 95% and 50% credible  intervals for the estimates from our model   and the points are the medians. I won't go through  and explain every one of these but the salient   takeaways from this are the data sparsity has the  largest effect among all the covariates (it has   more of an effect than the actual explanation  method itself), the person and item effects   often outweigh the explainer effects (so who  the end user is and what data is being explained   can matter more than the explanation method  itself), and lastly explanations don't have   monotonic utility, so there isn't one explainer to  rule them all (certain explainers work in certain   contexts). Here if we look at the interaction  between sparsity and ridge regression,   we can see a fairly negative parameter but if  we look at just the ridge and just the SHAP   parameters (all else being equal), ridge  performs as one of the best explainers. With this research and with all research,  there're always limitations and always caveats.   For us we only looked at one data and one  task with 796 people. Interpretability itself   is epistemologically fraught I say, in that there  isn't really a clear/easy/direct way to measure it   (and everyone has different proxies of what  interpretability is to them). And lastly,   it's just a really big design space to search  through of domains, methods, parameters,   and lots of things to vary and experiment with.  But in conclusion... [pause for slide until end]. 