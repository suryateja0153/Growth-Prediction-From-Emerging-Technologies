 This is Marvin the Cat. He installed a new chatbot on his phone that uses artificial intelligence. Unfortunately, the chatbot doesn’t always make sense to Marvin, which is why he gets confused. But why doesn’t that make sense? Artificial intelligence is often represented as this black box that takes in an input like Marvin’s texts, and spits out an answer word by word. If you peek into the box, you’re often confronted with these very confusing and complex neuro networks. They work by passing a huge amount of information forward, until they predict the next word. To make sense of why the neuro network predicted a specific word, we found in our research that you can go backwards through the net and connect the output to the input. For example, to come up with the word Marvin, we might find that the neuro net paid attention to the input words Marvin and you, but not chatbot. How this information is exactly visualized to Marvin, and whether that is enough for him to trust his chatbot, we don’t know. This is why we’ll run a user study in which we can evaluate our design of explainable artificial intelligence. 