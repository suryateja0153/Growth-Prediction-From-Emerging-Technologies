 Hi, I am Stefano De Sabbata and this is R for  Data Science at the University of Leicester   so today's lecture is the first of our series  of lecture on machine learning starting from   today we will shift our attention from series of  methods usually understood as classic statistics   to more complex methods which are usually studied  under the umbrella term of artificial intelligence   or machine learning as i mentioned before  there are a lot of overlaps between statistics   machine learning and artificial intelligence and  actually the terms used to refer to these methods   largely depend on which department you're working  in and which terminology is in faction this season   last time we have discussed the regression models  including simple and multiple regression their   relationship with general linear models their  assumption and how to test them and how to   interpret the results and compare different models  those lectures don't cover all that can be said   about linear regression models for once we haven't  had the chance to talk about dummy variables   but it should provide you with sufficient ground  to be able to expand your knowledge of those   methods as and if it will become necessary  for you wherever your career will take you   the same applies to the series of lectures  starting today on machine learning   we won't be able to cover everything that can be  said about machine learning as they will probably   take not just one module but possibly an entire  one year course the aim will be to provide you   with a broad understanding of the field and of the  main methods that are using gi science as well as   with sufficient understanding of the main concept  to allow you to expand those further in the   coming weeks we will be covering artificial neural  network with a brief hint to deep learning support   vector machine and unsupervised clustering i won't  be covering random forex classification which is   among the most widely used methods in remote  sensing and earth observation as it is covered   elsewhere in other models today we will start from  a brief introduction to what machine learning is   and different types of machine learning as well  as their limitations and issues tom mitchell who   literally wrote the book on machine learning has  wrote several of the very first books on machine   learning in the 80s and the 90s which are still a  point of reference even nowadays define the field   of machine learning as being concerned with the  question of how to construct computer programs   that automatically improve with experience to  mitchell machine learning sits at the intersection   of computer science and statistics where the first  can be defined as being concerned with how to   manually program computer to solve tasks whereas  the second can be defined as being concerned   with what conclusions can be inferred from data  now the method of least square that is the method   that we have seen last week to solve the problem  of reading regression was first described in 1805   so about 150 years before the arrival of even the  earliest computers so clearly there is a lot that   you can do in statistics without computers  at the same time there is plenty you can do   with a computer that doesn't necessarily require  statistical inference from data machine learning   sits at the intersection of computer science  and statistics and such it is concerned with   how to get computers to program themselves from  experience plus some initial structure as well as   how effective data capture storing index retrieval  and merge affect those abilities and how we can   make such problems tractable by computers  that is computational tractability indeed   that is still a fairly wide and generic field in  fact as there are probably not many statisticians   that calculate their statistics by hand are  all statisticians doing machine learning also   in the era of big data where everything we do  is capture and analyze and programs are expected   to adapt their users are all computer scientists  doing machine learning have we have started doing   machine learning in this module the first time we  run the function mean or sum indeed the boundaries   are very blurred nowadays machine learning is  also sometimes used in a general sense in a way   that is interchangeable with the term artificial  intelligence although the two are not the same   and in general machine learning is only a branch  of artificial intelligence which is as mentioned   intended to learn from experience to learn from  data but there are other parts of artificial   intelligence which are not concerned to learning  from data for instance knowledge representation   is a field of artificial intelligence which  aims to capture and enact rules for automated   reasoning starting from a representation  logic from which decisions are derived   in this case the logic rules are traditionally  encoded in a formal ontology rather than derived   from learning in fact this was one of the most  prominent fields in artificial intelligence   before the most recent hype around big data and  machine learning machine learning approach come in   two main flavors supervised approach are based on  the idea of training a predictive model from data   using one or indeed more attributes of a data  set to predict another attribute this should   sound very familiar as that's what we've done last  week with linear regression models but there are   other types of problems which are usually approach  users who provide methods such as classification   unsupervised approaches are instead descriptive  and i don't mean that in a bad sense it's not   like when you write an essay and the teacher  tells you that you have been too descriptive   in this case it means that they can be used  to describe the data or to be more precise to   discover patterns inside the data to find out  things about the data that we didn't know were   there or that we know should be there but we don't  know how they play out in the specific data set   at hand these approaches are very common for  data mining tasks although the term data mining   is now growing a bit out of fashion a classic  example is clustering which we'll see later on   in this module in order to define a supervised  model we need four things first we need a   training data set which should include a series  of input attributes also known as predictors or   independent variables and one attribute to predict  also known as outcome or dependent variable   in machine learning and in particular when it  comes to classification tasks the relationship   between predictors and outcomes is also referred  to as labeling and the outcome variables are   labels the image on the size illustrates one  of the most common datasets used for examples   in machine learning that is the mnist dataset  created by the national institute of standard   and technologies in the u.s and it is a data  set of images of handwritten digits from 0 to 9.   a common example task is to use as predictors the  color values of each pixel of those images and as   an outcome the actual value represented by the  image model constructed this way are at the core   of all tools that we can now use to automatically  recognize text such as the google lens app   or software used by mail services to automatically  read addresses from letters we also need a testing   data set with the same predictors and outcome  variables and then we need to decide what type of   machine learning model we are going to use this is  a bit of a tricky bit because different algorithms   have been proved to be very effective for some  types of problems and less effective for others   and vice versa some algorithms are generally  really good but they might require a lot of   computation or resources and they might  be completely overkill for some problems   whereas simpler and less expensive method  might have done just as fine and finally we   need an evaluation function which is required  to assess the difference between the prediction   and the output in the testing data again this  will sound very familiar if you remember what we   discussed last week with regard to evaluation and  cross-evaluation methods for linear regressions   however one of the key differences between what  we will call statistical learning methods and more   recent machine learning methods such as neural  networks is that statistical learning approach   which were devised in a time of data scarcity  were developed starting from the assumption   of working with a small although representative  sample of a larger population as such they don't   tend to improve massively with larger amount of  data and the diagnostic statistics commonly used   in conjunction with those methods such as the p  value tend to become effectively unintelligible   when working with very large data sets and a  lot of the common practices such as setting   the p-value threshold to 0.05 or 0.01 which  can be fine when working with a few hundreds   or maybe a few thousand cases become meaningless  when working with millions of data points   more recent machine learning methods instead  tend to improve significantly with increasing   amount of data used for the learning process  indeed one of the key reasons they have become   so successful and ubiquitous in the past decade is  exactly because with the increased digitalization   first in the industry and then in our daily  lives the amount of data available for   training has increased massively and so has the  quantity and quality of the models created and   we are now approaching that point mentioned  in a quite famous quote by science fiction   author arthur c clarke when machine learning in  particular when concerns image object recognition   and text analysis is becoming sufficient  advanced to be indistinguishable from magic   at the other end of the machine learning spectrum  we have unsupervised machine learning approaches   which commonly require three components a  data set with some attributes to explore   and a type of model for the learning process  that we assume is relevant for the data that   is some sort of rule or a set of rule that we can  check on for the data most of these approaches are   iterative and the algorithm tend to be designed to  basically work around data and check whether the   rules applied or not as an example in hierarchical  clustering the assumption is that we can go and   group data in our data set in groups and subgroups  and the algorithm effectively goes around in the   data space and puts together data points which  seem to be similar enough to be part of the same   group as for supervised approaches we will also  need an evaluation function to check the quality   of the pattern that is under consideration  during each iteration of the algorithm   as mentioned before supervised learning approaches  require labeled data that is the data should not   only include values for the predictor variables  but also for the outcome variable so that the   model can learn the relationship between  the predictors and the outcome however   labeled data can be expensive to acquire  for instance image recognition models have   been trained on larger number of image which  have been manually labeled as including or not   including a certain type of object semi supervised  learning approaches trying to overcome this issue   by combining a small number of labeled data with  a larger number of unlabeled data the model is   first trained on the small label data set and  then applied to the larger unlabeled dataset   generating prediction for the outcome variables  which are commonly referred to as pseudo labels   the model is then retrained using all the data  including the zelda labels even if these zelda   labels generated by the first version of the model  are not precise or fully correct because based on   a small training set they still provide a valuable  insight in the training of the final model   this model work under three main assumptions  continuity that the data points that are close   to one another are more likely to share a label  than points which are further away cluster the   data tend to be congregated in discrete clusters  and points in the same cluster are more likely to   share the same label than points which are in  other clusters and manifold which effectively   says that the input data has many more attributes  that would actually be required to represent or to   say it in other words that there are in fact  a few core hidden features of the data that   actually drive the entire pattern so far all the  learning approaches that we have seen supervised   semi-supervised and unsupervised are based on the  general idea of looking at some data extrapolating   rules and patterns but they are all conceptualized  as algorithms of serving as an upshot of reality   instead the branch of machine learning known  as reinforcement learning focuses on the idea   of an agent learning some rules also refers to  as behavior or policies by interacting with an   environment the fundamental idea is that an agent  starts from a particular state and at each step   he can choose among a number of different  actions each action will result in a certain   amount of reward depending on the state of  the agent and the state of the environment   by working through the same scenario over and over  again the agent is able to learn a series of rules   that will lead to the highest amount of total  reward in doing so the agent also needs to balance   how much he wants to explore new options thus  creating new rules and how much you want to   explain its own knowledge which has been generated  from previous interactions reinforcement learning   is conceptually akin to agent-based modelling but  while the aim of refreshment rendering is to learn   the rules that will lead an agent to win some sort  of game agent-based modeling is concerned with   examining emerging complexity from simple rules  which are provided to the agents as well as the   interaction between different agents one notable  success that involved the use of reinforcement   learning was an artificial intelligence program  named alphago who became the first computer   programmed to beat a human professional go player  a few years ago alas despite the pressing wins   and the widespread adoption machine learning  is not the solution to all humanities problems   and there are several limitations and issues  that needs to be addressed first and foremost   these methods are really very complex in this  module we are going to see some of the most   widely used approach but also some of the simplest  exploring the most recent and complex algorithms   will require an entire set of modules the  complexity of this method has several consequences   not only the difficulty in using the same method  although recent years i've seen the development of   many frameworks such as tensorflow which allow for  a very high interaction with those models which   can be defined with a reasonable amount of lines  of code if you know what you're doing however   even then creating a model requires hundreds of  decisions to be made about which variables should   be used and how to normalize them which approach  and model might be most suitable for the problem   at hand which component of a given model should be  used and in which configuration which algorithm to   be used to run the computation how to configure  a usually rather long list of parameters and how   to evaluate the performance of both the learning  process and the final model in fact think about   all the decisions that we had to make to create a  multiple linear regression model and multiply that   by some orders of magnitude that is usually the  complexity related to machine learning approaches   moreover many of these algorithms are quite  frequently referred to as black boxes in the   sense that they tend to create models which  are very difficult to scrutinize for instance   if you think back to our regression model  especially in the case of a simple regression   we could easily plot the scatter plot and  interpret the intercept and coefficient   of the model generated by the linear regression  as parameters of a line draw that line and see   how well it fit the scatter plot or look at the  scatter plot of a or a histogram of the residuals   and see what the shape of our error is we have  seen how that's already a bit more difficult to   do with multiple linear regressions a scrutinizer  model generated by very complex machine learning   algorithm is very very difficult however there is  some hope as given the increased usage of complex   machine learning approaches there is quite  a lot of interest in what is being defined   as explainable artificial intelligence those are  methods that aim to visualize those complex models   the contribution of each of the predictors and  how they are combined and their heroes to render   them easier to interpret and scrutinize one other  significant issue affecting all machine learning   approaches is the issue of overfitting that is  the risk of creating a model which is perfect   for the training data but not generic  enough to be useful for prediction   for instance in linear regression model if we  have a data set with 10 cases we could set up   a model with 10 predictors generating a line that  fits those 10 points exactly as shown by the blue   line in the image on the slide however the model  represented by that blue line would be basically   useless for prediction as whatever process  or phenomenon created in those data points   is rather unlikely to be actually following  that model rather the black straight line   is much more likely to provide a useful  approximation of that particular phenomenon   in general when creating a model is frequently  useful to follow the principle of parsimony also   known as the occam razor which in this case will  translate to the guideline to select the simplest   model and the fewer variables that can lead to a  good model rather than aiming for a perfect model   which might be leading to overfitting clearly  that's a rather generic advice which might be   difficult to interpret with regard to a specific  case other more specific guidelines exist one of   which is the 1 in 10 rule which suggests that  one would need 10 cases per each predictor   in order to create a good model now if you think  about the fact that in image processing the value   of each single pixel is used as a predictor in  a massively complex model you can see why even   using a very small image say 128 times 128 pixel  which is in total over 16 000 pixels if we follow   the rule of one in 10 we would require hundreds  of thousands of images to train a model properly   finally after a decade in which many seems to  suggest that artificial intelligence was on the   brink of solving every single problem recent  years have seen a new type of challenge to the   quality of those models and in particular for what  concerns their fairness especially toward minority   groups the key issue is that the assumptions  and the quality of the training data sets still   matter a lot in the final product in the field  like computer science which is still far away   from being representative of the larger population  at least in the us in europe and in a field like   computer science which is still far away from  being representative of the larger population   at least in the us and europe i can't really say  about the rest of the world the assumptions that   those who build those models bring with them have  an impact on the assumptions imposed on the models   and the assessment of the quality of the training  data and that matters bias authors could lead to   biased algorithms for instance for instance  joy pollenweeny and timnit gerbu did some   very interesting work on facial recognition and  they were able to show that black women were 35   percent less likely to be recognized but this  algorithm compare with white men gerber was   later hired by google as an ai expert in ethics  although recent news says she was fired for   complaining about google's ethical stance although  google says she resigned specific case aside   the reality is in a world in which ai and  machine learning will take more and more roles in   automating decision making there is a significant  risk to encode our biases into our models   and an even greater risk that once those  models and algorithms becomes institutionalized   the bias that they bring with them will become  institutionalized as well and even more difficult   to change especially if those algorithms are the  black boxes extremely difficult if not impossible   to scrutinize because of their complexity or  because they are intellectual property so today   i've provided you with a brief introduction  to what machine learning is the different type   of machine learning as well as their limits and  issues including overfitting and algorithmic bias   the next few sessions will provide you with  an introduction to the main models in machine   learning both supervised and unsupervised thank  you for watching and i'll see you next time 