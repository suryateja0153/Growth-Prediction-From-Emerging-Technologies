 A good day to all of you the title of my presentation is the social technical design of trustworthy AI. my presentation comprises of the following first, I will explain and describe the background of seeking a trustworthy AI. Following that, I will describe the knowledge gaps and the research questions on this topic subsequently, I will discuss the question of what is trust following that I will discuss HCI as a way of attaining trustworthiness by design following that I will discuss the idea of AI ethics as a way of building social Trust. I begin with the background of seeking a trustworthy AI predictive algorithms are being deployed in high-stakes decision-making with significant social consequences today as a recent case in the UK where students were unfairly graded by algorithms when examinations will cancel because of the covid-19. This case among many wherever algorithmic predictions exhibited evidence of unjust biases has consolidated the perception that algorithm should be seen as untrustworthy until proven otherwise Furthermore, as machine learning and artificial intelligence are increasingly deployed in public health scenarios. Where for example AI recommendations can impact many people simultaneously with irreversible impacts trusting AI technology on one hand and the trustworthiness of this AI on the other has become paramount. It is not feasible to verify and check on the what and why of AI recommendations at every point during a public emergency, trust streamline action. This concerns among others then constitute the imperative for seeking a trustworthy AI which has led to the rise of a new view of explanatory artificial intelligence or XAI. But the focus on interpretability and explainability in XAI is necessary but not sufficient for the quality of trustworthiness. Why is it so? there are at least two different reasons First, to have trust, explanability is key even so explainable models are interpretable by default, but interpretable models may not always the explainable Explainability requires one further step. Furthermore, explanations can be created to be persuasive for the purpose of increasing trust. For example, making a relatively simplified description of a highly complex neural system rendering a highly persuasive explanation does not increase the substantive trustworthiness of AI, Secondly explainability as a technical attribute does not yet address the roles that material, spatial and social dimensions of AI play in engendering trustworthiness AI is a social technical system people increasingly interact with AI through material artifacts with in spatial environment the quality of this interations are hypothesized to moderate trust Here, two different kind of questions can be raised The first, which is a primary question goes as stated. What other conditions must be present beyond the technical features of explanability for AI to be substantively trustworthy. and following that the secondary question In what ways do these conditions impact trust of AI and a trustworthy AI To answer these questions, It is necessary to begin with a foundational question of what is trust examining how trust has been studied in the fields of philosophy and the social sciences. What is Trust? Trust is a vast archipelago of many ideas in philosophy and the social sciences. The focus will be on the work of Baier and Hardin because of their comprehensiveness and reach they have been thinking on trust as evolved extensively over many years. Do both Baier and Hardin, trust is a three parts relationship. So what this means is that A trust B to perform a task C. For Baier, trust has to entail some degree of reliance, even though not all forms of reliance require trust to qualify as trust. This Reliance must be characterized by a goodwill to look after the interests of those who trust For Hardin, trust is expectation for the right reason To understand trust, is to know the reasons for trustworthiness. One common aspect of trustworthiness is competence to be capable of what one is expected to do. Furthermore, this competence is driven by a motivation too, that. the trusted is motivated to perform what he or she is deem competent to do. This is also known as Hardin's encapsulated theory of trust. From Baier, It is possible to say that trust emerges in scenarios characterized by reliance and some vulnerability. From Hardin, competence and shared interest as motivation have to be present in order for trust to exist. This small study shows a) there are the conditions beyond explanability that must be fulfilled for trust to exist and b) these conditions characterized by reliance, vulnerability, competence and also motivation are social by nature while trust relations between AI and people are not likely symmetrical to people-to-people trust relations nevertheless AI is increasingly becoming a social technology where factors of reliance, vulnerability, Competence and shared interest begin to compliment explainability towards the constitution of a more trustworthy AI. Here, we explore the idea of using human computer interaction to gain or to attain trustworthiness by design. Designers play important roles in influencing trust because they specify the ways in which users interact with AI-powered artifacts. There are at least five features through which design can influence trust, appearance, ease of use, communication style, transparency and feedback and the level of control among other design goals, This feature reduce vulnerability of the users by heightening control and offering the appearance of competence, especially in the form of a feedback. Yet design features tend to improve the perceptions of trustability. For example in offering high levels of control. They do not necessarily coincide with actual trustworthiness. Which is a substantive quality of reliable technology observed through various users across time more research is required to advance on the notion of trustworthiness by design in HCI. Following that, we explore the use of AI ethics as a way of building social trust The rapid rise of AI has prompted the formulation of many regulatory guidelines for ensuring greater accountability in AI practices globally. There are at least 84 documents including ethical principles or guidelines for AI. One primary purpose of ethics embodied by these AI ethical codes and principles is to explicitly state moral tenants that should not be violated. AI practices that subscribe to this quotes and principles where enforced can operate within a highly regulated setting which reinforces trustworthiness However, the system of AI ethics systems remains underspecified for existence should AI ethics be specified in terms of a handful of foundational principles? which will nevertheless be contested across different cultures. Or should it be grounded in the UN Universal Declaration of Human Rights which has ostensible universal warrant? This choice among others is an institutional design decision, which invariably will impact citizens' trust on AI Technologies with this I conclude my presentation. Thank you. 