 To explain the identification of jets by a neural network that has been trained on jet images and expert augmented variables we will take the output of that network and propagate it back through to the input. To begin let's take a look at how information moves through a neural network. A typical neural network takes some input which it propagates through the network layer by layer by passing the information between nodes as a function of the previous node's output, the weight of the connection and some bias. This propagation continues until the information reaches the final layer where a decision is made. In this trivial case, for instance, the network has identified the image as containing a castle. If we train this network with a loss function, we'll now have a tool that identifies a set of data within its allowed labels. However, we do not know what the network has based its decision on. Let's take a look at another trivial example. Here we have two sets of data that we want our network to distinguish between. If we train our network only on what is shown in the plot, knowing that the rest of our data sets will fill the ovals of corresponding color to the squares and circles, we will find that our network can settle into multiple states that accurately identify the initial data. However, we will see that not all of these trained networks will accurately distinguish the full dataset. We want to ensure that the network has learned meaningful patterns within the information provided. To achieve this, we employ a technique known as layerwise relevance propagation. LRP takes the output of a network for a specific piece of data, and uses this as the relevance of said decision, which it will propagate backwards through the trained network. The calculation of intermediate node's relevance scores conserves the total relevance of the final decision. Once the relevance has been propagated through the entire network back to the input, it is distributed among the features of the input. For images, this gives a heat map of the features, highlighting those that impacted the corresponding decision most. We'll now consider a toy model that resembles particle jet data, and our goal with this is to verify that lrp can work for this type of classification problem. During a collision event, a particle jet can be described by a jet image [that is] created from the data of the detector. For our simplified case we generate signal-like and background-like jet images constructed from simple gaussian functions. We assume two leading subjects in our jet with radii r1 and r2, momenta z and one minus z, and theta [which] describes the angle between them. These are the parameters used that form our toy model jet images. The data is then given to our network. The images are first given to the convolutional layer where a filter is passed over the image, extracting features as it goes and thus convolving the image in its output. The max pooling layer then highlights the most significant features of the image and removes a significant portion of the white space. The output of the pooling layers is then flattened to be passed to a dense layer with a ReLU activation function. And finally, the network's prediction is output. The toy model is simple enough that the network performs with essentially perfect accuracy just from the image data. To explain the network's reasoning for the answer it gave, we apply LRP. The network's prediction is propagated back through the network where a relevant score is assigned relative to the input. In the case of images this means that the significance of each pixel is measured. The final output of LRP is a heat map of the original image, showing the pixels that were most significant to the network's decision. Now we will look at the result of the LRP analysis on the images from the toy model. Looking at the heat maps for the composite images: blue highlights the pixels that lead the model towards its classification of signal or background, while red highlight the pixels in the image that are leaving the model away from its chosen classification. The signal jets have a larger separation in theta than the background jets and the LRP heatmaps show that the presence of a second subject was useful in its classification of signal, while the activity in the center was useful in the classification of background. We can zoom in and look at the LRP weights for individual events as well. A similar process occurs here where the LRP heatmap shows that separate activity in the images has led the model towards the classification of a signal event, and central activity has led the model towards a classification of a background event. Since this is a trivial toy model, the model has perfect performance with only the images as the input. Adding expert variables is not necessary for improving classification; however, we can add the expert variables and then use LRP to determine what the model finds the most useful out of whatever inputs it is given. So we've run the LRP analysis on the toy model with the expert variables now added as inputs to the dense layer. The weights from the analysis are then summed for each input and compared. The rock curve of this model and the model with just the images is the same, but LRP shows that the z variable, which has good separation for signal and background, is given the most importance by the model, while the images are now given very little importance. We can then perform a similar analysis, but with a particle level simulation. Using what we have learned from the toy model, we apply the same method to particle level simulation generated using pythia. We are using zjet production as our signal and light jet production as our background. The simulated particles are clustered into jets using anti-kt algorithm with distance parameter 0.8. Only the jets with transverse momentum greater than 200 GeV are considered. The n-subjettiness variable is calculated with Fastjet Contrib using the Winner Take All axis with kt configuration. The selected jets are groomed using soft drop with beta and z parameters set to 0 and 0.1, respectively. Once groomed, we end up with two subjets inside the jet. The particles inside the jet are then projected onto the eta-phi space and the leading subjet is centered at the origin. Then the particles are rotated and the distances between them are scaled such that the subjet one is located at (0, 1). The intensities are normalized through the transverse momentum of the jet, giving us the jet images. We use the 2D CNN with jet images as input along with the expert variables which includes: the kinematic information of the jet, the particle multiplicities, n-subjettiness variable, and the jet pull angle which is used to quantify color flow inside the jets. The images pass through the convolution layers, max pooling layer, and flattening layer. At this stage the expert variables are concatenated into the flattened convolutional output. These are then fed to the dense layers and a prediction is made by the softmax- activated output layer. Now it's time to use LRP to explain the prediction. Starting by propagating the prediction backwards using LRP propagation rules and ensuring the relevances are conserved, we propagate the relevances back to the input layer giving relevance values to each input pixel. In the heat map shown here, the pixels that led the network towards the decision are shown in blue and the pixels contradicting the decision are shown in red. For the images predicted as signal,  the network is able to identify separated activity inside the jet, placing high relevance on pixels around the origin and (0, 1). For background, the network is expecting centralized activity and is putting negative relevances on the pixels around (0, 1). The LRP also propagates back to the expert augmented variables and we're able to calculate the importance of each variable in making the decision. Let's take a closer look at the three most significant variables: the jet mass, ratio of n-subjettiness two to n-subjettiness one and charge hadronic multiplicity. For these variables, we can see the histograms for signal and background, and their profiles with the relevance scores. We see high relevance scores for variables that have the most physics significance. It is clear from the profiles that LRP is able to provide insights into the network's behavior. These XAUG variables are able to capture known features in the network and they can be utilized to improve the network performance, while simplifying them to exhaust the available information. In our upcoming paper we apply this method to a deep-ak8-like tagger, with particle list and flavor information as input. This method can also be used for anomaly detection and to explore new expert variables. 