 (upbeat music) Hi, I'm Raman Kalyan from the Microsoft 365 Product Marketing team. And I'm Talhah Mir from the Security and Compliance Engineering team. All right, welcome to episode one, where we're talking about using artificial intelligence to hunt for insider risks within your organization. Talhah, we're gonna be talking to Robert McCann today. Yeah, looking forward to this. Robert's been here for 15 years, crazy-smart guy. He's an applied researcher, a Principal Applied Researcher at Microsoft, and he'd been like a core partner of ours, leading a lot of the work in the data science and the research space. So in this podcast, we'll go deeper into what are some of the challenges we're coming across, how we're planning to tackle some of those challenges, and what they mean in terms of driving impact with the product itself. I'm excited. Let's do it. Let's get it. Robert has been focused on the insider risk space for us for, Robert, how long you've been in this space now? I've been doing science for about 15 years at Microsoft. Insider risk, about a year, I think, Talhah, something like that. Yeah. Nice. And so what's sort of your, what's your background? So I am an applied researcher at Microsoft. I've been working on various forms of security for many years. You can see all the gray in here, it's from that. (laughs) So I've done some communications security, like email filtering or attachment, email attachment filtering. I've done some protecting Microsoft accounts or users' accounts, a lot of reputation work. And then the last few years I've been on ATP products. So basically babysitting corporate networks, looking to see if anybody had got through the security protections, post-breach stuff. So that's a lot of like machine learning models across that whole stack. The post-breach thing is a lot about looking for suspicious behaviors on networks or suspicious processes. And then the last year or so, I wanted to try to contribute to the insider threat space. What does it mean to be an insider or to be an applied researcher? An applied researcher? That's a propeller head. So we all know what propeller heads are. (laughs) Basically, I get to go around and talk to product teams, figure out their problems, and then go try to do science on it and try to come up with technical solutions. You know, AI is a big word. There's a lot of different things that we do under that umbrella. A lot of supervised learning, a lot of unsupervised learning to get insights and to shift detectors. So I basically get to do experiments, see how things would work, and then try to tech-transfer to a product. You said you spent most of your time in the external security space, thinking about That's right. Things like phishing, ransomware, people trying to attack, you know, us from the outside. How is insider threat different? Like, what have you found, like to be, "Wow, this isn't what I expected" or here are some challenges, or here's some cool stuff that I think I could apply. Yeah. It's a very cool space. Number one, 'cause it's very hard from a scientist perspective. Which I enjoy. So the first thing that you hit on, that's really the sort of fundamental first thing that makes it hard is that they're already inside. They're already touching assets. People are doing their normal work and the inside threat might not even be malicious. It might be inverted. So it's a very challenging thing. It's different than trying to protect a perimeter. It's trying to sort of watch all this normal behavior inside and look for any place that anybody might be doing anything that's concerning from an internal assets perspective. So when think about somebody doing something challenging, is it just like, hey, I've downloaded a bunch of files. 'Cause today I might download a bunch of files and tomorrow I might just go back to my normal file thing. But if I look across like an organization, the size of Microsoft that's 200,000 people, that could probably produce a lot of noise, right? So how do you kind of filter through that? So actually the solutions are right now in the product and what we're trying to leverage to improve the product are built on a lot of AI things. So there's very sophisticated algorithms that try to take documents and classify what's in those documents or our customers might go and label documents. And then you try to use those labels to classify more documents. There's a lot of very sophisticated, sort of deep learning, natural language processing stuff that we leverage. And those are very strong signals to try to, to see, okay, this behavior over here, that's not so concerning, but this behavior right here, that's a big deal. Now we need to fire an alert or maybe it's a little more of a deal, but then I sorta got some sentiment based on what, how the person's doing, the employee. If I combine those things now it becomes compelling. It's a very hard noise reduction problem. As you were talking, Robert, one thing that sort of, you know, occurred to me is I've had conversations with customers and you mentioned this around leveraging, you know, artificial intelligence and learning and helping the system learn. A lot of questions I get from from customers like, what is artificial intelligence in this context? And how do I know that, you know, this is something that I should trust or how is it different than maybe what I'm doing today? I've seen this play out and time again on many, many times that sort of a security team has tried to start leveraging AI to do smart detections. It's a very different game. It's not, I have precise detection criteria, and if you satisfy that, then I understand what I did and I understand the detection. It is a very statistical machine that sometimes you have to assume it's going to make mistakes. So one key thing you need to be able to do to trust that machine is you need to measure how well it's doing. So you have to have a way to babysit the thing basically. And you have to set your expectations to understand that there is error going to happen, but there has to be an error bar met. So that's basically what you're babysitting against. Another very key thing is when it fires a detection, that thing can't be opaque. It needs to explain how in the heck or why in the heck it thinks that this thing is a threat, right? So the deep learning folks like for image classification or natural language processing work, they sort of jumped on board real fast with the deep learning thrust, without really worrying too much about being able to explain why that thing was classifying images the way it was. And they were ecstatic because they're getting so much better results than they've gotten like the decade before, right? But then it came to the point where they started realizing, hey, I can game this thing and I'll prove it to you. And then, you know, you take a picture and you change a few pixels, and then I make that thing classify the cat as somebody else. When you use a camera for detecting people, facial recognition and identity verification, that becomes a serious problem. So they sort of went under this phase now, and it's very hot right now. Can you do these sophisticated models that also can explain why they did what they did. And so there's a ton of science and a ton of work trying to crack open the black boxes, right? Those big sophisticated learners. But you don't have to go to that phase. There's all this other AI that works very, very well and is very effective. And I would say is probably the most common stuff that's used and delivers the most value in industry that's not so opaque. And you can, you, the models are simple enough or I guess opaque enough, or they're explainable enough that you can tell the customer, I detected this threat because this and this and this happened, right? So explainability is very key to try and to trust AI. That brings up another sort of key question we get from customers a lot, and it looks like you're, you know, this idea of transparency in the model or explainability in the model is like, again it's a key attribute, right? So it looks like we're learning from years and years of kind of data, science, and research in this space to apply that into models that we build. So can you talk about that a little bit, in insider risk what do you think constitutes a good model? What kind of explainability should be in that model so we can help our customers make the right decision on whether something is bad or not? Well, you have to put on the customer hat, which sometimes it's hard as a scientist. Like a scientist might be satisfied saying if the explanation for some prediction by some model is, you know, feature 32 was this far away from a margin, okay. So there's some technical explanations why a classification might happen. But the customer, they just want to know, you know, what are the actually human actions that caused that? You gotta a have a model where you can, and simple enough features, where you can boil it down and say, this person's suspicious because they printed this document that's highly confidential. And then they did it again two days later, and then they did it again three days later. And then they did it again four days later. And you have to have that very human intelligible output from your model, which is something that is very easy to skip, if you don't have explainability top of mind. You have to pick the appropriate technologies. Yeah, 'cause it's really about trying to abstract the way all the science behind the scenes, right. That should just be, we should just be able to easily explained to the customer here's what we saw. How we detected it should be irrelevant to them. Here's what is happening with this potential actor. Let's go make the decision on how to manage that risk. Yeah. And I think that's that, that is the sort of the key here, right? As you think about, there's the tech, which is how do I detect, try to detect these things? And then there's the person consuming the output of the tech, right? And typically the person consuming the output of the tech is somebody who may be in HR or maybe legal, maybe there's, yeah, it could be a security analyst, but they have to interface with HR and legal, and, you know, they may not be as sophisticated. Like I'll, look, I'm not, I'm technical, but I'm not as technical obviously as Robert and probably you, right? And so I don't want to go deep dive into some algorithm to try to figure out like, well, what's going on here. I want to just know like, hey, the risk score of this individual is high. And here's the related activity that the system found. And this is why you should kind of believe it. Yeah. In fact, we've seen this from our customers. We've seen this in our own experience in that the people that have to make the timely and informed decision on how to manage insider risk is oftentimes the business or HR or legal. Right. Right. They don't want to get into the technical details behind the model that was used or this, that, or whatnot. They just need something that's easy to understand in business terms, so they can make that the determination of what needs to happen. Raman and I were just on a call with a customer earlier this week. And they raised this question on why can't we do supervised learning for these detectors? So I'd love to get your thoughts on some of the challenges, or maybe some of the opportunities or how you're looking at the types of learning models that you use for these detectives. One of challenges is how much context it needs. And if you want labels, you gotta be able to take and give that context to the customer when they have alerts, right? They need to be able to accurately say, hey, this alert's right. And it's easy for me to tell that, and I could do it in an efficient way because the product just gave me an explanation. Now, once you're able to sort of explain yourself and you're supposed to, and you're able to give it to the customers so they can efficiently triage, now you're starting to crack open this sort of virtuous cycle where they can start giving you labels and you can pull them back in-house and you can start learning how to do supervised classification on this stuff. It's very key, you need this sort of label generation mechanism, right? So that's key for opening supervised learning. But it's also key in that insider threats can be very subjective. So one tenant can want to see the same activity and another tenant might say, eh, that's not important to me. Don't tell me that please, that's noise, right? So now you gotta be able to do classification that's customized per tenant, right? And each tenant doesn't want to go in and fiddle with all your AI and make it work just right for them. An easier way for them to express what they want is to give you feedback. So we explain detections, they give us feedback, and now we can start learning, okay, supervise model works for these types of customers. This other supervise model works for these types of customers. And then we can sort of get this customization game going as well. But all of that and all of those supervised learning techniques, they rely on labels. And you gotta do a good job explaining to your customers to get that feedback. One question Robert, so I also get is around, today, a lot of the tools or a lot of my detection capabilities are reactionary. I got fired or I'm not happy and I downloaded a bunch of stuff and I'm out of here. I resigned. Right? But prior to that, maybe a month prior, or maybe it's four months prior, or even three weeks prior, there might've been some activity that was happening that might've indicated that I was about to do it. Can you help me predict, can you help me be more proactive? And I think, again, I go back to like, this could be, this is a spectrum of things, right? We're not going to know like today is Talhah bad? Tomorrow? Probably not. Right? But it could be like, hey, review time's coming up, didn't get the bonus he wanted, he's been working on insider risk for the last two years. And now it's like, okay, I'm outta here, man. I'm gonna go somewhere else. So I guess the big question I want to ask is like, how do we answer that for question, for customers, right? When they ask us that, what would be your answer? There's something here and Raman, I think you sort of hinted at it is that there's past behavior that we could look at and we could say, okay, from our past experience, this sort of sequence, like 10% of the time end up with something that we didn't like. So if we see that in the future, let's do that again. So I actually, you know, on a technical side, we're doing a lot of work on sequential pattern mining. And it boils down to just that. What are sequences of activity based on the type of context that Talhah mentioned, it might be sentiment, or it might be something else that tend to lead up to things that, in hindsight, we knew were bad. Okay, so we're gonna use that to predict in the future. But there's also stuff we, you know, that maybe we didn't see before. So maybe we also look for here's some machinery that today, here's sequences that are totally abnormal. Let's go get somebody on 'em and let's look at that and let's start get that labeling loop going on that so we can understand if that sequence is good or bad so in the future we can protect other people with the same observations. But your question about preemptiveness is as a good one. And I think sort of the sequential mining aspect, very fine from a technical standpoint. And I think it'd be very valuable for our customers, for sure. I mean, 'cause I think that, you know, what this is highlighting for me from a tech perspective, you know, and I'm a marketing guy, so, you know, I'm about selling it, selling the story. But as I think about this, what becomes very clear to me is that you can't just use one thing, one signal. You can't just be like, oh, somebody is on an end point, and they tried to copy something to a USB and that might be bad. There is multiple things going on, right? There's sentiment analysis. There might be, you know, other activity, it's who they're talking to, how many times they're trying to access stuff. Did they come into a building when they shouldn't have been in the building? All of these different elements can come into play and to, you know, Talhah's earlier point, you know, it's really about like, because we're dealing with employees, you have to, you can't assume that everybody is bad, right? It could be like, wow, I couldn't get my PC to turn on at home. So now I gotta go to the office and do it there. Maybe that was in the middle of the night. I don't know. So but I think that's the big challenge in this space from my perspective is that you just can't rely on one set of signals. It has to be multiple signals and the machine and the machine learning is key to sort of really driving an exposure of like, this could be something that you might want to take a closer look at. You're always going to have a human element, I guess, right? Well, it's actually true. In fact, this reminds me, there's, when we're sort of establishing the program at the company, there was a lot, we had a virtual team put together and we're trying to kind of ground ourselves on a principle. And one of the guys on the team actually proposed something that just stuck, which is, this program should be built on the principle of assume positive intent, but maintain healthy skepticism. What that effectively means is you just follow the data. That's it. Don't start off thinking everybody's bad. Don't start off thinking you're gonna catch bad guys. This is about looking at the data, as much of the data, as much of the context of Rob's point, and just follow that until you get to a point where it's like, this looks odd, this looks potentially risky. Right. And then you take that information, you surface it for the business, with the right context, right explainability in the model, so that they can make the decision. I think presenting that in a way that allows you to make that informed decision, it does two things. One, it gives you the ability to kind of say, hey, this might be bad for me, but two, it also allows you to filter out the noise. To say, hey, not everything is bad, because what I also hear is like, you know, I'm done with, let's imagine like using a data loss prevention tool to try to detect insider risk, right? Right. That's challenging because A, that's just one set of signals. It's a very siloed approach. And B, you're gonna be overwhelmed with a ton of alerts because it's very rules-based, right? Kind of once you're using all this machine learning type of stuff. Yeah. So how do you prevent alert fatigue? And I think that's where you need this combination of signals to not only look at what might be potentially problematic, but presented in a way that you can then make that informed decision. So Rob, one of the things that, you know, as we look forward, there's a number of different types of detections that we could potentially look at. You know, one is a sequential modeling. That's an interesting one and we'd love for you to explain about that. The other one is around, you know, this concept of low and slow. From what I understand, it's not about this big burst of I come in today, I download a thousand files, and I'm outta here. It's more of over the next six months, I'm now a little bit irritated and over the next six months, I'm gonna download a file here and a file there, 10 files here. I'd love for you to kind of deep dive into that. Yeah, I mean, those are the really interesting cases, right? Those are the people that are being very stealthy, right? And people that we want to try to detect. It's a little bit different of a game. So like you said, the bursty stuff, I mean, did they do something abnormal to themselves or did they go over some globally agreed upon threshold that this thing is just bad behavior, right? That's a different game than looking at somebody who's trying to stay under the radar and taking longterm. You gotta model things a little differently. Number one, you got to look at longer history. I'm not looking at bursts of daily activity. I'm looking at what they've done in the longterm. So now you have engineering issues because you got to have the scale to look at everybody's rich, long history. But then after you get that, okay, I'm monitoring somebody. It's very hard to tell, I mean, you've looked at, you know, stock market charts where there's two very flat, how do you tell the difference between two flat lines where one's a good investment and one's not a good investment? It's hard, 'cause it's low and it's slow, right? The behavior is subtle. One thing that we're looking at is how can we tighten the screws when we do anomaly detection, right? So it's easy to tighten anomaly detection to the level of detecting a burst. Okay, you can do that, right? Now we want to tighten anomaly detection to the point that we can pick up two flat lines and tell the difference from good behavior and bad behavior, right? What does normal mean? I mean, normal has gotta be right in between those two. How do we find that normal, right? So the way that we're doing that is we're modeling people based upon what's normal for groups of similar employees, right? Does this employee, how tight can we say what's normal behavior for devs so that we can have a model that looks at low and slow and normal work behavior for devs and low and slow, little bit worse than normal behavior for devs and pick that apart. So you just gotta do tighter anomaly detection. You got to compare them to groups that's gonna give you a definition of normal behavior that's tight enough that you're gonna be able to pick out, even though they're low and slow, you're gonna be able to pick out the different behavior over a long period of time. So Rob, just for, just a couple of fun things on the side. So being a longterm researcher, what are some of the pet peeves or some of the things that really have annoyed you about maybe some of the product pitches you've seen or they maybe over promise or the way they position AI, right? I'd love to hear some of the stories that you have and what just kind of, just gives you the shivers. As scientists, we have a community, and we go talk to each other and you get to know people and you figure out what's really behind that magic sauce. And sometimes it's not as impressive sounding as the marketing. So that means the marketing's doing a good job, I guess. Right? But that's sort of a pet peeve from a scientist standpoint. I mean, good signs that you should see to sort of prove that stuff out is, you should scientific activity. If they say they're doing good science, they probably have scientists working for them. And if they have scientists working for them, then those scientists like to do things like publish or make patents or go out and you should see some scientific evidence happening there. I think that's sort of a telltale sign. So that's one pet peeve. Overselling how much is it going on there. Another pet peeve is this idea that machine learning or AI is a magic bullet that you just throw stuff at and it magically gives you exactly what you want. It doesn't work that way. Computers are basically just big, really fast calculators, right? And we've figured out some algorithms that they can look at some data and pick out some patterns quickly, but that's what they are. They're pattern finders. And the scientific community has been clever in how they take that sort of big fancy calculator and put it into making some business decisions that are crucial and stitching them together. Like we talked about, you know, here's a module that does sentiment analysis. Big fancy calculator, right? Here's a module that does confidentiality of the file. Big fancy calculator. And then there's all this business stuff that comes in that has to stitch that together to make a good decision. It's not just the AI, it's the stitching together in the appropriate ways that solves your business problem that's really the magic sauce, right? So that's another pet peeve. Like you just throw stuff at AI and then you suddenly got a million dollar business. It doesn't work that way. You've got to put these components together and work hard on them cause they're challenging, but you got to stitch them together correctly. It's the whole ecosystem. And that's actually a good, that's an interesting point, Robert. I mean, I like that because it's like, because in a way, what you say is like, okay, I took these like, imagine I'm on creating clothing, right? And I've got different types of fabric, different types of zippers. And I stitch it together and I produce it and it's like, hey, here you go. Here's your shirt. And somebody says, you know what? I don't like it that way. I want to be able to stitch it in a different way. And, or if new fabric comes out, I'm gonna use that in new types of clothing. And I think this is what to me is interesting about what you just said, which is you've got these different calculators that are looking at different parts of the puzzle, right? Taking different signals in. And then the secret sauce is how do you stitch together to produce something that you might want to consider as being an anomaly or abnormal behavior, but then be able to provide feedback back into that calculator to say, hey, I didn't like that or this didn't work for me, stitch together somewhat differently. Yeah, you're right on. I mean, there's like we talked before about how do you trust these black boxes? It's all that logic that babysits it. And like, you got to have some guardrails in there so the thing doesn't go off the rail and mess up with everything else that you're stitching together. It's that sort of business logic on top that's super, super valuable. And just as impressive to me as the AI under the hood to tell you the truth. So Robert, appreciate you being here today. This has been great, great conversation on the tech. As you think about, you know, the future and where we go from here, where we see ourselves, you know, in five years from now, what are your sort of projections in terms of what might be different than what we have today? Yeah, that's a great question. I think some of the big thing is, you know, solve these sort of challenging tweaks, which is like, you know, Talhah mentioned multi-users, we still have multi-users. We get enough anomaly detection that we can pick off the low and slow, even differentiate that. But I think one thing would be super powerful to get to is if you get this sort of feedback coming, right? 'Cause once you get this feedback loop coming, then you crack open the AI door for all kinds of algorithms. There's a lot more supervised stuff that we could use. and we could leverage that would make us even more powerful, which would give better detectors to people, which would give us more labels to get even more powerful. And when you sort of get that sort of mutual synergy going, I think the detections, they skyrocket. And then one other thing like, the industry is not really, hasn't really like, like external attack space, industry has these threat matrices, right? And they sort of have those benchmarks that they're trying to work against. And they're writing down simple rules to detect that. And they're using sophisticated AI targeted at known bad behaviors. I would like to see that sort of landscape roadmap start happening in insider threat space as well. 'Cause it's going to prioritize what we do from a product standpoint and also from a research standpoint, and it's gonna be an input to our models. Hey, this is known bad stuff. We better be able to detect that, stitch things together to detect those sequences. Thanks again for being here. Talhah, always good to talk to you, man. And I know we have another episode coming up with, I think it's Dan Costa from the Carnegie Mellon, right? That's right, looking forward to that very much. and Robert again, appreciate it, man. Yeah. Excellent. Thank you so much Rob, thanks for- Thanks for inviting me. Tell Dan I said hi. Yeah, I will. Talah, that was an amazing conversation with Robert Yeah. That guy is like surreal. I told you. I know, it's amazing. Well, if you enjoyed this, we've got another podcast coming up with Dan Costa from the CERT Institute at Carnegie Mellon. Dan is engaging with a lot of players like the NSA and Secret Service. So it's gonna be great. Definitely subscribe and continue listening. 