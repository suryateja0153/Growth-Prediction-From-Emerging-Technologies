 getting explanations for your predictions is becoming increasingly important how can we use Claudia platform's prediction service to generate explanations stay tuned to find out [Music] welcome to AI adventures where we explore the art science and tools of machine learning my name is Yu Feng Guo and on this episode we're going to explore AI explanations which is now built into cloud EAJA platforms prediction service allowing you to have feature attributions for each and every one of your predictions AI explanations integrates feature attributions into AI platform prediction and it helps you understand your models outputs for classification and regression tasks this means AI explanations can tell you how much each feature in the data contributed to the predicted result this is super useful because you can then verify the model is behaving as you would expect recognizer bias as it's happening and get ideas for how you can improve your training data and model feature attributions are available for tabular data as well as image data and are specific to individual predictions currently there is the limitation in AI explorations that it only supports models trained on tensorflow 1x and if you're using carrots to specify your model you will need to convert it into an estimator using the model to estimate or utility now let's talk a little bit about the particulars of feature attribution and how it works ai explanations offers two methods sampled Shapley and integrated gradients both methods are based on the concept of Shapley values which is a cooperative game theory algorithm that assigns credit to each player in a game for a particular outcome here each feature is treated as a player in that game and proportional credit is assigned to each of those features for the outcome of a prediction integrated gradients are best suited for differentiable models like neural networks it's especially useful for with large feature spaces it computes the gradients of the output with respect to the input multiplied element-wise with the input itself for those who remember their calculus class this is essentially a Taylor approximation of the prediction function at the input bet you didn't expect to see Taylor approximation reference today now let's talk briefly about the other feature attribution method sampled Shapley it assigns credit for the outcome of each feature and also considers different permutations of those features and is most suitable for non differentiable models like ensembles of trees there are a number of articles and research papers which dive way deeper into these explained ability methods which i've linked to below they're all very well-written and super interesting to read so if you like these topics I would encourage you to dive in and check them out if you are ready to try out AI explains for your deployed model head on over to the guides in the documentation there's one for tabular data and another one for image data and best of all they're presented as collab notebooks so it's super easy to try it out one final note before we wrap up last episode we talked about the what-if tool an open source tool for understanding your models predictions AI explains can be used in concert with the what-if tool to get an even more in-depth understanding of your predictions the process for doing that is also detailed in those collab note books I mentioned earlier and I'll be sure to link to it in the description below thanks for watching this episode of cloudy adventures and if you enjoyed it click that like button and be sure to subscribe to get all the latest episodes right when they come out for now check out AI explanations on google cloud a a platform to get your predictions explained [Music] 