 hello my name is sajnar simmon i'm a product manager uh in google cloud for cloud ai notebooks today i'm going to walk you through an overview of cloud ai notebooks our managed jupyter lab notebook service for enterprises on google cloud today we'll explore how cloud ai notebooks can help you explore data quickly and develop an ai and ml model and deploy it into production artificial intelligence and machine learning is are one of the most are one of the key disruptive technologies that enterprises have encountered in the last four or five years and ai and ml will continue to disrupt enterprises going forward for the next 10 years the key to unlocking value with artificial intelligence starts with a model and the simplest way to develop a model is to use notebooks and within notebooks to use open source frameworks to develop uh ai models uh tensorflow pi torch and many other oss distributions exist today that can help you quickly develop a model and gain insights as a data point today more than 8 million jupiter notebooks run on github and this node this notebook account only is going to continue to grow google has been in the forefront of bringing notebooks to different segments of customers over the past decade kaggle for example is targeted at enthusiasts and people who are learning to be data scientists colab is targeted at individual researchers and students and it provides a collaborative environment for developing data science algorithms ai platform notebooks are targeted towards enterprises this means that air perform notebooks come with security built in which enterprises really care about today we're going to spend some time looking at ai platforms and how it can speed up your model development so as we noted enterprises want insights in days not weeks this means that data scientists want to focus on the difficult job of model development and not essentially the peripheral task that goes around running a notebook and the infrastructure associated with a notebook enterprises encounter several challenges in running notebooks and open source frameworks the first thing is around managing open source frameworks and making these making the operating systems the frameworks the dependencies work correctly hosting notebooks and maintaining them on top of these frameworks is an additional pain point customers also want to make sure that the access to the data that data scientists use and the models developed from the notebook themselves are secure and they're only accessible by the right personnel since ai and ml involves accessing data across the enterprise landscape it is important for a notebook service to connect with all of the data sources that reside within uh an enterprises data estate our ai platform notebook service solves these problems by providing three key features we completely manage the installation of the jupyter notebook environment along with all the open source frameworks and dependencies so you don't have to manage any of that lifecycle our notebooks also ship with uh security features like vpcse uh cmac and other gcp horizontal features built-in and clari notebooks is also integrated with several data services like bigquery and dataproc that we'll dive into a little later uh notebooks are really easy to get started all you have to do is to go to the gcp console um find notebooks under the ai platform notebook create a new instance with the required deep learning framework and the type of vm that you want to run the notebook on so what makes cloud ai notebooks a fully managed experience what we do as part of claudii notebooks is to pre-install all the client libraries in all the environments we support popular open source environments like tensorflow pytorch and mathematical libraries like numpy and scipy to plot we also provide open source tools like matplotlib built right into our deep learning vm on top of these deep learning vms our notebooks run and they take advantage of these deep learning libraries customers can configure the notebooks to the cloud console however they can also get a link to access the jupyter notebook using a proxy outside of the gcp console clari notebooks are designed to work with different kinds of compute instances within gcp um cloud ai notebooks can run with dlvms as we just spoke about these are gce instances that come pre-installed with deep learning frameworks claudia notebooks can also work with dataproc clusters and dataflow ai claudia platforms also work with custom containers so if you had a specific deep learning library that you want to run within the custom container you can do so and install a notebook on top of these containers we will talk a little bit more about our data proc integration in in the slides that follow claudia platform also works with data sources within a gcp like bigquery so you'll be able to query data that's stored within bigquery and use that data to train your models let's take a look at how ai platform notebooks are architected ai platform notebooks are designed to work with a variety of compute instances we just talked about how it works with deep learning vms which are gce container gce instances with pre-installed deep learning frameworks within the vm ai platform notebooks also work with data pro clusters and data flow ai platform notebooks also work with custom containers which means that if you have a special deep learning framework that you wanna run you can create a custom container out of it and have ai platform notebooks work with those containers ai platform notebooks work with a variety of data sources within gcp for example you can query data from bigquery use that data as the basis to train your model ai platform notebooks are also well integrated with git repositories and other source control systems so it's easy to push and pull code into these repositories so why would you want to use ai platform notebooks air platform notebooks makes it really easy to get started with your model development since you don't have to deal with installing these deep learning frameworks or setting up the notebook environment you can get started very very quickly in addition since ai platform notebooks is a managed jupyter lab service the environment around code development is very familiar for data scientists ai platform notebooks enabled you to scale and control costs effectively so you could start with a very small instance for model development and as you ramp up your training you could move to a bigger vm or better yet you could use a gpu or much powerful machines in order to train your models ai platform notebooks comes with security built in so as we talked about uh vpcse iam controls as well as cmac controls are built right into ai platform notebooks ai platform notebooks are also integrated with other services within gcp so it plays within an ecosystem of analytic tools as well as our ai platform so ai platform makes it really easy to build train and deploy these models let's talk about some recent updates that we have made to our notebook service in the last six months we recently announced an integration with kaggle kaggle as you may know is a community of five million data scientists who write and share code with this integration between kaggle and ai platform notebooks customers have access to gcp's limitless and customizable compute environment that allows users to scale up their work very easily we've already seen thousands of customers from kaggle go to cloud ai notebooks uh using this integration and we expect these numbers to grow in the next few months one of the recent integrations we announced was with dataproc uh through the smart analytics framework the smart analytics framework brings together apache beam and dataproc together with ai platform notebooks customers typically need to analyze petabytes of data before starting to work on a model with this built-in integration with spark analytics and deep learning can be authored from one place the data proc hub brings enterprise controls to spark clusters based on notebook interfaces if you want to learn further about apache beam notebooks there's a fantastic tutorial when you launch when you launch a notebook using smart analytics framework and when you choose apache beam as you can see the experience to using smart analytics framework is super easy to get started all you would have to do is pick the smart analytics frameworks choice and either pick apache beam or data proc distributions instead of a tensorflow or a pi torch distribution as you would for a deep learning environment so the goal of cloudy eye notebooks is to help you develop models quickly and allow you to easily scale and streamline that model within your production workflows let's take a quick look at some of the examples around how you can go about doing that claudia notebooks provides a means to plug into your data pipelines that enables feature engineering as well as data labeling very easily cloud ai notebooks helps you model your data efficiently with what-if analysis and also uh embedded rml capabilities you can evaluate your model quickly with feature attributions what-if analysis and continuous evaluations we've also built in algorithms like optimization auto ml and nas right into cloud ai notebooks that makes model developments uh simple and faster on top of this uh claudia notebooks runs on a hybrid platform and a multi-cloud platform that also has access to some of the most powerful compute you will find anywhere in the world for example tpus cloudera notebooks makes it easy for you to scale and streamline your model development with ease cloud ai notebooks works in an ecosystem of data services and ai services cloud ai notebooks is can be easily integrated into your data pipeline so if you ingest data using data fusion or transform data using data proc and put it into bigquery you can easily access that data label the data and use that data for training as previously mentioned notebooks also makes it easy to build uh develop and train a model with built-in tools such as pre-built algorithms auto ml and access to our training service claudia notebooks also makes it easy to deploy these models by verifying your model performance using explainable ai as well as directly deploying the model into our prediction service our prediction service also ships with a continuous evaluation feature that helps you to monitor the model once it's in production all these components are built with access to a pipeline that makes it easier to containerize and monitor the entire workflow end-to-end one such tool that's built into ai platform notebooks is explainable ai explainable ai is a set of libraries that help you figure out why your model made a particular prediction for example on the right you can see a color-coded heat map of what the most important features were in making a particular prediction explainable ai can be used with tabular image text tensorflow models uh explainable ai also ships with a built-in what if tool when with a wardiff tool you could change the values for attributes that you're building with a model and figure out how much it impacts the prediction of that particular model explain explainable ai libraries are already available in some of our deep learning distributions today and they'll be available in more in the coming weeks of course ai platform notebooks runs in an ai optimized infrastructure within gcp for example our v3 uh tpu pods provide 84 percent faster performance in both object detection and machine translation compared to on-prem uh infrastructure this ai optimized infrastructure is only available on gcp among public clouds and they clearly our outperform on-prem systems in mlperf looking ahead at our roadmap extending beyond 2020 it's worth looking back on where we came from we launched ai platform notebooks in beta in april 2019 and since the time that we have launched we've seen tremendous interest amongst enterprises for developing models using cloud ai notebooks we've seen significant use of growth in adoption in the last year that helped us successfully ga the product in march 2020. beyond 2020 we hope to build in features around machine learning operations lineage tracking collaboration and integration across our entire analytics estate as next steps we invite you to read our notebooks manifesto blog that's available on the google cloud blog site as well as get started with cloud ai notebooks using a five minute tutorial that's available on our product page you can also learn about all the features and in-depth documentation on the product page on google cloud platform thank you for joining us today i hope you enjoyed this presentation and you have a good rest of the day and you enjoy our other next 20 presentations 