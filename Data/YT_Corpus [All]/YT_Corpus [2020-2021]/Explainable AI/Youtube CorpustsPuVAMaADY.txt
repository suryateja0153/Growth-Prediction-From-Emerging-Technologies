 really exciting to to start kick off the stanford's new academic year's hai seminar for those of you who are joining us for the first time hai is relatively newly established institute at stanford for human-centered artificial intelligence my name is fei-fei lee i'm a professor at computer science department and also co-director of hai with professor john h mundy and just very exciting that joining us today for the first kickoff seminar is one of the world's most famous renowned and also beloved member of uh stanford ai community uh dr angeline before i give a brief introduction about andrew let me just say a couple of words about hai is an institute whose mission is to advance ai research education policy and outreach to better human conditions we are a highly interdisciplinary institute here at stanford that works on very advanced advancing ai technology as well as many of the social and human issues related to ai whether it's fairness and bias ethics future of work geopolitics and and we work with important professional schools at stanford such as school of education uh business medicine and and all across the campus so uh for those of you who are part of the community we really invite you uh to join us in ev in any way uh whether you're students a researcher or faculty or or just alum for those of you who are joining us around the world we welcome you to sign up to our mailing list to our events list we hold a lot of exciting events and our weekly seminar is intended to bring you the latest uh thinkings by ai scholars and thought leaders who are at the forefront of making changes in ai so so um like i said it's so what an honor to to introduce andrew who has been a long-term friends for for um more than a decade it speaks of her age and you um that andrew was literally the first person i met when i joined stanford and he is the founder and ceo of lending ai also founder of deeplearning.ai and co-chairman and co-founder of coursera all of these entities are changing the world as we speak i can't imagine a person does all of them and is currently also an adjunct professor at stanford university he was a chief scientist at baidu and and part of the leading team of google brain project many many years ago when it was just starting and you know andrew will share with us his topic of bridging ai's proof of concept to production gap and before before we start our session today i just want to introduce also a really um important colleague of mine whom you'll become very familiar with as the the academic year goes and this is our new director of research at hai dr deep ganguly and he'll be sharing a few house rules with with everybody thank you and thank you andrew for joining us it says um i'll be very uh listening to your talk attentively yeah thank you and thank you and then before we we kick off just a few um house rules so first thanks everyone for tuning in today i'll be moderating the question and answer session um at the end of andrew's talk so to submit a question please use the slido website there's a link in the chat box of the zoom and you can also point your phone at that qr code or you can go to our events website at hei.stanford.edu events and click join the conversation and uh without further ado andrew please uh take it away great thank you thanks steven thanks dave yeah it was surprising uh when fede uh mentioned that aki is a relatively new institute because i think thanks to her leadership and john eshmendy's leadership uh uh at stanford in the ai world it feels like it was already a major institution so it's interesting to be reminded that despite aki's presence and all of these wonderful events i see across campus uh uh what the virtual campus all the time from hki is interestingly reminded that that this is just is this even just in the early days um so it's nice to see everyone here uh you know last night when i was looking at the attendees list i noticed that um all of you watching this a very diverse audience i counted 23 ceos on the attendee list there are a few dozen professors a few hundred students uh also a few hundred machine learning engineers and machine learning researchers uh and and one person in the registration phone listed themselves as a as a poet uh so whatever you are you know a cr professor a student machine learning engineer researcher or poet um i'm really glad to see you here today and thank you for joining us what i hope to do is um share with you a perspective on one of the challenges facing ai ai's created a ton of value but a challenge and almost a bottleneck of barrier for it to create even more challenge more value and this is something i see in multiple universities companies industries is bridging the proof of concept and and to production gap so let me share my slides and what i hope to do today is share with you a perspective on this proof of concept of production gap as well as what all of us in in academia or in business or or maybe even actually i saw a few government leaders as well um maybe they do to overcome some of these challenges so that ai can become even more useful so i've been saying for i think about five years now the ai's the new electricity uh similar to the rise of electricity about a hundred years ago ai is poised to transform every industry but what have we really done i think ai has already transformed the software industry especially the consumer internet industry um so you know we've transformed we as a community we've collectively transformed web search on advertising machine translation social media a lot of great things also some problematic things um but the software internet tech industry has many teams that know how to use ai well there's still a lot more work to be done but this class created tremendous value once you look outside the software industry um i think ai's impact is still modest um and and and and growing uh but i think that look into the future the impacts of ai outside the software industry will be even bigger than this impact on the software industry but the way we build and deploy ai systems um in all of these other industries will have to change a bit in order to make them more effective now uh given how diverse today's audience is i'm gonna take one slide to just say what i mean by ai so ai means a lot of things these days but you know as some of you will know 99 percent of the value created by today's ai technology is through one idea which is called supervised learning in which we learn input output mapping so if an email an output is this spam on on that's your spam filter or input an audio clip and i'll put a text transfer that's speech recognition powering voice search powering the smart speakers you you may have in your homes um most lucrative application of this is probably online advertising not the most inspiring application but certainly very lucrative for some large online app platforms in which you input have an ai system that inputs an ad and some information about you and tries to figure out if you click on the add-on because showing slightly more relevant ads has a very direct impact on the bottom line of the large online ad platforms um some work that my team at landing has been doing is visual inspection where we'll take as input a picture of a manufactured objects say a picture of a phone and we try to tell if this object that's been manufactured is scratched or gented or has had some other defect um or medical imaging uh at my my stanford group does a lot of work on um where we input chs x-ray image and output you know do we think this patient has uh has a has a pneumonia or some other condition so the ai world has generated a lot of amazing research progress a lot of amazing uh proof of concepts in the business world as well um for example this is uh one result that my collaborators and i had announced some time back uh this is with pranav rashberkar jeremy irvin matt lundgren kurt langlois buffet patel and many others um chest x-rays is one of the most commonly done medical procedures um you know we use it to help diagnose pneumonia lung cancer also covet said about two billion chest x-ray procedures per year worldwide and so we announced a result where we claimed that deep learning achieved radiologist level performance on 11 pathologies and and did not achieve radiology performance on three pathologies um and many groups have announced results of this flavor right faith-faced groups published wonderful papers of this flavor uh other groups of sanford sebastian's ruined through diagnosing skin cancer and just many girls around the world have published results like this saying that ai does as well as a human doctor on diagnosing something from some type of medical modality so given all of this amazing research progress and these amazing proof of concepts um why aren't these systems widely deployed to hospitals yet if you were to get a chest x-ray today uh in most countries in certain united states but in most countries it's very actually in all countries it's very unlikely that there's an ai system reading your chest x-rays so why is that if there are research papers they're peer-reviewed and that i will stand behind my papers you know saying that supposedly they they outperform even both certified standard radiologists so um what i see across the ai world is that there are many uh research studies a proof of concepts these are the things that works well on a researcher's laptop running in the jupyter notebook but that still needs to bridge that proof of concept to production gap so what i hope to do today is share with you three of i think the top challenges in bridging the proof of concept or production gap in the hope that wherever you are whether you're in academia or business or for-profit non-profit government or or governments that if you are exciting ai idea um if you can help your team get to proof of concept that's wonderful that should totally be celebrated but watching out for some of these challenges i hope will also help more ai projects um get into practical deployments so i think few of the top challenges in bridging the proof of concept production gap are challenges of small data of generalizability and robustness and of change management so let me go over these three and then also talk a bit about the full cycle of machine learning projects which i think will help all of us as a community take more ai projects to successful production deployments um so let's start with small data you know a lot of ai had grown up in consumer internet companies right very large tech companies that have hundreds of millions or billions of users and when you have that many users you have big data so i find that a lot of ai philosophies and tools and approaches were tuned to big data given the nature of the companies in which ai had grown up but a lot of industries have much smaller data sets and for ai to be useful in those industries we have to have better small data algorithm for example take visual inspection of smartphones the example that i alluded to earlier um if uh if you have a million pictures of scratch smartphones then today there are you know at least dozens maybe hundreds of teams that can build a neural network to diagnose if um there's a there's a scratch if a phone is scratched uh and in fact really building on a really phase it can't be emphasized more similar work on emissions all of that open source work right that people built on top of many of those models will work well um for for for the very important big data problems but they've only but but fortunately nilfactory has manufactured a million scratched smartphones which would therefore have to be thrown away and the question is given only a hundred pictures of scratch smartphones which may be all the data that exists um are you able to build a accurate inspection system and this is critical for breaking open um these applications of machine learning in visual inspection where only small data sets exist to dive more deeply into small data here's here's another example so the result i i mentioned just now was um i said deep learning achieved radiology performance on 11 pathologies and did not on three pathologies well these are the um 14 pathologies uh you can focus just on the first and the last column the middle two columns has the has a accuracy and confidence intervals but let's dive into a few of these rows so for the condition of um a fusion we have a lot of data we have eleven thousand examples and so there a deep learning algorithm was able to diagnose the level of accuracy that was statistically indistinguishable from a radiologist um but if we look at a rare condition like hernia where we have about 100 examples there radiologists still outperform the learning algorithm so it turns out that um learning algorithms work well on data sets where the distribution is like that on the left you have a thousand examples of every class then uh it's it's not easy but it is relatively easier to get the learning algorithm to do well in all of the classes but it doesn't do as well uh when your data distribution looks like that on the right which is what we actually face in in the medical domain and um i've been in a lot of conversations uh well actually i've i've listened in on a lot of conversations between a machine learning engineer and a product leader or business leader or hospital leader and the conversation goes like this the machine learning person will go oh look i have achieved very high accuracy on the test set it's a fair test said that i looked at you know the peaks was a fair test evaluation um and then the hospital leader or the doctor or the business leader of proper leader says uh congratulations on your high test set result and on your research paper publication um but your system just doesn't work and and the machine learning researcher the machine engine says um yes but i do really well on the test set um and the conversation kind of ends there unfortunately so if you i i think our job as your machine learning researchers and engineers and developers this is it's not just to do on the test set is to solve the problem that actually matters for the use case you want to address and um i find that common metrics such as average accuracy do not reflect these small data occurrences problems so for example if your data distribution looks like that on the right it's completely fine to ignore the hernia condition you know just never predict hernia and your accuracy is just fine because hernias are so rare but for practical applications is you know probably medically unacceptable to deploy a system that misses completely obvious cases of hernia and so even though hernia is very rare and on an average accuracy standpoint is called less important for the practical hospital needs and my team's work with you know a few hospitals so we're on on the ground doing some of this work it's important to actually address these um rare cases as well um so i think both on the uh i think unfortunately um i i think that the research community and the business committee is making progress on better algorithms for handling small data for example i'm excited about synthetic data generation uh using your gans were created by uh my former student ian goodfellow who who was actually a staff student way back uh but with gans this is actually an example in visual inspection generating scratches of cars using gans so you don't need a million scratch cards to learn to detect scratches you can synthesize scratches that actually i can't tell a synthetic scratch from a real scratch um and exciting research powers also on one shot learning and future learning where algorithms are able to learn from very few training examples and i think gpt3 released just a was a couple months ago just just quite recently it was a very exciting step uh to a one shot learning official learning in the language domain um i'm excited about self-supervised learning and self-taught learning uh we learn from large amounts of unlabeled data to do label tasks uh transfer learning and anomaly detection but i think all of these are technologies that um i think are exciting to help us overcome the small data challenges that are much more pervasive once you go outside your software consumer internet other than small data um a second challenge in bridging the proof of concept production gap is generalizability and robustness so uh going back to the ai you know deep learning for x-ray diagnosis example it turns out um you know those of us that work a lot in research and also production settings will will know this uh a model that works well called either published paper often doesn't work in a production setting so for example when um we collected data from stanford hospital you know stanford has a relatively modern x-ray machines and very well trained technicians so we collect images there and when we train and test on images collected from stanford hospital we can publish papers that are peer reviewed and i will stand behind showing that we can outperform human radiologists when we train intestinal data from the same hospital um but it turns out if you take this model and walk down the street you know maybe an older to hospital using older x-ray machines and where the imaging technician with the x-ray technician uses a slightly different imaging protocol so maybe the patients are tilted at a slight angle then the performance degrades and this is in contrast to the performance of any human radiologist that would be able to walk down the street from stanford hospital to this other hospital and do just fine so um there is a huge gap between what works in a research lab versus what will work in production and this is not this is true not just for health care uh this is true for many other industries as well and so i think one thing that we are we should work on both from the research and on the practical engineering side is better tools and processes to make sure our albums generalize to different data sets than those trained on i'll share a few more thoughts on this when i talk about the full cycle of machine learning projects um finally change management you know when ai technology can take a workflow um and automate part of it and that can transform the work of a lot of people around it and i think we need to get better at managing that overall change so here's an example uh this is a this is some work i did with uh anand levantine and charming kim jong and and others on palliative care so uh palliative care which is um roughly end of life care helps patients with terminal illness enjoy high quality of life we know that here in the united states doctors in general make fewer palliative care referrals than we would like i think you know doctors are good people many doctors want to keep fighting for the patient because they care so deeply about the patient and my father's a doctor and i know it's actually hard for a doctor to give up right you just want to keep fighting for the patient which is a great attitude we want our doctors to have um but we know across the country that doctors make fewer palliative care referrals than than one might wish now um there is in in many hospitals including stanford hospital there is a specialized palliative care unit those palliative care doctors could proactively reach out but the volume of patients makes chart review reviewing patient records and feasible so what we did was we built a learning algorithm to predict the chance of mortality of a patient over the next three to 12 months um and this recommends patients for consideration for palliative care so this is the workflow that we actually built this is a picture of doctor stephanie harm and hillary palette of care staff hospital and the data is made up for student privacy but this is actually pretty much what what she was seeing you know every morning but so dr hamill will wake up in the morning and pull up a table from the database which looks pretty much like this where she will see where she would see patient ids age of the patient and the learning algorithms estimated probability of mortality this allows her to decide what patient chance to review in greater detail and what doctors to call up to recommend for consideration their patients for palliative care or to make sure that the environment advanced care director is taken care of for example so what do you think happened when we first realized the system right when when a doctor calls up another doctor and says hey i think your patient is a high risk of mortality what do you think happens well maybe not surprisingly the doctor on the receiving end the phone call goes who are you and who are you to tell me that my patient is at high risk of mortality so what we realized was that we had to perform the change management process better because a system like palliative care affects a lot of stakeholders affects doctors affects nurses affects hospital administration insurance outpatient services and of course most importantly of all it affects the patient and so i feel like a lot of projects i work on i've learned over and over um to go through the appropriate change management process because we take a hospital's workflow and automate just a piece of it be reading x-rays or making pelican predictions or something else it disrupts or transforms the work of so many people around it that you know budgeting enough time identify stakeholders for flag reassurance experience happening right size first project all of these things are are are important for us as technologists and and and or business leaders uh and or academic researchers if we want to play a role in making sure our amazing technologies go out there to have an impact um and i just mentioned key technical tools the managing change are explainable ai and auditing and just go i think uh faith a and aji have been real thought leaders in you know helping advance the conversation on both of these important topics i know that some ai leaders i think explainable ai is not important that you train a black boss deep learning why do you care if you explain or not and i just and actually personally just don't agree with that um and maybe a quick story when we started and but the explainable ai is actually complicated so one quick story of where i got it wrong um when we built the first palliative care system and we started to show it to some doctors uh the doctor's feedback was you know hey how can your learning algorithm possibly tell me that this patient has a 78 chance of dying in the next few the 12 months like how could i possibly trust your ai system um and so we actually built on an avanti i actually built a system uh using uh an avram similar to lyme if you if the technologies have heard of this we tried to generate explanation for the doctor that says we think this patient is a high risk of mortality because looking at the health record the ehr the child health record they receive this diagnosis and they won this test so this is why we think this patient is at high risk of mortality um and guess what happened doctors looked at it they looked at a small handful of patients looked at the explanations we generated and they go oh got it and then they never looked at the explanations again uh and the lesson i learned was the doctors didn't actually need us to explain to them why a patient is a high risk mortality they completely qualify to look at the ehr and gesture themselves as a patient is at high or low risk of mortality what they actually wanted was some reassurance that our machine learning algorithm than our ai system was generating reasonable conclusions so what they wanted was just enough of an explanation for them to feel like our system was was being reasonable and once they had that level of comfort they didn't care about explanations anymore they just didn't look at it anymore and they would just look at our recommendations use our system for screening but then look at the patient charts patient records themselves in order to decide what they wanted to do um so i think one of the reasons explainable ai is so complicated is because i think our fear keeps on confusing who is this for are you trying to generate explanation for the doctor uh or for the patient or for or for a regulator or for someone else and also what is the action you want them to take do you need them to do something on a patient for a patient basis or do you want them to just be generally comfortable or do you want a regulator to help you spot if there's a major flaw so i think explainable ai is is is is is important uh uh and i think that we and i think auditing as well you know face recognition today is a technology with that that seems highly problematic and i think given where we are i think society we have a hard time trusting many face recognition systems uh certainly here in the united states unless we have some fair third audit to reassure us that they're doing the right things so so these are important technical things for us to work on to then bring into part of the overall change management now so i talked about three major issues um and what i think uh machine learning should do is start to be better at thinking systematically about the full cycle of machine learning projects and and so here's here's what i mean um you know we've been celebrating a lot the development of better machine learning algorithms and when the team develops a successful research paper or proof of concept it's wonderful celebrate them this is a phenomenal progress and the progress the work needed to actually take a system to production is is even much bigger than that there's all this stuff that needs to be done um and so i think what and there's actually very influential paper out of google titled the high interest uh credit cards of a machine a technical debt uh sorry but message of the title technical debt to machine learning high interest credit card something apologies so many of the times very intelligently for all love google several years ago to talk about this um and i think in addition to building the machine learning model all this other stuff is something that i hope we can become more systematic at now when i talk about these things some people ask me is this you know engineering or is it research and i think it can be either um i remember when a decade ago leading you know uh researchers were telling me that they thought neural networks weren't scientific they're the leading researchers uh uh in in uh in computer vision or not fifa other leading researchers in computer vision they're telling me neural network was unscientific and why she said hey andrew get real like why are you just messing around with figuring out neurons it didn't feel scientific to them at the time and i think someone i'll talk about today may will feel like engineering but i think both the ancient community and the research community can do a lot to make ai engineering much more repeatable and systematic so this is what i think of as um some of the major phases of an ai project you know we had to scope the project decide what problem to solve um acquire data uh uh carry out the modeling right build the model and then take it to deployment and what i want to do is go backwards and run through these four major phases and very quick some quick lessons learned from from from each of these uh so let's start the deployment they'll go back to modeling data and scope so deployments um after you train the machine learning model we still need to build a collar edge implementation and build monitoring tools and i think um the business world we're getting better at how to deploy these systems um to production so for example one design uh patent i often use this so-called shadow deployment where we may for example deploy an x-ray diagnosis system but not use it to make any decisions but just shadow a doctor right so this is safe because it's not doing anything it's just shattering doctor and this gives us time to monitor the performance of the system and make sure it's making reasonable predictions reasonable diagnosis before we then allow it to play a role in making recommendations um canopy deployments is another common design pattern but royal to a small subset of users to monitor it for uh to to make sure the data distribution hasn't changed and only after doing this do we ramp up deployment so i find that right now a lot of this work is done by engineering teams but uh more systematic tools as well as um research to help make this whole process more systematic i think will make the deployment process more more more repeatable and reliable and of course we also need some long-term monitoring and maintenance so one thing i'm i'm trying to do in the educational context as well is i think we've we've done a lot right you know uh stanford uh the deep learning dot ai uh really around the world uh many many institutions many universities many courses to teach people how to build models i think we still should teach more people how to go through this deployment process so that so that we can have more highly qualified machine learning engineers going backwards let's let's talk about the modeling process so it turns out that um i find that building machine learning models is highly iterative process it feels to me much more like debugging software than developing software uh so this is an interesting cycle right you come with an idea of an ai architecture and then you will you know code it up and train the model and then it never works the first time and so you analyze the result i i remember sometime back you know i trained a um uh soft max regression model on my laptop there was a small experiment around so it was small so i didn't need gpu or anything booted up jupiter notebooks on my on my mac laptop actually the same laptop that i'm using to speak with you by zoom right now um you know code it up simple sound fast regression stick data cleaning all in jupiter notebook train softmax model and then it worked the first time and i still remember to this day my personal sense of surprise because i just couldn't believe it wow i trained them all their work the first time like never happens and then i actually spent several hours debugging it because i just didn't believe it it turned out it actually was working but for the machine learning you know it it almost never works the first time and so a lot of the loop is current analysis to figure out what's wrong with the model so you can change the algorithm of data architecture what have you and you go around this loop and i find that in um engine execution machine learning projects here from the way we carry out sprint planning if you're using agile development process a lot of this iterative loop feels more like um debugging hopefully that guidance will be helpful to some of the managers of machine learning projects and uh you know if if if you look at the way we develop learning algorithms today a machine learning model has three major inputs you need the training data you need to choose your algorithm you know the neural network architecture or whatever the piece of code and you have the choice of hyper parameters and so when building um research results we often download the standardized training data or test you know def set test set benchmark and then uh uh and when you feed all these to train the machine learning model when doing research we tend to keep the training data fixed and uh vary the neural network architecture and vary the hyperparameters and we do that so that different algorithms can be compared to each other on a you know one-to-one basis um but i find that in a modeling production setting i often find myself holding the algorithm fix often holding the hyperparameter fixed and just varying the training data so you want one example and actually i've actually given direction to my teams where i'll tell the teams hey everyone yo the album is good enough please just use retina net and please don't change the algorithm let's just keep changing the training data in order to make the album work well um maybe one example one let's work on speech recognition we did a lot of work on the speech model but eventually i thought all right this algorithm is good enough you know the album of code works hyperparameters yeah spent a little bit of time tuning it but our daily daily workflow was we will look at the speech recognition systems output and do error analysis we'll figure out though our speech recognition system has a really hard time uh listening to people with a certain accent right i i was born in the uk so just as a hypothetical example let's say our album has a really hard time with people of a british accent right that wasn't what we saw but let's just say british accent since i was born in the uk uh and they will say great let's go and get some more british accented training data and that was actually the iteration we will keep on shifting the training data in order to improve the machine learning model's performance and that keeping the album fixed and varying the training data was really in my view the most efficient way to build this system now i know some of you i know that a lot of professors and you know research in the call some of you are saying hey andrew this is researchers in dream and i'll say i actually don't know i think it's it's both but i think the research community can do a lot to help make this process much more systematic as well all right working backwards um so talk about deployment modeling data how do you acquire data for the model in a in a corporate setting i've seen uh i i i sometimes uh i've met actually i think that 23 ceos are signed up for this so sometimes it talks to ceo and they'll say hey andrew uh give me give me two years or three years to let my i.t get into shape then we'll have this wonderful data that we'll do ai on top of that and i think that's almost always a terrible idea you should almost always often most companies have enough data to start getting going and zoning by building a system that you can then figure out how to build out your it infrastructure because there's so much data you can collect you want more user data more click stream data you know what data do you want it's often um uh by starting to build an ai system that you can then work backwards to help decide what additional data to to collect um and one one other aspect that i think is uh underappreciated in terms of thinking about the full cycle machine learning is deciding on clear data definitions so here's a here's an example i got from my my friend kian katan furush that teaches deep learning cs230 with me on campus at stanford and he's also ceo of work hera um but so ken really likes uh iguanas so he and christian baffle we came with this example um so let's say let's say you you you uh you know give labels instructions to draw bounding boxes around iguanas so yeah well some labels will draw this boundary box um different labor withdrawal boundary box like this right two boundary boxes so you're two equinox another labeler will draw boundary boxes like this um and so i find that there's a lot of inconsistency in how labelers will label things unless you drive to very clear data definitions um and this is true uh for you know well labeling iguana is a made-up example but i see this all the time in manufacturing i see this all the time in health care uh where even two doctors don't agree on the right label and i see this in speech recognition agriculture and and and other domains as well and i feel like um we've used the concept of human level performance to improve ai and what i see is there's some ai teams that measure human level performance and then they will go and say hey my ai system outperforms human level performance therefore i have proven that my ai system is better than humans and thus you must use it right um and i find that whereas human level performance is a very useful development tool is a very useful benchmark it's great for publishing your papers i find that in a practical deployment context um the exercise of proving we're superior to humans you know that that's often not like that's actually not the right approach because ultimately what we want in a healthcare system setting is not just superiority to humans we want to solve a problem and diagnose uh accurately if a patient has a certain condition or not so so i think that there's actually time to rethink um how we benchmark and how we use human level performance in in in building ai systems chat more about that later as well um so so just uh just a couple more slides you know just a few more sites that wrap up um i find that uh yeah one one of the things i i i'm still trying to get better at is scoping uh useful problems to to to work hard to solve uh and so you know i i find that um when uh ai is very interdisciplinary right i find that ai by itself is totally useless you know what was ai for it has to be applied to some important application or some useful application for it to create value so um when i meet with my healthcare friends or when i meet with business leaders you know in manufacturing in telco in agriculture i usually tell them don't tell me about your ai problems i don't hear about your ai problems tell me about your healthcare problems your business problems or your technical problems or your manufacturing problems whatever and then it's my job to work with you to see if there's an ei solution but so my my common workflow is to learn about my collaborators business problems and then to work together to brainstorm ai solutions um i think there's certain set of things that ai can do certain set of things that are valuable for business and then when i use the term business i mean in a very generic way i mean i mean also you know for a collaborative research lab or for a non-profit or for a government entity right and then but but we want to we want to select projects at the intersection of these two sets um only ai experts today have a really good sense of what's in the set on the left and only domain experts have a really good sense of what's in the sets on the right and so i tend to go to partners that the domain express asks them to tell me about their problems and then brainstorm um solutions and i think also you know the process of diligence on value and feasibility and resourcing and milestone um so these are these are important parts of the scoping process uh so just wrap up you know i i drew this picture it's clearly highly iterative process where sometimes you go from the later stages the early stages building models is a focus of a lot of ai research which is great because you know that's made a lot of progress there but i feel that for ai to reach its full potential especially outside the consumer internet industry which is maybe the one industry that's gotten really good at this um i think there's a lot we need to do to get better at the cross-functional brainstorming to paid projects uh the data acquisition um as well as as well as you know the deployment uh technologies and processes i want to end with just just uh just two more slides um you know uh mckenzie had a study estimating 13 trillion dollars worth of value creation through ai uh which is which is which which sounds like a lot it is a lot um but the most interesting thing to me about their study was showing that this untapped opportunity may lie outside the consumer internet industry that the amount we could do to help people all around the world in all of these other industries from retail to travel to transportation to various forms of manufacturing to health care to other industries maybe even bigger than than what we've seen in the consumer internet tech industry so far but to realize that value we need better research and better engineering um in order to make that happen to summarize um you know much work in industries outside uh consumer internet still needs to be done to bridge the proof of concept to production gap some of the key challenges of small data generalizability and robustness and change management and i think we should think more systematically about the full cycle machine learning projects um today in our intro to programming class you know cs101 or cs106 right stanford has wonderful lecturers like mehran sahami and others that teach undergrads how to debug software um and i think we as an industry have turned software industry increasingly into systematic engineering discipline where you know we now have we can now accurately relatively accurately predict what a software engineering team can and cannot do i think machine learning is still uh too much of a black art where there are people of experience they can get it to work for some strange reason uh but but why is it but i think that we together um academia industry should work to turn machine learning from this you know black art intuition skill based discipline into systematic engineering discipline and there's only if we do that we need to develop the processes and then also teach people the processes then that would be a big step in um breaking open ai into into many other industries um so with that let me let me say you know thank you very much i'm looking forward to seeing to taking some of the questions on slide you as well so thank you oh thank you so much andrew um thank you for the wonderful talk um there's some really interesting questions in slido um but first you know i have a burning question for you which is when you're working when you're working on the chest uh the chexnet problem of chest x-ray diagnosis you can write down an objective function it's a supervised classification problem you can build an algorithm you can go on and do it but at the end of the day you have a radiologist that's trying to make a diagnosis you have an algorithm that's trying to make a diagnosis on the one hand you can try to make a decision just based off of the algorithm on the other hand you could just have the physician make the diagnosis and then there's a whole spectrum in between how might you systematically study what that right human interaction is with that predictive model and how do you handle accountability if a decision goes awry between man and machine person and machine yeah right great question i think the the the the the short and simple answer is this is complicated uh um so i think you know that there are different groups uh including stanford uh proud of rush burger jeremy irvin that long green had quite language many teams developing uis to enable this human machine interaction and what we found for example is that um if the ui is poorly designed we can you know we could un unfortunately influence doctors to think to to just go with the ai decision so uh what we're actually designing you always to try to let the ai convey to the physician that we don't really know we think it's like 70 chance but so that's actually not very certain so please take a careful though and figure it out yourself so that ui design was was complicated um and then still evolving and i think also one one thing i love to see rise in ai is um auditing you know uh and i i i feel like yeah who wants someone to audit my code they just trust me it works right i think that's actually the wrong attitude because um for these systems to be deployed safely um we we do need to build trust and sometimes i look at the systems that my teams build and i look and go gee do i want to trust this myself and i would appreciate a third party not to audit my work in a negative way but to just help me spot problems so that i don't deploy a system and then find out much later it has some really undesirable you know bias some other problems i think our the ai community should welcome auditors uh to have third parties help us find problems in our own work proactively so we don't deploy a system which we've seen right systems deploy figures really biased against some ethnicity or some gender um uh so i think that that would be important step as well yeah i completely agree and sort of a related question here is um something you said earlier in your talk where the doctors at some point they didn't need um an ai explainability tool what they just wanted was the ability to build trust in that system so um is there a way to sort of study that systematically like you mentioned a ui but are there kind of best practices for allowing a human that's interacting with an ai system to sort of build trust in a collaborative relationship yeah so i think one of the reasons the explainable ai field it sounds so complicated is because um when we talk about explainable ai sometimes all the different use cases get moved together and i don't think it's possible at least i don't know how to build one technology one one visualization tool whatever that simultaneously serves the purpose of explaining to a machine learning engineer what's wrong and that's how they can iterate to improve the algorithm and also explains to an end user why the ai system generated this conclusion uh and why we hope they're comfortable with it and also you know show a doctor a subject matter expert why an ai system generated a conclusion but we want them to understand what we did but also intervene and actually think about it so those are really different purposes as well as regulators this is another stakeholder so the fact that the machine learning numbers are so different and the stakeholders are so different and we hope the stakeholders to take very different actions ranging from you know be comfortable with it if a low making algorithm either accepts or someone rejects someone for a loan maybe there's an appeal process for a long time we just want them to understand the decision maybe appeal it but most of the time you know kind of just understand uh uh so i don't think it's possible as i said they don't know how to build one technology and i think that if we could um clearly separate out the stakeholders and the purpose for explainable ai then we can build more distinct tools for these for these different groups yeah i got it these are these are all sort of tough uh things at the heart of human-centered ai which is what aj is all about um so if i'm sweating that's one thing i feel like i really welcome the input of sociologists because i think a lot of the problems we face is is is a you know i i i love getting uh economists and sociologists and other stakeholders to help me think through how do we deal with these things that not a pure technology problem but where the algorithms we develop can have a big role to help as well so so here's a question here's a question from an economist actually um and i'll just read it verbatim because it's really well written what do you see as the best way to address challenges of growing inequities especially economic inequality that ai may bring not an easy question but i think a good starting point yeah oh cool oh actually i see this from from eric brennelsen hey hey eric great to see you here we really enjoy interactions with eric and reading his many folks over the years so actually those of you if you're looking for good folks on uh ai and economics check out eric brennelson's uh folks this is nice here um i think one of the unfortunate things about ai which i think eric is alluding to is uh i i hate to say it but i think ai is and will uh has a risk of um accelerating uh economic inequality right just the very high level pattern is uh um let's see once upon a time uh here in the united states you could be a small scale chicken farmer and you know have a pretty nice life farming chickens and southern chickens uh but but but now with first of the internet um a centralized player you know say tyson chicken have no relationship with them right can now use iot to get sensors from all around the country on what's going on uh centralize the data using the internet uh to headquarters one data center use ai to process the data in a centralized way and then push conclusions of ipo technology back out to all of these farmers potentially and so um what was seen already is in the software internet world there is they wouldn't take most women take all dynamics um so that's why there's a relatively small handful of leading web search engines and roughly small handful of leading social media companies and because tech has now infected every industry fortunately or unfortunately we are infecting almost every industry from agriculture to manufacturing to retail to logistics with more and more of the winner take all type of dynamics and this is um uh contributing to inequality unfortunately um i wish i knew how to address this i think that government needs to play a huge role in this uh to ensure that we're going to create tremendous wealth that's clear have already done so you know as a community we're going to do so i'd love to see government and actually eric and i which having a lot about ideas like unconditional basic income or even conditional basic income to give people a safety net um and then i think uh um education is still not a panacea but very powerful too to make sure that people whose livelihoods are disrupted you know have a chance to learn more and uh contribute to the to the economy and earn a rental livelihood for themselves and their families i feel like as an ai technologist i have seen ai create tremendous value but i hope to all of us on this call you know that that when you're when you're a hot seat to make a decision do you try to make a decision to bias things to what making sure that the wealth we create is is fairly shared yeah i i completely agree and i think that's a nice segue to another question that sort of bubbled up to the top um you know about the role of sort of industry versus academia and um sort of a have and have-nots right so in industry you have access you might it had a internet consumer consumer internet based company right you have access to more data and more compute so you can do things like open i can partner with microsoft to build things like gpt-3 that for example academics and other people without those resources cannot so this is of course a cutting edge algorithm and for now there's an inequity here and you know the question here is like what effect do you think that this impact will have um on society and will we see an evening even playing field in the future like how should we all think about that you know i i i feel like the future is not yet determined and it is up to all of us um what what we saw in the semiconductor industry making making microprocessors is the center of gravity has shifted significantly from academia to corporations because most universities just don't have the process just don't have the resources to design and take a new semiconductor chip to fab and so a lot of the influence is now concentrated in you know a few great companies like nvidia intel amd right and and a few others um uh i think ai has made some of that shift where today there is some work that is much easier to do in a corporate setting than an academic setting um on the flip side there's still plenty of stuff to do in in academia uh and i think that you know there's plenty of stuff when i look at all of the amazing research that goes on across hai and across stanford and across academia i think if you look at um the the uh papers say you know at top conferences right icml in europe's iclear and so on it is true that large corporates has a growing share and that's great because i'm really glad that the large corporates are spending resources doing research and sharing their results with us that's great but universities also has plenty of great research as well and still do much smaller companies so i i think um you know i i think one thing i love about the ai community is there is excuse me that the ai community grew up with a genuine spirit of sharing uh and i think we as a community all of us any of you if any of you listening to this work for launch corporation you know you have a voice and your your your work matters go ahead and try to influence your your large corporation to to stay true to the spirit of sharing ideas because it's only by doing that that we could create more value for for for everyone um so i think the future is really not set and i think the values of all of us as a community will have a big influence uh on whether that continues to be very diversified very widespread research with ideas fairly shared uh or ends up being more concentrated i i am an optimist i think we're actually on a good path but it is up to us to keep pushing in that direction well i do know oh sorry and of course all we all know that yesterday super computers is today's smart so yeah i tend to agree with you that the future is not uh is not set but like as of now at least for like the best performing large language model at least in sort of a few shot um tasks right uh the you know everyone that's not open air is sort of stuck licensing the technology and you know i can see a way in which we move towards a more equitable version of the distribution of that technology but let's say if we were to be stuck in that like what would be the right way is this like a good thing a bad thing like how should we think about it in the present you know oh i think it'd be a terrible thing if everyone you know if if everyone has a license uh g53 like technology from a single provider they gain lots of credit to the openai team for building unfortunately i think there are multiple companies on the planet with the capability to replicate that so i would certainly you know welcome right more large companies to build these things to make sure that there's healthy competition um you know it is unfortunate dynamic uh uh of ai that there is this we create a lot of value but we also accelerate this when it takes all dynamics um it's possible that eventually we need to make sure that we have good regulatory frameworks um uh uh to ensure that we have the societal effects that we want uh although for now if you look at cloud computing there's a relatively small number of cloud providers so that i'm not saying that's not a problem fortunately it's remain relatively competitive uh but but this is something that i i think i think government you know should play should play a role to make sure that we as a society get to the outcomes that we want okay well there's one minute less i'll try to sneak this one in quickly um you know what do you think are the challenges for uh privacy preserving machine learning um in the health care industry and what do you think needs to be pushed forward the most in that area you know i feel like the one one challenge we have uh in in in privacy certainly buyers and other things is we've not yet come to agree on what is the standard we want to stick to and this has two effects both bad one an ai team will build something and as far as they know they're doing okay and then deploy it and then you know two years later some new standard arises like i think we all know there's certain protective characters you should not discriminate but monet actually here's a concrete example i went to major web search engine image search engine and i was searching deliberately for image search queries that will show gender bias it took me about 10 minutes to find one but one of the major image search engines eventually i figured out i think i found um elected official person that search query on one image search engine show all men on the first page now so we could get sensationalists upset about this because it was biased it's horrible how if my 19 month old daughter nova if she sees this maybe she'll think wow elected official should be all men maybe she should never especially official so we could get you know alarmist about this the other side of the story is actually took me 10 minutes to find highly biased queries so probably it won't affect me over that much one of the problems with these is that um a lot of these issues of bias privacy their statistical concepts and we as a society need to get better at not you know latching on to to anecdotal evidence and to measure these in a statistical way because i think i'm not going to name the search engine because i think on average the team actually did a great job and on average their queries are relatively fair um and i think part of what we need to do about business corporate regulators is established fair standards to clearly lay out what we want and don't want um and then also to rigorously audit the systems against established criteria and this will help two things one it diminishes on the research and engineering side the the fear then i'm going to rule out something and then two years later there'll be some new thing that i just never thought of you know hey who who would have thought that i i can't describe you some new idea right maybe you figure out with discrimination against people that live in my hometown of los altos and is that okay or not sounds like it's not okay but if an established care criteria and then audit then it makes life easier for the engineers and researchers and also when we roll these things out gives us a sense of what are exactly the privacy standards and the fairness standards we want the system to adhere to um but until we get there i think you know we we end up with um just more confusion and people getting surprise criticism which which isn't the goal the goals to make decisions fair not to not to randomly make people feel bad about this system so i hope we can get there i tend to agree with you well um we're a little overtime so with that um i just really want to thank you for coming and giving us our inaugural hi research seminar series and thank you to the community um for tuning in the recorded seminar will be posted on youtube by the end of this week and uh next week same time same place we'll have percy liang discussing semantic parsing for natural language interfaces so you can check that on our website um and thank you so much again andrew thank you andrew fantastic thank you thanks everyone 