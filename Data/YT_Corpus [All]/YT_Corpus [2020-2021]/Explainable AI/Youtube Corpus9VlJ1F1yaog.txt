 Artificial intelligence in recent years  and more specifically the machine-learning have seen an incredible success in all fields where  have been applied. Our societies are delegating more and more decisions  to these automated systems and therefore becomes increasingly important to understand the why and the  how these are taken. A simple example to explain the importance of the problem can be  Imagine this: if a self-driven car, it causes An accident whose fault is it? Understand why  of an automated decision becomes even more important in critical contexts, such as  the medical one. Technically we are talking about explainability which can be translated as an explanation of the  AI systems. This is still an open issue precisely because explainability is not a notion  objective but subjective. For example a concept which could be perfectly clear to a  person might be obscure to another. My thesis work, has been inserted in this context  characterised by the search for dialogue between man and machine, and has specifically sought to create  an artificial intelligence system that was capable of explaining its decisions to a team of  doctors. The collaboration with the cantonal institute of pathology in Locarno and its pathologists  engaged in the diagnosis of tumours, it has allowed to create a system that, after having  learned from anonymous patient data, was able to explain his decisions in an  clear and comprehensible to medical experts. In practice, to conclude, make  comprehensible the reasons for the automated decisions, makes people  able to apply their human yardstick to these without having to  whether or not to trust the machine. 