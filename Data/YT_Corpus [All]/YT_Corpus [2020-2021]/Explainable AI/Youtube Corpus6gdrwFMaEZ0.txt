 all right hello and welcome to an introduction to emma lapse on google cloud at next on air i'm excited to share this overview with you and i hope it will be super valuable as you continue to leverage ai to transform your business first a quick introduction my name is nate i'm a product manager on google cloud's ai platform where we have a mission to empower every enterprise to transform their business with ai at google we've pioneered a huge range of ai and machine learning breakthroughs and much of this industry leading technology underpins the products we launch within cloud ai but at the heart of our product decision making is a focus on the capabilities that will truly help our customers unlock new business value and so our claudia platform does just that by providing customer ai and ml teams the infrastructure and tool set to help them get more done more efficiently and more effectively let's take a quick look at the agenda we'll start with the definition and overview of mlaps and the core challenges at play we'll talk a bit about where data science teams are today and where we see them going and i'll share a simple framework for ml ops based on real processes that we see in practice both internally at google and from our mature customers and we'll wrap up with some of the services you can leverage today to get started as well as where we're making additional investments in the future that'll be exciting to hear so let's jump right in ml ops formally defined might be described as both a culture and a practice that aims at unifying ml system development and operations it takes both its name as well as some of the core principles and tooling from devops and this makes sense as the goals of mlaps and devops are practically the same to shorten the system's development lifecycle and ensure high quality software is continuously developed delivered and maintained in production but machine learning has its own unique challenges and different needs that require special attention and so let's jump to a slide that may be familiar to many which is from the canonical google paper of 2015 hidden technical debt in machine learning systems the takeaway being that production machine learning is more than training a model and that very little of properly functioning ml systems is actually model training code there are a number of different technologies and processes that must be in place to get the most out of production ml systems and it's really this combination and management of these different processes that comprises true mlofs a simple analogy might be the assembly line prior to the assembly line automobiles had existed for decades and even years before the model t ford had something called the 999 which set the land speed record at over 90 miles an hour but it really wasn't until the assembly line was introduced which is also just a set of technologies and processes that continuous high quality production and delivery of automobiles became possible and inexpensive and so at google the cloud ai platform is quickly becoming the assembly line for machine learning but let's take a look at where most teams are now most ml journeys will look something like this as a data scientist it starts hopefully with a clear ml use case and business objective and with that use case at hand you can start gathering data from different data sources and doing some exploratory analysis that helps you get a sense for what the data actually holds once you have a sense for what's in the data itself and you've you know imputed some missing values and normalized your data or done some feature generation you can actually start to approach the modeling and figure out how you're going to tackle some new experiments and so you would manually execute these different experimental steps doing more data preparation more feature engineering and testing and then you do some model training and hyper primary tuning on any models or model architectures that are particularly promising last but not least you would be evaluating all these generated models testing them against holdout sets of data that wasn't trained on evaluating their different metrics looking at lost curve stability and comparing those models with other models to see which one actually works the best and of course this whole process is really iterative and it's manually executed over and over and over again you might begin by doing multiple different concurrent experiments testing out different ideas and different architectures analyze and compare the performance there and then kick off new experiments based on what you learn until eventually you actually iterate towards an optimal model and you'll be experimenting again there with different hyper parameters as you tune that final model until again you get a model that is sufficiently performant that it passes all of your tests and then of course unfortunately at this step you put in storage and throw it over the wall to the it and operations folks and it'll actually be their job to deploy the model to production as a prediction service and they'll ensure all the necessary features are served in production make sure we have auto scaling make sure we set up any necessary deployment pipelines for all of various targets which actually may be in a distributed system and so it's easy to see why that process is difficult and why organizations are struggling here first of all it's really time consuming the steps are highly manual and are written from scratch for every use case it's also inflexible custom built steps can't be reused and are only understood by the author and so every additional data scientist in your team or in your organization can't easily leverage all the work that a particular team member has done towards their use case and you know i hear from data scientists all the time that even they can't understand the work they've done six months ago and leverage that for the future this is also an error-prone process you get issues like training serving sku where the lack of coordination between it and itops and the data science teams leads to unexpected differences in the online and offline performance not only is this sub-optimal from a performance standpoint but it's also really hard to debug and figure out like why that happened in short there is a high marginal cost for model development and it really shouldn't have to be this way and so let's lay out a framework here and start by addressing some of the challenges we saw using this framework across the ml solution life cycle and then we can dive into some of the specifics and the details of each of these steps first in our first box on the left experimentation is absolutely crucial we don't want to hinder the experimental process at all we want to lean into it data science today is still very much science and for most use cases requires a lot of trial and error instead of cutting down on the number of experiments we want a way to embrace experimentation let ml teams track and manage all our different experiments they're running to make sure they're confident in building the best model second re-running training jobs when new data is available or based on specific triggers you might have set ought to be really really easy we want a continuous stream of new candidate models to test out and challenge the model running in production to see if it actually is better third models must be properly tested evaluated and approved for release following a rigorous audible and even reversible ci cd process i think this step is really missed out on by a lot of the data science teams today and lastly for any model running in production you want to always have a sense of how that model is performing this is important to ensure quality and business continuity but it's also crucial for getting the high quality signals into how to improve the model for the next iteration what better training data is there than true production inferences the dotted line the center here roughly delineates the training and serving sides of this framework and building a world-class mlaps team and platform is a journey and from a maturity standpoint we tend to see that customers find success by focusing initially on the left-hand side of this framework the bedrock of ml ops is reliable and repeatable training pipelines and so as we go into the next section let's take a look at those repeatable and reliable training pipelines first we transform the previously manual steps of ml experimentation into a reusable pipeline you implement an orchestrated experiment of various steps as a pipeline which can be executed in its entirety in which it allows you to iterate by changing some of the configurations to point to different data or use different parameter values and then you execute the entire run as an experiment and it's this repeatable and reusable training pipeline that lets you run multiple iterations and actually produce different models to compare and though each pipeline here may develop one or actually many models the output of this process is not a model to deploy but rather the code of the pipelines and its components which is source controlled in a source control repository then similar to any software system you set up a ccd process for your code including building components running automated tests and tagging all the produced artifacts those produced artifacts like compiled packages and container images are stored in a central artifact store and then the pipeline is deployed to a target environment this can be a test or dev environment a pre-product staging environment or a production environment depending on the cacd trigger if this is a production pipeline it is executed automatically and repetitively to continuously train your model given new data that can be either based on a schedule like daily weekly we mentioned or based on a specific trigger for example the availability of new data or drop in performance of the model that's actually running in production when a continuous training pipeline runs till the end it outputs a newly trained model and that is stored in the central model registry for deployment as a prediction service at some point in the future crucially the link between the model artifact itself and the training pipeline is never severed meaning that all models in the model registry can tell you their provenance what pipeline trained them who created the model who ran the training job what data it was trained on and evaluated on and what were the metrics and results of the different evaluations this lineage tracking is incredibly key as we'll see in the future one additional thing to highlight here you'll notice the parity between the dev and experiment pipeline on the top row here and the continuous production pipeline on the bottom row here avoiding the dev and production inconsistency is crucial as it's the source of many bugs or under performance of models in production is really hard to debug for most teams now that we have an overview of the reliable and repeatable training pipeline which again is the foundational first step of mlaps let's take a look at the serving pipeline for deploying the train model we'll want to execute model serving as a csd process as well so you'll fetch the model artifacts from the model registry where our training pipelines have actually stored them as we just saw along with the code that wraps the model as a service for example as a rest api to execute the following steps first building the prediction service itself then running tests which may and probably should be unique to your use case and organization and then deploying the service to the target environment or environments it's during this point in the process where it's often advisable to run additional tests and add some release gates so for an online serving use case you may want to canary your model uh on production hardware to ensure that it meets certain latency requirements you might actually also want to test out different hardware to make sure you're deploying the model to the appropriately optimal hardware for that model architecture in addition you might want to run organized a b tests to ensure that your new candidate model truly outperforms the current model in production before routing all of your traffic to it and so you might take one percent or five percent of your traffic and send it to the new model to test out how it's performing on true production data for that you'll also need to capture the eventual business outcome to serve as the ground truth in evaluating that model's performance when the model is deployed in the serving infrastructure itself it starts to serve predictions on live data and you can then enable explainability and continuous evaluation where the serving logs are captured and stored for monitoring and analysis in the future we monitor model performance and production to know if models are going stale to identify if there's any outliers or if there's skew or concept drift exhibited in the data this would be something like the statistical properties of the data changing over time which can trigger a new iteration of model training or experimentation and this happens all the time we know we live in a dynamic world and the data is constantly shifting around underneath us and so it's incredibly key to keep a close watch on the production data that's coming in so we can identify for example if a particular class is seeing a shift in its underlying distribution again crucially because we have maintained model provenance and lineage like we mentioned we can use a model's offline performance as a baseline when comparing against production and quickly identify where concept drift has occurred as live data shifts away from the training data this also lets data science teams quickly pull up the training data in pipeline to reproduce the run figure out where the issue might be occurring and then debug things taking a step back and zooming out here's the end-to-end view of both training and serving using our mlops framework and map to the input and output artifacts and different storage systems so we start again with experimentation development where we develop a training pipeline that is reusable and repeatable and then the code is checked into a code repository for source control we then build that training pipeline and deploy it to the target environment either dev staging or production using a ccd process where the artifacts are registered in a central artifact repository where we can pull out those artifacts for deployment or roll back in the future now the model that the pipeline is running in production we can do continuous training either on a weekly or daily basis or when we have a particular trigger based on the model running in production or new data is available the outputs of these are stored in the model registry which can be picked up for a model cicd process in the future that model ccd process includes all of the tests stage gates and approvals necessary to rigorously make sure that the model you're deploying to production is ready and is actually outperforming the model that is currently in production and then crucially when that model is actually running on production hardware we want to make sure that it's constantly staying up to performance metrics and that we can identify when it's starting to decay over time or whether there's skew or drift so we can go back to an earlier step in this process retrain new models and deploy them to production for testing in addition this is a very complex and high volume process where customers may be running thousands of different training experiments for a single use case and testing dozens of different models in production at any given time and it's important that we make sure we get this full picture and make it available to our teams and so each of our pipelines and steps in this mlaps process writes out key executions and artifacts to a central metadata store where ml teams it leaders and governance folks have a system of record where they can do analysis or run an audit to keep track of these myriad processes ongoing as we automate the different steps of our assembly line for machine learning as shown here okay now that we have this big picture view and we put it all together let's talk about ml ops on google cloud's ai platform discuss some of the products and components that we offer to enable such a system and as mentioned the the claudia platform really is our suite of integrated products for building and using ai and here we'll just take a focus on ops then we'll provide a bit more context on the relevant individual products themselves so like before let's start with a team that has existing data and products and clear use cases they may be running analyses and experiments locally or in different siloed environments for different organizations this is kind of what data science looked like in sometime around 2010 there's a lot of ad hoc analysis a lot of custom tooling a lot of different experiments run locally not a lot of cloud usage laptops stored a lot of the knowledge of data scientists in the organization and when they left the organization that knowledge was largely lost the first thing teams usually standardize on from this original view is the core data science platform this would be things like which language they should use which frameworks they'll use the preferred development environments all the different things that most of the team members will probably argue about in 2020 the average data science team at most organizations has a pretty good handle on this and though there are many different options to choose from and there are different benefits and challenges associated with those different options and also there's a lot of low-hanging fruit for improvement here enterprises are still better positioned than they were five years ago from a core data science tooling standpoint and model building standpoint targeting back to our introductory section in our framework what most teams are now moving to build is this true production grade platform for mlaps these are the services that enable the end-to-end workflow that we saw before following our mlaps ml ops framework experimentation continuous training model ci cd and continuous monitoring and so let's take a look at what this looks like for specific gcp products at the data science layer we have our ai platform notebooks which is a hosted jupiter lab environment integrated with our infrastructure and various services you can connect to a source control system run notebooks in a scheduled manner and you have control over the underlying compute this is a familiar interface for data scientists who like to do ad-hoc development using jupyter notebooks especially in the upfront part of their prototyping experimentation phrase in the ml ops infrastructure layer we've built this on modular services backed largely by open source and mostly by open source created by google each of the boxes here can be used to varying degrees or not at all depending on your different needs based on many of the internal expertise we have and working with many of our customers we know that ai workloads can be highly customized and so our services are designed for novice intermediate and advanced machine learning teams to use whatever they need and also bring their own tools to the platform as well it's the core of this emmalops layer lives our ai platform training and prediction services which are the backbone of building and running models the items in green are where we're making our latest investments a feature store to help solving train help solve training serving skew issues and improve collaboration within teams by registering features during the upfront training process you can ensure that those same features are available in production for your model at low latency and you can also share those features with other team members to ensure that all teams have access to the great work that individual data scientists are doing and make sure we're not duplicating effort and we're using the most powerful and effective features to build new models in addition a robust experiment tracking service to enable large-scale experimentation the ml the ml model introspection use case and comparison incorporating great google open source products like tensorboard this is also really critical because the amount of experiments we run is directly proportional to how confident we are that we've really chosen the best model and by making experiments shareable and easily referenceable in the future we can then enable other members of our team working on the same or similar use cases to learn from the work that we've done in generating the optimal model for our use case at the bottom layer there we have our ml pipelines and centralized metadata store for orchestrated managed end-to-end workflows these are really critical for building this end-to-end capability and making sure we're not writing custom code for different steps of this this end-to-end process that can't be leveraged and reused in the future you want to make sure we're orchestrating full runs of end-to-end pipelines and tracking the outputs of each step the inputs and outputs of each step in our central metadata store and for many of these different items in green we offer great open source and hosted solutions today and we're working on bringing managed services to these areas in 2020 and so what's shown here is simply the high level overview focused on our mlops stack definitely stay tuned for announcements in these investment areas and more as our platform grows in breadth and depth and so with that i'll leave you with this view of our mlops platform your teams can begin leveraging the power of ml ops on gcp immediately by checking out our ai platform in the gcp console for more information i've included some detailed blogs and solution guides for further reading which are the best place to start here and they'll be published alongside this breakout session after reading these please reach out to google cloud rep to get in touch we would absolutely love to hear from you we'll also be answering your questions in the dory throughout the next week or so i thank you for tuning in and listening and we are immensely excited to see what you build with our cloud ai platform thank you 