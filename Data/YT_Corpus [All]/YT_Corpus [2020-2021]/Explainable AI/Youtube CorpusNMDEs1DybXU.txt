 I’m Reenita Malhotra Hora, Director of Marketing & Communications at SRI International and you’re watching The Dish. What is explainable AI? Why does it be explained, isn’t it just machine learning? We know that AI nowadays, they perform really well. So if you ask them to classify images, they can achieve some kind of superhuman performance. From the perspective of machine learning researcher, when we look at these models, so maybe you can say we are a little bit cynical about this, but when we look at these models, we know that it has certain kind of weaknesses. For example, it doesn’t do the level of reasoning that humans are doing. So humans can apply logics and perform reasoning to – that helps us to do multiple kind of tasks but if you look at these computer vision models we are now using. so they are basically using a very simple heuristics, that is saying similar inputs leads to similar outputs, and it’s extremely powerful and – because it has access to the vast amount of data that human don’t have access to. So even with this simple heuristics, you – it can go a long way and can reach superhuman level performance. But we want to make the AI even better, we want to make the AI perform human level reasoning. Are we getting there? So we have done a lot of research in different directions, sometimes on neuroscience, sometimes pure machine learning, sometimes human-computer interaction. Now it’s a particularly interesting time in that – so there are some people saying that AIs will never get there and some people are saying that AIs are getting applied to more and more domains, so we should be seeing like explainable AIs very soon. My take is more towards the second kind that I think we have already had some clues on where we could improve our current model to reach a human level reason. So I expect to see that in like five years. You mentioned to me that you were specifically interested in brain to brain communications. - Yeah. What is that? So we are trying to understand how human can communicate with each other because that will help us understand how AIs will be able to communicate with humans. How you can tell that one person understands what the other person is trying to tell him just purely from brain scans, so maybe you can pull a brain scan on a person that’s listening to the other person talking and maybe somewhere you’ll find a neuro spike that tells you that this person has understood part of what the other person was saying. So is that what you’re doing these days, looking at brain scans? Our collaborators at Princeton has collected a lot of brain scan data and what we were doing was analyzing it and trying to figure out how and where information was delivered from one person to another. So one problem arises from this general theme was that how can you figure out when and how much information was shared between a speaker and – a speaking person and a listening person? Can you perform an information estimation, that is can you – given two brain scans, will you be able to identify how much information is shared between these two brain scans? Either it being a sound or it being some kind of a language or it being meaning being shared. So towards that, we developed sort of a mathematical tool. As you can imagine, information is a very mathematical concept, so it’s basically measuring how many bits is transferred from one person to another just like computer communication. Right before our work, there was a recent breakthrough in machine learning that people have been able to – have developed such kind of an algorithm that can sort of, given two signals, compute how much information is shared between them using neural networks, and a particular contribution that we make to improve this algorithm is to add a guarantee to it. So because the algorithm gives you an estimate, so let’s say it says there’s 10 bits of information transferred from one person to another but it’s just an average, so it could be that this information is zero bits but it’s just a random chance or some uncertainty in the algorithm that ends up saying it’s – there’s actually 10 bits being transferred. So what we are able to do is that we are able to put a bound on – like on this estimate. So there’s an upper bound, there’s a lower bound. We’d not only want to give an average estimate but we also want a concrete bound say in terms of scientific discovery. And this is particularly important for like neuroscience research. You had mentioned in your notes that there are two kinds of communication, linear and nonlinear. That’s right. What are those? What does that mean? So this in terms of measuring how information is transmitted from one person to another. So let’s say if you have a signal, the responses in one person’s brain is linear to the other person’s brain. So what this means is that when the neural response in one person increases, the neural response in another person also increases. So this counts as like – as a linear correlation, but there is also a more complicated interaction. For example, if the signal in one person’s brain increases but the information in other person’s brain actually fluctuates - Right according to the other person. So in this setup, it counts as a nonlinear interaction. So traditionally, in the neuroscience literature, people have been mostly using – studying this kind of linear correlation. This method is very effective for something like blood flow because the blood flow, it’s often of a more linear interaction but when you actually look at the sound signal, the sound signal itself is sort of a wave. So its magnitude increases and decreases over time. There is a lot of this kind of nonlinear interaction in the auditory processing region in human brains. What we want to do is that using neural networks, we want to capture the whole picture, that is all possible nonlinear interactions and we want to quantify that. So the project you’re working on right now, this is a DARPA-funded project and you’re collaborating with Princeton University? This project was a seedling on understanding how people communicate with each other. So I think our research already points to a few very interesting direction. The initial goal was to get an – do a larger scale study and get better understanding of how human communicates but I think what we ended up was in a slightly different direction and what we ended up is an algorithm that can quantize mutual information between two signals. Although it’s not the original direction, but we hope this can help in a larger area, for example, physics, chemistry and hopefully quantum computing. Quantum computing. Yeah. That seems to be what everyone is talking about, right? Can you explain a little bit more about how it would translate to quantum computing? Quantifying the amount of information shared between two signals is a very widely used quantity in a lot of areas of research. So for example, given two variables in physics, we want to know whether or and how one variable is affecting the other because nowadays, although you can – you have this kind of elaborative like physical equations but when you actually get down to experiments, you have to do a lot of this kind of hypothesis testing. So what we essentially provide is a new type of hypothesis testing tool in terms of whether two variables affect each other and I think this will have a lot of pretty – a lot of general use cases. All right. Thank you, Xiao Lin. -Yeah. -Thank you. Thanks so much for joining us. 