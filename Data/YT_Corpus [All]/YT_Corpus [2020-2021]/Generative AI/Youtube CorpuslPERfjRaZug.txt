 one of the biggest headlines and AI research for 2019 was the unveiling of alpha star google deepmind's project to create the world's best player of blizzards realtime strategy game Starcraft 2 after shocking the world in January as the system defeated two high ranking players in close competition an updated version was revealed in November that had achieved grandmaster status ranking among the top 0.15% in Europe's 90,000 active player base i'm tommy Thompson and in this episode of ein games we're going to look at how alpha star works the underpinning technology that drives the truth behind the media sensationalism and how it achieved Grandmaster status and online multiplayer you might be wondering why deep mind is so interested in building a Starcraft 2 ball ultimately is because games be they card games board games or video games provide nice simplifications or abstractions of real-world problems and by solving these problems and games there is potential for it to be applied and other areas of society thus led AI researchers to explore games such as chess and go given their incredibly challenging problems to master this is largely due to the number of unique configurations of the board in each game often referred to as a state space research in goo estimates that there are around 2 times 10 to the power of 170 valid layouts of the board that's two with 170 zeros after it it's a hell of a big number meanwhile Starcraft is even more ambitious given the map size unit tapes and the range of actions are both micro level for unit movement a macro level for build behaviour and strategy a topic I'd recently discussed an episode forty sevens examination of the AI of Halo Wars to know even naive estimates by researchers and the number of valid individual possible configurations of a game of StarCraft suggester is around 2 times 10 to the power of 1685 in the case of both Gao and Starcraft these are on paper incredibly difficult problems to achieve expert level knowledge for an AI system as I explored back in episode 26 universities around the world have seen the potential of StarCraft as an AI research problem for well over 10 years now it's a game that has no clearly definable best strategy to win requires you to think about the effectiveness of decisions in the long term and without complete knowledge of the game world plus you have to think about all of this and react in real time hence research competitions such as the Starcraft AI competition and the SSC AIT have operated for the best part of a decade to try and solve the problem however neither of these competitions have the support of StarCraft creator Blizzard meanwhile the alpha star project is an official collaboration with the game's creator resulting in new tools such as the open source toolkit pi sc2 which allows training of AI systems directly in the Starcraft 2 game as well as the largest collection of anonymize replay data ever conceived from the franchise since its inception deepmind has directed a tremendous amount of money and effort into researching two specific disciplines within artificial intelligence deep learning and reinforcement learning to explain as simply as possible deep learning is a process for a large convolutional neural network attempts to learn how to solve a task based upon existing training data that reconfigures parts of the network such that it gives the correct answer to the training data is tested against with very high accuracy meanwhile reinforcement learning is an AI that learns to get better at a particular task by learning through experience these experiences then update the knowledge the system stores about how good a particular decision is to make at that point in time often you can use these techniques together first the networks are modified to learn from good examples already recorded and then the reinforcement learning kicks in to improve existing knowledge by solving new problems that comes up against these approaches have proven very effective in a variety of game projects for deepmind first creating AI that can play a variety of classic Atari games then defeating 9dan professional golf player lease at all with alphago and the creation of alpha zero achieved grandmaster status in chess shogi and goal and the next step was to take their expertise in these areas and apply it to Starcraft 2 headed up by Professor David silver and dr. Oriole venules who long-term viewers will remember from episode 26 as the former spanish starcraft champion and co-creator of the zerg bow over mind at the starcraft AI competition in 2010 the team behind alpha star is comprised of over 40 academic researchers not to mention additional support throughout deep mind and Blizzard in order to build the tools and systems needed for the AI to interface with the game once again another massive endeavor with Google money helping to support it so let's walk through how alpha start works and how it achieved that grandmaster status alpha star has at the time of this video had two major releases unveiled in January and November of 2019 the core design of our alpha star is built and learns is fairly consistent across both versions but alpha star is in just one AI it's several that learned from one another each AI is a deep noodle network that reads information from the game's interface and then outputs instructions that can be translated into actions within the game such as moving units issuing build orders or attack commands it's configured in a very specific way such that it can handle the challenges faced in parsing the visual information of a Starcraft game alongside making decisions that have long-term ramifications for anyone who isn't familiar with machine learning this is a highly specific and complex set of decisions that I'll refrain from talking about here but for those who want to know more all the relevant links are in the video description now eclis when you start training neural networks don't be given completely random configurations which means the resulting a I will make completely random decisions and it will take time during training for them to learn how to change their anarchic random behavior into something that's even modestly intelligent when you're making an RTS bar that means figuring out even the most basic of micro behaviors for eunice much less the ability to build them or more coordinated strategies using groups of them at a time so the first set of alpha star BOTS or agents are trained using deep learning by taking real-world replay data from human StarCraft match is provided by blizzards their goal to reproduce specific behaviors they observe from the replay data to a level of accuracy essentially they learn to imitate the players behavior both micro actions and macro tactics by watching these replays naturally the replays are based on high level play within the game but of course the data is anonymized so we don't know who those players are once training is completed these alpha star agents can already defeat the original elite AI built by Blizzard for Starcraft 2 and 95% of matches played but learning against the human data is just the start of the learning process it's not enough to replicate our behavior they need to find a way to surpass it and they'll do that by playing against each other and learning from the experience the technique adopted population based reinforcement learning embraces a common principle and computational intelligence algorithms where you can improve the best solution to a problem by having them compete with one another effectively creating an arms race dynamic between multiple competing agents to address this deep mind created the alpha star League where several of these pre trained alpha star agents battler Oh enabling them to learn from one another but there is an inherent danger that a machine learning algorithm can accidentally convince itself as found the best Starcraft AI especially if it evaluates how good it is by playing against other Starcraft AI that are also learning it may be a good player but it may have lost good knowledge along the way because all its competitors play very similarly and it's trying to find a new strategy or tactic that will give an edge but that might make a worse player overall any eye research we call this converging within local optima if you imagine the space of all best Starcraft strategies is like a series of rolling hills where each hilltop represents a particular strategy or style of play every AI player is sitting somewhere in these rolling hills and the reinforcement learning is helping them claim to the top of the nearest hill but once they reach the top of the hill they can't go any higher and might be convinced they've achieved the best possible Starcraft strategy because they've defeated all the other players nearby but all those other players are on the same hill because they use a similar strategy they won't even realize there might be a better hill for them to claim somewhere else hence you could have three Starcraft AI bots a b and c that are stuck in a situation where a can defeat b in a match beacon to VC but C can then defeat a because the strategy behind a is so specialized it's only good against a certain type of opponent this was evident in early training where Chi's strategies such as rushing with Photon Cannons or Templars dominated the learning process it's a risky move and it doesn't always work Haynes does a need for dominant strategies to be overcome within the training process deep mind addresses this by creating two distinct groups of AI agents in the league the main agents and the exploiters the main agents are the ones I've already been talking about they either are learning to become the best Starcraft 2 players new main agents are added to the league based on learning experience while existing ones are kept around to help ensure information isn't lost along the way and will be pitted against the new players periodically in combat meanwhile exploiters are AI agents in the league whose job isn't to become the best Starcraft player but to learn the weaknesses that exist within the main agents and then exploit them by doing so the main agents will be forced to learn how to overcome the weaknesses found by the exploiters which will improve their overall ability this will prevent them from creating weird specialist strategies that will actually prove to be useless in the long run there are two types of exploiter the mean exploiters that target the latest batch of mean agents to be created and the league exploiters whose goals are to find exploits across the entire league and punish them accordingly the entire alpha Star League process is trained using Google's distributed compute power running in the cloud using their proprietary tensor processing units or TP use the actual training is broken up into two batches one for each version of alpha star that deepmind have published so now that we know the inner workings let's look at each version how it was evaluated and what differentiates them from one another the first version of alpha star was trained within the league for 14 days using 16 TP use per agent resulting in 600 agents being bill each agent experienced the human equivalent of two hundred years of StarCraft playtime already surpassing any human equivalent to test the mo deep mind invited two professional players to the London offices in December of 2018 first dario once AKA TLO followed by Gregor Kaminsky known as manna both of whom play for the eSports organization Team Liquid playing a typical 1v1 matchup of five games under match conditions alpha star defeated both TLO and mana handsomely making it the first time that an AI it successfully defeated a professional Starcraft player no while this was a significant achievement there was still a lot of improvements that needed to be made to the system given that many concessions were made in the design choices for alpha star and the test matches at that time first of all version 1 of alpha star was only trained to play as Protoss and was evaluated against Protoss playing opponents as Starcraft players will know while a given pro player will typically focus on only one species they do need to be aware of and can counteract strategies from other species as a result TLO was at a disadvantage during these test matches given while he does rank Grand Master level for Protoss he plays professional as the Zerg however this was mitigated somewhat by mana who is one of the strongest professional Protoss players outside of South Korea secondly version 1 of Alpha star could only play on one map of the game catalyzed l-e this means that the system had not figured out how to generalize the strategies it was learning such that it could apply them across different Maps the third alteration was that the original alpha star boss did not look at the game through the camera they had their own separate vision system that allowed them to see the entire map while the fog of war was still enabled it did allow for alpha star to have an advantage over other players by letting it see the rest of the visible world now deepmind insists this was actually a negligible feature given that the bots behavior suggested that they were largely focused on it is the map like a human would be but it was still something that needed to be removed for version 2 what is undoubtedly the cheekiest part of this whole experiment is that in order to keep TLO and mana on their toes they never played the same bot twice across the five matches as I mentioned earlier alpha star is technically a collection of bots learning within the league hence after each match the alpha star bought was cycled out with each of them at that time optimized for a specific strategy this men that TLO and mana couldn't exploit weaknesses they'd spotted in a previous match but interestingly despite all these advantages over TLO and mana the one area that many would anticipate the bots to have the upper hand is the actions per minute or APM the number of valid and useful actions a player can execute in one minute of gameplay during these matches man as EPM average dough around 390 while TL or was just shy of 680 the Alpha star had a mean of 277 this is significantly lower and it's for two reasons first that because it's learning from humans it's largely duplicating their APM in addition alpha stars ability to look at the world and then act as a delay of around 350 milliseconds on average compared to the average human reaction time of only 200 milliseconds with the first version unveiled and its success noted the next phase was to eliminate the limitations of the system and have it play entirely like a human would the sync codes been able to play on any map with any race using the main camera interface as human players would with some further improvement to the underlying design the Alpha star league ran once again but instead of running for 14 days this time it ran for around 44 days resulting in over 900 unique alpha star boss being created during the learning process during training the best three main agents one per race terran protoss Zerg were always retained with three main exploiters again one for each race and six league exploiters two frite race forcing the main agents to improve but this time instead of playing off against professional eSports players the boss being trained would face off against regular players in StarCraft who's online ranked multiplayer 3 sets of alpha star agents were chosen to play off against humans the first batch being the boss that had only completed the supervised learning from the anonymized replay data referred to as alpha star supervised and then two sets of main agents that were training in the alpha star League called alpha star mid and alpha star final alpha star mid are the main agents from the league after being trained for a total of 27 days now alpha star final as the final say after 44 days of training given each main agent only plays as one race this allows for a separate matchmaking rating or MMR for each AI to be recorded to play off against humans alpha star snuck into the online multiplayer lobbies of battle NER buzzards online service and through ranked matchmaking would face off against an assuming player provided they were playing on a European server while blizzardon deepmind announced that players could opt in to play against alpha star after patch for point 9.2 for Starcraft 2 the matches were held under blind conditions meaning the Alpha star didn't know who it was playing against but also human players would not be told they were playing against the AI just that there is a possibility it could happen to them when playing online this anonymity was largely to prevent humans recognizing it was alpha star discovering its weaknesses and then chasing it down in the matchmaking to exploit that knowledge which could really have knocked it down in the ratings given it can't learn for itself outside of the league to establish their MMR the supervised agents played a minimum of 30 matches each while the med agents ran for 60 games while the final agents took the Med agents MMR as a baseline and then played for an additional 30 matches the alpha star supervised BOTS wound up with an average MMR of 3 6 9 9 which puts it in the top 84 percent of all human players and that's without the use of the reinforcement learning however the big talking point is that alpha star finals MMR for each race places it within grandmaster rank on the Starcraft 2 European servers 5 8 3 5 Rose org 6 0 4 e4 Terran and 60 to 75 for Protoss of the approximately 90,000 players that play Starcraft 2 on the European servers this place's alpha star within the top 99.85% of all ranked play there is still work to be done there are challenges in ensuring the learning algorithm can continue to adapt and grow with further interactions with humans addressing the need for human play data and also addressing some of the more quirky strategies that alpha star has exhibited sadly I'm no Starcraft expert so I can't really speak to that in detail but it sounds like there are still plenty of future avenues for this research to take not to mention taking the challenge to the South Korean eSports scene which is significantly stronger than it is in the rest of the world who knows we may well see new experiments and innovations being stress tested against the player base once again in the future but in the meantime I hope that having watched this video you've got a clearer idea of how alpha star works and why it's such a big deal for AI research this video is in many respects a massive simplification of the system and in the video description there are links to the research papers and other videos and blogs that cover the topic in a lot more detail if you're new to this channel and want to know more of a starcraft AI be sure to check out my episode from 2018 looking at academic research in the original Starcraft and the competition EP eyes you can download and use to build your own Starcraft BOTS these competitions are still ongoing and still seeking new entrants so while you might not be challenging alpha star you can certainly try your hand with fellow hobbyists and academics plus keep an eye on the channel for a new design dive episode where I talk about why the technology behind alpha star isn't ready to be rolled out across the games industry just yeah and of course don't forget to subscribe to the channel given only a mere 20% of people watching these videos are subscribed to AI in games so get clicking and you can get all my latest episodes straight up in your eyeballs I send it really dirty but anyway thanks for watching this episode on AI in games I figured for the first episode of 2020 we should start with arguably the biggest game in AI story of 2019 as always my work is only possible thanks to the support from patreon and YouTube memberships with a special shout-out to patrons Devin Allen James and Phan three kliks Philip and Zoe Nolan to get your name in the credits and watch these episodes first come join us and the AI in games patreon by clicking the links on screen and in the description see you later folks I'll be back [Music] Oh [Music] 