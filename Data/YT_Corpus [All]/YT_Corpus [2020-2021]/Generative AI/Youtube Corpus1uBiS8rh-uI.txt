 Peter Richter ik is a professor at the King Abdullah University of Science and Technology primarily working in the area of big data optimization and machine learning he is known for his work on randomized coordinate descent algorithms stochastic gradient descent and federated learning he is also NACO inventor of federated learning our Google platform to improve communication efficiency as well as data privacy [Music] I'm here with Peter rap direct the professor at the King Abdullah University of Science and Technology Peter welcome and thank you for joining us this morning my pleasure thanks your research group has 12 papers that have been accepted by nerds this year can you describe one of those papers and you know share some of the highlights that impressed you most no it's it's difficult to pick one out of these papers so let me think so there's some work on on reinforcement learning which from the viewpoint of optimization it's a zero order optimization optimization with their gradients you only get rewards function evaluation and you try to optimize that way there's some work on gradient time methods but maybe what I like to talk about is randomized second order methods randomized Newton type methods so for some reason and I understand the reason the the models machine learning models at home are trained with what are called first order method stochastic randomized first order methods and there's a very good reason for this because they're scalable they're very fast and they scale to big data and big models very well but they have one huge difficulty this is one more big issue with them and the issues that they depend on so called conditioning off of that problem so the data could be bad and then the methods are very slow in the data is good limiters are very good so in order for us to make these things as fast as possible we try to with these first automatic we try to do all kinds of tricks in order to reduce the bed affect of this bed conditioning so we may be using lionsearch we may be using something like important sampling we may be using mini batching we may be using adaptivity which might do some curvature informations there's all kinds of things all kinds of tricks that we try to employ and there's one trick that everybody knows would work and this is using Newton's type approaches but it only works in theory and practice nobody was able to really make this really work very well and we have one breakthrough paper I think which I hope in the future will be the basis of something they will work very in practice at the moment is theater Co where we developed first randomized Newton method which has certain properties which previous attendants didn't have and I don't know how technical I can be or should be but one of the property is that that we can look at a single data point at a time which is which is alarm when training commercial models or small numbers so-called mini batch of data points at a time and compute only the first and second order information coming from the model and the data point only of that and we can get a rate for training which is independent of any condition number so as a researcher with a strong math background what is most interesting to you about the intersection of math and computer science right so so in some sense the parts of computer science which are subset of mathematics so-called theoretical computer science but computer science is a wider field it includes ideas coming from engineering and other fields as well for me what is really interesting is this is the search for truth I like mathematical truth so when you can prove something well there's a theorem which describes reality you know it's always true and it can be the foundation for some other discoveries later on and now there are these truths to be found in computer science so for instance when we build algorithms for instance in the case of machine learning for training machine learning models then there are some simple settings in which we know these things work and we know how well they work and these are actual rules actual laws of nature so to speak and we know exactly what happens but unfortunately in the simplest simplistic scenarios this is not exactly how we train the models we have to go beyond that and then there's lots of engineering that needs to come into interplay so this is one of those things another thing is when practitioners come up with some ideas which seem to work but there is no proof that these do they work so for instance there is an idea I tried to call something around surprise it works ten times better than something before is it just coincidence am i lucky maybe on this data set it works but I try it another one it doesn't we see this all over again all the time then and theoreticians can come in and try to figure out is it was just consonants or is there something to be discovered can we again push the boundaries of what we know so that we have foundations which are a little bit more elevated than before so I like this kind of convergence of theory and practice and you know you talk to small about sarcastic optimization earlier and I know you're doing a lot of research here so can you talk about the interplay between data and machine learning and you know can you tell us somewhere around what this entails a potential real-world use case where it can be applied so so in fact all of the training that we do right now in machine learning and supervised machine learning so I would say 99% of all the busts that's out there there is an element of stochastic optimization behind this so stochastic optimization is the procedure with which we train machine learning models and now the question is why randomness stochasticity means randomness why do we inject randomness into the process it seems counterintuitive randomness is random maybe get a random result but there's not the case the randomness is injected because we want to learn faster it turns out one can show this imperfectly and also theoretically you inject the randomness in some very funny interesting ways there's many ways you can do this and this is active area of research where should the randomness be injected in what way and and so on and then everything is better right it's faster right and then when you think about a real-world use case where do you where do you see this being the attention is not potentially apply this is being applied as we speak in all the deployed products so for instance in in federated learning and distributed learning when you want to send messages from mobile phones say to some server you you would like to not reveal privacy okay you would like to maintain privacy not reveal the private information so one of the things that you could do you could add noise to the message arguing to send and by adding noisy office gates the data point whatever you want it to do to transmit and in this way you achieve certain level of protection but at the same time there's very interesting phenomenon going on which we found out in my group that's and this is some other work that we've done that this procedure in fact even improves the training procedure this seems very counterintuitive so what we try to achieve is privacy but we realize that mechanical also improves the training so this is very very interesting it can be captured theoretically and we can see this in practice and you're starting to cover one of the questions that we wanted to ask about of course because you're the co-inventor of federated learning which is a Google platform to improve communication efficiency as well as data privacy I know you started to talk a bit more around that because data privacy of course is so important to all of us in a hot topic and especially in AI as well and what are the current limitations though in managing data privacy so from my point of view where we're at the beginning of this whole idea of trying to merge two different fields which is privacy cryptography and and machine learning and so there's lots of rigid needs to be done some of the things we don't properly understand but we already see when we when we put some systems together that they do work to certain extent and they're practically useful and that is why they are deployed is what companies are starting up and and have products to sell so so we see huge potential there but from research perspective there's lots of other things that we don't understand so one of the things for instance exactly this precise interplay between machine learning at the training phase and and privacy protection mechanisms we don't even know whether we're using the right privacy protection mechanisms and we're using them I isolate in an isolated way so for instance there is a secure multi-party computation there are encryption mechanisms as Parsifal and quantization mechanisms and so on and so forth homomorphic encryption etc and for instance we don't have a product which combines all of them and we don't know whether there's some other encryption mechanisms which we haven't yet explored and at the same time once we do explore them how to combine them with these training methods and once we use the training methods what the network architecture should be using there's many many questions of this type and I'm very excited that we can be asking these questions testing them in the real life and seeing how that works and taking that and when you think about 10 trends and challenges and machine learning over the past 10 years what do you see have been the main learnings from that and I have a follow-up to it which is how are those learnings happiness developed the next 10 years of machine learning and where you see it going a lot happened over the last years so when I started doing something with machine learning we didn't have the book the deep learning winter was still very deep and it was freezing and and nobody was really talking about it at that point in time so a lot happened over the last 10 years and this is very difficult to really capture with few sentences so one thing that happen is deep learning which everybody talks about and which is one of the most exciting things that happen to machine learning in the last 10 years even less than that of course there has been deep learning around for a very long time but this time is going to stick I believe but what we need to do to to do we need to combine these technologies with other things so deep learning on its own is super powerful but it will not survive it will not solve all the problems so for instance we need to be using some rule-based methods combined with deep learning and this is this is very apparent in applications in natural language processing autonomous vehicles and so on and so forth if we just rely on that single tool and nothing else we will not succeed we need to be questioning all the foundations we need to be questioning whether the way we training at the moment which is through the Epicurus minimization paradigm is the right thing to do so for instance people in finance they have known for very long time that you have to take risk into consideration so there's this very famous mark of its model for building portfolios which takes into account the expected reward in correlation with risk so you want to have a lot of rewards but you know but only if the risk is mitigated and and bounded but this is not exactly will be doing with machine learning right now we are in this pre mark of its world where we don't take that risk into consideration and it's again this is one of the big things that we need to be doing so we need to transform everything by capturing the risk as well because we don't have to want to have cars which on average perform very well but every now and then they hit a building okay so we need to avoid that risk okay and how do you see when you think about the next 10 years how do you see like what do you see as being some of the things that we should look out for which are gonna be game changers in 20 in the next decade so what's happening at the moment a lot of stuff that was done in in research labs in in industry and in academia is becoming mature technology which is finding its own way into into products and this right right now I see a turning point and this is going to be transforming industries over the next 10 years so we're moving away from a research focused field so engineering focused product focused field where research will be super important but we already have figured out so much that those things that we already know right now can be immediately useful and we didn't yet except even the surface of the implications that we could we could be really getting out of this so there's lots of things we can do with mushroom in which there's no company in the world that is doing it right now and these companies will will need to be built and be doing these things so I think we'll see a lot of this but at the same time we have challenges all over the place so for instance with data one question about data is do we have enough of data so all the machinery models are very hungry for data and sometimes we have an update abut the data may be of questionable quality a lot of effort needs to be put into cleaning data now do we pay some people to do that if you have lots of data this is even impossible you upload hundreds of hours of videos to YouTube every minute we cannot be paying people to be looking at these videos to check for content so we need automated data cleaning procedures then some in some regimes we don't have enough data so we have to be asking can we learn from less data so can we do zero shot learning or can we maybe automatically generate some data through simulations or through some other procedures so that in the world where we have not enough data we simply generate it so that these machine learning models can be trained then with all of this we have privacy issues so where's the data coming from well it's usually coming from human activity personal activity we do something on our phones we do something on our computers we click online we have health records and we do not want this data to become public there have been lots of issues with data leakages with breaches and it's very clear that public is not very happy about these things we have GDP our last year in May China follows you later on so so it's increasingly important that we protect the data that these machines and models are really thriving on and now this is creating new challenges and I think this is one area which I'm interested in particularly and over the next ten years I think we'll be building this at Peter one more question for you you're based in Saudi Arabia how is AI adoption in Saudi Arabia so there's lots of stuff happening around AI in Saudi Arabia so for instance at the University I am I'm aunt which is cows King Abdullah University of Science and Technology we have been talking about an AI initiative for a number of years we have finally launched it the entire university is going to be transformed in a certain sense through AI we have autonomous vehicles on campus as we speak right now and AI is impacting science for instance so this is something which maybe not many people talk about but AI is not just interesting for industry but the AI is becoming a tool for scientific discovery and this is something which people at universities in in areas such as biology chemistry engineering are really excited about so this is one aspect another aspect is that we have national AI strategy in Saudi Arabia in a similar way that there are a lot of national AI strategies all over the world in many different countries countries increasing and realize that the business is not just a wave that's going to go away now some of it of course is high but lots of this is here to stay and we need to think ahead and we need to build these strategies and that is what we do it at our University in Saudi Arabia as well Peter thank you so much for your time today was a pleasure talking to you here at nor Norris and we hope you have a wonderful rest of your day [Music] 