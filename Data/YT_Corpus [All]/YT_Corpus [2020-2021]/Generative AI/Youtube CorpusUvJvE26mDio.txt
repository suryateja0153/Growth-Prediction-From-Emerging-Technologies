 Alright. Hi everyone. It's quite a pleasure to be here. It's my third time in Korea and each time it's delicious. The only problem is when I get back home I need to do a lot of sport. I'm also still recovering from the jet leg. It's pretty brutal I find. It's 14 hours with Montreal. So I'll try perhaps to wake everyone up with myself. So I'm going to talk about generative adversarial networks, and new perspectives on the topic. In particular, both a bit like fundamental understanding of what GANs are doing and as well, how can we train GANs. So basically, the talk will be split in two. I will first start with--actually, I will go like this--yea, we'll first start with going through the fundamentals and statistics, which is statistical decision theory. If you take any stat book, the first thing you will talk is about is decision theory, which is how to evaluate the statistical methods that you're using. And I will argue that GAN is actually a way to explicitly define much better task losses to do generative modeling. And this actually will appear in a paper which we are actually submitting to JMLR. And then in the second part of the talk I will mention something that Yann LeCun has mentioned on Facebook and Twitter awhile ago, which is that GAN is actually using games--it's game optimization-- which is very different than what we're used to doing with supervised learning which it's just single objective optimization. So then there's the question of how we optimize, how do we train these methods--it's very difficult-- and I will bring tools from standard mathematical programming-- which actually I have been looking at these for a long time-- but are a bit ignored in machine learning literature. So first let's just review what are these generative adversarial networks. So this is in the context of generative modeling, ok, so we are trying to-- We have some observations we are trying to fit some model of these observations so that we can generate a new model. For example, we could have a bunch of pretty images, and I want to be able to generate new pretty images. Actually, in the context of what Sanja mentioned, if I want to get very realistic simulators, these generative models can be quite useful to kind of improve the realism of your simulators for training robots, for example. And the GAN approach is in using what we call like implicit generative modeling or latent variable generative model. The idea is fairly simple. You start with a simple distribution, like a uniform distribution or a Gaussian, and then you plug it in using a neural network to transform it in some kind of like twisted manifold, and the whole question is how will you twist this manifold, how will you change the weights in your neural network, such that it fits the observation. And in standard density modeling, people would use maximum likelihood. And GAN, instead of using maximum likelihood--especially because it's often hard to have a likelihood model for these kinds of like implicit models--is to actually have a game between the generator-- so this neural network, G, here, we'll call it the generator--and a classifier. And so the idea is, I want to find a generator such that when I start from my noise and generate data, I will evaluate the quality of this generator by how good a classifier is able to distinguish my fake data from true data. So the discriminator is a classifier, it's on a neural network that will try to train on both, like trying to classify fake data versus real data, and the generator is trying to fool the discriminator. So it's a game between those two players, and a good generator is a generator which is really fooling the discriminator because then you get this notion that the fake data is undistinguishable from the real data It's quite a popular approach-- oh wait, I didn't mention, but this GAN was invented by Ian Goodfellow, at University of Montreal, a few years ago. And since then there has been a flurry of work, in particular, this is taken from a tutorial by Ian Goodfellow where you see the progress along time on the ability to generate fake faces, and there's an explosion of papers on the topic. There's also a zoo--basically every letter of the alphabet has an acronym for a specific GAN. I call it the LDA of deep learning because LDA, latent Dirichlet allocation, which is actually one of the most cited papers in machine learning, also had all these variants. But yea. So we have the same thing happening now for GAN. But so, why is that? Well, one reason is--I mean there's multiple reasons. One thing is it's hard to know how to evaluate these methods. Like you generate pretty pictures--here's my different, I don't know, WGAN, VEEGAN, rhinoceros GAN, or something--and generate different pretty pictures and they all look pretty, so you know, here's a new paper. So it's really hard to kind of like benchmark things in this, so also to explain so much different approaches which are tried. But the other thing is that, actually, it does work fairly well. Like it did very impressive pretty pictures in particular. But I would like to look at it more from a fundamental perspective, which--and so this is appearing in this paper called "Parametric Adversarial Divergences are Good Task Losses for Generative Modeling." And the leading author is Gabrial Huang. The main idea is in this slide, actually. Ok, so the idea is, I'm trying to solve what I call the final task, which is hard to formalize-- so for example, I want to do machine translation or I want to generate pretty pictures. So what are you exactly trying to do? Right now, it's a bit more like an intuitive question. And in statistical distribution theory, you will try to formalize how to evaluate what you're doing with something which is called a statistical task loss, which will say, you know, given what my model is outputting, how happy I am. And I make in this paper and this work, we make a link with structure prediction, where this notion of-- for example, machine translation could be seen as an example of structure prediction. So I come from an input sentence in English, I go to Korean, so the output as like a structure, how do I evaluate, for example, the errors I make. Usually you'll use something like BLEU score or something, and so what you do, for example, in this case is you're trying to get a classifier h, given an input data x, and label y, which has a small error according to your structure loss l, which will characterize how we evaluate mistakes. And the goal of your training mechanism is to learn this prediction function, so that if you have test data--which I call it like here with the expectation of our x, y, according to p--you will do well. So you will have low expected error, and this is called the generalization error. And so the generalization error is the task loss for supervised structured prediction learning. So i.e., your learning algorithm will have some training data, it will output some classifier, how do you value this classifier? Well, you evaluate it on this test data, and that's this task loss. Ok, so that very clear and supervised learning. So what the analogy in generative modeling. Well, generative modeling instead of outputting a classifier, what I'm outputting is a distribution, so in this case, for this distribution, we call it q, ok? And the question is, ok there's some true distribution in the world, p, where I have some data from, and I output it q--how good is q? So then you need a notion also of how to evaluate it. And the analogy here is that the GAN framework will enable you to define these things called adversarial divergences, which measure how bad q is with respect to p, and these adversarial divergences will depend on something which will be the particular type of discriminators we use. So the same way when we change our error function in a structure prediction problem, so instead of going from BLEU score, if we say, count the number of words which was wrong, which is the Hamming score, then you get a different notion of error. It's the same way in the GAN models when you change class of discriminators, so you go from a CNET architecture to say a ResNet architecture, or something like that. It will capture different things. And then the question is ok, well is this a good task loss, and in particular, you could have this cartoon here where you would have the different possible q that you want to output, and then there's this, in green here, there's this task loss that we're unable to formalize and it's hard to get. So a good task loss will be following somewhat, roughly, the true things that you're trying to do. So for example, generate good translations. And in red here is a bad task loss in the sense that yes, the minimum, like the q which will minimize my task loss, will be the same as the final loss, so it will be good. But it's way too harsh, in particular there are other models, there's other qs which would have been as good according to my final task loss. For example, they could be as pretty images, but my, the way I formalize it with my task loss, will be too harsh, I'll say these are really bad. And particularly this will make learning much harder, because I won't have signals to distinguish this model from this model, because they're equally bad according to my task loss, ok? Alright, so that's the big picture for this paper, and then the main question is, well, then what is the relationship between the discriminator and the properties of my divergence. Ok. But let's just dive in a bit more specifically to illustrate these things. So GAN, I called it generator, I'll call it q. Well the generator will be parameterized by some kind of theta, and specifying theta will specify my distribution q, and the discriminator will now try to distinguish my q from the true submission p. And so, if I call D-phi the output of my discriminator, which is basically the belief of whether x came from the true distribution, then the standard original GAN population objective was basically there's this logistic regression. So logistic regression, I will have that--I'm trying to maximize the log on the probability of the true data and then log of one minus my belief on the fake data. And so that's my classifier, it's trying to maximize a log likelihood. And the generator is trying to fool the discriminator, and thus it's minimizing, so it's a min/max problem. And in the original paper, by Goodfellow et al., they say if D is infinitely powerful, you can actually solve analytically the inner maximization problem. What you get is basically the Jensen-Shannon divergence between q and p. So a Jensen-Shannon divergence is basically some kind of like symmeterized version of the KL divergence, so it's basically a KL divergence in some sense. What I will argue in the next few slides is that actually the KL divergence is not a good task loss for the kind of thing we want to do, especially in high dimension. It's too harsh. But, in practice, people who do GAN don't use all functions as their discriminator, they actually parameterize them--for example using a neural network with a specific architecture-- and in this case, when you maximize with respect to a smaller class, you get a different divergence. And this one will give you a much more meaningful task loss. And we call them "parametric adversarial divergence" because now you have a parametric assumption on your classifier. Ok, so why is KL divergence bad, and I make analogy with the zero-one loss, which has been new in the structured prediction. Which we can actually formally show there are problems with it. Ok, so structured prediction, as I mentioned, I want to learn a mapping from input to discrete output, like in machine translation, and one property of such a prediction is that the discrete output space is exponential in the size of your input object. So it's combinatorial explosion of possibilities. And another property is usually in practice people will use a structured error function on the output to say how bad your prediction is, because if my whole sentence is wrong, it's not the same thing as if only one word is wrong. So it's not like, oh you're right or you're correct, there's also a lot of different degrees. And in a recent paper that we had, we were trying to find, you know, how well can we do in this task in kind of like the simplest setting possible, in particular to try to compute the simple complexity. How many samples do I need to get a good performance when I do surrogate loss minimization? So surrogate loss minimization is, I will construct an approximation of like the empirical error that that will minimize and that's how I will learn my classifier, and we were looking at the consistent setting, i.e., the setting where when you have infinite data, you actually get the correct solution. With a few caveats, what we showed is if you used the zero-one loss for your evaluation measures-- the transition errors using zero-one loss, and then your surrogate loss will also depend on your zero-one loss-- you need an exponential number of samples in the worst case to learn. So it's too harsh, this loss is too harsh. In general, you won't be able to learn. Whereas if you have something more structured, like a Hamming loss, which counts the number of parts which are wrong, in this case we could actually learn using a polynomial number of samples, which is log of the--basically the number of parts. So the takeaway from this fairly theoretical result paper was that zero-one loss is much harder in the worst cases than structured losses, and so if what you care about is a structured loss, you should not train with a zero-one loss, because this is just way too hard. And the analogy I want to make here--which is an analogy, this is not like a formal equivalence-- is that the KL divergence behave much more like a zero-one loss for this super high dimensional space. And that's why actually it doesn't make much sense to do it on high-dimensional data, like an image. I can, on the one or two D signal, like in standard statistics, sure, no problem to use the KL, but for an image in a million dimensions, it doesn't make any sense. And in particular, when you use a KL divergence, you don't care how far--all you care about it whether the mass falls on where the data is. You don't care how--whether it was close to where the data was. And close in which metric? Well, you could think, for example, of, say, in the discrete case, I just order all my outputs like in machine translation using BLEU score distance from, say, some reference translation. And the distribution in red here, and the distribution in red here, will have the same KL divergence with this, perhaps like the target distribution. Even though this one put mass on much, you know, more similar translations than this one. Alright, so the analogy that I make here is that we want to have more meaningful distance of distribution in order to be able to do reasonable, unsupervised learning of structured data. And from the GAN perspective, the question then is--I already argued at the beginning-- that the GAN is basically implicitly defining these task losses. The question then is, what are the properties of the discriminator which relates to the properties of the divergence? And just to formalize it a bit more clearly, there's this general definition from Liu, Bousquet, Kamalika-- they had these adversarial divergences which are basically this max over these differences of expectation, the details are not that important. But what's important for us is that a lot of standard divergence can be read into this form. In particular f-divergence, which is like KL divergence, the Wasserstein distance that has been popular more recently, or the maximum mean discrepancy. So all of these are special cases of this framework, which can be stated as a game with a discriminator. So they're all at max. So the question is, what's the constraints on my function and, you know, how am I comparing these expectations. But in all of these cases, like f-divergence, Wasserstein and MMD, I call them non-parametric because the space of functions, the space of discriminators I'm considering, is infinite dimensional. So it's all possible--say for example, Wasserstein--it's actually all possible functions which are lipschitz continuous with a constant less than one, ok? That's a lot of functions. And when we do GAN, we actually don't use a non-parametric function class. Usually we specialize our discriminator to a specific class, like neural networks with a specific architecture. So in this case what you get is a parametric version of these divergences. So you get, for example, a parametric version of the Wasserstein. So it's not the same thing as a Wasserstein. So that's important to know. Like in the original paper, where they called it the Wasserstein GAN, they made a big deal about why Wasserstein made more sense. But actually, we argue in this paper that having a parametric aspect is very important. So why is it important to have a parametric is because we can look at what are the properties of these divergences in terms of sample complexity, in terms of computational aspect, in terms of can we encode easily the prior about our task? And it turns out that, for example, the non-parametric Wasserstein, which is a standard Wasserstein, in order to estimate it with finite data-- if I wanted to go compute the Wasserstein between two distributions when I only have samples, I don't have the full distribution-- you need an exponential number of samples in the dimension. So it's really hard to have a good approximation of the Wasserstein. Whereas if you go to the parametric Wasserstein--i.e., you will have, I don't know, perhaps, like, neural networks inside it-- you only need now a polynomial number of samples to approximate. So it's much better behaved statistically. Alright, so let's just look at some experiments to highlight these concepts. So the first one was trying to compare learning, general modeling by minimizing the Wasserstein distance between the empirical distribution and your model, versus doing a GAN-more approach. So there's the question of how do you do the Wasserstein-- the non parametric Wasserstein. So there's a lot of technical details which are swept under the rug. You can use a sinkhorn approximation, et cetera, et cetera, et cetera. But the main thing to take away is, if you try to learn this on an MNIST dataset, so a very simple dataset, you compare the samples learned from using non-parametric Wasserstein versus WGAN-GP, which is a tweaked version of Wasserstein GAN, they are basically both good. And it's fine. There's no problem. And so, here it's fine. But when we try to do that on a more complicated dataset like CIFAR-10, then non-parametric Wasserstein, we were not at all able to make it work, whereas when you go to the parametric, like WGAN-GP, it was working. And here, the idea or the intuition we had is because the difficulty of the datasets, like the intrinsic dimension of the data is much higher in CIFAR-10 than in MNIST, that's where, for example, the sample complexity of Wasserstein kicks in. It behaves not very well in high dimension. It's too harsh, whereas the parametric version of GAN, which uses neural networks, actually is able to do something. Another example which highlights the different between, say, KL divergence, maximum likelihood and GAN is how to learn to generate Thin-8. And why Thin-8? Because usually when you have, say, a maximum likelihood model, you can put the Gaussian distribution such that you on your last layer so that you have a full dimensional distribution. And so basically, then you get--it's not like standard KL, where you only care about where you put the mass, in this case you get an L2 distance between your pixels, so that you actually are able to learn something. But then you get burry images usually, and if there's no overlap between two samples, then you won't get any signal. And so, when you have a small resolution, 8, then we compare WGAN-GP--trained on this versus a variational auto-encoder, which is basically an approximation of maximum likelihood--and both work fine. But if you go in a high dimension, like here it's 512 by 512 pixels, the GAN has no problem, but the variational autoe-ncoder starts to suffer a lot because, indeed, you had very little overlap between your digits. So in some sense, the divergence you get from GAN was able to get a much more meaningful signal. And finally, this last experiment I wanted to show here is with-- it's a surrogate task to kind of encode constraints. Let's say I want to have a general model which models the real world, it will have to satisfy a physical laws. For example, things cannot just float in the middle. So there are some constraints that my data will have to satisfy. But what if I want to figure out these constraints implicitly. And so, this task here is we will want to generate five pictures, five digits, but with the constraint that they will always have to sum to 25. But we won't tell the model that this is the case, we'll just give it data that's a sequence of five digits, which are always summing to 25. And so that's actually pretty easy, from a GAN perspective, to see whether five digits sums to 25. You just pass it through your CNN, you classify them, and then you look if the sum is equal to 25. So from a classifying perspective, it's easy to check whether this constraint is satisfied, which could indicate that the GAN framework would be able to implicitly learn this. Whereas from a maximum likelihood perspective, the fact that all your pixels are in such a way that, oh yeah, this will sum to 25 is really hard to encode. And indeed, if we compare, again here, VAE versus GAN, so it turns out that we can just--so these are the kind of samples you can get from-- on the left are VAE samples, on the right are GAN samples. They all look fairly decent in terms of image quality, but if you look at the sum of the digits-- so for example, I have here 3, 12, 20, 25--ok that was luck. 5, 11, 19, 24. Ok, not too far from 25. Whereas if you look at the VAE, the sum has nothing to do with 25. And the fun thing here is we can actually check it by running classifiers on the digits to see what's the sum. So that's what I was planning here was the distribution of the frequency of what sum do we get when we sample from our model. And in red is the VAE, which is not centered around 25. The GAN samples are actually centered around 25. It's not exactly 25 all the way, like, it's not perfect, but it was much better. Ok, so to conclude this part. We argue on the importance of looking at properties of parametric adversarial divergences. This is to be contrasted as first of all, the Wasserstein GAN paper which mentioned that they would kind of present these GAN as an approximation of the non-parametric version-- as the non-parametric version is the thing we really care about, but we're saying no, no, the parametric version has better properties. It has better task loss to what you're trying to do. And in the future, we would like to study what are the invariance properties, or the statistical complexity properties, or the relationship with the task that you care about, or depending on how we change the discriminator. Alright. So that concludes the first part. So in the second part, I will now talk about how do we optimize these GAN? So here I just swept all this under the rug. I said we trained, but how do we actually train these GAN? This is joint work with Gauthier Gidel, Hugo Berard and Pascal Vincent, and appeared at ICLR this year. And so it's looking at the fact that we're working with games, and so the original objective I mentioned-- we said it was a min/max problem--to do the original GAN. So the min/max problem-- in mathematical programming they're often called saddle point, because the equilibrium point which in the min max actually looks like a saddle, like a horse saddle-- but know that this is only true when it's convex/concave. In general, you cannot even switch the min and the max if it's an arbitrary non-convex and non-concave objective. So in the original GAN formulation, it was a min/max problem, but then in the original paper they actually proposed a different objective, which is what they ran in practice, which is called a non-zero sum game because there are different cross functions for the two players, which are not just a sign switch. So if the cross function for the generator is just the negative of the cross function for the discriminator, we call it a zero-sum game and you can rewrite it as a min/max. But in general, they could be different, and in particular, in the original paper, what we now call non-saturating GAN, they actually use a different objective for the generator and the discriminator which are not just a sign switch. So how do we solve these non-zero sum games? Well, one pet peeve I had with the literature was they had these counterexamples. Saying, oh these are hard to optimize because you get association and So in this tutorial by Goodfellow, for example, on the very simple bilinear objective, if you plug the gradient-- so theta or the parameter for the generator, phi is the parameter for the discriminator-- and so this is the gradient to optimize the generator and this is the gradient to optimize the discriminator. And so if you're just trying to optimize both at the same time, you will circle and never converge to the actual equilibrium, which is in the middle. And so they say, oh, this is hard, like if you just do gradient method it doesn't converge. But this has been known for many years in the mathematical programming literature. And indeed, the standard gradient method does not converge; even if you use alternate gradient in this case, it won't converge, it will just associate. But there are methods which are convergent, in particular, if you average your iterates, then it will converge. Or there's this method called the extragradient method, which will also converge. And so, these methods come from the literature which is called variational inequality, which generalizes the min/max problem, even non-zero sum game, to a much more generalized framework. And so, in this paper, we reviewed a bit this literature to try to kind of import these tools for training GANs. So, I'll briefly mention what are adversarial inequality because I think it's good for general culture. So the idea is, it's based on stationary conditions, ok? So I want to minimize my objective for the generator. So what does it mean? It means that if I take the directional derivative of my parameter in any feasible direction-- so suppose I had some constraints-- for example, in the Wasserstein you have the constraint that you need to be lipschitz which give constraints on you parameters, which are bounded, so you have constraints. It's not unconstrained optimization. So basically, you want that product between your gradient and any feasible direction are positive. So that means that any feasible direction will bring you up. Then it's a local min. Same thing for the discriminator objective. And so what you do then is you can just massage this equation by redefining the concatenation of the gradient of the two different objective functions as this vector field, F. And then also, you would consider this concatenation of the parameter of the generator and discriminator as just some variable, x, and this stationary condition can be written as, 'I want to find a point such that the deduct product between my vector function, x star, and any physical direction is positive.' And finding a point, x-star, which has this property is called a variational inequality. And that's generalize minimization, min/max, non-zero sum game, even stuff called--it's called equilibrium problem, which is using traffic flow modeling. And it turns out you can solve all of these with the same algorithm, in particular the extragradient algorithm. And so, what are these convergent methods that people use for solving variational inequality? I mentioned you could take the gradient method and average the iterate. Or there's this extragradient method. So let me go into the extragradient method because I think it's a very cool method. So the idea of extragradient update is to do a look ahead step instead of just one gradient. So I will start from my current entrant, I do a small step in the negative gradient direction-- so F here is the concatenation of the gradient of the two different objectives. So that's how you update the parameters of your two different players, you make a gradient step. But this is to see where our will be in the future. You compute the gradient at this future step. And then you move from the original point, and that's how you update. So it's called extragradient because you make two gradient computations per iteration. And you can see this update as an approximation of an implicit scheme which actually is super stable. Because if we're unconstrained, finding--solving-- the variational inequality is finding a point where the gradient is zero, right. This is a global--it's a local min. And if W-t is equal to W-t plus one, then that will imply in the scheme that F of W-t plus one is zero. i.e. I have solved my variational inequality. And this is called an explicit scheme because I put the W-t plus one both on the right-hand side and on the left-hand side. So if I was able to compute this implicit update-- which you can actually do in a non-linear, fixed-point equation-- it turns out that this converges super fast for any step size. So it's a very stable method. But the problem is that computing one iteration of this implicit scheme is super expensive because we need to solve a non-linear set of equations. So the extragradient can be seen as a first order approximation of this by approximating F of W-t plus one at F of W-t plus one half. Ok? And that's it. And then it turns out that extragradient method has very nice properties, it converges, it's more stable, you can run it on this, for example, bilinear experiment. I won't go into this in more details. I will mention something important, that all these guarantees are from convex minimization. Now we want to do GAN with deep neural network. So we have entered a twilight zone where all the guarantees go out and you do a bunch of hacks. In particular, you can say, oh, people use Adam to solve GAN, so I will just adapt now Adam into extragradient format by using Adam for the extrapolation and another Adam for the update. Ok, and so that's an algorithm that we have no idea if they converge, but you can try, and actually it works pretty well. And so that's what we will do in our experiments. But I will mention something which actually I think is one of the most important messages of this second part of the talk. And it's summarizing this picture. It's very simple idea is that noise is a huge issue in these games, differently than standard minimization. Ok, so where's the noise coming from? Well, there's noise coming from the fact that we have finite data, but there's also noise coming from the fact that we do, as SGD, for computation reasons, we use minibatch to compute the gradient, not the full batch gradient. And when you do minimization, you want to try to find this minimum, which is here. And what I'm plotting here is the gradient field, so the negative gradient direction. And so, to minimize, you want to follow the negative gradient direction, and this could be, for example, the red arrow. So when you do SGD, you follow a noisy version of the red arrow, which on average is actually correct. And it turns out that, you know, if you go in these directions, you still roughly go in the right direction, particularly you go in the sublevel set. But in a game, the gradient field is very different. Ok, so here I'm plotting the gradient field for a bilinear problem. And so if you look at the gradient of these two objectives, they are actually circling. That's why the gradient method was associate. So now if you add noise, for example in yellow, you will have that sometimes the noise will just make you diverge, it's really, it seems much worse than in this situation. And in particular, in our paper--in this different paper, which will appear at NeurIPS 2019--which is joint work with Tatjana Chavdarova, Gauthier Gidel, Francois Fleuret-- we show a counterexample where the standard extragradient method in a stochastic version diverged. So the fact that you have noise suddenly makes all standard methods break. Which is surprising, because when you have SGD on minimization, it doesn't break, like you can always make them converge for a complex position. And so in this paper which will appear at NeurIPS, what we did was added variance reduction to reduce the noise for solving this game, and it turns out it makes a big difference, in particular, this example where all the standard methods don't converge, you add variance reduction and suddenly now you can make a convergence method. So what's variance reduction? That's actually a big idea from optimization in the last ten years, in particular, not this paper but I've been reading a paper won the Lagrange Prize for optimization last year, which is one of the biggest contributions in optimization in the last three years. And so the SVRG method--is one of the standard methods to do variance reduction-- what's the idea for that? Well the idea is, I want to minimize an empirical sum of functions. These could be your individual loss on the data point. But it's too expensive to compute the full gradient on the whole batch, so what you do is compute it on a mini batch, ok? But then you get noise. So the idea of variance reduction is you will use a bit of memory to stabilize your method. And so the idea of SVRG, it's an epoch-based algorithm, so at the beginning of each epoch, you actually go over your whole training set and compute your batch gradient. That's super expensive, but you only do it once every-- a lot of iterations--so you don't do it often. So it's an expensive iteration that you do very rarely. So you compute the batch gradient at a specific stored parameter, which we call the snapshot. Then when you will do the actual--with an epoch, you will do multiple SGD style updates where you will use the standard stochastic gradients, so just the gradient of one of these functions that you've sampled one data point. But then you'll add this correction step, which is the difference between the batch gradient at the snapshot versus the stochastic gradient at the snapshot. So this is indicating of what's the error made from the noise, and then you'll correct your stochastic gradient at the current iterate with this correction. And it turns out that this is sufficient to reduce the variance of SGD in such a way as you can get-- particularly a linear convergence rate for SGD with SVRG with a fixed step size, whereas it's impossible for standard SGD to do that. So in convex optimization this makes a huge difference. Funny enough, people have tried that to do deep learning optimization and it didn't do anything, it has not really helped generalization. I guess the fact it's not convex changes a lot of things, but from the cartoon I gave you for games, noise is a big issue. And in particular, big GAN--which is a paper from last year from Deep Mind, which got state of the art results on ImageNet for generating images--one of the changes they made which made the biggest improvement was choose a bigger mini batch. So they increased the batch by a factor of sixteen I think. And increasing the mini batch size is equivalent to reducing the noise on your gradient. And so this was kind of an empirical indication that perhaps reducing the noise would be useful for GAN And so in this paper we've basically combined this variance reduction with extragradient and just run it on GAN, see how it does, and it turns out that it's actually stabilized tremendously the method. So here what I'm displaying is--this is on CIFAR data, this is as the number of iterations, what's the Freichnet inception distance of the samples of your model. This is basically your standard measure of quality of measures, ok? And this is using standard methods like Adam, basically, on fancy architecture, and what we are plotting here is different curves for different hyperparameters because you need to tweak your hyperparameter for Adam. What's the momentum parameter? What are the step sizes of the generator vs. the discriminator, et cetera, et cetera, et cetera. And for some of these parameters you get very good results. But one thing that actually people have noticed in practice is that you get very good samples, you go to sleep, you wake up, and then you get gibberish. So, basically, all these methods actually diverge at some point. So they get well, and then you go to sleep, but then they start to diverge. And so it turns out that for this deep architecture to get really the best samples, you get this very unstable behavior. So if you compare with SVRE, you have that for a bunch of different hyperparameters, you get this very stable behavior. So they all basically stay, they don't go crazy diverging-- often people call it mode collapse, by the way. Some does better than others, but it's actually much stable. And the difference here, was this was Adam-- Adam is adaptive step-size method with momentum, et cetera--and it turns out that I'm not aware of any non-trivial GAN experiment where people could make it work with just fixed step-sizes GD-- which in standard supervised learning, deep learning, you can use fixed step-sizes GD with perhaps a schedule. That works fine. But for GAN, people always either use either Adam or perhaps Adagrad, or something like that. But here, for the first time--as far as I'm aware-- we are actually using just fixed step-size GD. This was variance reduction. So reducing this noise really makes a big difference for games. The problem, though, is we were not able to combine variance reduction with adaptive step sizes like Adam yet. Ok, so this is an open problem. And so it turns out also that we don't get as fast of convergence. So we're not able yet to get a state-of-the-art results with this method. So if you want to use it right now, the use case is, you can use Adam to get your fancy, best image result here before diverge, and then you can keep improving it by running our method, because it's very stable. And actually it does keep improving after that. There's also an issue of computation. So at MILA, we don't have that many GPUs and frankly, and it turns out that the hyperparametric tweaking for this method is extremely expensive. So I'm calling, basically, Deep Mind, or FAIR, or these people to try this method. Alright. So to conclude, by what we presented with GAN was making links with the variational inequality literature to be able to import standard techniques-- optimization techniques--to stabilize the training of GAN, and interestingly, unlike in supervised learning, controlling the variance is much more critical in games, according to our observations. And so we proposed a method which combined extragradient, which is a better technique for games, with variance reduction. And actually, it turns out we can also prove it has the best convergence rate under some assumptions for games, and has good stability properties. And in the future, we could try to design other algorithms specific to GANs inspired from this mathematical programming literature, and it's still an open question, for example, how to do adaptive step-size variant of SVRE. And if you're interested in these games topics, we had a NeurIPS workshop last year and we have another one this year on smooth games optimization and machine learning workshop. The videos are online. Kamsahamnida. 