 alright let's get started so welcome to lecture 2 the first real content lecture of deep unsurprising let's start off with logistics class registration we work through all the applications on Sunday some of you got a code to register use it as soon as you can then some of you got an email saying you will be somehow in the course but we can't send you a code yet we just received a code so we'll try to send those codes tonight or tomorrow and some of you have got an email saying that well unfortunately the class is full actually you're welcome to audit but you really shouldn't count on getting into the class it's always possible I mean if some people drop we're not gonna keep those spots empty we'll let more people in but there's something like 50 people on the waitlist still and probably gonna be like a handful of people dropping I imagine so don't count it for your units but you're welcome to audit and if something's Oh something that was up will of course fill the spot other things about logistics our first homework will go out tomorrow or maybe Friday but we're hoping tomorrow I know we'll be due two weeks from yesterday yeah that's it photo justice any questions on that side oh the other thing is we actually post slides before lecture in this case like two minutes before lecture it will try to post the night before when whenever we can get the slides ready tonight before but last night the slides were still too far from what we're gonna use today and it wasn't meaningful to us to post them yet but now they're posted on Piazza and in general we try to do the night before all right let's dive in then autographs and bottles outline for today we'll look at some motivation why we're going to be doing what we're doing then we'll look at some very simple generative models histograms which I suspect all of you seen before but it'll help us ground a little cover next into things we're already familiar with and then the main chunk of lecture will be the modern neural auto regressive models and you'll see that this menu will actually expand a little bit the further in the lecture the more sub menus will appear on the topic so I'm asking base models is probably really two thirds of the lecture even though it looks like one bullet point right now okay motivation so what's the problem we'd like to solve well there's a few things we'd like to do with the models we train on our unsurprised data one is generating data that means just synthesize images videos speech be able to generate new images let's say that you've never seen before but that somehow look like the images that you trained on so they look like they're within distribution compressing data we're gonna have actually full lecture dedicated to compression but kind of the preview at this point is that if you can train a model well let me take a step back when you look at the theory of compression the quality of compression of your data is essentially going to be determined by the entropy of your data and 2p is a measure of kind of disorder in your data and the lower the entropy the more you can compress but to achieve that limit they theoretically can achieve you actually need to learn a model that moles distribution of your data as precisely as pulse and so the more precise your model of your data then when you do entry based compression you'll actually match with the data in the real world the way you design your compression scheme so the better our models the better our compression will be anomaly detection imagine you get you actually are doing supervised learning and maybe an image comes in for a self-driving car but it's very different from things you've seen before if all you have is supervised when you have multi way classification it is kind of forced to make a decision what this might be and it's usually not particularly well calibrated when it's out of distribution to do a uniform distribution on everything because you haven't really trained it to do that so if you had a generic model you can evaluate the probability of the image you're seeing under your generative model and that probability is very low compared to the probability of training data then it means this is an unusual image that you're seeing and maybe you cannot trust your classifiers decision on that image so unlikely based more likely based models we estimate the data distribution from some samples that come from the data distribution so and reality there is a true data distribution that you don't have access to you have some samples from it and the hope is that you can somehow fit a distribution to those samples that is close to the underlying data distribution what we want is a dispersion Peter allows to compute the probability of another sample whether it's an image video speech and so forth I'm gonna be able to compute the probability I'm sure this is all actually we're able to compute a probability of your sample and you are able to sample from the distribution and well I should see that it's not always the case that just because you can evaluate the probability that you can sample easily and it's not because you can sample easily that you can actually evaluate the probability with the models we'll see today it will be possible to do both it will turn out computing and probability will be a lot cheaper than generating samples that's a trade-off we're gonna be making with the models we cover today today we'll look at discrete data but the so ideas that we've covered today are actually carry over to future lectures where we'll have continuous data and actually popping one level up another thing that kind of is a common thread across all the work on unsupervised learning is that yes one part is it's unsupervised learning another part is that a lot of innovation happens in your net architectures to make this model succeed and then all that architecture innovations tend to carry over to other types of learning whether it's reinforcement learning or or supervised learning okay so what would be an example thing we might want to get out of our models we're gonna have maybe a distribution of complex high dimensional data for example 128 by 128 images which are not even that big I mean that's about a little more than 10,000 pixels you often have megapixel images or 10 megapixel images so 128 by 128 then times 3 because there is red green blue channels a total of 50,000 dimensional space so even what seems like a pretty small image actually lives in a 50,000 dimensional space and so we're gonna if we try to model this region over images we're gonna have to somehow model distribution in a very high dimensional space so gonna have to keep that in mind we're not going to be happy we just load them things that only work in low dimensions what do we want we want computational efficiency and statistical efficiency computational efficiency means that whatever operation we do should not take much time statistical efficiency means that it should be such that we don't need an insane amount of data to start to understand the patterns in the data with the model we use so completely it means we want efficient training both computationally and statistically and model representation we want expressiveness and generalization so we want to be able to express you know let's say any image or any sequence of words or characters or any sequence of sound generated we want good sample quality and speed and further down the road we'll see if we have good generative models we should be able to turn it into a good compression scheme so let's start with something I think all of you are familiar with to just ground this discussion histograms so the goal we have is estimating somehow the underlying data deviation from samples from that distribution let's consider a very simple scenario to build some intuition let's imagine we have a discrete random variable and you can take on the values from 1 through 100 and then we sample from the distribution and we can actually then plot what these samples look like and what the plot him is if something is high means that that thing was sampled more often so and the trend said there was the highest peak is around 0.06 so between 6 percent chance of sampling the number 81 it seems so that's what that graph means the probability distribution induced by the samples well if that's your sample distribution a very simple thing to do to model it would be to just say okay what I just plotted here is already my model I'm just gonna say probability of you know 81 is 0.06 probability of a 1 is seems like really really low some very low number and so forth and so that's what you that's what you can do and you could call it done it's a very simple scheme it's efficient both for learning because you have to do almost no competition for learning and it's very fast a sample from and we'll see that in a moment and the idea of how we sample from this will will need in the future also so inference is just a lookup if somebody tells you I got an 81 you can just look in a table and say Oh 0.06 and you're done sampling how do you do that the way to sample from an arbitrary distribution for a one-dimensional distribution is to look at the cumulative distribution what does that mean we can go back to this graph here I'm going to say ok the total probability mass under this graph should be 1 and what I can do is I can sample a number between 0 & 1 uniform distribution I assume I have access to that so it's a primitive we'll assume we have access to uniformly sampling from the interval 0 to 1 well that sample and maybe it's I don't know the sample might be 0.5 or something if the sample is 0.5 that would kind of count how much probability mass we encounter going from left to right I'll do it until we have encountered 0.5 probability mass and wherever we are once we have encountered 0.5 probability mass that's the number we choose if we sampled 0.9 we do the same thing we'd kind of run through see once we when do we have encountered 0.9 maybe somewhere around here and we'd output the corresponding number and there's a very generic way to turn a uniform sampling a uniform distribution into a sample from arbitrary solution question yes yeah good question so the question is how do you even represent this in higher dimensions you're essentially anticipating two slides down so a perfect question and we'll get at it there so I've talked about the chemo distribution by looking at the histogram in principle you can pre-compute this you can essentially every step along the way have a number that you store and so then you'll have a graph that goes from 0 up to 1 progressing and as you have more and more probability mass encountered you can essentially just do a binary search on that graph to find the point where you have whatever your sample was 0.7 okay so at this point are we done well let's see perfect question was asked how do these in high dimensions even amnesty which is even smaller than we talked about earlier and images are I would say 28 by 28 and binarize damnest we turn them into white black pixels so it's just one bit but even so that's about 10 to the 236 probabilities we need to estimate if you want to build a histogram that's the number of different images that are possible in a 28 by 28 binary image so if you build a histogram for that well if you get some training data there's no way you're anywhere close to 2 covering that space so it'll get very sparse training data points relative to the space that you're working in and your histogram will be fairly meaningless because I'll not really capture patterns in your data give you more concrete numbers the emne Stata sets as a digit data set that a lot of people use for kind of you know very fast quick first experiments typically you have about maybe 60,000 training examples and so 60,000 is well 10 to the 4 let's say 10 to the 4 10 to the 5 which is much smaller than 10 to the 236 and so clearly that existing data set is not going to cover any part of this space and a histogram will look meaningless so that means even though it might still be very efficient to use a histogram when we didn't see test it I will always say probability 0 because I'll be different from our training data and not generalize so not a great thing to work with there's another thing even for a single variable learning histograms can actually be pretty problematic thanks Wilson because even for a single variable when you let's say get some data from your underlying data distribution your training set look at the test set sampled from the same distribution they actually don't look exactly the same and there are some kind of holes in the training set where you say well if you look at this proud of the pattern is that this should be a lot smoother shouldn't just assume that just because one over it's a lot lower that that's actually true is just a random thing in the training data and so even for this kind of distribution it gives pretty poor generalization to just use a histogram what else can we do well ideally it might fit something like this so you might fit some kind of smoother distribution to the training data such that that smoother distribution can actually generalize to the test data okay let me switch to let's take a two minute break so I can switch to my iPad thanks for getting that Wilson okay there we go great so we're going to parameters our models our data will still come from the distribution and we'll introduce function approximation so instead of having a histogram we'll have some model P theta of X where theta are our parameters that we're trying to figure out and we wanted to get as close as possible to the data distribution so the question is gonna have to ask is how do we design these function approximator to effectively represent joint distributions over maybe high dimensional variables X yet at the same time keep it easy to Train and then there will be many many choices for our model design and each will have some trade-offs and we need to think about those often thing to keep in mind is that choosing your model architecture and your training procedure I should kind of go hand in hand so certain procedures will work better with certain architectures so fitting distributions what's the underlying principle there is some true distribution and of course we wish we could represent with our parameterize distribution P theta and effectively it's just a search problem you have a parametrized set of distributions now as you vary the parameter theta you'll have a different different distribution very simple case Gaussian theta would be mean and variance as you vary theta this very mean and variance you have a different distribution and different choices will have a better fit to your data and so we have a loss function that's supposed to capture how well the choice of parameter theta leads to a distribution that's a good fit for our data what do you want from this loss function a search procedure wanted to work with large data sets so no number of data points could be millions want to find a theta such that P theta mattress P data as close as possible and keep in mind we can never see P data you can only have access to individual samples from the data distribution so we're going to want it to generalize from those samples the most common objective when you look at estimating distributions is the likelihood objective so the idea here is that you want to find a set of parameters theta such that you maximize the probability of your data so P theta of excise per mil of data point I maximizing probability of the data is the same as maximizing log probability of the data so we want to maximize the sum of the log probabilities of the data points but because in all most of the kind of learning frameworks people have minimization setup so we're going to say we want to minimize the negative log probability of each data point so this exam tells that if the mall time was expressive enough and enough data is given then solving this problem we'll use real parameters that can also generate the data we express it here as minimizing the negative log probability or maximizing the log likelihood of the data equivalently you can think of this as minimizing the KL divergence which is a very common metric of divergence between two distributions the KL divergence between the empirical distribution of the data samples and the parametrize distribution P theta of X and so you think of it as okay minimize dark ale or maximize the log likelihood okay how to maximize such an objective or minimize the negative log objective well the common tool in the toolbox is do local optimization is not clear how if your parameters are spaces very high-dimensional it's not clear how you're going to exactly find which one's the best one by looking at all of them you're going to just make a guess and then tweak your parameters locally to become better and better and better so stochastic gradient descent what does it do it looks at the objective and says okay let me compute the gradient of the objective take a step in that direction if it's SGD maybe on just one data point or a meaning batch of data points say degree and of the objective take a step in that in the descent direction and repeat that's a cool question anybody everybody here has seen great SGD and Maxim likely before ok great so now they know what we're gonna do it's a question of designing the model we're going to choose models there are deep neural networks so P of theta will be a deep neural network which will allow it to be very expressive question so question is if we try to maximize the likelihood how well with the histogram score so the histogram will score really really well but the way we're going to maximize the likely we're gonna have a train and validation set and we're going to track like lead on the validation status validation set and so when you work with a histogram you're actually gonna have pretty poor score on the validation set whereas with a parameterize distribution as you optimize likely no training set you'll see it improve also in the validation set at some point you'll stop training because you're you would otherwise start overfitting the training set it's a very good point in if all you care about is likely on the training set histogram will score really well but we want to generalize so we actually want something that does well on the validation set and then later test set so in terms of representing this the thing we need to think about is when you have a neural network you need to and you want to represent a distribution you need to somehow output the probability for each possible image let's say so you feed in an image and it's supposed to say this has this much probability and so what does I mean outputting a number between zero and one so you need an output to be serum one that's easy sigmoid will do that for you but then the other thing you need is done you need to be the case that if you up with every possible image that could ever exist that has nonzero probability I'll put all of them one at a time you look at the output saying is sum it up it's got a sum to one otherwise it's not a proper distribution over images and so that means that you can't just naively take any neural network and say oh I'm just gonna you know assume that it's okay to increase the likelihood of my training data or something because if you increase the probability over training data point you're not normalizing anymore if you're not careful so we'll need to be very careful how we design the network to ensure some to one so one place you might have seen high dimensional distributions is when you might have seen Bayesian networks in the past and I sure will cover today even though not necessarily everybody thinks of it as business in many ways they are business the Bayes net I mean here's this case Bayes net has five random variables it's Maui distribution over five random variables and it's saying to model that distribution I'm gonna actually apply something called the chain rule so it's gonna say okay the distribution over B e a J M is always representable as for the distribution over the first variable and second variable given first third variable given the first to fourth variable given the ones coming before times v variable given all the ones coming before this here is just chain rule always possible now what's coming practicing Bayes Nets is to say well chain rule it's nice very general but maybe you know there's other assumptions we can make and introduce some sparsity and say okay instead of conditioning on everything here when we say m given JM b we're just gonna replace this by m given a that's an assumption you're saying in the distribution I'm assuming the distribution of M given a is the same as M given a and J and E and B it's often a strong assumption might not want it some case might be okay why do business make these exemptions it makes things representable because now all of a sudden since you think about the basic if probability of your variable i favorable given variables 1 through i minus 1 how large is this table it's going to be size if it's all binary 2 to the I which is very large we've a large them variables but if you reduce what your condition on your table becomes smaller and so the trick played here is sparsity to result in smaller tables that you need for these conditionals to be able to represent things that we'll be able to learn them efficiently from data now the trend we have seen recently in a lot of models will cover in class today is a little different essentially saying we're happy to condition on everything but we're going to not represent the conditioning as a table we're gonna represent the conditioning as a neural network and so we hope that this neural net will somehow generalize we'll see more details later but it's a very different kind of mindset but in some sense it starts on the same point chain rule always works but then let's say make it sparse to make it representable are aggressive malls that will cover it will say something more along the lines of parametrize that conditional probability table in a way that it has a small number of parameters compared to to TD to to the power a number of other variables in the network which would be in the in the random variable should be very large okay so Auto rates and models we're going to look at the log probability of data points data point X will have many many entries in it if it's an image every pixel is an entry in X and maybe each color channel of each pixel is an entry in X this here is just base nets this is the chain rule the more general version and instead of calling it a chain rule model people call the Auto bressant model just a terminology thing but if you're more familiar with business you say okay it's just a you know straight-up chain role model but we're called Auto recive models so what I want to learn models of this type where of course and this thing here is gonna end up being parameterize somehow in a way that we're gonna learn so let's look at a very very simple auto aggressive model we'll have two variables X 1 X 2 so the model is a joy situation over X 1 and X 2 can says Marshall over x1 times conditional of x2 given x1 and let's keep it simple for now x1 is still a histogram so nothing has changed there then x2 we need to condition the value of x2 on x1 now in principle you could have a histogram for every possible value x1 takes on but that would be very expensive because now you have not just a histogram like since but many many many histograms for just that one variable x2 so instead we're going to parameterize it with a multi-layer perceptron and the output distribution rap is gonna be the switched over x2 which we could be I'll say a soft max output if x2 has maybe 256 possible values like a pixel value often has then you have 256 outputs soft max makes it normalized to 1 so we can set this up alright somebody can give you that data and you can train a histogram for x1 and you can train a multi-layer perceptron that will look something like x1 goes in some processing some processing and then a soft max over possible x2 values comes out and that's going to be our Joint Distribution okay this is extent to high dimensions well to some extent it does actually when you have D dimensional data you'll have order D parameters because you have a multi-layer perceptron for every new variable you need to sample or generate you're gonna you could down a network so it's much better than the exponential thing we saw if you just naively histogram things out but it's still not great because D could be pretty large and that might make an impractical the other thing is done by using a new neural network for every new variable you have limited generalization you might hope that we regenerate maybe X 10 from X 1 through 9 that whatever you process X 1 through 9 for to generate extend that when you try to generate X 11 that it might share some processing on X 1 through X 9 to be more efficient statistically to genetics to learn to generate x11 how we're going to do this is to major lines of work recurrent networks and masking let me pause here see if there are any questions before we dive into the two types of models we'll cover yes so question is imagine you were training a multi-layer perceptron like shown here and given that we know on the training data the maximum score will be achieved by exactly matching the training data which will correspond to a histogram how come we don't end up with that same thing possibly represented as a multi-layer perceptron then and then what was the point of having the multi-layer perceptron so the reason in practice this works better is because you make careful choices of all you put here and so by making interesting choices there you build in a prior over what you think your data is going to be alike and by building in that prior in the architecture and for supervised learning which the assumptions you've all seen in image processing the prior tends to be okay spacial proximity kind of matter so confidence with local filters make sense to process image data we're going to see similar things here and if priors are the same type that's say okay we're processing image locality can matter same we process text words that they're nearby might be more related than words that are very far apart not that the far apart ones are not related it might be less related and the ones close by and it's a similar priors that we see in supervised learning will play a role in unsupervised learning to ensure that what you learn here is something that as you're training will do initially improve performance on both training and hold out data and then at some point that might stop you might still improve in the training that if you kept running you'd have a perfect fit of your training data but on your hold out there you wouldn't do so well anymore and so it's really about the architecture that's here that ensures that you have essentially a smoothness prior a simplicity prior over what your model will be like when you fit a histogram you have no prior I mean you couldn't I mean you could play what kind of gifts at histograms you could say I'm gonna fit a histogram where to decide what is in bin ten I'm gonna do some weighted average of what's in my data in eight nine ten eleven twelve and so you can get some smoothing effect on your histogram if you want to um if you do direct histogram fitting you will have no smoothing effect the big issue with histograms aside from not being smooth because that's fixable that way is just that they're very unwieldy that in high dimensional spaces you can't essentially you don't have space in memory to represent a full histogram over the data distribution that you're looking at or the in if it's in a high dimensional space let's say I don't know ten to the two thirty you can populate that histogram in a clean way so sometimes the simple other question so that's a good question so in classical statistics there are other aggressive models and that is where the terminology comes from so in classical statistics there's this notion that you essentially can have different orders of autoregressive models depending on how far in the past it's often time series how far in the past your condition on here it won't be as focus on time series per se but it also applies a time series but that's definitely where the name comes from correct we'll see that in a moment all right so here's an example of a auto regressive model in the form of a recurrent neural network so what are we doing here I tried to model the probability of a high dimensional variable X this case X could be a sequence of characters for example it could be the word hello and so X is a five dimensional Vera fire the missile variable where each entry can take maybe 26 avoid you have is letters but you might have more you might have other symbols you might have capitalization and so forth maybe I don't know try it 56 possible values I can take on at each entry and so we try to model distribution over water unlikely X instantiations why isn't our 10 makes sense what happens in an RN is that you say well I'm gonna estimate the probability of the next terrible condition on everything that came before and the way I'm gonna do that is I must say okay Brabham directly feeding everything that came before into some kind of multi-layer perceptron that then has to decide what the probabilities between over next it's gonna generate a hidden state so this comes in here generates a hidden state from there it's gonna make a prediction distribution over possible values at the second position in the word then same thing here the sentence state will then propagate was it seen the first two to decide distribution over the third one is gonna not directly look back here that's not happening especially using this hidden state it's looking at bringing the second character predicting third one and this repeats and so the further you go forward you see that the more in some sense you save the naive model would condition this thing directly on this this this through some multiple layers of processing but here in the our den what happens is we have a parameter shearing happening where as we condition as we condition we're just conditioning every time on hidden state to generate and hit a state comes from the last character and previous hidden state and so this gives a lot of parameter sharing because we only have parameters essentially on here there are some parameters living here there are some parameters living here and here but if we call this data maybe three to one this is also theta one this is also theta one is also theta one this one is also theta two this is also theta two and so forth so as the sequence becomes longer the number of parameters we need to learn is not increasing it stays constant and so it's a very efficient way to determine statistical efficiency to represent your model now can you also do this for M list even though people don't typically run recurrent neural networks on M nest so we can take a look at that so 28 by 28 60,000 training data points 10,000 test data points original amnesties great scale we can binarize it into black and white here are some original data points and so what would we hope for the hope would be that if you train an RNN that we'll be able to then generate new images and so you might have a order you say top left is first then the pixel next to it next thing you go left to right top row next row left to right next row left to right I see you're just raster scan through your image and now your image has become a sequence it's a sequence of 28 times 28 numbers you can train an RNN on that in fact Wilson did it and here's what it looks like so what you see is a grid of 2 4 6 8 2 4 6 a grid of 8 by 8 so 64 different images generated from this RNN and you can see in initialization randomly Association there are n n it's not outputting interesting images but see how our this case 19 epochs in you actually get things that look like and this digits going back to the question okay how efficient are things are we building in priors to have better generalization you might say well a sequence model seems not that great for building images why well think about it use the same thing to go from pixel 1 to 2 2 to 3 3 to 4 and so forth and then when you go from 28 to 29 you're actually going from all the way to the right to all the way to the left your mama has no no clue about that because well it's just the next one so that's not going to be very good your model as you're coming through let's say I'm pixel 29 you'd really want to look at pixel 1 that's above you but you're in this long chain so you probably have a very hard time for signal to propagate from there so a lot of reasons why this might not be what you want to do for images but in terms of our aggressive modeling this is the in some many ways a simplest thing you could do is sister it could be a reasonable starting point and it's not somehow it can fix some of those issues yeah mm-hmm okay I like that suggestion just repeating it four people are not here to hear it to the mic wait if you model the motion of a pen on a piece of paper that motion might be a lot more structured and might be much shorter sequence than 28 times 28 pixels to step through aisle like a suggestion we don't have that ready on the slides for you but oh I can't maybe somebody should try to extra credit on the homework or something curious how it goes there's another suggestion yeah yes a question is how do pass information other directions can that be done so think about it well what that would require is that your Arnon maybe have some skip connections right it skips from one also to 29 so it also knows that is something there's always multiple things coming in something's coming from the left and something's coming from above that that could that is I would say the foundation will do when we do maths models we'll do something that's a lot like that even though we will not call an RNN we'll called a mast model and we'll call the pixel CNN but it's much what you're describing the simplest thing we can actually do which fixes at least one part of what's going maybe not so great here is we can actually just do position encoding so any given pixel you generate you can feed in the coordinates you can say okay when you're supposed to generate the twenty-ninth pixel it's at coordinate 1 & 2 so in the first column and second row down and so you could just feed that in as additional thing to conditioned on and that way at least you know when you're skipping to the next row and you're RNN should probably learn that you know when you skip to a new row something's happening we don't look as much as what's right before should probably should also be able to remember and it's hidden state if it wants to what's happening in different rows such that when you come back to the in different calls they come to the next row you know what's happening in the column above you and can bring that in so ran that experiment it gives so much sharper images that are being generated because it has the extra information of just location pixel location appended into the art and so just to be more specifically to happen is the art and will look something like well you'd have the first pixel let's call it well let's say we call it the pixel add zero zero it's generated it goes up your pixel at zero zero its generated goes through some processing generates a hidden state this your hidden state at time zero zero then this would go to somehow generate your pixel at one one not one one zero one butthat your estimated pixel there then you would look at zero one that's the real one here as you're training or if you were sampling you'd have to use the sampled one process down go into inner it's the next hidden state this also goes in here go to stop comes out zero to and so filling the the coordinates would mean that when you feed in let's say the factor that you're generating zero to you wouldn't just condition on the hidden state which is here we would also get the condition on the coordinates either specifically fit in coordinates from somewhere maybe it's the addition to head of state here you'd have explicitly coordinates 0 to because that's what you're supposed to generate together they get fed up to generate the pixel that's being generated over there in terms of how you train this whether it has the extra conditioner or not training is actually fairly straight for is just regular RNN training a training time you are able to fill in the entire bottom and the entire top and for every every one of them you can just look at okay maximize probability of this one given as I do my Ford Pass everything I've processed so far and a test time you'd have to this generation time you'd have to take this thing and put it here to continue generating all right those are the basics hopefully many of you had I've seen those things before how its policy or just see if there's any questions about that and then we'll go into the main topics for today all right so I'm asking based models it's the kind of other major branch of autographs and bottles was the key property they can have paralyzed commutation for all conditions all conditionals so that's nice you don't need to go through this chain it's gonna somehow mask regular neural network is what we'll see happen and this could be a regular MLP or it could be convolutions or a self attention so the about the building blocks you've seen in regular MILNET training MLP convolutions self attention we're gonna come back here just used to generate samples there's a question somewhere yes we're here I wouldn't say it's a I think the simplest way to think of it is that is a recurrent neural network alright because it has I mean here's the kind of printed picture of the recurrent neural network it has a hidden layer going in the middle the hidden layer is is some sense an encoding of everything you've seen so far but it's a very specific way of encoding this encoding by using an RNN which has a lot of parameters sharing compared to a lot of encoders will be structured differently where you kind of take in everything at once then generate a hidden variable that then you can expand back out to the original and here we are stepping through the input variables one at a time bringing them in one at a time to accumulate our hidden state as we progress [Music] okay the first one we will study is made so masked autoencoder for distribution estimation what's the model here imagine you have a standard auto encoder and that's on the slide here it comes from directly from the paper at the time was very common to try to learn representations by putting noise on the day you could say I have some data here it's just three variables we can Majan it's an image many many pixel values they say I want to learn about what images are like well one we had to learn about us say I have a real image I'm now going to put noise on it and I need to denoise it into the original image if I can train a neural network that turns a noisy image into the original then that neural network understands something about what images could be like so that's a way to generate images that was pretty common at the time what it doesn't really do if you just train a denoising auto-encoder what you don't have is a probability if you train a denoising auto-encoder you will turn a noise image into hopefully non noise the image but if you give it an actual image you cannot output a probability what's the probability of this image compared to another image there is no such thing coming out of that model so the question asked at the time was okay can we somehow use similar ideas but outweigh the probability you say well how could we operate a probability well it's already out putting in the denoising auto-encoder it's already outputting a probability for x1 x2 x3 hat it's helping a distribution if it's pixels it might be zero one of is binarized or zero to 255 for grayscale it's opening a distribution it's just it's not normalized in any meaningful way so the question is can we make a small fix such that this thing that we output which is probability for this region for x1 x2 x3 somehow together makes for a distribution that's normalized well remember the chain rule if we somehow structure the model the first output x1 let's say and after is it up up at X 1 conditioned on X 1 output X 2 and in condition X 1 and X 2 output X 3 then we have a proper probability distribution so we just need to set it up that way now this model is not set up that way what you can do is you can remove a bunch of the edges and I'll automatically be set up that way so we see here is now she starts with X 2 in this case this thing is just generated on its own there is no dependence on any inputs is no path from the inputs to X 2 it's on its own generated what's generated next X 3 is generated next and let's look at the path where is X 3 coming from it's coming from here and here just coming from here and here which is in turn coming from here so when we generate X 3 the only thing it can see as X 3 is being generated is X 2 how about X 1 well for X 1 is this path this path this path this path well it's everything then this goes and it's following all these edges as a consequence and so but when we look at the end here we see there is no connection to X 1 so when generating X 1 the green path only connect us to X 2 and X 3 so if we use this denoising auto-encoder you can even you can even have train it as a denoising auto-encoder not saying that's how you should train but you could have done it the result is a neural network that can now be used to output the probability of any image and it'll be a normalize distribution we know because it's general it's px 2 times px 3 given x2 times p x1 given x2 and x3 so this might seem pretty arbitrary like and maybe this you know with these three variables and we kind of had to paint into that Network how we're going to structure it and maybe that's not that convenient we can actually do this in a much more structured way so let's draw out the general principle so what does a made model look like it's a general model okay so let's say we have some variables X 1 X 2 X 3 X 4 X 5 X 6 and with some I want to output the probability of the values they're taking on I'm going to apply chain rule I'm going to have multi-layer processing so they go in here but then you have maybe you know some layer here another layer here another layer here and then you're outputting the probability of P 4 X 1 P 4 X 2 given X 1 P 4 X 3 given X 1 and 2 B 4 X 4 given X 1 through 3 B 4 X 5 given this 1 through 4 and P 4 X 6 given X 1 through 5 okay so P of X 1 cannot depend on anything so we can't do much B of X 2 given X 1 and so this thing here can depend on X 1 but only X 1 so what does that mean we can for example say okay X 2 can depend on X 1 so we'll say ok X 1 can go to the layer the horizontal layer for X 2 and this can go this way this way this way and this can happen also if we want to it's not going to do much but it's allowed to happen and in fact this can even happen over here so this network is a valid network to have distribution for X 1 and X 2 given X 1 then how about X 3 can only depend on X 1 and X 2 so we cannot feed X 3 into this layer but we can feed in X 2 and X 1 and then from here onwards this can continue and this can also take in things from above no problem and we have our distribution for X 3 given X 1 and X 2 I mean the good distribution depends on the weights on these edges with the right parameters on these edges it'll be a good distribution for your data or the wrong parameters on these edges will not be a good distribution you can keep repeating this now for X 4 you can roll you can get in X 1 X 2 X 3 but not X 4 and then same thing here and everything coming in from above so you get the picture now this picture can even be more general right now a druid as equally many hidden units in every layer but I don't need to do it that way I can think of any one of these if you look on the inside as having may be many many hidden units on the inside and the same thing will still be true I still have a clean separation of signal from X 1 on the input not reaching X 1 but only the next ones and so don't think this is specific to the number of inputs needs to be number of hidden units you can have an arbitrary number of hidden units you just need to organize them in a way that the signal cannot reach the original so you cannot have a pass from X 4 to explore here and that's actually ensured right here because we don't make the connection and then in the remainder that's ensured by only getting things from above and same layer note that these are then named like like this is I call it type a and this type B there's two types of layers all these layers are identical in terms of structure this is different this layer is the one that prevents though that doesn't have the horizontal edge all the others do have the horizontal edge this has to be in the beginning type a you can put type a at the end you can put the type a layer anywhere in the middle you just need to have one of the layers be a type a layer to ensure there's no horizontal propagation happening so you block that signal so that's a general version of made and you can apply this to visible to any network I drew what I drew here is essentially a mask applied to a full seam as a fully connected layers but if you had a different architecture that you like that goes from input to output you can apply the same masking key here is that you're only allowed to have pass from the variables that come before you and you need to erase any other pass they should not exist anymore you could have something that looks more like comedy we'll see that later you could mask things out to make sure there's no path coming to you I remember the variables here X 1 3 X 6 that gives some kind of meaning it kind of might be that that's it right structure but it could be that's the wrong ordering for you maybe you want to reorder then the ordering doesn't really matter it's arbitrary and permutation that you can use and make this work and in fact let's see if we got some results here first actually let's pause here because this is probably one of the two most important concepts for today's lecture make sure that this is fully understood and also fully understood how it is actually quite straightforward to do you just need to mask out a bunch of edges make them zero and whatever original architecture you had yes so yeah the question is in some sense does the ordering matter because maybe some orderings are smart some are not smart because well the way the connectivity is set up a it's not all it's not I mean if you look at once you choose an ordering you get a network you choose a different ordering the connectivity will be quite different and so yes do the ordering can matter and we'll show some experiments where different orderings have different qualities of results Olivia yes that's a really good question so and we're also getting to that but at all I think it's good to highlight it already now so the way I've drawn this is essentially a fixed mask made since that this is the origin of arable so you have the ordinal variables implies this mask and now you could do if you had data X 1 3 X 6 you could feed it in process by a network and maximize the log probably is on each of these outputs to train your network then what you could do you could actually feed the same data in again and decide that the ordering you're using is different and then the masking or network will be different but it could be the same network status I have parameter sharing across many different orderings of your variables but with the same network and that can absolutely be done and actually it gives gives better results to just not if you have in all the orderings it seems but if you have a modest number of orderings it seems to generalize better than using just a single ordering question there let's take that offline because I don't have an immediate answer for that and need to think about a little bit offline other questions about this okay so that's made you can train a maid model on em this so it's essentially multi-layer perceptron on em nest and goes an image outcomes no probability of the pixel in that image but you're masking pattern ensures that you can't just look it up on the input initialization ffogg's zero epic one two eight and nineteen it's actually starting to generate these are samples china generate pretty good Sam Emnes samples North s multi-layer perceptron has no kind of knowledge about location the way a convolution that work would have but you know you could train with enough data and on it's not the hardest kind of data using just this kind of model actually works just fine so actually seen two ways now to design auto race and models for M this you can do iron M which is very naive you could do an RNAi augmented with coordinates that will work a little better but then here you can do something else that's also quite naive for images and it also works pretty well in small images like that yes so the ordering picked here is raster scan ordering so right well you running experiments is that correct Wilson yeah so raster scan ordering so you'd go top left go line by line and that's the order in which things get laid out but you're absolutely right that the even though we look at it as an image that's 2d it's laid out it's just one long sequence and then MLP is applied with masking to ensure it can go straight through to predict itself so here are some probabilities from the original paper showing that if you just gives negative log probability lower is better two hidden layers 32 masks which means you you know you can try multiple masks as we talked about at the same time gives a best score here these are small models this was from the original is for quite a few years ago now I think three or four years ago so you could prolly try this again today you might want to try bigger moles and maybe you can work with more masks done here is another thing to analyze in the paper is to look at nearest neighbors so something that's interesting to do when you train and this goes back to the histogram theme question from earlier how do we know we're not ending up with under the hood just learning a histogram of our training data and that we're actually generalizing well one way to look at this is to generate samples from your network that you just trained and then as you generate samples you go in your training data and you look up the nearest neighbor it's a pixel level nearest neighbor calculation and then visualize that and if that's always identical to what you generated immunity effectively generally memorize the training data that's typically not what you want but if it's similar yet different it means you might have learned the pattern of what is in your training data rather than just memorize it and so that's exactly what we see happen here they look similar to things in the training data most of them but aren't exactly copies of something in the training data you can do made with different orderings so here we're comparing a random permutation samples from that so the question was if you have certain things that are more connect and others well random permutation is not the best it turns out it's better to use raster scan ordering it seems even then odd so skipping over one pixel every time and then doing the left out pixels in a second pass and the second half of your generation I mean it gets some kind of structure but it's definitely not great and this is M this I mean kind of think in general is if if you know their ideas any good it should definitely succeed on emilis because that's a relatively small data set and easy to do in hybrid parameters on and so forth then rows raster scan columns raster scan pretty similar top to bottom bottom to middle also pretty similar maybe a little worse than the raster scan orderings yes obits Purdham good question so bits Purdum is a with two metrics to score your likelihood model so we maximize log probability or minimize negative log probability on our training data then we go to test data and on the test data we evaluate the log probability of each test input so you feed in an amnesty image and look at what's the log priority of this image but to standardize it across dimensionalities of them it just to make it comparable across different data sets instead of just looking at the log probability of a 28 by 28 by or smsed image and say you're gonna Express a score as number of bits per dim bits per dim looks at essentially the if it's well imagine it's a RGB image then one pixel has three dimensions are G and B if it's a grayscale or binary image then there's only one dimension so then the question is you look at the you look at the log to log under the to log the the probability averaged log probability of each pixel as you encounter it in your test set say okay imagine I have a lower probability score of the total image that's a 28 by 2008 say 100 pixels have some score then I divide by 100 to normalize by the number of pixels I generated if I had three channels and I also divide by 3 and then bits per dim would then look at the 2 log because it's bits if it's called Nats for Dempsey alternates like nuts its Nats for dim then it would be the regular log the natural log so thread is in encode I guess it would be along the lines of it would be log probability of some data point x divided by dim of X and then there would be this is this would be nuts for a dim and if I made that a 2 then it would be bits per dim yes hey can somebody close the door you can come in but it's just somebody's talking outside yeah okay so the question kind of goes to the very beginning in some sense here right you could say or even before what why do we want our network to output things that normalize so the alternative is dead essentially energy based models and energy based models what your output is just for every image you up with a score and energy and that's just your energy score and now you'd say well energy has to be low because the probability is 2 e to the negative energy so essentially you'd want low energy so you try to drive it down but now let's say you have a bunch of training data you're trained on it that you try to drive the energy down well you could just output a lowest possible number let's say negative infinity for all of them and what now now you have really good score but that's not a good model is giving everybody a negative infinity energy score and so when you train energy based model usually the challenges that normalization counsel you didn't still have to you can just drive it down on your training data energy you have to make sure it's also in some sense high on your things that are unusual and so to make that work often some intractable things have to happen that then get to get approximated to deal with the fact they need is normalization constant and so in some sense by designing the architecture of the network to make it normalize by design we don't have to deal with this intractable normalization constant that other otherwise come back to haunt you and to ensure that you don't just think you can drive energy down on everything because you're only driving down on training data and not on things you didn't see that will be an enormous Asian constant so question was earlier can we just have multiple masks and in fact is what was done in the paper here it shows lower is better when you have about seven six or seven different masks that is different orderings and some sense of your variables in the auto aggressive ordering you get the best performance once you go above that performance degrades again well why could that be well I mean this is test error it could be that once you have too many orderings your network is not expressive enough you you know I could not express the mapping from you know input to predicting the next ones in the ordering for every possible ordering at the same time because only a small network interesting thing you could do just playing around with it you could go back to the paper you could see if you make your network let's say you know 100 times bigger then a network they had and now you do you know see how many orderings where you find out optimum you probably with the larger network will have more orderings that are going to help but ultimately the mortar you see have the harder to fit and so you need a bigger and bigger network most likely to make that work one we can also analyze this which wasn't shown in the plot in the paper essentially you could also look at your training error you could see that maybe if you have too many orderings even your training error you're not able to drive it down if you can't even drive down your training error then essentially you're not expressive enough to capture what you're trying to capture this you know all possible orderings now the beauty if you were to train on all possible orderings is that you can fill in data in any way like let's say you have missing data some test time do some coordinates missing or some pixels missing if you train on all ordering so you can just say hey let's put the ones that are missing last and then just run in the ordering that puts those loss of a condition as many things as possible to generate the things that were left out as you'll probably have a general filler that you may not you might not have that ordering available if you didn't train on it see time is it 6:20 it's not halfway yet but we're about to start something very different supper put okay somebody check up just beads outside if you're okay it's here okay you go can you help out Alex and Wilson to see how we want to set it up so propose we take a 15-minute break now eat eat some pizza and then we'll start with mass convolutions all right let's restart and let's restart with this picture over here because over break I've got quite a few questions about this picture so I want to clarify a few things so what were our things we want to do and that's what the questions are about we wanted to somehow ensure that the probability distribution sums to 1 how do we know this thing is going to sum to 1 well if we have a distribution that satisfies if you have an expression here the expressions together satisfy the chain rule we know that if we if this is a valid distribution over x1 at all times so let's make it concrete imagine we're outputting pixels from 0 to 255 if the output on x1 is a softmax it's forced to sum to 1 for the probability of x1 3 out softmax then if the output for x2 given x1 is it going to saw 256 ways softmax it's forced to sum to 1 same thing for X 3 4 5 6 that means we're by architecture design forcing each individual output to be normalized so that thing that box is checked then in terms of the Joint Distribution if we have a bunch of conditional probabilities and we multiply them together if they satisfy the chain rule after multiplying together this will form a proper probability distribution so the distribution we're actually having here is P X 1 times P X 2 given X on and so forth till px6 given X 1 through 5 this is our probability for the input that we get X 1 through X 6 that is the probability we assign this will be a number between 0 and 1 and we also know that as long as each of these are proper conditional distributions then the Joint Distribution over all these variables will be a proper distribution that sums to 1 so that's where our summing t1 comes from each individual output is normalized n in the way we set up the architecture it follows the chain rule okay so sums to 1 that box is checked then what else another question is how do you how do you train it well you train it the objective would be sum over all I which is data points so from 1 through m our data points and then sum over all K log probability theta is the parameter vector of X the 8th example entry K given X I 1 through K minus 1 so we have the log probabilities of each output being optimized all sum together we maximize this over theta so that's how we train or if you're deep learning framework only minimizes you'd be minimizing the negative of this double sum then how do we sample the way we sample is we say okay what is our ordering our ordering in this case X x1 3 X 6 so the first thing you do is your sample x1 that's what does it mean to sample this one well you'd have to you feed in whatever came before x1 in this case there's nothing came before excellent you might say what am i feeding in actually it doesn't matter whatever you feed in it does not affect the distribution we get over here because we designed architecture such that the input is not going to influence P of x1 it's not conditioned on anything so no matter what you feed in some P of x1 comes out that is your distribution going to sample from you'll have a softmax distribution you sample from that after you sample the x1 so 0 X 1/2 comes out you now can feed this into here you still don't know what to feed into X 2 through X 6 but that's okay because we're going to be sampling x2 and the way we design the mask is that the only path that reaches to our past that come from x1 so everything that comes after x1 does not influence what's on the softmax over here and so we feed an X 1 hat so if X 1 hat now outcomes X 2 hat as a sample X 2 hat then gets fed in over here we can get out X 3 hat then X 3 how can go in there's a slow process imagine have a megapixel image you go through your multi-layer perceptron your mass multi-layer perceptron a million times to get a single image out and I believe remember correctly in the made paper the amount of said it took something like 10 minutes to generate one M nest sample forgot the exact time it takes I can take a lot of time because you do go through the network million times and so that's one of the downsides of these auto regressive models you need to generate all the previous entries before you can start generating the next one now remember it's not always the case data I mean you always in today all the previous ones but in some cases like the RN n you don't need to go all the way from the beginning again you have a hidden state that remembers things so it's architected to not have to go start from scratch every time you kind of just keep going but here you start from scratch every time next pixel all the way through and again and again okay so I think those are the three main questions that came up over break this was how do we know it normalizes how do you train it and how do we sample did I miss any questions okay so that's made and you'll actually a quick preview of your homework pretty much everything we cover in class today you will be implementing for your homework so made uses a multi-layer perceptron which has parameter sharing it's nice when you generate X 25 you use sometimes a lot of the same parameters as we used to generate X 24 and X 23 and so forth so statistically that's nice but let's say you're generating images you're not using much of the structure of the images we know commnets will use more of a structure can we bring that in here we'll first start with one diversions because easier to think about instead of having a made architecture we could have something a little sparse ER and essentially run a 1d convolution so what we have here look at the architecture to generate the next one over here we have something that looks a lot like where I drew for made but it doesn't go all the way back it's a finite window looks back at and actually you can do parameter share and you can have what happens here to be the same as what happens here and here and here that's what convolutions do for you you apply the same mask non mask well the same filter applied us in this case two by one filter in multiple spots because you think that the way you're gonna bring in this kind of low-level information is going to be the same independent of where you are in space and so you'll run filter here same thing here that will be another fill in it could be a filter Bank it doesn't have to be just one filter could be many filters then same here and here so this gives you a lot more parameter sharing than we have in a main because even though it might look like in many ways strong keep in mind again like these are the same these are the same and these are the same so there's really only four parameter sets here be considered far less and the deeper this network becomes the more it is going to matter the more is gonna fan out and the parameter will help more and more of course you can you don't have to use a mask filters only two by one you could have bigger filters to look further back and so forth you could have a deeper network such that by repeated filtering you can look further back those are architectural choices that you make you play around with those are hyper parameters and you see what works well for you so the receptive field now this concepts been pretty important receptive field is not so in made the receptive field of XK is x1 through XK minus 1 here the receptive field depends on how many layers you have how wide your filters are okay and so you should think about that I mean the smaller the receptive field probably faster to train less parameters to deal with this so forth but also if things that are further back really matter you're not gonna be able to see them so then you can't build a good model of your data it's too fast so here's a wavenet in action within the sampling so we're just talking about some what happens for sampling is that you've generated all the previous ones that you do your fourth propagation to generate the next one so here's the one generated you use someone to condition on go again use that one to condition on go again and so forth nothing you see here in wavenet which is are the most popular masked one big convolution architecture it's the way the the way the filters are set up they are set up to kind of go further back by compare this this one over here at every layer you look at just the two the one below you and one back but here it's a dilated convolution that allows you to look further back and have much farther reach now you get when you just do the naive thing in fact in historical speaking web net came out we've three years ago out of deepmind aaron been annoyed and it gave at the time prowess till now but very stunning beyond price here we are speech generation results so they're the input would actually be taxed conditioned on text conditioned on text it would generate sound condition the sound already generated feed that into the bottom generate the next part of sound and keep repeating this and get extremely realistic speech yes so yeah there's a bit of a abuse of notation there so at training time the output is a probability and we want to maximize our ability off with the actual values they're a training time we have the values there and we try to maximize the probability that test time all we get is a probability distribution we sample from it and whenever the sample is we bring to the bottom to continue so we don't bring a distribution down which we'd bring a specific sample instance down to generate the next one yes well so the idea here is that you want to capture the full distribution and so by by sampling you're able to if let's say there's many modes many imagine you have a piece of text and you start generating with a female voice you want to keep turning where the female voice won't be able to do that one generate with a male voice for you a male voice want to share it with maybe some kind of British accent or Australian accent or maybe a Russian accent who knows you you kind of want to be consistent and be able to generate all those versions not just the mean of all of them and so I would say is pretty important point is that in generative models we're really trying to capture the full distribution not the mean of the distribution I mean the mean will be captured implicitly but the goal is to capture much more than the mean okay and then of course thought goes into the details of exactly what do you put in these 1d convolutional layers you put there you know residual connections dilated convolution little thing that's used a lot in in unsupervised learning I don't I don't think it's nearly as much used in supervised learning is this kind of unit over here it's attached and sigmoid in one so the idea is you have your normally narrative rather than just a Darrell you or a 10-8 or Leakey rally or a sigmoid say okay it's gonna be 10 H I'm gonna have an output I'm gonna essentially have instead of having one output a minute two outputs and one is gonna be get a ten-inch apply to the other one sigmoid and I'm gonna multiply together what it does is sigmoid goes 0 to 1 this is sigmoid 10 H goes negative 1 to +1 and so the sigmoid acts has a gating function another place you've seen this is probably LSD mstr use where there are gating functions that decide whether or not a single comes through and so the sigmoid can zero things out whereas the ten age is in some sense propagating the actual signal that you're trying to get through this thing where it can let things through so you can actually apply wave that alumnist I'm not saying that's the thing to do but I want to show you the generality of what well you can do it just like you run an art in an amnesty you can just make a long sequence of all your pixels of course there's many such sequences may have 60,000 training images and you can train a wave net and well what do you get out it's not super good but I mean it's better than where it started so it's better than random it learns something what if we now give it the position encoding so has more information as it's generating things here's what you get actually makes a very big difference here I'm gonna Arnhem the difference is much smaller with the web net architecture difference is very big once you do you do position encoding it can do a lot better in M mist modeling okay so that's the 1d version let's pause here and then we'll go to the 2d version which is at least I find a lot less intuitive but hopefully we'll get to right oh yeah so good question is your model should be our go-to so people benchmark and so on different data sets you'll see different things win so we'll we'll pass some of these tables towards the end but essentially there is M missed where almost everything works pretty well but it's a good sanity check and it's fast turnaround sanity text and you know you're doing the right thing at least to some extent then see far a lot of benchmarking is a bunch of image set benchmarking maybe said benchmarking gets more expensive because it's more data to process and I'm surprised learning tends to work with a lot of data and so forth so I wouldn't say there is a single go-to I would more think of what we cover as a lot of different ideas even within autoregressive models that all have their merit and might see their own benefits in different situations and then this is we're now in ought aggressive bottles and then four weeks in now we'll have seen also flow models and latent variable models and implicit flash game models and will seem to have four choices and within those four yet more choices so it's it's not nearly as converged as maybe your favorite answer to your question would have been but from a research point of view you are getting a great asset to your question now that's still quite open what you might want to do yeah yes well I told you that wavenet was done for speech right and then I told you wavenet it's not really the right thing from this but I want to show for the generality that you can also apply at M NIST there's no reason you cannot you can see what happens so I'm not applying it to M mist to apply there to say your future of image modeling is going to be wavenet it's not but at the same time I think most people would not expect it would work this well as it does here once you just bother giving position encodings and so I think sometimes you know you can think you can just see these results which is what people might have tried in the past and think like okay it definitely could never work but then actually just a very simple change can you can show very good results and if actually this is also a happy models we'll see you later called self attention models multi-headed self attention they don't know about spatial structure but that you give position encodings and all of a sudden they know about spatial structure and they work really well okay any more questions about we've covered so far before I do it pixel CNN Heydrich yes oh I see we're saying yeah you can do that so you could say after I regenerated my 28 by 28 I'm gonna generate the next another row another row another row and see what happens absolutely possible yes or you can generate wider images see what happens wider might not work so well but it might work is so convolutional and it's somewhat shift invariant with confidence you can do this with pixels CNN people have shown Excel CNN essentially the generalization of wavenet but for 2d it's one way to think of it and people have done it essentially keep just keep generating even though you train on and maybe 28 by 28 you actually can generate that's something that's twice as large ten times as large I mean it might not be meaningful at super large scale because the patterns are large Kohan be different than small scale but people have done that and see they've gotten some decent results just actually brings the other point last year we taught the class our list of open questions included how to train a generative malas independent of the scale of your input and in the last 12 months that has happened so through pixel CNN ideas people have done exactly what you're describing being able to generate at arbitrary scale or scale independent learning okay so next hour we'll see pixel CNN and many of its variations so we can flatten an image into one of the vectors and run a RNN or a wave net but I saw it brought up it's kind of a crazy thing to do it seems like more of a kind of exercise to kind of see that it's possible then the right idea instead on 2d images you could run may be made but then you don't have any parameters sharing in terms of using filters so how about just using a 2d filter running it over your image and essentially think about it it's made but the connections are set up as if a 2d filters being run you have a common target Ector we still need to need to do the masking that you doing made now if you set up your entire Covenant architecture and then Nicci the masking you do in made it's a lot to think about a lot to keep track of what happens in pixel CNN is something little simpler you just say I'm gonna change I'm gonna directly say I'm gonna change my filter I'm gonna mask the filters directly rather than masking an end-to-end way I'm gonna mask the individual filters in a way that the kind of ordering is obeyed that I cannot get anything to leak from bearable a later variable to an early variable or to itself and so this is the ordering we want shown here when generating X I want to condition everything that came before it's called raster scan ordering line by line going down how do you do it you can effectively have a mask that looks like this if you have a three by three filter and you it looks like that what does it mean if you run this filter over your image what can any given pixel see where where does the results come from well I think about this pixel here it somehow gets something that comes from over here there's the only thing it has access to so I cannot see itself it cannot see anything that comes after it it's limited what it can see because I can only see those four pixels but at least it's not violating our made like or aggressive assumptions okay now of course you can do multiple layers so I say well let's see let's look at sampling actions so going to set up a filter filters that look like this multiple layers and if you apply another layer what will happen well what's D essentially what's the receptive field after two layers well I need to understand her says the field of this one this one this one this one if I understand receptive field of those then that becomes a receptive field of this last one so this one here will get something over here and this one will get something that looks like this this one here will get something that looks like that and this one yes so after two layers what I drew out here is gonna be this whole thing here it's gonna be the receptive field if I expand even more this receptive field will keep growing I have more and more layers and you've seen Anita expands the right way you can only think see things that came before you nothing that comes after you not yourself so let's take a look at this in action so Arab and the nord is a lean inventor of pixel CNN as well as many of the later variations so what happens when we want to sample from this we have our ComNet we now have the proper masking soon nothing can feed from anything to x1 because the first one will be the mask will be such that you can't see anything so then we just run things through there's a soft max at the end we sample from the soft max get our first pixel keep repeating this fix laugh - pixel after pixel go to the image and at this point maybe you know this is the soft Mac distribution for the next pixel thanks to Mendeleyev comp processing we look at there just a one output look at the softmax there sample from this and this is I mean you need to do this for three channels whose color image and this just kind of keeps going growing image one takes a lot of time to get something that looks like an animal here okay so that's the process much like may than others one at a time whenever your sample you defeat it in go do a full new fourth pass so definitely sampling it's not gonna be fast but let's focus on the training so one thing when I was drawing this out I don't have many house we have many layers of kung many conflicts have each other there's actually a blind spot that forms you can never reach this area over here even though we intend to do raster scan ordering by doing the three by three filter with the masking that we need to do to not get anything that comes from after us we actually have a blind spot you might say I don't care to generate this pixel over here what's in that part of the image is now gonna affect it often that's probably true it's gonna have not damage influenced per se it also depends on where does this I mean if this thing is a if the image is real an image dad looks like this and runs very far this way you lose a lot if this thing is all the way on the right you don't lose a lot so it depends on where you are in the image how big your blind spot is and some some pixels gonna be a pretty big blind spot in fact if you're Bob bottom-left here you have a very big blind spot so what can we do well this is again architecture choices how can we choose an architecture such that we don't have this blind spot can we find a different kind of mask and can we do something that avoids this well we know we can Chris will do the naive thing which is we can set up a full confident analyze all connections and then made mask it out and make sure nothing goes from anything the future to your current variable but that's just not practical that defeats kind of the purpose of setting up a confident is it gets very complicated because now you have this massive masking to deal with you really want to do something at the level of damask so you really have confident behavior still as you process so what else can we do to get more information so when we look at this blind spot over here how about here's an idea instead of running a filter a three by three centered on this guy where we can now only use this this there's this what if it's done we run a 2x3 center on this one if you run a 2x3 centered on that one this is the receptive field with one layer if we have another layer it'll expand I have the two layers it'll essentially be this after three layers it'll be this and so forth and what we see here is if we have a decent number of layers we're gonna see everything above us in the image some one tricky thing here what I did is I said well centering it around this one like the results come in there but if I run my filter that way effectively I'm seeing myself i if you just run what I just said the whole thing essentially by having this thing included here the way it's set up by ending up there it's that horizontal connection made that's effectively running all the way and coming through so we can't do that how to avoid that well what if we actually then say actually I'm gonna after I've done all this filtering I'm gonna use it to then at the very last layer generate this pixel over here so I'm getting all this except to fill them some sense bring it all together into this spot in some way but then at the end I'm not predicting that spot because that would be cheating everything able to see the input that's there I'm gonna have to predict the next one next one down then it works now have for septic field above me and I confirm that try to predict the next one so conditions satisfied this is some sense what we want we lose this blind spot on the side over here what we introduced is a blind spot over here if you have a different color for that we introduced a blind spot over here how to deal with that well how about this we run a separate 1d filter so we think of this within your row you you can name a slice just letting a one day thing that's within a row and you just run a 1d thing within that row for just the predicting from these this one and then of course if they have multiple layers it would expand out and so we now have a within the row separate thing within that row we don't get to see anywhere but that row the way I drew it and the thing from above gets to see everything above together they will have seen everything you can bring your prediction together and have a prediction that doesn't have a blind spot at all I will admit this is very subtle from reading the paper and seeing what's on line so far it took me like probably a date to figure out that this is what's happening and multiple conversations with TAS and other instructors but so let me pause here is one extracting one introduce and make sure that this part is is clear yes yeah you're getting exactly at the point that that's the missing piece and what I explained so far I love this question yeah any other questions aside from that question so point is correct like the linear one seems to have so much less information on the one coming from the top remember when we had made the sense you need this kind of ordering structure you can actually think of the one coming from the top as a whole as being before the one that is running horizontal and as you run the horizontal one here you can actually feed in from the top into that also in every layer so every layer of you actually have to run in parallel you have a two by three running it's something a 1 by 2 or 1 by 3 with every one by 5 running the horizontal things there will be two tracks of convolutions but after every stage the top can feed the thing that covers the top can feed into the bottom you can't feed both ways because then you the top gets to see the future but that one at the bottom comes after the top so can receive also this summary from the top as its we're doing repeated convolution she's going forward I didn't learns how to combine it all so I say so learn I mean she was essentially a gate with learning prayer and promise to be learned to decide how much it weighs one versus the other yeah I believe that's the double side but I believe essentially the two gets some together but it learns everything that comes before that to make sure that these numbers I get some together are scaled the right way so that it works out so it's one of those things where often two data streams will be sum together and deep network so you think well I'm not so naive how can you just sum it together but you're essentially forcing what comes before it to kind of adapt its scale and so forth so that's something together works out so the one on top is called vertical stack and one on the side is called horizontal stack and we know how to do one the mask on so you know how to do the vertical stack again that's just convolutions so this is what the vertical stack essentially it looks like is this 2x3 and on the left you see the receptive field on the right you see where it lands with where the result gets put and kind of circling back remember when we talked about made how does type A and type B masks in that you can have the horizontal connection and everywhere but not I won't spawn you but if there same thing here what there's this the layer at the end is different that's where you can all of a sudden get into what's below so here's what it looks like under the hood there is again the 10 H and Sigma multiplied together you just kind of get in hidden unit happening in both streams and then the vertical stack stream gets to feed into the horizontal stack stream to help its predictions as it's moving along but there's nothing going the other way because that would violate the ordering assumption ok so Olivia asked okay how old are the different models work here's a comparison on I believe see far though it doesn't say but this looks like C for numbers so this is the negative log likelihood test and in percent parentheses train I suspect expressed in mats for dim so or actually bits per them because 8 uniform is 8 so if you have a uniform distribution you don't know what to do you need 8 bits because you're outputting 8 bits if you have no idea what output you need the full information about 8 bits but the more the better your model the less you need to represent the data and so here we see that the original pixel CNN which had the blind spot got a 3.14 the gated pixel CNN which avoids the blind spot issue has a three point zero three the original pixel CNN paper had something else called a pixel Arnon actually very complicated and nobody that's the example something nobody's actually building on anymore the pixel art in so that's one but maybe that means somebody should do it again you never know but the pixel art ends seems to be largely a discard the pixel art an essentially is it's an RNN to generate all these pixels but be you don't just you then don't take it in linearly still have Kham Kham players bring in some embedding that goes in the hidden layer of the art and it's very complicated that's why Pro nobody's building on it it was the best model still a little bit better than gated pixels am I will soon see something that is even better than that and so we've surpassed pixel horn and with simpler models like gated pixel CNN okay pixel CNN plus plus goes and makes a few improvements on the gated pixel CNN so so far our output has been a soft max over a 256 way soft max that might sound pretty crazy because I mean the pixel being 233 or 234 who cares that's about the same and if it's it seems like you should learn that you should understand and your prior that those are similar and not have to learn from scratch that those are similar so we can do is you can parameterize this week instead of using a 256 way soft max you have a parameterization of your distribution so we're gonna put there well you want to put something there that's easy to sample from and easy to optimize with because otherwise your whole end-to-end optimization is going to be in bottleneck than that output layer here's something very popular use logistics so logistic is essentially if you have a sigmoid which usually is something like this you can actually parameterize it you can decide where you put the the the main of the sigmoid and the essentially the slope so how much is this sloped here more or less slope and where is this centered it goes from 0 to 1 which essentially means that this is a cumulative distribution function so you can think of this as the cumulative distribution function of some density and we know that for sampling where you really is a cumulative distribution function and then for probabilities maybe you know we can work with either one but so this can be seen as a cumulative distribution function that we can use to find a density and then turns out the density will look like this difference between two sigmoids so very easy to compute the density at a point X easy to compute that's good we get a probabilities up and then we're going to not just have one because then it's just going to be a multi-modal blob that's been the only thing we can output I mean our network will say this de mean this is a slope in some sense or standard deviation but may want to be able to do multimodal things all right especially in the very first pixel and many cases you want to be able to be multimodal so be a mixture distribution here so a mixture of multiple Sigma District distributions if I'm sure you've seen mixture of gaussians before is a bunch of bumps visitation looks like that a bunch of bumps but it's just kind of computationally easier to work with even though it looks very similar easier to work with them the mixture of gaussians so here's what these individual logistics could look like this is the PDF and then this is the cumulative density function and a mixture could then take on pretty much any shape what we still have to do is to turn into discrete distribution but that's not too hard to do think about what it mean to be a discrete distribution but I had let's say the only one of them the green one how do I make it into discrete I can just say well in the interval 0 to whatever probability mass is in here that's a probability of being let's say zero or a concentr that I can see the probability in the fall between negative 0.5 and 0.5 that is pagoda of being 0 then 0.5 till 1.5 is probably being 1 and so forth and so that's how does this set up now these things run till infinity so when you hit the end you will still have a bunch of probability mass left that will go all into the 255 value on one side and the 0 value on the other side okay so then you can train us on simple histograms so we see here on the left top left at pong 0 the samples it's a mixture of seams 3 logistics and then we see it kind of adapts itself to fit to those two peaks and we see the loss here nicely going down getting a good score long live hood score if we go back to the very early question is the log likelihood score of just having the exact histogram of the training data are going to be a better score yes that local accent score will be a little lower here so it'll be a little better the negative log likelihoods but it won't generalize as well and that's why we don't want it okay so that's one improving pixel CNN plus plus made what else it decided to capture long term depends which we talked about earlier maybe need to skip connection your RNN to look further back the wavelet data with dilated convolutions and so forth same thing here you introduced skip connections across your convolutional layers to make sure you can bring things in from pretty far back these lamppost bus also uses this to stream model there's a vertical stack horizontal stack vertically as shown in green horizontal shown in blue so you see the exact same thing but then in addition to get a Texas A&M architecture choices there is skip connections and mixture of logistics on the output unless you do that actually get a better score so a couple of simple changes is something that at the same time maybe not simple because I mean trying new architectures it's always hard to know what to try and which one will work better but if you try a few things do it right you actually get better score now a lot of recent results have actually stepped away from using RN ends and come architectures and instead used multi-headed self attention coming from the 2018 attention so you need paper by for petronia what's the first name ah thank you buzzer fish funny first somebody knows well title the paper attention is all you need it was invented in the context of machine translation but then found to be useful and pretty much any other data type and Ashley has a really nice property that masking is super super easy so with the motet attention you kind of go back and sometimes do the main idea that you just put up your architecture and then you do some masking that automatically works out and you're good to go so what is self attention let's start with attention and attention here's what happens you have a query what is it query it's a vector some kind of hidden vector I don't know 16 dimensional three dimensional thousand twenty four dimensional sixty-four racial some hidden vector which is generated by previous layers typically they say this is this one I'm interesting this is my query then another part of the network will have key value pairs this query will go in look at the keys and look at this inner product with the key so we're doing query key inner product the hired inner product to have her we are a better match then we essentially do a weighted sum of the values based on how good a match we have between key query so they have a very good match e to that power will be very high number if a poor match will be a very low number so some will be essentially a weighted average of the values based on how well we're matched between ki and query now this is softmax log calculation be careful I mean these to be stabilized if all the ki query products are high numbers you still going to be stable you need to subtract out the max it's know me with that subtract out the max and same thing here subtract out the max to ensure that you have a stable calculation so that's one attention query then another query will come in and you'll do the same thing and get a weighted average of the values for that other query and one way to think of this could be okay I'm processing some word as I'm translating something maybe or trying to interpret something processing this word is where it could have many meanings so I might have from this word multiple queries going out so from this local spot of multiple queries going out the query may be aligned to one meeting another meaning then that clear goes out finds for other words in the sentence key values that might be aligned I mean for one of your queries one interpretation of the word you find higher line key values you find values for that and you you're you know you're on that track the other one might not find any matches and you might not get anything out there you continue with I mean that's a high level intuition essentially you're you're keeping track of multiple hypotheses Mobley's you can look at for the same at the same time and see what it is that you've seen the rest of the image or the rest of the sentence that matches up with your various hypotheses okay so in convolution we have this kind of thing that slides over image and self attention anything any one of these will have its query or multiple queries going to all other spots in image so thinking back to me and you look at everything else so this is actually much like a multi-layer perceptron like model we now need to just do some masking to get rid of the connections that we don't want masking is very simple because in self attention you can essentially just you can multiply in a very well subtract out a very negative number to make all these values become very very small and essentially you get zero out so the only thing is you essentially make this calculation unstable you make it deliberately unstable such that e to the very negative number is zero everywhere and you get out 0 over all so very simple to implement and again the closest model really even though we see it after pixel CNN I would say the closest model is made you have something that's fully connected and then you install a mask to not be able to look any at yourself or anything that comes after you and then you're good to go one of the beauties here is that you can very easily try arbitrary orderings so this ordering is fine now you might say well that's great you are B orderings but what's so special about this compared to may it couldn't made also turn arbitrary ordering so why why are we're gonna see better results with this than we've seen with made the thing is that with this with this self attention a lot of parameter sharing is still happening the way everything is set up inside the net essentially the modules that set up the keys and the and the queries there's parameter sharing across all spots and so just parameter sharing everywhere you can have in fact a very highly connected network like we had with made but not nearly as many parameters to learn as you have in a naive multi-layer perceptron so key is that there is parameter sharing happening inside each of these attention modules cool then here's some details of an architecture we developed a little while ago called pixel snail where we have both convolutional blocks and selves attention blocks the volusia box i have to prior that local information is interesting attention block give the opportunity to look everywhere not have any blind spot at all in a very same you can also try to visualize the blind spot of the assessment think so gated pixel CNN as this what is this showing yellow is a spot you're looking at you wonder what when I try to predict this one this pixel the yellow pixel if I back period in my network which input pixels have an influence on my prediction obviously nothing after you should have an influence but we can receive anything before you has an influence and we see that for the this with I believe random weights so with random weights even though we know that in principle gated pixel CNN should have all of this in practice I mean all this if it's deep enough and so forth in practice I mean the longer the chain the harder to get the signal we saw with pixel CNN plus there were skip connections and so we see it gets much closer to that ideal it gets much better coverage and practice of work and draw it signal from pixel snail pretty much covers it entirely not sure what's going on at the top here but such and everything but that little top corner is covered it gets strong signal from everywhere because the self attention wallah can jump to any previous spot and the image doesn't have to be chained with many layers of filters before it finally reaches a faraway spot just quick analogy here a lot of people talk about vanishing gradients in the context of our Nets why give them iron in its long vanishing grains what do we do it they were exploiting gradients essentially that's the same thing here right it's like you have many many filter layers and the effect will have a vanishing gradient vanishing effect of everything out here on that pixel because it takes so many steps in a network to get there ok well we do this we can run as an EM nest here is the results gives nice samples we can also evaluate this on C far and we see the number of bits for dim is actually better than was possible before so yet an improvement upon state-of-the-art then in the last kind of 20 minutes we have I want to go through kind of more quickly through a bunch of ideas where we're now going to have the same detail of everything we've covered so far but just at a high level want you to know that these are ideas people have looked at and that have had some good results so first one here class convictional class conditional pixel CNN you can instead of just training on all your data in the same way you can say wait if I want to generate a 0 or 1 or a 2 how can I force it to do that well you can feed in a one hunting coding into your content that signals what you're trying to generate so you're feeding in the label and then you condition on that label to generate how can you feed it in here it's done by you have a weight matrices and every layer that turn especially one heart into a bias on each of the filters in that layer so let's see how I don't know eight filters then the weight matrix would turn the 100 coding into eight numbers that are eight biases one bias for each of the filters now we would repeat for every every layer then another thing I want to show is that these other aggressive models can generate very realistic images so there's a pretty recent result less than a year old because a lot of people think okay realistic image is always cancel worst case does have to begin these all pixel CNN like models what's done here there is hierarchy built into it which means that essentially right on just doing a raster scan generation you could decide to for example first generate a low resolution version of the image because we know that it's it's hard if the dependencies are too long you have very high reducing it and resolution image how are you gonna get all these dependencies to work out well first generate a sub sampled version then condition on the sub sampled version to generate high resolution high res high res and you can start generating very high resolution high fidelity images so you can do super resolution also which is very related you can essentially have a is that sometimes the second stage of the hierarchical pixel CNN where you have just trained from a subsample or low-pass filtered and then sub sampled version of your image to then generate the high resolution version now again your network will process low resolution version many many complex then output the first pixel of the high dimensional version you'll take a first pixel also condition that process again to the second pixel and repeat so it's still at all same process of you know pixel after pixel get generated one at a time you can do this you know to do super resolution on more I mean this case faces and I think the other ones bedrooms you can also do things where you say hey maybe the way I want to do a hierarchical generation of images is not just low res to high res but maybe first grayscale and then color it in this with a grayscale I can set the structure and again because signal has to propagate far enough if I go color right away in some sense the distance between two pixel spots is three times longer with color than it was just grayscale and so you're making that three times shorter with grayscale and then after that not super resolution but coloring it why are the colors the way they are they're essentially there's a colorized endless data set where that is actually what the data set looks like this is not some random coloring after the fact this is some kind of splashy pattern that is typical for the data set and has learned to master that splashy patterns this is with so you can essentially again you can do conditioning either on low low res versions or grayscale versions and then from there start generating pretty realistic color images let's take a step back here for a moment so we've seen how it works to train the training is very efficient because training to evaluate the log-likelihood is just one forward pass because you masked everything else and once pass will get old log probabilities one backward pass will get the grade in for all the log probabilities and we can update the parameters that's great so training is good it's very expressive because the factorization is very general and because it's so expressive we can use architectures that captured the priors we have about images and other types of data to learn fairly efficiently so that's good I think that's not so good is the fourth pass for sampling so this is now I think from two or three years ago but I mean it's still today very slow to sample eleven minutes to generate 16 32 by 32 images on state-of-the-art GPU at the time that guess maybe today would be I don't know two or three minutes but still a very long time to to generate sixteen images and again why is that because we have to generate the first pixel or the first character whatever it's you're generating then once you have that you go to the front of network do a few new forward pass you can sample the next one bring the two things you sample to the front do another pass here at the next one so is there anything we can do to speed this up actually curious any ideas people who have not seen the papers yet any ideas of how we can speed this up bonus point if is an idea that's not been done yet yes I'd work so it's a very interesting idea the high level idea that you're proposing is hierarchy you go coarse to fine and why would that speed things up let's double click on that the reason would the speed things up is because if you generate course first and maybe let me try this imagine you generate try to generate an image as I don't know eight by eight or something or seven by seven imagine your first generate maybe this one this one well maybe even more hierarchical you just stand there eight then this one here this one well let's take a better structure let's say we have actually eight by eight hey by eight so now that we have a by eight we could say well how about we maybe first generate things that are somewhat apart so maybe we generate this one here this one here this one here this one here and I don't know this one in the middle or something and then you could say well okay that's what the first thing I generated then after generated that in principle I mean this was five passes maybe through the network to generate those five because that's how it tends to go we say well how about the next one so maybe I can decide to maybe as I'm generating anything in this top box remember us also have generated one here and one here maybe we say everything in this top box and we actually don't worry about the things outside of that top boxes and as I can do a separate smaller network in some sense that does this part and forgets about conditioning on this one this one this one this one so it goes back to the Bayes net architecture actually we talked about very early it's actually than explicitly cutting dependencies that the chain rule would allow the channel would allow you to condition on those but you say I'm not going to condition on it now by not conditioning on some of these variables I don't need to do things sequentially I can do this one here I can look if I also have this one let's say and it could maybe do this one here this one if I had one here and here some throwing a few more in such I could have four separate regions that run in parallel and follow a similar pattern Oracle pattern where I say I'm gonna somehow fill in a few spots so that after that I'm going to do divide and conquer and paralyze it and so that means that we essentially masking out a lot more than we are required to mask out will be less expressive with these models so I'm very likely the log likelihoods course will be a little worse because not as expressive but if we choose it in a clever way maybe the things we don't get the condition on would not have had much signal anyway and so we're not losing much and we gain a lot of speed do we get in speed in training not really I mean in training everything was just a fourth pass anyway but a testing not that sampling time we can speed as I essentially think split in half half half and you can have some kind of logarithmic speed-up compared to what you'd have to do otherwise so that's one way and let me maybe skip that's does the second thing but some of that we skipped it out so it speed up by breaking autographs of pattern so here's a slide on that we first sample the one spots and the sample the two spots we only have the connections shown here to sample the three spots we only have the connections shown here to sample the four spots we all have the connections shown there and so what this allows us to do is exactly we describe drawing it out you can divide and conquer and highly paralyze your sampling it's not that parallel when there's only four by four pixels but if you have megapixel image this will make a very big difference this most a scale thing you can then look at the at the scores of how well it does and essentially if you look at what it's the key thing here is improved sampling speed but more limited modeling capacity because you removed some edges that you normally would have had to Commission on you lost em so limited markup acetate but improved sampling speed and in fact is that the core of scaling auto breast video models so there's some really nice recent work just about a half year old showing that you can use autoscent models to generate videos so now in goes let's say I don't know 32 frame video and you use this exact idea to and this colorized pattern here signal sawaki first here at the yellow ones then the green ones tend or the red ones tend the blue ones and that's exactly getting at that point you have a hierarchal generation scheme that allows you to do a bit more of divide and conquer and in this case also builds in a prior because in video you might actually want this prior that you know how that you both have a spatial which is this direction in this direction kind of hierarchical structure but also a temporal hierarchical structure you generate the first frame and the fifth frame coarsely at the same time and only later filling the things in between to capture better kind of long-term structure in your video sequences so that's one way to do it this is kind of clever ordering of variables and then just essentially masking out more to allow your competition to be faster what else can we do to speed things up the other thing we can do it has been very popular and wavenet and I think it's one of the biggest contributors that when it can run real time and says she and use is caching so if you look carefully at these convolution patterns I actually run this convolution to sample Y at the top there think about it as a sample Y before when you sample this one over here you actually already did this you already did this you already did this in fact you did almost everything at the time you would have had this also and then this but there's a lot of overlap in that calculation it's a lot of overlap and the only new things that have come in after you sample this one is that this one has been sampled this these two have been sampled so essentially I say it's not drawn the way what comes here will actually generate this one then this one will go here and so what happens is that everything that happened here can actually be reused and so the only thing you have to do another way to draw us when you want to generate Y you can go back get this from your cache all the work that happen underneath you don't need to redo you go one level down the left branch you can get from your cache right branch you need to still compute and then left branch and get from your cache and so every time you go down the left branch comes from your cache you've done that debt side before and going down we need to keep going and then of course this path still needs to be computed because something is coming in from the bottom that is making that different but what amazes that session now a linear calculation in just a number of layers that you have and you just kind of grow linearly true to generate the next one as opposed to having to do all these convolutions through your network that you do naively you can actually do the same thing with so this speeds things up quite a bit in some statistics on that you can actually also do this with the pixel CNM it's in the same paper it's much more complicated to draw out but the same thing happens you're running convolutions and you can again look okay where is this coming from have I done this for my previous path if I've done it from a previous pass what data went into it and if the data went into it has stayed the same then I can just reuse it and so you can get pretty significant speed ups in this case without reducing expressive power because all you're doing is caching you're not cutting any branch is not changing anything in how you're presenting you're just caching see ok so actually let me switch to my laptop for these videos because it seems like they're not loading here so to give you an idea of what the videos are like that can get generated so there's a data set of a robot pushing objects actually collected at the berkeley at a iom work by chelsea fin circa 11 and collaborators then you can see if you can like this full screen for a moment so what we see here is a bunch of videos generated all samples so it's all very small I mean the resolution video generation is not yet that may have megapixels and resolutions I think is 32 by 32 or something but you can see that there she look at this carefully these are actually meaningful videos and these are all samples so you are using this hierarchical sampling scheme to generate part of a frame part of mobile frames and fill parts in fill in the intermediate frames and although you have your video sequence and these look like realistic sequences and the beauty is there's a very generic model like this is just if your block that represents your sequence of frames so you turn an autograph and model on it with some complex some self attention and you're good to go then they have different versions in terms of how much what the scale is that we should do spatial-temporal subsampling but it's it's actually yeah it's somewhat hard to see the differences but maybe the conclusions are both both actually give recently realistic videos then there's another data set called kinetics so there's the bear robot pushing video set if you want to sum a video do the kinetics video said on the left they specialized on cooking videos in the data set so here are a bunch of cooking videos now I'm not sure why and the faces are in there but that's and that's what it's called I think there's a dog watching the cooking the top is more like cooking um so maybe only this is cooking and this is something else but this is also cooking yeah so the way this is set up is when it shows block it's the initial frame but it might use two frames or something to have some notion of speed I don't know what they exactly did their black surroundings means initial ground truth when I switch to the red framing it means the video auto regressive model and it's predicting the frames then what they did is they they did this and then asked you sample you can actually ask a sample some of these sample things that are likely some of these sample things are less likely so they actually start from the same frame I think 16 times run 60s samplings and then after the fact you can use your mod evaluate the log likelihood of each of your generated video sequences you can rank order them by more likely versus less likely and that's what I did here so on the left are the more likely sequences on the right the less likely sequences so what you should see assuming the model is good is that things that are out here look more realistic than things you see all the way on the right so let's watch this again I mean I definitely see the dog disappear all the way on the right and more naturally present in all the others but actually most of them are pretty similar and pretty realistic and then here is fool kinetics data set mainstay huh there's it's a bigger data set many more types of things in it same thing the ones on the left are the more likely ones and the ones on the right are less likely and they're all generated but considered more or less likely after the fact definitely there's a lot of less realistic things you know in fact the paper talks about it as a paper exposes says okay we've got some very good results on the cooking subset and on the bear pushing but we have not so good results on the a generic kinetics datasets so that's definitely a place where there's a lot of room for improvement if you want to look into that so the one thing and we actually have not talked about and that was a lot of the motivation of as far as learning is representation learning we've seen how to build a model that can sample that can evaluate the log-likelihood but usually you say I want to do a little mouse learning first and then I want to do a supervised learning that works more quickly than it would have otherwise worked I want to RL that works more quickly the challenge here is how do you get a rep of latent representation from a pixel CNN so with the Arnon models it's not as much of an issue because there aren't in and sometimes you're the RNN is building up a head of state that maybe is summarizing everything you've seen so far in a way that maybe is more directly usable there might be something there especially if you run it may be bi-directional but but you'll see a notice it's not so clear where's your Latin state in most of ours models to newer models the latent state is the random variable you go from a input to a latency variable and that zero is the encoding it is the thing you can suppose come from a gouge there's something you can sample that and get something back out that Z is supposed to be meaningful and then you start playing games like I said well I have two images I can turn them in both into their Z Z 1 Z 2 I can directly interplay between Z 1 and Z 2 and then decode and I'll see a natural interpolation between the two original images but how to do that with this pixel CNN it's not so clear as the random input is happening at a per pixel base every time we sample the next pixel and that's where the softmax kicks in and has that randomness but the random is in that softmax is not very high level information so problem not gonna be that informative proposed solution ashay Wilson's been working on this over past year is to use the Fisher score it's a very general idea whenever you have a generative model you could say well for any data point ax what is the grand log probability of that data point you say why is that an interesting feature why is that even a feature vector why is that something that tells me something about the data imagine you have a Gaussian mixture model okay very simple case - a mixture model - mixture components you look at the graph log probability of the parameters at your data point if you data point falls in cluster 1 then grab a little say I want to make that cluster more likely so the prior of learning cluster 1 that parameter will have a high gradient want to increase that if your data points in cluster 2 trying to get it the pilot of cluster 2 to go up will be the high gradient thing and so intuitively this grad log probability thing if you think of the underlying model is somehow capturing a multi-modal distribution it's trying to say the mode my data point is in should be a mode with higher probability I mean it's a high level intuition but it's a I think it's a good way to think about it the mode you're in should get higher probability that's reflected in this Fisher vector because your score and so depending which mode you're in this Fisher score will be different because there will be different parameters that will need to go up or down to increase the probability of a different mode and so in that sense this thing gives you an indication of which mode that you're in and it could be a great feature vector to use and so even though pixel CNN does not have a latent space per se you can use this as a workaround to get a representation out your first training pixel sinem once you're done training you have an image come in you want to live in representation compute the ground log probability score which is very easy just a forward pass through the network just like when you were training do a backward pass the results of the backward pass that gradient vector that's your future vector now so let's see if this works on the left it's a more naive interpolation scheme with just activations in the pixel CNN on the right it's with the Fisher score and if you look carefully it's always like this is real this is real in between is interpolated same on the right if you look carefully you would see that their interpolations on the right are more realistic let me zoom in on this maybe to point out what it is that we're looking for in these kind of experiments you're tried to unsupervised learning you try to see does my representation have good interpolations what we look for is well which one is better you think top or bottom who says top this is bottom ok raises bottom what do you look for we look for essentially the top why is it bad it's really the left image all the way and then it's an overlay of left in the right image and then it's the right image all right it's it's not doing anything it's it what you want is you want it to and then what that means that the things in the middle are not realistic faces which means that somehow in your latent space you're landing them in situations where you're not where you're supposed to be your model doesn't know how to deal with that you're not in good good position here there's official score based interpolation we see that throughout the faces are much more realistic and especially the middle part is of course where this one fails in this one does better then we see some kind of sometimes new people that were not this one or that person pop up and that's what we want we want every face along the way to be realistic not just an overlay of two images so we have a bibliography and at the end of the slide deck and then the other thing we'll have in three weeks after your homework has been do because she'll build a lot of it yourself also we will have a collab that will have essentially code for British all the things we discussed in class today but just a couple days too late for your homework but this is say you have also an hour reference of how we well we should say in this case Wilson implements all these ideas alright see you next week 