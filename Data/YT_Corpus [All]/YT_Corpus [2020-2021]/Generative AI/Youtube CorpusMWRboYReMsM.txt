 Niloufer Selvadurai: Hello everyone, my name is Niloufer Selvadurai and on behalf of the Macquarie University Law School and the data society Research Group, I would like to very warmly welcome you to today's webinar on AI health and human rights. Niloufer Selvadurai: Before we begin, I would like to acknowledge the traditional custodians of the land on which Macquarie University fits the Western medical plan of the designation we pay our respects to elder's past, present and future. Niloufer Selvadurai: The purpose of today's webinar is to explore the nexus between artificial intelligence health and human rights. Niloufer Selvadurai: We are delighted to have with us today. Professor Enrico Clara, the director of the Center for Health Informatics at the Australian Institute of Health Innovation. Niloufer Selvadurai: And the foundation professor in medical informatics at Macquarie University and requires expertise travels to versus both Niloufer Selvadurai: Computer science and medicine, a very rare and valuable combination trained in medicine with a computer science PhD in artificial intelligence and returns research background includes both industry and academia. Niloufer Selvadurai: He is a world leader in decision support and communication processes in biomedicine. So it's a great pleasure and privilege to have Enrico with us today. Niloufer Selvadurai: And Rico will begin by introducing today's topic, and then he would share a panel discussion with Dr. Rita metal and the tape. Niloufer Selvadurai: Carolyn Adams, Dr. Stephen mccombe and myself at the end there will be a question and answer time. So please do enter any questions you may have. During the webinar, using the Q AMP a function found at the bottom of your screen. Niloufer Selvadurai: Do. Please also note that today's webinar is being recorded. Let me now hand you over to your chair, Professor enrica Kira Enrico Coiera: Thank you very much for that lovely introduction. And really, the talent today will not be made, but all the panelists that you'll be hearing from who or come with different Enrico Coiera: Legal training and expertise and interests. What I thought I would do just for the first few minutes, given how varied the audience is today is just to Enrico Coiera: raise some of the core issues around artificial intelligence that may require some sort of legal framework to think about Enrico Coiera: So let's just think through really what we mean by AI and why it's of interest. So it's pretty clear that the last five years have seen a huge resurgence in interest in what is quite an old area of research AI and it's been driven by Enrico Coiera: Huge access to new sorts of data. Data often collected from us inadvertently massive improvements in the technology around machine learning and deep learning Enrico Coiera: And finally, tremendous investment from very large companies like Google like Facebook Amazon, etc. So the focus Enrico Coiera: Of AI today is around health care, but any of you with a smartphone already carrying around little neural chips in your smartphones and the data you're using, or the suggestions, you're getting from the system. Enrico Coiera: Already all i driven. And if you haven't seen that Netflix show the social dilemma. I strongly encourage you to see that to understand what I'm talking about in healthcare. Enrico Coiera: The opportunities for I are really tremendous it assists us in things like diagnosis interpreting chest X rays images. Enrico Coiera: In suggesting treatments in planning therapy in organizing Enrico Coiera: How hospitals might dispose of their resources and in things like the pandemic, it's it's being able to do things like suggest drugs that might be appropriate candidates were treating are covered. Enrico Coiera: So really tremendously interesting and it's no surprise that the last three to four years have seen a huge interest. Enrico Coiera: Again from industry in commercialization. So this slide is simply one of dozens of technologies that have been cleared by the US. Enrico Coiera: Food and Drug Association FDA for us. And so the fact that we're seeing so much investment clearly shows that people believe this is this is a real and and happening thing. Enrico Coiera: And it's not without surprise. Also, I guess that the arrival of this technology is seen by many to be disruptive and Enrico Coiera: Whether or not you believe that clinicians will be redundant or not might be a personal view. I think there's plenty of work to go around for everybody. And really, we need to be thinking not about replacing nurses, doctors physios, but augmenting their work with support from from AI systems. Enrico Coiera: One of the things that doesn't get mentioned, because this is always framed in a very first world way is that Enrico Coiera: While we might be able to use our API to optimize what we're doing in healthcare. We don't get three to 5% improvement in Enrico Coiera: Our processes. It could be revolutionary in other in other settings. So being able to Enrico Coiera: Embed smart I and small cheap devices in this case is a picture of a smartphone Which becomes an ultrasound scanner at AI to that. And all of a sudden you've got distributed expertise. Enrico Coiera: In countries that might be developing and don't have the resources or the infrastructure that other nations do. So really, very exciting from a developing nation perspective. Enrico Coiera: Today though gets us to focus. I guess on the challenges of that world and whether or not the arrival of AI is something that we should be concerned about or should at least be cautious about Enrico Coiera: And I showed this slide now Asimov's laws if your science fiction buff you know exactly what these are. Enrico Coiera: Because this is from to allow me to talk about ethics and every ethicist who's now watching or throw their arms up and say, Enrico Coiera: That these, these have nothing to do with the real ethical dilemmas and healthcare and you're absolutely right. So as mobs Lauren simply says that a eyes shouldn't hurt people should do what they're told. Enrico Coiera: Not blow themselves up and don't commit genocide, it's a reasonable rules, but if you think about healthcare. Enrico Coiera: Many decisions we make, which in the end for the benefits of patients do cause harm. Enrico Coiera: Or in some cases, for example, at the end of life, you might actually choose to actively withdraw treatment because that's the right thing to do. Enrico Coiera: For that patient rather than causing suffering. So those ethical dilemmas stretch and break what seemed like classic. I think a lot of that is a very difficult area and an important area to discuss Enrico Coiera: One of the things we'll talk about today. I think we're the panelists is really the locus of control. In other words, who is responsible when things go wrong and this Enrico Coiera: slide here is really just talking about a well known challenge and drivers with so called driverless cars if you buy a Tesla. Enrico Coiera: It is not a driverless car at the moment you meant to keep your hands on the wheels and pay attention, even if it's doing everything for you. Enrico Coiera: And certainly watching Harry Potter video yields control to the AI system and that may not be appropriate. And when something goes wrong, who is liable for that that problem was that the driver was that the designer of the car was somebody else on the road. Enrico Coiera: The last topic that I think will get some traction today is bias. And this is an issue that arises when I eyes inadvertently usually I trained on data that is not representative of reality. So Enrico Coiera: If you train an algorithm for example that's meant to deal with the general population, but all it sees is white males, then a lot of the decisions, it makes Enrico Coiera: May not be fair or accurate for people who come from that particular subgroup. And if you're following Twitter today, there's a real problem because at the moment. Twitter's algorithms prioritize white faces. Enrico Coiera: People who have different racial backgrounds and that's an inbuilt bias and the way the algorithm detected. It's not intentional on Twitter's part, they're mortified. Enrico Coiera: That it does that, but that's what happens when you train on unbiased, or sorry on bias data sets. So that's simply Enrico Coiera: A little walk through the park. I guess of the issues that we'd like to cover today and I'm not for me now, though, and I'd like to invite our first Enrico Coiera: Panelist along. And so I'd like a Carolyn Adams to join us. Carolyn, if you turn your camera on and and do microphone and I'll just give you a little intro Enrico Coiera: To Carolyn is a lawyer work in the era of human rights and administrative law particular interest of Carolyn's is the right to privacy, the right of access to information. Enrico Coiera: And the need for transparency and accountability and decision making all really important issues, I think. And so I've already raised a range of issues carrying around ethics and legal aspects of AI and healthcare. Enrico Coiera: We talked about who's responsible Enrico Coiera: When something goes wrong is that the developer, the AI, the user Enrico Coiera: Human rights is a way of thinking about these issues of brings a lens to thinking about this. I wonder Enrico Coiera: If you could start us off by reminding us what we mean by human rights because we're not all experts in the area and then tell us how you think this frame of human rights have been stuck thinking about these problems. Carolyn Adams: Right, thanks, Enrico Carolyn Adams: So my view is that it's important to look at certain developments like AI assistant decision making, through a human rights lens. Carolyn Adams: Because such developments clearly impact or have the potential to impact on the rights of individuals in both positive and negative ways so new technologies can Carolyn Adams: Increase or decrease societal inequality and we really need to be conscious of which direction we're heading. Carolyn Adams: So just like to very briefly touch on what human rights are. And for those of us who are not human rights lawyers so rights. Carolyn Adams: fundamental and inalienable interests that we all share, such as the right to life the right to security of the person, the right to equality and the right to health. Carolyn Adams: Because this so fundamental they demand a high level of protection. So what we find is that quite often the rights of an individual Carolyn Adams: Will outweigh the interests of the Community or the public interest itself. And that's, in fact, the role of human rights, they protect the individual from the tyranny of the majority, which often comes disguised as the public good. Carolyn Adams: And from abuse by the state. Carolyn Adams: So the modern human rights era dates from the chaos and carnage of the Second World War, and this is also true of the medical ethics framework. So there's some overlap here in terms of content and history with ethics. Carolyn Adams: So the right. If we take the right to consent to medical treatment and research. For example, this was written into international law in direct response to atrocities committed by certain Mexico's on prisoners of war and concentration camp teammates. Carolyn Adams: Human rights can be moral or legal or both international human rights law is an attempt to articulate those rights accepted by the global community into legally binding international treaties. Carolyn Adams: To have these treaties, the International Covenant on Civil and Political Rights and the International Covenant on Economic, Social and Cultural Rights. Carolyn Adams: A binding legal treaties negotiated after world war two that have almost global universal acceptance. Carolyn Adams: So it's important to remember that these treaties are legally binding on the streets that states that have signed up to them and that Australia is a party to both of them. So, this requires Australia to ensure that rights are protected in domestic law and policy. Carolyn Adams: Now how human rights related to AI assistant decision making. So if we think about AI desist assistant decision making in the healthcare context. Carolyn Adams: There. In fact, many points in which human rights issues arise only have time to touch on a couple today. But, for example, in terms of the data collected and used to train AI systems. Carolyn Adams: You need to consider the right to privacy of the individuals because information is being collected and used. It's always questionable whether a data set is ever really truly de identified and this fact gives rise to privacy risk. Carolyn Adams: You also need to consider whether your data collections have the potential to introduce bias into the system and whether your algorithm operates in a way that reflects embedded societal bias. Carolyn Adams: On what on one level, this is a matter of good science biased inputs result in biased outputs which is undesirable from a scientific point of view. Carolyn Adams: But it's also a breach of human rights, the right top quality and it's mirror image, the right to non discrimination are the foundations of human rights law. Carolyn Adams: The right to the highest attainable standard of health must be provided on an equal footing and without discrimination if you wish to be consistent with international human rights law. Carolyn Adams: If you create an AI system on the basis of data that is biased on grounds of race or sex, for example. Carolyn Adams: You are creating an AI system that will produce results that are in breach of the right to quality and non discrimination and mandate cause harm. Carolyn Adams: In the best case scenario is history decision making can reduce problematic bias and subjectivity, which we often find in human decision making. Carolyn Adams: When humans do it. It's quite often unconscious our systems might be a bit better in this regard as we can in principle, examine the basis on which an AI assistant decision is made. Carolyn Adams: In a way that we can't examine human decision. Carolyn Adams: But as we know, I systems can also embed and amplifying bias. So making sure your data and your algorithm a fit for purpose and I'm biased in the relevant sense Carolyn Adams: Is clearly it's not a straightforward or simple task, but it is important when developing and using such systems to consider whether is there is sufficient evidence that problematic bias has been sufficiently minimized or eliminated. Now, who is responsible for Carolyn Adams: This task. Well, developers, certainly, and users probably Carolyn Adams: We need processes and practices to test for bias. We need investment. Carolyn Adams: In bias research, we also need more data made available for research and development, and this is my particular area of interest and expertise. Carolyn Adams: Finally, I just like to touch on why human rights are important building AI systems that are rights compliant is not just consistent with international and domestic law. Carolyn Adams: It's also instrumentalist important to support trust or as I prefer to say the social license in the health system. So trust is a more is something that we developed with individuals social license is something that we need to develop with institutions. Carolyn Adams: My research into the social license needed to support the data health research suggests that human rights form part of the social norms necessary to support social licence. Carolyn Adams: The health care system like the rest of society is built on a foundation of trust. We need to work in ways that establish and maintain that trust. Thank you. Enrico Coiera: That's great, thanks. Carolyn, I'm in the couple of minutes we've got left. I wonder if you could also change lens again. And we've talked before Enrico Coiera: Privately about administrative law and how that's a different lens and I'm not legal type. So maybe if you could just very briefly tell us what administrative law is in general. And then what that tells us about this issue of regulation government Carolyn Adams: So, human rights and administrative law quite closely connected Carolyn Adams: Administrative Law is about control of government action and it's designed to empower individuals when dealing with the state and so human rights about protecting the individual from the from abuse by the state administrative law is a tool in that talk. Carolyn Adams: I just like to touch on why I think this body of principles in administrative law might be useful to us, although I have to admit this is very early days in my exploration of Carolyn Adams: These ideas so administrative law is essentially about good decision making. That is decisions that apply pre-determined rules fairly and transparently to each individual case. Carolyn Adams: That's what the government's supposed to do and it makes a decision about whether or not you're eligible for Social Security, for example. Carolyn Adams: The principles of good administrative decision make making include timeliness appropriate cost clear reasonable transparent criteria or rules for the decision. Carolyn Adams: The collection of all the relevant information about the individual case. Carolyn Adams: justifiable and reasonable application of the rules to the individual case transparent explanation of the reasons for the decision. Carolyn Adams: And an avenue for review of the decision so you can probably already see how some of these principles can be applied to decisions by healthcare practitioners Carolyn Adams: Artificial Intelligence is clearly going to assist with timeliness and possibly the cost of healthcare decision making, although we need to keep an eye on that. Carolyn Adams: The health care practitioner, of course, is responsible for collecting all the relevant information about the individual case. Carolyn Adams: But what about the rules or the criteria that determine the diagnosis or treatment options to be applied in the healthcare context. This is the part that might rely on an AI system. Carolyn Adams: So a health care practitioner will need to be confident that the system. He or she's using to generate a potential diagnosis is based on relevant quality data and good algorithms good systems. Carolyn Adams: In addition, he or she will need to be able to explain this process at some level to a patient to allow that patient to give voluntary and informed consent to treatment. Carolyn Adams: Is all reflects a good decision making process and this does seem like a good point to stop and hand over to our next speaker know Luca, who I know is working on explain ability. Thank you. Enrico Coiera: Thanks. Carolyn. That's fantastic, really appreciate that and there will be a chance at the end. I hope for questions, so please send your questions in and we'll Enrico Coiera: Try and get our panelists content. Thanks again. Carolyn. So yes, I'd like to invite me Luther to turn the camera on and also hopefully your microphone. Enrico Coiera: Deliver is a professor of law at Macquarie law school here and it's a co leader of the data society research group. Enrico Coiera: She's also an editor of the International Journal of technology policy and law and her area of research focuses around AI and governments, so I'm just hoping we'll get to see none of us soon. Enrico Coiera: There you are. Niloufer Selvadurai: Oh, sorry. Enrico Coiera: So the liver. We've, we've already started to talk a little bit about some of these issues around transparent transparency and accountability in decision making. Enrico Coiera: From, from a legal perspective, how can we better regulate the use of AI. Niloufer Selvadurai: Thank you very much and rica one part of the regulatory solution could be to annex a new statutory right to an explanation for AI generated decisions. Niloufer Selvadurai: This would enable an individual to better understand the rationale for the decision and if necessary contested. Niloufer Selvadurai: We've already heard how AI systems are being used to process model data and generated automated decisions and in many cases the underlying algorithm is opaque. Niloufer Selvadurai: It may embody bias. It may apply inappropriate or I data, data sets. It may even exceed the lawful authority of the decision maker, but the affected individual may not be aware of any of this. Niloufer Selvadurai: And applying this to the health sector, the national health back presently permits automated decision making. Niloufer Selvadurai: Section one of one be of the National Health Access the minister may arrange the use of computer programs for any purposes so very widely drafted. Niloufer Selvadurai: For which the minister may on mass. So both mandatory and discretionary decisions take administrative action. Niloufer Selvadurai: But while it's established that Minister Minister has a decision to provide reasons for decisions, it is unlikely to extend to explaining the functioning. Niloufer Selvadurai: Of the underlying algorithm and the relevant computer programmers. The term is used in the legislation. Niloufer Selvadurai: In 2019 the federal government tried to sort of address some of these issues and released an AI guidelines framework and this applies to both government and private entities. Niloufer Selvadurai: And of particular interest to our discussion is principles, six and seven so Principle six stipulates the need for transparency and responsible disclosure Niloufer Selvadurai: To enable individuals, and I quote, to know when they are being significantly impacted by an AI system. Niloufer Selvadurai: And to find out when an AI system is engaging with them. It sounds like a good plot for a sci fi movie. And actually, if you have seen the last Johnny English movie. Niloufer Selvadurai: It is actually premised on an AI system gone wild. It's really interesting. I mean, it is now beyond the sort of realms of possibility. Niloufer Selvadurai: But to get back to relevant matters principle seven fair. The stipulates that were an AI system significantly impact the person Niloufer Selvadurai: They should be afforded a timely process to challenge the use of output of the AI system. Now, this is a great step forward. And while this undoubtedly forms best practice for AI use and it's readily available on the website. Niloufer Selvadurai: It should be followed. Niloufer Selvadurai: It is wholly voluntary. Niloufer Selvadurai: And it doesn't create any legal rights for the individuals and to further complicate matters. And I don't know if I'm getting to sort of forensic illegal here. Niloufer Selvadurai: There's a recent federal court decision, which suggests that Australian law doesn't recognize automated decisions as being formal decisions for the purposes of obtaining review. Niloufer Selvadurai: So in this case called interlachen Deputy Commissioner of taxation, the Court held that no decision is made, unless accompanied by the requisite mental process of an authorized officer. Niloufer Selvadurai: This reminds me of some of the early copyright cases. Niloufer Selvadurai: Where computer programs were not considered to be capable of creating original work and the law moved on and recognized technological change. Niloufer Selvadurai: And amended itself. And I think we're gonna see the back despair discourse and that development in the next few years. But right now, what we have is a failure to recognize the impact of automated decisions. And while this decision was made under the income tax assessment act. Niloufer Selvadurai: And the facts are crazily Niloufer Selvadurai: Obscure it does address or foreground, the limited avenues of redress available to an individual has affected by an arbitrary or inequitable automated decision. Niloufer Selvadurai: And if we then extend this can analysis to AI health and human rights. Niloufer Selvadurai: Such automated decisions and lack of avenues of redress can undermine what Enrico and Carolyn have talked about the human rights to Niloufer Selvadurai: Quality of Service, the human rights and non discrimination and the human right, also to a fair trial and hearing so especially right to Niloufer Selvadurai: Attain an explanation for an AI generated decision would help address this substantial gap in the presence of protection. Yeah, a long answer to a good question. Enrico Coiera: Yeah, it's fascinating and you've got me thinking about robo get immediately. Yes. Enrico Coiera: Yeah, right. So, Enrico Coiera: If we have or should have this right to explanation. Enrico Coiera: Practically, what would it be look like. I mean, other other countries that are perhaps further along than we are. It sounds like we've got a long way to go. Niloufer Selvadurai: Yes, we are behind and that's typical. In terms of our technology law Australia is always been sort of a little bit. There's a time lag and the European Union has always been the sort of shining light on the hill and they are again in this area. Niloufer Selvadurai: So this right would provide some sort of logic or some sort of explanation of the logic for the algorithm and the functioning and operation of the algorithm. Niloufer Selvadurai: And the European Union's 2018 general data protection regulation refers to providing a person an explanation, when a decision has been based on automated processing and produces significant legal effects. Niloufer Selvadurai: In such circumstances, the relevant entity is required to provide, and I quote, meaningful information. I love that meaningful information about the logic involved. Niloufer Selvadurai: That critical fer fer phrase meaningful information is not defined. So, which is good because it means it's flexible and can evolve with changing circumstances. Niloufer Selvadurai: But it's likely to encompass the information needed to fully understand the decision and effectively challenge the decision if necessary. Niloufer Selvadurai: However, one of the key sections supporting the so called right to explanation in the GDPR is found in the recitals to the GDP app. So there's ongoing debate as to its legal status. Niloufer Selvadurai: But, irrespective of the precise legal status. It's mere conception is a really useful template for reform. Niloufer Selvadurai: Regulators can't police the writing of algorithms in many cases. Niloufer Selvadurai: Regulators aren't even understand machine learning models and won't be able to evaluate their legality, because they're highly complex technical and also designed to serve adapt. Niloufer Selvadurai: So what regulators can do is to impose a front end right to explanation and this promotes transparency and accountability and helps protect some of those fundamental human rights that Niloufer Selvadurai: Enrico and Carolyn have been talking about. If you're interested in reading a bit more of this. Niloufer Selvadurai: Just to get your town and I have just published an article on a static erratic explanation for AI generated decision in Niloufer Selvadurai: Oxford University's International Journal of Lord information technology, and it has all the things I've just touched on, but in much greater detail, but this is one of the Niloufer Selvadurai: Cutting edge to the regulatory solutions which has been is being discussed, to promote transparency and accountability in a decision making. Niloufer Selvadurai: And this is very much a fluid and evolving area in 2019 the OECD issued guidelines on AI, which are a little bit more forensic and a bit more detailed than the Australian version. Niloufer Selvadurai: The Australian Human Rights Commission is also presently undertaking a technology inquiry which encompasses AI governance. Niloufer Selvadurai: So we're likely to see a lot more law reform discourse in this area. It's an exciting moment. I think for lawyers in this area because we're going to see some very important statutory amendments in the near future. Enrico Coiera: I think that's a fantastic answer. Thank you. And I have to say, I've ever been on a panel. Everybody has kept to time and visa. Enrico Coiera: So you're all wonderful and you do remind me of one of the issues that I didn't have chance to talk about, which is that at the moment, the FDA and other regulators. Enrico Coiera: As a certified technologies that might have been developed by machine learning, but they cannot adapt in the real world. Enrico Coiera: So, but we know that people are developing our systems that are self, self adapting through learning at the moment that's not allowed in the healthcare. Enrico Coiera: Sector, at least from the point of view of the FDA TJ. So lots of work for for for you folks in the future. So we might get some questions and we can return to you later. Thank you so much. Enrico Coiera: Thank you. So our third panelist. I'd like to invite to join us read a metal noni tape or hypothesis that correct somebody with a difficult name. I understand it's always a challenge. And I get rid of to turn the camera on and turn your mic on. Enrico Coiera: Radio is a senior lecturer Evercore law school and her area of international expertise is intellectual property and technology law. Enrico Coiera: And hopefully, she'll APPEAR SOON. Rita Matulionyte: Can you see me. Enrico Coiera: We can welcome. Enrico Coiera: Thank you so so Rita. And I understand that IP law is is an important topic for people who are Enrico Coiera: Developers of our technologies and that's absolutely the case in healthcare to for obvious reasons company obviously want to protect their technologies. Enrico Coiera: And one of the things we also hear a lot about not just an eye, but in digital health is just the implications of copyright. Also on the development of system so Enrico Coiera: Would you just talk to us a little bit about copyright law issues, their relationship to AI and healthcare and how it relates back to the issues we've just spoken about before. Rita Matulionyte: And thinking regarding Rita Matulionyte: For an interesting question. Yes. So when we talk about data. Rita Matulionyte: Which is if you are in in in developing AI systems we normally kind of think about discrimination issues that were touched upon and privacy issues. Rita Matulionyte: But we sell them anything and probably don't. Most of us don't even know what copyright law material copyright law and law place here. And actually that the data that we Rita Matulionyte: Algorithm, and the data that a algorithm produce could be protected by copyright and what does that mean for AI developers and for, let's say, Rita Matulionyte: Companies all that apply. AI solutions like health institutions. So I went to distinguish between probably three issues here. Rita Matulionyte: So let's, let's start with with like the end with the first one. So when when they see we have an algorithm that we want to train particular data we have to collect the data and in the source. Rita Matulionyte: This this in health sector this data might, for instance, these images taken by Rita Matulionyte: Either machine or human being. If the images taken by human beings. They actually protected by copyright and they are owned by the person who took them. Rita Matulionyte: We might want to collect data that has tags bakes like medical reports reports and so on this data. If it is these texts that if they've been written. Rita Matulionyte: By a doctor or any other practitioner would be protected by copyright and then doctor or that person who wrote that text would own the copyright into that. Rita Matulionyte: Data. So what does that mean for AI developers well under current Australian law, which is very interesting. Rita Matulionyte: AI developers actually need to why permission from all those people who wrote text or make pictures or any other copyright protected data in order to be able to feed that data into algorithm or useful machine learning. Rita Matulionyte: So if you don't get that permission from those so called Ryan holders. Well, you could be live on the copyright law. Rita Matulionyte: You could be prevented from using that data, or you can pay damages. So it's a bit of legal risk involved in the current Australian law. Rita Matulionyte: Now, if we think about the second issue with relation to copywriters okay so you have collected Rita Matulionyte: Data hopefully not copyright protected so you can use it for machine learning purposes and you created a very high quality data set. Rita Matulionyte: So we all know that data and a high quality data sets. The biggest challenge in in grading grade our algorithms and Rita Matulionyte: As far as I know in health system is as well. So I'll now invested a lot of time and effort and creating that data set. Rita Matulionyte: Do you get any copyright over it and you right over it. Can you have any control over it, can you prevent anyone else from just taking it and reusing and their own machine learning project. Rita Matulionyte: So this is quite a tricky question at the moment in copyright law because if and sometimes such data sets actually could be protected so called databases. Rita Matulionyte: If the there is an original human contribution so called in selection and arrangement arrangement of data. This is the legal kind of rule. Rita Matulionyte: But if that data set was a large extent generated by computer software like in using computer which I imagine very often happens. Well, actually there is no copyright protection. Rita Matulionyte: That means that if you invested a lot of time you help kind of creating that data say. Anyone else got by a access to the data said could reuse it, let's say in their own machine learning project and so on. Rita Matulionyte: Now, and the third question that I'm the final one that I kind of would like to race here that might be interested for AI developers. Rita Matulionyte: And users like as a health Rita Matulionyte: Authorities that apply particular technology is who owns outfits generated by AI. So it may be currently in the health sector, this issue, maybe it's not very Rita Matulionyte: Topical if we think about AI tools that help provide information. Rita Matulionyte: That informs diagnosis or die like help in diagnostic processes because the outputs are not really provide protected. Rita Matulionyte: But if you think about AI tools that for instance we generate medical reports that say report about spread of disease among certain groups of people or about Rita Matulionyte: In between different countries and so on. So, if there is a bit of more elaborate text that is produced by AI application, it actually could be protected by copyright Rita Matulionyte: But at the at the moment in your city. We have a bit of a issue here because according to current legal practice. Rita Matulionyte: Copyright generated works or AI generated out goods are not protected if they're not protected by a human being. So it means if Rita Matulionyte: Let's say a pharmaceutical company like a bison, a application that helps them like generate let some sort of some sort of medical reforms and they have commercial value. Rita Matulionyte: At the competitor. If you know they get access to that report, they can use it reproduce the seminary do whatever they want without report without any legal liability or and nobody could stop them because Rita Matulionyte: Some sort of AI generated reports, but not protected when I apply. So these are these are my three issues that I want to touch upon an answer your question and Rico. Enrico Coiera: Fantastic. And you remind me of an issue. Last year in England, where the NHS sold a large amount of medical data to Google's DeepMind Enrico Coiera: And they probably were legally able to do that. But they didn't have the social license for sure. And there was enormous pushback by the community about their NHS records being used by a third party. Enrico Coiera: So even if the law is there maybe the social licenses. Enrico Coiera: I just, just to stick to this issue of IP, though. Enrico Coiera: One thing that has been a constant debate across digital health and now AI is is the regulatory balance between protecting people, whether it be in terms of IP or safety and then supporting innovation are and it seems to be an endless tussle. Enrico Coiera: Where do you think, or do you think the current IP structures we have a sufficient to promote innovation side in Australia, or do you think when they perform in that area. Rita Matulionyte: You can request a really good question and exactly intellectual property law and copywriting especially always tries to balance those two interest. So how do we actually promote innovation and at the same time we get more access to Rita Matulionyte: That innovative content to the broader public and intellectual property right itself. It's a human rights. So that's why it's kind of establishing balance between different Rita Matulionyte: Values and different writers very difficult, but if I think about the particular in corporate context, what sort of reforms we might want to that word for instance facility innovation, he is in a sector and including in it's a Rita Matulionyte: Applying AI innovation health sector, I would maybe think about, you know, two issues that let's say a industry might kind of consider whether it's an obstacle for them, and whether you would like to push for legal reform. So one thing is this issue that I mentioned as first one so Rita Matulionyte: I imagine know he Rita Matulionyte: Wants to go and try to get permissions from Rita Matulionyte: People who create a wrote text or make images and so on. Before the feed that into their algorithm. This certainly, certainly makes innovation more difficult, right, it Rita Matulionyte: requires much more effort and it will, and in other jurisdictions that you use of data corporate data, let's say in machine learning process is already legal Rita Matulionyte: In USA discovered bicycle fair use doctrine which is not important available Australia European Union has introduced recently a so called Rita Matulionyte: text and data mining section of copyright law which says that if you want to use copyright data in any machine learning process. You can do it for free without any authorization Rita Matulionyte: So if Australian AI community thinks that this copyright gap grunting copyright law and of impedes innovation makes the word difficult I think they should Rita Matulionyte: Press Australian Government to introduce some similar exception and our student go and hello REACTION BEEN THINKING and discussing this, but I think there is still lack of Rita Matulionyte: Understanding that it is a real obstacle in AI industry and it's probably close to some extent, lack of knowledge about this issue. Now a second probably legal reform kind of Rita Matulionyte: Option or like not option. In addition, what were legal fun reform could go is as far this Rita Matulionyte: Protection of out with is concerned. So if Australia in the interstices that, you know, and Rita Matulionyte: That you the lack of protection for out was generated by AI is a problem for them that, let's see. Let's see. Whatever. As I mentioned, medical reform reports when they are Rita Matulionyte: Generated by AI could be simply appropriated by their competitor without any legal and kind of tool to protect Rita Matulionyte: From this this from happening. So they there isn't a way to push again for some legal reform. So they say in United Kingdom. There is already a law that says that computer generated works on outputs or whatever you call it. Rita Matulionyte: is protected by copyright. So, essentially. So if you buy a software that produces, for instance, media articles. Rita Matulionyte: In talking like if we talk about other industries and then you publish them online. Other in that case. Other me news publishers can just take a media article and reproducing the website. So this would apply, of course, across different industries and then and if Australian Rita Matulionyte: Australian AI industry things as a suitable solution or they could raise that issue with a stranger, and I'm sure they would consider that seriously because there are presidents in other countries. Rita Matulionyte: By the similar regulation think yes i think that's that's enough. Enrico Coiera: Thank you. Enrico Coiera: I think as I'm going through the panel today I'm realizing just how Enrico Coiera: Complex these legal issues are and how little they have been addressed or thought through and how much work we have a head, given the speed of innovation. Enrico Coiera: In the space and how slow the regulatory and legal structures seem to move, Sabrina. Thank you for now I much appreciate it and hopefully we'll get some questions for you. And just a reminder to the audience. Enrico Coiera: If you do have questions, and you'd like to master the end, please send them through the question section. So we can see them and get them asked. So our final panelist today is Steve mccombe Stephen if you could turn the camera on and to my colleague Stephen McCombie: Grace and Rick, I can you hear me. Enrico Coiera: Yes, I can. Thanks for coming on there. So I think at the very beginning when I was giving some introductory remarks I talked about the risks of AI and Enrico Coiera: Those risks include Enrico Coiera: Challenges to safety and cyber risks and that's your area of expertise you leave them acquire a unique cyber intelligence lab. Enrico Coiera: And do you have a deep interest in cyber crime and forensics and cyber threats and all things cyber and bad Enrico Coiera: So, from your perspective, Enrico Coiera: Is AI tool to kind of make the world safer in terms of the cyber threats or is it really just another target and you kind of target to have to worry about. Stephen McCombie: Like, like a lot of technology and that's the nature of cyber, cyber attacks and cybercrime, it can be used to defend and it can be also uses up on a radio attack. Stephen McCombie: And you know, you can sort of see that through, you know, the development of cyber attacks over time. Stephen McCombie: And I always really important now in in cyber security tools because one of the things you need to identify is what's Stephen McCombie: What's the malicious traffic. What's the normal traffic and that. And that's, that's one of the greatest challenges and that's been done largely manually today. Like there's been a lot of secure price and sentence below operators that are looking trying to pick up Stephen McCombie: irregularities in what's being seen, to see if it's actually an attack that's that's Stephen McCombie: very labor intensive and and you may know that cybersecurity professionals are in high demand is now there's a massive shortage of the numbers that they estimate that even with the current Stephen McCombie: Courses in Australia. We're going to be something like 50,000 people short in the next three or four years that system Australia alone. So, so there's less people to do these jobs. Stephen McCombie: So I does a lot of work in this space in terms of identifying you know through through machine learning and and through through Stephen McCombie: Through through various processes to actually identify what what's not normal, and help those human operators actually identifier. But the problem is in cyber security. Stephen McCombie: It's actually very difficult to automate things. And we've tried many times I mean cypress trees have this history of using technology wherever it can be because I simply subscribe, people have been it people Stephen McCombie: Not, not necessarily from anywhere else. So they're thinking of technology solutions to the problems they're facing. Stephen McCombie: But we've seen there's lots of disadvantages in that that's very hard to automate responses. So, so that's that's a real problem. Stephen McCombie: But the other aspect is, I, I can be used by the attackers as well and IBM, which is obviously done a lot of research in the eye and leaders in that area also looked at threats and then area and they've looked at Stephen McCombie: Some various ways that attackers could use AI in aiding attacks and and there's a range of things they can do they can use it to help Stephen McCombie: Aspects of the attack, it can actually manage the whole attack. It can be used for things like making a bit of malware less detectable by AV Stephen McCombie: antivirus. It can be used to make an email more realistic compared to other emails. It can be used to identify a target before decrypt the actual malware. Therefore, the opportunity actually identified is is not actually on the on the target person's computer and IBM actually built Stephen McCombie: An AI malware that did this with basically just a proof of concept where when the person got to the target person. Stephen McCombie: Using the camera, it would actually identify who the person was being their particular target and then it would would decrypt the malware. Therefore, at that point. Stephen McCombie: Potentially identifiable but but but to light to decide the target. And that's one of the great challenges is with malware over time. Stephen McCombie: These attackers have used various ways to escape the malware. So, so in the, in the early days, they, they just did, simple things like using packets which is a program you're used to actually pack some code. So it's so it's smaller, but Stephen McCombie: Can also be used to obfuscate that then they used Stephen McCombie: Various types of encryption to do that. Stephen McCombie: They used any forensics without looking to see if it's being examined. Stephen McCombie: And those had to be overcome by the by the defenders and that's that that's the reality of cybersecurity is Stephen McCombie: It's the the innovation is where the attacker where we can only just observe what they're doing and try and respond. And so I argued see me ability to actually like much more difficult to identify malicious software. So basically, those attacks. Stephen McCombie: And health systems have been attacked numerous times. It's an example, such as anthem in the US. Stephen McCombie: And and sing health in this region with state based attackers. These are intelligence agencies operating with lots of resources have come after health records in the case of seeing how they got the Prime Minister of Singapore's Stephen McCombie: Health Record and nothing many hundred thousand others, despite Singapore being being seen as a as a model in terms of the sophistication. Stephen McCombie: And the investment in cyber security. So, so the the attackers will use iron, iron this why Enrico Coiera: That's, that's great that if we flip it. So I obviously can be a new class of weapon, but if we think about the idea that we've got any healthcare. Could it also be a target. Could it be attacked. What would happen if I was targeted. Stephen McCombie: Yeah, and it's Stephen McCombie: It's a real problem because Stephen McCombie: Because the attackers are actually very thinking about how they can use the technologies were using against us, you know, and even in the early days of when when hackers were, you know, little Stephen McCombie: Teenagers at home, you know, in the US actually attacked the the system that was used by the police to tap phones. So they broke into that they use that to listen to other people's phones. Stephen McCombie: And the very famous case of Stuxnet where where the US in Israel got Michelle software into the nuclear facilities in Iran to try and actually destroy centrifuges will be used in enriching uranium. Stephen McCombie: They use those very same SCADA systems, I broke into to trick the operators that everything was fine. Stephen McCombie: So not only did they use it to execute the attack, but they used it to trick operators that the systems were operating correctly. Stephen McCombie: So this is the ability to use it and you think about II systems in health, they have my unfettered access to all sorts of data, the ability to move around and if if your intent is not to steal data, but actually cause Stephen McCombie: Impact to hurt people to cause havoc. You know, there's great potential there and you know changing around how Stephen McCombie: How the IRS actually supposed to operate help how the rules are operating Stephen McCombie: And you think, again, historically, look at text. Stephen McCombie: Rushing GRU attacked the Stephen McCombie: power grid in your crimes. Shut up all the power and give yet. Stephen McCombie: threat actors like this have executed. These types of destructive attacks and in was, you know, we have a lot of hybrid warfare now. Stephen McCombie: Now, the Russians and others have have have demonstrated in this way where along with conventional means it systems are also a target to health systems are key you think in terms of other nations security that can cause Stephen McCombie: fear in people and so much potential there in terms of their systems but but I suppose. The one thing I would say, you know, Stephen McCombie: Obviously I'm planning a bit of a bleak picture of the future of it is the fact that, you know, from my experience and studying attack this by state based and criminals, is that they're largely they're lazy and I only do what they need to do. Stephen McCombie: So often, you know, we see we have examples where people can build this sort of a proof of concept where you can do these things. It's sometimes, sometimes like a lot lighter, the actual tackles do it themselves, because what I'm finding necessary, in fact, Stephen McCombie: Very simple methods still work and okay so I wasn't often talk about the 2016 Stephen McCombie: Democratic National Committee being broken that would have broken into by it by old fashioned phishing email selected been done for about 13 years. So the old method still work. Stephen McCombie: So I think the the challenges if the opportunity exists in I that doesn't exist elsewhere. That's when it will become a target and that may take some time. Enrico Coiera: Very 2020 sort of discussion. It's Dave anything about what's happening now in the years with the elections. So thank you very much for that. I wonder if I can invite all of the panelists to reveal themselves again turn on cameras and Enrico Coiera: Microphones and delivered. I don't know if there are any questions that you want to ask the panel from the audience. We have about five or six minutes left. Niloufer Selvadurai: Oh, I just been looking through the questions. And there are a customer questions around challenging AI decisions. And I thought, maybe I'll just present the questions and Niloufer Selvadurai: everyone on the panel could provide their perspective as to sort of main areas and I'll just mention the wording of one of them. Niloufer Selvadurai: So any says many AI systems are invisibly integrated into our everyday technologies. Niloufer Selvadurai: How would one know there indeed interacting with an AI system and bestest request the system to explain it reasoning and challenges explanation, a brilliant question. Niloufer Selvadurai: And there's a second similar question along the same lines of how patients can challenge and Karen, do you want to start by giving a perspective and we can just all share insight. Carolyn Adams: Oh, so I would say that the fact that an iOS system is being used to assist the decision is Carolyn Adams: Information that needs to be shared with patients or they can't give fully informed and voluntary consent. Carolyn Adams: That decision making process from an administrative lawyers point of view, the criteria and prices of decision are almost as important as the decision themselves and they need to be mind. Carolyn Adams: And what worries me. Great question. Any, by the way, is that if there's processes that go on behind the scenes. And we don't know when and where they're happening and which decisions are being assisted the whole system will fail because the health systems built on trust. Carolyn Adams: You know, part of social license is developing trust with users and so transparency. Carolyn Adams: will assist with that with developing and supporting social licence. So I think the way that medical practitioners explain decisions to patients is going to have to change. Niloufer Selvadurai: Thank you very much. Thank you very much. Carolyn. That's very, very accurate very useful. Niloufer Selvadurai: I could just add a little bit more from the regulatory reform perspective set prices, we have two models quite distinctive. One is I could call it proactive and one is reactive. So the Australian government guidelines in 2019 Niloufer Selvadurai: Is a proactive model where we proactively healthcare providers are proactively have to advise Niloufer Selvadurai: Patients when a health care system is integrating some aspect of AI and explain its functionality. So that's a proactive model. Niloufer Selvadurai: The European Union's GDP is somewhat of a reactive model because it doesn't have that preliminary duty to notify it once you're affected by decision. Niloufer Selvadurai: Then individual then goes back and examine what the decision making process was and then challenges it. So I think Australia is probably going to go down that proactive path because of their guidelines. Niloufer Selvadurai: But as always, as a long lag between policy pronouncements and voluntary codes and actual statutory enactment. So I think what will happen this time is best practice ethical principles to guide us rather than legal obligations. Niloufer Selvadurai: Does anyone want to add anything and reach out from an IT perspective of Stephen from a cyber security side of things. Rita Matulionyte: But I don't think I have to add anything from IP perspective. But you know, I think. Rita Matulionyte: Kind of in the Rita Matulionyte: Following up on this Caroline's idea that, yes, we actually need to change the practice how patients are informed about AI assisted decisions. I was just thinking Rita Matulionyte: That we have to keep in mind that in you know that it's been a long time that technology has been already playing a role. Rita Matulionyte: In making diagnosis or assisting doctors in different processes and it's not necessarily to information about all different technologies that doctors using their work. Rita Matulionyte: You know, in kind of deciding what treatment and others. So there will be a difficult task. I think for lawmakers of member to draw that line. Rita Matulionyte: Which what sort of AI assistant decision should be explained to patient before you know they accepted the treatment or deny Rita Matulionyte: And which ones not because technology. So I think there are very different, different types of technologies and different level of involvement of technology into decision making by a doctor. So I think it's, it would be an interesting exercise in in the area of policymaking Niloufer Selvadurai: Even, did you have anything from Stephen McCombie: Yeah. Just, just one quick point at someone made a comment about about, you know, do any more regulation for these issues. Stephen McCombie: Always sort of leave that to others, I think, I think the problem is more understanding what the realistic threats are. And I suppose you know obviously our cyber intelligence level we're thinking about Stephen McCombie: What's going to happen in the future. Having that foresight and understanding what type of AI systems are going to be targeted what to the health systems are going to be targeted. Stephen McCombie: By who and how and trying to stay instead of waiting for it to happen actually thinking about those risks and informing regulation to protect against those types of threats. Niloufer Selvadurai: Thank you very much. I think that's all the time we have. But if you do have a question or you think of something later, please do contact us contact details will be provided see a hand you back now to Professor Enrico but he's closing thoughts. Enrico Coiera: Thank you so much for this for me, really, to thank the panel. If you could hear it, the Orient is clapping and all stomping their feet. It was just Enrico Coiera: A very, very interesting and I think thought provoking our we've had, you know, it's researchers like myself focus on the technology and use we raise issues and problems that we Enrico Coiera: Are worried about, but we don't really own the solutions and lot of these solutions are going to fit in the space of the law. Enrico Coiera: And regulation and so it's just wonderful to start to get an inkling of how difficult that is. If, if anything, I am now probably more worried, given how complex it is and and how rapidly is is already becoming a bit of their lives so Enrico Coiera: I never thought I'd say we probably need some more lawyers right Enrico Coiera: Anyway, thanks. All I look forward to our next panel and hopefully if people have questions that they haven't been able to ask the the details for contacting our panelists today. So once again, thank you all for for a wonderful hour Rita Matulionyte: Thank you. 