 Let's here 'Airplane' rock style composed by artificial intelligence. If we make IU's music into midi file and input a certain song it would come out in IU's singing style there is K-pop too. Hello. This is Jo Coding who makes an easy coding channel in which anyone can learn coding. This time, I prepared something very special. It is this. It is artificial composer tool DeepComposer which was made by the world's biggest cloud service 'Amazon Web Service'. Even though you are not very good with music, if you input a certain melody it is a tool in which artificial intelligence composes the song for you. I will open it. There is an address with which we can use DeepComposer. There is a keyboard. It says that I can use it by connecting it after getting on to AWS. I will try it to see if it composes the music well. Let's take a look through the screen. I accessed AWS DeepComposer console. DeepComposer can be only used in the eastern part of the U.S. and the north part of Virgina. But we can just set it to the north part of Virginia so you can use it immediately in Korea too. You just have to change only the location to north part of Virginia. Seeing this part, it says that I can connect the keyboard. It says that I can get a free trial once I connect the keyboard. I will use it for free using the keyboard. You can connect it with the computer with usb. Click 'Link Keyboard' here and it says that the keyboard has been connected with usb. If you connect with the keyboard, it is free for three months and it says that you can make 4 customized models every month and compose 40 times. It says that I should input the keyboard serial number below. I will connect it by pressing Link Keyboard. The keyboard has been connected. Seeing here it seems like it is explaining the principles of composing so I will watch it. I will read the explanation by pressing 'Get started'. It says that it uses machine learning and that Generative AI is the most advanced AI. I think they compose music using this kind of thing. It says to judge whether it is human or AI and between these two, it says that one is music made by AI. I will listen to B too. I think this is made by AI. I will pick. Yes, I was right. But it was made quite well. It says that DeepComposer composes music through U-Net method and MuseGAN method. It says that U-Net was originally used to make images but it is also used to make music. MuseGAN. Is this right? MuseGAN? Anyways, it says that MuseGAN is an architecture made to compose music. This will be better. I will listen to each one. It learned based on the song composed by Bach and it was composed using U-Net. Should we listen to it? It sounds okay! Is there a feeling of Bach? The music is good. This is MuseGAN that made the song in jazz style and let's listen. It's strange! I think it made 'Twinkle twinkle little star' into jazz style. I think U-Net is rather better. I think it's more natural. I can make music with U-Net like this or with MuseGAN. To simply put generator randomly makes music and discriminator repeatedly judges whether the made music is okay or not and gives it back to the generator and I think it continuously gives feedback. To organize, the generator makes the music and the discriminator judges whether it is okay and if it's not okay, it gives feedback to the generator and it would make new music and compare and as this process is repeated a good music would be composed in the end. For learning, it uses MIDI file tempo, speed, pitch, scale, and MIDI file that contains what kind of instrument's sound it is to proceed with the learning. Now, it makes sounds like this. Here, there are basic example songs. You can listen to them if you press play. If there is basic melody you can select a model and this is an artificial intelligence that changes the music into rock style if you want rock style. Here, I will change 'Twinkle twinkle little star' into rock style with 'generate composition'. It seems like something has been composed so I'll try playing it. It doesn't seem bad. I think it's worth a listen... but, at the same time, it doesn't seem that good... I think it's nice that the AI can compose rock style music. Other than the given songs, with 'record a custom melody,' you can play the piano and directly convert it. Even if you don't have this keyboard, you can play it using your mouse or you can use the asd, it's connected right? You can even use your computer keyboard to play the piano. You don't need this keyboard to play. Now I'll try to directly enter the custom melody. You press the record button and play. I tried playing the song, 'Airplane.' I'll try to change 'Airplane' into a jazz style. Let's listen to how 'Airplane' changed into a jazz style. If I press play... it seems like the AI still has a long way to go. Although I did play it a bit weirdly... Shall we try listening to it one by one? I think this is the main reason why the song came out weird. Let's listen to the drums. This doesn't seem bad... By pressing this, you can change the instrument that makes the harmony. Right now, it's set as the piano but I'll change it to the guitar... There are different types of guitars. I'll try the jazz electric guitar. Is it better? It seems a little better... It doesn't mix well but... Is it because the song is weird? I'll try it again. I'm curious as to how it will turn out. This time, instead of the jazz version, let's listen to the rock version of 'Airplane' that the AI composed. Although I don't know much about music, I can see that it's not that good. But it's still cool. With the music inputed, although I'm not sure if it mixes well but, it does give a certain output... So that's how it is. I don't think it's that good. Is it not good because I used MuseGAN? Should I try using U-Net? I can also make a model... On the left, there's a button called 'Models.' You can make a model by pressing, 'Create a Model.' We tried using MuseGAN because a lot of people said that MuseGAN was good but after listening to the output, I don't think it's that good. I'll try using U-Net. I'm guessing they set this correctly, right? If you press, 'Start Training,' the model gets created. But it takes a maximum of 8 hours. I'll start this, do something else in the meantime, and come back to it later. I'll press 'Start Training' and create a U-Net model. I'm working on the U-Net training. I completed the U-Net model. Originally, I made it learn only this but because the graph came out so weird, I made another graph by changing these numbers. Both of the graphs aren't that good. I didn't mention this earlier but, the basic learning data this offers is Bach(?). I was wondering what Bach was but I realized that I was pronouncing it wrong. Bach not Bach. Bach's music is included in the basic training data set. Thus, if you make a model out of this, a model of Bach's music will be created. Thus, if you apply this and make music out of it, it changes to a Bach style kind of music. I'll try making music by applying this. This time, I'll use U-Net instead of MuseGan The model is U-Net that has learned Bach's music. First, I'll listen to U-Net1. The U-Net model isn't that good either. Then, I'll try playing U-Net2, which I made by changing the numerical values. It sounds like a bomb. I prefer the basic model created with MuseGAN. Additionally, if you visit GitHub, There is also a Lab here where you can practice. If you use this to practice, then you will be able to learn the principles and methods a lot. This is the same thing as what we learned earlier. In Lab 2 we did custom GAN model, we had models like rock or pop and we will be making these ourselves. Github already has some of the basic codes this is original midi, MIDI is a music file and you use this music file to make a model. For a rock model, you collect the MIDI files of rock music and make a model out of it. However in the example of original midi, there is a hymn called Chorale. These are 229 Chorale midi files. If you look at the original midi, there are 229 pieces of midi like this. So if you copy this code and use it, the hymns will be learned and a hymn model will be made. If we use this to do what we did earlier, the music that we enter will use this model and change it like a hymn. So if we use this midi file piece for example if we make IU's music into a midi file and put it in here, then whenever we enter a song it will come out in IU's music style, and if we put GD's music as a midi file, the music will come out like GD's music. I searched around to find some of these midi files but I couldn't get a lot of them due to copyright so I couldn't try it myself. If you happen to know a lot of free midi files, you can leave me a comment and I will try it again. AWS DeepComposer was shown in AWS's official event, re:Invent last December. I will show you a video of it. There is a scene where a singer uses AI to create a song and sing it. Let's watch it together. It's not like I'm talented in music or anything but it's not really good. Now looking at the comments, they seem similar to the comments of the music video Gang which went popular recently. The best comment here says that the music is 'embarrassingly bad.' The second best comment says that he would have 'fired the drummer.' 'Build some robots to listen to that because I'd rather not..' 'I love AWS but this is almost like an April fools joke.' It seems like musicians won't have to worry about losing their jobs for a while now. Other than DeepComposer, there are also other various composing AIs. I will show you a very intriguing composing AI that OpenAI made recently. This new technology was announced on April 30th, 2020. If you go to the homepage of OpenAI, they have an explanation and code of Jukebox that is on Github as an open source so that anyone can use this in their compositions. Unlike DeepComposer, Jukebox doesn't use MIDI files to learn but rather uses raw audios to learn. It learns the voice and overall vibe of a person and composes according to that information. It seems like a more advanced form. Let's listen to a sample. We will be listening to a rock music composed in the style of Elvis Presley. This one also has lyrics. It also has the cheering of the crowd in it. This is cool. The voice or pronunciation isn't clear but it's not bad. Let's listen to Bruno Mar's song. It's a bit odd but I can still feel Bruno Mar's style. Other than these few examples, you can listen to many songs of different artists made by AIs if you press explore all samples. I listened to a few of them and I found some good ones as well as some weird ones. I think that musicians will also be able to gain ideas from this. They also have K-pop here. It's hard to hear the lyrics but I think it kind of feels like Girl's generation? We listened to different songs that the AIs made using various singers. The source codes are open so that anyone can use it so it would also be fun for you guys to use Jukebox to make a song in the style of the artist that you like. If this was helpful, please subscribe, like and make sure that your alarm is on. I will see you again with a better video. Thank you. 