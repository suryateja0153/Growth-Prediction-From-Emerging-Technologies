 good evening ladies and gentlemen my name is such Sachit Menon and today I'll be presenting pulse where it's self supervised photo-op sampling via latent space exploration of generative models so before we get into the details of what pulse does let's think about the general problem behind it the core of this issue is the high-resolution image data is often difficult to obtain for example in medicine astronomy microscopy satellite imagery all these problems want high-resolution images but for reasons of hardware or cost it's very difficult to obtain in addition consumer photography drives a lot of for example smartphone sales and the desire for high-resolution images there is insatiable and it continues to grow year year by year so this makes us think well these restrictions on hardware and cost will always exist but is there anything that we can potentially do to get these same high resolution images without having to change these constraints that we render this motivates the problem of image super resolution an image super resolution we try to construct high resolution images or HR images from low resolution or lr images but here we have a problem trying to recover a ground truth high resolution image is inherently an L post problem in particular many HR images can match the same LR input image on this slide I have an example from our algorithm pulse producing three different examples of women that downscale to the same low resolution input face in brief many images can correspond to the same lr image traditional methods on the other hand simply try to recover a single high-resolution image they try to do this by optimizing on some average in in terms of pixel values of how close the output it gets is to this ground truth image but this inevitably leads to blurring the reason is as follows we can see that if I have stubble then the stubble being in one pixel or the pixel next to it to us as people just doesn't matter it's the same perceptually but to this algorithm it can't afford to get the pixel the stubble off by a pixel if it puts it one pixel to the left what'll happen is it'll get penalized both for not putting stubble where there should have been stubble and for putting it in a different place why these pixel wise averages what'll end up happening is a smoothing effect where my face just is entirely gray clearly if what we're trying to do is get sharper higher resolution images this is in many ways at odds with our actual goal here so this motivates a different approach in fact we argue that the whole paradigm of trying to recover a single high-resolution image is fundamentally flawed for this reason instead what we might want to do is generate multiple realistic images that were downscale appropriately to the LR input that could all work and are all perceptually realistic in other words we want to say okay this looks like a real image in other words it's perceptually realistic and it matches up with the downscaled version if you don't have either of these then you haven't done super resolution appropriately because the goals are looking realistic and matching the input so that brings us to the core idea here how could we possibly get these multiple images in this new paradigm that we're thinking of well what did I say our constraints are it needs to downscale properly and needs to again look like a realistic image so what we can do is we can take these deep learning models that have learned to produce realistic looking images in our case gans but more broadly speaking generative models and we can try to find images that they produce that one will look realistic again that's we already get by them being in the output of such models so tend to will down scale appropriately and we can search for these by just using that as a loss and traveling along the gradient there's one more kink here so I kind of hand wave something away earlier when I said that we could just take it as oh it's the output of this of this model it'll look realistic but simply requiring that it's the output of the model in other words we finding some point in the models input that gives us some output doesn't mean it'll be a realistic output only a subset of the potential inputs to the model those that it saw when it was being trained we correspond to realistic looking images in particular most of the time when we train these we sample from a high dimensional Gaussian distribution and provide that as input to the model it takes these high dimensional Gaussian samples and uses them to produce realistic looking images but if you gave it something that didn't look like a high dimensional Gaussian sample it would give you nonsense in our case what that means is we need to somehow constrain on the search that we're doing to only look at these points that look like high dimensional Gaussian samples this leads us to the soap bubble effect high dimensional Gaussian distributions actually have most of their density located on the surface of a high dimensional sphere unlike the ones you might think of in one dimension dimension or two dimensions so this originally could seem like a problem but for our case this is actually very nice what this means is we know where our samples should be located so we can force the algorithm to only find outputs of the models that correspond to inputs to the models from the surface of this sphere this is the trick that lets them look realistic after we've constrained this to the surface of the sphere instead of doing gradient descent in some arbitrarily large space we're now doing it in the smaller space that is the surface of this sphere here are some implementation details we used style again a recent advance in Gans for its ability to make high-resolution op output images sharper than anything else we had seen before we used spiracle gradient descent meaning gradient descent where you project onto the surface of a sphere at every step or there are multiple ways to implement that and the core part is is by initializing at any point we an appropriate solution so by starting at different points we can get different solutions like the one I showed you before with the three women that all downscaled to the same thing in our case as you may have noticed we focused on faces simply because this is what generative models are best at right now but as generative models advance you'll be able to do this with any kind of infant image so here are some of the results you can clearly see that pulse on the two right most columns produces people that are far sharper than any of the other images and downscale is appropriately as you can see on the right hand side in particular I'd call your attention to the man on the top but as it tuft of hair as a diagonal black line if you look at the low res image it's very unclear what that would be but pulse knows that it should be hair and that a realistic human face would only have hair that down scales like that so it makes this beautiful looking talked which none of the other algorithms can do only producing a blurry smudge of sorts in addition pulse is actually inherently robust to noise when we run pulse on images with noise added in the low res space we actually get output images the downscale more closely to the low res original image which pulse did not get this may seem strange at first but the reason for this is that pulse can only produce like I said realistic looking faces this means it can only produce images that downscale two faces that look reasonable as low res faces meaning it can't match all of the strange noise that it sees in its input pulse therefore implicitly D noises things thus in conclusion we've made a new formulation for the super resolution problem and provide a new algorithm to solve this problem we've created higher-quality images at high resolutions 1024 by 1024 pixels versus 128 versus 128 which was the previous state-of-the-art and at higher scale factors 32 times instead of 8 times what I think the most important advance impulse is is that it keeps up with advances in the literature of these generative models such as ganz without any changes to this algorithm itself their plug-and-play even the code just you change the one line of what generator model you're using and you're done thank you very much for your time and I hope you enjoyed this presentation 