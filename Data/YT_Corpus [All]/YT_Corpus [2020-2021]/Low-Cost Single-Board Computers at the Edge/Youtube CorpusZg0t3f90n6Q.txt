 hi everyone my name is hein and i'm a developer advocate working on tensorflow lite and today i would like to give you a brief introduction about on-device machine learning many people may have an impression that machine learning models run on the server side with large cpus large gpus but the thing is that azure devices has already become a very important platform for machine learning so why is important to run machine learning models on azure devices the first reason is latency the use cases that require low latency such as processing video in real time definitely requires running the machine learning models on the device the second reason is offline availability you can use the feature even when there's no internet connection and the third reason is user privacy protection because you don't need to send sensitive user data to the server for processing and with the availability to run machine learning models on azure devices a lot of new features has become possible let me give you some examples of undivided machine learning running in real apps the first one is the feature on youtube that allows users to try different kind of cosmetics using ar the apps detects users libs in real time as you can see in the video demo here and it replace the color of the lipstick with computer vision or there's another example of google translate app that has the features to allows you to capture text with your phone camera and translate them in real time without any internet connection tensorflow lite is a framework to help you implement on-device machine learning and it allows you to run tensorflow models on edge devices with tensorflow lite you can use the same machine learning models clocks platforms without having to customize for each one tensorflow lite currently supports three platforms the first one is android and ios smartphones then the linux based iot devices and the microcontrollers let me walk you through some details of the platforms that supply support so smartphone is probably the most popular platform for on-device machine learning at this moment because many of google lashes act already using tensorflow lite like google photos gboard youtube and the system and there's also many popular third-party apps like uber hike airbnb and so on or using light in production many developers are using tensorflow lite for use cases around image text and speech but you're also seeing a lot of new and emerging use cases around audio and content generation and you can quickly try transfer light on smartphones with our sample apps we have more than 10 sample apps demonstrating different home device machining use cases including both computer vision and natural language purchasing please go to the tensorflow lite website and check it out and besides tensorflow lite google also has a product called mlkit for using machine learning on mobile it builds on top of tensorflow lite and provides a list of pre-trained models through apis for popular on-device machine learning use cases for example it can recognize text detect objects and more via an easy to use apis that mobile developers can use even without learning the complex machine learning concepts so that was smartphones and let's take a look at the second platform that has light support which is the linux based iot devices such as the raspberry pi here's an example of tensorflow lite being used in an iot device so ecovax is a company that produced vacuum cleaner robots and they use tensorflow lite on their robot to detect obstacles so that the robot can avoid the obstacles effectively and clean your house besides tensorflow lite as a software solution google also has a hardware accelerator called hdpu that can make tensorflow lite models run faster on iot devices http use are available in several different form factors under the coral branding you can start with prototyping your model on http with the coral that board it is a mini computer that has the arm cpu and it can run linux os you can also use the coral usb accelerator that can plug into your computer or other iot devices and when running on production you can use the som form factor or the pci express one that can be integrated into your existing hardware and the final platform that tensorflow lite currently officially supports ac microcontrollers such as arduino microcontrollers are the small low-power all-in-one computers that our everyday devices arouse like microwaves smoke detectors toys and many different types of sensors they can cost as little as 10 cents for one unit and with tensorflow you can turn them into a device for machine learning you might not have realized that machinery models running on microcontrollers is already used in so many devices that you use every day for example power detection on many smartphones now typically runs on a small dsp which then can wake up the rest of your phone you can get started with tensorflow light on microcontrollers with the arduino hardware and you can start doing speech detection with tensorflow lite on arduino in less than 5 minutes check out the tensorflow lite website for instruction on how to get started and next let's talk about how to optimize your machine naming models to deploy on edge devices so why do we need this optimization well azure devices generally have less cpu capability and memory and unlike servers many of those devices is powered by battery so this is very important to reduce the power consumption model size is also a very important factor given that the mobile upside is about 20 megabytes so adding an additional 100 megabyte for a machine learning models is totally unacceptable there are several ways to optimize your models for on-device deployment first you should use a mobile optimized model architecture then apply quantization and pruning to your model and finally make sure to leverage the hardware oscillator available on the hardware so now let's go through each of them in more details first of all you should choose a architecture that is suitable for on-device machinery for example inception or rest net are very popular model architecture for image classification but if you want to deploy on azure devices you should go with mobilenet architecture instead of inception or stack for example mobilenet v3 is slightly less accurate than exception before but its influence speed is almost 40 times faster and the model size is almost 8 times smaller so mobilenet is more suitable for on-device machining than inception model accuracy versus model size and inference speed is always a trade-off that it will need to make to improve accuracy the model will needs to be bigger and it will take longer to run so it's very important that you find the right balance between accuracy and model size and influence speed for your use case the good news is many of the on device machining optimized models such as mobilenet or efficient light are published with multiple variants so that you can choose the one with the optimal trade-off for your use case the second optimization method that you need to consider is contamination when building and training a model it's very common that we use 32-bit float type to store model weights however when running inference it turns out that even if this 32-bit float whips are approximated into 8-bit integer the model didn't lock much accuracy quantization is the act of approximating your model weight with lower bit representation and in this case your integer quantized model will becomes four times smaller and is also run faster than the original float model and here is the accuracy number of quantized version of some of the popular model architecture you can see that the accuracy drop here is around one percent point which is very the third optimization method we should consider is pruning it shrinks the model by making some of its width to be zero a pruned model is not only smaller but because it has many zero weights it reduces the computational needs when doing inference and it makes the model influence faster and here's a benchmark of a pro model you can see that at 50 point which means we remove half of the weight in the model the accuracy drop was quite insignificant around like one percent point and pruning is compatible with quantization so you can prune your model first and then apply quantization so that it will get a model for example eight times smaller than the original float model with very minimum accuracy drop and lastly when you run tensorflow lite model on mobile devices you can speed up the influence time by using the hardware accelerators on the device cpus is a type of hardware accelerator available on many mobile devices so it's an easy win for you if your model is compatible with cpu there are also other types of chip such as the edge dpus which is built specifically for machine learning inference so you can speed up your model even more than the cpus so that was it i hope that you find this video useful as a brief introduction to on device machining and in the next video we'll talk about how to train and deploy tesla light models on edge [Music] devices you 