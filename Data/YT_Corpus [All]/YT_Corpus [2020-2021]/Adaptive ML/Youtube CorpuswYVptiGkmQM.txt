 [Music] [Music] hi all welcome to this session on causality and machine learning as a part of frontiers in machine learning i am amit sharma from microsoft research and your host now of course i i presume you would all agree that distinguishing correlation from causation is important and even at microsoft for example when we're deciding which product feature to ship or when we're making business decisions about marketing uh causality is important but in recent years what we're also finding is that causality is important for building predictive machine learning models as well so especially if you're interested in out of domain generalization having your models not brittle you need causal reasoning to make them robust and in fact there are interesting results even about adversarial robustness and privacy uh where causality may play a role and so this is really an interesting time at the intersection of causality and machine learning and we now have a group at microsoft as well that is looking at these connections uh i'll post a link in the chat but for now uh today i thought we can ask all ask this question what are the big ideas that will drive further this conversation between causality and ml and i'm glad that today we have three really exciting talks our first talk is from susan ethi she's an economics of technology professor from stanford and she'll talk about the challenges and solutions for decision making under high dimensional data and how generative modelling can help uh in fact when i first started in causality uh susan's work was the first one of the first i saw that was making connections between causality and machine learning so i'm really looking forward to her talk next we'll have elias barrenbaum who will be talking about the three kinds of questions we typically want to ask about data and how two of them turn out to be causal and and they're much harder uh and and he'll also talk about uh an interesting emerging new field causal reinforcement learning and then finally we'll have uh cheng zhang from microsoft research cambridge she'll talk about she'll essentially give us a recipe for how to build models neural networks that are robust to adversarial attacks and of course as by now you have guessed in this session she'll be using causal reasoning at the end we'll have time about 20 minutes for open discussion so we all all the speakers will be live for your questions uh before we start let me tell you one quick secret uh all these stocks are pre-recorded uh so if you have any questions during the talk feel free to just ask those questions on the hub chat itself uh and our speakers are available to engage with you on the chat even while the talk is going on all right with that i would like to hand it over to susan thanks so much for having me here today in this really interesting session on machine learning and causal imprints today i'm going to talk about the application of machine learning to the problem of consumer choice and i'm going to talk about some results from a couple of papers i've been working on that analyze how firms can use machine learning to do counter factual inference for questions like how should i change prices or how should i target coupons and i'll also talk a little bit about the value of different types of data for solving that problem doing counter factual inference is substantially harder than prediction and indeed there can be many data situations where it's actually impossible to estimate counter factual quantities it's essential to have the availability of experimental or quasi-experimental variation in the data to separate correlation from causal effects that is we need to see whatever treatment it is we're studying that needs to vary for reasons that are that are unrelated to other unobservables in the model we need the treatment assignment to be as good as random after adjusting for other observables we also need to customize machine learning optimization for estimating causal effects and counter factuals of interest instead of for prediction and indeed model selection and regularization need to be quite different if the goal is to get valid causal estimates and that's been a focus of a lot of research including a lot of research that i've done a second big problem in estimating causal effects is statistical power so in general historical observational data may not be informative about causal effects if we're trying to understand what's the impact of changing prices if prices always change in the past in response to demand shocks then we're not going to be able to to learn what would happen if i change the price at a time when there wasn't a demand shock i just won't have data about that from the past so i'm going to need to run an experiment or i'm going to need to focus on just a few price changes or use statistical techniques that focus my estimation on a small part of the variation in the data any of those things is going to lead to a situation where i don't have as much statistical power as i would like another problem is that effect sizes are often small firms are usually already optimizing pretty well so it would be surprising if making changes leads to large effects and you know the most obvious ideas for improving the world have often already been implemented now that's not always true but it's common and finally personalization is hard if i want to get exactly the right treatment for you i need to observe lots of other people just like you and i need to observe them with different values of the treatment variable that i'm interested in and again that's very difficult and often it's it's not possible to get the best personalized effect for someone in a small data set instead i'm averaging over people who are really quite different than the person of interest so for all of these reasons we need to be um quite cautious in estimating causal effects and we need to consider carefully what environments enable that estimation and give us enough statistical power to draw conclusions now i want to introduce a model that's commonly used in economics and marketing to study consumer choice this model was introduced by dan mcfadden in the early 1970s and he won the nobel prize for this work and the main crux of mcfadden's work was to establish a connection between utility maximization a theoretical model of economic behavior and the statistical model the multinomial logit and this modeling setup was explicitly designed for counter factual inference and the problem that he was starting to solve was what would happen if we expand bart which is the public transportation in the bay area what if i expand bart how will people change their transportation choices when they have access to this new alternative so the basic model is just that an individual's utility depends on their mean utility which which varies by the the user the item and time and an idiosyncratic shock um in general this we're going to have a a more specific functional form model for the mean utility and that's going to allow us to and to um to learn from seeing the same consumer over time and also to extrapolate from one consumer to the other we're going to assume that the consumer maximizes utility among items in a category by just making this choice so they're going to choose the item i that maximizes their utility now the nice thing is that if the error has type 1 extreme value distribution as independent across items then we can write the probability that the the user used choice at time t is equal to i just in this the standard multinomial logit type of functional form so utility maximization will where where these muse or the mean utilities will lead to multinomial logit probabilities so data about individualized purchases can then be used to estimate their mean utility and in particular if we write their their utility their mean utility as something that depends on the item in the user but that's constant over time so this is just their mean utility for this item like how much they like a certain transportation choice and then a second term which is the product of two terms the price the user faces at time t for item i and a preference parameter that's specific to the user if i have these this form of preferences and then the price varies over time for while the user's preference parameters stay constant i'll be able to estimate how the user feels about prices by looking at how their choices differ across different price scenarios and if i pull data across users i'll then be able to understand the distribution of consumer price sensitivities as well as the distribution of user utilities for different items so in a paper with rob donnelly david bly and fran ruiz we take a look at how we can combine machine learning methods and modern computational methods with the traditional approaches to studying consumer purchase behavior in supermarkets now the traditional approach in economics and marketing is to study one category like paper towels at a time we then model consumer preferences using a small number of latent parameters for example we might allow a latent parameter for how consumers can care about prices we might allow a latent parameter for product quality but other than that we would typically assume that there's a small number of observable characteristics of items and there's some common coefficients which express how all consumers feel about those characteristics the traditional models also assume that items are substitutes within a category and they would ignore other categories so you might study consumer purchases for paper towels ignoring everything else in the supermarket just throwing all that data away so what we do in our approach is that we maintain this utility maximization approach but instead of just studying one category we study many categories in parallel we look at more than 100 categories more than a thousand products at the same time we maintain the assumption that categories are independent and that items are substitutes within the categories and we select categories where that's true so categories of items where you the consumers typically only purchase one brand or one of the item we then take the approach of a nested logit which comes from the literature and economics and marketing where within each category there's a shock to an individual's need to purchase in the category at all and but then conditional on purchasing the errors are or the idiosyncratic shocks to consumer utility are independent so having the single shock to purchasing it all is effectively introducing uh correlation among the probabilities of purchasing each of the items within the category at all now the innovation and where the machine learning comes in is that we're going to use matrix factorization for the user item preference parameters so instead of having you know for each consumer a thousand different latent parameters each one for each product they might consider instead we use matrix factorization so that there is a lower dimensional vector of latent characteristics for the products and the consumers have a lower dimensional vector of latent preferences for those characteristics and that allows us to improve upon say estimating 100 different separate category models we're going to learn about how much you like organic lettuce from whether you chose organic tomatoes and we'll also just learn about whether you like tomatoes at all from whether you purchased lettuce in the past so it won't have time to go through it today but this is just a layout of what we call our nested factorization model showing the the the nest where first you decide what to purchase if you're going to purchase um and then the consumers deciding whether to purchase it all and we have in each case vectors of latent parameters that are describing the consumer's utility for categories and for items one of the reasons that this type of model hasn't been done in economics and marketing in the past is that what was standard in economics and marketing if you were going to do a model like this would be to use either classical methods like maximum likelihood without very many latent parameters or to consider markov chain monte carlo bayesian estimation which historically had very limited scalability what we do in our papers is use variational base where we approximate the posterior with a parameterized distribution and minimize the kl divergence to the true posterior using stochastic gradient descent we show that we can overcome a number of challenges in particular introducing price and time varying covariates slows down the computation a fair bit and the substitutability within categories leads to non-linearities um despite that we're able to overcome these challenges once we have estimates of consumer preferences for products and and as well we have estimates of consumer sensitivity to price we can then try to validate our model and see how well do we actually do in assessing how consumer demand changes when prices change and so in our data we see many many price changes we see prices typically change in this particular grocery store that we have data from on tuesday night and so in any particular week there may be a change in price from tuesday to wednesday and so in order to assess how well our model does in predicting the change in demand in response to a change in price we take held out test data from weeks with price changes in those weeks we break the um the price changes into large price changes and different buckets of the size of the price change we then look at what is the change from tuesday to wednesday and demand in those weeks finally we break out those those aggregations according to which type of consumer we have for each item so in particular on a on a week where we have a change in price for a product we can characterize the consumers as being very price sensitive medium price sensitive or not price sensitive for that specific product and then we can we can compare how demand changes for each of those three groups and so this figure here illustrates what we find in the in the held out test data in particular we find that the consumers that that we predict to be the least price sensitive in fact don't seem to respond very much when prices change while the consumers who are most price sensitive or most elastic as we say in economics are the ones whose quantity changes the most when prices change once we're confident that we have a good model of consumer preferences we can then try to to do counter factual exercises such as evaluate what would happen if i introduced coupons that and targeted them at individual consumers we'll take a simple case where we have only two prices we consider the high price or the typical price and the low price which is a discounted price now what we do is we look into the data and we and we evaluate what would happen if we sent these targeted coupons out so for each product we look at the two most common prices that were charged in the data we then assess what um which consumers would be most appropriate for coupons we might look for example and say i want to give coupons to a third of consumers i can see which consumers are most price sensitive and most likely to respond to those coupons i can then actually use held out test data to assess whether my coupon strategy is actually a good one and that will allow me to validate again whether my model has done a good job in distinguishing the more price sensitive consumers from the less the less price sensitive consumers so this figure illustrates that for a particular product there were two prices the high price and the low price that were charged over time in the actual data the different users might have seen come to the store sometimes on a low price day and sometimes on the high price day indicated by blue or red what we then do is say what would our model say about who should get the high price and who should get the low price and so we can reassign counterfactually the say the top four users to high prices indicated by these orange squares and we can counter factually reassign the low um the the the the fourth and the the fifth and sixth users to the low price indicated by the green rectangles now since we that the users we assigned to high saw a mix of low and high prices i can actually compare how much those users purchased on the the high price days and the low price days and i can also look among the people that i would counterfactually assign to low prices and also see what's the impact of high prices versus low prices for those consumers and i can use those estimates to assess what would have happened if i if i reassigned users according to my counter factual policy when i do this i can compare what my model predicts should happen in the test set to what actually happened in the test set and so what i actually find somewhat surprisingly here is that in fact what actually happens in the test set is even more and more advantageous for the firm than what we model predicts so in particular our model predicts that if i reallocate the prices to the consumers according to what our model suggests would be optimal from a profit perspective we can get an eight percent increase in revenue um that is instead of varying prices from high to low from day to day we always kept them high and then we targeted the coupons to the more price sensitive consumers in the data if we actually look at what happened in our held out test data it looks like that the benefits to high versus low prices and the difference in those benefits between the high and the low consumers are such that we it looks like in the test set we would have actually gotten more like a 10 or 11 percent increase in profits had the prices been set in that way so to conclude the approach that i have outlined is to try to learn parameters of consumer's utility through revealed preference that is use the choices that consumers make to learn about their preferences about product characteristics and prices and then predict their responses to alternative situations it's important to find a data set that's large enough and has sufficient variation in price to isolate the causal effects of prices and and also assess the credibility of the estimation strategy and it's also important to select counter factuals to study where there's actually enough variation in the data to be able to assess and validate whether your estimates are right and so i illustrated two cases where i was able to use test set data to validate the approach you use the training data to assess for example which consumers are most price sensitive and then look at the test data and see if their purchase behavior varies with price in the way that your model predicts in an ongoing work i'm trying to understand how the different types of data create value for firms and so in particular firms are using the kinds of machine learning models that i've been studying and they use those estimates in order to do things like target coupons we can ask how much do profits go up as they get more data and in particular how does that answer vary if it's more data about lots more consumers if or if we do things like retain consumer data for a longer period of time and preliminary results are showing that retaining user data for a longer period of time so that you really get to know an individual consumer can be especially valuable in this environment overall i think there's a lot of promise in combining tools from machine learning like matrix factorization but also could be neural nets with some of the traditional approaches from causal inference and so here we put the things together we used functional forms for demand and the concepts of utility maximization and approaches to counter factual inference from economics and marketing but use computational techniques from machine learning in order to be able to do this type of analysis at large scale uh hi guys uh good afternoon i'm glad to be here online today uh thank you for coming uh i also thank you for the organizer i appreciate the organizer amit and amber for inviting me to speak in the events today uh my name is elias barrimboy uh i'm pro from the computer from the computer science department and the cause artificial intelligence lab at columbia university uh check my twitter uh if you are interested in discussion some sometimes heated about causal inference artificial intelligence and machine learning um also apologies for my voice uh i'm a little bit sick but very happy to be here today um i will be talking about uh what i have been thinking about uh the foundations of artificial intelligence how it relates to causal inference uh and these notions of explainability and decision making let me start from the outline of the talk uh i start from the beginning uh defining what is a causal model uh i introduce uh uh three basic results uh that are somewhat underwine i usually say that they they if you understand them we understand like fifty percent of of what causal inference is about because there's a lot of more technical results but the conceptual part that is the most important um the first our start of the structural cause of models which is the most general definition of a causal model that we know to date that is by pearl himself then i introduced the second and third or let's say the second result which is known as the pro-causal hierarchy the pch which was named after him uh this is the main metaphor or object a mathematical object used by judah you the pro himself and then i'm occasion in the book of why uh if you haven't read the book uh please i strongly recommend it it's pretty good since i discussed the foundations of causal inference how it relates to the future of ai and machine learning uh more prominently in the last chapter as well as the intersection of the other sciences this is work uh partially based uh uh on the chapter that you wrote in order of yoda on pros iraq in the foundations of causal influence there's a joint worker juan correa delegate in berlin and thomas eckhart the first my student at columbia and the last two are collaborators from stanford university this is kind of the link here to the chapter take a look because most of the things that i'm talking here is there in some shape or form um then i'll move to a another result that is called the causal hierarchy theorem um that uh which is just proving the chapters out maybe a 20 years old 20 plus years old open result and used as one of the main building blocks as one of the main metaphors in the book of life then i will try to connect with mainstream machine learning and or more specifically supervised and supervised and personal learning how does it fit or how it fits with this pch of the causal hierarchy also called the ladder of causation in the book then i'll move to talk a little bit about what is causal inference and cross-layer inferences um i will then move to the design of artificial intelligence uh at efficient thousand systems with causal capabilities i will connect with modern machine learning methods and and refer to try to put deep learning and rl in perspective and my focus here will go more on my goal is to introduce the ideas principles and some tasks i'll not focus in the implementation details uh also i should mention that essentially this is the outline of my course that is a semester course at columbia um bear with me i'll try to give you the idea if you're interested to learn more uh check the reference or talk with me send me a message now uh without further ado let me try to introduce you the idea of what is a causal model or structural causal model and we will use the idea or leverage this idea from from the processes we take a process based approach to causality uh and the idea is borrow from physics chemistry sometimes economics and other fields other fields that we have a collection of mechanisms that underline us some some phenomena that we're analyzing in this case suppose that we're trying to understand uh the effect of taking some drug on the on on the headache and those are observable variables and we have the corresponding mechanisms here f sub d for the variable drug and f sub f sub h to the variable headache each mechanism uh takes as input or has as argument uh satoru observables in the case of f's of the age uh and an observables in this case u sub d is unobservable here is like oh other all variables in the universe that generate variation to drug that is not age can be included in this usability and the same here with the u sub h there is the drug and age that are observables goes to the transformation f sub h with the u sub h again all variables in the universe that are not uh drug and age and someone we have or not have headache um this is a this is the with real processes usually we have a possibly complicated function here in this f sub g f sub h that is instantiated usually you move some type of we have some type of coarsening and this is the causal graph related to this collection of mechanisms the causal graph is nothing then the the partial specification of the system in which the arrows here just means that some that some variable uh is participates in the mappings of the other other i just put x y z to make my communication our communication easier um now we have for example that uh uh age participates in the mechanism uh of headache f sub h then there is iaro here from z to y from h right the same with drug there's the zero from x y and the same age here also participates in the f sub d note here that in the graph we don't have the particular instantiation of the function we are just preserving the arguments that we have there now for sure uh we can uh we can try this is the process that is kind of unfolding in time we can sample sample from a process like that this this gives rise to a distribution uh observational or non-experimental distribution over the observables p z x and y in this case usually when you are doing machine learning supervised learning or other supervised learning we are playing about this this side here of the equation here we are trying to understand causality uh because all is about when you go to the system uh and you change something or you overwrite overwrite some as a computer science because we've got like this we're right overwrite some function here we like to overwrite the equation the natural way of how people taking drug here is drug issue yes this is called also uh introduced organically given the time but this is related to the do operator in which you override the original mechanism f sub d in this case is do x is equal to yes that now we no longer have our original equation we have a constant here and you can have another concept that there's a constant snow on the other side we don't have time in the slides uh and this is you know that's what we have um this is semantics without necessarily uh having access to the the mechanism themselves this is the meaning of the operation now here is the graphical the graphical counter part of that note that in there is the f sub d here we no longer has the age as an argument of this function that is just the constant then you put the constant here and we cut in relative to this graph you cut these incoming arrows throughout the x this is the mutilated graph again if you are able to contrive reality this way you can sample from this distribution or from this process which gives rise to the distribution called interventional distribution or experimental distribution p of z y given dou x is equal to yes now they i use these uh variables here x z and y but x could be any decision y can be any outcome and z can be any set of covariates or features now what is the challenge here the challenge that in reality this upper floor here is almost never observed this is usually called unobserved this is why i put in gray then this is one of the things we don't have that in practice or very rarely and another challenge usually observe the data that we have it's coming from the left side that is coming from this naturally unfolding or how the system is naturally evolving and we would like to understand what is the effect if you go there and do the intervention in this system and with our own our own will or deliberately as a policy maker or decision maker decide to set this variable access equity yes then we have data from the left from saying and you like to do inference about what would happen if you do something in this system now um we can try to generalize this idea and define uh what is the structural causal model this is chapter chapter in causality book approach 2000 uh i'll not go to the definition step by step i suffice here to say that we have a set of observables or endogenous variables like age drug or headache we have a set of exogenous or as i said other observed variables that could be the u sub d and the use of h that we have before and we have a collection of mechanisms for each of these observed variables we have a mechanism f sub d or f sub h exclusive this tree here is related it could be seen as some type of newtonian physics to kind of summarize the conditions outside the system excuse me outside the system uh if you're into somebody that kind of spring sprinkle mass of probability mass and you have this probability p of u over the exogenous variable now we understand very well how these systems work there is this awesome work by halpern as a logician in cornell and goddess and pro uh giving this type of very solid type of understanding over the systems now today we are interested in a different result that is the following once we have a scm a structural cosmo m that is fixed for the particular environment or setting that we embedded uh the particular agents these induce this pro causal hierarchy or the pch that is called the ladder of causation uh in the book of white let me try to let's try to understand what is the pc here is the pch there are different layers of the hierarchy uh this is the first layer um that is called the of layers related to the activity of saying uh how would sing some verbal acts as he could choose mark x change my belief in the verbal why or what does a symptom tell us about the disease uh synthetically this is written as this p of y given x quite important people ask this layer but it is this is very related to the machine learning supervised and unsupervised learning there are many different types of formulas there are base nets is one one type of model formulas for they are you have decision trees you have super bottom platform genes you have the deep neural networks and other types of net networks uh they all live in this layer here quite important we have been trying to scale up inferences here given these acts could be the pixels themselves our set of features could be in the order of thousands of even millions and we like to try to predict why that is some label you have kind of pixels and you like to know if it's a cat or not it's kind of classic and it's very hard we're kind of mastering that or understanding pretty well how to do that in recent breakthroughs in the field in the last 20 years i should say now i have a qualitatively different layer that is layer 2 that is the interventional um is related to the activity of doing what if i do acts actions what if i take the aspirin why will my headache be cured um the counterpart in machine learning will be enforcement learning you have a cause of bayesian networks backup decision processes partially observable mdps and so on um quite important i will comment more about that uh symbolically by the way you say p of y given to x comma c that's the the notation that you have now i have a qualitatively different layer that is layer three uh that is a counter factual layer i'll go back here soon but it's related to the activities of imagination if you want agents to have imagination retrospection intro introspection blame responsibility credit assignment uh it is the layer that gave the name for the book of white this is the white type of question what if i had acted differently uh was it the aspirin that stopped my headache synthetically we have this kind of nasty counterfactual here i took the drug that is x prime as instantiation of the big x pardon for my license here x prime i took the drug uh and i'm cured that is y prime now you can ask i would have deal with the headache that is y the opposite of y prime had i not taken the drug that is the x that is the opposite of x prime i took the drug and i'm good experiment my prime in the actual world in this world and i ask uh what if i hadn't taken the drug that is x would i be okay that is the why or not okay that is the why okay you'll be not white then there's no counterpart exactly in machine learning uh if you have some particular instance you can ask me offline but uh because whatever there is all kind of things written in the literature this comes from the structural causal model um now i would like to to try to see what is going beyond machine learning i already have men just mentioned this layer 3 here uh specifically i like to highlight a different family of tasks of inferential tax which falls very naturally causally that is called this cross-layer task cross-layer type of inferences as i i'm saying here layer one is related to saying suppose that as input we have some kind of data here and most of the available data to today is observational it's possibly collected alias numbers here 99 of the data that we have is coming from layer one and elias numbers as well can blame here if someone complains with you but 99 or let's put 90 of the inferences that we're going to do today is about doing or action or layer 3 about uh counterfactuals uh and and about policies treatments and decisions just to cite a few examples then his search question that we are trying to answer here across layers that we have the data and the inference that you want to do is how to use the data collected from observations passively that is layer one come maybe coming from the hospital to answer uh questions about the interventions that is layered true and what conditions uh can we do that now why is this task different uh is uh usually a good question why is the causal problem non-trivial um the the and the the answer is like scm are almost never observed but for a few exceptions such as such as in fields such as fields such as physics sorry uh chemistry and biology uh biology sometimes uh in which the very target there is to learn about this collection of mechanisms in general we do not observed uh that's young in the most of the fields that we are in ai machine learning we are interested uh that there is a human in the loop some type of some type of interactions that we cannot given that we cannot read minds and we don't kind of isolate the environment in some kind of precise way you don't have a controlled environment uh usually you cannot get the same itself still the observation here that the pch still exists the the this collection of mechanisms that underlies the system that we're trying to understand is still there inducing the pch and you could still have the query or the the task the cross layer task how can you get from data from data that is from from a fragment that we have from the asthma you can try to infer about that is layer one observational how can you um answer the question layer two then you have this loop serve phenomena here and you are trying to get fragments that are observed some other factors that are at least realizable how can that could be layer 3 as well how can you move across these layers um i like a lot i use in the class and you spend a little bit of time but i like the allegory of the plato's cave as a metaphor here since um there is this complicated reality usually you just observe the fragments or the shadows of these realities that is kind of fragments of the pch and we like to do an inference about the outside world under what conditions can we do that that's kind of the flavor uh or the consequence of these mechanisms that could be the other layers layer two or layer three for example um i'd like to talk about uh possibility results or the scroll cross layer inferences um uh as usual uh let me read the task here uh in for the cause of quantity y given to x from layer three from observational data that is layer one that's exactly the class that the task that i just showed now the effect of x and y is not identifiable id from the observed data proof there is this collection of mechanism or scms capable of generating the same observed behavior layer one p of x and y while disagreeing with respect to the causal creating to witness uh we will show two models this is mother one this is model two such that they generally they say i'm not part of the mother here it's just for you to go home and think a little bit but there are two simple models here this is xor by the way not x x or u uh there's this schuma these two models generate the same observed this is mother one p one and p two the same observed behavior in layer one however they generate different layer 2 behaviors different layer 2 predictions in this case excuse me in this case layer 2 here is saying that the probability of y given 2 x 1 is equal to half while model 2 is saying that this one then in other words uh we have a kind of uh layer one under determines what can we say about layer two there's not enough information there to move and we can yeah you know that's the result let me i would like to now to make a broader statement or generalize this idea again this is joint work with correa ebay and then the a card from the paper that i mentioned earlier um the the that's prove the following result uh theorem we respectfully a bad measure over uh there's some kind of technical conditions measure over scm the subset in which any pch collapse is measure zero let me let me read the informal version here uh you go home later and you can try to parse that but informally for almost any scm in other words almost any possible environment in which your agent or your system is embedded the pca doesn't collapse in other words the layers of the hierarchy remains distinct in other words you have this hierarchy here there is some kind of this will not happen that one layer usually underdetermines the others there is more knowledge in layer two than in there one alone there is more knowledge in layer three than in layer one and layer two then one layer under determines the other you don't get this type of situation um this caused the open problem i i say that is uh stated in the book of why uh as a corollary you can go to chapter one that says that the answer to answer a question at least i about a certain type of interaction could be layer two about the intervention one needs knowledge at layer i true or above now the the natural question here that you could be asking is like elias then after all how are causal our causal inference is possible or how are causal influence possible uh i get commonly now um um is these uh uh impos does it mean that you shouldn't do causal inference at all given that we have this type of under determination from one layer to a another and the answer is not not at all um the idea here this motivates the following observation if you know a little bit about if you know zero about sem this is the chp the causal hierarchy theorem that i just gave if you know a little bit about the asean um maybe possible and what is this little bit little bit is what we call the structural constraints which can be encoded in a graphical model you have different graphical models here you can have a graphical model that is a layer one graphical model layer two and so on um then in principle uh it could be possible to move across layers depending on how you would call these constraints here new families or graphical models i would like to examine it just for one minute the the graphical mobile layer one here that is very popular um uh such as a bayesian network that is layer one versus a causal based network start of a base net these not all graphical models are created equal this is the same task from the previous theorem it was shown that it is impossible to to move from layer one data to layer two type of statement now what if you have a bayes net that is compatible with the data now this is a base method is compatible with the data x point each y whatever data we get over x y and we like to know what is the p of y layer two quantity why do x in this case um if you play a little bit uh or if you know a little bit of causality there is no unobserved confounder here in this graph then the p of y given to x is equal to p of y given x by ignorability or back door admissibility those are names that you use to say there's an observed confounder now i would like to pick another bn another layer one object flip the arrow now no longer from x to y but it's from y to x and see what would be the positive effect of x and y in this case the y given 2x uh still compatible now it turns out for the semantics of causal inference or of intervention they do you'll be cutting the arrow here that is coming from the ax because are the one controlling the system which give p of y given to x turns out to be equal to p of y then these here highlights that they have different answers equipment there is not enough information about the underlying scm in the bm so so as to allow causal inferences just just say that this is not good even though the constraints could be coming from the axiom is to a layer one object is not good this is not at the end that we are looking now i would like to consider a second object that is a layered true kind of graphical model you go to the paper that you define more formally i'll not do that here but it's possible to encode layer 2 constraints that are coming from the sim the idea of the asymmetry of the causal relations and we like to focus on this one now now the idea is like there are positive instances that we can do cross-layer inferences let's consider a graphical model this is the other two graphical models the mental picture that i will i would like you to construct as the following suppose that this is the space of all calls all structural models here are the models models compatible with the graph g that is the l2 graph graphical model these are the models that scams compatible with pc could generate this observed distribution and here are the models that are the intersection of these guys you have the models that are giving the same y given to x what i'm saying in reality there are situations that for any true structural model encoding these unobserved nature let's call nature n one and two such that they have the same graph g g of n one is equal to g and a two layer two if they generate the same pv the same observed distribution then they will generate the same causal distribution and that's the notion of identifiability it is possible to get in some settings then let me try to summarize what i said so far uh some it's about some sort of tension between the reality and reality that is this true underlying mechanism that you don't have and our model of reality that would be the graphical model for example could be other or end the data we started from our defined world semantically speaking in which an scm a pair f and p of u mechanisms and distribution over the exogenous implying the pch which means these different aspects of their nature and types of behavior layer one layer truly a tree we do acknowledge that the collection of mechanisms are there but inferences are limited given that the scn is almost never observable or observed due to the cht we have this constraint about how to move across the layers now we move towards scenarios in which partial knowledge of the scm is available that is in called such as a causal graph a layer true causal graph causal inference theory then help us to determine whether the causal target the targeted inference is allowed in the prior example the inference is from layer one to layer two namely we are trying to understand if the graph plus the p v that is layer 1 distribution allows us to answer the p of y given to x now observation here sometimes this is not possible i mean for weak models if you have a weak model the mental picture here is like sometimes the true model is generating this green guy here this this distribution there is another model that have the same graph g it can induce the same observation distribution and generate another guide it's called p star of y given to x then my situation that we cannot do the inference about layer three just with l1 data now i would like to spend uh two minutes just doing a summary of the reinforced how does reinforced melanin fit into this picture i spent almost three hours in a tutorial last week in icml uh talking about that go to the crl dot causeway.net if you're in the details i'll try to give you two minutes what happened there this is the pch now my comment here that typical rl is usually confined to layer two or subset of layer two usually you cannot move from from layer one cannot leverage the data that is from layer one or very rarely and this also rl doesn't support us make statement about layer two about compare fractions layer three type of counter fractions then that's the global picture uh this is the kind of uh canonical picture of rl that you have agent uh and i that is embedded in the environment the agent is a collection of parameters the agent observes some kind of context of states commits to an action and then observe a reward uh there's a lot of discussion about the model based of model free i would like to say that all model based that they mentioned today in the literature is not causal model based is a causal it's important to not get confused you can ask me more later now the only difference here from the cause of reinforcement learning perspective that is what they the the that we leverage and i spent three hours it's almost three hours discussing that in the tutorial that now officially the collection of mechanisms that you just started the structural puzzle model will be the model of the environment officially and the the asian side we have this graph g now the two key observations again the environment and the agent will be tied to the pair scm in the environment side environmental side and the causal graph in the agent site will define different types of actions or interactions following the pch which means that observing experimenting and imagining will be these different modes please check the crl.causeway for more details there um let me skip this one here you can check later about uh the different types of tasks that were not acknowledged before um are like this now to move quickly and spend very sevens discussing how does the deep learning field in this picture uh here's the same picture that i have before from the left side observational in the types like 10 slides ago and the right side the interventional world now in reality this this is about reality and model this is every abstraction in reality have a data and you can sample from the data and this allows us to get the hats distribution the p hat and we have some results saying that the distance between the hat distribution and the the original distribution keeps decreasing which makes sense to operate in terms of the hat distribution now for sure you can use some kind of uh formalism to try to learn the head distribution including a deep network or a variation of that now challenge usually you are interested in this inference in the right side and you have zero data points in the right side i'm talking broadly you're not reinforced maloney um the reinforcement let me have other problems but uh you have zero here now how on earth can you learn about the head of this distribution some people is connecting the output of the dnn that you learn from the left side to the right side which is kind of put up a guy like that there is nothing in the data in this data nor in the deep net that takes into account the structural constraints that you discuss and not another nor the cht then it makes no sense to connect there's something missing there then um i could talk maybe uh uh one hour you invite me to talk about the relationship between neural nets and causal inference but i think this is the picture that i want to start the conversation now i'd like to conclude um an apologies for the short time it's like very very short talk but uh and thanks for the opportunity now let me conclude cause of inference and ai are fundamentally interwined now and novel learner learning opportunities emerge when this connection is fully understood most of the impediments for general ai today are orthogonal to the current a causal map is available in and now we're not even touching the problems the impediments for general ai including uh deep learning the huge advances that we have in deep learning and reinforcement learning in practice failure to acknowledge the distinct features of causality almost always lead to poor decision making and superficial type of uh explanations the goal here the agenda that we have been pursuing maybe for almost 10 years now we like to develop we are developing a framework a set of principle algorithms and tools for designing causally sensible ai systems integrating these three pch observational intervention and counter factual modes of reasoning and knowledge and my belief strongly is that these will lead to a natural treatment of human-like explainability given that we are causal machines and rational decision-making um i would like to thank you for for uh listening and also this is john kohler and my collaborators this joint work with the cause of ai lab uh at columbia and collaborators uh thank you juan sangat john c yuda andrew delegate thomas um and all the others that are this is a huge effort thanks and i will be glad to take questions hello everyone i'm chung from microsoft to research cambridge uk so today i'm going to talk about a causal view on robustness of neural networks so deep learning has been very successful in many applications however it's also vulnerable so let's take our favorite amnesty classification for example so deep learning can achieve it with 99 accuracy this is impressive however if we just shift image a little bit not much the safety range won't be more than 10 so the accuracy will drop to around 85 percent which is already not satisfying for an application and if we enlarge the shifting range to 20 the accuracy will drop to half which is not acceptable anymore the plot shows that the more we shift and the more the the less the performance this is not desired especially with minor shifts okay now we would like to be robust let's add vertical shifted image in the training set as well this is adversary training setting i have added an image that shipped up to 50 image in the training that so that you can see that the performance is much better with about 95 accuracy even when we shift up to 50 but do we solve the problem now what if i didn't know that it would be vertical shifting the test and that it would have been horizontal shifts then i will add horizontal shift in the training data then during the testing time we test the image with vertical shift as before the orange line shows the performance with vertical shift it is actually even worse than training with clean data only so adverse retraining does not solve the robustness problem deep learning because it can even harm the robustness to unseen manipulated images and we'll never know all possible attacks this is a real issue of deep learning this is a simple task with handwritten digits how about healthcare or policy making the decision quality is critical but you are very good at this task we can recognize the digit if it has shifted a level or if the background changes because we're very good at causal reasoning we know that shift or background changes does not change the digit number or make a cut to a dot this is a modular property of column reasoning and it is also referred as independent mechanism sometimes so the causal relationship from the previous example can be summarized in this way the final observation is an effect of three types of causes one is the digit number and the other one is the writing style etc and the last one is different manipulations such as shift or rotation the same applies to the animal example where the observation of cat is an effect of a real cat and it's for color etc features and different environments such as different view angles or backgrounds we use y here to denote the target of the task and z denotes the factors that cannot be manipulated and m denotes the factors that can be manipulated manually so m here is the factor that we would like to be robusted to before diving down into the robustness details let's review what is a valid attack we have seen the shifts in those amnesty digits or background change of the cat another common thing way is to add a bit noise as we see here we stressed a very small amount of noise we can pull a deep learning model to classify the panda as a given we can also rotate the image and even add stickers sometimes but it has also been pointed out that adding some small noise can also fool human in this case the left image looks more like a dog than a cat to me the question now is is this still a valid attack what type of change and how much change can we consider that can still form a valid attack we like to define a valid attack from a causal lens let's take the previous example from a causal view we can consider valid attack r generated from an intervention on m together with origin of y and z it produces manipulated data x in general a value attack should not change the underlying y because this is the target this we cannot intervene the target y or parents of y if there is any parents move the y while they are not able to intervene by our definition such as the genetic feature of a cat or writing style of the image itself in this regard recent adversary attacks can be considered as specific types of intervention on them such as the adding noise on manipulating the image in this way a learned predictor is dissaved so the goal of robustness of deep learning is to be robust to both the no manipulation hasn't done no manipulation adverse training can help with no manipulation but it fails unknown manipulation so our question is how to perform a prediction that can be adaptive to the potential unknown manipulations as the shifted digit example in this work we propose a model naming deep causal manipulation argumented model we call deep karma the idea is to create the deep learning model that is consistent with underlying causal process in this work we assume that the causal relationship among the variables of interest are provided the div comma is one of the generative model let's quickly recall the deep generality model variational auto encoder version auto encoder bridges deep learning and probably modeling and it has been very successful in many applications the graphic model is shown on the left from a probabilistic modeling point of view we can write down the model as this which is factorized in the way showing the right hand side of this equation we learned the posterior of j using variational inference in particular we can introduce a variational distribution q and we can try to minimize the divergence between the p and the q we can follow the standard step and form the evidence lower bound we call it elbow and optimize this evidence lower bound to get the posterior estimation different from traditional probabilistic modeling every link in the graphic model on the left are all deep neural networks this becomes an auto encoder where we can try to reconstruct the x with the stochastic column of z this can be learned as a standard deep learning framework with the last using the evidence lower bound we just showed before so karma is also a deep generative model instead of simple factorize like the be on the left our model is factorized in a causal consistent way you can see it in the right hand side the model is consistent with the causal relationship that we saw before that's next let's see how can we do the inference well we only have the clean data set which means like the data set without any augmentation or an adversary examples so in this way from a cover lens this is the same as do m equals clean now we translate it to the d karma model we can use value 0 indicating the clean data thus we set m to be 0 and we can consider it to be observed we only need to infer the latent variable z in this case for variational distribution instead of conditioning only an x as traditional inversion autoencoder in comma we can use x y and m together to define the variational distribution we follow the same procedure and form the evidence lower bound elbow shown below as m is a root note we have do m and the dual calculation can be rewritten as conditioning and in the adversary training way we may have manipulated data in the training set as well in this way we may not know the manipulation with the street m as a latent variable in this case we need to invert both m and z thus we have the q phi the m condition on x and y again we can write the evidence lower bound in this form finally we both clean and manipulate the data in the training set the final last is in a combined form with corresponding loss for the clean data and mutilated data as shown before here d is a clean subset of the training data and d prime is the subset of the data which are manipulated this is an adversary training setting using karma in this way comma can be used either with only clean data and with manipulated data together the final neural network architecture are shown here so the encoder and decoder are shown on the right the decoder network corresponds to the solid error is a graphic model in the left side and then the encoder network corresponds to the dash lines so the inference network can help us to compute the posterior distribution of m and the d in the tesla time we would like the model to be robust to answer manipulation we just would like to learn in the test of time while with the network respon representing the generative process from y and z to x fixed this will fine-tune the network to adapt to the new m and how m influences x so that in this way the network can learn a new unseen manipulation at tesla time the label is not known so we don't know the why in this way we need to marginalize y and optimize the fine tuning last year to adapt to the answering manipulation for prediction we use the base rule to increase the posterior in the white with important sampling as we can see karma is designed in the causal consistent way we can efficiently train the model following similar procedure as variational autoencoder we can also find user model in the tester time to adapt to unseen manipulation and make prediction next let's see the performance not comma first let's use only the clean data as we see in the first slide of this talk we bring the blue curve from the first slide which is the regular deep neural network our model without fine tuning is shown in orange with fine-tuning on the corresponding manipulation in the test time which is showing the green curve in the figure a we can see significant improvement in the performance in the figure a we can also see with fine tuning on a different manipulation in the middle panel the performance does not drop unless unlike the traditional neural network this is thanks to that we fix the mechanism from y and z to x so fine tuning a one type manipulation does not affect the robustness to other type of manipulation which is desired so in the middle panel the fine union was down on the horizontal shift and the testing was on vertical furthermore we use different percentages of test set for fine tuning we see that the more data we have for fine tuning the group the more robust we have for the performance for the angsty manipulation more importantly we can see that with only more than 10 percent of the data we already obtained very good performance which means that the final thing the fine tuning procedure is very data efficient we also tested our method with popular gradient based adversary attacks in particular the first gradient sine method as shown on the left and the project the gradient descent attack method as shown on the right the blue one is a traditional deep learning method which is very vulnerable the orange one is a karma model without fine tuning and the green one is the one with fine tuning we can see that karma with fine tuning is so much more robust to even gradient-based attacks the red line shows the clean test performance after fine tuning which shows that the fine tuning does not deteriorate the clean data performance either so the improvement in the robustness of karma model comparing to the traditional model is significant with gradient-based attacks with adversary training setting we can obtain similar results as with clean data we see before please see our paper for more results i will not repeat here but moreover i would like to say that our method can obtain natural disentanglement due to we model z and m separately and we can even apply do operations to generate counterfactual examples figure a shows some examples that are vertically shifted in the training data after feeding the data we can apply do an operation and set to do m equals 0 and generate new data which is shown on the right hand side you can see that we can shift the image back to the centered location we have showed that kama works well in the image classification case and how does it work for general case for example with many variables and much more complicated called relationships such as the one shown in the future in the picture for example with many with the ring there can be multiple causes and whether the road is wet or not can also be caused by multiple factors for example both whether it is ringing or not or the hydrant is broken or not can we use karma in this case the answer is yes we can have generalize the deep karma in this setting we consider the markov blanket and the variable of interest and we can build a deep generative model that is consistent with a causal relationship with target y we put other variables in the corresponding location which has either ancestor a children x and co-parent c that is consistent with a causal relationship we introduce z and m in the same way where z represents the hidden factors which cannot be intervened and m is a hidden manipulation we also extend the inference and find unique methods in the same way for this generalized department model we will use synthetic data which have complicated causal relationship for the experiment we shift the children's dataset for testing again blue line is the baseline and the orange line is the one without fine tuning and the green one is the one we signed uni we can see that such generalization is significantly more robust and the red wireline shows that has promoted the undercling data after manipulation and we see that the clean data promise remains high even after the model has adapts to ancient manipulation the same holds for gradient-based adversary attacks the attack can be on both children and co-parents which are valid attacks as the target y remains the same comparing the green light and orange line to the baseline which is in blue our method is significantly more robust to gradient-based syntax last you may ask what if we don't have caused relationship as till now we always assume that the causal relationship is given already in general there are many methods for color discovery from observational data and the international data so given a data set you can use different tools to find the causal relationship a good review paper is provided by clark etc last year which summarized different types of positive discovery methods myself also did some research on this topic however just to be honest the color discovery is a challenging problem answers without may not be perfect all the time so what if the causal relationship that we use was not completely correct there may be small errors so here we perform experiments to show it with the synthetic data with many variables the blue line is the baseline and the orange line is the case when the color relationship is perfect so here different color of lines shows different degrees of mystification in the causal relationship in this experiment we have 10 children variable in total and we make them have different degrees of misperceived color relationship the green line shows that two variables are misspecified in the causal relationship and the red line means four variables are misclassified we see that with misdeficient relationship the performance drops comparing to the to the ideal scenario however if you just miss specify a small fraction we can still obtain more robust results comparing to the baseline this it is helpful to consider causal consistent design even we may not have the perfect causal relationship given in the end i would like to summarize my talk i present the causal view on model robustness and the causal inspired deep generality model called deep karma our model is manipulation aware and robustness to ansi manipulation this is efficient with or without manipulated data during training please contact me if you have any questions thank you very much all right so we are back live for the panel session uh i think one of the questions that was asked a lot uh during the chat was this question about model miss specification and model miss specification can happen in two ways one is that while we are thinking about the causal assumptions we may miss something so there could be for example an unobserved confounder and the other way could be that when we build our statistical model we might parameterize it too simply or too complex and so on so maybe this is the question for both susan and elias is uh how do you reconcile with that are there tools that we can use to detect which kind of error is happening or can we somehow give some kind of confidence intervals or guarantees on when when we are worried that such errors may occur so maybe susan yeah you can go first sure that's a great question and it's definitely something that um i worry about a lot about in in different aspects of my work so i think one approach is to exploit additional variation so i guess we should just start from the fact that you know in general in many of these places these models are just identified so there's a theorem that says that you can't detect the presence of the confounder without additional information but sometimes we do have additional information so if you have like you know multiple experiments for example um that you can exploit that additional information um and so in in one of my papers we do an exercise where we try to assess we can look at certain types of violations of our assumptions and see if we can we can accept or reject their presence so for example one thing that we worried about was there might be an upward trend over time and demand for a product that might coincide with an upward trend in prices so we were already using things like um you know weak effects and and throwing out products that had a lot of seasonality but still our functional form might not capture everything and so we we did these exercises called placebo tests where you you put in fake price series that are shifted up or shifted back and then try to assess whether we actually find a treatment effect for that fake price series and then in in the we had 100 different product categories so we could test across those 100 categories and we found basically a uniform distribution of test statistics for the effect of a fake price series which sort of helped helped us convince ourselves that at least like these kind of overall time trends were not a problem but that was that was designed to look at this very specific type of misspecification um and in another setting you know there might not be an exact analog of that another thing that i emphasize in my talk was trying to validate the model using test data which again was only possible because we had lots of price changes in our data and so those types of validation exercises can also kind of let you know whether on the right track because if the you know if you if you've mis-estimated price sensitivities then your predictions about differences in behavior between high and low price sensitivity people the test set won't be right um but broadly this this issue of identification you know the fundamental assumptions for identification and testing them is challenging one of the most common mistakes i see from people from the machine learning community is sort of thinking that oh well i can just test it this way or just test it that way without realizing actually in many cases there's a theorem that says that even infinite data would not allow you to to distinguish things so you you have to start with the humbleness that there are theorems that say that you can't answer some of these questions directly you need assumptions but sometimes that you can you can be clever and at least provide some some data that supports your assumptions so maybe i can come back to the functional forms and let um elias take a crack at the at the first question and then because there's sort of a completely separate answer for the the functional forms i think go ahead oops you're muted there we go oh can you hear me yes okay cool uh thanks susan thanks amit yeah the model is specification machine learning first comment that i would say that is very common that's people trying to use this idea of the training and testing uh sets of paradigm as i usually like to call to to use whatever stretch valley data cause a model or try to validate the cause of query or to verify it it makes no sense in causality as i just talked i just kind of summarized in my talk usually one type of data that we have in the kind of training and testing data is layer one that is observational data and we are trying to make a statement about another distribution that is the experimental one then there is no training or testing the world that one distribution can tell you about the other at least not naively or not in general and this is the first comment the second one i think the the interesting scenario as i mentioned in the chat earlier is about when you are in the reinforcement learning before enforcement learning the observational setting for sure you can try to get the task for implications of your causal model uh condition independence and equality constraints and there are many other types of constraints to try to validate the model i think this will be the principal approach called taskable implications and a lot of the people doing causal influences usually trying to understand what what are these kind of constraints that we usually have uh and then you can submit that to some type of statistical test now moving to the the reinforcement learning that is kind of more active saturn that i think is quite interesting um the we in this setting we're already taking the decision we already kind of randomized on controlling uh in the environment then the very goal of doing that by fischer perhaps 100 years ago was to avoid the unobserved confounding that was what originated the question then enforced money is good for that and uh and then this is and and if you have something wrong um many times i can be super critical about that but many times the effects that what we have wrong will kind of wash away um the i think that's um another nice idea in the enforcement learning setting that we are kind of pursuing but i think is quite nice and other people should think about is how can you use the combination of these different data sets not only to the decision making itself but try to validate the model which parts of the model the model are wrong um and uh there's kind of different types of tasks that are usually uh very unconventional about how to try triangulate these different observational and the different types of experimental distributions in order to detect this the parts of the model that have problems uh my last note here my my last idea is like just do sensitivity analysis uh usually you don't have so many methods you have good ones or initial ones but you don't have so many in particular tailored to the causal inference problem i think it is a very good area they'll have some initial work but i think it's very promising and you're talking about the future or from tears of an hour i think some more people should do sensitivity i pass the ball here to uh admit sure yeah right i think it's it's sort of the fundamental distinction between identification and estimation right and i think uh maybe susan now maybe we can talk about the uh statistical uh specification yeah the functional forms right i mean so in in econometrics we often talk about look at non-parametric identification or and then and look at things like semi-parametric estimation so in you you might think for example in this choice problems i was talking about we had behavioral assumptions the consumers were maximizing utility we had identification assumptions which basically say that whether the consumer arrived at the store just before just after the price change was as good as random and so the the price was within a period of two days was randomly assigned to the consumer so that's kind of the identification assumption and then there's a functional form assumption which is the type one extreme value which allows you to use this multinomial logit formulation that functional form assumption is incredibly convenient because it tells you that say if one product goes out of stock i can predict how you're going to redistribute your purchases across substitute products and so it's going to allow you to make these counter factual predictions and it's very efficient and if i change one if i have one price sensitivity it's going to tell me i can learn that on one product and i can apply it to other products as well so those types of things are incredibly efficient and they've been shown to be incredibly useful for for studying consumer choice behavior over many decades but you know there are still functional form assumptions and so there are also theorems that say actually in principle you could identify choice behavior even if you don't assume the type one extreme value don't assume this the slogit formulation um but then you need a lot of variation in prices like in order to trace out what the distribution of your errors really are and and to fully uncover the joint distribution of all of your all of the shocks to your preferences you would need lots of price variation in lots of products over a long period of time so theoretically you can learn everything without the functional form assumptions but in practice it's not practical and so you you're always going to be relying on some functional form assumptions in practice even though theoretically you can identify everything non-parametrically um with enough price variation so then again comes the sensitivity analysis you you want to check whether you know your results are sensitive to these um to the various assumptions that you've made and that becomes more of a standard exercise but i think it's really helpful to frame the exercise by first saying is it even possible to answer this question and what would you need and many problems are impossible and just as elias was saying like you know if you have a confounder in your training set you're also going to have a confounder in your test set and just splitting trust and train you know doesn't solve anything so you you have to have a theoretical reason why you think that you're going to be able to answer your question makes sense yeah in fact i have a similar question for cheng as well in the sense that it'll be great if we have a training method that is robust to all adversarial attacks but obviously that will be difficult right so there are some assumptions you're making in the structure of your causal model itself in the deep camera method uh so my question to you is of course how sensitive it is and what kinds of attacks can be would your model be robust to but i'll also throw a more ambitious question like is it possible to formally define the class of attacks on which a causal model may be robust to so i think like the key here is like how can we formulate attack in a new causal way so i think like for some attack it's very easy to formulate in the causal way for example this is shifting it is a manipulation it's just another cause for the fact you're observing but for some attack it is a little bit more tricky to formulate it in the causal way for example the gradient based uh white box attack right so it is like in the causal setting and especially with this multiple step gradient based uh attacks so i think then it goes to like over time with cycle causal model as an underlying model so i think if you can formulate it properly as a causal model and design a model that is consistent and then we can be robust to the attack but not all cases are so easy or like it can be technical challenges when there's uh cycles over time uh for certain type of attacks um so i think uh in general or it's just always good to consider more causality but how difficult and how much assumption you need to make on you which degree you violate your assumptions i think that depends on the situation you are in yeah that makes sense and maybe i think one question i want to ask and maybe this will be the last question live is so we talked about really interesting applications of causality uh so susan you talked about sort of the classic problem of price sensitivity in economics elias you briefly talked about reinforcement learning and chen you talked about it versus attacks right so these are all interesting ideas that we are seeing i wanted to ask you to look in the future a bit maybe a few years what are the areas or applications where you're more excited about or you think that this amalgamation of causality and machine learning uh is poised to help or may have the biggest impact uh maybe susan you want to go sure that's a that's a good question so um one thing that i'm working on a lot in my lab at stanford is um just personalization of digitally provided services um education and training which of course like all of the partners i'm working with have had you know huge uptake in the kova 19 crisis and so of course you can you can start to attack personalization in digital services without thinking about causality you can build sort of classic recommendation systems without really using a causal framework but as you start to get deeper into this you you realize that you actually can do a fair bit better in some cases by using a causal framework and so first of all you know as it's using reinforcement learning for example is is i i would argue that reinforcement learning is just intrinsically causal i mean you're running experiments basically and but if you're trying to do reinforcement learning in a small data setting um you you do want to use ideas from causal inference and also be very careful about how you're interpreting your data and how you're extrapolating so i think that at this sort of intersection of causal inference and reinforcement learning in smaller data environments um where the statistics are more important worrying about biases that come up in when naive reinforcement learning you're you're creating selection biases and confounding in your own data and if the statistician inside the reinforcement model isn't actually factoring everything in you can make mistakes um and more broadly we're seeing you know a lot of the companies that i'm working with edtech and and training tech are running a lot of randomized experiments um and so we're combining historical observational data with their experiments and so you can learn some parts of the model using the historical observational data and and use that to make the experimentation as well as the analysis of the experimentation more efficient and so i think this this whole intersection of combining observational experimental data when you're short on statistical power is another a super interesting area that you know a lot of companies will be thinking about as they try to improve their digital services elias what do you the think emery thanks for the question by the way i was trying to answer you thanks amit the the i think that in terms of application uh applications my my general goal they go in the lab is to build the general more general types of ai i would say uh that is kind of human as people say human friendly everyone is using this name or you have some type of rational decision you can attach this label rational decision-making uh i would like to review these notions or have been doing that for the last maybe uh five years or so uh review what what this could mean because since if you go through books ai books come to enter 30 years ago where all of them are using the same label and they are usually not causal then i would say i i personally don't see and i see any way of doing uh uh general ai or more general types of ios i should say uh without being serious or kind of attacking causal influence front and center and i can count in my hands that today how many people are doing it and i cannot count the number of people that is excited which is very good i'm excited about the lack of excitement at the moment um the the then my suggestions like just don't go around it just trying to understand what is a causal model what cause all it is about and then just do it right it's there is a little bit of a learning curve but i don't i think this is the critical path uh if you want to do ai or more general types of ais they're the two obvious applications that uh that also we have been working and then just go through our website cause ii.net but cause of enforcement learning as you mentioned uh we are chatting before in the internal chat here i just gave almost three hours tutorial in icml that is trying to explain my my vision or what how i see this intersection of causality and impossible learning uh then check it out crl uh uh causal impersonal learning and the cause of all the the notions of explainability and fairness and ethics again there are many papers and works that are not attacking causality or saying that causality is hard or it's difficult to get a causal model so one it's inevitable in some way then there is no point in postponing uh if you go to the court or if you talk with human beings uh usually causality is required uh in in the law or in legal circles and in humans we are causal machines then there is no way of going around uh then i would just like to see more work more people working on it including microsoft for sure microsoft was the leader by the way in the base in the basin act in the early 90s uh revolution that takes the 90s until early to 2000s i think which push a lot the limit and labs to many of the events that we have today including via variation out and coders and so on uh still i'd like to see much bolder steps for microsoft eric stack because due to eric harvitz and david hackerman those are the two leaders that they understood very well they are the developers of the the theory even of course of graphical models of base net in the late 80s and they pushed that in such a good way uh now i'm not advertising from the base gnats and that's completely different than the cause of graphical models then this is this expectation for microsoft on my side and i think huge potential uh but anyhow that's the idea thank you alice and chang what sort of domains or applications you're most excited about yeah i i would like to second suzanne and uh elias i think these are all interesting directions uh i see like a great importance in considering causality in all current machine learning all other directions because i mean for for deep learning reinforcement learning like a fairness i think i really like your work i mean as well on the privacy robustness generalization you know a lot of current uh problem in machine learning is like if we actually consider quality i really see that the last magic ingredient to solve a lot of this drawbacks in the current machine learning model but i would like to bring another angle is like if you think about causality and the other direction of machine learning i think in past years i mean in recent years a lot of gap has been bridged but i think in early days it's what i see it's a little bit more separated so i would like to to second that like uh from a causal chair i think a lot of more modern machine learning technique can also improve just the causal discovery itself because traditionally we care about like you know how these serums prove identifiability and and all these things and commonly we limit ourselves to a simpler functional form i think in recent years there's a little bit more advances like a non-part microform and other things but i also do see a lot of advances in model machine learning techniques to help with cost discovery for example with a lot of non-linear ica work from apple recently you know it actually bridges non-linear ica and vae and liberal self-supervisor learning time series can also help with causal discovery from observational data and i also see this is a great trend i mean like for example even from bernard some recent work like how you use like active learning element-based active learning for causal discovery so i actually see not only cause quality too as a machine learning but also see a great potential for other machine learning learning methods to call that itself great uh on that road that's a wrap uh thank you again all the speakers for taking your time and attending this session and of course thank you all to the audience for coming to the frontiers in ml event uh we'll start again tomorrow at 9 00 a.m pacific and we'll have a session on machine learning reliability and robustness thank you all thank you thank you thank you see you all thank you you 