 you [Music] okay welcome boxful while folks are still too clinging after lunch here let's begin so that we're not too late for the day's program I'd like to kick off the post lunch session by introducing my colleague Amit Sharma Amit did his PhD at Cornell and did a postdoc stint at MSR in the US and now he's with us as a colleague at MSR India Amit looks at problems of causality and related areas and he's also interested in a lot of issues related to fairness and and other projects which are of societal impact and I'll and were to omit for more about him and also his fellow session panelists thank you the fish so the last session was a good segue into what we're going to talk about in this session in the last session we talked about what kinds of different challenges new technology brings and today let me give you some examples of what machine learning systems nei systems can do so this slide actually from Emory who's also a panelist and you can see here that you can do a lot of different things with these modern systems you can for example design an app which can help a child learn something you can help doctors assess different kinds of diseases you can also sense different kinds of things happening the city from pollution to people walking around and then yet they're also applications in areas you may not think technology is applicable or is being used like agriculture right and so what has happened in the past five or six years are that there are different concerns being raised about the use of these technologies our job automation tends to get a lot of press and it's some of the biggest issues that is discussed there are also in the research community issues about fairness accountability explain ability of these systems and that in fact today there is a conference called fat star which is really a conference about fairness accountability and transparency of systems and in fact is happening right now in Spain but there's also another sense in which you can think about the impact of these systems which is in the sense of privacy see and what we're gonna do today is we touch upon the broader topics of fairness and explain ability but we try to focus on privacy because often this is something not as well sort of studied all talked about when we think of machine learning systems but it's critical right like with all previous technologies that came before us and with these technologies privacy is critical and here's a set of questions that had one talk about now but I'd like you to think about as you see our panelists give talks about the work they're doing you can think about systems leaking data you can think about consent we can think about profiling people and you can also think about what's really changed about machine learning or these new technologies because we had the same privacy issues earlier as well and here's the agenda I'll introduce your next who's going to give us a talk on explain ability and the different kinds of challenges it proposes after that Emre who's from Mike sorry such a I will take the stage and he'll talk about different considerations in privacy and security while building systems and then finally our star from opti Institute is going to talk about the broader legal landscape and how we can think about privacy and the modern systems that we're building and then of course we have a panel discussion and I invite you all to ask questions during the panel discussion so we have about 20 minutes each for each talk after each talk we'll have maybe one or two quick clarifying questions if you will and then what we'll hope is that during the panel discussion we get into a much broader debate about these issues with that I introduce a ire SiC professor at National University of Singapore and I'm glad that he's here and he's gonna share his work with us Thank You Amit for the introduction and hang on we need to do a little bit of clicking come on okay hi everybody so um I'm very excited to be here so this is my first time to India in particular my first time to go I and it has been really really lovely and interesting in the summit so far has been really exciting and it's really great to see all the people here I'm going to talk a little bit about my work on formally analyzing model explanations and of course I haven't done this alone so this is the list of people who have I've worked with on these topics so I meet a new partner Yale were for my time in Carnegie Mellon University is doing a meal on shark of course and there's a-- and mountain work that we worked on this we're working on it right now and most of the work happens and today and kuba has been my research assistant and we worked on this about a year ago alright so and I'm going to take this opportunity to also shamelessly plug in my my other work so I generally work so I come from a background of game theory mathematics so all of my formal training is in math I am NOT a computer scientist I don't know why you keep inviting me to these places so and but generally what I work on is designing it very broadly speaking socially responsible algorithms so I worked extensively on problems of how algorithms can be used to ensure fair treatment of individuals or groups in various domains and group fairness a cooperative games ramp division I've worked on a problem of ensuring a diverse outcomes of algorithms specifically when you talk about a influence maximization and in housing allocation and what today's talk is going to be about which is about transparency so although we make sure that algorithms that are generally black boxes that are very complicated and difficult to understand for various reasons how do we make sure that they the decisions are better understood by others all right why we all here and why why do we even care about algorithmic explain ability well I suppose that one answer is that it seems that everyone else does right so there has been a lot of media attention about this topic right so what I like to do here is put up a bunch of headlines that have been grabbed from the past few months so this gets updated every time I give a evaluation of this talk and you can see right you know a clear view the facial-recognition is soft or getting sued over facial recognition privacy concerns ethically i for companies we don't understand youtube healthcare algorithms are biased and so on this one is actually not that new but I just really like that the the title what does an 8 a IFE sees to do I'm considering putting it up in a little post up on my door or something so I can you know remind myself or ask myself every day I don't know so and Beyond media attention there has been real a attention from government bodies so the previous session talked extensively about that in India I'm at the National University of Singapore in Singapore too there is a lot of interest in AI and governance and the intersections of the two and there have been some concrete steps made towards legislating regulating thinking about regulating AI so just two notable examples one is the EU GDP our that I'm sure some of you at the very least heard about so very broadly speaking it just talks about how what are people's rights with respect to AI machine learning technology the part that I care about for this talk is what's called the right to explanation okay so people have a right to have the decisions of automated decision makers explain to them okay they have that fundamental right and also for me the u.s. just in June 2019 they came up with a new national ail in this no teaching plan will they also talk about the transparency at length and as a meet mentioned as well the research community has not been a slow on the uptake here and we also see a extensive interest in fat topics for fairness accountability transparency in machine learning in AI so I want to say all but I'm probably wrong but let's say most AI machine learning conferences that I am aware of have at least one or two tracks dedicated to this topic right so just series of recent works on these topics of a fairness accountability transparency there are dedicated conferences of fat star was mentioned also a AI in ethics and the society which also runs annually usually in concurrence with a co-occurring with Tripoli I so how do I look at this problem what do we think about this problem I'm I'm going to take I'm a computer scientist I guess even though I'm a mathematician and adult I'm at the department of computer science and we tend to look at problems in a very abstract way right so we try to boil away all of the stuff that well we think doesn't matter and try to stay with a very very simple problem that then we can analyze right so you will forgive me if there is a little bit of a simplification in what comes all right but I try to at least make sure that we boil away then the stuff that really doesn't matter or at least matters less okay so for the purpose of this talk we're going to treat all of the algorithms that we deal with as algorithms that are black boxes that means that we do not have any access to what they do inside ok that means that could be either because we just don't have legal access to them right there may be proprietary knowledge by Google and Facebook Microsoft and so on okay all it's because even if you do have knowledge of them then they're so complicated and so a weirder like they they have some underlying neural network some complicated decision tree some other model inside that you don't really understand and what's worse is usually these systems are learning right so they adapt themselves according to what data they have seen which makes it even more difficult to understand even if you understand the initial parameters to understand what goes on after is very very difficult right by the way I'm not going to be upset if I don't finish all my slides okay so I'm going to deviate a little bit from what Amit says if you have any questions please ask okay okay good excellent good okay so the algorithms were talking about here are just algorithms that are very simple we take his input something let's say for now I use L okay when I say they take it as input they just take a bunch of features right so your age number of bank accounts you have amount of money in each one number of credit cards and so on and so forth they plug it into this algorithm okay this algorithm was some neural network it was trained whatever I don't care okay for the purpose of this talk is just a black box and that algorithm processes the input and outputs some decision yes or no okay what we want to do is we want to come up with an explanation for this algorithm and for the purpose of this talk we're going to focus on a very specific type of model explanation which is feature based okay what does that mean it means that their model explanation that we want to have is one that outputs a number for each one of the features that the user has the user has as this is not a word and each one of the features that the user has and that number should roughly correspond with a degree of responsibility that that feature has for the decision that they all will be made okay why do we want that well for a variety of reasons okay so when you talk about the fact ecosystem the entire thing right it seems that one of the baseline requirements that we need is some understanding of the complex systems that we use okay without that a lot of other things become very very difficult the people that I started working with this on are a new partner who are not I mean at the time they didn't even call themselves algorithmic transparency people they just call themselves they were formal verification people they thought about what we were doing is a way of debugging code okay so the algorithm is misbehaving in some way it's biased it's outputting wrong stuff we want to understand why okay so that's how we came at it in the beginning at least okay to me it ties into a little bit more of a baseline requirement of trust okay a lot of the reason that we have so much concerns about these systems is that we do not trust them okay we just don't trust them you go to a banker okay you go to a bank and do a new you talk to a human at the car at the desk at the office whatever and you sit at their office and they tell you Luke yeah you didn't get their loan I'm sorry okay I as a person believe that they're they've done their due diligence right they have a little certificate on the wall that says that they have a degree in economics they have a little calling card on the table they have a little widget there and stuff like that all of these are signals that society has trained me to agree that these are people that are qualified to make loan decisions right as opposed to I'll go with them as well let's say it's a little strange still right and it's completely opaque right I mean not to say that I know what's going on in the the person's a head right I mean it's not like I know that the clerk that denied me the loan didn't do it maybe they don't like my face that's entirely possible okay I don't know but at least I am inclined to believe Society assigned me to believe that this is okay okay so what we want to do is to make sure that we build trust between algorithms and humans okay so now little bit more formally what do we have we get an data set okay so most model explanations techniques we rely on having some kind of a data set that underlies what's going on here so think about this as a bunch of users that have previously applied for a loan and these users have already been labeled by the bank as like yes or no decisions and labeled by some classifier okay and our job is to assign a value Phii to every one of the features so I give you a point X like P I apply for long assign a number to each one of my features each one of my attributes that corresponds to our important that attribute was in determining my long mile my outcome okay so that's me that's the user X this is the data set this is the labels on the data set so I don't get access to the classifier itself I don't necessarily get to ask it even questions right I just get to see what the labels were okay and then I'm putting a little layer you know void style you know like access to maybe other stuff okay so maybe other valuables okay so really depends I mean whatever the paper you read you'll see do they make different assumptions on what kind of extra access we get okay and the output usually looks something like that if you visualize it so if we have data points that have only three features right so red green and blue so you look at something that looks like that okay so for x1 the most important parameter in the decision was the red feature then the green feature and the blue one wasn't so important okay but we also allow negative values okay so for the second point you have that the red as a negative value that just means that I'm kind of surprised to see this so think for example for me my loan was denied but I have a lot of money in my checking account okay so usually people that have a lot of money in the checking account they get the loan I don't I didn't get it so for me it's a negative value okay make sense and also note that of course two people can I get two different explanations for what happened with him white four and that's fine we're not looking to generate necessarily global explanations with generating explanations that are personalized to the person that requested them okay so this line of work was certainly not invented by me and is certainly well explode in the literature so there are generally a lot of them around okay so if you look at the literature this is a very coarse partition that I'm not even sure about myself but I figured I'll do something okay so there are gradient based ones which basically look at the classifier at a point they take the derivative of the classifier at that point assuming you can do that but hey and then you release that to the user as an explanation there are some Valiants of it but that's generally what you do then there are optimization based ones they basically say okay well the explanation needs to minimize some loss function which could be the best one around based on some preset a loss function will release that okay and so here we have stuff like salience C maps and deeply deeply IFFT and the pattern attribution pattern net and so on here we have stuff like lime for those of you are familiar with model explanations counterfactual ones are just ones that say well okay you yeah you didn't get the loan the vector that I will release will be just how how do you need to change its features in order to get to the right outcome okay and then there is like code based ones okay which basically released a point that explains the decision now I will approach our approach is when we look at it is kind of what we call a normative approach okay so we say okay instead of like setting okay we want salience in the app so you want line we want something like that we say what properties what makes these explanations good and then we go from there okay and then we try to derive kind of cut away a functional classes that don't satisfy them and hopefully arrive at one model explanations that satisfies all of these nice properties and I think that approach makes a lot of sense because then when you come to stakeholders to people who are not necessarily computer scientists and you ask them well okay what a you know do you do you think this is a good idea you have really easy ways of telling it because usually the properties we have our stuff like symmetry continuity like now features like new features that don't do anything get no importance and stuff like that that's really easy for people to parse okay as opposed to the mere measures themselves that are usually very very complicated now when you talk about model explanations what you want is you want to balance broadly three things okay you want to balance explanation quality you want to make sure you're releasing stuff that's good it makes sense okay you want to make sure that the explanations are fair and I mean you can come up with a few definitions I wrote something like but but but the ones that I'm going to stick to are things that say okay well your attention should be good independent of a which subgroup of the population you belong to okay no matter where you are in the data landscape then you need to get good explanations the other thing is that it needs to be good no matter what class you have like whether you got accepted or rejected alone you should get equally good explanations okay and another parameter that we recently started exploring is privacy okay because model explanations at their essence what they do is they're in the released information right and if you release information where all of the privacy experts in this room will tell you that there's an avenue for an attack they're right people can exploit these model explanations in order to recover sensitive data okay so you need to kind of balance out all of these things right so the general idea is that okay if the classic machine learning model looks something like that now suddenly you're adding another Avenue okay I'm releasing this model explanation then somebody can come along and exploit it in order to reveal a private information or basically the ones that we're looking at now is called membership inference were you trying to identify members of this group okay now I'm not just just for brevity I'm not going to go over too much but basically what this graph shows this is kind of a what it shows it shows a something interesting right once I can okay so basically we looked at Guardian based model explanations and what we can show is that if you show people the gradient if you show an attacker the gradient they can get to be much much better they can get very very effective attacks as if not showing it to them okay of course there are some things that are better like for example just releasing to an attacker the total loss function like the the thing that you use to train your model but that's usually not a realistic baseline and what you can see is that they've fixed it so 0.5 is basically I'm guessing whether you're in the training set okay so I'm flipping a coin okay so higher is better and you can see that as you over fit your models more and more then you get to be more and more effective okay now just again I want to so why does that work generally because the gradients the model explanations are very very good signals of model uncertainty about the prediction okay so there are good signal of valiance like I'm not sure like this is a person who should get alone but I'm not entirely sure so if you get that signal then yeah it's very bad okay so we can show actually that that's actually what you want and and some smoothing approaches to gradients basically randomizing them actually help in addition to that we also looked at making differentially private poor model explanations okay so basically trying to avoid the bad stuff so this map this is like two papers one of them bad news one of them good news okay and here we basically say okay every time you listen you ask for model explanation it's got to be differentially private it is to protect user data but more than that if you do it for long enough you can't because the point is like think about it you're releasing a drop of public of private information every time you do it but if you get enough of these drops you eventually can do something bad right oh I'm getting mixed signals here so I meet at five minutes but then I got the one in no time is the wait which one is correct okay so I'm going to go with you because that gives me five minutes mom so now so I just tell you a little bit about this part right so because that part I kind of skimmed over and that's important okay so as I said right so what what's the kind of pipeline of private what are we trying to protect we're trying to produce protect users right you don't want it to be the case oh you have relinquished your role and left in shame oh sorry so so so we have the training that we want to protect that right but we also have the data the day model explanation itself either generates all uses it's often enough they offered enough these two data sets are the same thing okay the data that fuel to generate the model explorations is actually the training data okay in a lot of the implementations okay but you gotta be careful because when you're releasing a model explanations you can do a lot of stuff so other people for example showed that you can record do a modern reconstruction attack basically learn the model from observing the model explanation we show that you can get the training data itself okay to varying degrees of the efficacy okay I need what we're trying to do is we want to make sure that our explanations are both safe in terms of protecting user privacy and sound okay so sound in the sense that they make sense because the safest model explanation is none at all right just don't release anything just released zero right why didn't you get the lawn I don't know talk to someone else okay that's easy but you want to make sure that they are approximately good okay so briefly how we got that is by a using differentially private Valiant of gradient descent okay so what we said is okay well don't don't worry too much about what this says okay but this is basically some kind of a loss function okay that tries to minimize whatever we're trying to do is you're trying to take okay so if this is the point that you try to explain okay we're going to use some local region around that point in order to explain it and we're going to give different weights to different points so points that are closer to you we're going to try to explain to to be more faithful to them points that are further are going to be less okay and then this is controlled by this alpha function okay and what we can show is that is that alpha this weighted function is not is putting too much weight on faraway points it can be very bad okay what it needs to look like honestly it needs to look kind of like a volcano okay we're in some region heal it's flat ok and then it kind of does this quadratic decay okay from that point onwards and if that happens then if that happens then a this loss function becomes what we call have low sensitivity okay which means that it doesn't really care about what individual points it sees that much okay very very broadly speaking okay so this what we want to find is we want to find our model explanations that minimize this loss function okay and then we did a bunch of approaches to this the first one was just a very simple baseline one okay so learn this learn find a model explanation that minimizes that loss function using differentially private gradient descent okay and so gradient descent is basically an optimization technique that we're doing in a way that doesn't leak user information okay and and this works okay so this works that's not difficult but it's kind of but not that bad okay so but the point is that still were dripping out information every single time we were releasing okay an explanation we're still dipping out a little bit of information that's very bad because if you have an attacker that gets to see a bunch of them that they can take all of that a guide we get information and maybe use it so what we did is we said okay well we're releasing this information right each one of these model explanations is an information release right so if we have all of these information releases that for us is effectively done we can adaptively use them to do too we can reuse them we usually do C cycle okay and that actually give buys us a lot okay so now I'm officially out of time so I'm going to stop but basically this graph shows you that we can do a lot more with adaptive approaches and even after we're all done we can still get approximately accurate stuff a for effectively free if we generated enough of these okay mmm and now I will stop I would just say that there are really interesting trade-offs here between quality of explanations differential of privacy and group fairness so I didn't really talk about the fairness part here but it does exist right so if what we show other parts of this paper is that different groups get affected differently and basically spouse regions of the data set get hurt the most okay so think like minorities okay so if you have stuff like that in your data set then they are particularly vulnerable to a either getting bad explanations bad predictions or their data leaked you gotta kind of choose which bad outcome you want right thank you very much for your time if you have any questions we can take them now or offline thank you should we take one question I will just move to the next speaker okay well embrace except maybe we have time for one question yeah there's one at the back right there so you talked about personalized explanation okay right so it's not really for one prediction you're giving one explanation but whom you're giving explanation based on that you're going to be the explanation is that okay I mean yeah who wants explanation basically based on that so it really depends on the application but usually the way that people that at least people look at it is that you reduce them to the user right so the user gets an explanation about what what happened with him so maybe they don't really get to see like this really long vector but maybe you take that vector and you for example release the top five features with the highest score oh you you know some variation some function they're off okay because there's a notion that every user has a different level of knowledge and explanation should be based on that so we also are looking into that way to me now do you write I mean so one of the things you want in an explanation is for it to be simple but not too simple right so if that really depends on who your users are but that's I mean but this is kind of going into a whole other field of exploration that is interesting in itself but not entirely the focus here for now we're just assuming we release this vector or some well function of it thank you oh there are diverse aerial attacks soon on the one like image on image dataset you change the image slightly and the outcome of the class is entirely differ how does this thing work like if you give example for the original image you give some example you just slightly modified a bit you can't give the same example right so it's so this is talking about robustness right so this is like you know you take a picture of attacked or you change a bunch of pictures and it becomes an ostrich right so yeah so this is actually in some sense and indirect criticism of counterfactual explanations because the problem is that if you look at the the space of predictions it's usually very convoluted then like all of the like basically most complex machine learning models overfit very dramatically in in various regions if you want to avoid that happening too badly than yeah I mean model explorations would be similarly affected in some yeah we just take questions in the panel thank you yet thank you the microphone next up is mr a cattleman it's my absolute pleasure to introduce em ray he's a reason a principal researcher at Microsoft Research AI it's a long-term collaborator and I've learned a lot just talking to him about causality fairness and different kinds of issues that come up when we use machine learning and ethics so I'm glad that he's here and look forward to his talk hello my name's America Jomon thanks so much I'm it for inviting me I'm really honored to be here in giving this presentation to you today so the the topic I'm going to try and cover some of the new challenges that come up in security privacy when we when we start integrating AI into these systems to drive their behavior this is based this presentation is based on lessons I've learned working with collaborators back at Microsoft Research weighed on even AJ and Jerry and just a heads up they have never seen these slides I did not see these slides until noon today because they didn't exist and so if there are any mistakes they are all mine do not do not go to them and and and please don't go to them alright so the first thing I want to do wanted to do and when I started at this presentation was to tell you obligatory about how wonderful AI and machine learning is but I think we've probably already covered this in hundreds of presentations over the last few years I do want to pull out one key point and this is why privacy and security matters when we in these a are driven systems today and that's because these AI systems are now interacting with in impacting the real world so the things the mistakes of these AI systems make are going to actually matter just as one kind of minor interaction with the real world this slide was actually other than the text was created entirely by powerpoints AI it went out and found the robot image did all the formatting and everything once I had the text that's kind of cool it there is actually it's PowerPoint is getting kind of smart oh so I'm gonna try and cover two different ways that AI is beginning to create challenges for privacy and security basically I'm going to talk about new and surprising ways that an adversary can gain effective control over a system by manipulating their eyes behavior and I'm going to talk about new and surprising ways that an adversary can steal information because of the way that AI changes our intuitions about about the information content inside data so okay good yeah so so I many of you have probably already seen this example where where an image is being manipulated in order to fool an image classification algorithm so we have an original image here and it's being classified as a panda correctly with reasonable confidence and by adding some random noise the image classifier can be fooled into thinking this thing that to a human still looks like a panda is actually a given with very high confidence now this noise to us doesn't mean anything but it's actually manipulating some of the underlying generally generally ignore about patterns that the DNN is paying attention to and by manipulating those it's able to override the kind of the pixels that we see and and make the the image classifier think that this is actually a given so this is a cute example it's unclear when it would really matter maybe your your google photos your online photo album is going to be misclassified if you apply this adverse or noise to it but it's it's perhaps not that big a deal but it turns out that it's not only these types of digital images and these types of adversarial noise that can be that can be manipulated turns out when we're looking and trying to understand what's going on in the real world we can create the physical analogue to these adversarial patches to fool a classifier for example if you're imagine you are in your self-driving car and it's trying to understand the roadsides around you if it sees if we see this stop sign we think it's a stop sign and you might think that maybe these couple of spots here are put on as justcome graffiti that someone has slapped on some stickers or things like that no I these are actually very carefully placed stickers that are intended to fool an image classifier into thinking that this stop sign is actually a speed limit sign don't have to stop keep going maybe you even go faster that obviously is dangerous the reason this is one of the reasons in particular this dangerous is because it's hard for a human to tell that something like that has gone on here so it's harder to detect that this problem has happened yeah now we can go and it's worth pointing out that that a lot of the challenges putting this Alba shell patch into the real world the fact that the that the image classifier is going to see the stop sign from different angles it's going to see it at different distances it's going to see it under different lighting conditions all of those have basically been addressed those have been solved and no matter what angle you look at the stop sign from it's going to look like a speed limit sign to that image classifier it's not only that type of two-dimensional image that's being manipulated that stop sign that flat stop sign it's also 3d printed objects this turtle to an image classifier looks like a rifle it's been specially 3d printed such that there's some my new pattern somewhere that no matter what angle you look at it from if you you know you can rotate it around take another picture the image classifier is going to continue to say that this that this is a rifle so now now you can see that this is really getting quite quite complex there's no way we are going to look at that as a human and say that there's something wrong with this object it's suspicious but you put it through some sort of any sort of system that's going to try to classify and behave effect some change in the real world based on what it thinks this object is and it's going to make a big mistake and obviously there's nothing special about this being a rifle you could have chosen any object you wanted this turtle to look like and it would have been fooled and you can create a different kind of perturbation to fool it into thinking that you can also do this in audio so we it's in 2018 Carlini and Wagner demonstrated that you can take any sort of speech and you can perturb that audio in order to make it still sound like one sentence to the human ear but be interpreted as a completely different text to a to a speech detect system so your your Alexa your Siri is going to is going to say do the wrong thing if it hears audio coming from like a TV show or commercial or radio or something like that now these are all rather sophisticated attacks you have to have some knowledge about what the model looks like you have to have knowledge of DN ends you have to know that you can basically do or how to do over a gradient descent on your input signal in order to create a certain output there's a lot of technical details you have to know and it's actually worth pointing out I think this is really critical that while there's a lot of attention being paid to these types of adversarial patches you don't actually need them in order to gain control of many AI driven systems just a couple days ago NASA at all posted a paper where they demonstrated that you could fool a real a real self-driving car a test loved it was at least notes trying to goal its merry way down a straight lane so the type of self-driving functionality that they already have deployed publicly and what an adversary can do is fly a drone over the car and project onto the road in front of the car any image it was in this case it's projecting a picture of a person and it looks like from the from the cars point of view there's a person standing in front of it in the road it can't tell you know we can kind of tell the something weird and suspicious about this but the car doesn't know enough it's close enough to a person and it'll in it'll break now this might be okay breaking is a relatively safe thing to do maybe if it's it's dangerous you're on a high-speed freeway but in many circumstances it's fine but the you can project anything you want so for example if you the Drunken project bright lane lines that kind of are brighter than the real lane lines and fool the car into thinking that the road is curving to the left and make that car curved over to the left into potentially oncoming traffic and things like that - there was no adversarial examples no math going on here it's just someone being really clever and realizing that the AI doesn't really know what is looking at and it doesn't know the context the structure of what is trying to understand and so simple kind of tricks can can't fool it pretty significantly similar issues come up we'll talk a little bit more about why I think these these happen and maybe some directions and what can be done but similar surprises happen when we start thinking about privacy issues in the context of AI systems it's a lot of this these types of challenges first started being uncovered maybe I guess eight or nine years ago when well in 2013 when Kaczynski I'll published a paper called the private traits and attributes are predictable from digital records of human behavior this is the paper that eventually led to the Cambridge analytic ah I don't know what to call it debacle that's a great word yes there are Cambridge analytical debacle uh that Facebook had and but basically what it boils down to is that human behavior creates correlations between everything we do and if you have enough data you can learn a model that learns those correlations and then lets you guess private attributes that people don't want to expose you can guess those private attributes based on the publicly visible out to be a records that people leave behind all over the place so that's a surprise what's happening is that the data that we think is innocuous is actually contains in it the seeds of private data and if we have the ability to combine that innocuous seeming data together with larger models we can extract out and identify the private attributes themselves this can happen in more direct ways perhaps where the information value of a piece of data is not entirely obvious to us so here is for example let's say this is an image classifier or face recognizer or something and we have a DNN that's supposed to analyze this and put it into some sort of embedded space so we go ahead and run this image through the work and we get some sort of vector representation of this image in this space so this is maybe a couple of hundred real numbers seems like it's not a big deal if we wanted to pass it around seems probably pretty private but it turns out that if you run this backwards if you run gradient descent on some random input in order to generate that you get something that kind of looks like the original image it's a little distorted how good this image is actually depends on how good this model is the better your AI model the better the revert the the Ripper's and if you look at what these images really look like so if we go away from the cartoon version a park at all showed that they can take this photo run it through one of these face recognition systems and only when they reverse it they get this kind of fuzzy version of that that face coming right back out so while you know now there's an easy fix to this from a privacy point of view you just have to realize that if you think that this input is private this embedded representation has to be treated just as private as that so if you're analyzing people's text data forms they fill out and running it through any sort of classifier any sort of like nearest neighbor search to run any algorithmic analysis of that original data set you have to make sure that you keep that output as private as you would be original input and there's many more privacy and security vulnerabilities their data poisoning attacks where the people creating your data sets can inject in different kinds of data that can affect the behavior of your system at runtime much later down the road there's a model stealing attacks where you can query your model very frequently in order to identify what your models behavior is and then rebuild your model there's supply chain attacks as we saw as we are getting into this regime of reusing models that other people have built that creates this opening for adversaries to go on manipul those machine learning models and have them basically create these backdoors or other issues in all the downstream context so machine learning is essentially in in these cases breaking the patterns of software behavior and our intuitions of what software should of how software should behave it breaks and I'll go into detail on how I think that machine learning is breaking this software engineering patterns and how it's breaking our intuitions of what's inside information they manipulated by by by machine learning but basically machine learning does the surprising things so a yeah thanks PowerPoint so the machine when I talk about machine learning breaking software engineering patterns what I'm referring to is the fact that one of the critical things that we need out of out of a system is to make sure that its behavior is predictable in order to understand whether it's secure or not and we've developed over decades common software engineering patterns that enforce that predictability so the first thing we do for example is we make sure that any software module checks its inputs and if these inputs are not the type of input that we were expecting as a developer we will throw up an exception and say we don't know what's going to happen if you run the software module against this type of input you know we don't we don't handle null cases we don't handle negative numbers whatever it is and similarly we check the outputs of these modules so if anything does go wrong we're going to try and identify that problem immediately we're not going to let kind of some undefined behavior happen downstream and of course once we've kind of put this input and output gates around the software module we're going to do everything we can to test and analyze the software to make sure that it behaves in a way that we believe is safe now what machine learning does however is that it breaks our intuitions we don't know how to characterize machine learning behavior so it's much harder to to really test it rigorously and systematically and what think about what its behavior depends on it's not just what's inside this box it's also stuff outside the box like training data and you might think well if the train gate is so important for this machine learning algorithms behavior let's just move the true the training data into the box right well it's a little more complicated it turns out the training data doesn't usually go straight into the ml module it goes through a whole pipeline of pre-processing and manipulation before it goes there which means that everything in that pre-processing pipeline ends up contributing to the behavior of this machine learning model as well and so this is what's breaking this kind of complex data dependency that goes through the code of other modules ends up really breaking what we think of as basic kind of software modularity similarly if we're trying to make this machine learning model match some sort of objective or word function then then everything that happens after this is also becomes part of the behavior of this ml model and obviously our input and output checks are much harder to run how would you specify that your face recognition system is only working on real faces how do you specify that your self-driving car is not trying to interpret drone projected images on the street in front of it that's a whole new ML problem in love itself yep and similarly machine learning not only breaks our intuition of software behavior and modularity it also breaks our intuition of information content basically it takes what was an existing problem like basically really duplication attacks was one instance of this and it generalizes it and enables basically much more kind of easy ways for people to to run these attacks so you take some input data and some learned correlations and you can infer private attributes you take your output vector in the model you can get your private input data you take many input output pairs and you can steal the the model that was connecting those two together so what can we do we can I think that there's three things that that we should be doing right now one is that we need to start finding new patterns to circumscribe the the behavior of a machine learning model we might try to train new models to detect known attacks so maybe maybe the drone projected image is important enough to try to especially identify those but that's that's always going to be that's that's going to give us some robustness to all the attacks but obviously there's always going to be an arms race going on we should obviously be looking at more robust machine learning methods and new testing methods but then also trying to think outside the the the tasks of the machine learning module is doing and try to enforce safe behavior kind of at the end-to-end level in the system when we're thinking about I think in addition we should be rethinking these abstractions of software modules and where as these current abstractions have worked for a long time there seems to be something fundamental in how machine learning models work that breaks those modules and maybe that means that we need to rethink how we encode for example the data and implicit code dependencies that we've been able to basically cut out of how we develop software in the past maybe now we need to think about the fact that they do exist and think about what we do to help reason about those situations another important thing I want to highlight as I'm running out of time is that we need to I think also think about how we adapt our metrics for security and privacy for the machine learning error as well while we are doing this research and I think these first two things clearly our research is that we need to we need to write to do do what we can right now to help avoid these problems as well I think the only thing we can do right now is improve the design and development process itself through better education awareness diversity of teams etc and the idea is that even without technical solutions even without new algorithms we can put systemic systemic changes to help avoid many of these issues there's - I want to well there's one I'm going to highlight and that's basically as you're building your system try to include diversity and try to include the stakeholders or who are affected in order to really brainstorm the potential harms and threats that your system could potentially have on its environment and that I think is kind of the beginning of really understanding what then you might need to do in the rest of your system so to summarize these AI systems really are creating what seem like new threats to security and privacy and they're doing this by breaking how we think about the way we have thought about conventional software and I think we do need new kinds of patterns to circumscribe those behaviors and also to help us reason about these new behaviors when we can't circumscribe them as well in the meanwhile while we're doing that right now I think we need to humor improve human processes to fill those gaps even before those technical solutions exist so thank you do you wanna take a question now should be running short of time okay laptop needs be change so maybe one question hi that was that was really an excellent talk and thank you one of the things that in developing software we try to minimize the trusted computing base and it looks to me that with M in ml systems or AI systems in general with training data that trusted computing bases just becomes enormous because it starts including all the the entire data space and this would then you know sort of explain one of the practices we need to change in trying to build in just identifying all your vulnerabilities you just need to be able to say oh I what I just don't have to look at that if we could do that probably you get more predictable machine learning systems I agree with you right now if I think I think that comes down to trying to find ways so make sure that those pre-processing modules that that anything happening before that core ml model is orthogonal in a sense of the behavior of a male model and identifying when that's today is a challenge there are potentially some I think ways like hopeful directions around my causal causality in machine learning and robust ml more generally but I don't think we have a good answer and that's a great that's a great thing to be looking thank you we have a staff a pool from the opti Institute and she's gonna bring a different perspective to questions of privacy and law when it comes to technology systems interesting and enlightening first half this is totally a it's very very different from what you've been hearing a whole morning it is it's a perspective that is coming more from the legal sphere as well as trying to think about what privacy means to these citizens just a quick word on what octi is we're a research firm we sit out of the Microsoft Research Office in Bangalore and we look at the intersection of technology and society so just to jump into it so I mean this is basically what the privacy paradox is in many ways right like everybody knows that their parting with their data for highly tailored and experiences and I also have the same idea as you with your headlines but looking at you know the ways in which parting with your data impacts you it's not just it's not a high you know distant problem you possibly get different prices on uber as a result of you know I'll one of the anecdotes that we've been hearing a lot is that women get charged different prices on uber then men do our Clearview was also mentioned our life insurance obviously we know that how algorithms calculate what your premium is even things like protests right all the antici a protest or the cops are basically using video cameras at any point and similarly in Hong Kong there was a mask ban so that the facial recognition devices all over the city could identify protesters and then interestingly enough there the equifax reach or leak which was one of the largest ones in the world that leaked the data of 147 and financially the 147 million people was one of the largest settlements which is also that there's an increasing bit of thinking around how can you quantify the loss of privacy into a dollar amount and then I think that the people whose data was lost were paid 125 dollars for their financial data so just with that like I'll jump into what the law or how privacy is being imagined right it is the state of being left alone and not watched or interrupted by other people which is a little bit funny in the time where like text messages and notifications interrupt us all the time but there is a translation that it has in clear rights which is there is the right to be left alone that unless there's a just and reasonable and fair law you have the right to be left alone it extends to the attachments of the individual such as property privacy of body privacy of identity there's informational control which is that in individuals half the control over their information lightly private information but also it can extend to collectives such as companies in your trade secrets there's a right of self-expression I'll dwell on that a little bit later but it basically enhances autonomy and it's critical for self-expression I may not be the person I am if I did not have the privacy to express myself and finally to decision-making which is that it is important to for us to decide what kinds of decisions we make whether it's in a secret ballot whether it's client vendor relations and one of the things that Cambridge analytic are did was overwrite this decision-making so again like this is just sort of letters no it's being discussed for a long time it finds its expression in the universal right a declaration for human rights in common law it's existed since the 17th century which is a sort of law that the Commonwealth adopts in many ways and of course more recently and possibly the best-known in GDP are where it is embedded across the regulation in lawfulness firms and transparency in purpose limitation in data minimization accuracy storage limitation integrity and confidentiality and accountability so that that that's the sort of I mean privacy has a long history in in the lettered word and increasingly in India as well was mentioned in the session before lunch the IT Act the puter Swamy judgment which gives us all the fundamental right to privacy which is actually quite significant and also it ties it to the two right to life and liberty which is again something that the GAO the Supreme Court has recognized and also affords us the right to be forgotten and more recently there's a Personal Data Protection bill which is a comprehensive set of protections and prohibits processing of personal information without consent but also offers some loopholes to the government in many many ways and and is still to be fleshed out but one thing like I said I wanted to dwell on this comes from the judgment on the right of privacy is the idea of privacy being fundamental to self-expression and I think if we start to think of it as that as a very positive thing that we need to be ourselves and not something that we can just lose as we exchange our data I think that framing is critical to thinking about privacy its intrinsic to the right to life and personal liberty and inherent part of the fundamental freedoms it safeguards individual autonomy and recognizes the ability of the individual to control vital aspects of their life and finally personal choices governor way of life that I intrinsic protects heterogeneity which is also interesting that the court says that privacy is also critical for diverse which is something that doesn't get talked about enough but we also need privacy to be or diverse people and plurality is protected also by privacy so just again and what the law what the bill seeks to do is to set up which is also like how are we going to impose this right what are the regulation all of these regulations are well and good but who's going to actualize it there's a Data Protection Authority that has been suggested which is going to look at you know renouncing the recommendations and also maintaining a database of trust scores the bill just mentions this it needs to be unpacked as to what trust scores are but it possibly means that every fiduciary would have a trust score based on the law and then you know as individuals or users we would be able to see the trust for and decide how we engage with that fiduciary and and directions to fiduciaries to provide data I use fiduciaries in assuming that everybody sort of is familiar with it but in terms of the law there is a fiduciary responsibility which is that there's a duty of care there's a duty of loyalty but from what is interpreted in the Union build right now fiduciaries are just data holders and may not have that extended duty of K but there are lots of questions on the DPA as well which is that how do you enforce something that happens in to court in board rooms and bedrooms across the country how do you enforce something like that from a centralized authority how do you decentralized the enforcement of data protection and data governance what is redressal look like aha which is what I was saying is that is there a quantification of harm does somebody receiving you know an unsolicited photograph or somebody identity getting stolen are those how do you quantify that harm in an open question right now we don't know what the answer is and also how can it be made more accessible because obviously the DPA is situated in India where digital awareness and rights are not as well understood and so how can the DPS and as a body be more accessible to the people but we found that there is a very interesting way of thinking about privacy at a more personal level and beyond the law this is a constitutional thinker Daniel solo maybe some people have heard of it heard of him so he categorizes harm as information collection processing dissemination in invasion and defines it as you know information is obviously gathered about the data subject which is the users is processing is after collection holders store combine manipulate search and use the data information dissemination is transferring of the data and invasion is impingement that occurs directly to be individual now we find this as a very interesting so this font has become very very small but I'll talk through this is an interesting way of like thinking about the types of harm's that occur wants you you know this is offers a taxonomy around around privacy harms that occur once you lose privacy which is in the invasion there's intrusion and decisional interference such as unsolicited telemarketing calls spam junk email but also on the extreme end stalking there's information collection which could be surveillance and Inter interrogation such as viewing private activities without knowledge unsolicited telemarketing also falls in this and inappropriate probing for information information dissemination is Beach of confidentiality exposure or blackmail distortion disclosure increase accessibility and appropriation which sort of manifests itself in various ways such as financial flawed in the case of exposure there's a revenge porn which is blackmail and then also distortion for instance fake news which was the misinformation question from earlier today and then finally in Meishan processing which is aggregation identification exclusion decision interference again and secondary use in obviously profiling and discrimination is one part but then also things like welfare exclusion from welfare as a result of this is also another part of it so just kind of getting into what the citizen perspectives and concerns are on on privacy because we always think about it from what technology can activate in terms of privacy and what the law can regulate but there is very little information unfortunately on what's happening on the ground something that we hear all the time at least in my work is that Indians don't care about privacy because we're a country which believes in sort of community living and we don't care for privacy but that unfortunately is not true and a lot of people think that it's okay to build technologies in India that don't prioritize privacy because we don't care about it but just wanted to again reiterate that there is a very personal cost to privacy which is also felt across the board in our work as talking to people there is a realization that expression self expression as I said there's a relationship with Society which is culture like can i really be myself whether it's in the you know LGBTQI movement or in other parts of in now minority cultures etc can you really express yourself as a community relationship with the state right privacy as a space affords you t does pay to negotiate with the state which can be invasive and then finally with market is consumer benefits which is that if you know how your data is being used and you can negotiate for better consumer rights but also the sense that privacy is fluid right you choose to share your data based on who you know this data that you would share with your family but there's data that you would not share and the same data you would not share with your community similarly there's data that you'd be willing to share with the government which you wouldn't want to share with your family for instance a lot of state governments now require pregnant women to register on their platform so that they can get the benefits such as folic acid and iron pills etc and you have to register in your second trimester but lots of women in smaller communities don't want to declare their pregnancy because they're keeping it from their from their families but that also means that they'll be excluded from governmental support so these are some of the big questions that come up when you start to think about how people decide who to share their data from and what they get at the end of it particularly for women this has become a big question that and also it's problematic because oftentimes there are well there's benefits and welfare at the end of that so there's almost an enforcing mechanism and obviously one understands what the state needs to do to deliver benefits and understand their people better but at the same time privacy is is is is at a loss at that point so just and I know that a lot of these words have been being thrown around and everybody's discussed them from their various perspectives but given this complexity and the fact that users need to decide who they share their data with you know there are a few things that need to be done one is of course informed consent which which which I don't think I mean I guess it's it's just not a salt issue in the context of privacy there are some experiments that are occurring as we speak on behavioral nudges on on on privacy can you what what kind of privacy policy communication works better can you notch beep Olimpo reading these policies but at the same time like all relevant and by the way these informed consent control trust and transparency are are ways in which you know consent is sought when when thinking about medical resource so these are not new concepts it's just that they haven't necessarily been translated into our digital lives so all relevant decisions should be made based on the information that you have you should be able to understand consent there should be certain voluntary nature which I suppose is an idea that's been totally jettisoned in this time and a capacity to give some consent which is obviously very hard to quantify but important to think about when you're thinking about children online there's control user should know what their data is being collected and what for they should be able to correct and amend their data they should be able to revoke consent at any point you know I've never managed to successfully open send on line for anything because I don't know how and I'm a privacy minded person our trust which again is an interesting concept and I'll come to this a little bit later as well but how do you inculcate trust in technology you think here I'd also mentioned it technology should be in the interest and welfare of people it's very hard to communicate that customer preferences should be taken into account products should be accessible and easy to understand and high levels of security in products to prevent leaks and breaches transparencies users should know what the data is being useful but at the heart of this is is a strong grievance redressal system how do you how do you enforce and use this without grievance it's all well and good to recommend it but where does it go who who makes sure that all of this is happening in the technology companies and how we engage with them but fundamentally I think that what we believe in is that the or online needs offline and you need to make sure or we need to make sure that anything that we build is supplemented by a social architecture that sits alongside the technology and works with people to help ease technology into the lives so one is of course awareness which is also mentioned in the morning is critical and and awareness needs to we're trying to do some work about how can we can we build audio and video content can we build ads around privacy and not just like this is your privacy and you should protect it but what are the ways in which you can do that it needs to be action-oriented awareness which is change your privacy settings or go call try and make sure that you know you get on the Do Not Disturb list like very simple things and this is obviously not to push all the responsibility of safeguarding your privacy on to users but also equipping them with any mechanisms that they can do so wait then there's a question of intermediaries which we think is really critical because intermediaries are people who've been translating in the state for people for a long time there are political intermediaries there are social intermediaries there are intermediaries of welfare there needs to be some kind of digital intermediaries as well that can be instrumentalized on the ground to translate some of the aspects of Technology better for people and finally a sense of community because the the data one person is not relevant and also it's impossible for one person to negotiate with technology so how do we build communities around tech so we've done some work in Kerala looking at the digitization of welfare in which are now called direct benefits transfers and we found that both well awareness which was done very well by the state of Kerala they had you know these posters all over to tell people what digitization was coming and how they were supposed to deal with it the second thing they did very well is they deployed these intermediaries who were trusted key informants they were Anganwadi workers teachers etc who were also equipped with this knowledge of ok finders digitization coming your way how you need to deal with it and then finally these communities so the old self-help groups were brought into play to deal with digitization and help people understand how they could you know negotiate both with the government what they would do in in light of of exclusion and that also meant of the government because they were more organized but it was able to deal with them better and wasn't just a case-by-case resolution and we think that all of these those offline architectures can actually be embodied in what is called a datastore this is an intermediary that sits between users data fiduciaries or holders and data users who could be technology companies governments whatever and we think that the data stored can help action allies some of the social architecture as well as the technological architecture on both ends they can create awareness because they are the intermediary they in connect community consent which is becoming increasingly important they can and also important or non-personally that it shows they can negotiate with technology companies for users they can safeguard the interests they can organize users for collective bargaining which is again possibly non like cool thing to say but collective bargaining on data rights has become a really really big you show now and finally communicate value in limits to technology on the technical architecture side and part of my like limitations on this is I mean you can ensure purpose limitation we're working with Saturn locum a Saturn couple to go through some of this and and so try to create some structures that can get can mediate between the users and technology companies on both ends and then finally what it would look like so you know I'm two times up but what it would look like is that you would have a choice of different kinds of intermediaries or trusts or personally the stores that offer your different services and one of the ways in which you could do that is through a legal set of trustees who you delegate your consent to and then they work on your behalf to ensure your privacy with technology companies and then finally finally like I mean I found this very nifty little framework you know this is coming out of the conference in Barcelona on on how to build trust and accountability in the health system which I thought was applicable across which is you can't engage stakeholders you need feedback loops how do you engage both offline and the online you design technology thank you [Applause] you Thanks actually really nice talk I thought especially because you you summarized a pretty field with a lot of opinions Zack and I know that's sort of hard to do but you did a good job with that so I have my question is to you and maybe the panel as they get settled down is what is your opinion about the right to forget right so they're both sides to this correct and there's like you know use your data and you want to sort of like not have for example that data and the web but then from a society perspective that data is actually pretty important as well so where do you where do you personally stand on it and where do you think the community isn't on that so that's a difficult one personally I I mean just if it was me I would like to prioritize my right to forget but I also understand that there's a counter movement and it's true because I mean the societal value of data is so much particularly on you know health nutrition exactly in fact as a movement to ensure mandatory donation posthumously your danger to society and somebody knows about it and your right is to get rid of it but my right is actually yeah so I I don't know what the answer to that is but all the laws of the land afford you the privilege which is the right to be forgotten what GDP are as well as the person leader protection bill how you go about that still needs to be nuanced which is where I think like a lot of these questions of what data can you get forgotten and not forgotten is possibly and I also think it won't be a blanket statement right like you might take some things with you to the grave as well while you donate some bits I think there are some lessons in the way organ donation happens on that because there is an increasing understanding the data is of the body and a lot laws that govern the body are what are going to decide that [Applause] so what I'll do in the interest of time I'll just ask one question to all the panelists and then after that we'll open it up to the floor we saw a number of concerns Ohio you raised concerns about explanations giving up details about people in the model Emily we talked about so many ways to break machine learning model and after you talked about how we can think about when it knowledge is deployed whether it's whether it's serving its purpose of helping and serving the people so there are two questions that come to mind one is that well each technology that's built would have a stated purpose right so there's some benefit for which he build it and then there's also a trade-off with security and privacy and other things right so when you're building the technology what are reasonable ways or tests that we can think of to make sure that we're making the right trade-offs and around the other side also when you're given a technology anyone evaluate it what are good ways to think about that clear out so maybe from your experience or from theory I can start I think like I mean I'm a social scientist so for me a lot of the answers are talking to people I think that as we build technologies I know that a lot of technology companies do market researchers and user experiences but those are highly controlled and not necessarily done in the right way because they are looking to get the right answers for that technology does this product work etc but I think that we need to place these technologies in the political economy think about the structures of power and find I mean there's a lot of there are a lot of methodologies in social science to do good tests on because we do it in public policy right like when we introduce a scheme that the government does we do the test we do the pilots we do and and also do evaluations and feedbacks which I don't think happened I mean it's the whole make it so big that it's in struck table model of tech that I have noticed it is particularly from the government in many ways and that I think is a little bit hazardous so feedback more research I think it's hard to for a tech designer to know what's what's right or wrong and I think that the best kind of practice is to go and talk to stakeholders who really thinks broadly about who benefits and who is harmed and if you know to avoid like the market research style information there's other alternatives you can try and partner with nonprofit groups that try to represent particular viewpoints and perhaps have more expertise in technology at the same time I think but I think you do have to go and do that type of legwork of working with everybody who might be affected by the system maybe to drive that point home I think that I don't think we should be like asked this right I mean I don't think that I don't feel like it's my place to say what is reason I think that we are that people who study these kinds of problems are good at quantifying them so for example we had recent study that we showed that if you want to have diverse outreach so if you have a can outreach program and you want to reach different groups in the population then if you want to have diverse outreach then the social cost of it as opposed to simply ignoring the five different groups and just doing the best you can this the cost to that is like 10 percent right so is is it worth it is 10 percent worth it in order to make sure that you are getting to everyone III don't know I can tell you they will cost you 10 percent right so and that's something that we are good at it and I think that that's something that that yes I mean when you're talking to people who actually deploy these systems and think about them they yes that they should come to us and get these questions because sometimes people implement policies and don't ask them and then you can get really weird and potentially bad outcomes because you didn't think through about like you said okay there is a social cost but it will be low but it turns out that it might be high right so and another example is in public housing our locations we had a project where we talked about how do we allocate public housing and then we show that if you want to do it in a way that is ethnically diverse then the economic cost of doing that is such-and-such and is that a good thing a bad thing well it really depends on how important it is to you to have diverse housing estates hearing is that it's very a social science problem and yeah yeah so my question was about the explanations being offered by the AI system and see instead of looking at why it did it it might be more useful to find how reliable it could have been so if somebody is giving a challenging data point let's say a customer saying why was my loan rejected it might be worth asking what is the nearest data point to you that the system had in its training set and how much error it achieved on that during the final version that went in right so especially when it comes to say minority cases very often if the training is not done correctly you could have huge errors on those red data points yeah and you might be a red data point such that you are nowhere close to anything in the database and that should then be revealed so that perhaps the human being is there as the customer or in the so this provider might itself then say that this is the case where the supervisor overrides the mmm right so I completely agree yes I mean whenever you have problematic data points then one option is to involve humans right that seems like a good idea the issue is I think that a lot of first of all sometimes you don't kind of not know a priori where you know this path data is or especially if it's going to be applied new a new dataset or something like that then you might get into these issues and the second thing is that if you want to if you want to make these things a lot of time okay if you want to do it it's Kelvin maybe pinging humans every single time is not feasible then if you want to kind of correct for that automatically in some way then you will you will get you will get privacy issues again so one of the things also Emily also mentioned a robust machine learning so one ways there are ways to collect for it but right now we are what we are saying at least that we have some preliminary result is that when you do that when you try to wait for it idle by by doing some kind of an especial form a time to make it more robust you are also making it more vulnerable the piracy disk right so this is the equation and I'm not quite sure about it or something right but note that that also again reveals strictly more information to somebody who might not be as benign as you think right so so you're absolutely right that it would be good to reveal that information but my point is that it reveals information right so that's the so that's the that's the key issue raised and that's when we give these explanations it's not just about saying why the model made the decision it did it's often in many scenarios also about what the person can do to get a different decision next time so I mean if we tie it's like a financial decision if it says you know pay back your loans and we'll give you a new one or something that could be a very positive responsible thing to do but if it's a decision that says like well you know you're 40 years old and we would have given you the load if you were 30 it's not a very useful explanation so if you want the explanation to serve that type of role for the person then you have to I think change how you or have that influence on you have you come up with it there's this this idea of actionable because the action of a recourse right so people talk a lot about that exactly so yeah it is it is a very important term when you talk about explanation so some people treat them exactly as that so we're talking about I heard more in the context of students applying for universities right so people they tell you well you didn't get accepted to our ph.d program get a bit of PA GRE maybe get one or two publications or something like that and that will make your life easier yeah for sure so in reality even though everyone agrees in spirit that it's great to have interdisciplinary teams and for social scientists to be working with computer scientists that's not really happening to the extent that is ideal in your experience what is the best model both in research and in engineering and building products where we can leverage the opportunity the limited opportunity that we have for that an interdisciplinary approach well first of all for instance we work out of the Microsoft Research Office which gives us an opportunity to talk to technologists more than other people would but I'm not sure if I have good answers I think that in the site in in though in the journey of designing something I guess we have to find opportunities to work with each other I've also realized that the vocabulary that social scientists and technologists or you know Jews are totally different we've experienced this in many rooms at Microsoft Research way you know conversation often breaks down between the two sets because they also have the emerging market team so I don't know what the answer to that is but I know that it's critical I know that we as an organization are trying to work at it then also you know I've had some openness from or from the people as well I think at least a lot of the larger technology companies are hiring more social scientists so that's that's a step toward making this type of collaboration more systematic people get to learn the technology well February they learn to talk are the same line in terms of why that's not happening if you don't have to feel like there's enough like social scientists with technology expertise to do this more broadly I mean while they were happy painting about machine learning scientists and so salaries enough I think there's lots of ways to attract people to this problem if we think it's important I would say that in in academia there is also that same kind of like getting you know funding towards interdisciplinary centers right so we now have one the u.s. that's basically a merge of people from AI disciplines and from the Faculty of Law and we are meant to sit down together it just started so we haven't really done that yet in other places Nautilus is unique in that one of the things that will say it like if you want to talk about the difficulties is that a lot of a lot of times these collaborations fail for reasons right that different vocabulary and also different constraints right I mean I'm not yet a tenured professor right I want to get tenure kpi's that you evaluate with programmers bio research shows by and if you don't meet them then you know just because you tried something different or try to work with people who are not exactly within your sphere of productivity then it knew you back you pay a price for it right so I think that to some degree like these activities are just not rewarded in the same way that's that's one other job you take one question from Victor and Amelia this question is reasonable so every this is for you you your presentation was very interesting of course I mean you're talking about limits of AI and things so let me ask you something that I worry about you so worried about the data on which your we are creating your models right and it's been shown now I mean people there have been several papers that have shown three corrupt that data then your model is corrupted and can corrupt it in a way that you wouldn't even know that it's corrupted the thing happens you can cause an accident to happen so is there being enough work in that space because that's fairly fundamentally if I wanted to if I was adversary I was a sort of like as it was mentioned earlier countries that are you know not friendly to you I would attack the data more than the models because the models tend to be pretty well protected but the data doesn't seem to be well protected so any thoughts on that I think data poisoning attacks are becoming more serious of being taken more seriously now the initial large data sets that were being used to train image classifiers and things or just images crawl from the web with no no real controls or provenance information there now I think we're seeing companies shifting to using more kind of collected data sets where they know where the provenance is you know they've got they've hired the photographer's to go and take these pictures themselves and so that's providing more control it's hard to do for every scenario there are some things that are difficult to take pictures of once one I think kind of longer-term medium to longer term solution is going to be certification of data sense so I think if we think of like how we certify people who know certain technology test skills today they take exams they we can do the same types of process and knowledge certifications for third parties I think that's one one way to take it into a better position to make sure that people are curating these datasets correctly the second is that there's actually been some interesting findings that at least the simple manipulations that have been used to poison these datasets end up showing up in the internals of the DNA in easy detect to detect ways so if I'm corrupting a whole bunch of images that look very different from one another and adding a little dot in the corner and saying whenever there's a dot here spit out this absurd results and I'm gonna take advantage of that later it turns out the DNN learns that like everything else kind of looks like this and when you see this not take it's completely different part of the network and in order to trigger that absurd result and so that's something that people are trying to find ways to identify inside the network more generally it's unclear whether there might be though more sophisticated attacks that would manifest differently internally so there is still a bit of a question mark in terms of the technical side of these data all the time we have thank our panelists again 