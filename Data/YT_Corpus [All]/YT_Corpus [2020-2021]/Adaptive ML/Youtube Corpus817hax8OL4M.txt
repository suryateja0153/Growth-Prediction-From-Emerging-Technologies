 you [Music] okay so good afternoon everyone so today we have the great invited speaker that I'm delighted to introduce to you so Orhan fraud is a research scientist at Google research working on the sequence modeling problems and applications to machine translation so he has so far published too many papers in top-tier conferences such as the ACO in menapii in Europe c3po AI and so on and I'd like to stress so he is one of the pioneers on the multilingual new machine transition areas at last so he recently built the massive promoted Engel massive MT code and for like hand drink the 103 languages which is super exciting and so all of us welcome your hand thanks for the warm and generous introduction and thanks for coming in it's great to be back here so I was here three years ago I gave another talk with the title explorations on multilingual mission translation it was three years ago I think 16 yeah okay and the title was but I remember titled because I checked the slides explorations on mass of the explorations of multilingual machine translation so over the last three years we've been trying to add two more massive so that's that's the short story but that talk also was given with an author like with a 303 list now we have 30 authors or collaborators spanning so many teams at Google research translate tensorflow you'll see problems that is at scale and it's sometimes it's a systems problem sometimes it's a theoretical problem sometimes in your network problem sometimes the machine translation problem so this is a very multifaceted problem that I'm very excited to talk about and so I'm happy to be a part of the team and the project so there are two two things to a massive that we edit over the last two to three years the first massive is about the number of languages that yet within a single model it's about basically trying to extend the coverage of a of a common machine translation model which is used to translate from one source language into one target language into you if you can think of like machine translation matrix or WMT matrix basically trying to cover the entire all cells in that matrix and increasing our coverage from one to hundred as the first phase I'll talk about what is our next phase and so on that's where we had to Universal translation and the second M is about the massiveness of the neural networks so over the last couple of years it's actually more than three years we are seeing a trend it's it's almost a sure shot if you increase the capacity of your neural network and if you're successful to Train if you if you're successful at training the neural network success seems almost guaranteed there's that there are a lot of criticism about is this really a neural art is it where is the science in it where is the research question but it seems like that trend is holding and another did the second massiveness of the entire project is is basically trying to scale the neural networks by using this trend or by trying to understand what is happening in this trend so it's as we called our project massively multilingual massive machine translation it's a bit of a mouthful but if you say m4 it's I think it feels a little bit more okay alright and this is not working okay I'll go here okay I'll just go over what is our goal motivations I'll I'll describe how did we try how are we attacking to the this problem and I'll try to end with some open problems so our our entire goal our end goal is trying to build a universal translation model so this is a goal that's like a Holy Grail in connectionist sequence modeling or connectionist paradigm since 1950s we are trying to promote eyes the interlingua or trying to parameterize the intermediates medium between languages or sequences or communication and universal translation as the actualization of it how do we define Universal translation it's basically building a single model for all the languages out there and being able to translate between any of these sources languages than any of these target languages so we believe it's the holy grail of new machine translation and whole effort here that you're going to be seeing it's a step forward towards that direction just for as a summary we also give we also summarize the our recent our recent progress in a google glare blog post it's pretty much what I'm I'm gonna be talking about here in my talk so you can find some references and so on in that blog post as well so what is the problem that we are solving what is our motivation I'll be using here I hope I'm not going to block anyone maybe I'll use here ok ok so our first motivation is the improving translation quality but increasing current translation quality across the board not particularly for one language but for all languages so this is the data set that we're using that's a it's a it's a curated data set it's a crawl data set from the web and we kept it around 25 billion examples and it covers its covering hundred three languages that Google Translate supporting it's an I learned that our hundred three is an arbitrary number but that's all the language that we try to keep our the number of languages at some point so we decided 103 and let's use all the data that's available for all these hundred three languages if you look at on the left left hand side of this plot you see high resource language and on the right you see a low resource languages for high resource languages you see the number of examples is actually over billions it's of course noisy but it's it's a lot and as you go this is this is a log scale by the way and as you go as you go towards the low resource end you'll see the example number of available training examples drops dramatically and on the left hand side we can buy training join models like trolling all the tricks that we have we can actually achieve approached human qualities and you can also see that if you have more than 100 million examples you can throw so many tricks you can narrow down the domain etc you can actually reach almost human quality there but that's not the case for low resource languages if you're if the number of examples is less than say 100 1 million it of course depends on the difficulty of the language etc etc but it's a general trend that you see low resource if you have really small amounts of data you're not that good in the translation quality on high resource you're actually doing quite good as a community but our goal is to try to improve everything across the board and in multilingual nmt it's basically a transfer mechanism it's a positive language transfer you can also think of it's helping it's basically what happens if you train a multilingual model you observe it transfer from this high resource languages towards the low resource languages this is known as positive language transfer or positive transfer in machine learning so high resource data or high resource languages or tasks are going to help the low resource or easier tasks but how what is okay so we got this coverage low resource languages we're gonna do good but what are we gonna do on the high resource language the far goal is to build a universal translation system so that's the first low resource languages impure improving low resource is coming from the massively multilingual part of the entire project and improving the high resource is coming from the massiveness of the neural networks so this is a two-pronged attack it's massively multilingual to improve mid to low resource languages it's also the other branch is basically scaling up your neural networks to the to the capacity limits to increase the quality of high resource languages the other thing is ok we kept it at 103 languages but 103 is not the total number of languages in the world here I throw some numbers from what Google Translate right now supports there are 7,000 plus languages Google on the supports under 3 mm I freaking languages Google Translate only eleven and hundred plus Native American languages Google supports zero so actually we really need to build a universal translation model as soon as possible because by the end of this century half of these languages are going to be gone so it's also our kind of do kind of a duty as a research community the third interesting thing is about the recent trend that I taught previously this is about neural networks caring scaling and our new understanding of generalization I don't know the audience the the background of the audience but please let me know I can go into the detail but let's first talk about the amount of data with the error that we get the generalization error how do they how do they interact with each other so if you're also interested you can take a look at the details here in this paper but let's assume there are three regions in the region in the in the current paradigm the first region is the small data region it's basically your basket you're at the best guessed error you're basically a chance level or you can think of it even smaller than iw SL T you're not even able to reduce the increase the blue score or you're not getting anything rather than memorizing everything the second right so in this region you should what you should do you should gather more data right but what about the second region this is the power lo region as you scale as you get more training data you're actually going to reduce the generalization error so this is the we believe that we this is the region that we're at now as the research community but also there's we're gonna hit the wall at the this irreducible error region so even if you get more data you're not gonna be seeing anything better this is because of not the not being able to scale the models up this is more about the sampling error of the training set so data generating distribution is noisy so you cannot reduce the generalization error even further so this is so if you're here what should we do it's actually very simple recipe and it's actually explains what's happening over the last couple of years we just searched for a model and then problem fit ok this model seems good for this problem and then we scale and then it literally improves the generalization error so this is one trend on it's only it's on the amount of data how how does it correlate with the generalization error which ties back to the number of examples that I just mentioned we were using 25 billion examples so we believe we have a long way to go if you actually scale the model sizes up the other trend is about why does generalization error in a decrease or why am i generalizing as I increase the model capacity so this is kind of counterintuitive if you think about the statistical learning theory it suggests that as you increase your if as the approximation error decreases your estimation estimation error will increase so this is what we know in the statistical learning theory but recent trend engine neural networks it suggests as you decrease your approximation error your generalization error will also get better so this is a mind-blowing finding or a lot of people are working on it right now it's tightly coupled with the optimization which we call training error I'm not gonna be going into details but if there's interest I'll be happy to talk after or in the last couple of minutes so that's the motivation why we are actually scaling neural networks and we're expecting some gain and these gains are actually targeted to generalization not memorization so the another motivation is this is a really compelling test but it's you can you could guess from the authors there are 30-something authors 30-something participants it's it's a testament of how compelling of a test bad is this problem if you think about the mess of the multilingual part its we just talked about okay you build the system but how are you building this system it's a it's a it's a big problem mostly studying the multitask learning framework and current approaches or current solutions to multitask learning problems are mostly studied on their meta learning or continual learning paradigms I will talk about what is what these entail and what are the future direction that we can take at the end but that's the first part that's coming that's the interests thing research problems lying there the second interesting research problems about the massiveness of these of these approaches of the other project so you to achieve this mess of multilingual tea you need to also scale up the model capacity and vice versa as you scale the model capacity you need more data where this cow data is gonna become coming from only one language it's probably not because you will have some other problems so you have to increase your coverage to enrich your data but also increase the number of samples in your training set if you remind if you remember the log scale and a generalization error plot so the increasing the model capacity is not is not easy there are so many options it's a very large search space and there is no single way of doing it depending on the architecture depending on a problem things change but there's also another dilemma trainability and expressivity dilemma it's basically the second bullet trainability an optimization just because you scaled your neural networks to trillion trillion parameters it doesn't mean that you will be able to fit on the data or you'll be able to make that model come merge so there's a huge problem going on there it's it's the it's an active research problem in right now in the field and also if you train a trillion weights parameter 2 trillion parameter network will you be able to analyze will you be able to optimize will you be able to serve at the end these are open questions so there are a lot of efficiency improvements if you can think of Network pruning lottery ticket reporters they all fall into the third bullet here so it's a very compelling testbed for machine learning research in general so how did we actually partition this project how did we help make progress over the last three years so we of course probe three different directions first probably not you think we probably on the contrary what you think of what we first probed the depth or scaling up the neural networks because that was the crucial thing and that was the key so that's why we first analyze how we can train very deep neural networks without even having say debugging routines or debugging practices at end so main problem right now in deep learning or in the network research or across the prof Amelie of models the problem is what is if you're gonna plot something to understand what's going in a model which dimensions what should be the the x-axis of the plot and y-axis of the plot so that was that's crucial you should probably find out if the what should I plot to understand what is going on it's not only about okay Locke perplexity is decreasing or my blue score is increasing how can you interpret what is going on in this model by using it these two so we actually devised a couple of monitors or debugging tools and then we managed to scale neural networks new and machine translation models fairly good amounts create a fairly good depths and number of parameters in that paper second we control by the way every time that we are attacking one we're approaching one particular branch we were fixing everything else for example in the first one we're not analyzing multilingual tea everything is controlled data set size the amount of everything is controlled and the second one be controlled now everything about the depth scale and other things other factors we just focused on okay how many languages we can cram into a single neural machine translation model so there we answered that question and we that our answer was we can actually if you kept the amounts of data to 100 for all of us or 1 million for all hundred three languages it's basically a kept version of this plot so we're capping it here so everything like we're using 1 million examples for all languages it's actually getting rid of all these trainability issues and so on or multitasking issues with that we were able to cram 103 languages within a single model but these two combined if you want to combine these two that's if you relax some of the assumptions that you have they commit a lot of trainability challenges think about the data distribution here again sorry about that I'm jumping back and forth talk about the training the training the training process of this using all this data and then you made one epoch over the whole entire set and you're using SGD you're basically subsampling examples within this but it it's in this the within within this mix back what ends up happening you're going to be over sampling high resource languages I watch you're not going to be touching any of the low resource languages so at the end of you you think that you passed you made one epoch over this data set because of the stochastic City what you end up seeing is you over fit on low resource languages and you have a long way to go for the high resource languages or you haven't seen any low resource and any examples on the low resource languages and your models favoring high resource languages so there's a trade-off that you should be actually balancing out that's what we studied in this and in the last paper I will talk about that the details of of this tool in shortly but these three directions combined we call this ok this each one of these directions are promising directions and we can control things and make progress and then we decided ok let's call it m4 after the pilot studies we moved to a realistic scenario removing all the constraints that we have use hundred three languages scaled your neural networks to the chip limits or the hardware limits and also do not restrict the constraint number of examples per language so this is literally m4 in the wild no no rules no nothing and no constraints that was that that was our setup we put one paper about the m4 in the for in the wild that that was more our position paper that we were outlining what are the problems in the field in this prayer in this direction or in this realm and we follow that we followed that position paper with a couple of papers which I'm going to be talking about now addressing each one of these open problems that the address that we laid out in the in the open problems paper but first we had to develop some baselines and we we wanted to be careful because it's gonna be using a lot of resources compute and and headcount and manpower so we did some date so I'm gonna be talking about how we set our baselines and how did we learn given this data imbalance and then at last I'll talk about how are we increase the model capacity our goal in this phase was okay 303 language and retreat language translation model and attain parity with your baselines basically beat your baselines on all these under three languages with a single model so I also catch cold last week sorry about my voice so let's look at the data distribution again where we overlay the bilingual baseline blue scores on top so here blue scores so do these blue scores don't all read it these are just held out set that we believe it's it's a good proxy for generalization but it actually reflects the high resource low resource quality issues so the high resource language is we're reaching good forty-something quality blue score on the low resource actually quality the blue score drops quickly this is also the case for perplexity or how well are you fitting on the data but I just showed here a blue scores so now let's collapse these or the next couple of plots I will be collapsing this 100 300 to bilingual base lines into the 0-0 line and then I'll be only showing negative and positive blue scores to make it easy to digest so what about the model model is actually not super fancy it's a slightly different wiring of transformer big it's a it's a bigger than transformer big our base lines but they're basically using two tricks coming from these two two papers it has different normalization scheme it also has very deep models it has some transparent attention it's basically weighted skipped connections to the encoder but in terms of the architecture itself the sharing paradigm our shake sharing scheme it's extremely important for multi task models and I just sketched the history on the on the multi task models or multilingual models melting and empty models so you can what you can share and what you cannot share basically that's the that's these that's the spectrum here so you can share a fixed link representation across all these source and target languages but that creates a huge bottleneck so you have to basically cram the meaning into a single vector here and decode from that or you can share some sub modules which naturally emerged as a tension module in our nm base sequence of sequence models and you can actually share a single attention network across multiple encoders and multiple decoders but increasing it increases the number of encoders and decoders linearly as in SZ add more languages and in the third case is you basically share everything including very dim bearings whatever you can think of you can share everything that's the basically the Google and the Google Map multilingual nmt system and actually because of the simplicity and in order to reduce the search space to narrow down the search space we go we go with the the last one here and how you guide the language that you want to translate into you either prepend the token or you just learn and embedding for that language and then add that embedding while you decode either on the encoder or decoder site okay this is our initial baseline as I mentioned we collapsed the bilingual baselines into zero line here so anything so zero line there are actual hundred three languages here since our data set one thing I forgot to mention our data set is mostly English centric so examples are for example Spanish to English and English to Spanish so when you train using all this data you end up with a multilingual model that is English centric and you can easily evaluate in an English centric fashion and on all my slides you'll be seeing on the Left English - any of these trends on any of these languages and on the right hand side you always be seeing any two english translation pairs translations directions so what it means these two plots are coming from the same model but we evaluate it either on English - any of the languages in the hundredths remix or any other hundredth remix - English you can also evaluate cross basically a off-diagonal cells in this matrix some sometimes they're zero shots or they're called zero resource but I'm not going to be talking about that here but if you just train picked up the data you pick up data don't scale anything but just train a comparable baseline a comparable model with your baseline using all the data this is what you see let's look at the English - any so it's basically only on on a few languages you see this trend line blue trend line is coming from a single model using the original data distribution it's actually quite bad it's only beating high resource languages for some languages but from both other from those couple of languages it's actually below the bilingual baselines on all the languages that you considered and the story is the same when you're translating into English as well so in the multilingual mods we're expecting to get a great amazing English language model because it's the target site the model is seeing some in English examples but it's not the case so what is going on it's because that you're using original data distribution so if you just change the original data distribution with some smarter sampling strategy it doesn't even actually have to be smarter just some simple heuristic you can use for example multi news versus using some simple temperature by heuristics alta multilingual models they were using some simple heuristics some temperature based some some wouldn't out we actually employed those techniques and it you can get some quite some improvements on the lower source languages so that's that's plotted that the red curve here sorry green curve here so if you see high resource languages are still regressing they're still behind compared to the bilingual baselines but you you start to see the gains of multilingual 'ti which emerges as a boost in the quality of low resource languages this is especially the case when you're translating into English because now you're training a more balanced English language model and you start to see the gain for all the low resource languages here you see up to ten blue score improvement for low resource languages when you're translating from any of these languages into English so that checked one of our goals ok we can improve low resource quality but what is why is it this bad why is it not really getting close to the bilingual baselines so it's because of the you're cramming so many tasks you're basically asking the single model with a fixed amount of capacity to do so many things that at the same time so it is hard so what we're here doing here controlling okay let's gradually reduce the number of languages that we consider in a multilingual model down to 10 within intervals of 25 like 50 we drop it to fifty drop it 25 drop it to ten and Nancy elected to be as as expected as you read remove number of languages in the in the in the whole mix the per capacity per language capacity increases so you get close to your bilingual baseline and in this case your upper bound of capacity is kept by the bilingual baselines so this is just to test is this is there something else going on or not so we validate it okay this is just the capacity issue so then what happens if you drop if you train an individual model so this is we control the number of languages what we did in control okay there seems to be different trends happening when I'm translating into any or when I'm translating into English so what is happening is it also related with the task interference so that's why we trained I saw individual models we picked one model we trained one model only two only from English to any of these languages so this is coming from a one of a single model this coming from a another different model so here we see when we separate when we individually trained two models when we're training English to any it's not actually any better than our massively multilingual hard to say multi way model so there this indicates there is a massive interference problem going on so all the tasks are interfering with each other or there are some other problems happening say in the during the search process maybe maybe we're not guiding our search process and literally beam search properly it's it's basically solving a harder and harder problem at every step so there is research needs to be done here okay what about on the right right hand side this is literally a capacity problem and then interference doesn't seem to be affecting this this this direction as expected so as you drop half of the tasks as you drop a English to any of the tasks you see a delta improvement across the board so it's also indicating us okay for the future direction we can just either remove the if you remove some of the tasks or we can increase the model capacity to attack this the this direction of translations so so at that point scaling what seem to be a sure shot so but it's also a research question how are you gonna scale so there are so many variables that you can play with and they also interact since this is a multi task problem or you're learning so many things all at once go different hyper parameters have different effects of either generalization or memorization of the of the giant Network so the first question that we did ask was which should we go deep or white which one transfer is better or which one generalizes better that was the first question that we asked the first two actually the same sorry about that and we also answered okay can we what we can do about the task interference when you're translating from English to any so those are the questions that we're trying to answer so the first one is scaling up the model capacity so if you're familiar with transformer big I mentioned our base lines are slightly bigger than transformer big so they have around 400 million parameters it's not the case for all languages for low resource languages they're playing with some of the hyper parameters to actually regularize the model and then fit to get to get better generalization but in general here you see our previous model in the light blue this is the 100 400 million transform multilingual model compared with compared against the bilingual basslines it's the y-axis and we trained this was a control study we did okay let's limit the number of parameters in the network and then scale it in two axes these two axes were they were width or depth with a parameter budget of 1.3 billion so I'll give you want 1.3 billion parameter you scale it as you wish and which one is doing better which one is numbers and how is the transfer characteristics of these two two regimes interestingly but also supporting the literature deeper models transfer better very deeper model here you see is the dark blue curve it's as is doing as good as the white model on high resource languages but it's actually transferring very better to the low resource languages so if we can actually say transfer is an indicator of generalization then we can say deeper models generalize better that aligns very well with the current understanding of deep networks what the what white models are doing white models are spending their capacity mostly on memorizing things so if you memorize too much then it's hard for you to generalize as good as the deeper your deeper counterparts so if you have a 1.3 billion parameter budget so it's and if you're solving multiple tasks at once it's better to go deeper then going wider but what is the limit okay if you go deeper it gets better so how can you push the limit of it so how we can actually even go beyond 1.3 billion just by increasing the depth so that's what we so we actually hit the hardware limit we hit at that point the system's limit so we had to stop and develop the systems or the tools to scale even beyond 1.3 billion so we took a pause we we developed G pipe it's a pipeline parallelism framework that allows you to train very big networks which I'll be talking shortly but by using G pipe we trained 128 layers later transformers and it was actually getting better and better of course for some low resource languages the quality was all was saturating already but we owned high resource languages without losing anything on the transfer capability the models was actually getting better and better as we make to make these models deeper ok but that is one axis one dimension of scaling things up how we can also exploit the multitask nature of this problem so there are different architectures or different wirings that you can play with to increase the model capacity dramatically and we chose mixture of experts you can think of mixture of experts as a as a learnt and sampling but everything is happening inside the model so you have multiple experts within the model and you have some routing mechanism and then you have some gather you basically distribute and gather and then you learn how to wait in the individual experts and then you basically come up with with implicit and samples and each of these experts you can play with its capacity by to inflate your model the parameters and by using g pipe sorry by using mixture of experts what we did this mixture of experts the particular implementation that we used was sparsely gated mixture of experts I put the I put the paper link in a couple of I think paper link is here if you're interested take a look at the paper but the implementation of mixture of experts into in the transform it's not trivial how do we do it so transformer consists of consecutive transformer layers each layer consists of self attention and a feed-forward network here what the how we implemented mixture of experts we if you replaced every other feed-forward layer with a mixture of experts layer so here you see for example this is the mixture of expert layer sitting on top of the multi at attention this this is the original the feat work this is the original transform layer this is the mixture of expert transformer layer and we implemented mixture experts at token level so at in the forward pass each token is actually routed to K number of experts and by increasing the number of experts here to the extreme say for example five hundred twelve twelve twelve experts you can scale the model is scale the number of parameters in your model to be beyond ten billion or hundred billion yes so we're replacing few code names here few there's a fetor name you're just replacing it with mixture of expert layers each expert is a transformer feed-forward layer but it has a different combination and it has a different routing and combination subroutine where you send which a fifth Ward Nair layer with those and there's also a dispatch happening at the end of the multi detention it's looking at all these tokens and then okay that's it saying these these tokens should go to that expert these tokens should go to the other expert it's we also sacrifice some of the quality gains for efficiency probably it's better to if you route the entire language or entire sentence you can device language specific experts but if you implement that token level token level experts is their high throughput high device utilization and it also increase the model capacity dramatically Kate's gates are those gates are tiny networks learn the the value of those gates are learned those are tiny networks and they're also looking at the token level yes yeah I will talk about a special specific case of mixture of experts at the end where the the conditioning is on the language index we can also look at the language ID and they're out individual tokens by doing so we first scaled up to 10 billion parameters and then scale 250 billion parameters with an interesting so here by the way I'm only showing two English all languages to English because English to any other language it's still an open problem that I'm going to be touching at the end so if you're willing to contribute on that direction that's very promising direction for research you have 50 billion tunable parameters the picture of expert in giving you that yes yeah but at every step you might be you might be tuning different experts there is also additional cost term which I didn't put the details which is ensuring the balance across each expert so it's trying to maintain a uniform balance across each expert to utilize all the experts basically all the parameters that you put in by doing so as you see 10 million if you just train a model with 10 billion parameters mixture of experts you're already better than all their bilingual baselines so the second goal check but why are we stopping at 10 billion we can also scale it even further this is 50 billion if you wanted to see where is the upper bar everywhere should we go where should we hit very very we should stop because we want to also understand which part of the data regime that we are at are we in the reducible error region what does it say about the transfer and so on so it's not only about improving the quality it's also a research endeavor that we will really want to get the answer so that was the purpose of scaling the models even beyond but here you see lower source transfer since these are mixture of experts they can specialize on high resource languages and if you specialized too much on high resource languages then the transfer is happening less and less so actually as you scale beyond some parameter limit you're not seeing that much transfer happening or to the low resource compared to the others of course so you can also what you can do you can drop half of the tasks to kept the upper bound to basically set the upper bound you can drop half of the tasks which was basically English to any you can drop all those tasks and you can train a single model from any of the any to English and it sets your upper bound in terms of number of here we just plot it if what happens if you this dark green is our quality upper bound with 50 billion parameters and the improvements here I don't trust too much on blue I wouldn't read too much into it but the improvements are even on high resource languages they start from five blue scored so this is a huge improvement if you just if you believe in blue and if you read in blue but we how we optimize the usual look at the perplexity how are we how well are we fitting on the data and you see the same plot which I'll be showing shortly in the perplexity or on the loss that you train your models on okay which one is better deeper models or mixture of experts if I kept if I actually limit the the capacity of between these two if I control the number of parameters between these two which one is gonna get me more gains or what can I learn from these two so here we try to control number of parameters or the throughput here Green you see again mixture of experts that has which has ten building parameters and transformers which is very deep six billion parameters transformer deep this has 256 layers and here the quality gain it's actually supporting what I've said earlier deeper models transfer better in the case of mixture of experts they bring you huge yuto's device utilization so we believe the future is gonna be again at best of the both worlds so we're at the process of mixing these two together the final results this is the the final is a summary of everything that I said but let's look at the scale of things let's take a sidestep and Oscar said what what the heck are we doing what is this madness of scaling neural networks capacity so this is a this is another endeavor towards if in terms of number of synapses in order to reach the human then the numbers that human have and it's arguable you can correlate intelligence with number of synapses or connections in the brain including cerebellum and let's assume it's the case and let's look at where we are right now as a community so this is an empty with attention it had approximately 25 to 50 billion 50 million parameters and the transformer came it had around 400 million parameters it's literally it was a revolutionary architecture but it was around it was even worse than honeybee in terms of number of synapses and the recent advances over the last year these are the models that we were seeing Google t5 it's using 11 billion parameters Nvidia megatronus also 8 billion parameters Microsoft 0 it's I think 1.5 billion open energy PT to one point something and ResNet X they all actually you're not even there to reach to the number of synapses that the mouse has and m4 is also not there yet so we have a long long way to go if we correlate if you keep correlating number of synapses with the intelligence what does it okay at the end of what what what's in it for me if you're asking what we can learn from these models for what all these experiments all these very expensive experiments what we can learn there are two takeaways deep models deeper models generalize better and then second no matter what you do if task interference is a big problem and scaling is not gonna solve that problem here you see all these models all the models that I showed you when translating into X these are this is a this coming from models when coming from multi way models that we only evaluate it translating into X and this seems to be an open problem there is interesting research questions here to be studied to get the gains as we are seeing here so this is also I should admit this is also affected by the particular wiring that they're using I mentioned we chose sharing everything right sharing everything inherently amplifying the interference if you're amplifying interference then it's this problem is that apparently a to heart in a multi task setup to be solved but here since all the languages are translating into the same language they are all benefiting from the amazing English language model or the transfer properties from one language to the other works better in that scenario yes that whenever you actually in a dropped number of languages in this bundle 200 to 10 they are always better than what you get here we actually shared that in the paper the in the wild paper you can take look at the details but yes yeah that's you you should be solving lesser tasks that smaller tasks smaller number of tasks when you're translating into X or you have to devise a different that the different decoder wiring and decoder network to basically you should inject your inductive bias to wire your decoder differently to mitigate the negative transfer so we summarized I talked about so this is the ground this is the position paper that we put all this this lists all the problems what are the problems this paper didn't mean to answer any of these problems so we try to answer the problems here in this in this paper in the following papers which I'll be talking about now I'll go into the details of other papers right now so massive networks I mentioned about the systems aspect of the problem by the way this is the paper that I mentioned this is how we implement it mixture of experts parse the gated mixture of experts and fused it into transformers this was the earlier paper that I talked javi wire our baselines and this now I'm gonna talk be talking about G pipe how we efficiently train these networks so what you're seeing this is general trench we also validated both on different tasks image tasks or vision tasks and also mission translation as you scale the the average improvement is increasing so here number of parameters average blue score it's you see the improvement so how what are the systems that enable this so now I'm gonna be talking a little bit about the systems aspect of the things so these do these models very deep models they're trained on thousand too many for TP version three chips by using G pipe they're also using v-fold sixteen the different floating-point operator they're using Rimet realization or known as gradient check pointing trick we also scale the best sizes to the limit by putting all these things together as we add more and more chips we observe sub linear scaling so let's look how we implemented G pipe so this is a regular deep neural network this is vanilla a neural network nothing is the the nothing is pipelined but you have four different devices and you partition you basically distribute your network into four different devices if you train your model directly in a normal way what you do is you do the forward pass device one device device zero one two or three you compute the loss and then you back propagate button going through all the devices and then you compute the gradients and then you do the update but if you do some pipelining you can also increase this this looks like by the way if you put this into a time axis it looks like this so your first chip the first device is waiting all the way until here to get some signal to get to do some work so all this time here this is called bubble it's the time that you spend now by not utilize your your device and device utilization is huge because you they're expensive so you want to utilize if you are all located if you if you're given an if you're under specific allocation schedule you should utilize your device so we want to minimize the time that the devices are idle here so what we can do we just inspired by the instruction pipelining in operating systems we applied the instruction pipelining in at batch level you can basically let's look at here you have you have four devices and we split our input batches into micro batches and as we process the BA micro batches we are actually sending them to the next device and then we don't have to we don't we decrease the bubble time here and this is pretty much it it's not it's nothing fancy but combined it doesn't the it doesn't solve this bubble problem which we we also try to mitigate it by using gradient checkpointing which means you can put multiple layers in and if in a device say you have ten layers you put ten layers in one device and if you think about how you compute the gradients you have to back wait for the backpropagation deltas to come in and a top on till that time you have to basically wait what we did is we only store this basically I'm talking about checkpoint grades and checkpointing trick we also you only need to store the activations that are at the edge you only store the last layer in the first layer activations and then you recompute all the intermediate activations before when it when they're needed so it also says out of memory and it's also decreases the bubble time by increasing the device utilization okay what does it entail well we here see some sub linear scaling so let's ignore the GPU this is not using env link it's not high speed interconnect but here let's look at the transformer on when we are using 32 micro batches with eight eight eight way split it means we distribute our models on eight devices we're getting 8.6 speed up that's what we meant by something you're scaling that was the systems a spec of things but it's also it enabled us to investigate and understand the deep deep neural network dynamics at scale if you look at so I talked about late neural networks that are thousand players deep on the other end if you go to for example europe's people analyzed single layer neural networks and their properties so we have to come find a common ground middle ground to analyze but to motivate people that are more on the theory side he also did some analysis okay well what we can what these deep networks tell you if you and if you manage to analyze them at scale we actually validated two things deep neural Nets there's an implicit acceleration as you go deeper so these models converge very faster given the same number of examples so they're more parameter efficient this is what we show on the left so you see you reach the same perplexity very faster than your shallower counterpart it's very intriguing it's also validating a couple of things so maybe there is this the over parameterization is also introducing some implicit acceleration on top of your say momentum based optimizers the second thing is the effect of large batch sizes it's it's particularly interesting for machine learning machine translation researchers if you have large enough data set there doesn't seem to be a limit in your bed size but you should be careful here for example we stopped at four million but as we tried eight million is also giving you improvements we just didn't put that here the problem here as we as we increase our bed size to four million eight million you're still seeing improvement but there is some additional problems for example as you include more and more samples that you increase the chance of hitting a bad example so you should be more careful about your sample selection policy or data filtering policy but it's interesting our optimization subroutines are up our to misers seem to be doing good given this low variance gradient so it's actually it was a very good observation that we validated this is pretty much it in terms of the systems and analysis aspect I'll talk about some byproducts or some additional papers so you can also we talked about how are we gonna train these networks now let's talk about okay I'll give you this network what's in it for me we try to do we try to plot the map of languages by using the intermediate representations of this network so we have a data set of three thousand sentences that those ten extensions are given on hundred three languages so there basically n Vapor 103 way parallel test set and what we did we give the hundred three we give all these sentences to the encoder and then we picked up the encoder representations that are at the end and by using some similarity analysis in in this case you use SV CCA which gave us an affinity matrix the partition definite matrix using a spectral clustering algorithm and this is the clustering of all hundred three languages and then we asked our linguists okay what are the families can you draw some languages and can you draw some circles what we found does do they really look like language family do they recover the member of languages and partially in two dimensions with the basic project all the way down to two dimensions there it seems to be the case so at lunch we talked about Turkish and finish if anyone can see finish here I see Turkish is closed authored by Chinese Chinese I'm not sure if it is close but definitely closed is back and Kazakh it's pretty good in the same place although it's weird they have germinated repetitive inside of indo-aryan hmm that might be an artifact of two-dimensional and projection history shared between one aerial but the interesting thing so I'm I don't have much of a Ling NLP background so to me was okay how do we know it's happening in the finer granularity so we looked at some token and bearings and so there's I'm not super clear about how did they plot these by the way I'm sure about the next one though so but to take away here is as you go more finer grained they also resemble the the three of languages apparently but I was curious about one thing so these are highly influenced by the burden bearings at the end of the day we use Werdum bearings and they they apply heuristics like SPM sentence based model or very piece model or by pairing coding what happens if we take two semantically similar languages that we know they're coming from the same family but they are given in different scripts so we actually have such languages for example I think Turkey Turkish and one other language is such two languages and so these two Serbian and Croatian I'm not sure yeah so those two languages actually a lot of languages here that you see at the encoder embedding they come close together at the upper layer the topmost layer of the encoder so this was a very good indicator for us that's telling us okay these encoders this mess of the multilingual encoders can map actually their mapping based on some semantics and if you have a semantic encoder you can use it for so many other things which I'm going to be talking next so if there if you if they can if you can if the your encoder is good at encoding semantics then you can it should be you should be able to do so would cross Dingle downstream transfer as a by-product so what we did we okay we trained all these models what's in it for the other tasks like part of speech or X and a lie like natural language understanding document classification part of speech not from named entity recognition but so and we compared it against the multilingual bird they are not even very comparable in terms of number of parameters but the interesting thing here was we were seeing improvements on different the different variety of tasks and sometimes it's better sometimes it's worse but this came as a by-product of just training a multilingual nmt model and then just reusing it's encoder out of the box surprising for is it bad I think multilingual person love working with just random working with language yeah I find it hard to understand why one inverter to work and actually I thought is it bad is it good I mean if you if you think about what is the utility of it you don't have to any if you can be more practical okay I all just train a thousand language model rather than training a multilingual bird model on thousand languages and I'll use it to translate across all these thousand languages and also do all these tasks together to find it frustrating lingo so we just these models can also the translation by the way here right it's yeah I mean I don't know how to the translation with dirt so these models are just trained for translation and then we just fine-tuned one layer to do these tasks so I don't know argument is that you have way more semantic information in the multilingual translation model than in the birth model he was expected to do better all right I think this will say okay yeah experience is stronger if you do dream very different okay because you have the strong signal from the other language which you don't have in birth by just no horns good question yeah cross lingual downstream transfer is one thing but you're more interested in cross single transferring translation like zero shot so that wasn't our focus that was more like okay what's in it for other tasks yeah and what we also observed translating to English from all languages implicitly forced the representation in same space and also as you fine-tune high resource language quality doesn't change so they are really hard to move from their commerce point alright so we talked a lot about scaled models joint models but if you're if you have only limited amount of capacity how are you going to use it let's move talk a little bit about this practical aspect and say you have you train this giant model and then you want to you you have new data are you gonna train it again are you going to fine tune or but if you gonna fine-tune then you have to store this joint network again even the storage is a big problem of these networks so we came up with a very simple adaptation technique inspired by residual adapters our goal is so this actually also connects with the language level mixture of experts this is basically implementing a hard language level mixture of experts what we are doing here you take a pre trained an for model and you add some language-specific layers these are what we call adapter layers and you just sprinkle it right after these feed-forward layers of the entire network and then you route the the examples that by looking at their language ID and then the adapter layer is looking like this it's very simple it normalizes so allows you to plug in and play anywhere and it also has two projection layers with some non-linearity if you have if you don't have enough data you make it like a down projection it makes it a bottleneck Network but if you have a lot of data you actually expand the expanded hidden dimension here to make the most out of your data so it's actually allowing you to adapt to new domains and also new lengths new new domains or additional data on the on a particular language and by doing so so we edit 400-million transform and for model these residual adapters and on left you're seeing English - any this is when you're translating into X or any-any and by giving these language specific adapters you're actually covering all the recovering from recovering the high resource regression but also it's not affecting the transfer capability because you're not touching anything on the original Network you're only fine-tuning this adapter networks with the new data and also you see the similar trend on the on any to English tasks half of the other tasks you're seeing huge improvements on high resource languages you can still play with and without any regression on the lower source thing which is so it's it's really making it practical and if you actually add more and more data more capacity you can by adding 13% of 13 per person extra capacity we that was the point that we reached the by the if you reach the bilingual baselines so 400 million hunt sorry 200 translation models 400 million parameter each versus 400 million plus 13% it's actually getting as good as those the the army of translation models and also you can increase the adapter size to get more gains so you can actually cram more and more layers in each adapters and then train as you get more data or you can make this hierarchical adaptation scheme to adapt both language and domain at the same time so that's what very summarized everything here enhancing zero shot this was this was also another primer paper that we put it's basically these models are as you increase the multilingual 'ti these models are getting better and better in zero shot translation because it's adding some implicit regularization it has to actually align things together but you can also encourage it even even further by adding some alignment loss on top of encode representation it's very simple and very practical and it scales to hundreds of languages I'm not gonna go into detail of it I want to talk a little bit about the trainability training problem and the hyper parameters how are we actually doing the hyper brain research in given that joint models let's first talk about the task scheduling so I talked pre earlier I talked about the viewer using some temperature based data temperature based data sampling strategy to balance the balance the learning process on different low resource and high resource languages this experiment and the following experiment they're not implemented at scale they're scaled down and tested on WMT data so they are not actually 103 languages but this is an attempt to improve the translation quality when you're translating from English to X so we picked up English German and English French we separated out the decoders for these two languages after sharing a couple of layers in the decoder we separated them out and we gave the model if you're familiar with WMT english french english 32 german set up one hand you have formulated samples on the other hand you 40 million 40 million sentence examples so there's a imbalance between the training examples so if you just mix everything together even the data sampling strategy it's not going to help that much or you have to devise something else but we thought okay why are we learning how to wait different tasks in an ultra loop opt as an ultra loop optimization and we also used the our baseline knowledge here it's in the loss function I don't want to go into detail but it gives you a weight at the end at every update you're updating and after doing the parameter update you're also doing an additional update and in the ultra loop and it is giving you a weight for each task by using so this is the learned task weights from the beginning of the training here Green is French blue is German so it's learning a two for example here they update the French task and then down weight the German task because apparently German was too easy to learn because you don't have enough data yeah you're you're basically down weighting your German examples and updating a French example to reach the P also giving the baseline scores we were telling okay for German beat 28 blue score or beat this perplexity and for French beat 40 blue score or beat this perplexity which are given here these are the baselines that we give to the model and the this ultra loop optimization by reading a couple of things okay what is the Delta blue what is the what is the what is the gain that I'm getting doing the up the previous update by integrating these observations into a loss function and doing an ultra loop optimization you get some weights which are here and then you either apply it on top of your you apply to change your gradients or your learning grades we chose to change the learning rates themselves here you see two learning rates for different language pairs it's basically adding some noise on top of what you have in it as a transform learning rate schedule but it was still dominated by the transformer learning rate schedule which was hand tuned so it was trying to change the learning rate itself so it was changing the learning rate but it the magnitude of change was not big enough to change the structure of the learning rate so which is dominated by our hand tune learning rates here so what we did okay why are we also not learning the learning rate itself that's what we do by using a technique from 90s the it's a it's a very simple trick technique and it's scalable you take the derivative your fear of your loss function with respect to your learning rate and you end up with a very simple and intuitive formula which is here okay let's just look here this is this that tells us if you are agreeing with your previous update direction with your current gradient so if your current gradient update direction is agreeing with your previous update just increase your learning rate it's very intuitive and if you're not agreeing with your previous update direction just decrease the learning rate it's the close form and you can actually again implement it as an ultra loop optimization these are all what we what meta learning or learning to learn framework can enable and by doing so these are the learning rates that we learned these are in the previous slides the the underlying form was not learned but here these learning rate schedules are learned so that was very interesting for us and they're there if you if you have budget if you fine-tuned your baseline bilingual baseline like crazy so they are on par but but not better than you're very well tuned single language pair baselines but if you go to multitask setup they were actually very better than our bail baselines and they we also tested it on vert and they were also on par with hand to invert learning rate schedule etc yes so we set it to zero some numbers very close to zero e to the minus eight or nine something like that and it starts it learns to increase the learning rate first and then it starts decreasing interestingly so it's this looks like inverse squared scaling it's actually coverage it's redraws that what we had tuned for transformer but interestingly it also goes below zero sometimes so this means it's deciding to the gradient ascent and we thought okay this is this this is counterintuitive we shouldn't do that and if you don't do that it just didn't work so it means that so this is also very sharp drop so we also try to slow it down as in this case here it's not that and it's it's not that fast so it's not actually going below zero but even in this case quality was pretty much the same so it's telling us something about lost lost surface area surface of these networks so you should you really do the place that you are you can actually take a step back and then come back to the same regime go back and then come back to the same same region we tried a few different things but the the what seems to be working you just use it for example softmax you bundle softmax and embeddings together and then learn a single learning rate for those two and then entire encoder body transform body can use a single learning rate and you'll be surprised what that looks like that learned learning rate looks like and same goes for the encoder you can learn basically three different learning rates for the entire network so this is pretty much the end of my talk I'll finish with some open problems what the future entails what we see were as promising directions so this cross single downstream transfer it's still interesting and there's a lot of interest now bird for example became power horse of like so many NLP tasks right but what is it what's enabling bird to do that it's still unknown and it's also interesting to see just the training of multilingual model or machine translation model you can also read recycle the encoder or even decoder of these models on downstream toast I think there's something going on that we should understand we should investigate more so probably we should revisit our bed like objective functions just like you you said maybe the semantics that we're injecting is pretty good so what what what does that tell to the other tasks and what we can learn from other tasks then inject it into our objectives that wasn't that's interesting and also in the new tasks are these like document classification is it really the task that these models should be used or not and are those the languages for example if you look at the overlap in terms of the number of languages xlm are multilingual Bert this M for there if you add all of them up it adds up to 180 languages actually so they're not also overlapping so if you're trying to extend it okay let's just find that these two more languages and then make the analysis more fair because maybe the PDS adding something or edit data from common crawlers adding adding something so we're also trying to make it a more fair comparison on the unsupervised machine translation so I said in the second slide so we are going to Universal translation and as we take more and more take more steps we will see less informative data or the data that has less structure in terms of bilingual data for example it's it's aligned right or trilingual data or hundred way aligned data if you're moving into the regimes or we only have say more Lingle data and or even one step further we don't even have any written form data you they have audio now or you have to go and find the data in a different form or your data is gonna kind of be coming from different contextual form right so we these are more in the unsupervised empty and and also multi-domain or some multimodal translation phase and we believe this knowledge base of huge machine translation model is a good starting point to jumpstart any answer for his empty direction that's basically what what's here adapting to unseen languages and basically jump-starting the answer was empty also there is a huge opportunity if you want to study transfer learning transfer learning is let's make a crude partition we can split into two there's parallel transfer which I talked almost only about parallel transfer where you learn tasks simultaneously and you expect transfer happening in parallel but this is not the only way only way of doing transfer even even parallel in the parallel transfer case you can devise smarter schedules learning great schedules which I showed in the last two slides to mitigate the interference and so on so there's actually an opening to more core machine learning concepts and metal learning and so on and the second one is the stereo transfer you can also learn incrementally right okay I'm gonna first learn ten languages i'm and i'm gonna add five more languages without hurting without forgetting what i have learned so this is a continued learning set up or lifelong learning setup this is also another way to go it's not and i haven't presented the only thing that I presented here was residual adapters that you learn base model and then extend it into new domains or more data but there's also some promising directions in the serial transfer case by using continue learning metal learning basically maximizing without forgetting so that's pretty much it thanks so much it was a home run thank you [Applause] if you have any questions or face time if you can still speak French they should still have some way for this scheduler problem did you try that on the working world system one that was monstrous that was that was a multilingual multitask system system is English to French and German the best model coming home bilingual this baseline is coming from them all to the fighting quietly yes yes yes yes what was it this SJ or a spark that's coming from here basically this dark line it's the baseline that we are they asked the scheduler okay this is where you should beat you should per surface we haven't scaled it to that level when we were busy with other should we hit so honestly why did we didn't try that was the the particular date the particular way that we implemented our framework it wasn't allowing us to do a lot of sharing and not sharing these experiments are done on GPU and there is a there is a difference in the in terms of implementation between GP and TPU so we haven't back to report at this one to TPU yeah or the learning rate learning if you experimented with different depths models dismissed the different sizes different numbers of players cuz it seems to me that deeper models are quite sensitive to your learning my choice so this is solving the problem for you can you just go now to 60 120 layers and not have to find you in the lining anymore no it's not so this is it's solving the learning rate problem it's not solving so deep models if you are able to train a 64 layer model fizzing by using your heuristics then you can also you can swap your learning rate schedule here stick with this but if you scale your model okay let me be more specific this is not the first ball that you will hit if you make your if you try to make your model deep in the paper in the GPI paper we talk about it it's not like a window you just you know this stretch as you make your models deeper you will have a lot of month finite elements and the source of those ix non finite elements can be coming from a learning rate very large learning rate but most of they are either overconfident in their predictions which blows up say logits or other things or you don't have the gradients that you can distribute to the the lower layers of the of your network so there we use two things we are basically applying some logit clipping this is a heuristic and we are also using say transparent attention for example we are learning how to distribute gradients to the to the layers down below so if you are able to train as if you if you're fairly confident if you're at this stage you have you have all these heuristics tricks and then you can replace your learning rate schedule with this the the benefit here is when you're training a hundred building parameter network you only have a few shots you can only train these net you cannot do a grid search you cannot do anything and you will only be training one or two such systems right given the budget given time and we are not able to tune those hyper parameters but these are actually kind of suboptimal if you if you apply it on for example on single-payer language pair baseline these are suboptimal but our since we cannot tune the models at scale they also suboptimal so it's actually these type of meta learning the techniques they excel at scale for encoder decoder there so we coupled decoder softmax decoder embedding an encoder embedding those three are coupled together they all move in the with the same pace and we tried so many different things to to change the say transformer encoder block use separate learning rate for each layer use separate learning rate for parameter etc it seems like they don't matter much you can just use a single learning rate for the whole encoder if you separate out thread and bearings this was actually I I was reading this paper I saw this kind of curve and this paper they applied it on sci-fi 10 it's an image classification problem and I solved this curve they learned something like this I said okay this is its then let's replicate then we replicated we saw the same thing yes there is a there's actually a theory why it should be inverse square root or why you should decay learn at the learning rate so on starting from a very small learning rate basically this warm-up phase it's more about its indicating our lack of understanding on initialization and it's also using very very large batches yeah there are some other techniques that allow you to not start from you know there is zero it allows you to get rid of the warm-up hyper parameter but the haven't you for the English French I think so I didn't have too much detail so we are here view comparing against always multilingual Bert and here again right hand side is the low resource lower source languages it's actually worse than bilingual multilingual birch on xn li but it's better in an better than multilingual berth on glory source low resource languages and it's the same for part of speech this might be the read this may hint something else multilingual Bert is not using that smart learning rate schedules or it's not using tasks waiting it's not using we studied that a lot so we expect vir we try to maximize transfer to low resource so I think it's kind of expected [Applause] 