 okay topic for the classes interpretability as you know the modern machine learning models are justifiably reputed to be very difficult to understand so if I give you something like the GPT 2 model which we talked about a natural language processing and I tell you that it has 1.5 billion parameters and then you say mm why is it working clearly the answer is not because these particular parameters have these particular values there's no way to understand that and so the topic today is something that we raised a little bit in the lecture on fairness where one of the issues there was also that if you can't understand the model you can't tell if the model has baked in prejudices by examining it and so today we're going to look at different methods that people have developed to try to overcome this problem of inscrutable models so there's a very interesting bit of history how many of you know of george miller 7 plus or minus 2 result only a few so miller was a psychologist at at harvard I think in the 1950s and he wrote this paper in 1956 called the magical number 7 plus or minus 2 some limits on our capacity for processing information it's quite an interesting paper so he started off with something that I had forgotten I read this paper many many years ago and I had forgotten that he starts off with the question of how many different things can you sense how many different levels of things can you sense so if I put headphones on you and I ask you to tell me on a scale of 1 to n how loud is the sound that I'm playing in your headphone it turns out people get confused when you get beyond about five six seven different levels of intensity and similarly if I give you a bunch of colors and I ask you to tell me where the boundaries are between different colors people seem to come up with seven plus or minus two as the number of colors that they can distinguish and so there's a long psychological literature of this and then Miller went on to do experiments where he asked people to memorize lists of things and what he discovered is again that you could memorize a list of about seven plus or minus two things and beyond that you couldn't remember the list anymore so this tells us something about the cognitive capacity of the human mind and it suggests that if I give you an explanation that has 20 things in it you're unlikely to be able to fathom it because you can't keep all the moving parts in your mind at one time now it's a tricky result because he does point out even in 1956 that if you chunk things into bigger chunks you can remember seven of those even if they're much bigger and so people who are very good at memorizing things for example make up patterns and they remember those patterns which then allow them to actually remember more primitive objects so it you know and we still don't really understand how memory works but this is just an interesting observation and I think plays into the question of of how do you explain things in a complicated model because it suggests that you can't explain too many different things because people won't understand what you're talking about okay so what leads to complex models well as I say overfitting certainly lead two complex models I remember in the 1970s when we started working on expert systems and in healthcare I I made a very bad faux pas I went to the first joint conference between statisticians and artificial intelligence researchers and the statisticians were all about you know understanding the variance and understanding statistical significance and so on and I was all about trying to model details of what was going on in an individual patient and in some discussion after my talk somebody challenged me and I said well what we a I people are really doing is fitting what you guys think is the noise right because we're trying to make a lot more detailed refinements in our theories in our models than what the typical statistical model does and of course I was roundly booed out of out of the hall and people shunned me for the rest of the conference because I had done something really stupid to admit that I was I was fitting noise and of course I didn't really believe that I was fitting noise I was believed that what I was fitting was what the average statistician just chalks up to noise and we're interested in more details of the mechanisms so overfitting we have a pretty good handle on by regularization so you can you know you've seen lots of examples of regularization throughout the course and people keep coming up with interesting ideas for how to apply regularization in order to simplify models or make them fit some preconception of what the model ought to look like before you start learning it from data but the problem is that there's there really is true complexity to these models whether or not you're fitting noise there's the world is a complicated place human beings were not designed they evolved and so there's all kinds of bizarre stuff left over from our evolutionary heritage and so it is just complex it's hard to understand in a simple way how to make predictions that are useful when the world really is complex so what do we do in order to try to deal with this well one approach is to make up what I call just so stories that give a simplified explanation of how a complicated thing actually works so how many of you have read these stories when you were a kid nobody my god okay must be a generational thing so Rudyard Kipling was a famous author and he wrote the series of just-so stories things like how the lion got his mane and how the camel got his hump and so on and of course they're all total bull right I mean it's not a Darwinian evolutionary explanation of why male lions have Mane's it's just some made-up story but they're really cute stories and and I enjoyed them as a kid and maybe you would have to if your parents had read them to you so I mean I use this as a kind of pejorative because what the people who follow this line of investigation do is they take some very complicated model they make a local approximation to it that says this is not an approximation to the entire model but it's an approximation to the model in the vicinity of a particular case and then they explain that simplified model and I'll show you some examples of that through the lecture today and the other approach which I'll also show you some examples of is that you simply trade off somewhat lower performance for simple model that's simple enough to be able to explain so things like decision trees and and logistic regression and so on typically don't perform quite as well as the best most sophisticated models although you've seen plenty of examples in this class where in fact they do perform quite well and where they're not outperformed by the fancy models but in general you can do a little better by tweaking a fancy model but then it becomes incomprehensible and so people are willing to say okay I'm gonna give up 1% or 2% in performance in order to have a model that I can really understand and the reason it makes sense is because these models are not self-executing they're typically used as advice for some human being who makes ultimate decisions your surgeon is not going to look at one of these models that says take out the guy's left kidney and say okay my guess they're gonna go well does that make sense and in order to answer the question of does that make sense it really helps to know what the model was what what the models recommendation is based on what is its internal logic and so even an approximation to that is useful so the need for trust clinical adoption of ML models there are two approaches in this paper that I'm going to talk about where they say okay what you'd like to do is to look at case specific predictions so there's a particular patient in a particular state and you want to understand what the model is saying about that patient and then you also want to have confidence in the model overall and so you'd like to be able to have an explanatory capability that says here are some interesting representative cases and here's how the model views them look through them and decide whether you agree with the approach that this model is take now remember my critique of randomized control trials that that people do these trials they choose the simplest cases the smallest number of patients that they need in order to reach statistical significance the shortest amount of follow-up time etc and then the results of those trials are applied to very different populations so david's talked about cohort shift as a generalization of that idea but the same thing happens in these machine learning models that you train on some set of data the typical publication will then test on some held out subset of the same data but that's not a very accurate representation of the real world if you then try to apply that model to data from a totally different source the chances are you will have specialized it in some way that you don't appreciate and the results that you get are not as good as what you got on the held-out test data because it's more heterogeneous I think I mentioned that Jeff Drazen the editor-in-chief of the New England Journal had a meeting about about a year ago in which he was arguing that the journal shouldn't ever publish a research study unless it's been validated on two independent data sets because he he's tired of publishing studies that wind up getting retracted because not because of any overt badness on the part of the investigators they've done exactly the kinds of things that you've learned how to do in this class but when they go to apply that model to a different population it just doesn't work nearly as well as it did in the published version and of course they're all the publication bias issues about you know if if 50 of us do the same experiment and by random chance some of us are going to get better results than others and those are the ones that I get published because the people who got poor results don't have anything interesting to report and so there's that whole issue of publication bias which is another serious one okay so I wanted to just spend a minute to say you know explanation is not a new idea so in the expert system zero that we talked about a little bit in one of one of our earlier classes we talked about the idea that we would take medical human medical experts and debrief them of what they knew and then try to encode those in patterns or in rules or in various ways in a computer program in order to reproduce their behavior so my son was one of those programs petroglyphs PhD thesis in 1975 and they published this nice paper that was about explanation and rule acquisition capabilities of the Meissen system and as an illustration they gave some examples of what you could do with the system so rules they argued were quite understandable because they say if a bunch of conditions then you can draw the following conclusion right so given that you can say well when the program comes back and says in light of the site from which the culture was obtained and the method of collection do you feel that a significant number of organism one were detected were obtained right in other words if you took a sample from somebody's body and you're looking for an infection do you think you got enough organisms in that sample and the user says well why are you asking that me this question and the answer in terms of the rules that the system works by is pretty good it says it's important to find out whether there's therapeutically significant disease associated with this occurrence of organism one we've already established that the culture is not one of those that are normally sterile and the method of collection is sterile therefore if the organism has been observed in significant numbers then they're strongly suggestive evidence that there's therapeutically significant disease associated with this occurrence of the organism okay so if you find bugs in a place carefully collected then that suggests that you ought to probably treat this patient if there are a bunch of enough enough bugs there and there's also strongly suggestive evidence that the organism is not a contaminant because the collection method was sterile and you can go on with this and you can say well why did why why that so why that that question and and and it traces back in its evolution of these rules and it says well in order to find out the locus of infection it's already been established that the site of the culture is known the number of days since the specimen was obtained is less than seven therefore there's therapeutically significant disease associated with this recurrence of the organism so there's some rule that says if you got bugs and it happened within the last seven days the patient probably really does have an infection right so and I mean I've got a lot of examples of this but you can keep going why you know this is the two-year-old but why daddy but why but why well why is it important to find out a locus of infection and well there's a reason which is that there's a rule that will conclude for example that the abdomen is a locus of infection or the pelvis as a locus of infection of the patient if you satisfy these these criteria and so this is a kind of rudimentary explanation that comes directly out of the fact that these are rule based system and so you can just play back the rules one of the things I like is you can also ask freeform questions 1975 the natural language processing was not so good and so this worked about one time in five but you could walk up to it and type some question and for example do you ever prescribe carbon a salon for Pseudomonas infections and it says well there are three rules in my database of rules that would conclude something relevant to that question so which one do you want to see and if you say I want to see rule 64 it says while that rule says if it's known with certainty that the organism is a pseudo Pseudomonas and the drug under consideration is gentamicin then a more appropriate therapy would be a combination of gentamicin and carvanha Siwan again this is medical knowledge as of 1975 but my guess is the real underlying reason is that there are probably worse Pseudomonas that were resistant by that point to gentamicin and so they used a combination therapy now notice by the way that this explanation capability does not tell you that right because it doesn't actually understand the rationale behind these individual rules and at the time there was also research for example by one of my students on how to do a better job of that by by encoding not only the rules or the patterns but also the rationale behind them so that the explanations could be more sensible okay well the granddaddy of the standard just so story approach to explanation of complex models today comes from this paper and a system called Lyme locally interpretable model agnostic explanations and just to give you an illustration you have some complicated model and it's trying to explain why the or human being made a certain decision or why the model made a certain decision and so it says well here are the data we have about the patient we know that the patient is sneezing and we know their weight and their headache and their age and the fact that they have no fatigue and so the explainer says well why did the model decide this patient has the flu well positives are sneeze and headache and a negative is no fatigue right so it goes into this complicated model and it says well I can't explain all the numerology that happens in that the role network or Bayesian network or whatever network it's being it's using but I can specify that it looks like these are the most important positive and negative contributors yeah I'll show you some other kind of data and in a minute I think they originally worked it out for notes but it was also used for images and other as well okay and the argument they make is that this approach also helps to detect data leakage for example in one of their experiments the the headers of the data had information in them that that correlated highly with the result I think they're I can't remember if it was these guys but somebody was assigning study IDs to each case and they did it in a stupid way so that all the small numbers correspond it to people who had the disease and the big numbers correspond it to the people who didn't and of course the most parsimonious predictive model just used the ID number and said okay I got it so this would help you identify that because if you see that the best predictor is the ID number then you would say there's something a little fishy going on here well so here's an example where this kind of capability is very useful so this was another this was from a news group and they were trying to decide whether a post was about christianity or atheism okay now look at these two models so there's algorithm one an algorithm - or model one and model two and when you explain a particular case about using model one it says well the words that I consider important are God mean anyone this Koresh and through does there anybody remember who David Koresh was he was some cult leader who I can't remember if if he killed a bunch of people or bad things happened oh I think he was the guy in Waco Texas that the FBI and the ATF went in and and set their place on fire and a whole bunch of people died so the prediction in this case is atheism and you notice that God and Koresh and mean are negatives and anyone this and through our positives and you go is that good but then you look at algorithm two and you say this also made the correct prediction which is that this particular article is about atheism but the positives were the word by and in not not terribly specific and the negatives were things like nntp you know what that is that's the network time protocol okay it's some technical thing and posting and host so this is probably like metadata that got into the header of the articles or something so it happened that in this case algorithm two turned out to be more accurate than algorithm one on their held-out test data but not for any good reason and so the explanation capability allows you to clue in on the fact that even though this thing is getting the right answers it's not for sensible reasons okay so what would you like from an explanation well they say you'd like it to be interpretable so it should provide qualitative understanding of the relationship between the input variables and the response but they also say that that's going to depend on the audience it requires sparsity for the george miller argument that i was making before you can't keep too many things in mind and the features themselves that you're explaining must make sense so for example if i say well the reason this decided that is because the eigenvector for the first principal component was the following that's not gonna mean much to most people and then they also say well it ought to have local fidelity so it must correspond to how the model behaves in the vicinity of the particular instance that you're trying to explain and their third criterion which i think is a little iffy er is that it must be model agnostic in other words you can't take advantage of anything you know that is specific about the structure of the model the way you trained it anything like that it has to be a general-purpose explainer that works on any kind of complicated model yeah I think their their reasoning for why they insist on this is because they don't want to have to write a separate explainer for each possible model so it's much more efficient if you can get this done but I actually questioned whether this is always a good idea or not but nevertheless this is one of their assumptions okay so here's the setup that they use they say all right X is a vector in some D dimensional space that defines your original data and what we're gonna do in order to make the data explainable in order to make the data not the model explainable is we're going to define a new set of variables X prime that are all binary and that are in some space of dimension D prime that is probably lower than D okay so it's we're simplifying the data that we're gonna explain about this model then they say okay we're going to build an explanation model G where G is the class of interpretable models so what's an interpretable model well they don't tell you but they say well examples might be linear models additive scores decision trees falling rule lists which see you later in the lecture and the domain of this is this input the simplified input data the binary variables in D prime dimensions and the model complexity is going to be some measure of the depths of the decision tree the number of nonzero weights and the logistic regression the number of clauses in a following rule list etc so it's some complexity measure and you want to minimize complexity so then they say alright the real model the the hairy complicated full Bohr model is F and that map's the original data space into some probability and for example for classification F is the probability that X belongs to a certain class and then they also need a proximity measure so they need to say we have to have a way of comparing two cases and saying how close are they to each other and the reason for that is because remember they're going to give you an explanation of a particular case and the most relevant things that will help with that explanation are the ones that are near it in this high dimensional input space okay so they then define their loss function based on the actual decision algorithm based on the simplified one and based on the proximity measure and they say well the best explanation is that G which minimizes this loss function plus the complexity of G pretty straightforward so that's our best best model now the clever idea here is to say instead of using all of the data that we started with what we're going to do is to sample the data so that we take more sample points near the point we're interested in exploit explaining we're going to sample in the simplified space that is explainable and then we'll build that G model the explanatory model from that sample of data where we wait by that proximity function so the things that are closer will have a larger influence on the model that we learn and then we recapture the sort of the closest point to this simplified representation we can calculate what its answer should be and that becomes the label for that point and so now we trained a simple model to predict the label that the complicated model would have predicted for the point that we've sampled okay yeah it's a distance function of some sort and I'll say more about it in a minute because that's one of the critiques of this particular method has to do with how do you choose that distance function but it's basically a similarity so here's a nice graphical explanation of what's going on suppose that the actual model the decision boundary is between the blue and the pink regions okay so it's this god-awful hairy complicated decision model and we're trying to explain why this big red plus wound up in the pink rather than in the blue so the approach that they take is to say well let's sample a bunch of points weighted by shortest distance so we do sample a few points out here but mostly we're sampling points near the point that we're interested in we then learn a linear boundary between the positive and the negative cases and that boundary is an approximation to the actual boundary in the more complicated decision model okay so now we can give an explanation just like you saw before which says well this is some D prime dimensional space and so which variables in that the D prime dimensional space are the ones that influence where you are on one side or another of this newly computed decision boundary and to and to what extent and that becomes the explanation okay nice idea so if you apply this to text classification yes well that's why I called the called just so story right should you believe it well the engineering disciplines have a very long history of approximating extremely complicated phenomena with linear models right I mean I'm I'm in a Department of Electrical Engineering and computer science and if I talked to my electrical engineering colleagues they know that the world is insanely complicated nevertheless most models in electrical engineering or linear models and they work well enough that people are able to build really complicated things and have them work so I that's not a proof that that's an argument by by history or something but it's true linear models are very powerful especially when you limit them to giving explanations that are local notice that this model is a very poor approximation to this decision boundary or this one right and so it only works to explain in the neighborhood of the particular example that I've chosen all right but it does work okay there in the middle of the red space then well they did so they sample all over the place but remember that that proximity function says that this one is less relevant to predicting that that decision boundary because it's far away from the point that I'm interested in so that's the magic well but but they they would I mean it's suppose they picked this point instead then they would sample around this point and and presumably they would find this decision boundary or this one or something like that and still still be able to come up with a coherent explanation okay so in the case of text you've seen this example already it's pretty simple they for their proximity function they use cosine distance so two bag of words model and they just calculate cosine distance between different examples by how much overlap there is between the words that they use and the frequency of words that they use and then they choose K with the number of words to show just as a preference so it's sort of a hyper parameter they say you know I'm interested in looking at the top five words or the top ten words that are either positively or negatively an influence on the decision but not the top 10,000 words because I don't know what to do with 10,000 words okay now what's interesting is you can also then apply the same idea to image interpretation so here's a dog playing a guitar and they say how do we interpret this and so this is one of these labeling tasks where you'd like to label this picture as a Labrador or maybe as an acoustic guitar but some some labels also decide that it's an electric guitar and so they say well what counts in favor of or against each of these and the approach they take is a relatively straightforward one they say let's define a super pixel as a region of pixels within it within an image that have roughly the same intensity so if you've ever used Photoshop the magic selection tool can be adjusted to say find the region around this point where all the intensities are within some Delta of the of the point that I've picked and so it will outline some region of the picture and what they do is they break up the entire image into these regions and then they treat those as if they were the words in the in the word style explanation okay so they say well this looks like an electric guitar to the algorithm and this looks like an acoustic guitar and this looks like a Labrador so some of that makes sense I mean you know that dog's face does kind of look like a lab this does look kind of like part of the body and part of the fret work of a guitar I have no idea what the stuff is or why this contributes to it being a dog but such as such as the nature of these models but at least it is telling you why it believes these various things so then the last thing they do is to say well okay that helps you understand the particular model but how do you convince yourself I mean a particular example where a model is applied to it but how do you convince yourself that the model itself is reasonable and so they say well the best technique we know is to show you a bunch of examples but we want those examples to kind of cover the gamut of places that you might be interested in and so they say let's create this matrix an explanation matrix where these are the cases and these are the various features you know the top words or the top pixel elements or something and then we'll fill in the element of the matrix that tells me how strongly the feature is correlated or anti-correlated with the classification for that model and then it becomes a kind of set covering issue find a set of models that gives me the best coverage of explanations across that set of features and then with that I can convince myself that the model is reasonable so they have this thing called the sub modular pick algorithm and you know probably if you're interested you should read the paper but what they're doing is this is essentially doing a kind of greedy search that says what feature should I add in order to get the best coverage in that space of features by documents and and then they did a bunch of experiments where they said okay let's compare the results of these explanations of these simplified models to two sentiment analysis tasks of two thousand instances each bag of words as features they compared it to decision trees logistic regression nearest neighbors SVM with a radial basis function kernel or random forests that use word to vacuum betting's highly non explainable with a thousand trees and k equal ten so they chose ten features to explain for each each of these models they then did a side calculation that said what are the ten most suggestive features for each case and then they said does that covering algorithm identify those features correctly and so what they show here is that they're their method line does better in every case than a random sampling that's not very surprising or a greedy sampling or a partisan sampling which I don't know the details of but in any case there's what this graph is showing is that of the features that they decided were important in each of these cases they're recovering so there recall is up around you know 90s 90 plus percent so in fact the algorithm is identifying the right cases to give you a broad coverage across all the important features that matter in classifying these cases they then also did a bunch of human experiments where they said okay we're going to ask users to choose which of two classifiers they think is going to generalize better so this is like the picture I showed you of the Christianity versus atheism algorithm where presumably if you were a Mechanical Turk er and somebody showed you an algorithm that has very high accuracy but that depends on things like finding the word and NTP in a classifier for atheism versus Christianity you would say well maybe that algorithm isn't going to generalize very well because it's depending on something random that may be correlated with this particular dataset but if I try it on a different data set it's unlikely to work so that was one of the tasks and then they asked them to identify features like that that looked bad they then ran this Christianity versus atheism test and had a separate test set of about 800 additional webpages from this website they the underlying model was a support vector machine with RBF kernels trained on the 20 News group data I don't know if you know that dataset but it's it's a well known publicly available dataset they got a hundred Mechanical Turkish and they said okay we're gonna present each of them six documents and six features per document in order to ask them to make this and then they did an auxiliary experiment in which they said if you see words that are no good in this experiment just strike them out and that will tell us which of the features were bad in in this in this method and what they found was that the human subjects choosing between two classifiers were pretty good at figuring out which was the better classifier now this is better by their their judgment and so they said okay the the sub-module or pick algorithm which is the one that I didn't describe in detail but it's this set covering algorithm gives you better results than a random pick algorithm that just says pick random features again not totally surprising so and the other thing that's interesting is if you do the feature engineering experiment it shows that as the Turkish interacted with the system the system became better so they started off with real-world accuracy of just under 60 percent and using the better of their algorithms they reached about seventy five percent after three rounds of interaction so the users could say I don't like this feature and then the system would give them better features now they tried a similar thing with images and so they this one is a little funny so they trained a deliberately lousy classifier to classify between wolves and huskies right this is a famous example also it turns out that that Huskies live in Alaska and so and wolves I guess some wolves do but most wolves don't and so the data set on which that are written which was used in that original problem formulation there was an extremely accurate classifier that was trained and when they went to look to see what it had learned basically it had learned to look for snow and if it saw snow in the picture it said it's a husky and if it didn't see snow in the picture it said it's a wolf so that turns out to be pretty accurate for the sample that they had but of course it's not a very sophisticated classification algorithm because it's possible to put a wolf in a snowy picture and it's possible to have your husky indoors with no snow and then you're just missing the boat on this classification so these guys built a particularly bad classifier by having all wolves in the training set had snow in the picture and none of the Huskies did okay and then they presented cases to graduate students like you guys with math machine learning backgrounds ten balanced test predictions but they put one ringer in each category so they put in one husky in snow and one wolf who was not in snow and the comparison was between pre and post experiment trust and understanding and so before the experiment they said that 10 of the 27 students said they trusted the bad mom this bad model that they trained and afterwards only 3 out of 27 trusted it so this is a kind of sociological experiment that says yes we can actually change people's minds about whether remodel is a good or a bad one an experiment before only 12 out of 27 students mentioned snow as a potential feature in this classifier whereas afterwards almost everybody did so again this tells you that that the method is providing some useful information now this paper set off a lot of work including a lot of critiques of the work and so this is one particular one from just a few months ago the end of December and what these guys say is that that distance function which includes a Sigma which is sort of the scale of distance that you're willing to go is pretty arbitrary in the experiments that the original authors did they set that distance to 75 percent of the square root of the dimensionality of the data set and you know okay I mean that's a number but it's not obvious that that's the best number or the right number and so these guys argue that it's important to tune the size of the neighborhood according to how far Z the point that you're trying to explain is from a boundary so if it's close to the boundary then you ought to take a smaller region for your proximity measure and if it's far from the boundary this addresses the question you guys were asking about what happens if you pick a point in the middle and so they show some nice examples of places where for for instance if you compare this explaining this Green Point you get a nice green line that follows the local boundary but explaining the Blue Point which is close to a corner of the the actual decision boundary you get a line that's not very different from the green one and similarly for the red point and so they say well we really need to work on distance function and so they come up with a method that they call leaf äj-- which basically says remember what lime did is it sampled nonexistent cases simplified non-existent cases but here they're gonna sample existing cases so they're going to learn from the trainings the original training set but they're going to sample it by proximity to the the example that they're trying to explain and they argue that this is a good idea because for example in law the notion of precedent is that you get to argue that this case is very similar to some previously decided case and therefore it should be decided the same way I mean Supreme Court arguments are always all about that lower court arguments are sometimes more driven by what the law actually says but case law has been well established in British law and then by by inheritance in American law for many many centuries so they say well case based reasoning normally involves retrieving a similar case adapting it and then learning at that as a new precedent and they also argue for contrastive justification which is not only why did you choose X but why did you choose X rather than Y as giving a more satisfying and a more insightful explanation of how some model is working so they say okay similar set up F solves the classification problem where X is the data and Y is some binary classifier you know 0 or 0 1 if you like the training set is a bunch of X's y sub true is the actual answer and Y predicted is what F predicts on that on that X and to explain f of Z equals some pretty or outcome you can define the allies of a case as ones that come up with the same answer and you can define the enemies as one that ones that come up with a different answer right so now you're gonna sample both the Allies and the enemies according to a new distance function and the intuition they had is that the reason that they're that the distance function in the original line work wasn't working very well is because it was a spherical distance function in n-dimensional space and so they're gonna bias it by saying that the distance this B is going to be some combination of the difference in in the linear predictions plus the difference in the two points and so the contour lines of the first term are these circular contour lines this is what lime was doing the contour lines of the second term are these linear gradients and they add them to get sort of oval shaped things and this is what gives you that desired feature of being more sensitive to how close this point is to the decision boundary okay again there are a lot of relatively hairy details which I'm going to elide in the in the class today but they're they're definitely in the paper so they also did a user study on some very simple prediction models so this was how much is your house worth based on things like how how big is it and what year was it built built in and what's some subjective quality judgment of it and so what they show is that you can find the examples that are the the allies and the enemies of this house in order to do the prediction so then they applied their algorithm and if it works it gives you better answers I'll have to go find that slide somewhere all right so that's that's all I'm gonna say about about this this idea of using simplified models in the local neighborhood of individual cases in order to explain something I wanted to talk about two other topics so this was a paper by some of my students recently in which they're looking at medical images and trying to generate radiology reports from those medical images right I mean you know machine learning can solve all problems I give you a collection of images and a collection of radiology reports should be straightforward to build a model that now takes new radiological images and produces new radiology reports that are understandable accurate etc I'm joking of course but the the approach they took was kind of interesting so they taken a standard image decoder and then before the pooling layer they take essentially an image embedding from the next last layer of this imaging coding algorithm and then they feed that into a word decoder and word generator and the idea is to get things that appear in the image that correspond to words that appear in the report to wind up in the same place in the embedding space and so again there's a lot of hair it's a lsdm based encoder and it's modeled as a sentence decoder and within that there's a word decoder and then there's a generator that generates these reports and it uses reinforcement learning and it you know tons of hair but here's what I wanted to show you which is interesting so the encoder takes a bunch of spatial image features the sentence decoder uses these image features in addition to the the linguistic features the word embeddings that are fed into it and then for ground truth annotation they also use a remote annotation method which is this check spurt program which is a rule-based program out of stanford that reads radiology reports and identifies features in the report that it thinks are important and correct so it's not always correct of course but but that's used in order to guide the generator so here's an example so this is an image of a chest and the ground truth so this is the actual radiology report says cardiomegaly is moderate by basilar atelectasis is mild there's no pneumothorax a lower cervical spinal fusion is partially visualized healed right rib fractures are incidentally noted by the way I've stared at hundreds of radiological images like this I could never figure out that that this image says that but that's why radiologists trained for many many years to become good at this stuff so there was a previous program done by others called tie net which generates the following report it says ap portable upright view of the chest there's no pool no focal consolidation effusion or no pneumothorax the cardio meta mediastinal silhouette is normal imaged us structures are intact so if you compare this to that you say well if the cardio mediastinal silhouette is normal then where would the the lower cervical spinal fusion being partially visualized because that's along the middle and so these are not quite consistent so the system that the students built says there's mild enlargement of the cardiac silhouette there's no pleural effusion or pneumothorax and there's no acute osseous abnormalities so it also missed the healed right rib fractures that were incidentally noted but anyway it's you know the remarkable thing about a singing dog is not how well it sings but the fact that it sings at all and the reason I included this work is not to convince you that this is gonna replace radiologists anytime soon but but that it had an interesting explanation facility and the explanation facility uses attention which is part of its model to say hey when we reach some conclusion we can point back into the image and say what part of the image corresponds to that part of the conclusion and so this is pretty interesting you say an upright and lateral views of the chest in red well that's kind of the chest in red there's moderate cardiomegaly so here the green certainly shows you where your heart is okay about there and a little bit to the left and there's no pleural effusion or pneumothorax this one is kind of funny that's the blue region so how do you show me that there isn't something and we were surprised actually when the way it showed us that there isn't something is to highlight everything outside of anything that you might be interested in which is not exactly convincing that there's no no pleural effusion and here's another example there's no relevant change tracheostomy tube is in place so that roughly is showing a little too wide but it's showing roughly where a tracheostomy tube might be bilateral pleural effusion and compressive analytic ASIS and left ASIS is when your lung tissue stick together and so that does often happen in the lower part of the lung and again the negative shows you everything that's not sort of part of the action yeah no it's trying to predict the whole model the whole note yeah but these guys were ambitious you know what was it geoff hinton said a few years ago that he wouldn't want his children to become radiologists because that field is going to be replaced by by computers I think that was a stupid thing to say especially when you look at the state of the art of how well these things work but if that were true then you would in fact want something that is able to produce an entire radiology report so the motivation is there now after this work was done we ran into this interesting paper from Northeastern which says attempt but listen guys attention is not explanation okay so attention is clearly a mechanism that's very useful in all kinds of machine learning methods but you shouldn't confuse it with an explanation so they say well assumption it's the assumption that the input units are accorded high attention that are accorded high attention weights are responsible through the model outputs and that may not be true and so what they did is they did a bunch of experiments where they studied the correlation between the attention weights and the the gradients of the model parameters to see whether in fact the words that had high attention were the ones that were most decisive in making a decision in the model and they found that the evidence that correlation between intuitive feature importance measures including gradient and feature erasure approaches so this is ablation studies and learned attention weights is weak and so they did a bunch of experiments there are a lot of controversies about this particular study but what you find is that if you calculate we can Corden's you know between on different data sets using different models you see that for example the concordance is not very high it's less than a half for this data set and you know some of it below zero so the opposite for this data set the interestingly things like like diabetes which come from the mimic data have narrower bounds than some of the others so they seem to have a more definitive conclusion at least for this study okay let me finish off by talking about the opposite idea so rather than building a complicated model and then trying to explain it in simple ways what if we just built a simple model and Cynthia Rudin was now a Duke used to be at in the Sloan School at MIT has been championing this idea for many years and so she has come up with a bunch of different ideas for how to build simple models that trade-off maybe a little bit of accuracy in order to be explainable and one of her favorites is this thing called a falling rule list so this is an example for a mammographic mass data set so it says if the if some lump has an irregular shape and the patient is over 60 years old then there's an 85 percent chance of malignancy risk and there are 230 cases in which that happened okay if this is not the case then if the the lump has a speculated margin so it has little spikes coming out of it and the patient is over 45 then there's a 78% chance of malignancy and otherwise if the margin is kind of fuzzy the edge of it is kind of and the patient is over 60 then there's a 69% chance and if it has an irregular shape then there's a 63% chance and if it's lobular and the density is high then there's a 39% chance and if it's round and the patient is over 60 then there's a 26% chance otherwise there's a 10% chance okay and the argument is that that description of the model the decision-making model is simple enough that even doctors can understand it he's supposed to laugh now there are still some problems so one of them is notice some of these are age greater than 60 age greater than 45 age greater than 60 it's not quite obvious what categories that's that's defining and in principle it could be different ages in in different ones but here's how they build it so this is a very simple model that's built by a very complicated process so the simple model is the one I've just showed you and there's a Bayesian approach a Bayesian generative approach where they have a bunch of hyper parameters falling rulest parameters theta they calculate a likelihood which is given a particular theta how likely are you to get the answers that are actually in your data given the model that you generate and they start with a possible set of if clauses so they do frequent Clause mining to say what conditions what binary conditions occur frequently together in the database and those are the only ones that are going to consider because of course the number of possible clauses is vast and they don't want to have to iterate through those and then for each setup for each Clause they they calculate a risk score which is generated by a by a probability distribution under the constraint that the risk score for the next Clause is lower or equal to the risk score for the previous Clause okay there are lots of details so there's this frequent itemsets mining algorithm it turns out that choosing our R sub L to be the logs of products of real numbers is an important step in order to guarantee that monotonicity constraint in a simple way L the number of clauses is drawn from a Poisson distribution and you give it a kind of scale that says roughly how many clauses would you be willing to tolerate in your in your following rule list and then there's a lot of computational hair where they do they get mean a posterior a probability estimation by using a simulated annealing algorithm so they basically generate some clauses and then they use swap replace add and delete operators in order to try different variations and they're doing hill climbing in that space there's also some Gibbs sampling because once you have one of these models simply calculating how accurate it is is not straightforward there's not a closed-form way of doing it and so they're doing sampling in order to try to generate that so it's a bunch of hair and again the paper describes it all well what's interesting is that on a 30 day hospital readmission data set with about 8,000 patients they used about 34 features like impaired Mental Status difficult behavior pain feels unsafe etc they mind rules or clauses with support more than 5% of the database and no more than two conditions they set the expected length of the decision list to be 8 clauses and then they compared the decision model they got to SVM's random forest logistic regression cart and an inductive logic programming approach and shockingly to me their method the falling rulest method got an a you see of about 0.8 whereas all the others did like 0.7 9.75 logistic regression as usual outperformed the one they got slightly right but this is interesting because their argument is that this representation of the model is much more easy to understand than even a logistic regression model for most human human users and also if you look at these are just various runs and the different models and their model has a pretty decent day you see up here I think the green one is the logistic regression one and it it's slightly better because it outperforms their best model in the region of low false positive rates which may be where you want to operate so that may actually be a better model so here's their readmission rule list and it says if the patient has bedsores and has a history of not showing up for appointments then there's a 33% probability that they'll be readmitted within 30 days if I think some note says poor prognosis and maximum care etc so this is the result that they came up with now by the way we've talked a little bit about a 30 day readmission predictions and getting over about 70% is not bad in that domain because it's just not that easily predictable who's gonna wind up back in the hospital within 30 days so these models are actually doing quite well and certainly understandable in these terms they also tried on a variety of University of california-irvine machine learning data sets these are just random public data sets and they tried building these falling rulest models to make predictions and what you see is that the au C's are pretty good so on the spam detection data set their system gets about 91 logistic regression again gets 97 so you know part of the unfortunate lesson that we we teach in almost every example in this class is the simple models like logistic regression often do quite well but remember here they're optimizing for explained ability rather than for getting the right answer so they're willing to sacrifice some accuracy in their model in order to develop a result that is easy to explain to people so again there are many variations on this type of work where people have different notions of what counts as a simple explainable model but that's a very different approach than the Lyme approach which says build the Harry model and then produce local explanations for why it makes certain decisions on particular cases all right I think that's all I'm going to say about explain ability this is a very hot topic at the moment and so there are lots of papers I think there's I just saw a call for a conference on explainable machine learning models so there's more and more working in this area so with that we come to the end of our and I just wanted I just went through the the front page of the course website and listed all the topics so we've covered quite a lot of stuff right you know what what makes healthcare different and we talked about what clinical care is all about and what clinical data is like and risk stratification survival modeling physiological time-series how to interpret clinical text in a couple of lectures translating technology into the clinic the italicized ones were guest lecturers so machine learning for cardiology and machine learning for differential diagnosis machine learning for pathology for mammography David gave a couple of lectures on causal inference and reinforcement learning or David and a guest which I I didn't note here disease progression and subtyping we talked about precision medicine and the role of genetics automating clinical workflows the lecture on regulation and then recently fairness robustness the data set shift and interpretability so that's quite a lot we I think we're we the staff are pretty happy with how the class has gone it was our first time as this crew teaching it and we hope to do it again I can't stop without giving an immense vote of gratitude to Irene and Willi without whom we would have been totally sunk and I also want to acknowledge David's vision and putting this course together he taught a sort of half-size version of a class like this a couple of years ago and thought that it would be a good idea to expand it into a full semester a regular course and got me on board to work with them and I want to thank you all for your hard work and I and I'm looking forward to 