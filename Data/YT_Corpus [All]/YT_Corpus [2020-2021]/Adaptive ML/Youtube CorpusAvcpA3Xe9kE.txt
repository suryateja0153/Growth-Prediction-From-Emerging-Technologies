 Kevin Jamieson: Before his talk begins, I just wanted to remind people that during his talk, please interrupt Ludwig directly with your voice. If you have any clarifying questions, he'd be happy to take questions during the talk. However, any other questions, feel free to hold those until the end of the talk, in which time we will do questions with the hand, type in hand in the chat and we'll go one by one. So again, we're very thrilled to have Ludwig here today. He's coming to us from a postdoctoral researcher at UC Berkeley. He works with Moritz Hardt and Ben Recht. He completed his PhD at MIT under Piotr Indyk and he has received a Google PhD fellowship, a Microsoft science fellowship, a best paper award ICML, and the Sprouse Dissertation Award at MIT. His research interests are all around the empirical and theoretical foundations of machine learning. It's really a mix of the two. And, he often is interested in trying to look at ways that make machine learning more reliable and predictable. And so with that, today he's going to be talking about classifier generalization and please take it away Ludwig. Welcome. Ludwig Schmidt: Cool. Thanks for the introduction, Kevin. I'm excited to be here today, and I'm excited to tell you about what broadly around tech question how reliable is current machine learning. I think everyone by now, so many, have been affected over the past decade just because of an explosion of interest and machine learning. And what is all this excitement about, what are some of the key advances behind this excitement and machine learning? I think everyone in the field would agree the progress on the IMAGENET, IMAGENET data set has been one of these key advances. Many of you have heard of IMAGENET before. Let's just make sure we're all on the same page. What is IMAGENET? Imagine there's a large image classification data set, contains about a million training images divided into 1,000 different classes and the task that we asked a computer to do is simply given a much, like this dog here, assigned a correct label from the image in a class hierarchy which here would be going retriever and so on. You get the idea. And history around IMAGENET. It's nicely summarized with plot here. So on the x-axis, we have the year of the IMAGENET competition. So the team around [INAUDIBLE] didn't just release a data set, they also had an annual competition. The team submitted the classification algorithms and the goal was to figure out in a given year who did best. So under y axis, you have classification error of the best submission in a given year and initially when this dataset and the competition came out, people considered this to be a really hard problem, like error, error rates were high and it wasn't exactly clear how to make much progress. But then in 2012, a breakthrough happens. So, this is when the team around Jeff Fenton from Toronto released their seminal AlexNet paper and they showed for the first time that large convolutional networks could make substantial improvements on ImageNet and then in the following years, the machine learning and computer vision communities really went with this approach and had a lot of success improving the error rates on image net further and further. And by some measure now, take this with a grain of salt, we'll return to this later but by some measure now we had superhuman performance levels on ImageNet. And this was been tremendously impactful. So in academia, for instance, Jitendra Malik calls the AlexNet paper "the most impactful paper in machine learning and computer version in the last five years" to say, which I think is quite uncontroversial. The ImageNet paper itself won the CVP our test of time award last year. AlexNet was mentioned in the Turing award citation last year and their ResNet paper which won the competition in 2015 was the most cited paper in all of the sciences this last year. Ok. This is the impact in academia. Then in industry, a lot of the research labs actually joined this competition fairly early on. In 2014, a team from Google won and in 2015, a team from Microsoft won. The researchers are now at Facebook AI and overall, companies have invested large amounts of money into machinery research, as a result of all of this excitement of machine learning. And then finally last year, ImageNet also arrived in government. This is from the Economic Report of the President 2019. It has an entire chapter devoted to the impact AI will have in the US economy, and then the first figure in this chapter on AI, this again exactly saying the [INAUDIBLE] plot that which is used here to convey to people how much progress we have seen in machine learning. And to be clear, it's not just image that there's a sort of representative for whole variety of different benchmarks in computer vision, natural language processing, and other fields of machine learning we have seen tremendous progress over the past decade in a variety of benchmark, and over the subscribers of the [INAUDIBLE] of a few hours to use these emerging technologies in a variety of different applications. For instance, transportation as in self-driving cars, healthcare, robotics, online content moderation, and so on, and I think this is great if you can use machine learning in these different areas, I think there's the potential for real societal benefit. I think we also need to be careful and I think one key question here is always how reliable is current machine learning? Okay, and how do we even think about this to begin with, so what can be learned from this machine learning benchmarks? So, and optimistic interpretation of this plot here would be that we have truly surpassed human level performance and ImageNet and we can now do ImageNet image classification better than humans. If you work in this area, it's pretty clear that things are more nuanced and this is pretty optimistic. So, why do we get so excited about these performance numbers in the end? What does good performance on a test set imply? And for that, we have this concept of generalization of machine learning. This is arguably the main goal of machine learning. We don't want to get models that generalize and what we mean by that is at the very least the classifiers should perform similarly well on new data from the same source. A bit more completely in the context of ImageNet, what we mean by that. So, there are some data source completely, if a phase team used Flickr, the photo hosting website, as a source for the data set, then they got a large set of candidate images from Flickr. This is still quite noisy, so we need to do some amount of data cleaning. They use [INAUDIBLE] for that so crowdsourcing and then you have a nice clean data set. You can feed it into your favorite confidence, so when we look at this last spring, so february 2019, the state of the art top one accuracy on ImageNet was 83%. Okay. Now [INAUDIBLE] generalizations, just nothing special these specific images. Theoretically, if I generated a new dataset from the same source, following the same data cleaning process, and feeding it into the same [INAUDIBLE], you should get roughly the same number back. Not exactly the same number in the intercessor random process, but the test sets are quite large, so the error bars are quite small and you would expect about 82 to 84 percent accuracy. Okay, and at Berkeley we want you to understand, so how reliable is this generalization story? So what would happen if you actually instantiate did this experiment and this is what we did in the paper with the same name. Do ImageNet classifiers generalize to ImageNet was a big experiment, cost almost a year. It sort of summarized how to harvest light and pointers at the end, we had a new test set. We'll go through the details of how we burned it later. But, when we fed this into the state-of-the-art classifier, we didn't get 83% back, but we only got 72%. Okay, to put this in context. This is an 11 percentage point drop on ImageNet. That's a lot. This corresponds to about five years of progress in a very active area of machine learning research. So, this is clearly an issue and the main message of the target is going to be good and bad. You did a solo fitting but not enough when we think about it and it's here that we need to understand this better. Okay. So high level, this is what the talk is about. In the first part, we'll do a deep dive into this experiment that I just summarized and try to understand exactly what causes machine learning to be unreliable. And the outcome is going to be it's not overfitting through tests that we use, but even small variations in how you build the test sets can lead to large changes in the model accuracies. And in a second part we will look at one facet where we have learned over the past three years how we can make machine learning more robust. So for experts, this is going. to be about adversarial robustness So, this is what the talk is mostly about and these are important points. The way I think about this and something to keep at the back of your head throughout the talk is sort of this, maybe, a bit more fundamental question. How do we gain reliable knowledge about machine learning in the first place? As I said, we'll only really return to this at the very end but to keep this at the back of your mind. Okay, cool. So what causes machine learning to be unreliable? What's going on in this experiment? And arguably you have a simple answer for this in machine learning. Clearly, the models are overfitting. Okay? What do we actually mean by this? So, in machine learning there is, unfortunately, three different mechanisms that we use the term overfitting for. And to understand in more detail what's going on, we need to be more precise. So, the first notion of overfitting that's commonly studied in machine learning is this comparison of test error in training here, which is a key idea initialization theory. This is very important, but it's not going to tell us what's different between our two new test sets. Okay. So, we really ignore this for the rest of the talk. Instead, one overfitting mechanism that we look at in more detail is this idea of overfitting through tests that we use. We've used the ImageNet test sets many many times and used feedback from the ImageNet test set to develop our models. Maybe they are only good on the specific images in the ImageNet test set. And then the third mechanism, the third overfitting mechanism that we look at is what we call distribution shift. This is the idea that small changes in a data set you creep in and maybe the small systematic differences are substantial enough to lead to large accuracy drops, okay. So, think of these as sort of, as high-level as two hypotheses that these are part of our experiment. And to be a bit more precise and to convince ourself these are the right notions to look at, let's be a bit more formal about this. In particular, what we want to understand is this accuracy difference here. So, the accuracy on the original test set s of a given classifier f versus accuracy on the new test set s prime and I put a hat on these accuracies because these are finite sample estimates. Nothing special going on here. Just the average zero one loss over the examples in our tests. And overall as we said, these are 11 percent. This is what we need to account for and we'll do this in three parts. The first part is what we call overfitting through tests that we use and this is the gap between the accuracy on the finite test set s and the accuracy on the Corresponding data distribution b that the test set s is drawn from, so I put this here at the bottom left. No hat anymore because this is now the true average at the distribution. That was an expectation of the zero one loss. And the problem here is that the classifier F may depend on a test set s and this is why we don't have an easy way of finding this. The second term is now entirely at a distribution level. This is simply the accuracy difference between existing tests at distribution D and new tests at distribution D Prime and then in order to make the equality work out, we need a third term and the good news is the third term is easier to about so this is now the difference between the new tests at distribution D Prime and the new tests at s Prime and now here since the test set was split after the model F was fixed, we can just use standard concentration results to get a handle on this third term. And we can say with high probability this third term is at most one percent. So the remaining at least 10% need to be in either the over fitting [INAUDIBLE]. Ok So what do we do now is do a deep dive into these two terms to understand whether to what extent they're responsible for the accuracy drop. Okay. So first let's look at overfitting through test set re-use. And why is this an issue? Okay, let's think of how, what is the ideal machine language flow? How do we teach machine learning in an ML 101 class? Typically, might be teachers if you wanted a machine learning you build a data set by first collecting a large amount of data and then you split it randomly into a training set, a validation set, and a test se,t and from the beginning we sort of set the test set aside and never touch it until the very end. That's the contract. So, we build our model by training on a training set then tuning hyperparameters on the validation set and you repeat that many times until sort of the end, the day before the [INAUDIBLE] had a deadline, we have our final model and then only at the very end when the model is fixed you use our test set, feed it into the classifier once, and then we get our final accuracy number out. I would say 84% and because we've never used the test set so far, this is indeed a valid estimate of the population accuracy. So, we can put this into our paper and we can be confident that there's no problem with overfitting. Okay, so that's the ideal scenario. Unfortunately, this is not how much [INAUDIBLE] works these days. Machining has become a very competitive field and also, some of the datasets don't even come with a validation set anymore, like for so for [INAUDIBLE] for instance, we'll return to that in a second. There's only a training set and a test set and how the stability works now is people want to improve over the state of the art because that's the way to get a paper accepted, so we don't know a model that's already doing quite well. The problem is this is already some other activity day trips. So someone has tuned that model on the test set and we start out with them and then we do further development, we train on a training set. But often, what people do is they select the best model, the best type of parameters based on feedback from the test set. Okay so, there's more activity coming in and now the next deadline comes around, say Europe's, we have a larger model. they get higher accuracy, but what does this really mean? So we don't have any nice formal guarantee anymore that this ninety percent are indeed a valid estimate of the population accuracy, with a question mark here. And we are certainly not the only one to point this out like this. It's a classical theme of machine learning. For instance, Kevin Murphy's widely read textbook calls it a golden rule of machine learning not to use the test set for model selection because otherwise we will get an overly optimistic performance of our estimate of our performance. This also appears in data set, like Pascal VOC. It was the main image classification challenge before ImageNet and in the data set, paper, it clearly says they needed to build a new test set every year in order to avoid participants overfitting to the test set. Last year a group built a new test set for MNIST and then Yann LeCun, junior award winner last year, sort of called on the community to evaluate the amnesty models also on the new test set because their assumptions that probably people have overfit to the existing MS test set because it has been around for so long. And it's also often appears in online discussions or blog posts. Here's one that specifically calls our progress on the ImageNet data set is probably over fit and to be fair here, there's cause for concern because in the ImageNetcompetition, they did not build a new test set in every year. Instead, the test set that was used in 2012 is the same test set that they use in all following years. So many people have really over referred to those test set and then the variance that overall progress and the community looks more like this. If you think of this as a sketch, this is not real data, this is just a sketch. X-axis tested accuracy, y-axis the true accuracy on fresh data. If you had fresh data or deployed a model and then the issue is that if the overfill to the test set over time we only make progress on the x-axis only on the test set. But we don't really make true progress on your data anymore. And then this gap between the dashed line, which is what I do generalization and what's actually happening and a blue line would be this overfitting from tests that we use, okay. How do we figure out to what extent that's happening? And what we did in our omission experiment is the following. So this is one of the key plots in the talk. So I go through the step by step. X-axis-- original accuracy on the original image and test set. Y-axis--accuracy on our new test set. And to [INAUDIBLE] ourselves, this is again the no change line y equals x and now we build a large test set of ImageNet classifiers ranging from onyx net to the state of the art and early 2019. This already mentioned, the best model from early 2019 sees an 11 percentage point drop. So now the question is where do the other classifiers fall? And then you get the following. This is very surprising. So it's not just that the best model from early 2019 drops. In fact, all of the models drop by roughly the same amount. So the relative ordering of the models is very nicely preserved. In fact, it's an almost perfect linear fit and also the alex net model model from 2012 drops. If anything, it drops by a little bit more, so the absolute numbers change for the relative ordering is well preserved. And we did a similar kind of experiment also on another data set. So for 10, this is again image classification and overall, the picture here is quite similar. So again, all the models are below the dashed line, so all the models see low accuracy on our new test set. Again, built in a very similar way, but one thing that's striking here is that the slope of the line is substantially steeper than one. So what does this mean concretely? So if you look at the state of the art model, AutoAugment and it compares performance to ResNet. This is in this blob here. It's about a five percentage point difference on CIFAR-10, but this grows to a 10 percentage point difference on CIFAR-10.1. Okay, and this is important. This is exactly the opposite of what you would expect under this overfitting through tests that we used in the scenario that we had earlier, right. We said over time if there's overfitting through tested reuse these accuracies would plateau and if anything what we see a happening on [INAUDIBLE] is that the later models are better. They are not more overfit but they actually see a smaller performance drop than the earlier models. Okay. So this was very surprising to us. This is the exact opposite of what you would think would happen if you reuse a test set many years. We have what a community has reused [INAUDIBLE] an ImageNet for ten years in a very competitive process and they have effectively no sign of overfitting for tested views. This is really good news for their benchmarks. The relative ordering is preserved and the progress we have seen on these benchmarks is real. There's really no doubt about this anymore. But on the other hand, as I said, this is also quite surprising. So we were curious what happens on other data sets. Another one that I already mentioned is MNIST. So after our paper came out Yadav and Bottou and there are similar experiments of follow-up and this actually has a quite nice back story. So what happened there? It was that Leon's paper number two, and you don't know, okay, what's going on MNIST this is really interesting. And the funny part is that when they build MNIST, they actually build a larger test set of size 60,000 examples, but they only released 10,000 examples. At the time, we thought they will release the remaining 50,000 examples soon, but effectively they procrastinated on this for 20 years and then after [INAUDIBLE], we thought like okay now it's time to get out these remaining 50,000 examples because he wanted to know for sure if they had been overfitting on MNIST or not and then their paper reaches very similar conclusions. They nicely say that in their abstract that they observe the same trends as in our paper. And so again, the misclassification rates are slightly off and the classifier ordering and model selection remain reliable. So again, good news for the MNIST data set. But again, this is again only a small part of machine learning. It's the image classification. So directly we wanted to know what's happening in machine learning more broadly. And for that, we went to Kaggle, so Kaggle is a large platform for machine learning competitions. And we looked at hundred and twenty most popular classification competitions on Kaggle and did a meta-analysis of these competitions. And at a high level, the takeaway is that there's also little to no evidence of overfitting through tests that we use on Kaggle. So, if you plot public versus private accuracy on the kind of contestants, you always get this nice diagonal plots. So this course match. There's none of this [INAUDIBLE] behavior that people have worried about. Okay, so really this is surprising what's going on here. What is protecting us in the machinery community from overfitting through test set reuse. And I think this is a question we still need to understand better. There might be different mechanisms that people have proposed why there's little adaptive overfitting. One specific one that we looked at in Berkeley over the past year is this idea of model similarity. So at a high level what we do in these papers, we propose a way of measuring how similar to ImageNet models are, because what we observed is that overall, the ImageNet models are substantially more similar than what you would expect if the model errors are uncorrelated. And this is an important enzyme because once you have this notion of similarity and you can show that the models are quite similar. You can build a theory around this and show with an approvable way that this amount of model similarity on ImageNet already protects you to some extent from overfitting through tests that we use On the plot on the right we can see that for the model similarity that we're in, [INAUDIBLE] on ImageNet, depending on the assumptions you make on the theoretical side, you get a roughly 10 to a thousand x increase in the number of models you can test before you start overfitting. And so look, this is just a high-level picture here. As I said, there's more going on. If you want to learn more about the lack of overfitting here through tests that we used and I recommend you have a look at [INAUDIBLE NAME] keynote at code last year. For now, we are going to close the overfitting through tests that we use chapter. We're going to return to the high-level question. Where does the accuracy drop come from? So as I said, that this is the decomposition into three parts and now we can say there's overwhelming empirical evidence that it's not overfitting through tests of views. So this is 0%. And that means at least 10% actually do need to come from this distribution shift run. So let's understand it in more detail. Okay, in order to understand what's changing at the level of the ImageNet data distribution, we first need to talk a little bit about how was ImageNet ability to begin with? So let's do that. This process, ImageNet, is a creation process is nicely described in a data set paper and at a high level, it's the following steps. So first, you need a set of classes to have a thousand classes and they took these classes from WordNet, which is a large database hierarchical database of words in the English language. And then you get your classes and with every class comes a set of synonyms. For instance, goldfish and the Latin name for goldfish. And then what they did was to use these synonyms, search terms in the Flickr API and then Flickr gave them a set of images. And as I said, in the source this is a noisy data set. So we need to do data cleaning and the use mechanical turk in order to have the track work to select which emerges [INAUDIBLE] in the data set. Then, you get a nice clean data set. You solve for the [INAUDIBLE] subset and that gives you an image. Okay, so now where do the differences come in that we have in our reproduction exactly. So the high level we did exactly the same steps. We obviously use the same classes. We use the same Flickr API with the same search queries. We used a very similar mechanical turk process and obviously at the end we was a sample class balance dataset so we have way to the differences lie. One question we often get is what about time? We do this experiment in 2018 2019. This is 10 years after the original ImageNet data set was built and clearly in these 10 years, some objects have changed a lot. For instance, the cell phone now looks quite different from a cell phone 10 years ago, but we were very much aware of that when we built our new test set and a nice aspect of the Flickr API is that you can select a time constraint on your search query. So we selected that the Flickr API only give us images from a similar time range as the original image from a dataset. So those controls for temporal distribution shift on the Flickr site. Our hypothesis that even small variations in how you do Mechanical Turk process can have large impacts on the classifier accuracy. So for us, the MTurk part is the likely source of distribution shift. And to convince you of that, we look at this in more detail now. And so first of all, how do you even use Mechanical Turk to put ImageNet. And the main task is you ask the workers to do the following. You show them a grid of images. You [INAUDIBLE] certain candidate class already predefined. For instance, these will be candidate images for a class four and then instruction is: Please select all images containing a bow. So the work that would do that is these are the images containing a bow and you don't just do this once because otherwise it would be too noisy. So, you replicate this in the case of a minute and on our experiments, we replicated every grid of images at least ten times and what you in the end get for every image is what we call a selection frequency. This is the number of workers who selected an image divided by the number of workers who saw an image and you can think of this as a rough measure of quality. Images which very clearly show a bow get high selection frequency. Images that have nothing to do with a bow get those lower frequency. Images where the bow is smaller, like here, are somewhere in the middle. Okay, and then it comes like what do you do with the selection frequencies? How do you build a data set out of this? And this is a tricky part now because the exact details are unfortunately lost. But, what we did was sort of a best effort, in the sense that the pipe the existing ImageNet test set through our annotation process as part of building our new test set. And from that we got an average selection frequency for the existing test set, and then we built our first test set which we call approximately calibrated so that the average Mechanical Turk selection frequency roughly matches what the existing ImageNet test set has. There are some details but overall this is the high level picture and then you see a roughly at 12 percentage point drop in accuracy across our model test. Now the obvious question is like can we use our candidate set of images also to build easier data sets that don't have the accuracy drop and so we explore this with two different data sets for the purpose of this talk. Let's just call them easier and easiest. These data set sampling strategies lead to higher mechanical track selection frequencies and correspondingly, this is crucial. They also lead to higher accuracies for the ImageNet models. So in fact on our easiest data set that we could build with our candidate pool, the models actually see higher accuracies than on the original ImageNet test set, okay. So first of all, just clearly shows that the employ process through selection frequencies can have a large impact on the classification accuracies. And another crucial part that's important here is all the data sets are correctly labeled. This is not an effect coming from the fact that some images are just mislabeled. Why shouldn't I carefully event for our test sets many times to make sure the label quality is high? And now you can ask the question, okay, which data set is the right one. Should we calibrate the Mechanical Turk selection frequencies? Should we calibrate based on their ImageNet course and that's a good question. I would say so at a high level. Then models shouldn't really be so susceptible to these small differences to begin with, like when you look at these images as a human, you don't really see a big difference across this data set on average and the expectation from a human perspective is that you should get the same accuracy on all of these three data sets. But of course, this is just an intuition and we were curious, can we make this more rigorous, like more precise? What about humans in this plot here? Where would humans land and compare to the models. If you believe that humans are more robust classifiers, they should land somewhere on a dashed line here and get the same accuracy on the original and new test sets. Okay. How can we find this out? So to give up a little bit more context here, let's return to this plot from the beginning, ImageNet progress over time. I mentioned that we have to assume a baseline for ImageNet. One thing to keep in mind here is that this is actually only a single human. This is the accuracy that Andrej [INAUDIBLE LAST NAME] achieved and he did this experiment on himself for a blog post that he wrote in 2014. And he did this in order to get context for the performance numbers from the 2014 ImageNet competition. It's awesome that Andrej did this. Without this number, we wouldn't have any context. But I think that's also an interesting question, like how much variation would we get around this number from Andrej's experiment and do humans see a difference between existing tests at a new test set? Okay. So this was what we wanted to understand at Berkeley. Do humans see a drop in accuracy in our new test set? And again, this was a pretty long experiment, it took us actually more than a year. And I only give you the high level summary here. First and carefully, we annotate the original and new test sets to make sure we have very high annotation quality for both original and new test sets to make this a meaningful task. And then, we trained and evaluated five expert labelers on 1000 images each from the original and new test sets. So we took this 1000 images each randomly shuffled them. So, we didn't know what we were labeling when we label it and just each of us label these 2,000 images independently. And then what you get is the following. So, this is again original versus new accuracy and now you see that the orange points here there are five humans who fall very close to the dashed line. They are all within one percent of the dashed line. One of them is actually above the dashed line so that you want is doing better on the new test set and you can see a clear qualitative difference between human performance here and model performance. So I think this is very interesting because basically what we've done here, We've added a second performance dimension to the ImageNet task the humans effectively solve implicitly, like when you do this, it is subjective. It's not harder to classify the new images, but the modelers have a really hard time generalizing to the new tests. So like just a systematic drop and one interesting data point here is also this green one. So I highlighted this one because this is a model trained on a lot more data. This was an experiment done by Facebook. They trained a dimensional model on a billion images from Instagram. So, a thousand times more data and the image in the training set and even up training on a thousand times more data is far from closing the gap. It's a little bit above the red line given by the other models. But, this is just problem here is not easily solved just by training on more data. Okay, so far so good for image classification. I'm always interested as in the case of overfitting through test set reuse, to what extent are these universal phenomenon that happened in machine learning more broadly. So we did again. This is just a one slide summary of a long approach project. We did a very similar set of experiments for a natural language processing, in particular, to Standford question answering dataset, and effectively built a set of new test sets for a SQuAD and then the high-level takeaways are exactly the same. There's no overfitting through test set reuse, but the classifiers are quite susceptible to small changes in how you build the dataset, but human performance is more reliable. Okay, so these are the experiments that we did. What are the takeaways for machine learning? So I think one clear take away is that distribution shifts are a real problem, even if you've worked in a carefully controlled usability experiment. And I think this is really the fundamental challenge we have in machine learning at the moment, that often these models fail in ways we don't really understand yet. And this is type of, sort of implications again for the application areas we mentioned earlier. So for instance. safe driving cars is a good example and specifically the promises made by Elon Musk in Tesla. So the initial promise was by end of 2017, Tesla would have an autonomous coast-to-coast drive. Then, in early 2018, Tesla was like, ahh not quite there yet, maybe three to six months. Then some time passed and then in July last year, Elon tweeted parking lots are remarkably hard problem. Doing an in-depth engineering review of enhanced summon later today. So, enhanced summon is the feature they built where you can have your car drive towards you from the other end of the parking lot. Since that was coast to coast, we are now doing 20 meters in a parking lot and it's still remarkably hard and then to be fair, they released this feature and I'm sure it works in many cases. But you also quickly had videos like this up here on the internet where the person was calling the car to drive towards them and the Tesla would have hit incoming traffic if they hadn't stopped it. And this is the benign end of the spectrum. People have died in accidents due to Tesla's autopilot and one person has died in that crash of the Google self-driving car. I think it's very much a challenge at the moment for machine learning So, how can we make this one's more reliable and how can we test them to actually certify that there are three layers we would like them to be. This is not just transportation, this also comes up in healthcare. This is another area I mentioned, so this is a nice paper which does a generalization for x-ray chest classification and then the conclusion in the paper is, in quoting the paper now so they say, even in the absence of recognized confounders, you will caution following directing colleagues, they are referencing our paper here, that current accuracy numbers are brittle and susceptible to even minute natural variations in the data distribution. So this phenomenon that you see in the benchmarks are very much representative for challenges in concrete applications of machine learning as well. I think this has implications for how we evaluate machine learning. So the current paradigm often, is that the data, that is, I mentioned by splitting it randomly into training validation and test and it's a good starting point. But, it's important to remember they're not learning that much from having separate validation and test sets. In the end, effectively all the classification competitions we looked at validation and tests and accuracies are agreeing, but it's not a good measure of reliable generalization This is just telling us that there's no overfitting through testing we use but that doesn't seem to be happening. Anyway, so I think what we need to be doing instead of this is building new test sets by introducing small variations of the existing data set creation process. I think what these small variations are is a very good research question. But overall, going a little bit away from the training distribution can give us a better sense of how reliable [INAUDIBLE] currently are. Okay, cool. So, this was the first part of the talk. The conclusion is no overfitting for tests and reuse but small distribution shifts are already a challenging problem for machine learning. And for the second part of the talk, let's look at one facet we have made some progress in making machine learning more robust and just for context here, we're definitely not the only ones who are saying that robustness is a challenge, a challenge for machine learning right now, the distribution shift, is one facet here, but there are many others. So, one we talk about more now is adversary examples. There's been lots of effort in this direction and there are many other sort of failure modes of current training models that people are pointing out in the moment. Various image corruptions to people looking at you can fool models with small rotations and translations. And overall, this is a quickly growing area. So Nicolas Carlini, one of the researchers in this area is keeping track of all the papers on adversary robustness and yeah, I guess by now sort of roughly a thousand papers being published on this topic alone. Yeah, and this also has a long history obviously, so when you want to trace this back, there's a paper from Abraham Bile in 1944 with the title statistical decision function and functions which minimize the maximum risks. So, over 70 years 80 years later, this is still very much the same topic that we're interested in. Ok. So for now let's look at adversarial  examples. I'm sure many of you have seen this before. The idea is that you can take an image that I classify as correct, like this pig, add a little bit of carefully tuned noise that's ben carefully tuned that is easy to find, and then you can fool the model to basically say whatever you want. For instance, let us classify the pig ears as the airliner and then a security setting is very much a challenge. So, what can you do about this? And this is work I did with my collaborators back at MIT, where I got my PhD. And just to give you some context before we start to look at our robustness. The state of the art was roughly our arms race between attackers and defenders, in the following sense that one group would propose a new attack, a new way to construct these small perturbations. Then, another group was proposing a defense that worked against that specific attack. But unfortunately, what happened almost always is that soon thereafter, another group also proposed a different attack, fine-tuned the existing attacks in order to break the new defense. And this happened over and over again, so then eventually Nicholas Carlini, one of the main people on the attack site started writing papers like this bypassing 10 detection methods, because there were so many papers out coming out that he could easily break, these all have started batching the attack papers he wrote into these larger meta papers. So this is very much not robust, and what can we do about this? And we are thinking about, I might use that when you want to be adversarially robust, you really need to think about a fundamentally different optimization program. So this is how we typically optimize our models in machine learning. We want to minimize over model parameters--the expected loss over data distribution. Okay. But now when you have an adversary in the picture, what you actually want is the following. You minimize over the model parameters. You still take an expectation of our data distribution, but now you maximize over some perturbation set, before you feed the example the perturb example into the loss function, okay. This is what you would like to get and the idea is if we can truly get such a min/max guarantee, then we would be robust against any attack, then we can sort of put an end to the arms race. But the obvious question is like how do you actually get a min/max guarantee like this? And that's where our background in optimization theory came in handy. So in particular, there is a classical result in optimization in Danskin 's Theorem, which tells you how to take the gradient of a function that's defined as a maximizer of another function and the idea is simply that you take the maximizer and then the gradient of the inner function at that maximizer, like it is overall just maximization over other functions exactly what we have in this robust loss function. Now the tricky part is, okay, how do you actually find the maximizer? And this is, unfortunately, a hard problem, in the sense that people before us have already shown that it's NP-hard to find that maximizer exactly or in a simple network. So, you can't hope to get a nice look in [INAUDIBLE] anymore, what we found empirically was that a few directions of projected gradient descent already performed very well empirically in at least approximating that maximizing. The question is what do you do with this? So, we plug this into a training procedure as the robust optimization and at the time, we've got the state-of-the-art accuracies on MNIST and CIFAR10 remain true benchmarks in this area. And but now, as I said, this is empirical. So the question is are these models really robust? So the first thing we did was we put out two nicely polished github repositories to make it very easy for the community to attack our models. And this has been out now for almost three years and there has been no progress beyond maybe shaving off two or three percent. And it's not because people didn't try. So, Nicolas Carlini and collaborators continue to break models. This is a table here from the ICML paper in 2018. They actually won the ICML best paper award for that paper and because then what they showed us again, they could break all proposed defenses, except ours. So, the highlighted line here 47%, this is exactly what we put in our paper. They were not able to break this. Okay. And at a high level, this is still a summary of the state-of-the-art. The highest robust accuracy numbers that we have at the moment follow the robust min/max optimization approach that we introduced in our paper. And we're, sort of also a separate line of work on methods which come with proven guarantees for robustness. So [INAUDIBLE] group has done work on that, also some work from Stanford, like John [INAUDIBLE], but in terms of you just look at the accuracies that we can't currently break those min/max approaches still giving me the best numbers. Of course, this is still empirical, so you could worry, and this is a good thing to think about. What if the next people find a better attack and it can then maybe that way break our model. But by now, we actually have good evidence that this is not going to happen. So in particular, Nicolas and collaborator and also on a paper from Alexander's group, they did the following type of experiment. They use this robust optimization approach on a small enough problem so that you can at the end just take a trade network and use an exhaustive verification method on the train network. So for MNIST problems, that works, so you don't get a guarantee from how you train a model. You just take the model as a fixed black box and then verify that it has certain properties. And they have shown these papers that the model that comes out of min max robust optimization is indeed provably robust. So, we know for these models, there's not going to be an attack which is going to break this model. So that's good and asking lots of follow-up work that we could talk about now. People have scaled it up to ImageNet. People have used it for different perturbation setsand so on. For the remainder of the section, I'd like to focus on why did we get stuck at 47 percent, why I missed this work, on MNIST this looks substantially better. And when you look at the details of training these models and to be able to sell robust on CIFAR-10, the interesting phenomenon is that on the training set that actually works really well on the training set, you effectively get hundred percent robust accuracy. But, it fails to generalize to the test set, then you've got only about 50 percent accuracy. There's a large gap between training and test set. And then we were wondering is there maybe some fundamental reason for why you would see this behavior. And this paper we brought in from Alexander's group, together with [INAUDIBLE NAME] who back then was a Google brain is now at Apple and a question we asked, does robust generalization maybe require fundamentally more data? And we show in a simple data model and that is yes. This is the case already in this simple data model video, we have classified data coming from two Gaussians. You can show that there's a severe OD, some complexity separation between stand up generalization and Adversarial robust generalization. Okay. So so by analogy, this is a nice piece of evidence that adversarial robust generalization may just be a fundamentally harder problem. What I often try to do is ideally connect these theoretical insights back to the empirical side, like can we use as theoretical ideas in order to also improve the empirical state-of-the-art. And the nice thing is that this happened here. So in the meantime, so now, I'm going to talk about work of collaborators at Stanford. So what happened in the meantime was that a group from Berkeley and CMU improved over the result Alexander's group and I had and they got to 53% [INAUDIBLE] and I was curious, okay, if there's more data limit, is there really the bottleneck? Can we train a better modelat higher robustness by using more training data? And the answer is yes. And a nice aspect here is that we don't even need more labeled data, unlabeled data suffices already. Initially, I was a good point that you would have to do a lot of labeling to make this work, But, then we found out that unlabeled data is already enough. This was actually motivated by insights also from the theoretical more analysis, joined back with [INAUDIBLE] and at a high level what we do there is we use a high accuracy standard model to generate pseudo levels for unlabeled data. And then, we do robust optimization on the data on a large or unlabeled data are using pseudo-labels just as true labels. We call it robust self training. It has certain guarantees. And as I said on the empirical side, we do get state-of-the-art robust accuracy So if you combine the label so for ten data with unlabeled data from tiny images, you get 60% adversarial robustness and this is as of now still state-of-the-art. So there's nice things you could connect back and kind of close the loop between the theoretical and empirical side. Okay, this is all I want to say about adversarial robustness, how do the two parts fit together? And so at a high-level how I think about this is that there's a lot of work at the moment around robust machine learning. This is something we clearly need to understand better and you can sort of organize this as a spectrum. At the one hand is the very worst case for it, like adversarial examples where someone really tries to fool your model, and at the other end of the spectrum is the first half of the talk and what is distribution shifts. Here, we try to be as benign as possible in order to really recreate the existing distribution. So this is the entire spectrum. So from a more average case to worst case, when I go question at like what is in the middle and collaborators and I both at MIT and CMU in Berkeley, we have some works on this. In interest of time, I'm not going to say more here, but feel free to ask me about this later. And another question is like how well do current robustness methods, how much do current robustness methods have on this new ImageNet data set. As we said, ideally, a robust method just like humans would land on the dashed line at the same accuracy on original and new test sets. So we thought okay, maybe something already helps here, so we built a large test set. This is ongoing work at Berkeley. The baselines here are just the models trained on damage that you would like to improve over the space line here the red line. And the sobering outcome was that we took everything out there, not just an infinity at the cell robustness, also the other methods the people had proposed and these are the brown points. And the takeaways is that the brown points effectively follow the same trend as the standard models. So, current robustness interventions do not help close the gap for the new ImageNet test set. One thing that helps a little bit are these green points here. These are models trained on a lot more data. So I already mentioned this experiment here trained on a thousand times more data at Facebook. This data point at the very right is trained in a similarly large experiment, at Google they trained on 300 million images, so three hundred times more data than the ImageNet training set. The takeaway is that yes more data helps, but only by a small amount, so we are not going to solve this just by throwing more data at the problem. Okay, and I think this is already pointing towards a very clear direction for future work, so the current tools plus just more data and compute-- they are not going to close these accuracy drops. So what can we do on the algorithmic side in order to make the models for robust? We know we have some tools now that work and some settings. Maybe we can take some of the ideas that work for adversarial robustness and also make them work in the more benign distribution shifts. Yeah, I think this is one facet. The other one is what a good benchmark is, so promoting robustness in the sense that what are the types of robustness that we want our models to happily deploy them in the real world and how can we make sure that these are the notions of robustness we measure in our benchmarks? The second direction is what about machine learning in more interactive settings? Everything I told you so far was about this classic unsupervised learning setting and it's a good starting point, but it was curious what happens in more interactive forms of machine learning, like causal inference or reinforcement learning. And I think here simulation is going to be an important tool in order to probe the performance of the models that we train from data. We've already done some work in this direction with [INAUDIBLE] group at Berkeley, building a test bed for causal inference by using domain knowledge that people have capsulated in various simulators, like epidemiology, climate economy models, and so on and seeing if causal inference methods can figure out these simulators In a third direction, then I'm very interested in us studying data sets in addition to models. Over the past decade, we've become very good at this framework of here's a fixed benchmark data set and then iterate by making better and better models over time and this is this is good, this video-- improvements accumulate over time. But we don't have a good understanding of what made ImageNet such a good data set to begin with. The fact that it supports progress for a decade is highly non-trivial and effectively supports transfer learning quite well is also not something you would necessarily expect. So I think understanding sort of what makes ImageNet a good dataset and having some guidelines for producing sort of new data sets in machine learning both for academic use and for people in industry, I think it's an important question. Cool. So for the next, for the last two to three minutes, let's return to the iceberg. Right, at the beginning, I mentioned this overarching underlying question. How do we gain reliable knowledge about machine learning? Because one topic this has come up a couple of times in the talk is that some of those of conventional wisdom that we have in machine learning doesn't hold up that well under closer scrutiny. So for instance, lots of people told us that tests that we use would lead to overfitting. But then we looked at a lot of different highly competitive machine learning benchmarks, and there's effectively no evidence of overfitting through tests that we use. Similarly, many people have said that neural networks have now surpassed human-level performance, or benchmarks on ImageNet. Once you take a closer look, things are actually a lot more inwards. And then another point that has come up in the talk is to what extent the robust models actually are robust. So Nicolas Carlini has done lots of important work here to test proposed defenses. There's also this question-- how does one notion of robustness transfer to other notions of robustness? Okay to be fair, it's not like all is bad. I think the fact that we have these benchmarks like ImageNet, as I mentioned support progress in a measurable way for decade is really a success. This could have the graph could have looked quite differently But the fact it is such a nice line is so I think really good evidence that yes there's progress and these benchmarks are a good way of measuring that, but on the other hand, our conferences are now getting up to 7,000 papers submitted per year. We have multiple conferences. So I think there's a real challenge for the field how do these those of broad reliable knowledge from this large set of papers we are publishing. So I think we all agreed and what we need to do if we want to have machine learning in these safety critical domains I mentioned earlier, these of convert machine learning from the maybe or maybe doing it right now. So sometimes speculative area of science into a field that looks more like a reliable engineering discipline. This is going to involve many facets of experimental work and theoretical work. As Kevin mentioned, my background is actually only theoretical sided in a PhD in computer science. I'm really excited about working on both aspects here, but I think one key point that is going to be important is do we need to have experiments to test and build foundational understanding this is a quote that I like a lot by Henry Margenau. Henry Margenau was a German-American physicist and philosopher of science and he said measurement is the contact of reason with nature. So on that note, I'm going to leave you with my favorite ImageNet plot, and I'm happy to take questions. Kevin Jamieson: Thank you Ludwig. Thanks for a wonderful wonderful talk. So if any one of these questions, please just type hand in the chat and I'll be the MC. Jamie. Jamie: Hi, thanks for a great talk! So I have a question related to the Kaggle set you talked about. How many of those were image classifications? Ludwig Schmidt: Well, it's a great question and off the top of my head. I don't know exactly but not the vast majority. So what you often get in Kaggle--what you often get in Kaggle is Sometimes NLP tasks or you have time serious data and you want and the companies want you to help them predict what our customer is going to do and so on. That's a good question. I should look that up. But I mean, okay I don't want to say a number now, but I think probably more than half is not images. Jamie: Specifically for the cases you looked at, is that the case? The ones you were looking at weren't images? Ludwig Schmidt: Yeah, so I'm trying to remember which are these four. So I think out of these four, I would guess maybe even all of them are not images. It's a great question. Yeah, I mean et me look it up. If you're having dinner later today, then I can tell you how many of them are images But it yeah, it's I think it's fair to say that more than half of them are not images. If not, if I think it's actually a few more, but I want to be conservative so I'd say more than half is not images. Kevin Jamieson: Jennifer. Jennifer: Thanks for the talk. I'm having difficulty understanding part of the-- so you redid this ImageNet like test set gathering and you use the Mechanical Turkers and then you had this this comment about like the selectivity and there was like a threshold there. Then you also talked about, you know, you look at human performance and something I'm a little bit confused about is the people on Mechanical Turk are also humans. Ludwig Schmidt: Yes. Jennifer: So, presumably like like two questions. One, presumably there's some sort of like test set accuracy like you don't just have to have these five people that labeled... Ludwig Schmidt: Yes. Jennifer: ...examples. You could also ask like how Mechanical Turks do and then also I guess the fact that I don't know this that you said I mean it looks like if you start to be numbers and I don't remember exactly what was here, that it looks like in some sense, it's harder for the Mechanical Turkers to get this right. So I guess I'm also a little bit confused how your accuracy plots show humans doing so well. Ludwig Schmidt: Yes, okay. This is a great question because they think it gets an important distinction here. So important point about building these data sets like ImageNet, when you build ImageNet, you never ask a human to classify the same way you ask a network to classify. The only decision that humans make when building a data set is here's an image of a bow. Should we include that yes, or no, humans make only these binary decisions, humans never are given a single image, like which of the thousands of ImageNet classes do they belong to. So during data set construction, you only have humans filter not classify. So all the selection frequencies here are measures of how do humans react to the images for the filtering task, these are not comparable directly to human performance when you ask humans to classify images. So what I had humans do here for this plot was here humans actually did this thing you showed them and how much and then which of the 1000 classes does it belong to. So and why are things set up that way? In order to classify ImageNet. You actually need to know a lot about class hierarchy. So ImageNet as I mentioned has 1,000 different classes. 120 of them are dog breeds we learned a lot about dog breeds as a result of this experiment and I don't think it's, you don't, you can't expect MTurk workers to notice 1,000 ImageNet classes. It takes a long time to basically become good at ImageNet. And this is why we will eventually do this also mechanical turk. We are obviously curious how well mechanical turk workers can do that. So this is definitely on our to-do list. It's a good question. But I don't expect them necessarily to do well and I don't think it's going to be that meaningful, given the large ImageNet class hierarchy to ask humans to do this. So I think yeah. Thanks for pointing this out. I think it was quite helpful if people understand the distinction between selection frequency as a measure of image quality and then human performance on the actual classification task. So, I hope I clarified that but if it's still unclear, please let me know. Jennifer: Thank you. Kevin Jamieson: Lee. Lee: Hey, so I think one of the main takeaways was that models still have like issues with shift of the distributions right now, yeah region shift. So how does this translate to transfer learning because I mean as far as I understand transfer learning, the promise is that you trained on a certain day. I said like a certain distribution and then you do the transfer on a different distribution. Right, so does this imply that maybe transfer learning shouldn't work as we expected? Ludwig Schmidt: Yes, that's a great question. So yes, I think there's a whole-- you're right--there's a whole variety of advice you could take it that question and I think depending on the overall problem you are looking at, some ways are more appropriate than others. So I think to create, I interpret transfer learn at least this here's some data from the new domain. Let's fine tune in on the data from the new domain and then we evaluate on test data from the new domain. What are you doing here is different. The images in ImageNet models are just trained on ImageNet and we evaluate them on a new test set, which is very close to the existing ImageNet distribution. But the models have never seen our new test sets or our training data from our new test sets. Obviously, we were also curious. Okay, this is easy to fix, we did some experiments along the lines of transfer learning if let's say we have our user for [INAUDIBLE]. I just fold some of the images from our user for ten test set into our training set and then I train on the combined training set, but it doesn't help and yes, we have some movement in the direction now. I'm happy to elaborate it now or later. But it looks like these data sets are hard, or not just different. Just one note there and I think for some of for these test sets I think in the end if we want reliable machine learning, the answer can't be every time you see a performance drop you need to train right because then every time you make an error, you would say, oh it's not the model's fault. We just didn't train on this thing at some point. You really need to expect, okay that for some distribution shifts, the models have higher performance on both distributions because as we have seen, humans can do it, so it's not sort of information theoretically impossible Lee: Yeah, so I have I'm wondering about sort of whether old-style techniques in the coming out of like PAC learning. So boosting techniques can can be used. Do you see any sort of possibility about using them to kind of come up with a variety of distributions in order to improve what's going on or Ludwig Schmidt: So, when you say come up with a variety of distributions, lo I would the way I think about boosting and if you have a different perspective please let me know, as a way to construct classifiers. So we could, for instance, try to build a classifier with as well on the harder examples. If we say that CIFAR-10.1 and ImageNet lead to a harder one. Maybe we can up way--so and examples and the existing dataset in order to do better on the new data set. So boosting like that, and yeah, we have tons of preliminary experiments in this direction. So far the answer is no, but I think it's a very interesting question. To explore this more, so if what techniques do we have in order to make a model more robust distribution shift. And there's a lot of stuff out there, I mean I had the small plot that we took everything that people have proposed in a sense of pre-trained models. There is obviously much more out there, which is not in a pre track model. So I'm very interested in sort of evaluating how other current approaches do this. So far, it looks like it's a hard problem and kind of approaches don't help. But yeah, if you have sort of concrete ideas, please do let me know and we'll be happy to try that. Kevin Jamieson: Linda. Linda: Yeah, I was excited by your talk because we have a problem that kind of fits into this. We I was looking at my students results today and we have a data set on melanoma cancer divided in five classes and class one is benign. There's nothing wrong even though there was thought to be enough wrong to do a biopsy and class five is melanoma cancer of the worst kind and my students are developing sort of new approaches that are not just throw it at the deep learning classifier. But one of the doctors asked us can't you just try throwing it at the deep learning classifire to just separate classes one and five? So, the students tried that and to our surprise it did not come out a hundred percent classification. What came out was maybe 74 percent and it turned out that it was slanted in favor of class 5 and in looking closely, it turned out that there were about twice as many samples of class 5 as class 1 because they had purposely sampled more of the more difficult classes. So, we were discussing what we would do next and whether we would try to augment them and whether we would try to use guns, and then I realized that a few weeks before we all went into shutdown, one of the students had gone over to some office on the Ave and obtained another whole set of samples was given to us. And we have never looked at them yet, and I'm I'm saying it was obtained in an office on the Ave, but they're actually digital and they were put on to our discs and there's like another data set just like what you're looking for. Ludwig Schmidt: Yes. Linda: Somebody had blessed us. They had another set of samples collected from the same patients as I understand it. So now I'm thinking, boy, we better start using these right off. We should have-- should not have just put them into deep freeze. And the so the question is what is the best thing to do? My my first idea instantly was to grab 31 more samples of benign and throw it in and see if we can do better, just because we have a deadline for the report of the grant coming up in a few months and we'd like to show really good classification. But maybe there's something else we ought to do before we spoil those new samples. Ludwig Schmidt: Yeah no, that's a great question. I think at some level what I'm saying is we tend to at least for reasonably large test sets that we've looked at in Kaggle, CIFAR10, ImageNet, [INAUDIBLE] it seems hard to actually spoil samples in the sense that we tend to not overfill through tested reuse to the samples. So it wouldn't be as much I would be that worried about that aspect. I think what you could do, which will could be quite informative us if these new samples come from a different data set generation process, like maybe it's a different hospital or different patients or was connected with a different method or something and using both of them as test sets could be quite informative to see how where does your message generalize to data from a different distribution. Linda: Okay, but it's not a different set of patients it's the same patients with different biopsies of the same patients.. Ludwig Schmidt: Okay. Linda: and that's why it could be very helpful to us to make additional data. But it's sort of like you tried to make more ImageNet. It's like we suddenly got more samples of the same patients without having to do anything because they gave it to us. Ludwig Schmidt: Yeah, I think the first thing I would try to make sure is if they're actually from the same distribution, you should get the same performance. If that happens, then it's a good test that at least you are sort of generalizing across these two data set collection processes. Linda: Okay, we'll try that first and then I'm gonna grab 31 benign just just just to see. Ludwig Schmidt: Yes. Audience member: And I think Ludwig. This is a question that I know we've been trying to answer That perhaps you have some insights on. So the the problem is, you know, you trained your network on CIFAR 10. And now I want to generalize it to see 400 and the the network at least All I want it to do is at least tell me that it's a novel class. Right, and I think at least from the conventional and traditional deep learning point of view, these networks are pretty highly confident. Even if I give you you know the 11th class from your CIFAR 10 sort of classes that, hey just one of those 10 classes that I've been trained on so these two insights that I've heard about in the last maybe a couple of years ago was that you need to truly model the generative process itself as opposed to training it in a CNN. So, any insights on how to tackle the novelty detection problem when you go from CIFAR-10 to C400? Ludwig Schmidt: That's a great question. It's not something I've worked on myself. I know that people have studied us and have come up with decent baseline methods sort of compared across the front method. So off the top of my head, I would look at one or two of the papers that [INAUDIBLE] written on this sort of distribution detection problem and go from there if yeah, I can send you a link to that later. I don't remember the titles off the top of my head, but it's certainly something people have looked at intensively. Audience member: All right. So so I guess the idea is not necessarily that you have to model the whole generative process. Ludwig Schmidt: No. I don't think that's what they're doing there. I think they're using confidence plus some modifications at the top. Audience member: That's interesting. Yeah. Thanks. I'll look it up. Kevin Jamieson: Okay, well we're over time and let's thank Ludwig again and send him off. Thank you. Ludwig Schmidt: Thanks. Kevin Jamieson: Thank you. 